<?xml version="1.0" encoding="utf-8"?><rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>JSON REF</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1646</link><project id="" key="" /><description>Support indexing and query traversal through inter-document references according to the JSON-REF ietf proposal 
- http://tools.ietf.org/html/draft-pbryan-zyp-json-ref-00
- http://json-schema.org/json-ref
- http://livedocs.dojotoolkit.org/dojox/json/ref
- http://www.sitepen.com/blog/2008/06/17/json-referencing-in-dojo/
</description><key id="3001538">1646</key><summary>JSON REF</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">mattaylor</reporter><labels /><created>2012-01-27T23:15:45Z</created><updated>2014-07-18T17:59:48Z</updated><resolved>2014-07-18T08:42:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T09:22:18Z" id="20508644">Interesting proposal. How would you want to use this inside of elasticsearch? Do you have some samples?

 We would also need to reference index and types in order to work cross index wise. Do you think this can be done? (I only had quick peek at the documents, one is 404 - maybe there are newer versions).
</comment><comment author="mattaylor" created="2013-11-05T15:15:04Z" id="27780322">http://elasticsearch-users.115913.n3.nabble.com/JSON-REF-and-interdocument-link-traversal-for-indexing-and-retrieving-related-docs-td4018483.html
</comment><comment author="clintongormley" created="2014-07-08T14:05:37Z" id="48340137">Hmmm. This could be incredibly expensive to implement on a distributed store like Elasticsearch. I'd rather have users implement such heavy operations in their applications, so that they are aware of the cost.
</comment><comment author="mattaylor" created="2014-07-18T17:25:27Z" id="49457359">Coul this cost be made more manageble by using  numeric keys for fast query time joins? (http://blog.seecr.nl/2014/02/24/a-faster-join-for-solrlucene/)
</comment><comment author="clintongormley" created="2014-07-18T17:59:48Z" id="49461135">Query time joins in a real-time distributed environment are enormously costly.  It's a no-go.  The blog you link to sounds impressive, but the environment is not real-time distributed.  We would need to ensure that related documents are on the same shard, and we'd need to maintain a real-time map (their cache) between related documents.  We have this functionality already with parent/child support.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Counts in date histogram facets are inaccurate on nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1645</link><project id="" key="" /><description>When using date histogram facets we're finding that the values returned are incorrect when using nested documents.

A script to load 5 documents and query them is here:

https://gist.github.com/1684494

It loads documents with the following structure:

``` json
{
  "title": "Article x",
  "actions": [
    {"date": "2010-11-18", "updated": 1, "action": "updated"}
  ]
}
```

and then queries for all documents that are called 'Article 1', with date of '2011-11-18'. This correctly has 1 as the value for the _updated_ facet.

However, if the script is changed to load _6_ documents, the same query returns a facet count of 2 instead of 1:

https://gist.github.com/1684506

We've shown that increasing the number of stored records also gradually increases the error, but this is the smallest number of records we've found that illustrates the problem.

We've also found on our live system with millions of documents, that if we increase the date range in the nested part of the query that then reduces the scale of the error. However, we've not been able to come up with a set of scripts that illustrates that problem.

We're using ES 0.18.7.
</description><key id="2991821">1645</key><summary>Counts in date histogram facets are inaccurate on nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markbirbeck</reporter><labels /><created>2012-01-27T08:49:13Z</created><updated>2014-07-08T14:02:32Z</updated><resolved>2014-07-08T14:02:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markbirbeck" created="2012-02-15T22:59:18Z" id="3991396">Hi @kimchy,

Have you (or anyone else) had a chance to look at this issue? It would be much appreciated if you could.

Given how badly out the numbers are I'm surprised that no-one else is seeing the same issue. I'm open to the idea that it could be a configuration issue on my part, but I can't for the life of me think what it could be. And what's most strange -- as you'll see from the Gist -- is that the problem can be reproduced with a very small number of records.

Thanks for any help you can give!

Mark
</comment><comment author="kimchy" created="2012-02-16T19:25:19Z" id="4007452">It might be because not many use scoped facets on nested query within a bool query. To be honest, its still open as to what is the correct count to return in this case, is it an effort to return just the matches on the nested query, regardless of what other query, or return just relevant intersection of the two. What counts do you expect to get back in your gist?
</comment><comment author="markbirbeck" created="2012-02-18T01:34:40Z" id="4031039">Hi Shay,

Thanks for getting back to me.

I'd expect a count of 1 in both gists since they both have:

```
...
{ "query_string": {"query": "title:\"Article 1\""} }
...
```

as part of their query. The first gist does return 1, but the second doesn't, and the only difference is that in the first gist 5 records are inserted, and in the second gist 6 are inserted.

I'm afraid I haven't had time to experiment further but I was going to see whether the number of shards made a difference; it seems a bit of a coincidence that the jump happens at 5! (And I was also going to see whether I got another jump between 10 and 11...I'll try to find some time over the weekend to see if I get that.)

Thanks again.

Mark

PS Scoped facets are the main reason we're using ES...putting aside this inaccuracy they work really well. After I first spotted this problem I changed tack and tried to see if I could get the behaviour I needed with some of Solr's newer features. Although I could get exact numbers by using hierarchical joins, the performance was substantially slower than with ES. In fact it was so much slower that we're back to using ES, and living with the incorrect -- but fast -- numbers. (With fingers crossed of course, that the numbers will at some point be correct.)
</comment><comment author="markbirbeck" created="2012-03-23T12:58:25Z" id="4658479">Issue still present in version 0.19.1.
</comment><comment author="markbirbeck" created="2012-06-11T11:25:29Z" id="6242032">Bumping this again.

@kimchy The issue is related to the number of shards. I thought I'd run the tests again to see how things were with 0.19.4 and at first it seemed that the the error had disappeared. However, we've started using 6 shards as our default setting now, rather than 5, so I increased the number of records inserted by 1, and sure enough the error reappeared.

One other thing: you mentioned in our previous exchange that it was an open issue as to what values should be returned when doing this kind of query, and whilst I agree with you that there are a number of ways this could be interpreted, the key problem here is that we get _different_ results depending on how many records are in the index. So to clarify the issue:
- if we insert 5 records and search for a specific record with facets we get 1 hit (correct) and 1 facet (correct again);
- however, if we insert 1 record more than the number of shards and run the query again we get 1 hit (correct) and _2_ facets (incorrect).

If you have any chance to look at this it would be much appreciated. I would be really surprised if this was a localised issue, and would guess that there are other results that would be impacted by this problem.
</comment><comment author="clintongormley" created="2014-07-08T14:02:32Z" id="48339731">This issue has been resolved in aggregations.  Nested scopes are no longer supported.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Comparators defined in TermsStatsFacet violate contract that is strictly checked in jdk 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1644</link><project id="" key="" /><description>While running elasticsearch on jdk 1.7, faced a bug described above.
I've also made some code refactorings, I think you will like it.

There are 6 failed tests when building from sources, is it a normal practice to commit to master branch with failed tests or is it because of jdk 1.7?

P.S. There are also probably other places in the code where this fix is required, please check.
</description><key id="2991383">1644</key><summary>Comparators defined in TermsStatsFacet violate contract that is strictly checked in jdk 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kodart</reporter><labels /><created>2012-01-27T07:47:23Z</created><updated>2014-07-16T21:55:42Z</updated><resolved>2012-01-29T16:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-27T23:34:49Z" id="3696220">Does the problem comes from the fact that if both are null, 0 is not returned? (I haven't check the new tim sort impl in 7).
</comment><comment author="kodart" created="2012-01-28T07:01:15Z" id="3698735">Yes, I think that's the reason.
The contract states that compare(a,b) = -compare(b,a)
But previous implementation violated that fact when both entries were null.
</comment><comment author="kimchy" created="2012-01-29T16:05:05Z" id="3709121">I have created a new issue here: #1647 and will push a fix there which is similar to what you did (though will not add the extra polymorphic call to another method) and also compile under 1.6.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expand documentation for include_in_parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1643</link><project id="" key="" /><description>`include_in_parent` and `include_in_root` are only mentioned in passing on http://www.elasticsearch.org/guide/reference/mapping/nested-type.html – it'd be helpful to have an example of how to use it or have a table for the extra attributes nested takes similar to how the core types are documented on http://www.elasticsearch.org/guide/reference/mapping/core-types.html
</description><key id="2988644">1643</key><summary>Expand documentation for include_in_parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abh</reporter><labels /><created>2012-01-27T00:29:46Z</created><updated>2014-07-08T13:54:28Z</updated><resolved>2014-07-08T13:54:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gjb83" created="2012-02-03T19:47:34Z" id="3803072">+1
</comment><comment author="rb2k" created="2013-09-13T11:59:19Z" id="24389707">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Improved the docs for nested mapping</comment></comments></commit></commits></item><item><title>Facets incorrect when scrolling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1642</link><project id="" key="" /><description>When requesting facets on a scrolled search, the counts just keep rising, incorrectly.

I'd say it makes more sense to only return the facets on the first request, not on subsequent scroll requests

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1' 
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 111 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 112 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 113 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 114 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 115 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 116 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 117 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 118 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 119 }'
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d ' { "num" : 120 }'
```

Then run facets:

```
curl -XGET 'http://127.0.0.1:9200/test/foo/_search?scroll=1m&amp;pretty=1'  -d '
{
   "facets" : {
      "num" : {
         "terms" : {
            "field" : "num"
         }
      }
   },
   "size" : 1
}
'

# [Thu Jan 26 19:58:12 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 113
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "ApvhoFAnTTayEyB4NHdMGw",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 1,
#                "term" : 120
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 118
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 115
#             },
#             {
#                "count" : 1,
#                "term" : 114
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             },
#             {
#                "count" : 1,
#                "term" : 112
#             },
#             {
#                "count" : 1,
#                "term" : 111
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 10
#       }
#    },
#    "took" : 4
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 116
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "bJUOIFylTBqyTS4kSzeIcw",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 2,
#                "term" : 120
#             },
#             {
#                "count" : 2,
#                "term" : 118
#             },
#             {
#                "count" : 2,
#                "term" : 115
#             },
#             {
#                "count" : 2,
#                "term" : 114
#             },
#             {
#                "count" : 2,
#                "term" : 112
#             },
#             {
#                "count" : 2,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 16
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 117
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "QVlLmSz6QUm7zi3WthUntQ",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 3,
#                "term" : 120
#             },
#             {
#                "count" : 3,
#                "term" : 118
#             },
#             {
#                "count" : 3,
#                "term" : 115
#             },
#             {
#                "count" : 3,
#                "term" : 114
#             },
#             {
#                "count" : 3,
#                "term" : 112
#             },
#             {
#                "count" : 3,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 22
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 119
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "OdDkJXgRTPOsvbYA4PjXtg",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 4,
#                "term" : 120
#             },
#             {
#                "count" : 4,
#                "term" : 118
#             },
#             {
#                "count" : 4,
#                "term" : 115
#             },
#             {
#                "count" : 4,
#                "term" : 114
#             },
#             {
#                "count" : 4,
#                "term" : 112
#             },
#             {
#                "count" : 4,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 28
#       }
#    },
#    "took" : 0
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 114
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "f24bgaz7ROC1jTldU6UGiQ",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 5,
#                "term" : 120
#             },
#             {
#                "count" : 5,
#                "term" : 118
#             },
#             {
#                "count" : 5,
#                "term" : 115
#             },
#             {
#                "count" : 5,
#                "term" : 114
#             },
#             {
#                "count" : 5,
#                "term" : 112
#             },
#             {
#                "count" : 5,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 34
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 111
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "B0u6QEKZSKCwaV5-uqkoFQ",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 6,
#                "term" : 120
#             },
#             {
#                "count" : 6,
#                "term" : 118
#             },
#             {
#                "count" : 6,
#                "term" : 115
#             },
#             {
#                "count" : 6,
#                "term" : 114
#             },
#             {
#                "count" : 6,
#                "term" : 112
#             },
#             {
#                "count" : 6,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 40
#       }
#    },
#    "took" : 0
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 112
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "3Gho6ITESYmIDdqCIpWWPg",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 7,
#                "term" : 120
#             },
#             {
#                "count" : 7,
#                "term" : 118
#             },
#             {
#                "count" : 7,
#                "term" : 115
#             },
#             {
#                "count" : 7,
#                "term" : 114
#             },
#             {
#                "count" : 7,
#                "term" : 112
#             },
#             {
#                "count" : 7,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 46
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 118
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "gwtal_yVRhqx8ACAOU2Dog",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 8,
#                "term" : 120
#             },
#             {
#                "count" : 8,
#                "term" : 118
#             },
#             {
#                "count" : 8,
#                "term" : 115
#             },
#             {
#                "count" : 8,
#                "term" : 114
#             },
#             {
#                "count" : 8,
#                "term" : 112
#             },
#             {
#                "count" : 8,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 52
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 120
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "_tcnUkiFR2mxw6Puzll14A",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 9,
#                "term" : 120
#             },
#             {
#                "count" : 9,
#                "term" : 118
#             },
#             {
#                "count" : 9,
#                "term" : 115
#             },
#             {
#                "count" : 9,
#                "term" : 114
#             },
#             {
#                "count" : 9,
#                "term" : 112
#             },
#             {
#                "count" : 9,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 58
#       }
#    },
#    "took" : 0
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 115
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "G0Te7qRFRWSq9XzyLNb74g",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 10,
#                "term" : 120
#             },
#             {
#                "count" : 10,
#                "term" : 118
#             },
#             {
#                "count" : 10,
#                "term" : 115
#             },
#             {
#                "count" : 10,
#                "term" : 114
#             },
#             {
#                "count" : 10,
#                "term" : 112
#             },
#             {
#                "count" : 10,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 64
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7' 

# [Thu Jan 26 19:58:21 2012] Response:
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : 1,
#       "total" : 10
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz
# &gt;    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl
# &gt;    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT
# &gt;    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 11,
#                "term" : 120
#             },
#             {
#                "count" : 11,
#                "term" : 118
#             },
#             {
#                "count" : 11,
#                "term" : 115
#             },
#             {
#                "count" : 11,
#                "term" : 114
#             },
#             {
#                "count" : 11,
#                "term" : 112
#             },
#             {
#                "count" : 11,
#                "term" : 111
#             },
#             {
#                "count" : 1,
#                "term" : 119
#             },
#             {
#                "count" : 1,
#                "term" : 117
#             },
#             {
#                "count" : 1,
#                "term" : 116
#             },
#             {
#                "count" : 1,
#                "term" : 113
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 70
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 20:02:12 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1' 

# [Thu Jan 26 20:02:12 2012] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d '
{
   "num" : 1
}
'

# [Thu Jan 26 20:02:15 2012] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "4kjF8ezXQUSTdm0wqHCDmg",
#    "_type" : "foo",
#    "_version" : 1
# }

# [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d '
{
   "num" : 2
}
'

# [Thu Jan 26 20:02:15 2012] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "0AmRRD58QnWE-hyeRF11Kw",
#    "_type" : "foo",
#    "_version" : 1
# }

# [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d '
{
   "num" : 3
}
'

# [Thu Jan 26 20:02:15 2012] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "whjOB26XQFq5eiYiThNk-Q",
#    "_type" : "foo",
#    "_version" : 1
# }

# [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d '
{
   "num" : 4
}
'

# [Thu Jan 26 20:02:15 2012] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "Ax7l0JEWTgWZUYsUcl4oog",
#    "_type" : "foo",
#    "_version" : 1
# }

# [Thu Jan 26 20:02:18 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/test/foo/_search?scroll=1m&amp;pretty=1'  -d '
{
   "facets" : {
      "num" : {
         "terms" : {
            "field" : "num"
         }
      }
   },
   "size" : 1
}
'

# [Thu Jan 26 20:02:18 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 3
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "whjOB26XQFq5eiYiThNk-Q",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 4
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV
# &gt;    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU
# &gt;    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz
# &gt;    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 1,
#                "term" : 4
#             },
#             {
#                "count" : 1,
#                "term" : 3
#             },
#             {
#                "count" : 1,
#                "term" : 2
#             },
#             {
#                "count" : 1,
#                "term" : 1
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 4
#       }
#    },
#    "took" : 2
# }

# [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D' 

# [Thu Jan 26 20:02:20 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 2
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "0AmRRD58QnWE-hyeRF11Kw",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 4
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV
# &gt;    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU
# &gt;    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz
# &gt;    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 2,
#                "term" : 4
#             },
#             {
#                "count" : 2,
#                "term" : 2
#             },
#             {
#                "count" : 2,
#                "term" : 1
#             },
#             {
#                "count" : 1,
#                "term" : 3
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 7
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D' 

# [Thu Jan 26 20:02:20 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 4
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "Ax7l0JEWTgWZUYsUcl4oog",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 4
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV
# &gt;    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU
# &gt;    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz
# &gt;    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 3,
#                "term" : 4
#             },
#             {
#                "count" : 3,
#                "term" : 2
#             },
#             {
#                "count" : 3,
#                "term" : 1
#             },
#             {
#                "count" : 1,
#                "term" : 3
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 10
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D' 

# [Thu Jan 26 20:02:20 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "num" : 1
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "4kjF8ezXQUSTdm0wqHCDmg",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 4
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV
# &gt;    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU
# &gt;    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz
# &gt;    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 4,
#                "term" : 4
#             },
#             {
#                "count" : 4,
#                "term" : 2
#             },
#             {
#                "count" : 4,
#                "term" : 1
#             },
#             {
#                "count" : 1,
#                "term" : 3
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 13
#       }
#    },
#    "took" : 1
# }

# [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/_search/scroll?scroll=1m&amp;pretty=1&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D' 

# [Thu Jan 26 20:02:20 2012] Response:
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : 1,
#       "total" : 4
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV
# &gt;    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU
# &gt;    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz
# &gt;    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",
#    "facets" : {
#       "num" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 5,
#                "term" : 4
#             },
#             {
#                "count" : 5,
#                "term" : 2
#             },
#             {
#                "count" : 5,
#                "term" : 1
#             },
#             {
#                "count" : 1,
#                "term" : 3
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 16
#       }
#    },
#    "took" : 1
# }
```
</description><key id="2984292">1642</key><summary>Facets incorrect when scrolling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-01-26T19:09:06Z</created><updated>2014-09-03T07:17:45Z</updated><resolved>2014-09-03T07:17:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-01T08:42:01Z" id="3755497">Mmm, interesting.... I did not cover the case of using faceting with scrolling since I did not think it makes sense, but, it should at least be consistent... .
</comment><comment author="clintongormley" created="2013-04-10T07:29:52Z" id="16159426">We should probably only return facets on the first request, and not on scrolls
</comment><comment author="s1monw" created="2013-04-10T07:42:00Z" id="16159811">+1 to only return facets on the first request
</comment><comment author="dadoonet" created="2013-04-10T07:46:17Z" id="16159975">I'm wondering if it's a common use case to ask for facets when doing _scrolling_... I would vote to not compute facets at all when doing scroll or to reject the query with a clear error message.
That way we won't have to answer to questions such as "why do I get facets on the first round trip and facets disappear after?"
</comment><comment author="clintongormley" created="2013-04-10T07:48:18Z" id="16160045">@dadoonet i've just had pecke01 in IRC saying that he has customers who are using facets with scrolling, and I don't see any reason to remove them on the first request. but it is meaningless to calculate them on subsequent requests
</comment><comment author="s1monw" created="2013-04-10T07:48:28Z" id="16160048">well even if it's not a common usecase it seems pretty straight forward form an API perspective to just return the facets on the first response no?
</comment><comment author="seiflotfy" created="2014-05-12T16:40:47Z" id="42856157">What is the status of this issue?
</comment><comment author="clintongormley" created="2014-05-12T18:39:01Z" id="42870676">I don't think anything further has happened, but I think a reasonable resolution would be to disable facets/aggs on subsequent scroll requests.  

/cc @uboness 
</comment><comment author="clintongormley" created="2014-07-03T19:21:31Z" id="47973382">In fact, in master, the call to _scroll with facets now throws an NPE:

```
Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [test][2]: query[ConstantScore(cache(_type:foo))],from[1],size[1]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:162)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:282)
    at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:274)
    at org.elasticsearch.search.action.SearchServiceTransportAction$9.call(SearchServiceTransportAction.java:271)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.search.facet.terms.longs.TermsLongFacetExecutor$StaticAggregatorValueProc.onValue(TermsLongFacetExecutor.java:220)
    at org.elasticsearch.search.facet.LongFacetAggregatorBase.onDoc(LongFacetAggregatorBase.java:35)
    at org.elasticsearch.search.facet.terms.longs.TermsLongFacetExecutor$Collector.collect(TermsLongFacetExecutor.java:157)
    at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:175)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.searchAfter(IndexSearcher.java:243)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:135)
    ... 7 more
```

Aggregations return the correct counts on each call to scroll, but it still makes more sense to me to only return them on the first request. 
</comment><comment author="jpountz" created="2014-07-04T10:11:44Z" id="48027987">&gt; Aggregations return the correct counts on each call to scroll, but it still makes more sense to me to only return them on the first request.

Not sure if the bug still exists in recent releases (#5799 might have fixed it), but I think this behavior has probably been the cause of https://github.com/elasticsearch/elasticsearch/issues/3801.
</comment><comment author="jpountz" created="2014-09-03T07:17:31Z" id="54260391">Closing this issue as facets are deprecated and the issue has been fixed for aggregations via #7497.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java</file><file>src/test/java/org/elasticsearch/search/aggregations/AggregationsIntegrationTests.java</file></files><comments><comment>Aggregations: Only return aggregations on the first page when scrolling.</comment></comments></commit></commits></item><item><title>Highlighting broken after 0.18.5 when using CustomScoreQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1641</link><project id="" key="" /><description>Hi

We are using Elasticsearch with pyes (0.16.0) and CustomScoreQuery. When upgrading from ES 0.18.5 to 0.18.6 or higher the highlighting beaks. 

Example _broken_ query:

{"custom_score": {"query": {"bool": {"must": [{"query_string": {"fields": ["title", "text", "org_comments_text", "other_comments_text", "tags"], "query": "filmer"}}]}}, "script": "_score \* (1 + (0.5 \* min((max(7 - ((time() - doc['last_activity'].value)/(1000_60_60_24)), 0)), 1)_0.1) + (0.5 \* min((max(30 - ((time() - doc['last_activity'].value)/(1000_60_60_24)), 0)), 1)_0.1) + (1.0 \* min((max(90 - ((time() - doc['last_activity'].value)/(1000_60_60_24)), 0)), 1)_0.1) + (2 \* min((max(180 - ((time() - doc['last_activity'].value)/(1000_60_60_24)), 0)), 1)_0.1))"}}

Example of _working_ query: 
{"bool": {"must": [{"query_string": {"fields": ["title", "text", "org_comments_text", "other_comments_text", "tags"], "query": "kundo"}}]}} 

When reading the changelog it appears as this fix could be the cause: 
https://github.com/elasticsearch/elasticsearch/commit/be282cc4c8b44956be4ec98be73061ca3e25b73a

Issue #1314 from september 2011 raises the same issue, has it perhaps reappeared by the above fix? 
</description><key id="2978753">1641</key><summary>Highlighting broken after 0.18.5 when using CustomScoreQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bjornlilja</reporter><labels /><created>2012-01-26T12:25:44Z</created><updated>2012-05-04T14:33:02Z</updated><resolved>2012-05-04T14:33:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-26T18:37:55Z" id="3674011">What do you mean by broken? It does not highlight? Is there a chance for a recreation (curl would be best)?
</comment><comment author="bjornlilja" created="2012-01-26T23:04:51Z" id="3678723">It does not highlight. The best I can give you is the above query, we have tried it consistently on two different computers. It fails with 0.18.6 and works with 0.18.5. When adding a custom_score highlight breaks.
</comment><comment author="kimchy" created="2012-01-29T16:12:57Z" id="3709179">You can't try and recreate it? Its going to be much more harder to try and solve it if you can't.
</comment><comment author="EmilStenstrom" created="2012-04-26T08:30:28Z" id="5351827">Hi, sorry for the delay in getting back to you with a reproducible test case. We are quite new to elastic, so any hints on how to improve our setup is appreciated.

We're using elasticsearch 0.19.2, and the queries are run through pyes 0.16.0.

Here's the query we would like to run, that DOESN'T highlight the title of the dialog inputed:

```
REQ PUT /forum_999998
JSON {"index": {"analysis": {"analyzer": {"default": {"stopwords": "p\u00e5,s\u00e5,sa,vi,ni,de,\u00e4r,om,av,du,d\u00e5,nu,ej,ut,\u00e4n,ha,ju,\u00e5t,er,alla,allt,allts\u00e5,andra,att,bara,bli,blir,borde,bra,mitt,ser,dem,den,denna,det,detta,dig,din,dock,dom,d\u00e4r,edit,efter,eftersom,eller,ett,fast,fel,fick,finns,fram,fr\u00e5n,f\u00e5r,f\u00e5tt,f\u00f6r,f\u00f6rsta,genom,ger,g\u00e5r,g\u00f6r,g\u00f6ra,hade,han,har,hela,helt,honom,hos,hur,h\u00e4r,iaf,igen,ingen,inget,inte,ist\u00e4llet,jag,kan,kanske,kommer,lika,lite,man,med,men,mer,mig,min,mot,mycket,m\u00e5nga,m\u00e5ste,nog,n\u00e4r,n\u00e5gon,n\u00e5got,n\u00e5gra,n\u00e5n,n\u00e5t,och,ocks\u00e5,r\u00e4tt,samma,sedan,sen,sig,sin,sj\u00e4lv,ska,skulle,som,s\u00e4tt,tar,till,tror,tycker,typ,upp,utan,vad,var,vara,vet,vid,vilket,vill,v\u00e4l,\u00e4ven,\u00f6ver", "type": "snowball", "language": "Swedish"}}}}}
RESP {u'acknowledged': True, u'ok': True}

REQ PUT /forum_999998/dialog/_mapping
JSON {"dialog": {"properties": {"other_comments_text": {"type": "string", "boost": 0.2}, "tags": {"type": "string", "boost": 0.2}, "text": {"type": "string", "boost": 1.0}, "num_votes": {"index": "not_analyzed", "type": "integer"}, "topic": {"index": "not_analyzed", "type": "string"}, "last_activity": {"index": "not_analyzed", "type": "date"}, "user": {"index": "not_analyzed", "type": "multi_field", "fields": {"first_name": {"index": "not_analyzed", "type": "string"}, "id": {"index": "not_analyzed", "type": "integer"}}}, "pub_date": {"index": "not_analyzed", "type": "date"}, "id": {"index": "not_analyzed", "type": "integer"}, "org_comments_text": {"type": "string", "boost": 0.5}, "archived": {"index": "not_analyzed", "type": "boolean"}, "title": {"type": "string", "boost": 2.5}, "num_comments": {"index": "not_analyzed", "type": "integer"}, "absolute_url": {"index": "not_analyzed", "type": "string"}}}}
RESP {u'acknowledged': True, u'ok': True}

REQ PUT /forum_999998/dialog/2
JSON {"other_comments_text": "", "tags": "", "text": "\tEn exempeldialog. ", "num_votes": 0, "topic": "Q", "last_activity": "2012-04-26T10:19:59", "user": {"first_name": "", "id": 1}, "pub_date": "2009-05-02T14:18:21", "id": 2, "org_comments_text": "", "archived": false, "title": "Exempel", "num_comments": 0, "absolute_url": "/org/kundo/d/exempel/"}
RESP {u'_type': u'dialog', u'_id': u'2', u'ok': True, u'_version': 1, u'_index': u'forum_999998'}

REQ GET /forum_999998/dialog/_search
JSON {"highlight": {"pre_tags": ["&lt;span class=\"highlighted\"&gt;"], "fields": {"other_comments_text": {"fragment_size": 150, "number_of_fragments": 1}, "text": {"fragment_size": 150, "number_of_fragments": 1}, "org_comments_text": {"fragment_size": 150, "number_of_fragments": 1}, "title": {"fragment_size": 150, "number_of_fragments": 1}}, "post_tags": ["&lt;/span&gt;"]}, "query": {"custom_score": {"query": {"bool": {"must_not": [{"term": {"archived": "True"}}], "must": [{"query_string": {"fields": ["title", "text", "org_comments_text", "other_comments_text"], "query": "exempel"}}]}}, "script": "_score * (1 - 0.1 * min(7.0, (max((((time() - doc['last_activity'].value)/(1000*60*60*24) + (time() - doc['pub_date'].value)/(1000*60*60*24)) / 2), 180.0) - 180.0)/180.0))"}}, "from": 0, "size": 40}
RESP {u'hits': {u'hits': [{u'_score': 0.1877942, u'_type': u'dialog', u'_id': u'2', u'_source': {u'other_comments_text': '', u'num_comments': 0, u'archived': False, u'tags': '', u'text': u'\tEn exempeldialog. ', u'title': u'Exempel', u'topic': u'Q', u'last_activity': datetime.datetime(2012, 4, 26, 10, 19, 59), u'user': {u'first_name': '', u'id': 1}, u'num_votes': 0, u'pub_date': datetime.datetime(2009, 5, 2, 14, 18, 21), u'id': 2, u'org_comments_text': '', u'absolute_url': u'/org/kundo/d/exempel/'}, u'_index': u'forum_999998'}], u'total': 1, u'max_score': 0.1877942}, u'_shards': {u'successful': 1, u'failed': 0, u'total': 1}, u'took': 5, u'timed_out': False}
```

And here's the working query, with the "custom_score" removed:

```
REQ GET /forum_999998/dialog/_search
JSON {"highlight": {"pre_tags": ["&lt;span class=\"highlighted\"&gt;"], "fields": {"other_comments_text": {"fragment_size": 150, "number_of_fragments": 1}, "text": {"fragment_size": 150, "number_of_fragments": 1}, "org_comments_text": {"fragment_size": 150, "number_of_fragments": 1}, "title": {"fragment_size": 150, "number_of_fragments": 1}}, "post_tags": ["&lt;/span&gt;"]}, "query": {"bool": {"must_not": [{"term": {"archived": "True"}}], "must": [{"query_string": {"fields": ["title", "text", "org_comments_text", "other_comments_text"], "query": "exempel"}}]}}, "from": 0, "size": 40}
RESP {u'hits': {u'hits': [{u'_type': u'dialog', u'_source': {u'other_comments_text': '', u'num_comments': 0, u'archived': False, u'tags': '', u'text': u'\tEn exempeldialog. ', u'title': u'Exempel', u'topic': u'Q', u'last_activity': datetime.datetime(2012, 4, 26, 10, 23, 1), u'user': {u'first_name': '', u'id': 1}, u'num_votes': 0, u'pub_date': datetime.datetime(2009, 5, 2, 14, 18, 21), u'id': 2, u'org_comments_text': '', u'absolute_url': u'/org/kundo/d/exempel/'}, u'_score': 0.23539662, u'_index': u'forum_999998', u'highlight': {u'title': [u'&lt;span class="highlighted"&gt;Exempel&lt;/span&gt;']}, u'_id': u'2'}], u'total': 1, u'max_score': 0.23539662}, u'_shards': {u'successful': 1, u'failed': 0, u'total': 1}, u'took': 7, u'timed_out': False}
```
</comment><comment author="kimchy" created="2012-05-04T09:31:40Z" id="5507066">Can you check maybe #1894 fixed it?
</comment><comment author="EmilStenstrom" created="2012-05-04T14:31:53Z" id="5511654">@kimchy Yes, #1894 fixed it. You can close this issue. Thanks!
</comment><comment author="kimchy" created="2012-05-04T14:33:02Z" id="5511687">I'll close this one then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better failure when passing invalid options to a query/filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1640</link><project id="" key="" /><description /><key id="2978483">1640</key><summary>Better failure when passing invalid options to a query/filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-26T11:53:16Z</created><updated>2012-01-26T11:53:50Z</updated><resolved>2012-01-26T11:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CustomBoostFactorQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/CustomScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/LimitFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NumericRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TextQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file></files><comments><comment>Better failure when passing invalid options to a query/filter, closes #1640.</comment></comments></commit></commits></item><item><title>Mapping: _source mapping to allow for format to convert to (if needed)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1639</link><project id="" key="" /><description>Allow to specify `format` in the _source mapping to convert to that format when storing. This can allow to store the source is `smile` format even if its sent in `json`. There will be an overhead when sending back hits though and expecting to get it in json as it will need to convert from `smile` to `json` (it will do it automatically). 
</description><key id="2972411">1639</key><summary>Mapping: _source mapping to allow for format to convert to (if needed)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-25T22:18:04Z</created><updated>2012-01-25T22:18:56Z</updated><resolved>2012-01-25T22:18:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentType.java</file><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/test/java/org/elasticsearch/test/unit/index/mapper/source/DefaultSourceMappingTests.java</file></files><comments><comment>Mapping: _source mapping to allow for format to convert to (if needed), closes #1639.</comment></comments></commit></commits></item><item><title>Illegal latitude value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1638</link><project id="" key="" /><description>I might be missing something, but elasitcsearch is complaining about lats/lons that seem to be valid.  Here is the source I'm using: http://api.simplegeo.com/1.0/features/SG_7JerIlcgwkzWJ4AWU5MxFa_61.123799_-149.875612@1293731153.json from SimpleGeo

And below is the error. What should I do? Thanks! Brian

[2012-01-25 04:00:35,763][DEBUG][action.search.type       ] [Golem] [mynyte_places][3], node[u9g1OBEiRdqoY3MrofUCzA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@33b06b78]
org.elasticsearch.search.query.QueryPhaseExecutionException: [mynyte_places][3]: query[_all:food],from[0],size[50],sort[&lt;custom:"loc": org.elasticsearch.index.search.geo.GeoDistanceDataComparator$InnerSource@1eee72c8&gt;]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:221)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:234)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:204)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:191)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:177)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalArgumentException: Illegal latitude value -149.857789
    at org.elasticsearch.index.search.geo.LatLng.&lt;init&gt;(LatLng.java:35)
    at org.elasticsearch.index.search.geo.GeoDistance$2.calculate(GeoDistance.java:51)
    at org.elasticsearch.index.search.geo.GeoDistanceFilter$GeoDistanceDocSet.get(GeoDistanceFilter.java:154)
    at org.elasticsearch.common.lucene.search.FilteredCollector.collect(FilteredCollector.java:52)
    at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:55)
    at org.apache.lucene.search.Scorer.score(Scorer.java:90)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:526)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:198)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:153)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:433)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:356)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    ... 9 more
</description><key id="2960860">1638</key><summary>Illegal latitude value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">briansea</reporter><labels /><created>2012-01-25T04:04:03Z</created><updated>2012-01-25T23:46:42Z</updated><resolved>2012-01-25T23:46:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-01-25T10:44:11Z" id="3648652">Latitude ranges from -90 (south pole) to 90 (north pole), which is why it is incorrect. However, I thought lat/lon values would be automatically normalized (see #1264). Or are you using a version of ES pre 0.18?
</comment><comment author="briansea" created="2012-01-25T23:46:42Z" id="3661697">Ah, yes I am. Thank you for the prompt reply :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow for top level "settings" element in Settings API JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1637</link><project id="" key="" /><description>Just for the sake of consistency between the 'Index' and 'Indices Settings' APIs, it would be nice to allow for top level 'settings' object in the Settings API's JSON

Example:
curl -XPUT localhost:9200/myindex/_settings -d '{   "settings" : {   "analysis": {  "analyzer": {... }  }  } }'
</description><key id="2954366">1637</key><summary>Allow for top level "settings" element in Settings API JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">focampo</reporter><labels /><created>2012-01-24T18:11:12Z</created><updated>2014-07-04T12:13:58Z</updated><resolved>2014-07-04T12:13:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T12:13:58Z" id="48036785">The settings, mappings, warmers etc APIs have all been made consistent, and don't take a top level "settings" or other key.

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolator for alerts between specific time intervals?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1636</link><project id="" key="" /><description>Hi,

I have been researching the Percolator feature of the ES, from which I understood that, this is useful for dynamic alerts or Streams(for every indexing of document or bulk indexing).

We have a requirement to create alerts between specific time intervals. For Eg: Once in 10mins, Once in an Hr, Once in a day. I am really interested to know if the Percolator could be used for this purpose.

We have a 24/7 live Indexing scheduler, indexing documents in multiple indices. Almost 18 million documents are created per day.

Please let me know, if I want to give any more information.

Thanks in Advance

Manoj
</description><key id="2948987">1636</key><summary>Percolator for alerts between specific time intervals?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Manokrrish</reporter><labels /><created>2012-01-24T12:04:56Z</created><updated>2012-01-24T15:09:27Z</updated><resolved>2012-01-24T15:09:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-24T15:09:27Z" id="3633722">So, I told you on my blog repo issues to ask the question on the mailing list, and you open an issue on elasticsearch? Questions on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Headless mode in the right place</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1635</link><project id="" key="" /><description>Hi Shay, 

Now It's in launch_service() function. 
</description><key id="2941786">1635</key><summary>Headless mode in the right place</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shairontoledo</reporter><labels /><created>2012-01-23T21:13:25Z</created><updated>2014-07-16T21:55:42Z</updated><resolved>2012-02-03T16:23:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-24T16:59:40Z" id="3635895">I think the best place for it is in elasticsearch.in.sh (and in elasticsearch.bat). I am not 100% sure what the implication for this are. Also, can you munge it into a single commit?
</comment><comment author="shairontoledo" created="2012-02-03T16:23:26Z" id="3799411">Given I'm not able to test in Windows env, I'm going to close this pull request. We could just to add JAVA_OPTS=-Djava.awt.headless=true in plugin's the documentation.

Thank you
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Headless mode for tika/mapper-attachements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1634</link><project id="" key="" /><description>Hi Shay,

I was testing mapper-attachements here I came across an java app starting up in indexing and searching mode. I added -Djava.awt.headless=true to bin/elasticsearch to avoid that. I dont know if there is the best place to do it.
</description><key id="2939379">1634</key><summary>Headless mode for tika/mapper-attachements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shairontoledo</reporter><labels /><created>2012-01-23T18:17:09Z</created><updated>2014-07-16T21:55:43Z</updated><resolved>2012-01-23T21:11:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-23T20:09:08Z" id="3620822">You only added it to the version output, not when elasticsearch actually runs..., does it work?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>REST: `/` to properly return error code (HEAD and GET) when blocked (not recovered, no master), as well as cluster health (red == 503)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1633</link><project id="" key="" /><description /><key id="2929068">1633</key><summary>REST: `/` to properly return error code (HEAD and GET) when blocked (not recovered, no master), as well as cluster health (red == 503)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-22T21:56:44Z</created><updated>2012-01-22T21:57:09Z</updated><resolved>2012-01-22T21:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlocks.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/rest/action/main/RestMainAction.java</file></files><comments><comment>REST: `/` to properly return error code (HEAD and GET) when blocked (not recovered, no master), as well as cluster health (red == 503), closes #1633.</comment><comment>Sad to see the quotes are gone... but I think I will get over it.</comment></comments></commit></commits></item><item><title>Published cluster state might override newer state (rarely)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1632</link><project id="" key="" /><description>Fixed as part of #1631.
</description><key id="2928938">1632</key><summary>Published cluster state might override newer state (rarely)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-22T21:35:13Z</created><updated>2012-01-22T21:35:23Z</updated><resolved>2012-01-22T21:35:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-22T21:35:23Z" id="3606443">Fixed as part of #1631.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Local Gateway: Store specific index metadata under dedicated index locations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1631</link><project id="" key="" /><description>Currently, all the indices metadata is stored in a single file called _state/metadata. Separate it to store each index metadata under its own index location. We still need to store global metadata state (index templates and persistent settings), store it under global file.

When starting the node it will automatically upgrade from the previous format to the new one, and backup the current one.
</description><key id="2928930">1631</key><summary>Local Gateway: Store specific index metadata under dedicated index locations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2012-01-22T21:34:00Z</created><updated>2012-01-22T21:34:45Z</updated><resolved>2012-01-22T21:34:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/collect/MapBuilder.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/Gateway.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayModule.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/TransportNodesListGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file><file>src/main/java/org/elasticsearch/gateway/shared/SharedStorageGateway.java</file><file>src/test/java/org/elasticsearch/test/integration/AbstractNodesTests.java</file><file>src/test/java/org/elasticsearch/test/integration/cluster/MinimumMasterNodesTests.java</file></files><comments><comment>Local Gateway: Store specific index metadata under dedicated index locations, closes #1631.</comment></comments></commit></commits></item><item><title>error messages are not helpful at all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1630</link><project id="" key="" /><description>The error message caused by the query at the top does not give any sane hint as to what is actually wrong:

https://gist.github.com/0dc816b64318e5898b62

Error messages should be a bit more helpful and actually try to tell the user what was expected.
</description><key id="2920900">1630</key><summary>error messages are not helpful at all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wchristian</reporter><labels /><created>2012-01-21T11:06:24Z</created><updated>2012-01-21T22:41:33Z</updated><resolved>2012-01-21T15:49:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-21T15:49:01Z" id="3596873">@wchristian can you please not open 3 issues for the same problem? #1629 and #1627 are exactly the same. I am closing this one.
</comment><comment author="wchristian" created="2012-01-21T16:00:32Z" id="3596920">These were opened after talking with @clintongormley in private and your claim that the two you linked are exactly the same is completely wrong. They both use the same code and error message, but each of them highlights a specific issue, one of which was a misunderstanding and invalid ( #1627 ), while the other highlights the actual issue that happened and was valid ( #1629 ). The two issues presented in these two fall into entirely different classes of importance, one of them being fixable by workaround from the outside, the other being a core issue in your infrastructure. True, i might've edited #1627 and it could've been reopened, but that would've made it more difficult for people coming later to get to and understand the meat of the issue.

Furthermore: Why did you decide to close this one? It is as well a unique issue that is not a duplicate of any of the other two. No matter whether you fix the accidental masking of errors or not, the errors generated are entirely unhelpful for the a user who simply sends a query and gets that kind of error back.

Are you saying you are happy with the situation of your error messages being unhelpful? Or that they are unfixable?
</comment><comment author="kimchy" created="2012-01-21T19:22:54Z" id="3598364">All of those issues refer to the same problem, you send an illegal search structured request, and elasticsearch does not properly tell you that its a wrong structure.
</comment><comment author="wchristian" created="2012-01-21T22:41:33Z" id="3599555">Oh. Alright, that actually makes sense then. Thanks for explaining! :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>differences in key ordering in JSON cause errors to be masked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1629</link><project id="" key="" /><description>According to @clintongormley this kind of error should happen for both of the queries, but changing the key ordering masks the occurence:

See: https://gist.github.com/0dc816b64318e5898b62

It should throw the error for both forms.
</description><key id="2920890">1629</key><summary>differences in key ordering in JSON cause errors to be masked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wchristian</reporter><labels /><created>2012-01-21T11:03:16Z</created><updated>2014-07-08T13:12:41Z</updated><resolved>2014-07-08T13:12:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T13:12:41Z" id="48333594">This is now fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_open and _close does not resolve aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1628</link><project id="" key="" /><description>The _open and _close endpoints don't work on aliases:
- http://localhost:9200/my_idx/_open works fine
- http://localhost:9200/my_alias/_open does not work

This should be trivial to fix as the _status call works fine on both indices and aliases.
</description><key id="2920435">1628</key><summary>_open and _close does not resolve aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-21T08:30:40Z</created><updated>2012-01-23T18:35:07Z</updated><resolved>2012-01-23T00:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ahfeel" created="2012-01-23T09:30:08Z" id="3611108">woot ! Thanks Shay !
</comment><comment author="ahfeel" created="2012-01-23T17:55:20Z" id="3618413">Is there a reason not to merge this for the next 0.18 release ?
</comment><comment author="kimchy" created="2012-01-23T18:35:07Z" id="3619105">Yea, 0.19 is just around the corner, there isn't a planned 0.18 release currently.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file></files><comments><comment>_open and _close does not resolve aliases, closes #1628.</comment></comments></commit></commits></item><item><title>JSON keys in non-sorted order can cause ES to respond with obscure error messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1627</link><project id="" key="" /><description>See: https://gist.github.com/0dc816b64318e5898b62

MetaCPAN uses ES as backend and when i sent the first query to it, i got the error message at the top back, which seems to be generated by ES itself. However when the keys of the JSON are sorted, then the query goes through fine.
</description><key id="2915840">1627</key><summary>JSON keys in non-sorted order can cause ES to respond with obscure error messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wchristian</reporter><labels /><created>2012-01-20T19:54:48Z</created><updated>2012-01-21T10:53:02Z</updated><resolved>2012-01-21T10:47:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-01-21T10:47:02Z" id="3595584">This is an error in your query, not in the JSON parsing.  You can just drop the match_all clause.  Also, your `size` parameter should be at the top level, not inside your query:

```
{
   "fields" : [ "release.distribution", "release.date" ],
   "query" : {
      "range" : {
         "release.date" : {
            "to" :  "2011-12-22T23:00:00",
            "from" : "2011-12-22T00:00:00"
         }
      }
   },
   "size" : 10
}
```
</comment><comment author="wchristian" created="2012-01-21T10:53:02Z" id="3595599">That still doesn't explain why reordering the query makes it work?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removing a node with TRACE logging enabled causes cluster state not to be properly updated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1626</link><project id="" key="" /><description /><key id="2896450">1626</key><summary>Removing a node with TRACE logging enabled causes cluster state not to be properly updated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-19T13:16:35Z</created><updated>2012-01-19T13:42:32Z</updated><resolved>2012-01-19T13:42:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file></files><comments><comment>Removing a node with TRACE logging enabled causes cluster state not to be properly updated, closes #1626.</comment></comments></commit></commits></item><item><title>updatable default TTL in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1625</link><project id="" key="" /><description /><key id="2890057">1625</key><summary>updatable default TTL in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-01-18T22:53:59Z</created><updated>2014-07-16T21:55:43Z</updated><resolved>2012-01-19T14:17:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Thread Pool: Add a dedicated thread pool for refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1624</link><project id="" key="" /><description>Have a dedicated thread pool for refresh (it currently uses the built in cached one). This will allow to get better stats specific to the refresh operation (using the new thread pool node level stats), and, configure it to be a different type of thread pool (defaults to cached).
</description><key id="2889642">1624</key><summary>Thread Pool: Add a dedicated thread pool for refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-18T22:22:02Z</created><updated>2012-12-20T09:21:48Z</updated><resolved>2012-01-18T22:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sweetbaybe" created="2012-12-20T07:27:08Z" id="11562745">How can I view the state of Elasticsearch ThreadPool
</comment><comment author="martijnvg" created="2012-12-20T09:21:48Z" id="11565416">@sweetbaybe 
The current state can be viewed via the node stats api: http://localhost:9200/_nodes/stats?thread_pool
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file></files><comments><comment>Thread Pool: Add a dedicated thread pool for refresh, closes #1624.</comment></comments></commit></commits></item><item><title>index.recovery.initial_shards is not being taken into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1623</link><project id="" key="" /><description>The initial_shards recovery for local gateway that can be set per index is not being taken into account. Note,  `gateway.local.initial_shards` works (but is set on the node configuration).
</description><key id="2888422">1623</key><summary>index.recovery.initial_shards is not being taken into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.8</label><label>v0.19.0.RC1</label></labels><created>2012-01-18T20:48:48Z</created><updated>2012-01-18T21:02:22Z</updated><resolved>2012-01-18T21:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java</file></files><comments><comment>index.recovery.initial_shards is not being taken into account, closes #1623.</comment></comments></commit></commits></item><item><title>Node Stats: Add fs level stats (size + iostats)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1622</link><project id="" key="" /><description>Add `fs` flag and `fs` endpoint (like `/_nodes/stats/fs` to get back file system stats including size and iostats
</description><key id="2887020">1622</key><summary>Node Stats: Add fs level stats (size + iostats)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-18T18:59:45Z</created><updated>2012-01-18T19:00:17Z</updated><resolved>2012-01-18T19:00:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/common/io/stream/AdapterStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/AdapterStreamOutput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/main/java/org/elasticsearch/monitor/MonitorModule.java</file><file>src/main/java/org/elasticsearch/monitor/MonitorService.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsService.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsStats.java</file><file>src/main/java/org/elasticsearch/monitor/fs/JmxFsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/SigarFsProbe.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file></files><comments><comment>Node Stats: Add fs level stats (size + iostats), closes #1622.</comment><comment>Should it be public?</comment></comments></commit></commits></item><item><title>Simplify filter on date facet results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1621</link><project id="" key="" /><description>A date_histogram facet, being provided with an interval (let's say month) returns entries using millisecond timestamps like this:

"entries"=&gt;[
  {"time"=&gt;1325376000000, "count"=&gt;4},
  {"time"=&gt;1325378000000, "count"=&gt;2}
]

The problem with the current filter API is that if you want to allow the user to filter on a given month, things can be pretty hard while all the logic is present in elasticsearch.

Right now, the only API to filter on the given month is a range filter as:

```
    "filter" : {
        "range" : {
            "date" : { 
                "from" : "1325376000000", 
                "to" : "XXXXXXXX", 
                "include_lower" : true, 
                "include_upper" : false
            }
        }
    }
```

Computing the "to" can be very tricky when dealing with months and years. While some language do provide helpers for that, I suggest an addition to the range API to be able to give back the interval instead:

```
    "filter" : {
        "range" : {
            "date" : { 
                "from" : "1325376000000", 
                "interval (or duration)" : "month", 
                "include_lower" : true, 
                "include_upper" : false
            }
        }
    }
```
</description><key id="2883023">1621</key><summary>Simplify filter on date facet results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2012-01-18T14:32:18Z</created><updated>2014-07-08T13:11:10Z</updated><resolved>2014-07-08T13:11:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jettro" created="2014-01-19T12:28:14Z" id="32707133">I am running into this same problem. Would be nice to have something like the interval option in the filter as well.
</comment><comment author="uboness" created="2014-01-19T18:51:11Z" id="32716179">you can use date math for that:

``` json
"filter" : {
    "range" : {
        "date" : { 
            "from" : "1325376000000", 
            "to" :  "1325376000000||+1M",
            "include_lower" : true, 
            "include_upper" : false
        }
    }
}
```

see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html#date-math
</comment><comment author="clintongormley" created="2014-07-08T13:11:10Z" id="48333435">Also aggregations allow you to return date strings. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RabbitMQ river does not support Custom Analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1620</link><project id="" key="" /><description>rabbitmq river creates index with following bulk message format,but custom analyzer will not work:

{ "index" : { "_index" :"idx_testtable", "_type" :"test_type", "_id" :"1"},{"analysis": {"analyzer":{"myindexanalyzer": {"type" :"custom","tokenizer": "whitespace","filter": ["lowercase", "customstopfilter", "asciifolding"]},"mysearchanalyzer": {"type" :"custom","tokenizer": "whitespace" }},{"filter":{"customstopfilter":{"type":"stop" ,"stopwords_path" :"F:\ElasticIndexManagementService\resources\stopwords_eng.txt" ,"ignore_case":true}}}}}}
{ "test_type" :{"index_analyzer" : "myindexanalyzer","search_analyzer" : "mysearchanalyzer"}}
{ "test_type" :{"id":7,"Description":" Technologies Inc"}}
{ "create" : { "_index" :"idx_testtable", "_type" :"test_type", "_id" :"1" }}
{ "test_type" :{"id":7,"Description":" Technologies Inc"}}
</description><key id="2880818">1620</key><summary>RabbitMQ river does not support Custom Analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sameek</reporter><labels /><created>2012-01-18T10:27:01Z</created><updated>2012-01-20T07:58:32Z</updated><resolved>2012-01-18T11:00:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-18T11:00:28Z" id="3544024">Mate, you got to a point where you are spamming. Asking the same question on the mailing list several times, and now on the issues as well. Please stop.
</comment><comment author="karussell" created="2012-01-18T13:02:23Z" id="3545262">Please sam, understand what kimchy is saying:
1. try to solve your questions on your own for a bit more time ;)
2. formulate your questions to the point and in this case with more background
3. use gist to show what your problem is and showing your json otherwise unreadable for us
4. be so kind and use the same thread for the same question

Last but not least:
5. Before posting an issue use the mailing list!
</comment><comment author="karussell" created="2012-01-18T13:04:23Z" id="3545281">Before I forget it - it seems to be that google has sometimes problems to post the answers in the same thread:

http://groups.google.com/group/elasticsearch/browse_thread/thread/dc9fe984ba3d5df7
</comment><comment author="kimchy" created="2012-01-18T20:07:05Z" id="3552266">@karussell thats not really what I am saying. This question was asked 3 times in different ways on the mailing list, on stackoverflow, and now on the issues. I answered it on the mailing list, yes still an issue is opened. This is spamming.
</comment><comment author="karussell" created="2012-01-18T20:56:24Z" id="3552961">@kimchy yes. of course its spamming. but I wanted to safe a bit of your time and hoped to add some valueable description
</comment><comment author="sameek" created="2012-01-19T07:54:19Z" id="3559226">Thanks for putting your views.The reason why i have posted such an issue is that i'm not getting clear response from the group.I got some response from your side but at the beginner level it's quite difficult to standard for me as the documentation and the official site of elastic search is not Java friendly. The reason why I asked the question on mailing list by different ways is because I wanted you to explain in better way.

It is not spamming by any means and if you and your team is unable to answer or provide me the documentation please suggest me some better product which is usable. 

Finally I want to take your attention to a problem that your team member have now replied me that "Before you create the river, you need to precreate the index it will index into, and have the relevant mappings set on it (and settings, like analysis settings)." If that would have been replied me much earlier it wont have killed my time and I wont have asked the questions several times.

While formulating the questions I have always used the standard rules you mentioned above so I don't think that the problem is with the readability.I have put my code,mappings,configuration file in Gist and you can see here : https://gist.github.com/1638251 .

Please let me know if I am still formulating my questions below your expectations.
</comment><comment author="karussell" created="2012-01-19T09:49:00Z" id="3560291">&gt; It is not spamming by any means

aeh, well it is. If you would express your thoughts in ONE thread on the group and probably saying what and that you do not understand it would help. Imagine: this service is all for free and you are not the only one. If you want 100% solutions without investing YOUR time, well then hire some elasticsearch expert via money ;) !

&gt; If that would have been replied me much earlier

thats a bit brash... I won't comment this ...
</comment><comment author="sameek" created="2012-01-20T07:58:32Z" id="3580274">Thanks for your support. I understand your  situation and i apologize for above issue..I have put my updated code,mappings,configuration file in Gist and you can see here : https://gist.github.com/1638251  . which worked perfectly for me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>concurrent inserts cause an index to be deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1619</link><project id="" key="" /><description>I have an application that performs concurrent inserts on an elasticsearch index and is using 4 cpus on a cluster with 5 nodes using 0.18.5 - a single node has 16 cores and inserts are being performed on the node with the elasticsearch instance.

At a certain point in time (and its not that predictable) I see remove_mapping in the logs and a new index is being referenced when using count.

If I force/bind the application that performs the inserts to one cpu the problem goes away ; as in the index does not get rebuilt and index counts don't get reset and I don't see remove_mapping in the logs
</description><key id="2878114">1619</key><summary>concurrent inserts cause an index to be deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abdollar</reporter><labels /><created>2012-01-18T02:55:44Z</created><updated>2012-01-20T19:16:08Z</updated><resolved>2012-01-20T19:16:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-18T11:08:11Z" id="3544108">Remove mapping is only being called when you actually do `curl -XDELETE localhost:9200/index/type`, or you execute a delete mapping Java API. There is no other way to do it. Do you delete data in your indexing code? Also, what do you mean a new index being referenced? Can you share your indexing code?
</comment><comment author="abdollar" created="2012-01-18T18:59:44Z" id="3551225">I do not delete data in my indexing code. Its just performing inserts. The application that performs the inserts is an http server written in go.

If I do call delete explicitly I can see it happen in the logs : like so

[2012-01-18 01:40:35,920][INFO ][cluster.metadata         ] [Tag] [users] deleting index

However, I am not seeing this. I am seeing this

[2012-01-18 00:11:32,057][INFO ][cluster.metadata         ] [Tag] [[users]] remove_mapping [user]
[2012-01-18 00:11:32,110][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:11:32,500][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:11:32,779][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:23:23,058][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user]
[2012-01-18 00:24:59,086][INFO ][cluster.metadata         ] [Tag] [[users]] remove_mapping [user]
[2012-01-18 00:24:59,106][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:24:59,442][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:24:59,734][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:40:11,283][INFO ][cluster.metadata         ] [Tag] [[users]] remove_mapping [user]
[2012-01-18 00:40:11,306][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:40:11,683][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:40:12,460][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:54:25,358][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user]
[2012-01-18 00:56:26,375][INFO ][cluster.metadata         ] [Tag] [[users]] remove_mapping [user]
[2012-01-18 00:56:26,389][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:56:26,770][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 00:56:27,054][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 01:01:26,416][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user]
[2012-01-18 01:06:29,784][INFO ][cluster.metadata         ] [Tag] [[users]] remove_mapping [user]
[2012-01-18 01:06:29,808][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 01:06:30,150][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)
[2012-01-18 01:06:30,420][INFO ][cluster.metadata         ] [Tag] [users] update_mapping [user](dynamic)

Each time remove mapping is called it seems the old index is deleted and a new index is created.

If I change the code in the http server from using 4 cpus and change it to use 1 cpu - the problem goes away - as in I dont see remove_mapping getting called and index keeps getting bigger.

In go I do this to force it to use 1 cpu.

runtime.GOMAXPROCS(4) 
to 
runtime.GOMAXPROCS(1)
</comment><comment author="kimchy" created="2012-01-18T19:54:41Z" id="3552108">I am not talking about deleting an index, but deleting a mapping. In your case, for an index called `users`, and a mapping type called `user`, someone called `curl -XDELETE localhost:9200/users/user`. Thats the only code base that can cause this logging message to appear. Its not called as part of the indexing process...
</comment><comment author="abdollar" created="2012-01-18T20:24:08Z" id="3552481">Hmm - Thats strange. From my understanding there are no DELETE requests going to elasticsearch - just PUT requests

I will see if I can reproduce this with a simpler example.
</comment><comment author="kimchy" created="2012-01-18T20:41:31Z" id="3552745">Here is a custom build: http://dl.dropbox.com/u/2136051/elasticsearch-0.18.8-SNAPSHOT.zip that logs when delete mapping is called, can you run it with it and see if you see the log. The logging is INFO level and states: "&lt;------ DELETE MAPPING CALLED!".
</comment><comment author="abdollar" created="2012-01-18T22:45:50Z" id="3554643">OK. I updated all the nodes on the cluster to the custom build - elasticsearch-0.18.8-SNAPSHOT.zip 
Reset the concurrency to 4.
So far I don't see any log messages at all related to mappings.
It'll take around 3 days to ingest everything at the current rate.
I'll keep an eye on the index size and logs and will keep you posted.
</comment><comment author="kimchy" created="2012-01-19T07:27:35Z" id="3559014">I assume that you don't see the `remove_mapping [user]` message as well?
</comment><comment author="abdollar" created="2012-01-19T18:24:28Z" id="3571553">Yep. I don't see any log messages related to mappings. I don't see the remove_mapping message. It's been running happily for quite some time now - as in the index is growing and if I search for id:1 I can still see it.
</comment><comment author="kimchy" created="2012-01-19T18:27:42Z" id="3571616">Maybe its the 0.18.5 version? Thats weird, since there wasn't a problem like that. Well, if it works, you can run with 0.18.7 since the 0.18.8 snap you have has almost no changes on top of it.
</comment><comment author="abdollar" created="2012-01-20T19:16:08Z" id="3588841">Thanks. Currently at 15 million records and climbing. I don't see any issues. Closing 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Local Gateway: Move shard state to be stored under each shard, and not globally under _state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1618</link><project id="" key="" /><description>Currently, the shards state (which shards are active on a node) is stored in a global file in the (data) node. Move it to be stored on each shard location (under its own _state) so its collocated with the shard storage location, and, so we can optimize and only write shards that change their state and not everything each time.

The old shard state will automatically be upgraded to the new state location once its stated, with a backup created in case a revert to the old structure is required.
</description><key id="2876285">1618</key><summary>Local Gateway: Move shard state to be stored under each shard, and not globally under _state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-17T23:08:06Z</created><updated>2012-01-17T23:08:45Z</updated><resolved>2012-01-17T23:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayModule.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayStartedShards.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/ShardStateInfo.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/TransportNodesListGatewayStartedShards.java</file><file>src/test/java/org/elasticsearch/test/integration/gateway/local/SimpleRecoveryLocalGatewayTests.java</file></files><comments><comment>Local Gateway: Move shard state to be stored under each shard, and not globally under _state, closes #1618.</comment></comments></commit></commits></item><item><title>0.18.7 Failure exception while executing a valid query after an invalid query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1617</link><project id="" key="" /><description>Hi,
I created a gist which explains the problem.
https://gist.github.com/1626815

First I inserted a document. Then, I search with an invalid query.  For subsequent valid search request ES returns a failure message as written in gist.
</description><key id="2869189">1617</key><summary>0.18.7 Failure exception while executing a valid query after an invalid query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels><label>bug</label><label>v0.18.8</label><label>v0.19.0.RC1</label></labels><created>2012-01-17T14:36:14Z</created><updated>2012-01-18T13:08:45Z</updated><resolved>2012-01-18T13:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-18T12:32:39Z" id="3544910">Thanks for spotting this!. Its a concurrency issue and caching of shard failures, I will fix t.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchCache.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollScanAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file></files><comments><comment>Failure exception while executing a valid query after an invalid query, closes #1617.</comment></comments></commit></commits></item><item><title>fix bug in TTL handling where default TTL value was not set properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1616</link><project id="" key="" /><description /><key id="2866337">1616</key><summary>fix bug in TTL handling where default TTL value was not set properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-01-17T09:35:43Z</created><updated>2014-07-16T21:55:44Z</updated><resolved>2012-01-17T10:54:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>/_status doc count of index wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1615</link><project id="" key="" /><description>we have an index with about 5*10^9 docs, when calling /_status the indices.&lt;name&gt;.docs.num_docs and max_doc returns a wrong value. looks like an overflow somewhere

when calling /&lt;idx_name&gt;/_stats,  the number of docs is right. in _all.primaries.docs.count

to reproduce add about 2*10^9 docs to an index

this issue also affects the index display in elasticsearch-head
</description><key id="2851964">1615</key><summary>/_status doc count of index wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobe</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-16T08:33:34Z</created><updated>2012-01-16T11:48:41Z</updated><resolved>2012-01-16T11:48:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-16T11:46:31Z" id="3509605">Yea, it is overflowing on the _status API. The aim it to have the stats API to replace the status API (it is better designed), but, this need to be fixed...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/status/DocsStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/ShardStatus.java</file><file>src/test/java/org/elasticsearch/test/integration/nested/SimpleNestedTests.java</file></files><comments><comment>/_status doc count of index wrong, closes #1615.</comment></comments></commit></commits></item><item><title>Highlighting: Add boundary_chars and boundary_max_size to control text boundaries with fast vector highlighter (term vector)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1614</link><project id="" key="" /><description>Add `boundary_chars` (a string consisting of characters to define boundaries) and `boundary_max_size` (to control how far to look for). Note, they apply only for the fast vector highlighter which is used when enabling term vectors.
</description><key id="2847938">1614</key><summary>Highlighting: Add boundary_chars and boundary_max_size to control text boundaries with fast vector highlighter (term vector)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-15T21:04:58Z</created><updated>2012-01-16T21:40:05Z</updated><resolved>2012-01-15T21:05:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brianmario" created="2012-01-16T21:40:05Z" id="3518775">AWESOME!!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/vectorhighlight/SimpleBoundaryScanner2.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceScoreOrderFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceSimpleFragmentsBuilder.java</file><file>src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/test/unit/deps/lucene/VectorHighlighterTests.java</file></files><comments><comment>Highlighting: Add boundary_chars and boundary_max_size to control text boundaries with fast vector highlighter (term vector), closes #1614.</comment></comments></commit></commits></item><item><title>Plugins: Allow for plugins to implement onModule method that will be automatically injected with the relevant module type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1613</link><project id="" key="" /><description>Currently, to add custom injections to different modules, plugins need to implement `processModule` method and then do instanceof checks.

Allow for plugins to implement an `onModule` method that will be injected automatically with the parameter module type. For example: `onModule(AnalyzerModule module) {...}`.
</description><key id="2846109">1613</key><summary>Plugins: Allow for plugins to implement onModule method that will be automatically injected with the relevant module type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2012-01-15T14:42:09Z</created><updated>2012-03-06T02:53:37Z</updated><resolved>2012-01-15T14:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-03-06T02:53:37Z" id="4336986">+++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/AbstractPlugin.java</file><file>src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>Plugins: Allow for plugins to implement onModule method that will be automatically injected with the relevant module type, closes #1613.</comment></comments></commit></commits></item><item><title>Add generic execution of APIs to Client (and indices/cluster) and allow for plugins to register custom APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1612</link><project id="" key="" /><description>This change is driven by allowing for plugins to register custom (strongly typed) Java APIs as well as REST APIs. The `Client`, `IndicesAdminClient` and `ClusterAdminClient` interfaces now allow to `execute` and `prepareExecute` custom actions (as well as the built in ones) in a strongly typed manner.

A new class called `GenericAction` was added, where each specific action needs to extend and implement. Main actions need to extend `Action`, indices actions need to extend `IndicesAction` and cluster actions need to extend `ClusterAction`. Those actions need to be strongly types.

Plugins should then use the `ActionModule` module to register custom actions based on the instance of the action, the transport action class, and any supporting classes.

The generic execute usage then requires providing the action when executing, which automatically derives the request and response types based on generic.

For example, here is how an index operation will be executed using the generic execute method:

```
// direct execution
IndexResponse response = client1.execute(IndexAction.INSTANCE, new IndexRequest(...)...).actionGet();
// prepare sample
client1.prepareExecute(IndexAction.INSTANCE).setIndex("test").setType("type").setSource("").execute().actionGet();
```

As a side note, implementing custom actions has become much simpler, where transport client level proxies are automatically generated and there is no need to create custom ones per actions.
</description><key id="2846014">1612</key><summary>Add generic execution of APIs to Client (and indices/cluster) and allow for plugins to register custom APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2012-01-15T14:14:14Z</created><updated>2012-03-06T02:54:33Z</updated><resolved>2012-01-15T14:15:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-03-06T02:54:33Z" id="4336998">great!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/Action.java</file><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/GenericAction.java</file><file>src/main/java/org/elasticsearch/action/TransportActionModule.java</file><file>src/main/java/org/elasticsearch/action/TransportActionNodeProxy.java</file><file>src/main/java/org/elasticsearch/action/TransportActions.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ClusterAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/NodesRestartAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/TransportNodesRestartAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/NodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/broadcast/BroadcastPingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/broadcast/BroadcastPingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/broadcast/BroadcastPingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/broadcast/TransportBroadcastPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/IndexReplicationPingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/IndexReplicationPingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/ReplicationPingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/ReplicationPingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/ReplicationPingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/ShardReplicationPingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/TransportIndexReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/TransportReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/TransportShardReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/single/SinglePingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/single/SinglePingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/single/TransportSinglePingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/IndicesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/IndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/TransportGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/count/CountAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportIndexDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/get/GetAction.java</file><file>src/main/java/org/elasticsearch/action/get/GetRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/search/SearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollAction.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/node/NodeIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/client/transport/action/ClientTransportActionModule.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/health/ClientTransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/node/info/ClientTransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/node/restart/ClientTransportNodesRestartAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/node/shutdown/ClientTransportNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/node/stats/ClientTransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/ping/broadcast/ClientTransportBroadcastPingAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/ping/replication/ClientTransportReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/ping/single/ClientTransportSinglePingAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/reroute/ClientTransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/settings/ClientTransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/cluster/state/ClientTransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/alias/ClientTransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/analyze/ClientTransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/cache/clear/ClientTransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/close/ClientTransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/create/ClientTransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/delete/ClientTransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/exists/ClientTransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/flush/ClientTransportFlushAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/gateway/snapshot/ClientTransportGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/mapping/delete/ClientTransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/mapping/put/ClientTransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/open/ClientTransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/optimize/ClientTransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/refresh/ClientTransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/segments/ClientTransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/settings/ClientTransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/stats/ClientTransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/status/ClientTransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/template/delete/ClientTransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/template/put/ClientTransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/admin/indices/validate/query/ClientTransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/count/ClientTransportCountAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/delete/ClientTransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/deletebyquery/ClientTransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/get/ClientTransportGetAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/get/ClientTransportMultiGetAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/index/ClientTransportIndexAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/mlt/ClientTransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/percolate/ClientTransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/search/ClientTransportSearchAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/search/ClientTransportSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/client/transport/action/update/ClientTransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/ping/broadcast/RestBroadcastPingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/ping/replication/RestReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/ping/single/RestSinglePingAction.java</file><file>src/test/java/org/elasticsearch/test/integration/client/transport/DiscoveryTransportClientTests.java</file><file>src/test/java/org/elasticsearch/test/integration/ping/PingActionTests.java</file></files><comments><comment>Add generic execution of APIs to Client (and indices/cluster) and allow for plugins to register custom APIs, closes #1612.</comment></comments></commit></commits></item><item><title>Java API: Move all request builders to org.elasticsearch.action... from org.elasticsearch.client.action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1611</link><project id="" key="" /><description>This is part of the effort to simplify action (API) development, and plugins allowing for custom actions and have them exposed as part of the Client interface. Sadly, it breaks backward comp., though its a simple change. Some plugins will require to be modified (like the couchdb one).
</description><key id="2845283">1611</key><summary>Java API: Move all request builders to org.elasticsearch.action... from org.elasticsearch.client.action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-15T10:42:29Z</created><updated>2013-10-18T07:24:42Z</updated><resolved>2012-01-15T10:45:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-01-30T20:45:21Z" id="3727310">Hi Shay,

Why don't you keep the old class in org.elasticsearch.client.action package, use inheritance from package org.elasticsearch.action and just mark the old class as deprecated ?
It would be easier to migrate all plugins smoothly.

Am I wrong ?
</comment><comment author="dadoonet" created="2012-01-30T21:07:27Z" id="3727724">BTW, I created pull request https://github.com/elasticsearch/elasticsearch-river-couchdb/pull/9 for couchDb plugin
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/NodesRestartRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/NodesShutdownRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/broadcast/BroadcastPingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/ReplicationPingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/single/SinglePingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/support/BaseClusterRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/IndicesExistsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/support/BaseIndicesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/get/GetRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/BaseRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/facet/HistogramFacetSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/facet/QueryFilterFacetSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/facet/TermsFacetSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/stress/SingleThreadBulkStress.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/analyze/AnalyzeActionTests.java</file><file>src/test/java/org/elasticsearch/test/integration/percolator/SimplePercolatorTests.java</file><file>src/test/java/org/elasticsearch/test/integration/readonly/ClusterAndIndexReaderOnlyTests.java</file><file>src/test/java/org/elasticsearch/test/stress/fullrestart/FullRestartStressTest.java</file><file>src/test/java/org/elasticsearch/test/stress/indexing/BulkIndexingStressTest.java</file><file>src/test/java/org/elasticsearch/test/stress/search1/ParentChildStressTest.java</file><file>src/test/java/org/elasticsearch/test/stress/search1/Search1StressTest.java</file></files><comments><comment>Java API: Move all request builders to org.elasticsearch.action... from org.elasticsearch.client.action, closes #1611.</comment></comments></commit></commits></item><item><title>Plugin loading broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1610</link><project id="" key="" /><description>Commit 38e8727a89e88c533191196a5bad4bb6af6a6f81 appears to have broken plugin loading, specifically the change to PluginsService.java. Are plugins required to be packaged differently now? Tested with lang-python and head. If I revert the change to PluginsService.java, they load fine.
</description><key id="2844148">1610</key><summary>Plugin loading broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vermaport</reporter><labels /><created>2012-01-15T02:35:47Z</created><updated>2012-01-15T15:20:21Z</updated><resolved>2012-01-15T15:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-15T15:19:27Z" id="3499513">No, there isn't a change in the structure of plugins, its a mistake in iterating over the wrong file handler. Fix it coming shortly...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>Plugin loading broken, closes #1610.</comment></comments></commit></commits></item><item><title>add percolation support to update action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1609</link><project id="" key="" /><description /><key id="2841941">1609</key><summary>add percolation support to update action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-01-14T17:24:30Z</created><updated>2014-07-16T21:55:44Z</updated><resolved>2012-01-14T23:04:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Multicast Discovery: if it fails, still start in a single cluster mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1608</link><project id="" key="" /><description /><key id="2828687">1608</key><summary>Multicast Discovery: if it fails, still start in a single cluster mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-13T08:40:04Z</created><updated>2012-01-13T08:40:38Z</updated><resolved>2012-01-13T08:40:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file></files><comments><comment>Multicast Discovery: if it fails, still start in a single cluster mode, closes #1608.</comment></comments></commit></commits></item><item><title>Update API: update by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1607</link><project id="" key="" /><description>#1583 allows to update individual documents. Update by query will reduce the network roundtrips radically if you want to update a number of documents and push work from the client to ES.

```
curl -XPOST localhost:9200/index/type/_update -d '{
    "query" : { "constant_score" : { "filter" : { "term" : { "counter" : 0 } } } },
    "script" : "ctx._source.counter += count",
    "params" : {
        "count" : 4
    }
}'
```
</description><key id="2822535">1607</key><summary>Update API: update by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monken</reporter><labels><label>:Reindex API</label></labels><created>2012-01-12T19:36:40Z</created><updated>2016-08-09T17:56:36Z</updated><resolved>2014-07-18T07:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2012-04-12T15:26:43Z" id="5093468">Would really love this feature too!
</comment><comment author="r10r" created="2012-06-03T10:06:05Z" id="6084334">+1
</comment><comment author="darklow" created="2012-06-05T07:54:47Z" id="6119931">+1
</comment><comment author="mrgautamsam" created="2012-06-14T07:00:36Z" id="6321569">+1
</comment><comment author="feridcelik" created="2012-06-16T09:23:08Z" id="6371914">+1
</comment><comment author="gc20" created="2012-09-25T09:48:23Z" id="8848817">+1
</comment><comment author="serpent403" created="2012-11-15T11:07:41Z" id="10404995">+1
</comment><comment author="devilankur18" created="2012-11-19T00:56:41Z" id="10498447">+1
</comment><comment author="willtrking" created="2012-12-01T00:06:24Z" id="10909252">+1
</comment><comment author="ignatiusreza" created="2012-12-10T09:48:00Z" id="11187072">+1
</comment><comment author="kenshin54" created="2012-12-13T06:25:11Z" id="11323890">+1
</comment><comment author="noodlehaus" created="2013-01-03T02:39:35Z" id="11833438">+1
</comment><comment author="Aoseala" created="2013-01-03T03:49:02Z" id="11834308">+1
</comment><comment author="timotta" created="2013-01-24T17:52:31Z" id="12663777">I really need this feature
</comment><comment author="burzum" created="2013-02-04T11:19:04Z" id="13071669">:+1: 
</comment><comment author="ofavre" created="2013-02-13T16:09:47Z" id="13501804">While waiting this feature to be officially finished and released, I've packaged the pull request #2231 as a plugin: [yakaz/elasticsearch-action-updatebyquery](http://github.com/yakaz/elasticsearch-action-updatebyquery/).
Have fun.
</comment><comment author="bobbyrenwick" created="2013-02-27T12:35:55Z" id="14171541">+1
</comment><comment author="geoffwatts" created="2013-02-27T13:38:59Z" id="14173868">+1
</comment><comment author="AnSavvides" created="2013-02-28T09:43:12Z" id="14224952">+1
</comment><comment author="neogenix" created="2013-03-04T02:50:08Z" id="14361844">+1
</comment><comment author="scriby" created="2013-03-04T14:40:27Z" id="14383583">+1
</comment><comment author="gnurag" created="2013-03-07T14:00:56Z" id="14561800">+1
</comment><comment author="damienalexandre" created="2013-03-28T09:50:09Z" id="15576923">+1
</comment><comment author="paulsabou" created="2013-04-11T07:52:27Z" id="16221195">+1
</comment><comment author="acarrasco" created="2013-04-11T11:22:15Z" id="16228841">+1
</comment><comment author="steegi" created="2013-04-11T20:43:32Z" id="16260080">+1
</comment><comment author="thomasma" created="2013-04-17T20:11:27Z" id="16532654">+1
</comment><comment author="oowl" created="2013-04-27T23:00:29Z" id="17124929">+1
</comment><comment author="ttghr" created="2013-05-15T09:45:35Z" id="17929186">+1
</comment><comment author="acerb" created="2013-05-23T11:58:19Z" id="18338703">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Provide more context variables in update scripts</comment></comments></commit></commits></item><item><title>Add a simplified setting to disable shutdown API: action.disable_shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1606</link><project id="" key="" /><description /><key id="2817403">1606</key><summary>Add a simplified setting to disable shutdown API: action.disable_shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-12T12:31:22Z</created><updated>2012-01-12T12:31:45Z</updated><resolved>2012-01-12T12:31:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file></files><comments><comment>Add a simplified setting to disable shutdown API: action.disable_shutdown, closes #1606.</comment></comments></commit></commits></item><item><title>18.6 timeout REST request parameter is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1605</link><project id="" key="" /><description>Specifying timeout in the request parameter seems to be ignored

https://gist.github.com/1596260
</description><key id="2808619">1605</key><summary>18.6 timeout REST request parameter is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frazerh</reporter><labels /><created>2012-01-11T19:18:44Z</created><updated>2013-06-06T11:17:32Z</updated><resolved>2013-06-06T11:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-06-06T11:17:32Z" id="19038755">Closed in favour of #3129 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to provide timeout parameter in request body (as well as URI parameter)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1604</link><project id="" key="" /><description>Allow to provide the timeout parameter in the request body. The value can either be numeric (millis) or a String (time valu).
## Original Request:

curl -XGET 'http://localhost:9200/test/_search?pretty=true' -d '  
{
  "timeout" : 10,
  "query" : {
        "term" : { "id" : 397 }
   }
}  
'  
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[Yk0AMMBbQaqR2dhRV_o91Q][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\n    \"timeout\" : 10,\n\"query\" : {\n        \"term\" : { \"id\" : 397 }\n    }\n}\n]]]; nested: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [No parser for element [timeout]]]; }{[Yk0AMMBbQaqR2dhRV_o91Q][customers_1323957964.851052][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{\n    \"timeout\" : 10,\n\"query\" : {\n        \"term\" : { \"id\" : 397 }\n    }\n}\n]]]; nested: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [No parser for element [timeout]]]; }]",
  "status" : 500
}
</description><key id="2808492">1604</key><summary>Allow to provide timeout parameter in request body (as well as URI parameter)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frazerh</reporter><labels /><created>2012-01-11T19:08:11Z</created><updated>2012-01-12T12:19:30Z</updated><resolved>2012-01-12T12:19:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java</file><file>src/main/java/org/elasticsearch/client/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchRequest.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>src/main/java/org/elasticsearch/search/query/TimeoutParseElement.java</file><file>src/test/java/org/elasticsearch/test/integration/search/timeout/SearchTimeoutTests.java</file></files><comments><comment>Allow to provide timeout parameter in request body (as well as URI parameter), closes #1604.</comment></comments></commit></commits></item><item><title>Better Exception Message when client version is out of date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1603</link><project id="" key="" /><description>A client of me reports he saw:

&gt; org.elasticsearch.client.transport.NoNodeAvailableException: No node available

When in fact the client version was the real problem: 0.17.7 and the version from the server 0.18.7
</description><key id="2805279">1603</key><summary>Better Exception Message when client version is out of date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2012-01-11T15:07:26Z</created><updated>2012-01-12T12:30:34Z</updated><resolved>2012-01-12T09:24:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-11T18:14:26Z" id="3450865">IF you enable logging on the client side, you will see the failures. But, this is the correct failure to raise, since there are no nodes to talk to as none were discovered.
</comment><comment author="karussell" created="2012-01-12T09:25:25Z" id="3460336">which log4 config?
</comment><comment author="kimchy" created="2012-01-12T12:30:34Z" id="3462394">`org.elasticsearch.client.transport`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve latitude and longitude normalization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1602</link><project id="" key="" /><description>The current latitude and longitude normalization code seems to work fine, and surely will in most situations.
However it won't behave correctly in some corner cases and more importantly can result in infinite loop with infinite values.

My proposition is based on the modulus operator used with floating point values.
Note that it should not be significantly faster or slower in normal cases.

See [this commit comment](https://github.com/elasticsearch/elasticsearch/commit/779dc4309bfb03fe0f1335d5c9e3b06e802ebc70#commitcomment-853482) for more.
</description><key id="2793268">1602</key><summary>Improve latitude and longitude normalization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2012-01-10T16:28:41Z</created><updated>2014-06-16T16:48:56Z</updated><resolved>2012-01-12T14:59:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-12T11:38:09Z" id="3461773">Can you add some tests for it? under org.elasticsearch.unit...? (I know, it doesn't have any now...)
</comment><comment author="ofavre" created="2012-01-12T14:50:30Z" id="3464279">Done. I think the test is exhaustive enough!
</comment><comment author="kimchy" created="2012-01-12T14:59:56Z" id="3464438">Pushed.
</comment><comment author="asterite" created="2012-05-29T08:42:33Z" id="5978326">We found a bug in this pull request. If the filter is this:

``` json
{"geo_bounding_box": {
  "location": {
    "top_left": {"lat":90,"lon":-180},
    "bottom_right":{"lat":-90,"lon":180}
  }
}}
```

That is, the latitude range is [-90, 90] and the longitude range is [-180, 180] then it doesn't work because the normalized value of -90 is, with this new code, 90 (previously it was -90 and it worked). Same goes for -180, which gets normalized to 180.

I know in this case we can just not put the filter because it's the whole world. But if you have a filter where lat is [-90, -45] then it will get translated to [90, -45] which is, again, wrong.
</comment><comment author="ofavre" created="2012-05-29T14:41:00Z" id="5985340">When indexing, -90 (lat) and -180 (lon) must be normalized to 90 and 180 respectively.
However, you are absolutely right that such normalization must not be used directly for normalizing a range query.
We have 2 scenarii:
- One want to clamp the search to the `[-90;90] x [-180;180]` (_inclusives_, you are right) domain
- One want to handle split searches.

For instance if one has a range query over, considering latitude only for simplicity, `[80;100]`. Such a range should be split into `[-90;-80] U [80;90]`.
This can also arise for the longitude, hence potentially leading to a union of 4 subranges.

I'm working on an update pull request... thanks for reporting this problem!
</comment><comment author="asterite" created="2012-05-29T16:03:40Z" id="5987775">Awesome. Thanks! :-)
</comment><comment author="ofavre" created="2012-05-29T16:33:48Z" id="5988616">Clamping is not a feasible solution.
I was tempted to define it as "If either of the top left or bottom right point lie in the normalized domain, just clamp the other to lie within it too."
But that leaves us with the else case. Should we normalize the top left or bottom right point (or even top right or bottom left) and clamp the other one? This would make the result pretty unpredictable.

I think the splitting approach is the only true answer to the problem.

If you see a good definition and use case of the clamp idea, please comment, I can still leave the function in `GeoUtils`, even if unused by the filters.
</comment><comment author="asterite" created="2012-05-30T02:58:49Z" id="6001165">Hi. I'm not familiar with the code base. I just updated ElasticSearch and bumped to that problem. I looked at where the problem could be and found it. But I'm not sure how to fix this and how it could affect other parts of the code...
</comment><comment author="ofavre" created="2012-05-30T08:40:11Z" id="6005041">Oh you don't even need to be able to read code.
Do you think it would make sense to have a clamped geo range request for anyone's needs? that's the whole question. I guess the answers is no.

Progress update: I'm writing tests to ensure the splitting works as intended.
I'll then check the usage of normalizeLat/Lon to check which ones would benefit from the "new functionality".
</comment><comment author="ofavre" created="2012-05-30T14:32:05Z" id="6011281">I fixed the `GeoBoundingBoxFilterParser`, but the `GeoPolygonFilterParser` will be a lot more tricky to fix!
In addition, I think the latter is subject to silly `DivideByZeroException`s.
</comment><comment author="ofavre" created="2012-05-31T10:21:31Z" id="6032413">I found another problem: when wrapping latitude, one must not proceed like for longitude.
When in at 89° lat 0° lon and going up 3°, you end up in 87° lat 180° lon, ie. there is a 180° shift in the longitude for crossing the poles.
This will make more changes, but complexify even more the range splitting...
</comment><comment author="ofavre" created="2012-06-01T10:54:55Z" id="6057963">I'm opening [issue #1997](https://github.com/elasticsearch/elasticsearch/issues/1997) for the pull request and subsequent issues.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add thread_pool to nodes info and nodes stats APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1601</link><project id="" key="" /><description>Add `thread_pool` option (flag, URI) to return thread pool info in the nodes info API, and thread pool stats in the nodes stats API.
</description><key id="2792605">1601</key><summary>Add thread_pool to nodes info and nodes stats APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2012-01-10T15:44:41Z</created><updated>2012-01-10T15:45:21Z</updated><resolved>2012-01-10T15:45:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/client/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>src/main/java/org/elasticsearch/client/action/admin/cluster/node/stats/NodesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPoolInfo.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPoolStats.java</file></files><comments><comment>Add thread_pool to nodes info and nodes stats APIs, closes #1601.</comment></comments></commit></commits></item><item><title>handle timestamp and TTL in update action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1600</link><project id="" key="" /><description /><key id="2781252">1600</key><summary>handle timestamp and TTL in update action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-01-10T09:37:45Z</created><updated>2014-07-16T21:55:45Z</updated><resolved>2012-01-10T11:44:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Date Histogram Facet: Add `pre_offset` and `post_offset` options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1599</link><project id="" key="" /><description>Allow to define a `pre_offset` and `post_offset` for offsets applying pre rounding and post rounding. The values are time values with a possible `-` sign.

For example, to offset a `week` rounding to start on Sunday instead of Monday, one can pass `pre_offset` of  `-1d` to decrease a day _before_ doing the week (monday based) rounding, and then have `post_offset` set to `-1d` to actually set the return value to be Sunday, and not Monday.
</description><key id="2774110">1599</key><summary>Date Histogram Facet: Add `pre_offset` and `post_offset` options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-09T19:28:24Z</created><updated>2015-03-03T18:50:30Z</updated><resolved>2012-01-09T19:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tmandry" created="2015-03-03T18:50:30Z" id="77008638">Found this because I was looking for how to do exactly what you described (make weeks start on Sunday). If I'm not mistaken you actually want a `pre_offset` of `1d` (positive) and a `post_offset` of `-1d`.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/TimeZoneRounding.java</file><file>src/main/java/org/elasticsearch/search/facet/datehistogram/DateHistogramFacetProcessor.java</file><file>src/test/java/org/elasticsearch/test/unit/deps/joda/TimeZoneRoundingTests.java</file></files><comments><comment>Date Histogram Facet: Add `pre_offset` and `post_offset` options, closes #1599.</comment></comments></commit></commits></item><item><title>Start of the week(i.e. Sunday or Monday) in the Histogram or Date Histogram facet with interval </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1598</link><project id="" key="" /><description>Hi, It seems there is no way to specify the start of the week in these facet. 
I have searched the discussion group and issue list, and there seems to be no such discussion yet. so I decided to submit it as an  issue. 

This is when you use interval with week, you will need a way to specifiy when is the start of the week.

If you look at the mysql documentation on the week . there are 7 mode, i am not sure if we need to support all of them, but it might make sense to start different 

Mode    First day of week   Range   Week 1 is the first week …
0   Sunday  0-53    with a Sunday in this year
1   Monday  0-53    with more than 3 days this year
2   Sunday  1-53    with a Sunday in this year
3   Monday  1-53    with more than 3 days this year
4   Sunday  0-53    with more than 3 days this year
5   Monday  0-53    with a Monday in this year
6   Sunday  1-53    with more than 3 days this year
7   Monday  1-53    with a Monday in this year

do we have the option to support this or if it is not supported,what is the default for interval week .
</description><key id="2770906">1598</key><summary>Start of the week(i.e. Sunday or Monday) in the Histogram or Date Histogram facet with interval </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">n0rthwood</reporter><labels /><created>2012-01-09T16:17:45Z</created><updated>2014-01-22T11:51:59Z</updated><resolved>2014-01-22T11:51:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="n0rthwood" created="2012-01-09T17:07:51Z" id="3416334">We notice this issue from move one of our old analytic system to ES .  one fo the weekly calculation doesn't match our mysql version. we reviewed the problem, and find out that the difference might be that we are using a different way to generate the week number and the week actually means slight different time range  for the calculation when the facet try to aggregate it. 
</comment><comment author="kimchy" created="2012-01-09T19:11:55Z" id="3418388">The week is from Monday till Sunday.

#1580 has extensive refactoring in the data histogram to better support time zones. It allows to configure `pre_zone` (pre rounding time zone) and `post_zone` (post rounding timezone). We can add a similar `pre_offset` and `post_offset` that can be applied, and then you can "move" the calculation however you want...
</comment><comment author="kimchy" created="2012-01-09T19:29:53Z" id="3418689">Pushed #1599, should do the trick and allows you to control what you want. Still requires a bit of work on your end to apply it only when `week` rounding applied, but its the best (and most flexible) way for now...
</comment><comment author="n0rthwood" created="2012-01-09T19:54:27Z" id="3419094">thanks, this sounds like a good solution
On Jan 9, 2012 7:29 PM, "Shay Banon" &lt;
reply@reply.github.com&gt;
wrote:

&gt; Pushed #1599, should do the trick and allows you to control what you want.
&gt; Still requires a bit of work on your end to apply it only when `week`
&gt; rounding applied, but its the best (and most flexible) way for now...
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1598#issuecomment-3418689
</comment><comment author="Downchuck" created="2013-08-02T19:52:30Z" id="22030954">Go ahead and close this issue?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node Stats API: Add specific flags for stats, simplified REST paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1597</link><project id="" key="" /><description>Simplified paths for node stats include: `/_node/stats` and `/_node/{nodeIds}/stats`. Also, add specific flags to get only specific stats, by default, returning `indices` stats. The flags are: `indices`, `os`, `process`, `jvm`, `network`, `transport`, and `http`. `clear` can be used to clear all flags and then only apply the ones provided. For example: `_nodes/stats?clear=true&amp;os=true&amp;process=true`.

Specific REST paths are provided for each stats type, for example: `/_node/stats/process`, and `/_node/{nodeId}/stats/process`. 

As usual, nodeId can be a list of node ids, or a list of IPs, hostnames, or attribute matching, all supporting wildcards.
</description><key id="2770543">1597</key><summary>Node Stats API: Add specific flags for stats, simplified REST paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-09T16:01:12Z</created><updated>2012-01-09T16:01:54Z</updated><resolved>2012-01-09T16:01:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/client/action/admin/cluster/node/stats/NodesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file></files><comments><comment>Node Stats API: Add specific flags for stats, simplified REST paths, closes #1597.</comment></comments></commit></commits></item><item><title>Nodes Info API: Allow to specify which info to get back, simpler URI paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1596</link><project id="" key="" /><description>The nodes info API to allow for simpler usage and specifying which part of the info to get back. By default, it just returns the attributes and core settings of the node (breaking change!).

Also, allow also for `/_nodes` and `/_nodes/{nodeId}` endpoints (with the smart nodeId resolution, can be name, hostname, IP, with simple wildcards).

The flags: `settings`, `os`, `process`, `jvm`, `network`, `transport` and `http`, can be set to true to return the relevant info. For example: `/_nodes?os=true&amp;process=true`.

Specific endpoints to further simplify it are provided, for example: `/_nodes/process`, or `/_nodes/192.*/process`.
</description><key id="2768670">1596</key><summary>Nodes Info API: Allow to specify which info to get back, simpler URI paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-09T13:23:41Z</created><updated>2012-01-09T13:24:39Z</updated><resolved>2012-01-09T13:24:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/client/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file></files><comments><comment>Nodes Info API: Allow to specify which info to get back, simpler URI paths, closes #1596.</comment></comments></commit></commits></item><item><title>Improve serialization (stream) of UTF strings, note, requires flush when upgrading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1595</link><project id="" key="" /><description>Improve string serialization. Since the format changes, it requires a flush when upgrading since the translog will not be serialized differently.
</description><key id="2760876">1595</key><summary>Improve serialization (stream) of UTF strings, note, requires flush when upgrading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-08T13:25:57Z</created><updated>2012-01-08T13:26:46Z</updated><resolved>2012-01-08T13:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/CachedStreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file></files><comments><comment>Improve serialization (stream) of UTF strings, note, requires flush when upgrading, closes #1595.</comment></comments></commit></commits></item><item><title>Move phonetic token filter to a plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1594</link><project id="" key="" /><description>Move the phonetic token filter to a plugin: https://github.com/elasticsearch/elasticsearch-analysis-phonetic.
</description><key id="2757911">1594</key><summary>Move phonetic token filter to a plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-07T21:18:03Z</created><updated>2012-01-09T13:53:36Z</updated><resolved>2012-01-07T21:18:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-01-09T09:55:06Z" id="3409364">Heya - Why did you move phonetic to a plugin?  Are you planning on doing this for other filters as well?
</comment><comment author="kimchy" created="2012-01-09T13:53:36Z" id="3411682">Its going to be supported in Lucene 3.6, so will use that codebase. But, its going to be a separate module in Lucene, so it makes sense to have it as a plugin similar to ICU.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>src/main/java/org/elasticsearch/index/analysis/phonetic/DoubleMetaphoneFilter.java</file><file>src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticFilter.java</file><file>src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticTokenFilterFactory.java</file><file>src/test/java/org/elasticsearch/test/unit/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>Move phonetic token filter to a plugin, closes #1594.</comment></comments></commit></commits></item><item><title>Highlight snippets have whitespace appended when using term_vector:with_positions_offsets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1593</link><project id="" key="" /><description>When configuring and searching the index with the following:

```
curl -XPUT http://localhost:9200/code_files -d '{
    "settings" : {
        "analysis" : {
            "analyzer" : {
                "standard_nostop" : {
                    "tokenizer" : "standard",
                    "filter" : "standard,lowercase"
                },
                "standard_nostop_nolower" : {
                    "tokenizer" : "standard",
                    "filter" : "standard"
                }
            }
        }
    },
    "mappings" : {
        "file" : {
            "properties" : {
                "body" : {
                    "type" : "multi_field",
                    "fields" : {
                        "body" : {
                            "type" : "string",
                            "analyzer" : "standard_nostop",
                            "term_vector" : "with_positions_offsets"
                        },
                        "nocase" : {
                            "type" : "string",
                            "analyzer" : "standard_nostop_nolower",
                            "term_vector" : "with_positions_offsets"
                        }
                    }
                }
            }
        }
    }
}'

curl -XPUT http://localhost:9200/code_files/file/1 -d '{
    "body" : "Yajl::Parser.parse(str, :symbolize_keys =&gt; true)"
}'

curl http://localhost:9200/code_files/file/_search?pretty=1 -d '{
    "query" : {
        "dis_max" : {
            "queries" : [
            {"text_phrase" : {"body" : {"query" : "Yajl::Parser.parse", "boost" : 20}}},
            {"text_phrase" : {"body.nocase" : {"query" : "Yajl::Parser.parse", "boost" : 30}}},
            {"text" : {"body" : {"query" : "Yajl::Parser.parse", "boost" : 1}}},
            {"text" : {"body.nocase" : {"query" : "Yajl::Parser.parse", "boost" : 3}}}
            ]
        }
    },
    "highlight" : {
        "fields" : {
            "body" : {},
            "body.nocase" : {}
        }
    }
}'
```

I get this result:

```
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.26849622,
    "hits" : [ {
      "_index" : "code_files",
      "_type" : "file",
      "_id" : "1",
      "_score" : 0.26849622, "_source" : {
    "body" : "Yajl::Parser.parse(str, :symbolize_keys =&gt; true)"
},
      "highlight" : {
        "body" : [ "&lt;em&gt;Yajl::Parser.parse&lt;/em&gt;(str, :symbolize_keys =&gt; true) " ],
        "body.nocase" : [ "&lt;em&gt;Yajl::Parser.parse&lt;/em&gt;(str, :symbolize_keys =&gt; true) " ]
      }
    } ]
  }
}
```

Notice "body" and "body.nocase" both have an extra space appended to the end. Creating the index without `{"term_vector" : "with_positions_offsets"}` set fixes it.
</description><key id="2753391">1593</key><summary>Highlight snippets have whitespace appended when using term_vector:with_positions_offsets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brianmario</reporter><labels /><created>2012-01-06T23:22:03Z</created><updated>2012-06-07T14:06:07Z</updated><resolved>2012-06-07T14:06:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-07T19:56:04Z" id="3397804">This seems to be coming from the Lucene implementation, I will chase it down.
</comment><comment author="brianmario" created="2012-01-14T22:53:15Z" id="3495673">Any luck with this?
</comment><comment author="kimchy" created="2012-01-15T20:04:24Z" id="3501404">Yea, tracked it down to a Lucene bug: https://issues.apache.org/jira/browse/LUCENE-3698. Submitted a patch, lets see how it goes.
</comment><comment author="brianmario" created="2012-01-15T20:15:23Z" id="3501494">awesome thanks for the update!
</comment><comment author="TwP" created="2012-05-31T22:45:49Z" id="6049570">Possibly related to #1994 as well.
</comment><comment author="kimchy" created="2012-06-07T14:06:07Z" id="6176584">This has been fixed in Lucene 3.6, which is part of 0.19.2 version. @TwP will answer on the other issue you opened.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk API does delete item does not respect routing associated with an alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1592</link><project id="" key="" /><description>This results in a broadcast delete, which is a shame, not really a problem though....
</description><key id="2752001">1592</key><summary>Bulk API does delete item does not respect routing associated with an alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-06T21:01:18Z</created><updated>2012-01-06T21:39:01Z</updated><resolved>2012-01-06T21:39:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-06T21:39:01Z" id="3391255">Fixed in #1589.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi Get does not respect routing set on aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1591</link><project id="" key="" /><description /><key id="2751991">1591</key><summary>Multi Get does not respect routing set on aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-06T20:59:53Z</created><updated>2012-01-06T21:38:52Z</updated><resolved>2012-01-06T21:38:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-06T21:38:52Z" id="3391252">Fixed in #1589.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter cache to have just weighted (node) and none, and index query parser cache to be size based</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1590</link><project id="" key="" /><description>Upgrading to guava 11, use its `Cache` interface. Also, do some refactoring in some of our caching layers:
- Filter cache now only has two modes: `weighted` (the old node mode) and `none`. The others are really meaningless and the `weighted` one is the best one to use as it allows to assign a percentage from the JVM heap for filter cache.
- Index query parser cache is a very lightweight cache, mainly there not to parse the same query several times when several shards exists on the same node. Move away from using weak values to simply using a size bounded cache (100 by default).
</description><key id="2738733">1590</key><summary>Filter cache to have just weighted (node) and none, and index query parser cache to be size based</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-05T18:43:36Z</created><updated>2012-01-05T18:44:19Z</updated><resolved>2012-01-05T18:44:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/cache/CacheBuilderHelper.java</file><file>src/main/java/org/elasticsearch/common/inject/internal/BytecodeGen.java</file><file>src/main/java/org/elasticsearch/common/inject/internal/ConstructionContext.java</file><file>src/main/java/org/elasticsearch/common/inject/util/Modules.java</file><file>src/main/java/org/elasticsearch/index/cache/field/data/resident/ResidentFieldDataCache.java</file><file>src/main/java/org/elasticsearch/index/cache/field/data/soft/SoftFieldDataCache.java</file><file>src/main/java/org/elasticsearch/index/cache/field/data/support/AbstractConcurrentMapFieldDataCache.java</file><file>src/main/java/org/elasticsearch/index/cache/field/data/weak/WeakFieldDataCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/node/NodeFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/weighted/WeightedFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/QueryParserCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/resident/ResidentQueryParserCache.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/soft/SoftQueryParserCache.java</file><file>src/main/java/org/elasticsearch/index/cache/query/parser/weak/WeakQueryParserCache.java</file><file>src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesNodeFilterCache.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/test/java/org/elasticsearch/test/unit/index/cache/filter/FilterCacheTests.java</file></files><comments><comment>Filter cache to have just weighted (node) and none, and index query parser cache to be size based, closes #1590.</comment></comments></commit></commits></item><item><title>No master (startup / minimum_master_node) / not recovered blocks should cause proper failures on operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1589</link><project id="" key="" /><description>When there is no master node yet elected (or got into this state because of minimum master nodes), or the cluster has not recovered yet, a proper failure should be raised on all actions. Rest status code should be `503`.
## Original Request:

When a node in a cluster reboots, there is a brief period of time while that node is searching for a master that that node can 404, even if `discovery.zen.minimum_master_nodes` equals 2:

```
% sudo service elasticsearch restart
...
time passes
...
% curl http://localhost:9200/index/type/id
{"error":"IndexMissingException[[index] missing]","status":404}
% curl http://localhost:9200/_cluster/health
{"error" : "MasterNotDiscoveredException[]", "status" : 500}
```

Since this is the same error message you get when the there's actually no index, I need to check with /_cluster/health to see if the cluster is in this startup phase to tell if this is a temporary error or not. It'd be preferable if instead elasticsearch could just raise a separate error so I don't need to do that.
</description><key id="2736961">1589</key><summary>No master (startup / minimum_master_node) / not recovered blocks should cause proper failures on operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-05T16:14:34Z</created><updated>2012-01-06T21:39:19Z</updated><resolved>2012-01-06T21:39:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/UnavailableShardsException.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/broadcast/TransportBroadcastPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/TransportIndexReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/TransportReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/replication/TransportShardReplicationPingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ping/single/TransportSinglePingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/TransportGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/index/TransportIndexDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/index/TransportShardDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportIndexDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndexReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndicesReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/TransportSingleCustomOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/test/java/org/elasticsearch/test/integration/cluster/MinimumMasterNodesTests.java</file><file>src/test/java/org/elasticsearch/test/integration/cluster/NoMasterNodeTests.java</file></files><comments><comment>No master (startup / minimum_master_node) / not recovered blocks should cause proper failures on operations, closes #1589.</comment></comments></commit></commits></item><item><title>CouchDB river plugin fails to delete documents in multi-type indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1588</link><project id="" key="" /><description>We have a CouchDB river configured like this:

``` json
{
    "type" : "couchdb",
    "couchdb" : {
        "host" : "couchdb-host",
        "port" : 5984,
        "db" : "database",
        "script" : "ctx._type = ctx.doc.type; if (ctx.doc.parent_id) ctx._parent = ctx.doc.parent_id"
    }
}
```

The CouchDB database contains documents of various types which are mapped to Elasticsearch index types. The problem comes when one of those documents is deleted.

The changes feed doesn't seem to include the document data when a delete occurs so the script configured for the river can't decide the correct _type. Thus Elasticsearch ends up issuing a delete for a document that doesn't exist.

A workaround for this is to create one river per type with a filter function on the changes feed but this won't scale nicely if there's many document types.
</description><key id="2731506">1588</key><summary>CouchDB river plugin fails to delete documents in multi-type indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">deverton</reporter><labels /><created>2012-01-05T03:51:12Z</created><updated>2012-01-05T04:44:51Z</updated><resolved>2012-01-05T04:44:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="deverton" created="2012-01-05T04:44:51Z" id="3365590">Woops. Just realised this should be on the plugin.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add ttl tests with routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1587</link><project id="" key="" /><description /><key id="2729299">1587</key><summary>add ttl tests with routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-01-04T22:38:03Z</created><updated>2014-07-16T21:55:45Z</updated><resolved>2012-01-05T13:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>TTL does not respect routing when expiring (deleting) documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1586</link><project id="" key="" /><description /><key id="2726560">1586</key><summary>TTL does not respect routing when expiring (deleting) documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.7</label><label>v0.19.0.RC1</label></labels><created>2012-01-04T18:33:50Z</created><updated>2012-01-04T18:37:22Z</updated><resolved>2012-01-04T18:37:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/selector/UidAndRoutingFieldSelector.java</file><file>src/main/java/org/elasticsearch/indices/ttl/IndicesTTLService.java</file></files><comments><comment>TTL does not respect routing when expiring (deleting) documents, closes #1586</comment></comments></commit></commits></item><item><title>No need for plenty of RAM for plugin utility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1585</link><project id="" key="" /><description>RAM is often already taken by the elasticsearch service running beside so maybe this could be helpfull limiting its usage for the little plugin utility (usefull for puppet rules for example)

Avoiding :
Error occurred during initialization of VM
Could not reserve enough space for object heap
Could not create the Java virtual machine.
</description><key id="2721956">1585</key><summary>No need for plenty of RAM for plugin utility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dhardy92</reporter><labels /><created>2012-01-04T11:32:09Z</created><updated>2014-07-16T21:55:46Z</updated><resolved>2012-01-04T17:15:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-04T11:53:09Z" id="3353605">When did you get the problem? Cause the scripts were changed cause they used to use the same memory settings (if set in `JAVA_OPTS`) as the elasticsearch server, but they no longer do. The defaults for Java are pretty low...
</comment><comment author="dhardy92" created="2012-01-04T12:04:29Z" id="3353688">I shay,

I get it when I want to install some plugin while elesticsearch is running on an openvz system with OpenJDK

[root@ies01:~]# cd /usr/share/elasticsearch/
[root@ies01:/usr/share/elasticsearch]# bin/plugin -install lukas-vlcek/bigdesk
Error occurred during initialization of VM
Could not reserve enough space for object heap
Could not create the Java virtual machine.

[root@ies01:~]# java -version
Error occurred during initialization of VM
Could not reserve enough space for object heap
Could not create the Java virtual machine.
[root@ies01:~]# java  -Xmx64m -Xms16m -version
java version "1.6.0_18"
OpenJDK Runtime Environment (IcedTea6 1.8.10) (6b18-1.8.10-0+squeeze2)
OpenJDK 64-Bit Server VM (build 14.0-b16, mixed mode)
</comment><comment author="kimchy" created="2012-01-04T12:32:16Z" id="3353923">Sure, we can get it in, can you also change the plugin.bat to streamline the change?

Btw, how much memory do you have for the OS? how much is allocated to ES? Also, use a newer JDK.
</comment><comment author="dhardy92" created="2012-01-04T16:14:01Z" id="3356602">I modified the plugin.bat too 
The JDK is latest OpenJDK from debian distribution.
My OpenVZ serveur is 6G RAM allocated elasticsearch run with ES_MAX_MEM=4g
</comment><comment author="kimchy" created="2012-01-04T17:15:36Z" id="3357557">cheers, pushed. Regarding the JVM, its a pretty old version, might make sense to explicitly upgrade.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugins: If a plugin has a bin directory, move it under the main bin location under the plugin name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1584</link><project id="" key="" /><description>If a plugin has a bin directory, move that directory to the main installation `bin` location under the plugin name as a directory.
</description><key id="2709691">1584</key><summary>Plugins: If a plugin has a bin directory, move it under the main bin location under the plugin name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-03T12:02:48Z</created><updated>2012-01-03T12:03:35Z</updated><resolved>2012-01-03T12:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>Plugins: If a plugin has a bin directory, move it under the main bin location under the plugin name, closes #1584.</comment></comments></commit></commits></item><item><title>Update API: Allow to update a document based on a script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1583</link><project id="" key="" /><description>The update action allows to directly update a specific document based on a script. The operation gets the document (collocated with the shard) from the index, runs the script (with optional script language and parameters), and index back the result (also allows to delete, or ignore the operation). 

Note, this operation still means full reindex of the document, it just removes some network roundtrips and reduces chances of version conflicts between the get and the index.

For example, lets index a simple doc:

```
curl -XPUT localhost:9200/test/type1/1 -d '{
    "counter" : 1,
    "tags" : ["red"]
}'
```

Now, we can execute a script that would increment the counter

```
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "ctx._source.counter += count",
    "params" : {
        "count" : 4
    }
}'
```

We can also add a tag to the list of tags:

```
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "ctx._source.tags += tag",
    "params" : {
        "tag" : "blue"
    }
}'
```

And, we can delete the doc if the tags contain `blue`, or ignore (noop):

```
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "ctx._source.tags.contains(tag) ? ctx.op = \"delete\" : ctx.op = \"none\"",
    "params" : {
        "tag" : "blue"
    }
}'
```
## Parameters:
- `routing`: Sets the routing that will be used to route the document to the relevant shard.
- `parent`: Simply sets the routing.
- `timeout`: Timeout waiting for a shard to become available.
- `replication`: The replication type for the delete/index operation (`sync` or `async`).
- `consistency`: The write consistency of the index/delete operation.
- `retry_on_conflict`: How many times to retry if there is a version conflict between getting the document and indexing / deleting it. Defaults to `0`.
</description><key id="2704490">1583</key><summary>Update API: Allow to update a document based on a script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2012-01-02T20:01:47Z</created><updated>2013-12-09T16:18:31Z</updated><resolved>2012-01-02T20:02:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="n0rthwood" created="2012-01-05T23:07:37Z" id="3378087">Thank you so much on this update. assume this is still in trunk and will be released0.19.0?
also, if i have a nested document , updating it can also be expressed in the script i assume:
something like :
with array and netsted object supported ?
ctx._source.classification=[{"cid"=10,"cvalue"=20},{"cid"=20,"cvalue"=30}]

also, may i ask where is this script reference, I checked though the documentation, but it seems (http://www.elasticsearch.org/guide/reference/modules/scripting.html) it didn't mention this kind of gramma, i.e. refering to a document as ctx and assign value to it. is there more to come?
</comment><comment author="kimchy" created="2012-01-06T11:11:34Z" id="3383295">@n0rthwood yea, it will be part of the next release. The grammer in the scripting part relates to search, not for this one. its mainly a set of hashes and lists (json), stored under `ctx._source`. You can change it however you want, either using `mvel` (the default) or using one of the plugin scripting langs (including javascript).
</comment><comment author="medcl" created="2012-01-07T17:22:32Z" id="3396874">very happy to see this feature,my poor paritalupdate plugin can be retired now~
</comment><comment author="monken" created="2012-01-10T15:43:15Z" id="3431502">Great feature (I think you can close #426)

One thing I miss:
Woud it be possible to call update on a scroll id? That would allow to update a number of documents in ES instead of pulling them to the client and push the update for each document.

Proposed API:

```
curl -XPOST 'http://localhost:9200/_search/update?scroll_id=c2Nhbjs...WUc1'
-d '{"script":"...","params": ... }'
```
</comment><comment author="kimchy" created="2012-01-10T17:56:55Z" id="3433874">@monken thats basically "update by query", no need to provide a scroll id, just provide the query to use. Its much harder to do, and will come with a lot of caveats (i.e. the query update might fail in the middle of the operation and only be partially completed).
</comment><comment author="monken" created="2012-01-10T18:04:06Z" id="3434003">@kimchy is "update by query" already implemented? I couldn't find it.
I was just worried about the syntax because you have to provide both the query and the update script in the request body. How would they live next to each other?
</comment><comment author="kimchy" created="2012-01-10T18:10:34Z" id="3434107">@monken no, its not implemented. The structure should be simple, the query under `query`, and script under `script`.
</comment><comment author="monken" created="2012-01-10T18:12:16Z" id="3434140">@kimchy should I open a new ticket for this request?
</comment><comment author="kimchy" created="2012-01-11T18:15:26Z" id="3450884">@monken yea, open one, though I am still not sure how to best implement it. It going to come with a lot of caveats. 
</comment><comment author="monken" created="2012-01-12T19:38:53Z" id="3469124">@kimchy I bet you'll figure something out :-)
</comment><comment author="folke" created="2012-01-12T21:57:34Z" id="3471306">Awesome! :-)
</comment><comment author="seyyedi" created="2012-01-23T05:18:34Z" id="3609376">@kimchy can the update action be used in bulk? I would have expected something along the lines of 

{ "update" : { "_index" : "main", "_type" : "visits", "_id" : "21" } }
{ "script" : "ctx._source.count += i", "params": { "i" : 3 } }

but i can't get it to work (with current version in trunk). Is there any inherent problem with update/bulk or am i using the wrong syntax or is it just too early? :-)

Awesome feature by the way!
</comment><comment author="kimchy" created="2012-01-23T14:22:10Z" id="3614451">@seyyedi No, it can't be used in bulk (but nice imaginative format for it :) ). I supposed there is an option to support it in bulk, but I was thinking that if we have the update by query (which we still don't) then it will be less needed. Though, I guess it has its uses.
</comment><comment author="rolyv" created="2012-01-27T15:26:02Z" id="3688334">Can you update a document's TTL and Timestamp?
</comment><comment author="Paikan" created="2012-01-28T08:49:31Z" id="3699125">@rolyv yes you can update TTL and Timestamp on master branch.

You can use ctx._ttl and ctx._timestamp in your script.
</comment><comment author="Alex-Ikanow" created="2012-02-07T17:03:19Z" id="3851705">Nice new feature! Is there any way this could be (theoretically modified to be) used to update just a nested object, while leaving the "parent" document alone? 

Eg in my use case I have documents with a large full text index, and many nested sub-objects that have a smallish number of indexed fields. 

Any given sub-object has (numeric) attributes that change every few hours, but I don't want to re-index the entire document (store in MongoDB), so at the moment I just discard that attribute and try to combine them as best I can in the application layer.

If I could "just" modify (numeric) fields inside specified nested sub-objects (eg all sub-objects matching a query), without touching the "parent" document, that would remove my last MongoDB-elasticsearch synchronization issue.
</comment><comment author="msayapin" created="2012-02-12T12:12:01Z" id="3927400">Thanks for the great feature! Is it possible to update stored field if the `_source` is disabled? I tried `ctx.fields`, `ctx._fields`, `ctx['...']` to no avail...
</comment><comment author="kimchy" created="2012-02-12T16:56:27Z" id="3929074">@msayapin no, this only works when _source is enabled, otherwise, we can't reindex the doc. 
</comment><comment author="gzsombor" created="2012-03-06T13:31:47Z" id="4344277">Is it documented anywhere, what variables can a script access ? The documentation briefly mentions ctx._ttl, and ctx._timestamp : http://www.elasticsearch.org/guide/reference/api/update.html but it's not clear, what is the type of this variables (string, number or timestamp? ) And how to enable / disable __source_ ?
</comment><comment author="Paikan" created="2012-03-06T13:44:07Z" id="4344442">@gzsombor the _ttl can be a number or a string representing a TimeValue like "1d". The _timestamp is a String wich can be a timestamp or use configured date format. You can check http://www.elasticsearch.org/guide/reference/mapping/timestamp-field.html and http://www.elasticsearch.org/guide/reference/mapping/ttl-field.html for more information.

The _source is enabled by default. Have a look here http://www.elasticsearch.org/guide/reference/mapping/source-field.html to disable it.
</comment><comment author="alex-in2" created="2013-12-09T15:56:58Z" id="30143526">Is it possible to use _version in conditions ? 

like 

if(ctx._version == 1)
{
ctx._source.field1 = 'some value'
}
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/TransportActionModule.java</file><file>src/main/java/org/elasticsearch/action/TransportActions.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/InstanceShardOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateResponse.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/action/update/UpdateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/action/ClientTransportActionModule.java</file><file>src/main/java/org/elasticsearch/client/transport/action/update/ClientTransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClient.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentParser.java</file><file>src/main/java/org/elasticsearch/common/xcontent/support/AbstractXContentParser.java</file><file>src/main/java/org/elasticsearch/index/engine/DocumentMIssingEngineException.java</file><file>src/main/java/org/elasticsearch/index/engine/DocumentSourceMissingEngineException.java</file><file>src/main/java/org/elasticsearch/index/get/GetResult.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>src/main/java/org/elasticsearch/search/lookup/SourceLookup.java</file><file>src/test/java/org/elasticsearch/test/integration/client/transport/TransportClientDocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/test/integration/client/transport/TransportClientMoreLikeThisActionTests.java</file><file>src/test/java/org/elasticsearch/test/integration/client/transport/TransportClientSniffDocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/test/integration/document/AliasedIndexDocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/test/integration/document/DocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/test/integration/document/LocalDocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/test/integration/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/test/integration/mlt/MoreLikeThisActionTests.java</file></files><comments><comment>Update API: Allow to update a document based on a script, closes #1583.</comment></comments></commit></commits></item><item><title>S3 blob storage gateway: deleting an index named x destroys data for any index with name beginning with x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1582</link><project id="" key="" /><description>Thank you for your help in issue #1564. I am using ElasticSearch v0.18.5 with the S3 shared storage gateway and have been experiencing index data loss: after a restart, some indexes will appear with correct mappings but zero documents.

It looks like this happens because deleting an index named "x" causes ES to remove all keys with prefix 
"clustername/indicies/x" which includes data for any index whose name begins with "x". It should only remove keys with prefix "clustername/indicies/x/".

To reproduce:

&lt;pre&gt;
$ curl -XPUT 'http://localhost:9200/x/'
{"ok":true,"acknowledged":true}
$ curl -XPUT 'http://localhost:9200/x2/'
{"ok":true,"acknowledged":true}
$ curl -XPOST 'http://localhost:9200/x/_gateway/snapshot'
{"ok":true,"_shards":{"total":5,"successful":4,"failed":0}}
$ curl -XPOST 'http://localhost:9200/x2/_gateway/snapshot'
{"ok":true,"_shards":{"total":5,"successful":5,"failed":0}}
$ curl -XDELETE 'http://localhost:9200/x/'
{"ok":true,"acknowledged":true}
&lt;/pre&gt;


After restarting and restoring from the gateway, x2 should have zero documents. After the DELETE request is sent, ES deletes the commit data for index x2:

&lt;pre&gt;
[2012-01-02 14:53:00,664][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/indices/x, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
...
[2012-01-02 14:53:00,954][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F4%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
&lt;/pre&gt;


The full logs for the DELETE request are below:

&lt;pre&gt;
[2012-01-02 14:53:00,405][DEBUG][cluster.service          ] [aws-e1b-8.xxxxxxxx.net] processing [delete-index [x]]: execute
[2012-01-02 14:53:00,405][INFO ][cluster.metadata         ] [aws-e1b-8.xxxxxxxx.net] [x] deleting index
[2012-01-02 14:53:00,409][DEBUG][cluster.service          ] [aws-e1b-8.xxxxxxxx.net] cluster state updated, version [220], source [delete-index [x]]
[2012-01-02 14:53:00,409][DEBUG][river.cluster            ] [aws-e1b-8.xxxxxxxx.net] processing [reroute_rivers_node_changed]: execute
[2012-01-02 14:53:00,409][DEBUG][river.cluster            ] [aws-e1b-8.xxxxxxxx.net] processing [reroute_rivers_node_changed]: no change in cluster_state
[2012-01-02 14:53:00,410][DEBUG][indices.cluster          ] [aws-e1b-8.xxxxxxxx.net] [x] deleting index
[2012-01-02 14:53:00,410][DEBUG][indices                  ] [aws-e1b-8.xxxxxxxx.net] deleting Index [x]
[2012-01-02 14:53:00,410][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [0]
[2012-01-02 14:53:00,410][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [1]
[2012-01-02 14:53:00,411][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [2]
[2012-01-02 14:53:00,411][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][0] state: [STARTED]-&gt;[CLOSED], reason [deleting index]
[2012-01-02 14:53:00,411][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][1] state: [STARTED]-&gt;[CLOSED], reason [deleting index]
[2012-01-02 14:53:00,411][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][2] state: [STARTED]-&gt;[CLOSED], reason [deleting index]
[2012-01-02 14:53:00,411][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/indices/x/1, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,412][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [3]
[2012-01-02 14:53:00,412][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [4]
[2012-01-02 14:53:00,412][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][3] state: [STARTED]-&gt;[CLOSED], reason [deleting index]
[2012-01-02 14:53:00,413][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/indices/x/0, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,413][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][4] state: [STARTED]-&gt;[CLOSED], reason [deleting index]
[2012-01-02 14:53:00,414][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/indices/x/2, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,415][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/indices/x/3, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,415][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/indices/x/4, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,432][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 1CF9DDBE56ACAE52
[2012-01-02 14:53:00,432][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][4]), total is [1gb] with [5] active shards, each shard set to [225.1mb]
[2012-01-02 14:53:00,437][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 5439E48D1C6C66E3
[2012-01-02 14:53:00,437][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F0%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,456][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: D5F6AEE4C87331BA
[2012-01-02 14:53:00,456][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F0%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,465][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 4C3580B5A6536906
[2012-01-02 14:53:00,465][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F3%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,480][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: E4C5A1CD3FCC0A51
[2012-01-02 14:53:00,480][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F2%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,483][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: BFE115230486752E
[2012-01-02 14:53:00,483][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][0]), total is [1gb] with [5] active shards, each shard set to [225.1mb]
[2012-01-02 14:53:00,494][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 9E9CE9A22BFF2209
[2012-01-02 14:53:00,494][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F3%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,500][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: ACAC04C51612FE27
[2012-01-02 14:53:00,500][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F2%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,517][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: EFBA0DA2BAB7D65E
[2012-01-02 14:53:00,517][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][2]), total is [1gb] with [5] active shards, each shard set to [225.1mb]
[2012-01-02 14:53:00,518][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: DA077754B6C41906
[2012-01-02 14:53:00,518][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][3]), total is [1gb] with [5] active shards, each shard set to [225.1mb]
[2012-01-02 14:53:00,626][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: C97B12628A245B67
[2012-01-02 14:53:00,626][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F1%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,643][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: C444D05B9C2526EC
[2012-01-02 14:53:00,643][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx%2F1%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,662][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: A2EC73D53CEF11B5
[2012-01-02 14:53:00,663][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][1]), total is [1gb] with [5] active shards, each shard set to [225.1mb]
[2012-01-02 14:53:00,664][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/indices/x, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,701][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 126AAAFACB6B9E52
[2012-01-02 14:53:00,701][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F0%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,719][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: AE11CA882DC3B5F2
[2012-01-02 14:53:00,720][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F0%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,747][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 611EB0181D55BE54
[2012-01-02 14:53:00,747][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F1%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,769][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 756ACDD05674B387
[2012-01-02 14:53:00,769][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F1%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,789][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: F840A4C6AC34606D
[2012-01-02 14:53:00,789][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F2%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,811][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: D7CDEDD29EBC7D61
[2012-01-02 14:53:00,812][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F2%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,833][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 9617334FDC66FB23
[2012-01-02 14:53:00,833][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F3%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,904][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: E7B2AC2C2DFE2ED6
[2012-01-02 14:53:00,904][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F3%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,926][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: A47D9D007502212A
[2012-01-02 14:53:00,926][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F4%2F__0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,954][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 38EC545C47C99D65
[2012-01-02 14:53:00,954][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Findices%2Fx2%2F4%2Fcommit-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:00,974][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: A709CE7D578AAF0B
[2012-01-02 14:53:00,981][DEBUG][gateway.s3               ] [aws-e1b-8.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@3d761403 ...
[2012-01-02 14:53:00,984][DEBUG][cluster.service          ] [aws-e1b-8.xxxxxxxx.net] processing [delete-index [x]]: done applying updated cluster_state
[2012-01-02 14:53:00,984][INFO ][com.amazonaws.request    ] Sending Request: PUT http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Fmetadata%2Fmetadata-2455 Headers: (Content-Length: 77420, Content-Type: application/octet-stream, ) 
[2012-01-02 14:53:01,057][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: BAD343B536EDE0F7
[2012-01-02 14:53:01,057][INFO ][com.amazonaws.request    ] Sending Request: GET http://xxxxxxxx-app-dev-es.s3.amazonaws.com / Parameters: (prefix: es-amalgam-20111025/metadata/, ) Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:01,105][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 34F9E73651ADB6CD
[2012-01-02 14:53:01,105][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /es-amalgam-20111025%2Fmetadata%2Fmetadata-2454 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2012-01-02 14:53:01,131][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 75732598971E7A5F
[2012-01-02 14:53:01,131][DEBUG][gateway.s3               ] [aws-e1b-8.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@3d761403, took 150ms
&lt;/pre&gt;


I can provide additional logs if needed. Thank you!
</description><key id="2701835">1582</key><summary>S3 blob storage gateway: deleting an index named x destroys data for any index with name beginning with x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alambert</reporter><labels><label>bug</label><label>v0.18.7</label></labels><created>2012-01-02T15:08:08Z</created><updated>2012-01-02T21:08:03Z</updated><resolved>2012-01-02T21:08:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-02T21:07:07Z" id="3333673">Thanks for spotting that!. I will push a fix to `0.18` branch, but also to the `elasticsearch-cloud-aws` repo. I will release a version of the `elasticsearch-cloud-aws` in an hour or so, so you don't have to wait for the next 0.18 version to come out (you can just install it from there, which will be the place to install plugins from with `0.19`).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename mispelled count package info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1581</link><project id="" key="" /><description>Hi Shay,

Really small change to rename `.../count/pacakge-info.java` to `.../count/package-info.java`, came across it while working on the validate stuff.
</description><key id="2696218">1581</key><summary>Rename mispelled count package info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2012-01-01T00:59:46Z</created><updated>2014-07-16T21:55:46Z</updated><resolved>2012-01-01T10:12:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-01T10:12:00Z" id="3324180">cheers!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date Histogram Facet: Improve time zone handling, add factor option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1580</link><project id="" key="" /><description>The current date histogram handling is incorrect when it comes to handing timezone. It applies the time zone when doing day level resolution and above (but still return the milliseconds offseted by "-" instead of "+"), and below it won't allow to get back the results based on a specific time zone. So, the logic changed a bit (and defaults):

A new `pre_zone` and `post_zone` parameters allowed, but default to `UTC`. When setting the current `time_zone`, it will set `pre_zone`. (backward comp. change!). The idea is that the value stored in ES (milliseconds since epoch in UTC) is applied the `pre_zone`, rounded, and then applied `post_zone`.

The new `factor` parameter allows to use date histogram on long fields that store different resolution than milliseconds. For example, a field that stores seconds since epoch can pass a factor of 1000 which will allow to still use the date histogram.
</description><key id="2695897">1580</key><summary>Date Histogram Facet: Improve time zone handling, add factor option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>bug</label><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-31T22:09:31Z</created><updated>2014-04-30T14:14:25Z</updated><resolved>2011-12-31T22:10:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/TimeZoneRounding.java</file><file>src/main/java/org/elasticsearch/search/facet/datehistogram/CountDateHistogramFacetCollector.java</file><file>src/main/java/org/elasticsearch/search/facet/datehistogram/DateHistogramFacetProcessor.java</file><file>src/main/java/org/elasticsearch/search/facet/datehistogram/ValueDateHistogramFacetCollector.java</file><file>src/main/java/org/elasticsearch/search/facet/datehistogram/ValueScriptDateHistogramFacetCollector.java</file><file>src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file><file>src/test/java/org/elasticsearch/test/unit/deps/joda/TimeZoneRoundingTests.java</file></files><comments><comment>Date Histogram Facet: Improve time zone handling, add factor option, closes #1580.</comment></comments></commit></commits></item><item><title>Scan Search: Improve performance while scrolling through it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1579</link><project id="" key="" /><description>The way scan search is implemented is by executing the query and collecting only "from" docs till "to". So, while doing scrolling, getting up to "from" can become more and more expensive (relatively). We can optimize that by marking the segments we already processed, and not proces them if we scroll again.
</description><key id="2694828">1579</key><summary>Scan Search: Improve performance while scrolling through it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-31T15:48:57Z</created><updated>2011-12-31T15:49:28Z</updated><resolved>2011-12-31T15:49:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>src/main/java/org/elasticsearch/search/scan/ScanContext.java</file><file>src/test/java/org/elasticsearch/test/integration/search/scan/SearchScanScrollingTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/scan/SearchScanTests.java</file></files><comments><comment>Scan Search: Improve performance while scrolling through it, closes #1579.</comment></comments></commit></commits></item><item><title>Allow 'ignore_unmapped' to process field mappings on each shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1578</link><project id="" key="" /><description>The 'ignore_unmapped' feature needs to poll the mapping on each shard rather than going with an all/nothing assumption for the field.

In reference to: https://groups.google.com/group/elasticsearch/browse_thread/thread/4b677ad9e88b1967
Related to issue 1558: https://github.com/elasticsearch/elasticsearch/issues/1558

Initial setup:
Fails with ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: 
url: kk,system,library/statustype/_search 
query: {"from":0,"size":999,"sort":{"order_no": {"order":"asc","ignore_unmapped":true}}} 
The "statustype" item type only exists in the system and library indexes but not in the kk index. 
The query executes against the kk index and either the system or library index BUT when all three are together the search fails. 
The title field is defined in the statustype item mapping and exists in all instances. 
Removing the sort of the query dsl causes the search to succeed. 
If I remove the "ignore_unmapped" part of the sort then I receive errors from the kk index but the search hits still come back for the other indexes... 
Preferably, the "ignore_unmapped" feature should allow all data to be returned but provide a cascaded sort feature whereby ignoring certain sort keys but still continuing with the next sort field (if available.) 
</description><key id="2690114">1578</key><summary>Allow 'ignore_unmapped' to process field mappings on each shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels><label>adoptme</label></labels><created>2011-12-30T17:11:34Z</created><updated>2014-08-01T16:38:49Z</updated><resolved>2014-08-01T16:38:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jakew1ll" created="2012-08-14T18:47:13Z" id="7736025">+1 please.

I have also gist'd an example of this behavior using 0.19.0 and Java 1.6 on RHEL : https://gist.github.com/3350526
</comment><comment author="bryangreen" created="2012-10-05T23:17:24Z" id="9192692">+1 -- this one is biting me again. I gotta find a way around this...
</comment><comment author="clintongormley" created="2014-07-08T13:04:36Z" id="48332761">Related to #808 
</comment><comment author="clintongormley" created="2014-08-01T16:38:49Z" id="50906329">The case for treating unmapped fields as missing (and so sorting as first or last) is solved by #7039.

I think this use case with unmapped fields "falling through" is a bit too specialised to be expressed with generic parameters, especially seeing different docs would have different numbers of sort values.

Seems like it would be best handled by a script instead, then you've got complete control over the logic, eg:

```
DELETE /_all 

PUT /t/t/1
{
  "foo": 1,
  "bar": 2
}

PUT /x/t/1
{
  "bar": 1
}

GET /_search
{
  "sort": [
    {
      "_script": {
        "order": "desc",
        "type": "number",
        "lang": "groovy",
        "script": "try { doc['foo'].value } catch(e) { doc['bar'].value }"
      }
    },
    {
      "bar": {
        "order": "desc",
        "unmapped_type": "long"
      }
    }
  ]
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighter: Add require_field_match (both global and per field) option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1577</link><project id="" key="" /><description>`require_field_match` can be set to `true` which will cause a field to be highlighted only if a query matched that field. `false` means that terms are highlighted on all requested fields regardless if the query matches specifically on them
</description><key id="2684146">1577</key><summary>Highlighter: Add require_field_match (both global and per field) option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-29T23:59:55Z</created><updated>2011-12-30T00:00:32Z</updated><resolved>2011-12-30T00:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file></files><comments><comment>Highlighter: Add require_field_match (both global and per field) option, closes #1577.</comment></comments></commit></commits></item><item><title>Fix failed test ClusterAndIndexReaderOnlyTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1576</link><project id="" key="" /><description>The fix passes all tests in test suite, except QuorumLocalGatewayTests and BlobStoreSmallBufferSizeFsIndexGatewayTests, which should be unrelated.

Please review the code.
</description><key id="2674192">1576</key><summary>Fix failed test ClusterAndIndexReaderOnlyTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edwardw</reporter><labels /><created>2011-12-28T19:52:48Z</created><updated>2014-06-14T21:24:47Z</updated><resolved>2011-12-28T20:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-28T20:15:12Z" id="3294633">thanks for the fix!, it actually exposes a problem where the cluster update settings API would hang, so I will fix that as well after pulling your change.
</comment><comment author="kimchy" created="2011-12-28T20:23:03Z" id="3294715">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Two different indexes with the same nested object name causing ArrayIndexOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1575</link><project id="" key="" /><description>If two different mappings are created within an index that both have a nested object with the same name an ArrayIndexOutOfBoundsException can occur during searches.  See this gist to reproduce: https://gist.github.com/1528898

The gist creates an index called main with two mappings alpha and bravo.  Alpha and bravo both have a nested object called charlies.  The gist indexes two alphas and a bravo.  Only the bravo has a nested document.  When searching for an alpha using a nested query an ArrayIndexOutOfBoundsException can occur.  This error only occurs if the alpha was indexed before the bravo.

This issue is related to the group discussion here: https://groups.google.com/group/elasticsearch/browse_thread/thread/486e7568f82c216a 

The stack trace for the exception:

```
org.elasticsearch.search.query.QueryPhaseExecutionException: [main][1]: query[filtered(ConstantScore(org.elasticsearch.common.lucene.search.AndFilter@91837b24))-&gt;FilterCacheFilterWrapper(_type:alpha)],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:221)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:238)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
    at org.apache.lucene.util.OpenBitSetIterator.advance(OpenBitSetIterator.java:166)
    at org.elasticsearch.common.lucene.docset.AndDocIdSet$AndDocIdSetIterator.advance(AndDocIdSet.java:127)
    at org.apache.lucene.search.DeletionAwareConstantScoreQuery$DeletionConstantScorer.advance(DeletionAwareConstantScoreQuery.java:91)
    at org.apache.lucene.search.FilteredQuery$2.score(FilteredQuery.java:157)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:199)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:445)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:426)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:342)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:330)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:217)
    ... 9 more
```
</description><key id="2673357">1575</key><summary>Two different indexes with the same nested object name causing ArrayIndexOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasongilman</reporter><labels><label>bug</label><label>v0.18.7</label><label>v0.19.0.RC1</label></labels><created>2011-12-28T18:12:08Z</created><updated>2011-12-28T20:20:19Z</updated><resolved>2011-12-28T20:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file></files><comments><comment>Two different indexes with the same nested object name causing ArrayIndexOutOfBoundsException, closes #1575.</comment></comments></commit></commits></item><item><title>Add query validation feature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1574</link><project id="" key="" /><description>Hi Shay,

This adds a query validation feature to ElasticSearch, allowing users to validate that potentially expensive queries have correct syntax without actually running them.

It defines a new REST endpoint, `_validate/query` that validates the query and returns a boolean.

Here is some example usage:

```
% curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elastic Search"
}'

% curl -XGET 'http://localhost:9200/twitter/_validate/query?q=user:foo'
{"valid":true,"_shards":{"total":1,"successful":1,"failed":0}}

% curl -XGET 'http://localhost:9200/twitter/tweet/_validate/query?q=post_date:foo'
{"valid":false,"_shards":{"total":1,"successful":1,"failed":0}}

% curl -XGET 'http://localhost:9200/twitter/_validate/query?q=us:er:foo'    
{"valid":false,"_shards":{"total":1,"successful":1,"failed":0}}
```

Or with a request body:

```
% curl -XGET 'http://localhost:9200/twitter/tweet/_validate/query' -d '{
  "query_string":{
    "query":"message:foo"
  }
}'
{"valid":true,"_shards":{"total":1,"successful":1,"failed":0}}

% curl -XGET 'http://localhost:9200/twitter/tweet/_validate/query' -d '{
  "filtered" : {
    "query" : {
      "query_string" : {
        "query" : "*:*"
      }
    },
    "filter" : {
      "term" : { "user" : "kimchy" }
    }
  }
}'
{"valid":true,"_shards":{"total":1,"successful":1,"failed":0}}
```
</description><key id="2666196">1574</key><summary>Add query validation feature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2011-12-27T20:56:27Z</created><updated>2014-06-28T11:40:06Z</updated><resolved>2011-12-29T12:32:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-27T21:39:41Z" id="3285503">Heya, a bit confused as to why we need it? I mean, the cost of validating (round trip) does not make sense compared to just executing it and letting it fail if its not valid as part of a search request. No?
</comment><comment author="dakrone" created="2011-12-27T21:59:41Z" id="3285816">If you are building a front end that uses ES on the back end and have any kind of user-configurable querying fields/filters, it is extremely nice to be able to tell the user immediately if they have a problem with their query syntax.

We allow a wide range of query operations from within in our front end, and it would be extremely nice to be able to tell the user constructing the query that it is invalid before actually running a query that could take minutes to return.
</comment><comment author="kimchy" created="2011-12-27T22:04:51Z" id="3285864">But if its an invalid query, it will not take minutes to run, it will fail fast...
</comment><comment author="dakrone" created="2011-12-27T22:07:00Z" id="3285881">True, however if you are creating queries to be executed at a later time and attempt to validate a query, it could end up taking minutes just to get back a result allowing you to determine that a query is valid.

Additionally, much more memory will be used if the actual query is executed while trying to validate it.
</comment><comment author="kimchy" created="2011-12-27T22:16:09Z" id="3285961">I see. But even if thats the case, wound't you want to show the results of the user constructed query to the user, to show him what that query matches. If we are talking about usability here, then just validating the query does not mean it is going to return what the user expects, no? If the query takes minutes to execute, the user should be aware of it, no? You can pass timeout flag to bound the time it takes to execute the query (it works ok'ish).

Also, are you going to expose the full ES query structure to the user? Usually, thats not the case, so you actually validate the query you are going to pass when constructing the ES equivalent query, no?

Last, regarding the implementation, there is a single action base class (for Transport) that you can use that will simplify things. Check the Analyze API for an example. You don't need to broadcast the request (even if its to a single shard).
</comment><comment author="dakrone" created="2011-12-28T16:35:45Z" id="3292425">&gt; If we are talking about usability here, then just validating the query does not mean it is going to return what the user expects, no?

We can validate the query to eliminate things that are easy to catch (like searching for a string when a date is required in the mapping).

&gt; If the query takes minutes to execute, the user should be aware of it, no? You can pass timeout flag to bound the time it takes to execute the query (it works ok'ish).

Quite a few of our queries take this long (and the users are aware and okay with it).

&gt; Also, are you going to expose the full ES query structure to the user?

No, we are exposing only a certain part of the query (including a querystring field) to the user that we build on in order to construct a query to send to ES.

&gt; Usually, thats not the case, so you actually validate the query you are going to pass when constructing the ES equivalent query, no?

We can perform a minimal amount of validation, however validating against the actual ES index and mapping is much more valuable to us.

&gt; Last, regarding the implementation, there is a single action base class (for Transport) that you can use that will simplify things. Check the Analyze API for an example. You don't need to broadcast the request (even if its to a single shard).

After checking out the Analyze API, it looks like that wouldn't allow us to broadcast the validation request to multiple indices (which is what we would like to do). I added a commit to prefer local shards for the broadcast operation, is there a better way to do the same request, but still allow us to broadcast it to all specified indices?
</comment><comment author="kimchy" created="2011-12-28T20:12:53Z" id="3294615">I see, ok, then lets work on getting it in. I suggest we move it under indices, and name it validate.query or ValidateQuery. The package should be indices.validate.query, and all the class name to include ValidateQuery and not just Validate (we might have other types of validations in the future). The API should be expose on IndicesAdminClient and be called `validateQuery` on not on Client.

Also, the validation operation should be done in the TransportAction implementation, no need to have IndexShard expose an API for that.
</comment><comment author="dakrone" created="2011-12-28T22:33:35Z" id="3295950">Hi Shay,
The changes you asked for have been committed, in addition the REST endpoint was slightly changed to `/_validate/query` so that different kinds of validation could be added in the future.

I've updated the initial request with the correct curl url also.
</comment><comment author="kimchy" created="2011-12-29T12:33:17Z" id="3300261">cool, pushed. I changed the routing value to be randomized.
</comment><comment author="dakrone" created="2012-08-06T16:53:18Z" id="7528917">-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 8/4/12 8:45 AM, Martijn Laarman wrote:

&gt; Not all languages/libs allow you to post data using GET's and it
&gt; seems POST isn't supported. Could this be updated to allow for
&gt; POST's too?

The validate feature already supports POST requests:

∴ curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
  "user" : "kimchy",
  "post_date" : "2009-11-15T14:12:12",
  "message" : "trying out Elastic Search"
}'
{"ok":true,"_index":"twitter","_type":"tweet","_id":"1","_version":1}

∴ curl -XPOST 'http://localhost:9200/twitter/tweet/_validate/query' -d '{
  "filtered" : {
    "query" : {
      "query_string" : {
        "query" : "_:_"
      }
    },
    "filter" : {
      "term" : { "user" : "kimchy" }
    }
  }
}'
{"valid":true,"_shards":{"total":1,"successful":1,"failed":0}}
- \- Lee Hinman
  -----BEGIN PGP SIGNATURE-----
  Version: GnuPG v1.4.12 (Darwin)
  Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iQIcBAEBAgAGBQJQH+LaAAoJEJ1kZdQ6zsrguYcQALdHknNqbc+qrC1h0Q1E25qR
Pvn4JgQD0OcojITgiNX7E5AIhP41jeHtG+i5aLNwEl/7deIYU77+apdSr9aHp78j
NG2dfx1WBIrFzNnx4iGfHv/jiO6bH+jRPd/A6YebrqKGetM74MTdWl0P+fqqHa2n
DlNMjU1WcSP3Yk/GOvw4OC0pf87AopWIzOSdV1gEH2RH5KWS1/JUxoOUFz1CrkWP
9slVDNbpWssDPpCh8fYAh7QQWCql4T6yJUYUL87bZ3rBcc+zDtJ5FgvVkZx2nz+Q
HR3iOo3XZaH05ibQfSLfVZx4qQCYqnLYUu0k2cOuCW6xQyVLRLXc0tNcikACCFAu
he8R2tSz3GIvvKAGfaKm1OaORwkDzHvwzkQCSxe5GStaFBahzPmzomEcWm8N+hYk
bWynnR4eb67pxeNobBg2AmnEKnAJq8+39PF3jo+zd2K+350AcqgG2vpVtsaY9cdn
BTsrys0T9DY0dLQiy2hgv/2yLLuSDChfM5av3JImdXCNzUcsUZnVy+A5gVtLDgM+
Q5BjjunH8a77al1kzyHsRo4fGdtCe3J3ocNRQSfQ1CV7FYXFU/O0Of1fQ5EgQFSQ
h7gDFSzQhJn32CIp1dOEDmhjE0GOyd2ZKWRMf84XRHtHhaYefLtmMvKc/2Id5+Kh
qiBSNRLKcpJ08/XT6ouH
=4Mpd
-----END PGP SIGNATURE-----
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set an index / indices to read only, or make the cluster read only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1573</link><project id="" key="" /><description>Allow to set a setting `index.blocks.read_only` on an index, and by setting it to `true`, will cause the index to become readonly (no index/delete operations allowed). The setting can be updated on an index / indices using the index update settings API.

Also, allow to make the cluster read only by setting `cluster.blocks.read_only` to true using the cluster update settings API.
</description><key id="2665257">1573</key><summary>Set an index / indices to read only, or make the cluster read only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2011-12-27T18:27:59Z</created><updated>2012-02-10T02:24:36Z</updated><resolved>2011-12-27T18:35:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-02-10T02:24:36Z" id="3900080">nice feature,really useful~
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>src/test/java/org/elasticsearch/test/integration/readonly/ClusterAndIndexReaderOnlyTests.java</file></files><comments><comment>Set an index / indices to read only, or make the cluster read only, closes #1573.</comment></comments></commit></commits></item><item><title>Deleting a missing index returns invalid json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1572</link><project id="" key="" /><description>Using a fresh and empty 18.6 elasticsearch, this:

```
 curl -XDELETE localhost:9200 http://localhost:9200/test/
```

returns

```
{"ok":true,"acknowledged":true}{"error":"IndexMissingException[[test] missing]","status":404}
```

Which is clearly invalid json.

I also see this when using older versions (oldest: 17.2).
</description><key id="2663870">1572</key><summary>Deleting a missing index returns invalid json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2011-12-27T15:11:28Z</created><updated>2011-12-28T07:40:23Z</updated><resolved>2011-12-28T07:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-27T18:40:42Z" id="3284015">The curl command you send is wrong. you end up sending two HTTP DELETE requests, one to `localhost:9200` and one to `http://localhost:9200/test/`. Just send one of them (the one without an index name deletes all indices).
</comment><comment author="skade" created="2011-12-28T07:40:23Z" id="3288801">Well, thats embarrassing, but kind of obvious. Thank you :).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL custom_filters_score nested groups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1571</link><project id="" key="" /><description>For issue #1561, add nested aggregation groups to custom_filters_score.
 Maintained backwards compatibility for the query DSL JSON and the same
JSON is generated when no nested groups are used, and extra JSON only
when nesting is used at one or more levels.  Test cases updated to
improve the testing of both this query class and for the new changes as
well.  Needs feedback, heavy changes and not sure the desired way of
structuring the nesting is within the parser, query builders, etc.  It
feels clunky at the moment.
</description><key id="2660600">1571</key><summary>Query DSL custom_filters_score nested groups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2011-12-27T00:29:49Z</created><updated>2014-06-15T03:47:28Z</updated><resolved>2014-04-07T10:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2011-12-27T00:45:22Z" id="3277799">I wonder if arbitrary nesting levels is too much, and not really needed.  THe use cases I started with were one level of nesting so you could take the highest boost of a group, the lowest (as a de-boost) of another, and multiply the two.  The code supports any amount of nesting but adds complexity throughout (groups containing groups).  But then again, it doesn't save that much since even one level of nesting does introduce a fair amount of code.
</comment><comment author="kimchy" created="2011-12-27T14:14:48Z" id="3281634">I have created a patch (being git lazy...) based on your codebase that has some modifications: https://gist.github.com/1523762.
1. It simplifies `CustomFiltersScoreQueryParser`.
2. It simplified `FiltersFunctionScoreQuery` and has a change to compute factor, and apply it to the sub query score last. This allows to support multiply properly (I went ahead and did that because I was refactoring the code while I was at it). It also fixes a bug in calling function nextReader and removes the need for a map with docsets.

Leftovers:
1. Fix the test for groups because of the change to factor instead of scoring on each one.
2. Continue to refactor FiltersFunctionScoreQuery and apply the same logic as done to actual scoring to explanation. Also, rename `ScoreFunction#explain` to `ScoreFunction#scoreExplain`, and add `ScoreFunction#factorExplain`, use factorExaplin in our case here.
3. Apply same refactoring (OO over if/instanceof) to the query builder.

I suggest once its done, squash all commits to a single one and have a fresh pull request.
</comment><comment author="apatrida" created="2012-07-15T20:56:56Z" id="6994913">Oh, didn't see this response.  I'll pick it up and see about the leftovers...
</comment><comment author="javanna" created="2014-04-07T10:57:25Z" id="39717330">I think we can close this one being the custom filters score and friends all deprecated in favor of the function score query.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: Support partial fields that can returns partial view of the _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1570</link><project id="" key="" /><description>Support `partial_fields` that can return a partial representation of `_source` based on `include` and `exclude` patterns. For example, include only data starts with `obj1.obj2`:

```
{
    "query" : {
        "match_all" : {}
    },
    "partial_fields" : {
        "partial1" : {
            "include" : "obj1.obj2.*",
        }
    }
}
```

And one that will also exclude `obj1.obj3`:

```
{
    "query" : {
        "match_all" : {}
    },
    "partial_fields" : {
        "partial1" : {
            "include" : "obj1.obj2.*",
            "exclude" : "obj1.obj3.*"
        }
    }
}
```

Both `include` and `exclude` supports multiple patterns: 

```
{
    "query" : {
        "match_all" : {}
    },
    "partial_fields" : {
        "partial1" : {
            "include" : ["obj1.obj2.*", "obj1.obj4.*"],
            "exclude" : "obj1.obj3.*"
        }
    }
}    
```
</description><key id="2658494">1570</key><summary>Search: Support partial fields that can returns partial view of the _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2011-12-26T14:49:28Z</created><updated>2011-12-26T14:50:10Z</updated><resolved>2011-12-26T14:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/client/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/main/java/org/elasticsearch/search/SearchHitField.java</file><file>src/main/java/org/elasticsearch/search/SearchModule.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/partial/PartialFieldsContext.java</file><file>src/main/java/org/elasticsearch/search/fetch/partial/PartialFieldsFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/partial/PartialFieldsParseElement.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/lookup/SourceLookup.java</file><file>src/test/java/org/elasticsearch/test/integration/search/fields/SearchFieldsTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/scriptfield/ScriptFieldSearchTests.java</file><file>src/test/java/org/elasticsearch/test/unit/common/xcontent/support/XContentMapValuesTests.java</file></files><comments><comment>Search: Support partial fields that can returns partial view of the _source, closes #1570.</comment><comment>great.</comment></comments></commit></commits></item><item><title>Java API: ScriptSortBuilder does not pass the lang used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1569</link><project id="" key="" /><description /><key id="2655410">1569</key><summary>Java API: ScriptSortBuilder does not pass the lang used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.7</label><label>v0.19.0.RC1</label></labels><created>2011-12-25T16:37:24Z</created><updated>2011-12-25T16:37:51Z</updated><resolved>2011-12-25T16:37:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java</file></files><comments><comment>Java API: ScriptSortBuilder does not pass the lang used, closes #1569.</comment></comments></commit></commits></item><item><title>Query DSL: Add different execution models for terms filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1568</link><project id="" key="" /><description>The way `terms` filter currently executes is by iterating over the terms provided and finding matches docs (loading into a bitset) and caching it. Sometimes, we want a different execution model that can still be achieved by building more complex queries in the DSL, but would be nice to support them in the more compact model that `terms` filter provides.

The `execution` option now has the following options :
- `plain`: The default. Works as today. Iterates over all the terms, building a bit set matching it, and filtering. The total filter is cached (keyed by the terms).
- `bool`: Builds a `bool` filter wrapping each term in a `term` filter that is cached. The bool filter is optimized in this case since it bitwise or's the different term filter bitsets. The total filter is not cached by default in this case.
- `and`: Builds an `and` filter wrapping each term in `term` filter that is cached. The total filter is not cached by default in this case. Most times, `bool` should be used as its faster thanks to its bitwise execution.

The "total" terms filter caching can still be explicitly controlled using the `_cache` option. Note the default value for it depends on the `execution` value.
</description><key id="2650709">1568</key><summary>Query DSL: Add different execution models for terms filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-23T22:29:38Z</created><updated>2014-09-12T10:04:45Z</updated><resolved>2011-12-23T22:30:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2011-12-27T07:15:51Z" id="3279662">hi,shay,
looks great,any examples for this feature?
</comment><comment author="kimchy" created="2011-12-27T09:36:55Z" id="3280217">You mean how it looks like? Something like this:

```
{
    "terms" : { "field_name" : ["value1", "value2"], "execution" : "bool" }
}
```
</comment><comment author="medcl" created="2011-12-27T15:20:54Z" id="3282144">thanks.

------------------ Original ------------------
From: "Shay Banon"; 
Date: 20111227(ڶ) 5:36
To: "Medcl"; 
Subject: Re: [elasticsearch] Query DSL: Add different execution models for terms filter (#1568)

You mean how it looks like? Something like this:

```
{
    "terms" : { "field_name" : ["value1", "value2"], "execution" : "bool" }
}
```

---

Reply to this email directly or view it on GitHub:
https://github.com/elasticsearch/elasticsearch/issues/1568#issuecomment-3280217
</comment><comment author="AndreKR" created="2014-09-04T13:13:33Z" id="54473052">What if I have a field named "execution"?
</comment><comment author="clintongormley" created="2014-09-06T16:57:19Z" id="54720483">@AndreKR good point, i've opened #7629 
</comment><comment author="svnv" created="2014-09-12T09:36:35Z" id="55380947">Does `bool` use `must` or `should` for the terms?
</comment><comment author="AndreKR" created="2014-09-12T10:04:45Z" id="55383422">`should`, see https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/TermsFilterParser.java#L235
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file></files><comments><comment>Query DSL: Add different execution models for terms filter, closes #1568.</comment></comments></commit></commits></item><item><title>Date histogram facets do not adjust `time_zone`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1567</link><project id="" key="" /><description>ElasticSearch 0.18.5 appears to be ignoring the `time_zone` attribute for `date_histogram` facets.  This gist demonstrates the issue: https://gist.github.com/1510658

The google group thread for this issue is https://groups.google.com/forum/?hl=en#!searchin/elasticsearch/allan$20caffee/elasticsearch/H5jvJ7pqElI/EB7I7AgaGwAJ
</description><key id="2647113">1567</key><summary>Date histogram facets do not adjust `time_zone`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">allancaffee</reporter><labels /><created>2011-12-23T13:23:08Z</created><updated>2014-07-08T12:47:16Z</updated><resolved>2014-07-08T12:47:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-23T20:57:47Z" id="3264658">Just to complete the picture, it works well for day level interval, just not minute level (and probably hour as well).
</comment><comment author="kimchy" created="2011-12-29T14:14:49Z" id="3300928">Hey, so, I chased this one down, and basically, the documentation is wrong. What you get back is always UTC, even with things like day interval. It will round things properly based on the time zone though, so it will put it in the correct bucket based on the time zone offset.
</comment><comment author="kimchy" created="2011-12-29T17:47:04Z" id="3303059">So, still with it :). It is strange, since the behavior is not consistent between day level rounding (and up), and rounding of things like hour or minute. I can see why it happens in the date library I use, still need to figure out how make it work...

Side: The problem is that the UTC millis value is converted to the "local" time, which is to say applying the timezone offset. Then, it is rounded. With day, it is rounded to the beginning of the day, and then the offset it applied again to bring it back to UTC. With minute level resolution, flooring does not change much, so it remins the same...
</comment><comment author="kimchy" created="2011-12-31T22:10:43Z" id="3323065">Ok, pushed #1580, can you take it for a spin see if it works for you?
</comment><comment author="allancaffee" created="2012-01-03T16:58:34Z" id="3342623">I've tried this with "pre_zone" and "post_zone" and it seems to work perfectly.  Although it seems like the naming of "pre_zone" and "post_zone" are a little confusing.  Would these be clearer as "bucketing_time_zone" and "return_time_zone" or something along those lines?

Thanks for the speedy turnaround!
</comment><comment author="kimchy" created="2012-01-03T18:53:02Z" id="3344263">Yea, those were the names I used but was not too happy with... . Actually, bucketing is done after pre and post are applied, the idea was pre rounding, and post rounding.
</comment><comment author="allancaffee" created="2012-01-05T01:29:51Z" id="3364311">Well is there even a use case for the `post_zone` being different from the `pre_zone` (other than returning them in UTC)?  Would it make more sense to present it as `time_zone` and a boolean field `local_time` or some such thing?
</comment><comment author="kimchy" created="2012-01-05T12:58:38Z" id="3369413">I think there is, especially with charting systems that do not support timezone properly, so you need to hack the timezone info either into the local one or the UTC one, which might require different time zones.
</comment><comment author="kimchy" created="2012-01-09T19:10:20Z" id="3418356">closing this one then, thanks for bringing it up, fixed in #1580.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>deleteBy queries failing to parse has_child filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1566</link><project id="" key="" /><description>If I run this script:

``` sh
curl -XDELETE 'http://localhost:9200/test'
curl -XPUT 'http://localhost:9200/test' -d '
{
  "mappings" : {
    "child" : {
      "_parent" : {
        "type" : "parent"
      }
    }
  }
}
'
curl -XPUT 'http://localhost:9200/test/parent/1' -d '{"value" : "1"}'
curl -XPUT 'http://localhost:9200/test/child/1?parent=1' -d '{"value" : "1"}'
curl -XDELETE 'http://localhost:9200/test/parent/_query' -d '
{
  "filtered": {
    "filter": {
      "has_child": {
        "query": {
          "match_all": {}
        },
        "type": "child"
      }
    },
    "query": {
      "term": {
        "value": "1"
      }
    }
  }
}
'
```

this output appears:

``` log
[2011-12-22 17:45:04,780][INFO ][cluster.metadata         ] [Glorian] [test] deleting index
[2011-12-22 17:45:05,984][INFO ][cluster.metadata         ] [Glorian] [test] creating index, cause [api], shards [1]/[0], mappings [child]
[2011-12-22 17:45:07,147][INFO ][cluster.metadata         ] [Glorian] [test] update_mapping [parent] (dynamic)
[2011-12-22 17:45:08,326][INFO ][cluster.metadata         ] [Glorian] [test] update_mapping [child] (dynamic)
[2011-12-22 17:45:09,430][DEBUG][action.deletebyquery     ] [Glorian] [test][0], node[bw5_Dif4Q2-8k1w5VefBSg], [P], s[STARTED]: Failed to execute [delete_by_query {[test][parent], query [
{
  "filtered": {
    "filter": {
      "has_child": {
        "query": {
          "match_all": {}
        },
        "type": "child"
      }
    },
    "query": {
      "term": {
        "value": "1"
      }
    }
  }
}
]}]
org.elasticsearch.index.query.QueryParsingException: [test] Failed to parse
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:186)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:175)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:350)
    at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnPrimary(TransportShardDeleteByQueryAction.java:80)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:487)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:400)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.query.HasChildFilterParser.parse(HasChildFilterParser.java:100)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:202)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:65)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:176)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:232)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:182)
    ... 8 more
```

I believe that delete-by query is valid though because the Search API validates and runs it fine. I'm using elasticsearch 0.18.6. 
</description><key id="2642912">1566</key><summary>deleteBy queries failing to parse has_child filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hlian</reporter><labels /><created>2011-12-22T22:49:23Z</created><updated>2013-10-09T08:48:40Z</updated><resolved>2013-10-09T08:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-22T23:27:24Z" id="3256276">Yes, has_child is not (yet) supported in delete_by_query.
</comment><comment author="martijnvg" created="2013-10-09T08:48:40Z" id="25955418">Implemented via #3822
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>unit tests for issue #1560, customfiltersscore min and multiply scoremode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1565</link><project id="" key="" /><description>Forgot the unit tests previously.  Want these committed before starting on issue #1561
</description><key id="2641569">1565</key><summary>unit tests for issue #1560, customfiltersscore min and multiply scoremode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2011-12-22T20:37:30Z</created><updated>2014-07-16T21:55:48Z</updated><resolved>2011-12-22T21:24:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-22T21:24:22Z" id="3255050">cheers, pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster metadata files destroyed when using blob store gateway causing data loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1564</link><project id="" key="" /><description>I am using ElasticSearch v0.18.5 with the S3 shared storage gateway. Last night, I restarted my single-node cluster; when it came back up, all indexes were lost (no shards or metadata.)

The debug logs are below. ES shuts down at 21:36:16,783 and starts up at 21:36:45,830. Both before and after shutdown, there are multiple threads running Gateway#write()(elasticsearch/modules/elasticsearch/src/main/java/org/elasticsearch/gateway/shared/SharedStorageGateway.java). In the blob storage implementation (elasticsearch/modules/elasticsearch/src/main/java/org/elasticsearch/gateway/blobstore/BlobStoreGateway.java), after the metadata write is complete, all metadata files other than the one just written are deleted. Before the crash, the threads each write their own metadata file and then delete the others', which causes the cluster to start up with no metadata after the crash (causing index loss.) After the crash, the two threads write to the same metadata file.

&lt;pre&gt;
root@aws-e1b-12:/md/elasticsearch/app/elasticsearch-0.18.5/logs# egrep "(to gateway|metadata%2F|metadata found|stopping|initializing)" dev.log.2011-12-21 | grep " 21:3" | head -26
[2011-12-21 21:34:13,468][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@2547c6ef ...
[2011-12-21 21:34:13,471][INFO ][com.amazonaws.request    ] Sending Request: PUT http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-914 Headers: (Content-Length: 109057, Content-Type: application/octet-stream, ) 
[2011-12-21 21:34:13,636][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@ab78020 ...
[2011-12-21 21:34:13,641][INFO ][com.amazonaws.request    ] Sending Request: PUT http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-914 Headers: (Content-Length: 109223, Content-Type: application/octet-stream, ) 
[2011-12-21 21:34:13,747][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-913 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2011-12-21 21:34:13,769][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@ab78020, took 133ms
[2011-12-21 21:34:13,854][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@5b8b8615 ...
[2011-12-21 21:34:13,857][INFO ][com.amazonaws.request    ] Sending Request: PUT http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-916 Headers: (Content-Length: 111257, Content-Type: application/octet-stream, ) 
[2011-12-21 21:34:14,026][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-914 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2011-12-21 21:34:14,045][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@5b8b8615, took 191ms
[2011-12-21 21:34:15,451][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-916 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2011-12-21 21:34:15,472][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@2547c6ef, took 2s
[2011-12-21 21:36:16,783][INFO ][node                     ] [aws-e1b-12.xxxxxxxx.net] {0.18.5}[2514]: stopping ...
[2011-12-21 21:36:45,830][INFO ][node                     ] [aws-e1b-12.xxxxxxxx.net] {0.18.5}[30336]: initializing ...
[2011-12-21 21:36:49,640][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] Latest metadata found at index [-1]
[2011-12-21 21:36:53,510][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@2ef9748f ...
[2011-12-21 21:36:53,653][INFO ][com.amazonaws.request    ] Sending Request: PUT http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-0 Headers: (Content-Length: 43, Content-Type: application/octet-stream, ) 
[2011-12-21 21:36:53,776][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@2ef9748f, took 266ms
[2011-12-21 21:37:37,034][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@161e14f0 ...
[2011-12-21 21:37:37,084][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@b5a191e ...
[2011-12-21 21:37:37,775][INFO ][com.amazonaws.request    ] Sending Request: PUT http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-1 Headers: (Content-Length: 210, Content-Type: application/octet-stream, ) 
[2011-12-21 21:37:37,781][INFO ][com.amazonaws.request    ] Sending Request: PUT http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-1 Headers: (Content-Length: 376, Content-Type: application/octet-stream, ) 
[2011-12-21 21:37:37,861][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2011-12-21 21:37:37,872][INFO ][com.amazonaws.request    ] Sending Request: DELETE http://xxxxxxxx-app-dev-es.s3.amazonaws.com /dev%2Fmetadata%2Fmetadata-0 Headers: (Content-Type: application/x-www-form-urlencoded; charset=utf-8, ) 
[2011-12-21 21:37:37,878][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@161e14f0, took 844ms
[2011-12-21 21:37:37,887][DEBUG][gateway.s3               ] [aws-e1b-12.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@b5a191e, took 802ms
root@aws-e1b-12:/md/elasticsearch/app/elasticsearch-0.18.5/logs# 
&lt;/pre&gt;


I can provide additional logs if needed. Thank you!
</description><key id="2639564">1564</key><summary>Cluster metadata files destroyed when using blob store gateway causing data loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alambert</reporter><labels><label>bug</label><label>v0.18.7</label><label>v0.19.0.RC1</label></labels><created>2011-12-22T17:29:00Z</created><updated>2011-12-22T19:44:47Z</updated><resolved>2011-12-22T18:42:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-22T18:32:02Z" id="3252549">Yes, you are right, I will push a fix... (nice catch!).
</comment><comment author="kimchy" created="2011-12-22T18:57:57Z" id="3253215">also, backported to 0.18 branch, if 0.18.7 is released, it will include it.
</comment><comment author="alambert" created="2011-12-22T19:44:47Z" id="3253855">Thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/shared/SharedStorageGateway.java</file></files><comments><comment>Cluster metadata files destroyed when using blob store gateway causing data loss, closes #1564,</comment></comments></commit></commits></item><item><title>Node Stats rest api returns open_file_descriptors: -1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1563</link><project id="" key="" /><description>_cluster/nodes/stats :
nodes.nodename.process.open_file_descriptors: -1

ES version 0.18.4
Operations System Windows Xp 32 bit
</description><key id="2635661">1563</key><summary>Node Stats rest api returns open_file_descriptors: -1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">ifountain</reporter><labels /><created>2011-12-22T10:02:28Z</created><updated>2015-01-28T15:33:21Z</updated><resolved>2015-01-28T15:33:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-22T12:04:40Z" id="3248319">Sadly, its probably not available on windows xp, might be available on later releases of windows, not sure...
</comment><comment author="ifountain" created="2011-12-23T08:29:11Z" id="3258987">Using sigar.getProcFd(pid).getTotal() to get file descriptor count works on windows xp.
Elasticsearch gets file descriptor count from jvm is there a special reason for that ? . 
</comment><comment author="kimchy" created="2011-12-23T20:52:30Z" id="3264616">Yea, I had bad experience with sigar and getting proper counts for file descriptors, and getting the count from the JVM means that even if sigar is not available, we can still get it (on some platforms). We can possibly revert to sigar if the VM is not returning the correct number.
</comment><comment author="ifountain" created="2011-12-26T08:12:32Z" id="3273308">Thanks for the information. Idea of getting from sigar if VM does not return correctly is also a very good approach. 
</comment><comment author="spinscale" created="2013-07-05T10:31:12Z" id="20512385">@ifountain do you know if this is still broken on current windows operating systems? I dont feel to obligated to support windows XP anymore to be honest :-)

Thanks for your help!
</comment><comment author="ifountain" created="2013-07-11T12:59:01Z" id="20808948">@spinscale We are using ES 0.19.0 and file descriptor is -1 on both Windows XP 32 &amp; 64 bit and Windows Server 2008 R2 Standard 64 bit ( VM ) .
</comment><comment author="colings86" created="2014-07-18T12:01:17Z" id="49423102">Reproduced on 1.2.2
</comment><comment author="clintongormley" created="2014-11-08T18:52:28Z" id="62270655">@Mpdreamz could you take a look at this and see if there is anything we can do to fix this please?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/process/SigarProcessProbe.java</file></files><comments><comment>Nodes Stats: Fix open file descriptors count on Windows</comment></comments></commit></commits></item><item><title>Query DSL: custom_filters_score allow score_mode ... closes #1560</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1562</link><project id="" key="" /><description>Added both "min" and "multiply" modes
</description><key id="2635059">1562</key><summary>Query DSL: custom_filters_score allow score_mode ... closes #1560</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2011-12-22T08:22:05Z</created><updated>2014-07-16T21:55:48Z</updated><resolved>2011-12-22T12:08:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-22T12:08:55Z" id="3248343">Pushed, thanks!
</comment><comment author="apatrida" created="2011-12-22T17:20:27Z" id="3251662">I should also then update the docs on the website in the web project?  Sí?
https://github.com/elasticsearch/elasticsearch.github.com

On 2011-12-22, at 4:08 AM, Shay Banon wrote:

&gt; Pushed, thanks!
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/1562#issuecomment-3248343
</comment><comment author="apatrida" created="2011-12-22T17:54:27Z" id="3252106">(docs updated, doing pull request)
</comment><comment author="kimchy" created="2011-12-22T18:23:11Z" id="3252430">Usually I go over the docs and update them once a release is out. So a pull request would be cool, and it can be pulled once we release 0.19.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: custom_filters_score allow nesting of boosts with operators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1561</link><project id="" key="" /><description>In relevancy cases there are things like:
- given a starting query
- for situation A and B, multiply score by the highest of A or B producing new score
- for situation C, D and E multiply score by the highest of those producing a new score
- for situation F and G, discount score by the worst discount producing a new score

So you have groups of queries.  Which right now you have to nest custom_filters_score to accomplish (or combine with boosts DSL for the negative and still nest queries).

In the custom_filters_score DSL would be nice to nest the filters part in groups, using the mode as the grouping.  These should be grouped with operators "multiply", "max", "min", "sum" or "total", "avg"...

``` json
{
    "custom_filters_score" : {
        "query" : {
            "match_all" : {}
        },
        "filters" : {
           "multiply" : [
              {
                "max" : [
                    {
                        "filter" : { "term" : { "myfield" : "datavalue" } },
                        "boost" : 3
                    },
                    {
                        "filter" : { "term" : { "myfield_alt" : "differentvalue" } },
                        "boost" : 2
                    }
                ]
             },
             {
                "min" : [
                    {
                        "filter" : { "term" : { "danger" : "badValue" } },
                        "boost" : 0.5
                    },
                    {
                        "filter" : { "term" : { "badfield" : "horrible" } },
                        "boost" : 0.1
                    }
                 ]
             }   
          ]        
        }
    }
}
```
</description><key id="2634804">1561</key><summary>Query DSL: custom_filters_score allow nesting of boosts with operators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2011-12-22T07:18:36Z</created><updated>2014-05-23T14:20:29Z</updated><resolved>2014-05-23T14:20:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2011-12-22T07:19:35Z" id="3246262">note:  nesting levels should be arbitrarily deep.
</comment><comment author="apatrida" created="2011-12-22T17:23:57Z" id="3251703">I'm working on this one now...
</comment><comment author="clintongormley" created="2014-05-23T14:20:29Z" id="44014554">No progress in two years, and custom_filters_score no longer exists.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: custom_filters_score allow score_mode for "min" and "multiply"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1560</link><project id="" key="" /><description>Add `min` and `multiply` as score modes for `custom_filters_score`.

Would be nice to use custom_filters_score for a negative boost as well, so having a "min" score to pick the worst boost is sometimes desirable.  You can fake it with "first" by ordering them from lowest to highest, but would be easier for people to comprehend as "min" (you can also use "first" to cause the same as "max" but we don't force people to do that either).

Assuming this works with fractional boost, which I'm assuming it does since it is multiplicative.
</description><key id="2634763">1560</key><summary>Query DSL: custom_filters_score allow score_mode for "min" and "multiply"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-22T07:08:52Z</created><updated>2011-12-26T22:38:10Z</updated><resolved>2011-12-22T12:08:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2011-12-22T08:18:58Z" id="3246593">Also "multiply" as a score_mode to multiply the values against each other.
</comment><comment author="apatrida" created="2011-12-26T22:06:31Z" id="3277166">Working on another issue, but need clarification first.  What is the intended result when you have a custom_filters_score and you total or average the items.  Does it do this:
- for all matching filters, and on each doc, calculate the aggregate boost
- then apply resulting aggregate boost multiplied times document score

Or does it do this:
- for all matching filters, and on each doc, calculate the boost \* the document score
- then aggregate those calculations.

For total it doesn't matter as the net result is the same when min, max, total and average are used since the math works out that this doesn't matter.   For example, these don't care:

FIRST:  B1 \* score
MIN:  B2 \* score 
MAX:  B3 \* score
AVG:  (B1 + B2 + B3)/3 \* score == (B1_score + B2_score + B3_score)/3
TOTAL:  (B1 + B2 + B3) \* score == B1_score + B2_score + B3_score

but, MULTIPLY would care.  Is it:

B1 \* B2 \* B3 \* score

or is it:

B1_score \* B2_score \* B3*score

The first answer seems right, so that you can do things like discount 10% for something using boost of 0.9 and discount further by 20% using another 0.8.

Think I need to patch my previous #1560 patch to change how multiple works.  I think have a new patch now for #1561 and I can include it there, but in case that more complicated patch doesn't happen soon I should do this one on its own.  Advice?
</comment><comment author="apatrida" created="2011-12-26T22:24:45Z" id="3277267">So multiply doesn't make sense in this context without a change to take the subquery score out of the picture until the end.  Might be sane to do that for all of the types, then multiply it in after.  Would clean up the explain plan a lot as well to do this.  Leaving Multiply broken while finishing patch for #1561 then will come back and fix this after so that I'm not on two conflicting versions of the same code.
</comment><comment author="kimchy" created="2011-12-26T22:25:52Z" id="3277269">I agree that the first option feels better, but its trickier to implement because of how `ScoreFunction` works. If you have a nice solution for it, then have a pull request only for it. #1561 is a different case and its good to separate the two.
</comment><comment author="apatrida" created="2011-12-26T22:38:10Z" id="3277322">doing #1561 first, I'm 95% done there and want to get it out of the way.  It'll need some feedback, as it is tricky as well, and causing more work and more code.  I don't know how things fit into your world yet (I am about to be trained by your feedback)...  It covers issues with query parsing and builders not present in the codebase so far (groups that aren't real queries or filters themselves).  So leaving this one for a bit and coming back to it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryParser.java</file></files><comments><comment>Query DSL: custom_filters_score allow score_mode for "min" and "multiply", closes #1560</comment></comments></commit></commits></item><item><title>Improve multi field mapper with highlighting based on source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1559</link><project id="" key="" /><description>When wanting to highlight a multi field mapper inner mapping, for example, field.xxx, and using source to extract the data, improve the extraction of the content from source based on `field` (as `field.xxx` is meaningless). It kindda work now (0.18.x) but not by design....
</description><key id="2631483">1559</key><summary>Improve multi field mapper with highlighting based on source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-22T00:24:07Z</created><updated>2011-12-22T00:24:48Z</updated><resolved>2011-12-22T00:24:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/ContentPath.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/multifield/MultiFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceScoreOrderFragmentsBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceSimpleFragmentsBuilder.java</file><file>src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Improve multi field mapper with highlighting based on source, closes #1559.</comment></comments></commit></commits></item><item><title>Allow search to continue when sort field is missing from type mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1558</link><project id="" key="" /><description>I'd like the search to ignore the sort key when it is missing from a type's mapping.

This could be an option for the sort key such as "ignore_unmapped": true

For instance:

```
{
    "from": 0,
    "size": "15",
    "sort": {
        "title": {
            "order": "asc",
            "ignore_unmapped": true
        }
    }
}
```

I have this error throwing on some indexes and sometimes throws the search. This would be a great feature! 

Updated references to "ignore_unmapped"
</description><key id="2630819">1558</key><summary>Allow search to continue when sort field is missing from type mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-21T23:02:45Z</created><updated>2014-05-15T06:55:10Z</updated><resolved>2011-12-22T12:26:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file></files><comments><comment>Allow search to continue when sort field is missing from type mapping, closes #1558.</comment></comments></commit></commits></item><item><title>Support passing multiple fields to a "text" query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1557</link><project id="" key="" /><description>Per our discussion, something like `text : { query : "query here", fields: ["first", "second^5"] }` would be awesome
</description><key id="2630401">1557</key><summary>Support passing multiple fields to a "text" query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brianmario</reporter><labels /><created>2011-12-21T22:17:02Z</created><updated>2012-08-13T11:33:09Z</updated><resolved>2012-08-13T11:33:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-13T11:33:09Z" id="7689333">Implemented by #2153.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Boolean Type: Consider `F` as false as well when searching on it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1556</link><project id="" key="" /><description>Because we end up indexing `F` for false and `T` for true, consider `F` as false when searching using it (for example, using term filter).
</description><key id="2630126">1556</key><summary>Boolean Type: Consider `F` as false as well when searching on it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-21T21:47:48Z</created><updated>2011-12-21T21:48:28Z</updated><resolved>2011-12-21T21:48:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file></files><comments><comment>Boolean Type: Consider `F` as false as well when searching on it, closes #1556.</comment></comments></commit></commits></item><item><title>Analyze API: Allow to execute it without pre-creating an index, and allow to build custom analyzer (tokenizer + token_filters)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1555</link><project id="" key="" /><description>Allow to execute a the `_analyze` API without needing to specify an index. `/_analyze` endpoint (note, it won't work without an index if a field is specified).

Also, allow to pass `tokenizer`, and an optional comma separater list of token filters, to "dynamically" build a custom analyzer that will be used to analyze the data.
</description><key id="2629860">1555</key><summary>Analyze API: Allow to execute it without pre-creating an index, and allow to build custom analyzer (tokenizer + token_filters)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-21T21:24:33Z</created><updated>2011-12-21T21:25:07Z</updated><resolved>2011-12-21T21:25:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/TransportSingleCustomOperationAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/action/admin/indices/analyze/AnalyzeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>src/test/java/org/elasticsearch/test/integration/indices/analyze/AnalyzeActionTests.java</file></files><comments><comment>Analyze API: Allow to execute it without pre-creating an index, and allow to build custom analyzer (tokenizer + token_filters), closes #1555.</comment></comments></commit></commits></item><item><title>Create pidfile in the foreground mode in `bin/elasticsearch`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1554</link><project id="" key="" /><description>If option `-p` specified in foreground mode create the pidfile, so that things like supervisor can take control the process.
</description><key id="2621462">1554</key><summary>Create pidfile in the foreground mode in `bin/elasticsearch`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lsm</reporter><labels /><created>2011-12-21T04:04:26Z</created><updated>2011-12-21T08:27:33Z</updated><resolved>2011-12-21T08:27:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-21T08:27:33Z" id="3231009">See #1553.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Generate pid file even when running in foreground mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1553</link><project id="" key="" /><description>Move the generation of the pidfile into the elasticsearch proces itself, allowing us to also generate it when running in foreground mode. Also, streamline properties: `es.pidfile` and `es.foregroud` (old ones also supported).
</description><key id="2621268">1553</key><summary>Generate pid file even when running in foreground mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-21T03:28:08Z</created><updated>2011-12-21T08:30:18Z</updated><resolved>2011-12-21T03:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lsm" created="2011-12-21T08:30:18Z" id="3231035">thanks for the quick fix
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/bootstrap/ElasticSearchF.java</file></files><comments><comment>Generate pid file even when running in foreground mode, closes #1553.</comment></comments></commit></commits></item><item><title>Analysis: Add phonetic encodder called `bm` or `beider_morse`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1552</link><project id="" key="" /><description>Add another encoder on top of the current ones for `phonetic` token filter called `bm` (or `beider_morse`) which is great for names. Allows to further set `rule_type` (either `approx` the default, or `exact`), and `name_type` (`generic` the default, or `ashkenazi` or `sephardic`).
</description><key id="2620786">1552</key><summary>Analysis: Add phonetic encodder called `bm` or `beider_morse`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-21T01:53:24Z</created><updated>2011-12-21T01:53:59Z</updated><resolved>2011-12-21T01:53:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticTokenFilterFactory.java</file></files><comments><comment>Analysis: Add phonetic encodder called `bm` or `beider_morse`, closes #1552.</comment></comments></commit></commits></item><item><title>Additions to the River API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1551</link><project id="" key="" /><description>(This is somewhat related to this couchdb-river-related issue):

https://github.com/elasticsearch/elasticsearch-river-couchdb/issues/3

I'd like to suggest some additional actions for the river API:

1) stop: Allow Rivers to instruct ES to stop themselves. This allows to write Rivers that do one-shot tasks (like migrations etc.) and indicate to ES once they are done. Currently, this can be done by letting the river delete itself using the client passed to itself, but this is more of a hack.
2) restart: Allow Rivers to flag themselves for restart (close -&gt; start). This would allow rivers to "fail early". In the issue given above, I do have the impression that the CouchDB river actually fails at reconstructing its proper runtime state. It could be better to just let the river fail and shut down properly and then let ES restart it, which would make it much easier to assess in which state the river actually was when failing. Also, it takes the burden of implementing full crash-safety from the river, as implementations can decide to handle specific failures by requesting a restart.
3) store/get state: Allow the river to persist its state through a blessed API. At the moment, this can be done by writing directly to the river index (e.g. /_river/myriver/_mystate), but keeping this consistent across rivers might be worthwhile. This comes in handy when using restart strategies as in 2).

If there is interest in these changes, I would try to implement them myself.
</description><key id="2618285">1551</key><summary>Additions to the River API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2011-12-20T21:17:00Z</created><updated>2014-07-08T12:45:17Z</updated><resolved>2014-07-08T12:45:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T12:45:17Z" id="48330874">Rivers will be removed in a future version of Elasticsearch. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete a river index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1550</link><project id="" key="" /><description>I have a river with mongodb:

curl -XPUT localhost:9200/_river/zoom_noticia/_meta -d '{
        "type":"mongodb",
        "mongodb":{
                "host":"localhost",
                "db":"zoom",
                "port": 27017,
                "collection":"noticia"
        },
    "index" : {
            "index" : "zoom",
            "type" : "noticia",
            "bulk_size" : "100",
            "bulk_timeout" : "10ms"
        }
}'

When I run: curl -XDELETE 'http://localhost:9200/zoom/' one only row is not deleted. But if I run: curl-XDELETE 'http://localhost:9200/_all' all indices are excluded.

How to delete only a single index? Put example please.
</description><key id="2615154">1550</key><summary>Delete a river index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phenrigomes</reporter><labels /><created>2011-12-20T17:04:40Z</created><updated>2011-12-20T20:29:00Z</updated><resolved>2011-12-20T18:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-20T18:02:22Z" id="3222578">You delete from `_river/zoom_noticia` to delete the river (you might want to also delete the actual index the river is indexing into. Btw, questions on the mailing list, not as issues.
</comment><comment author="phenrigomes" created="2011-12-20T20:17:05Z" id="3224628">I deleted using

curl-XDELETE http://localhost:9200/_river/zoom_noticia
curl-XDELETE http://localhost:9200/zoom/noticia

So, I managed to delete the mapping with the second command, and the river with the first.
But only when I ran the first, seems to be an active cache, but this only occurs with river. Why this? Thank you!
</comment><comment author="phenrigomes" created="2011-12-20T20:29:00Z" id="3224792">In my case using river, I have to run DELETE on my_river and index:

XDELETE http://localhost:9200/_river/zoom_noticia/_query-curl-d '{
     "term": {"name": "Test3"}
}
'

curl-XDELETE 'http://localhost:9200/zoom/noticia/_query'-d '{
     "term": {"name": "Test3"}
}
'

How it works correctly. If I run only in my_river, a record is always active. Thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Translog: When not sync'ing on each operation, buffer writes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1549</link><project id="" key="" /><description>Buffer writes when not sync'ing on each operation. Sync will cause flush and sync. Buffering will be on by default if not sync'ing on each operation. Settings (can be dynamically updated using update settings API):
- `index.translog.fs.type`: Either `simple` or `buffered`. Defaults to `buffer` if not sync'ing on each operation.
- `index.translog.fs.buffer_size`: The buffer size, defaults to `64k`.
</description><key id="2590767">1549</key><summary>Translog: When not sync'ing on each operation, buffer writes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-17T22:14:24Z</created><updated>2011-12-17T22:19:45Z</updated><resolved>2011-12-17T22:19:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/BufferingFsTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/SimpleFsTranslogFile.java</file><file>src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/test/java/org/elasticsearch/benchmark/stress/SingleThreadIndexingStress.java</file><file>src/test/java/org/elasticsearch/test/unit/index/translog/AbstractSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/translog/fs/FsBufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/translog/fs/FsSimpleTranslogTests.java</file></files><comments><comment>Translog: When not sync'ing on each operation, buffer writes, closes #1549.</comment></comments></commit></commits></item><item><title>No 'greek' stemmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1548</link><project id="" key="" /><description>org\tartarus\snowball\ext does not contain a 'greek' stemmer. Throws a runtime exception when using 'greek':

RuntimeException[java.lang.ClassNotFoundException: org.tartarus.snowball.ext.GreekStemmer]

http://www.elasticsearch.org/guide/reference/index-modules/analysis/stemmer-tokenfilter.html
</description><key id="2588791">1548</key><summary>No 'greek' stemmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lindstromhenrik</reporter><labels /><created>2011-12-17T13:21:13Z</created><updated>2011-12-19T15:52:31Z</updated><resolved>2011-12-19T15:52:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-19T12:13:00Z" id="3202434">which version have you tried? It is in the latest 0.18.x version.
</comment><comment author="lindstromhenrik" created="2011-12-19T12:35:24Z" id="3202622">I've checked 0.18.2 - 0.18.4 (lucene-analyzers-3.4.0.jar) and 0.18.5 - 0.18.6 (lucene-analyzers-3.5.0.jar) and non seem to contain the GreekStemmer.
</comment><comment author="kimchy" created="2011-12-19T12:42:55Z" id="3202680">there isn't a stemmer, but there is a stemfilter for it: https://github.com/elasticsearch/elasticsearch/blob/0.18/modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java#L155.
</comment><comment author="lindstromhenrik" created="2011-12-19T14:07:36Z" id="3203443">OK, sorry for the confusion. The one I was testing at runtime was
0.18.4 and for that version the Greek stemmer hadn't been added.

Den 19 december 2011 13:42 skrev Shay Banon
reply@reply.github.com:

&gt; there isn't a stemmer, but there is a stemfilter for it: https://github.com/elasticsearch/elasticsearch/blob/0.18/modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java#L155.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1548#issuecomment-3202680

## 

---

Henrik Lindström

## lindstrom.henrik@gmail.com
</comment><comment author="kimchy" created="2011-12-19T15:52:31Z" id="3204852">cool, closing then...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>0.18.5 ignores query string analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1547</link><project id="" key="" /><description>This script shows a regression between 0.17.8 and 0.18.5's query string searching:

```
#!/bin/sh

set -e

URL="http://localhost:9200"

function run() {
   echo "%" "$@"
   "$@"
   echo
   echo
}

run curl -XDELETE "$URL/test/"
run curl -XDELETE "$URL/_percolator"
run curl -XPUT "$URL/test/" -d '
{
  "index": {},
  "settings": {
    "analysis": {
      "analyzer": {
        "dash_path": { "type": "custom", "tokenizer": "dash_path" }
      },
      "tokenizer": {
        "dash_path": { "type": "path_hierarchy", "delimiter": "-" }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "location": { "type":"string", "analyzer": "dash_path" }
      }
    }
  }
}'
run curl -XGET "$URL/test/_mapping"
run curl -XGET "$URL/test/_analyze?field=test.location&amp;pretty=true" -d "A-A-A"


run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"A-A-A"}'
run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"A-A-B"}'
run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"A-B-A"}'
run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"A-B-B"}'
run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"B-A-A"}'
run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"B-A-B"}'
run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"B-B-A"}'
run curl -XPOST "$URL/test/test/?refresh=true" -d '{"location":"B-B-B"}'
run curl -XGET "$URL/test/test/_search?pretty=true" -d '
{
  "query": {
    "query_string": {
      "query": "location:A-B",
      "analyzer": "keyword"
    }
  }
}'
```

In 0.17.8, it returns the proper values:

```
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "Z2PnXuc9TbmPiLwdh9sOjA",
      "_score" : 1.0, "_source" : {"location":"A-B-A"}
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "RK20FlyfQBq29K503MiZSQ",
      "_score" : 1.0, "_source" : {"location":"A-B-B"}
    } ]
  }
}
```

But in 0.18.5, it returns this (It should only include things with the prefix `A-B`):

```
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.4142135,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "sHEIGa73QbmU3V6btCMmFg",
      "_score" : 1.4142135, "_source" : {"location":"A-B-B"}
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "dxSFQNu_RLCwNU4rQYnB4g",
      "_score" : 1.4142135, "_source" : {"location":"A-B-A"}
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "VyrcXa3AS3qoJ6ZueL1ufQ",
      "_score" : 0.5085423, "_source" : {"location":"A-A-A"}
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "AYPtN0ppS7GNcBCKmLfIvw",
      "_score" : 0.5085423, "_source" : {"location":"A-A-B"}
    } ]
  }
}
```
</description><key id="2584670">1547</key><summary>0.18.5 ignores query string analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels><label>bug</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-16T21:38:02Z</created><updated>2011-12-16T21:50:18Z</updated><resolved>2011-12-16T21:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/queryParser/QueryParserSettings.java</file><file>src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file></files><comments><comment>0.18.5 ignores query string analyzer, closes #1547.</comment></comments></commit></commits></item><item><title>Search: When searching against a type with a dfs search type, dfs is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1546</link><project id="" key="" /><description /><key id="2581805">1546</key><summary>Search: When searching against a type with a dfs search type, dfs is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-16T17:01:57Z</created><updated>2011-12-16T17:17:38Z</updated><resolved>2011-12-16T17:17:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file></files><comments><comment>Search: When searching against a type with a dfs search type, dfs is ignored, closes #1546.</comment></comments></commit></commits></item><item><title>Wrong value for pathType [{type=string}] for objet [page]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1545</link><project id="" key="" /><description>curl -XPUT 'http://localhost:9200/test/page/_mapping' -d '{"properties":{"path":{"type":"string"}}}'

{"error":"MapperParsingException[Wrong value for pathType [{type=string}] for objet [page]]","status":400}

1.misspelling for objet
2.path is a normal field name,why can't set type=string ?
</description><key id="2577050">1545</key><summary>Wrong value for pathType [{type=string}] for objet [page]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">quaff</reporter><labels /><created>2011-12-16T07:30:45Z</created><updated>2014-07-08T12:44:57Z</updated><resolved>2014-07-08T12:44:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-16T14:46:35Z" id="3177935">Its because the mapping definition should be wrapped with the type itself, otherwise, it won't work properly. I will try and make it better (at least a better failure message).
</comment><comment author="clintongormley" created="2014-07-08T12:44:57Z" id="48330847">This behaviour has changed in 1.x. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>support multiple uri params with same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1544</link><project id="" key="" /><description>When working on a plugin that requires the use of passing multiple uri parameters with the same name, I found that ES only honors the last parameter.

&amp;param=1&amp;param=2

In a RestRequest, calling param("param") will result in 2, completely ignoring the first parameter.  I was going to write a patch for this that modifies the decodeQueryString of RestUtils to append multiple values with a comma so that we can use the paramAsStringArray to get each value.  This might not be the best approach considering a parameter value can have a comma in the value.

Netty has a utility class org.jboss.netty.handler.codec.http.QueryStringDecoder that could be used.  It returns params as a list of strings.  Would it be better to create a multiParams variable that uses this and leave the old params in place for backwards compatibility?
</description><key id="2569023">1544</key><summary>support multiple uri params with same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2011-12-15T16:28:14Z</created><updated>2014-07-08T12:44:26Z</updated><resolved>2014-07-08T12:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-15T20:50:08Z" id="3168756">I did not really need it in elasticsearch (and I think its a bad REST design to support same named variables), so I could create a parser that is much more optimized (speed and memory wise) compared to one that supports it. Why do you need it?
</comment><comment author="mattweber" created="2011-12-15T20:55:42Z" id="3168846">For my Mock Solr plugin.  Solr accepts many fq parameters for filter queries.  It is probably not important for ES itself, but it might be useful to plugin authors like myself.  I can always parse the uri directly in the plugin though...
</comment><comment author="kimchy" created="2011-12-15T21:25:20Z" id="3169277">For now, it might make sense to reprase it in the relevant rest action that needs it..., at least until I figure a good solution on ES side.
</comment><comment author="clintongormley" created="2014-07-08T12:44:26Z" id="48330801">No further discussion on this ticket for 3 years.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>timestamp term match broken in 0.18.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1543</link><project id="" key="" /><description>using a boolean query with a timestamp in a term, returns non-matching items. 

the example below returns all items with searched domain and ANY "ts" value in 0.18.5. in 0.17.6 the "ts" field must match. we already tried it with integer and string fields instead of the timestamp field which works, so it seems this is timestamp related?

we also tried using a filtered query and filtering by "ts" which does not work either.

e.g:

``` javascript
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "domains": "adidas.com"
          }
        },
        {
          "term": {
            "ts": 1316995200000
          }
        }
      ]
    }
  },
  "fields": "*"
}
```

we also tried using the "ts" as the only requirement. in this case entries from the matching day are returned but the time does not need to match.

``` javascript
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "ts": 1316995200000
          }
        }
      ]
    }
  },
  "fields": "*"
}
```

the searched timestamp is Mo Sep 26 00:00:00 UTC 2011, but results appear with timestamps like this ts: 2011-09-26T04:19:38.000Z
</description><key id="2567194">1543</key><summary>timestamp term match broken in 0.18.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobe</reporter><labels><label>bug</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-15T14:44:50Z</created><updated>2011-12-15T19:02:10Z</updated><resolved>2011-12-15T18:49:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-15T15:25:28Z" id="3162844">Can you gist a full curl recreation that shows this, with creation of index (with custom mappings if you have it), and then search that shows the problem.
</comment><comment author="jukart" created="2011-12-15T15:32:56Z" id="3163138">The recreation: https://gist.github.com/1481503
</comment><comment author="dobe" created="2011-12-15T15:41:09Z" id="3163549">for backward compat we enabled this:

index :
    mapping:
      date:
        parse_upper_inclusive: false

but does not work without this config too
</comment><comment author="kimchy" created="2011-12-15T17:02:39Z" id="3165377">Yea, its a regression due to the new "parse_upper_inclusive" setting. You can disable it for now and I will push a fix to master and 0.18.6.
</comment><comment author="dobe" created="2011-12-15T17:24:54Z" id="3165711">thanks for your immediate replies.

i can't disable it in our cluster, because an application relies on the old behavior. when do you plan to release 0.18.6?

thx
</comment><comment author="dobe" created="2011-12-15T17:30:47Z" id="3165790">fyi: i tested it again if it works without setting parse_upper_inclusive ... it does not fix the issue
</comment><comment author="kimchy" created="2011-12-15T18:49:00Z" id="3166908">The old behavior is preserved when setting the parse_upper_inclusive to false, if you don't set it, it will use the new behavior which has the bug with term matching.
</comment><comment author="dobe" created="2011-12-15T19:02:10Z" id="3167125">you are right, the setting fixes the issue!
thanks for the very fast responses and fix!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file></files><comments><comment>timestamp term match broken in 0.18.5, closes #1543.</comment></comments></commit></commits></item><item><title>Read only indices for issue 1452</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1542</link><project id="" key="" /><description /><key id="2561331">1542</key><summary>Read only indices for issue 1452</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bbgordonn</reporter><labels /><created>2011-12-14T23:59:40Z</created><updated>2014-06-18T05:19:49Z</updated><resolved>2011-12-27T18:35:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-27T18:35:38Z" id="3283970">Pushed. Note, the settings changed to be `index.blocks.read_only` and `cluster.blocks.read_only`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>source not returned when * specified in fields list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1541</link><project id="" key="" /><description>The source field is not returned in a query where \* is specified and source is explicitly asked for.

Works as expected, returns the name and source

&lt;pre&gt;
curl -XGET 'http://localhost:9200/testindex/testtype/_search?q=lucene+in+action&amp;pretty=true' -d '{"fields":["name","_source"]}'
&lt;/pre&gt;


Does not work, returns only the stored fields, no source.

&lt;pre&gt;
curl -XGET 'http://localhost:9200/testindex/testtype/_search?q=lucene+in+action&amp;pretty=true' -d '{"fields":["*","_source"]}'
&lt;/pre&gt;


Happens using both DSL and URI Request fields parameter.
</description><key id="2545575">1541</key><summary>source not returned when * specified in fields list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-13T23:34:55Z</created><updated>2011-12-14T19:18:21Z</updated><resolved>2011-12-14T19:18:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/test/java/org/elasticsearch/test/integration/search/fields/SearchFieldsTests.java</file></files><comments><comment>source not returned when * specified in fields list, closes #1541.</comment></comments></commit></commits></item><item><title>Query DSL: Allow to default certain settings in query_string / field queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1540</link><project id="" key="" /><description>Add `indices.query.query_string.analyze_wildcard` and `indices.query.query_string.allowLeadingWildcard` that can be set in the config file to control the default values for query_string or field queries (across all indices).
</description><key id="2539358">1540</key><summary>Query DSL: Allow to default certain settings in query_string / field queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-13T16:45:26Z</created><updated>2014-07-03T08:12:21Z</updated><resolved>2011-12-13T17:02:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryParser/QueryParserSettings.java</file><file>src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file></files><comments><comment>Query DSL: Allow to default certain settings in query_string / field queries, closes #1540.</comment></comments></commit></commits></item><item><title>Query DSL: query_string analyze wildcard option with prefix to automatically do OR'ed wildcard when its broken down into several tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1539</link><project id="" key="" /><description>Now, when the prefix wildcard query is broken down into several tokens, and not one, it will just not use the analyzed option. Instead, break down the analyzed terms into a boolean query OR'ed between each one prefix option.
</description><key id="2539156">1539</key><summary>Query DSL: query_string analyze wildcard option with prefix to automatically do OR'ed wildcard when its broken down into several tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-13T16:38:10Z</created><updated>2012-03-14T12:06:37Z</updated><resolved>2011-12-13T16:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="anhthu" created="2012-03-14T12:06:37Z" id="4496817">hi kimchy, how could I set it to AND instead of OR?

My query like this:
{
  "query": {
    "query_string": {
      "query": "*Strasse-Schiene",
      "fields": [
        "denotation"
      ],
      "default_operator": "and",
      "analyze_wildcard": true,
      "minimum_should_match": "100%",
      "use_dis_max": true
    }
  }
}

and it automatically do OR'ed wildcard when its broken down into several tokens

Thanks
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file></files><comments><comment>Query DSL: query_string analyze wildcard option with prefix to automatically do OR'ed wildcard when its broken down into several tokens, closes #1539.</comment></comments></commit></commits></item><item><title>Query DSL: Replace index.query.bool.max_clause_count with indices.query.bool.max_clause_count (old one still supported)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1538</link><project id="" key="" /><description /><key id="2538144">1538</key><summary>Query DSL: Replace index.query.bool.max_clause_count with indices.query.bool.max_clause_count (old one still supported)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-13T15:46:46Z</created><updated>2011-12-13T15:47:16Z</updated><resolved>2011-12-13T15:47:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file></files><comments><comment>Query DSL: Replace index.query.bool.max_clause_count with indices.query.bool.max_clause_count (old one still supported), closes #1538.</comment></comments></commit></commits></item><item><title>Nested objects not deleted on "delete by query"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1537</link><project id="" key="" /><description>Recreation: https://gist.github.com/1453163

(Forum post: https://groups.google.com/group/elasticsearch/browse_thread/thread/77760418a856c0a6#)

Delete by query doesn't appear to delete the nested objects (observed on 0.17.9 and 0.18.5)

(Delete by id works fine)
</description><key id="2537019">1537</key><summary>Nested objects not deleted on "delete by query"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels><label>bug</label><label>v0.18.7</label><label>v0.19.0.RC1</label></labels><created>2011-12-13T14:26:52Z</created><updated>2011-12-25T11:33:13Z</updated><resolved>2011-12-25T11:33:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/search/nested/IncludeAllChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/test/java/org/elasticsearch/test/integration/nested/SimpleNestedTests.java</file></files><comments><comment>Nested objects not deleted on "delete by query", closes #1537.</comment></comments></commit></commits></item><item><title>Nested queries: getDocIdSet NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1536</link><project id="" key="" /><description>Possibly trivial recreation: https://gist.github.com/1453137

(Forum post: https://groups.google.com/group/elasticsearch/browse_thread/thread/77760418a856c0a6#)

I encountered this on my pre-operational cluster in an actual live scenario, but haven't been able to reproduce except as above. 

It only occurred the very first time I tried out the nested functionality (updated from 0.16.x, then tried a couple of different mappings, then uploaded the same data a couple of times with minor fixes), so it's possible it was related to some combination of unusual activities.

So anyway, wanted to report the above possibly trivial issue, and maybe it's a sub-case of the more serious one I can't reproduce.
</description><key id="2536997">1536</key><summary>Nested queries: getDocIdSet NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels><label>bug</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-13T14:24:56Z</created><updated>2011-12-14T13:07:43Z</updated><resolved>2011-12-14T13:07:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-13T15:11:55Z" id="3124089">Does the gist recreates the failure each time? I used the gist on a fresh 0.18.5 and it works well.
</comment><comment author="Alex-Ikanow" created="2011-12-13T15:57:55Z" id="3124814">Ha, I figured it was more likely to be a bit random vs having "nested"  in the name!
My steps were:
- upgrade index from 0.17.9 to 0.18.5
- delete random indexes from previous debug (different names)
- run above test ("nested" fails, "test_type" works)
- delete indexes
- rerun above test, same result

So on a sample of 2, yes! I'll fire up my VM and try a few more samples/deleting ES and rebuilding from scratch.
</comment><comment author="Alex-Ikanow" created="2011-12-13T17:07:40Z" id="3125889">Of course, now my super simple gist works fine :(

Hmm, interestingly I'm not the first person to have this problem happen consistently and then disappear the next day....

https://github.com/elasticsearch/elasticsearch/issues/1263 (vanished on all versions after installing a new version; some other dude had a different but similar NPE still open?)
https://github.com/elasticsearch/elasticsearch/issues/1510 (still open, you couldn't reproduce)
http://pastebin.com/8rxqY954 (couldn't find the parent issue - the code for this one was uncannily similar to my report)

Up to you whether to keep this one open or close it (or join it?)
</comment><comment author="Alex-Ikanow" created="2011-12-13T18:02:02Z" id="3126756">FWIW I just had a look at https://github.com/elasticsearch/elasticsearch/blob/a8fd2d48b8f3f17d68ed27c3104e2c9e2eb6cc9c/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java and (apologies for super naive probably dumb question...)

Is it not possible to get "stuck" with a LateBindingParentFilter with "filter" member == null if you exception out in between lines 66 and 124? (ie usAsParentFilter gets created and set in the threadlocal on line 64, but doesn't have its filter parameter set until 124)

(and obviously the NPEs reported above are all caused by LateBindingParentFilter::filter not being set, and this would be semi consistent with the intermittent reproducability, you'd only see the problem if you made an invalid query at some point)

I'll see if I can reproduce that...
</comment><comment author="Alex-Ikanow" created="2011-12-13T18:04:52Z" id="3126815">Bingo!

To reproduce the error in my gist first do:

curl -XGET 'http://localhost:9200/nested_test/nested/_search?pretty=true' -d'{"query":{"nested": { "path":"obj1"}}}}'

then subsequent valid calls, eg:

curl -XGET 'http://localhost:9200/nested_test/nested/_search?pretty=true' -d'{"query":{"nested": { "path":"obj1", "query": { "match_all":{}}}}}'

Will return the error in the various error reports
</comment><comment author="deverton" created="2011-12-14T00:49:56Z" id="3133617">I'm some other dude :) I could never get it down to a test case so I didn't open a new bug. Following the steps in your last comment generates the error every time for me. The exact steps are in https://gist.github.com/1474679 I just pulled it in to a simple shell script
</comment><comment author="kimchy" created="2011-12-14T13:07:09Z" id="3139201">Heya, great catch and simple recreation!. Yes, the problem was when an invalid nested query was sent, things were not being cleared up properly on the relevant thread that executed the search, which left it "dirty", a fix is coming shortly.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file></files><comments><comment>Nested queries: getDocIdSet NullPointerException, closes #1536.</comment></comments></commit></commits></item><item><title>Allow to configure a shard with no file based data location locking</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1535</link><project id="" key="" /><description>Allow to set `index.store.fs.lock` to `none` (defaults to `native`).
</description><key id="2522655">1535</key><summary>Allow to configure a shard with no file based data location locking</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-12T13:04:32Z</created><updated>2011-12-12T13:05:00Z</updated><resolved>2011-12-12T13:05:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/fs/FsDirectoryService.java</file></files><comments><comment>Allow to configure a shard with no file based data location locking, closes #1535.</comment></comments></commit></commits></item><item><title>River: When deleting a river, make sure when its closed that its data is deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1534</link><project id="" key="" /><description>In case the river writes additional data until it closes, double delete it after its closed on the relevant node it is running on.
</description><key id="2517835">1534</key><summary>River: When deleting a river, make sure when its closed that its data is deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-11T21:57:48Z</created><updated>2011-12-11T23:09:54Z</updated><resolved>2011-12-11T23:09:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/river/RiversService.java</file><file>src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file></files><comments><comment>River: When deleting a river, make sure when its closed that its data is deleted, closes #1534.</comment></comments></commit></commits></item><item><title>Multicast: Add discovery.zen.ping.multicast.ping.enabled setting to disable sending ping requests while still having multicast enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1533</link><project id="" key="" /><description>Defaults to `true`.
</description><key id="2516048">1533</key><summary>Multicast: Add discovery.zen.ping.multicast.ping.enabled setting to disable sending ping requests while still having multicast enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-12-11T14:23:53Z</created><updated>2011-12-11T16:40:52Z</updated><resolved>2011-12-11T14:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-11T14:25:26Z" id="3097413">implemented.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support Multicast discovery for external clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1532</link><project id="" key="" /><description>The multicast zen ping should also support handling "external" discovery multicast requests (multicast settings remain the same defaults, described here: http://www.elasticsearch.org/guide/reference/modules/discovery/zen.html). The request should be sent in the following format (indicating the cluster name requested):

```
{
    "request" : {
        "cluster_name": "test_cluster"
    }
}
```

And the response will be similar to node info response (with node level information only, including transport/http addresses, and node attributes):

```
{
    "response" : {
        "cluster_name" : "test_cluster",
        "transport_address" : "...",
        "http_address" : "...",
        "attributes" : {
            "..."
        }
    }
}
```

Note, it can still be enabled, with disabled internal multicast discovery, but still have external discovery working by keeping `discovery.zen.ping.multicast.enabled` set to `true` (the default), but, setting `discovery.zen.ping.multicast.ping.enabled` to `false`.
## Orinal Request:

I would like to write a client that uses multicast to determine the members of a cluster.

How would this work?
1. Send multicast ping with JSON.
2. Multicast pong with cluster details in JSON
3. Then use the API.
</description><key id="2508377">1532</key><summary>Support Multicast discovery for external clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnwilliams</reporter><labels><label>feature</label><label>v0.19.0.RC1</label></labels><created>2011-12-09T23:18:48Z</created><updated>2012-03-19T08:13:16Z</updated><resolved>2011-12-11T16:54:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeremy" created="2011-12-09T23:25:49Z" id="3087635">:+1:

Zero-config external clients!
</comment><comment author="clintongormley" created="2011-12-10T12:45:47Z" id="3090652">I'd be interested in doing this for the Perl API too
</comment><comment author="kimchy" created="2011-12-11T16:48:15Z" id="3098069">I updated the description with more info as to the format of the request/response.
</comment><comment author="kimchy" created="2011-12-11T16:54:59Z" id="3098111">Pushed to master, would love to get feedback on this by "other" clients :)
</comment><comment author="karmi" created="2011-12-11T18:40:20Z" id="3098791">@kimchy: Sounds great!, we're in a hot discussion over at karmi/tire#162 :)
</comment><comment author="clintongormley" created="2011-12-12T11:52:50Z" id="3105062">Hiya

Tried this out, and ES is responding, but without the HTTP address:

```
'{"response":{"cluster_name":"iAnnounce-demo","transport_address":"inet[/192.168.5.10:9300]","attributes":{}}}
```
</comment><comment author="clintongormley" created="2011-12-12T11:55:33Z" id="3105081">To be clear, what I'm doing in Perl is:

```
use IO::Socket::Multicast; 
my $s=IO::Socket::Multicast-&gt;new(
    LocalPort   =&gt; 54328, 
    LocalAddr   =&gt; '224.2.2.4',
    ReuseAddr   =&gt; 1
); 
$s-&gt;mcast_add('224.2.2.4');
$s-&gt;mcast_send('{"request":{"cluster_name":"iAnnounce-demo"}}','224.2.2.4:54328'); 

$s-&gt;recv($d,1024);
# '{"request":{"cluster_name":"iAnnounce-demo"}}'

$s-&gt;recv($d,1024);
# '{"response":{"cluster_name":"iAnnounce-demo","transport_address":"inet[/192.168.5.10:9300]","attributes":{}}}'
```

using this module: https://metacpan.org/module/IO::Socket::Multicast
</comment><comment author="johnwilliams" created="2011-12-12T16:04:36Z" id="3107896">@kimchy This is awesome. I am going to try it out shortly. Is it possible to have the cluster multicast the same json format when a node comes online or goes offline? This will allow the client to just listen for updates to know active clients rather than pinging all the time.
</comment><comment author="kimchy" created="2011-12-12T18:24:31Z" id="3110106">@clintongormley it should be fixed now
</comment><comment author="kimchy" created="2011-12-12T18:26:31Z" id="3110136">@johnwilliams thats a bit different, since the multicast will not be a message of all nodes in the cluster (possibly sent by the master) on changes to the topology, whereby now, it simply reacts to a direct request and sends back that node info.
</comment><comment author="clintongormley" created="2011-12-15T11:26:57Z" id="3159876">@kimchy I'm trying to think of how to best incorporate multicast discovery in the Perl API. 

Currently, we connect to a cluster using a configured server address. We extract the list of live nodes and store them for (a) round robin'ing and (b) to re-sniff if any node goes down.

We can only use multicast if the user provides a cluster name. I think that if they provide a cluster name AND one or more server addresses, then we should still use the server address in favour of multicast (but possibly confirm that the cluster name is the same?).

Question comes when (a) we've sniffed the cluster using multicast and (b) when one of the sniffed nodes disappears.  Do we:
  (1) use the current mechanism of doing a `/_nodes` request against each of the previously sniffed nodes, or  
  (2) use multicast to re-sniff instead.

My instinct would be to use multicast, as it kinda works in parallel (ie any server could respond), even if our code is sync, while the `/_nodes` requests would have to be done serially. So in the case that one node is hanging (eg a broken switch or bad firewalling), with multicast we would avoid the delay of waiting for that node to timeout.

If using multicast is better, then even if the user specifies a static server list initially, should we extract and store the cluster name and use multicast for future sniffing? It may be that multicast is disabled on the network, but we won't know that until we try it out. (of course, could add an option to disable multicast in the client)

Another question is: we're likely to get multiple multicast responses (1 from each server).  Do we just accept the first one, or do we wait for any responses received within a certain timeout?  In the case of split brain, we may get different responses, and want to connect to the cluster which has more nodes.
</comment><comment author="kimchy" created="2011-12-16T21:14:05Z" id="3182692">&gt; &gt; We can only use multicast if the user provides a cluster name. I think that if they provide a cluster name AND one or more server addresses, then we should still use the server address in favour of multicast (but possibly confirm that the cluster name is the same?).

Yes, make sense. I think if cluster name is provided, and a list of servers is also provided, it makes sense to verify that the cluster name is the same.

Regarding the re-sniffing if a node fails. I think that as long as you have live nodes, use the /nodes API (and you only need to execute it once, no need to do that on each node). Once you no longer have any more nodes, to connect to, use multicast.
</comment><comment author="medcl" created="2012-03-19T08:13:16Z" id="4569157">i can use this hot feature in my client,great~
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file></files><comments><comment>set the node service on the discovery to get the additional service level attributes (issue #1532)</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>src/main/java/org/elasticsearch/discovery/zen/DiscoveryNodesProvider.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>src/test/java/org/elasticsearch/test/unit/discovery/zen/ping/multicast/MulticastZenPingTests.java</file><file>src/test/java/org/elasticsearch/test/unit/discovery/zen/ping/unicast/UnicastZenPingTests.java</file></files><comments><comment>Support Multicast discovery for external clients, closes #1532.</comment></comments></commit></commits></item><item><title>facets OutofMemoryError</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1531</link><project id="" key="" /><description>running a term facet on a large string field runs into an OutOfMemoryError (-Xmx50g -Xms50g).

Would be great to have the the field cache access disk or that a facet query does not load all the terms of a field into the cache but only the terms of a query.
The second would allow to run multiple facet term query's within different forms and aggregate at the end.

Opened issue as discussed in irc.
</description><key id="2502440">1531</key><summary>facets OutofMemoryError</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">locojay</reporter><labels /><created>2011-12-09T15:12:54Z</created><updated>2013-06-26T16:11:17Z</updated><resolved>2013-06-26T16:11:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-12-14T12:49:00Z" id="3139024">link to discussion?
</comment><comment author="gustavobmaia" created="2012-01-07T21:58:08Z" id="3398564">I have the same problem.

I think it creates a cache of the fields you want to do the facet, so if you have many documents will have to have a lot of memory. Imagine that I have a billion documents, I'll never have enough memory to make facets.
</comment><comment author="kimchy" created="2012-01-08T11:05:58Z" id="3401500">It depends on the number of terms you have for that field. The terms facet is not aimed to have facets on a "body of text", its more aimed at having facets on categories, tags, and the like.
</comment><comment author="gustavobmaia" created="2012-01-08T15:55:41Z" id="3402701">Today, I have a million and 4 million document tag when I do in every facet documents, I have a problem with OutOfMemory. I started the JVM with 4GB.
This is normal I have problem with OutOfMemory?
</comment><comment author="kimchy" created="2012-01-08T18:12:07Z" id="3403444">@gustavobbamaia it depends on what you facet on. All the values need to be loaded to memory for fast access.
</comment><comment author="jsuchal" created="2012-02-03T17:42:07Z" id="3800850">@kimchy Having a facet on a field with millions of entries and trying to only find top-N is a great interest for me. I have done some preliminary research how this can be done with fixed memory constraints. Basically I've spent several hours reading research papers.

The best solution I've found is called Count-Min Sketch (http://www.eecs.harvard.edu/~michaelm/CS222/countmin.pdf) or blog for non-academic readers (http://lkozma.net/blog/sketching-data-structures/) or this is a pretty good paper explaining a similar approach http://www.ece.uc.edu/~mazlack/dbm.w2010/Charikar.02.pdf

These algorithms however give approximate counts (with parametrizable error bounds), but that is something most users we can live with IMO.

Despite academic jargon, proofs and equations everywhere this is actually pretty straightforward to implement. Any chance that this will land in ES?
</comment><comment author="drewr" created="2012-02-22T19:03:55Z" id="4115520">This has been biting us as well.  Tracked it down to `ordinals` List in FieldDataLoader.load().  With few segments and a query on a field with a large set of unique terms it can grow to a huge size even with a modest data size.  The fewer the segments, the larger maxDocs() becomes (seems to be max out at 1k in our setup), resulting in a duplication of that `1k*sizeof(int)` footprint as it adds a row to `ordinals` for every doc where a single term is present.

Would you accept patches that use a different storage scheme or have you already planned out how you will address in 0.20?
</comment><comment author="kimchy" created="2012-02-22T19:15:12Z" id="4116286">Which patch were you thinking about. I just commented on the another issue #1683, where I explained why the structure is as is.
</comment><comment author="Alex-Ikanow" created="2012-02-22T19:29:14Z" id="4117216">@Drewr, others: In case it's useful: I solved a similar problem (related to having lists of terms, where the list could be 
"large" but was "small" on average) by keeping a separate index for documents with many list elements.

Eg I reduced my memory usage from ~14GB down to ~2GB by moving the "worst" ~0.1% documents into the new index (some more details are available in issue #1683 ref'd by Shay above, and the original thread in the es forum that spawned the issue).

Note that if you are only faceting/searching on fields, then making the objects nested will also solve the memory problem (at the expense of some performance, though it wasn't noticeable for me). I couldn't do this for me one of my fields (geo-tags) because I needed them for custom scripts, hence the secondary workaround.

Also, despite Shay's warnings, I have had success using the "soft" cache (in my case sometimes people create facets on big fields once or twice and then don't use them again; obviously if you have a single facet or set of common facets that exceed memory then this is a terrible idea!)

@Shay: My suggestion in #1683 was to have a hybrid storage, where most documents were stored as at present, but "large" (user-tunable parameter?) documents were stored "the other way round". (ie effectively an internal version of what I did with seperate indexes)

(fwiw I think it's possible that "good" containers could let you do whole thing "the other way round", but every time I sketched down some thoughts I ended up needing to write it in C++!)
</comment><comment author="kimchy" created="2012-02-22T19:32:20Z" id="4117449">@Alex-Ikanow yea, agreed, other modes of storage / usage would be great. The first thing I want to do is to create that relevant abstraction (or improve on the current one) in elasticsearch, and be able to configure to use different models in the mappings (or dynamically detected and used). This will allow for better plugability that is missing today, and hopefully will help foster other implementations.
</comment><comment author="spinscale" created="2013-06-26T16:11:17Z" id="20059942">Closing this one for now. 0.90 brought great improvements in regard to high cardinality fields and memory consumption when doing faceting on these. In addition there are ideas about the different modes of storage/usage Shay mentioned above. I guess we should rather create different issues for those than using this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>all_terms capability of terms facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1530</link><project id="" key="" /><description>Currently terms facet has an option all_terms which is expected to bring all the terms in the index (for a
field) with frequency counts on them. However, it does not work. Can you fix this?

Discussed in 
http://elasticsearch-users.115913.n3.nabble.com/Terms-facet-all-terms-does-not-work-td3568708.html
</description><key id="2498953">1530</key><summary>all_terms capability of terms facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels /><created>2011-12-09T06:50:34Z</created><updated>2014-04-08T16:17:04Z</updated><resolved>2013-04-12T09:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MrJohnsson77" created="2012-09-23T17:19:03Z" id="8800481">+1
</comment><comment author="georgkoester" created="2013-04-10T10:11:04Z" id="16165703">AFAIK This works in current versions and should be closed.
</comment><comment author="clintongormley" created="2013-04-12T09:23:18Z" id="16283597">This has been fixed in the 0.90 branch. Note: `all_terms` doesn't mean that it will return all terms in the index. It means that it will return all terms even if their count is zero, up to the specified `size`.
</comment><comment author="Tobion" created="2014-04-07T09:01:52Z" id="39708729">@clintongormley What all_terms actually means needs a doc update. Currently one still thinks something different based on the doc.
</comment><comment author="clintongormley" created="2014-04-08T16:17:04Z" id="39868616">@Tobion Want to send a PR?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sort: Support "missing" for string types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1529</link><project id="" key="" /><description>Sorting on documents that are missing a key would be very helpful, partially matching the api for sorting numerical values. For example:

```
{
"sort" : [
    { "user" : {"missing" : "_last"} },
],
"query" : {
    "term" : { "user" : "kimchy" }
}

{
"sort" : [
    { "user" : {"missing" : "_last"} },
],
"query" : {
    "term" : { "user" : "kimchy" }
}
```

Unfortunately we can't use this pattern for custom values. To support that, we'll need a sub-object:

```
{
"sort" : [
    { "user" : {"missing" : { "order": "_first"} } },
],
"query" : {
    "term" : { "user" : "kimchy" }
}

{
"sort" : [
    { "user" : {"missing" : { "order": "_last"} } },
],
"query" : {
    "term" : { "user" : "kimchy" }
}

{
"sort" : [
    { "user" : {"missing" : { "value": "nobody"} } },
],
"query" : {
    "term" : { "user" : "kimchy" }
}
```
</description><key id="2497940">1529</key><summary>Sort: Support "missing" for string types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2011-12-09T02:26:18Z</created><updated>2013-07-05T08:52:37Z</updated><resolved>2013-07-05T08:52:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nickhoffman" created="2012-01-10T22:18:50Z" id="3438207">I think this is a duplicate of #896.
</comment><comment author="spinscale" created="2013-07-05T08:52:37Z" id="20507429">Agreed. duplicate of #896.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consider merging release branch commits into master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1528</link><project id="" key="" /><description>When exploring elasticsearch's master branch, it can be a little difficult to separate out the commits cherry picked from the release branch from the commits unique to the master branch. Instead of cherry-picking commits onto master, could the release commits be merged instead?

There's an even more elaborate version of this branching strategy called [git flow](http://nvie.com/posts/a-successful-git-branching-model). That link explains this merging strategy a bit more, and fleshes things out into a full release management cycle. There's even a [git plugin](https://github.com/nvie/gitflow) for it too.
</description><key id="2497265">1528</key><summary>Consider merging release branch commits into master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2011-12-09T01:06:03Z</created><updated>2013-07-05T10:28:44Z</updated><resolved>2013-07-05T10:28:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2011-12-23T18:32:13Z" id="3263429">Git Flow is a bit odd for me, ...  A master branch with release branches, and some work on feature branches.  Use merge instead of cherry pick seems like a good request, the full Git Flow model seems less so.
</comment><comment author="spinscale" created="2013-07-05T10:28:44Z" id="20512093">Closing this. I guess we will stick with our current workflow for now - which admittedly makes it hard to find cherry-picked commits in other branches but keeps a clean merge-free history
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>java.io.IOException: Failed to obtain node lock</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1527</link><project id="" key="" /><description>Sometimes my app start throwing following stacktrace, endlessly:

```
Error injecting constructor, java.io.IOException: Failed to obtain node lock
  at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.env.NodeEnvironment
    for parameter 6 at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData
    for parameter 2 at org.elasticsearch.gateway.local.LocalGatewayAllocator.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.LocalGatewayAllocator
  while locating org.elasticsearch.cluster.routing.allocation.allocator.GatewayAllocator
    for parameter 1 at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators
    for parameter 2 at org.elasticsearch.cluster.routing.allocation.AllocationService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.cluster.routing.allocation.AllocationService
    for parameter 3 at org.elasticsearch.cluster.action.shard.ShardStateAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.cluster.action.shard.ShardStateAction
    for parameter 5 at org.elasticsearch.action.delete.TransportDeleteAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.action.delete.TransportDeleteAction
    for parameter 4 at org.elasticsearch.client.node.NodeClient.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.client.node.NodeClient
  while locating org.elasticsearch.client.Client
    for parameter 1 at org.elasticsearch.rest.action.admin.indices.delete.RestDeleteIndexAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.rest.action.admin.indices.delete.RestDeleteIndexAction
Caused by: java.io.IOException: Failed to obtain node lock
at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:118)
    at sun.reflect.GeneratedConstructorAccessor105.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
Destroying 1 processes
Destroying process..
Destroyed 1 processes
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:56)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
```

ElasticSearch 1.18.5, 

Java --version

```
java version "1.6.0_26"
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) Server VM (build 20.1-b02, mixed mode)
```

Ubuntu 11.10 32bit
</description><key id="2487100">1527</key><summary>java.io.IOException: Failed to obtain node lock</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">splix</reporter><labels /><created>2011-12-08T12:44:46Z</created><updated>2017-07-04T03:44:51Z</updated><resolved>2013-07-15T16:36:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-08T14:55:35Z" id="3063296">Its a lock file obtained within the data location elasticsearch uses. Is it writable?
</comment><comment author="splix" created="2011-12-08T15:30:16Z" id="3063763">Hm, must be. Actually it's happening in unit tests (and not always). And it configured with `node.local=true`, as I understood it means that its using in-memory index at this case.
</comment><comment author="kimchy" created="2011-12-08T15:33:48Z" id="3063801">node.local does not mean that it will use in memory indices, and even if you configure to use in memory indices, the transaction log is still file system based which requires the data location.
</comment><comment author="splix" created="2011-12-08T15:43:39Z" id="3063934">oh, seems that I misunderstood it :( btw, all data must be writable, tests are running at my dev machine, its using only one thread, etc.

I'll send you more details, if it happen another time
</comment><comment author="kimchy" created="2011-12-08T16:56:07Z" id="3065024">thanks!, I improved the exception message and the annoying crazy guice exception when this happens...
</comment><comment author="woodliu" created="2017-07-04T03:44:51Z" id="312773485">does that sloved?</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Moving debian package to maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1526</link><project id="" key="" /><description>Starting, indexing and stopping worked on my local ubuntu machine after installing via:

```
mvn package -Dmaven.test.skip=true
cd target &amp;&amp; sudo dpkg -i elasticsearch_0.19.0+SNAPSHOT.deb
```

remove it via:

```
sudo dpkg -r elasticsearch
```

BTW: now compare the 100+ files of the pom.xml to the 10 lines gradle script ;) 

(Although I really appreciate the step for 'NetBeans' support!)
</description><key id="2485474">1526</key><summary>Moving debian package to maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-12-08T09:46:39Z</created><updated>2014-07-16T21:55:49Z</updated><resolved>2011-12-08T11:42:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-08T10:05:42Z" id="3060196">Heya, thanks!,
- How does the copy dependencies part work? Which libraries end up there? Are they the same as the installation?
- We should tie it to the assembly:assembly goal (if its possible) which creates the installation files.
</comment><comment author="karussell" created="2011-12-08T10:12:18Z" id="3060292">&gt; How does the copy dependencies part work?

I'm not sure to be honest but it is called before the jdeb plugin and will copy all dependencies into target/lib

&gt; Are they the same as the installation?

what did you mean here?

&gt; We should tie it to the assembly:assembly goal (if its possible)

ah, yes. I'll check
</comment><comment author="kimchy" created="2011-12-08T10:21:52Z" id="3060437">I mean the debian package should have only the jar files in lib that match the ones needed by elasticsearch, you can find those in the assembly which creates the zip/tar.gz files.
</comment><comment author="karussell" created="2011-12-08T11:06:18Z" id="3061099">I've now bound the assemblies to package*\* (agree?) and fixed the point you mentioned and some minor changes. So you can now produce the zipped and deb packages via:

```
mvn package -Dmaven.test.skip=true
```

Some questions: isn't slf4j missing there I do not see it in the shaded deps? Why are the assembly ids empty (it prints a warning)?

**
It seems to me that the assembly stuff is deprecated:

"Usage of the assembly:assembly, assembly:attached, assembly:directory, and assembly:directory-inline are deprecated, since they wreak havoc with normal build processes and promote non-standard build practices."  From http://maven.apache.org/plugins/maven-assembly-plugin/

E.g. the examples bind the single goal to the package phase
</comment><comment author="kimchy" created="2011-12-08T11:21:50Z" id="3061242">sl4j is not really needed in elasticsearch, just log4j. Binding all to package sounds good. The ids are empty so the same file name will be created and they won't be added, did not find a nicer way to do it.
</comment><comment author="karussell" created="2011-12-08T11:30:42Z" id="3061307">ok. 

Deps should be fine - only the ones mentioned in the common assembly (without sl4j). And the deb package still seems to work (not massivly tested).
</comment><comment author="kimchy" created="2011-12-08T11:39:40Z" id="3061382">I pulled your work, fixed some things (sigar wasn't being copied, some warning on properties), will push it soon.
</comment><comment author="kimchy" created="2011-12-08T11:42:11Z" id="3061398">pushed, closing, thanks!.
</comment><comment author="karussell" created="2011-12-08T12:02:38Z" id="3061566">Cool, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate debian package creation to maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1525</link><project id="" key="" /><description>With gradle, we had a debian build created using jdeb. We need to port it to maven.

Looking at 0.18 branch, we need to copy over pkg: https://github.com/elasticsearch/elasticsearch/tree/0.18/pkg (preferable location is under src, so we have src/pkg/debian).

Then, we need to translate this code to maven using the jdeb plugin: https://github.com/elasticsearch/elasticsearch/blob/0.18/build.gradle#L191. Note, we no longer have an exploded jar, so need to figure out how to copy it all to the correct locations.

And of course, test it. And while we are at it, think of having a redhat one as well :)
</description><key id="2480236">1525</key><summary>Migrate debian package creation to maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label></labels><created>2011-12-07T21:22:59Z</created><updated>2013-05-22T14:12:20Z</updated><resolved>2013-05-22T14:12:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-12-07T23:09:16Z" id="3055124">I found a sample with maven

https://github.com/tcurdt/jdeb/blob/master/src/examples/maven/pom.xml

Also the comment on the mailing list (ant plugin) is a good hint. I'll check now. E.g. jetty is going the way via maven-deb-plugin http://jira.codehaus.org/browse/JETTY-351

For the redhat pkg ... couldn't it be done via
alien --to-rpm elasticsearch.deb
?
</comment><comment author="jprante" created="2011-12-08T00:39:53Z" id="3056126">Please note http://fedoraproject.org/wiki/Packaging:Java for Redhat/Fedora packaging guidelines.
There may be some extra effort if the target is to get the RPM cleanly included into the main Fedora distribution.
</comment><comment author="karussell" created="2011-12-08T09:47:43Z" id="3059951">If someone wants to test it:

https://github.com/elasticsearch/elasticsearch/pull/1526
</comment><comment author="dzen" created="2011-12-14T12:58:56Z" id="3139126">I 'm currently testing it (from current "trunk") build with maven &amp; co
</comment><comment author="kimchy" created="2011-12-14T13:10:32Z" id="3139240">@dzen thanks!. One thing that I was thinking about is hte upgrade process. Does "uninstalling" it removed the data directory / config?
</comment><comment author="dzen" created="2011-12-14T13:20:25Z" id="3139328">Seems that the package correctly create the user elasticsearch when installing. It also removes it when uninstalling.
It does remove the /usr/share/elasticsearch directory and the /etc/elasticsearch/ config directory :)
</comment><comment author="karussell" created="2011-12-14T13:27:57Z" id="3139395">For deleting this is what I would assume, no?

I think for upgrading the scripts needs special support:

https://github.com/elasticsearch/elasticsearch/pull/1526/files#L5R22
</comment><comment author="dzen" created="2011-12-14T13:38:19Z" id="3139522">I'm just confirming what it would do
</comment><comment author="kimchy" created="2012-01-26T11:15:14Z" id="3667353">Coming back to this, its a good question if removing the package should remove the data as well? How does the mysql package works for example? We need to have a good story when upgrading, so people won't remove and then install a new version and suddenly loose data...
</comment><comment author="karussell" created="2012-01-26T12:19:44Z" id="3668032">Config is kept but according the data I found mixed stuff. Here mysql data is not removed
http://serverfault.com/questions/306567/removing-mysql-with-yup-does-it-remove-the-data

and here it is removed
http://www.cyberciti.biz/ref/apt-dpkg-ref.html

dpkg -P &lt;package&gt; Purges an installed package named &lt;package&gt;. The difference between remove and purge is that while remove only deletes data and executables, purge also deletes all configuration files in addition.

But using purge it'll definitely remove config and data:
http://serverfault.com/questions/62691/how-to-completely-remove-a-package-in-debian

I would prefer to avoid removing data and config. Then upgrading is easier + quicker implemented via remove + install. 

Then using purge should remove all stuff.
</comment><comment author="karussell" created="2012-01-26T12:26:32Z" id="3668095">I asked it to the world ;)

http://serverfault.com/questions/353835/will-removing-a-debian-package-remove-the-data
</comment><comment author="jprante" created="2012-01-26T18:37:19Z" id="3673996">For Redhat, mysql RPMs package will leave all the data in place (/var/lib/mysql). 

In RHEL system init, where the mysql server is being started / stopped, distribution-specific automatic upgrade routines may take place, but this practice is discouraged (not only for mysql). There is already a mysql_upgrade binary in the mysql distribution. Mysql will refuse to write to the database if it is not in a usable state and display a hint to the user to run mysql_upgrade to refresh the tables.

I would prefer separating the tasks of packaging and upgrading.

It would be nice to have a diagnostic script within Elasticsearch that can handle the detection of the installed version, also the version of the running processes, if any, and the current index state. If necessary, it should display hints to the user what to do next, whether it's required to re-index the data from scratch, if a backup/restore can be accomplished, how to stop/start the software afterwards etc.
</comment><comment author="kimchy" created="2012-02-01T08:40:46Z" id="3755483">So do we want to change it to not remove the data in any case? Is that the consensus? I would prefer to err on the cautious side on this one.
</comment><comment author="karussell" created="2012-02-07T20:12:22Z" id="3855123">&gt; Is that the consensus?

Yes, I think we just need to replace the text in src/deb/control/postrm for the remove action (and the purge stays the same):

rm -rf /var/log/elasticsearch /var/lib/elasticsearch

with

rm -rf /var/log/elasticsearch

I think as the config files are already placed in a standard file src/deb/control/conffiles this should already have standard behaviour. I'll try this.

&gt; I would prefer to err on the cautious side on this one.

What do you mean her?

BTW: should we prefer oracle sun jdk or even put an error message for open jdk? Should we even autodownload it? (I have a bash script for this)
</comment><comment author="kimchy" created="2012-02-07T23:47:23Z" id="3859516">@karussell cautious as in not deleting the data location. Regarding JVMs, we should just prefer to run with the oracle ones if they exists, but elasticsearch works good with openjdk as well, so no need for error message. 
</comment><comment author="karussell" created="2012-02-08T07:50:03Z" id="3863879">Ok, the minor change in a pull request:
https://github.com/elasticsearch/elasticsearch/pull/1681

I tried the following to confirm that it works:

mvn -DskipTests=true clean package
sudo dpkg -i target/releases/elasticsearch-0.19.0.RC2-SNAPSHOT.deb
curl -XPOST "http://localhost:9200/articles/article" -d '{"title" : "Three", "tags" : ["foo", "bar", "baz"]}'
curl -XPOST 'http://localhost:9200/articles/_search?pretty=true&amp;q=_:_'
=&gt; ok, data there
sudo apt-get remove elasticsearch

sudo dpkg -i target/releases/elasticsearch-0.19.0.RC2-SNAPSHOT.deb
(wait a bit)
curl -XPOST 'http://localhost:9200/articles/_search?pretty=true&amp;q=_:_'
=&gt; old data present and config not overwritten

sudo apt-get purge elasticsearch
sudo dpkg -i target/releases/elasticsearch-0.19.0.RC2-SNAPSHOT.deb
curl -XPOST 'http://localhost:9200/articles/_search?pretty=true&amp;q=_:_'
=&gt; no data (IndexMissingException), config overwritten
</comment><comment author="kimchy" created="2012-02-08T13:57:05Z" id="3867857">Thanks!, pulled the changes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default mapping means new types not mapped explicitly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1524</link><project id="" key="" /><description>If the `_default_` mapping for an index matches all the fields in a new type, then that new type is not explicitly mapped, which also means that a `delete_mapping` doesn't delete the docs of that type.

```
# [Wed Dec  7 21:38:03 2011] Protocol: http, Server: 192.168.5.10:9200
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "mappings" : {
      "_default_" : {
         "properties" : {
            "bar" : {
               "type" : "string"
            }
         }
      }
   }
}
'

# [Wed Dec  7 21:38:03 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Wed Dec  7 21:38:06 2011] Protocol: http, Server: 192.168.5.10:9200
curl -XPUT 'http://127.0.0.1:9200/foo/bar/1?pretty=1'  -d '
{
   "bar" : "aaa"
}
'

# [Wed Dec  7 21:38:06 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "1",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Wed Dec  7 21:38:09 2011] Protocol: http, Server: 192.168.5.10:9200
curl -XGET 'http://127.0.0.1:9200/foo/_mapping?pretty=1' 

# [Wed Dec  7 21:38:09 2011] Response:
# {
#    "foo" : {
#       "_default_" : {
#          "properties" : {
#             "bar" : {
#                "type" : "string"
#             }
#          }
#       }
#    }
# }
```
</description><key id="2479752">1524</key><summary>Default mapping means new types not mapped explicitly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2011-12-07T20:39:55Z</created><updated>2011-12-07T21:17:38Z</updated><resolved>2011-12-07T21:17:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file></files><comments><comment>Default mapping means new types not mapped explicitly, closes #1524.</comment></comments></commit></commits></item><item><title>Move project build to maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1523</link><project id="" key="" /><description>Move to maven for the build system. Since we moved all the plugins, we can use a single module (maven crap with multi module), which makes things simpler.
- Creating a distribution using `mvn assembly:assembly -DskipTests`, it will be under `target/releases`.
</description><key id="2462366">1523</key><summary>Move project build to maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.19.0.RC1</label></labels><created>2011-12-06T14:04:55Z</created><updated>2012-05-14T19:10:52Z</updated><resolved>2011-12-06T14:05:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-06T14:05:19Z" id="3032596">Implemented, leftover is migrate the deb packaging from 0.18 branch to master using maven.
</comment><comment author="clintongormley" created="2011-12-07T17:51:43Z" id="3050640">Hiya

I tried building with maven - a couple of things:
1) my `mvn` command turned out to be `mvn3`
2) maven complained of the following:

```
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.elasticsearch:elasticsearch:jar:0.19.0-SNAPSHOT
[WARNING] 'dependencies.dependency.systemPath' for sigar:sigar:jar should not point at files within the project directory, ${basedir}/lib/sigar/sigar-1.6.4.jar will be unresolvable by dependent projects @ line 213, column 25
[WARNING]
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING]
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING]
```

but it built successfully
</comment><comment author="kimchy" created="2011-12-07T18:19:49Z" id="3051090">Cool, btw, that warning is "ok".
</comment><comment author="karussell" created="2011-12-07T23:53:47Z" id="3055673">Yeah, build is here ok too. Really nice :) BTW: what made you switching?

Some tests are failing (not sure if related to maven).

testSnapshotOperations \* 2
  assertThat(clusterState.state().metaData().index("test").mapping("type1"), notNullValue());

testRequiredRoutingMappingWithAlias
  assertThat(client.prepareGet("test", "type1", "1").setRouting("0").execute().actionGet().exists(), equalTo(false));

testScriptFieldUsingSource
  assertThat("Failures " + Arrays.toString(response.shardFailures()), response.shardFailures().length, equalTo(0));

testFiltersWithCustomeCacheKey
  assertThat("Failures " + Arrays.toString(searchResponse.shardFailures()), searchResponse.shardFailures().length, equalTo(0));
</comment><comment author="kimchy" created="2011-12-08T09:32:26Z" id="3059787">@karussell switched because gradle made some changes in latest version that make it harder to get started without IDE support. Also, since I moved all the plugins to their own repos, then we can have a single module, and I never had major problems with maven on single module projects, its the multi module ones that caused me pain.
</comment><comment author="karussell" created="2011-12-08T09:34:33Z" id="3059808">Ah, ok.

I'm now done with the migration of the deb pkg and ElasticSearch even starts ;)

Pull request is coming ...
</comment><comment author="oravecz" created="2012-05-14T19:10:52Z" id="5699227">So, are the plugins still hosted on Maven somewhere? Can't find them in their pre-0.19 location.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Lithuanian language for snowball stemmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1522</link><project id="" key="" /><description>java class is located here:
https://github.com/emilis/PolicyFeed/blob/master/src/search/java/org/tartarus/snowball/ext/LithuanianStemmer.java
</description><key id="2461981">1522</key><summary>Add Lithuanian language for snowball stemmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xawiers</reporter><labels /><created>2011-12-06T13:20:57Z</created><updated>2014-05-09T05:04:12Z</updated><resolved>2013-05-22T14:46:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-22T14:46:21Z" id="18283289">You can add this stemmer by building a plugin, no need to wait until it is included in the lucene analyzers (every stemmer in this package is exposed in elasticsearch) jar or in elasticsearch itself (currently we do not ship any custom stemmers in the core package and try to stay clean here).
</comment><comment author="optimum-dulopin" created="2014-05-09T04:58:31Z" id="42633581">Hi,

Im' searchinkg to ass new stemmer to elastisearch to use with tire / rails

I've downloaded found java file
I've created a jar from this file
I've put it in elasticsearch's lib folder

here my rails file

tire.settings :analysis =&gt; {
                :filter =&gt; {
                  "lt_stemmer" =&gt; {
                    "type" =&gt; "stemmer",
                    "name" =&gt; "lithuanian",
                    "rules_path" =&gt; "lithuanian_stemmer.jar"
                  }
                },
                :analyzer =&gt; {
                  "lithuanian" =&gt; {
                  "type" =&gt; "snowball",
                  "tokenizer" =&gt; "keyword",
                  "filter" =&gt; ["lowercase", "lt_stemmer"]
                },
            },
  } do
  mapping do
    indexes :titre_lt, :analyzer =&gt; "lithuanian"

  end

I succeed them to create index and index data, but when I test, it seems it doesn't use the rule in my jar file.

curl -XGET 'localhost:9200/lituanieindex/_analyze?analyzer=lithuanian' -d 'smulkių, dalinių, pilnų krovinių pervežimas nuosavais arba partnerių vilkikais su standartinėmis 92 m3 puspriekabėmis ir 120 m3 autotraukiniais;'

{"tokens":[{"token":"smulkių","start_offset":0,"end_offset":7,"type":"&lt;ALPHANUM&gt;","position":1},{"token":"dalinių","start_offset":9,"end_offset":16,"type":"&lt;ALPHANUM&gt;","position":2},{"token":"pilnų","start_offset":18,"end_offset":23,"type":"&lt;ALPHANUM&gt;","position":3},{"token":"krovinių","start_offset":24,"end_offset":32,"type":"&lt;ALPHANUM&gt;","position":4},{"token":"pervežima","start_offset":33,"end_offset":43,"type":"&lt;ALPHANUM&gt;","position":5},{"token":"nuosavai","start_offset":44,"end_offset":53,"type":"&lt;ALPHANUM&gt;","position":6},{"token":"arba","start_offset":54,"end_offset":58,"type":"&lt;ALPHANUM&gt;","position":7},{"token":"partnerių","start_offset":59,"end_offset":68,"type":"&lt;ALPHANUM&gt;","position":8},{"token":"vilkikai","start_offset":69,"end_offset":78,"type":"&lt;ALPHANUM&gt;","position":9},{"token":"su","start_offset":79,"end_offset":81,"type":"&lt;ALPHANUM&gt;","position":10},{"token":"standartinėmi","start_offset":82,"end_offset":96,"type":"&lt;ALPHANUM&gt;","position":11},{"token":"92","start_offset":97,"end_offset":99,"type":"&lt;NUM&gt;","position":12},{"token":"m3","start_offset":100,"end_offset":102,"type":"&lt;ALPHANUM&gt;","position":13},{"token":"puspriekabėmi","start_offset":103,"end_offset":117,"type":"&lt;ALPHANUM&gt;","position":14},{"token":"ir","start_offset":118,"end_offset":120,"type":"&lt;ALPHANUM&gt;","position":15},{"token":"120","start_offset":121,"end_offset":124,"type":"&lt;NUM&gt;","position":16},{"token":"m3","start_offset":125,"end_offset":127,"type":"&lt;ALPHANUM&gt;","position":17},{"token":"autotraukiniai","start_offset":128,"end_offset":143,"type":"&lt;ALPHANUM&gt;","position":18}]}

what do I do wrong ?

thanks for help
</comment><comment author="dadoonet" created="2014-05-09T05:03:54Z" id="42633771">Hey @optimum-dulopin 

You already posted your question on the english mailing list and french one (where I answered).

It's definitely the best place to ask for questions.

Issues on github are more for issues and pull requests.

Let's keep the information and questions on a single place.

Thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DELETE by query does not accept top-level query wrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1521</link><project id="" key="" /><description>The `/_search` interface is inconsistent:

```
GET /_search {query:{term:{id:123}}}
DELETE /_search {term:{id:123}}
```

Using a query wrapper with a DELETE reports failures but no error message indicating why it did not work.
</description><key id="2460110">1521</key><summary>DELETE by query does not accept top-level query wrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tmm1</reporter><labels /><created>2011-12-06T09:18:24Z</created><updated>2013-07-05T08:45:28Z</updated><resolved>2013-07-05T08:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T08:45:28Z" id="20507159">Hey,

A delete-by-*_query_ just needs a query for deletion, whereas a search can contain other top-level fields (like `facets`, `sort`, `fields`) which are useless for delete-by-query. That's the reason for being able to remove the query part for a delete.

However it makes sense to return a more descriptive error message in case a delete-by query starts with a `query` field instead of the query itself.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrate built in plugins to their own repos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1520</link><project id="" key="" /><description>Move the built in plugins to their own repos. For example, `mapper-attachments` moved to https://github.com/elasticsearch/elasticsearch-mapper-attachments. This also means new installation path, for `mapper-attachments`, the installation is: `bin/plugin -install elasticsearch/elasticsearch-mapper-attachments/1.0.0`.

All plugins will start with version `1.0.0`, and have compatibility matrix between the plugin version and the elasticsearch version supported. All plugins will work with `0.18.5` and are recommended to be installed from their new repos compared to using the shorthand notion.

This move will help both with more samples to how to build external plugins, and simplify the elasticsearch project. Note, all the plugins are using maven as the build file to simplify people using it.
</description><key id="2452832">1520</key><summary>Migrate built in plugins to their own repos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.19.0.RC1</label></labels><created>2011-12-05T18:40:42Z</created><updated>2011-12-05T19:00:30Z</updated><resolved>2011-12-05T19:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2011-12-05T18:47:36Z" id="3021063">Great news !
Will contribute there now...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuAnalysisBinderProcessor.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuFoldingTokenFilterFactory.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/plugin/analysis/icu/AnalysisICUPlugin.java</file><file>plugins/analysis/icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuAnalysisTests.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsModule.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsSettingsFilter.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/AbstractS3BlobContainer.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3ImmutableBlobContainer.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2DiscoveryModule.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/gateway/s3/S3Gateway.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/gateway/s3/S3GatewayModule.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/index/gateway/s3/S3IndexGateway.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/index/gateway/s3/S3IndexGatewayModule.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/index/gateway/s3/S3IndexShardGateway.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/plugin/cloud/aws/CloudAwsPlugin.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/common/blobstore/hdfs/AbstractHdfsBlobContainer.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/common/blobstore/hdfs/HdfsBlobStore.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/common/blobstore/hdfs/HdfsImmutableBlobContainer.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/gateway/hdfs/HdfsGateway.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/gateway/hdfs/HdfsGatewayModule.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/gateway/hdfs/HdfsIndexGatewayModule.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/index/gateway/hdfs/HdfsIndexGateway.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/index/gateway/hdfs/HdfsIndexShardGateway.java</file><file>plugins/hadoop/src/main/java/org/elasticsearch/plugin/hadoop/HadoopPlugin.java</file><file>plugins/hadoop/src/test/java/org/elasticsearch/hadoop/gateway/HdfsGatewayTests.java</file><file>plugins/lang/groovy/src/main/groovy/org/elasticsearch/groovy/client/action/GActionFuture.java</file><file>plugins/lang/groovy/src/main/java/org/elasticsearch/plugin/groovy/GroovyPlugin.java</file><file>plugins/lang/groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java</file><file>plugins/lang/groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptMultiThreadedTest.java</file><file>plugins/lang/groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptSearchTests.java</file><file>plugins/lang/groovy/src/test/java/org/elasticsearch/script/groovy/SimpleBench.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/plugin/javascript/JavaScriptPlugin.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/support/NativeList.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/support/NativeMap.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptValueConverter.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptableLinkedHashMap.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptableMap.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/support/ScriptableWrappedMap.java</file><file>plugins/lang/javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java</file><file>plugins/lang/javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTest.java</file><file>plugins/lang/javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptSearchTests.java</file><file>plugins/lang/javascript/src/test/java/org/elasticsearch/script/javascript/SimpleBench.java</file><file>plugins/lang/python/src/main/java/org/elasticsearch/plugin/python/PythonPlugin.java</file><file>plugins/lang/python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java</file><file>plugins/lang/python/src/test/java/org/elasticsearch/script/python/PythonScriptEngineTests.java</file><file>plugins/lang/python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTest.java</file><file>plugins/lang/python/src/test/java/org/elasticsearch/script/python/PythonScriptSearchTests.java</file><file>plugins/lang/python/src/test/java/org/elasticsearch/script/python/SimpleBench.java</file><file>plugins/mapper/attachments/src/main/java/org/elasticsearch/index/mapper/attachment/AttachmentMapper.java</file><file>plugins/mapper/attachments/src/main/java/org/elasticsearch/index/mapper/attachment/RegisterAttachmentType.java</file><file>plugins/mapper/attachments/src/main/java/org/elasticsearch/plugin/mapper/attachments/AttachmentsIndexModule.java</file><file>plugins/mapper/attachments/src/main/java/org/elasticsearch/plugin/mapper/attachments/MapperAttachmentsPlugin.java</file><file>plugins/mapper/attachments/src/main/java/org/elasticsearch/plugin/mapper/attachments/tika/TikaInstance.java</file><file>plugins/mapper/attachments/src/test/java/org/elasticsearch/index/mapper/xcontent/SimpleAttachmentMapperTests.java</file><file>plugins/mapper/attachments/src/test/java/org/elasticsearch/plugin/mapper/attachments/test/SimpleAttachmentIntegrationTests.java</file><file>plugins/river/couchdb/src/main/java/org/elasticsearch/plugin/river/couchdb/CouchdbRiverPlugin.java</file><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiverModule.java</file><file>plugins/river/couchdb/src/test/java/org/elasticsearch/river/couchdb/CouchdbRiverAttachementTest.java</file><file>plugins/river/couchdb/src/test/java/org/elasticsearch/river/couchdb/CouchdbRiverTest.java</file><file>plugins/river/rabbitmq/src/main/java/org/elasticsearch/plugin/river/rabbitmq/RabbitmqRiverPlugin.java</file><file>plugins/river/rabbitmq/src/main/java/org/elasticsearch/river/rabbitmq/RabbitmqRiver.java</file><file>plugins/river/rabbitmq/src/main/java/org/elasticsearch/river/rabbitmq/RabbitmqRiverModule.java</file><file>plugins/river/rabbitmq/src/test/java/org/elasticsearch/river/rabbitmq/RabbitMQRiverTest.java</file><file>plugins/river/twitter/src/main/java/org/elasticsearch/plugin/river/twitter/TwitterRiverPlugin.java</file><file>plugins/river/twitter/src/main/java/org/elasticsearch/river/twitter/TwitterRiver.java</file><file>plugins/river/twitter/src/main/java/org/elasticsearch/river/twitter/TwitterRiverModule.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/plugin/river/wikipedia/WikipediaRiverPlugin.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiver.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/WikipediaRiverModule.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/InfoBox.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/IteratorHandler.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/PageCallbackHandler.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/SAXPageCallbackHandler.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/WikiPage.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/WikiPageIterator.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/WikiTextParser.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/WikiXMLParser.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/WikiXMLParserFactory.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/WikiXMLSAXParser.java</file><file>plugins/river/wikipedia/src/main/java/org/elasticsearch/river/wikipedia/support/package-info.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/MemcachedRestRequest.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/MemcachedServer.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/MemcachedServerModule.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/MemcachedServerTransport.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/MemcachedTransportException.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/netty/MemcachedDecoder.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/netty/MemcachedDispatcher.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/netty/MemcachedRestChannel.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/netty/NettyMemcachedServerTransport.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/netty/NettyMemcachedServerTransportModule.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/plugin/transport/memcached/MemcachedTransportPlugin.java</file><file>plugins/transport/memcached/src/test/java/org/elasticsearch/memcached/test/AbstractMemcachedActionsTests.java</file><file>plugins/transport/memcached/src/test/java/org/elasticsearch/memcached/test/BinaryMemcachedActionTests.java</file><file>plugins/transport/memcached/src/test/java/org/elasticsearch/memcached/test/TextMemcachedActionTests.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/Method.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/Rest.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/RestRequest.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/RestResponse.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/Status.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/plugin/transport/thrift/ThriftTransportPlugin.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftRestImpl.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftRestRequest.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftServer.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftServerModule.java</file><file>plugins/transport/thrift/src/test/java/org/elasticsearch/thrift/test/SimpleThriftTests.java</file><file>plugins/transport/wares/src/main/java/org/elasticsearch/wares/NodeServlet.java</file><file>plugins/transport/wares/src/main/java/org/elasticsearch/wares/ServletRestRequest.java</file></files><comments><comment>Migrate built in plugins to their own repos, closes #1520.</comment></comments></commit></commits></item><item><title>Analysis: Add arabic, brazilian, czech to stemmer token filter language options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1519</link><project id="" key="" /><description /><key id="2442060">1519</key><summary>Analysis: Add arabic, brazilian, czech to stemmer token filter language options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-04T14:36:19Z</created><updated>2011-12-04T14:36:57Z</updated><resolved>2011-12-04T14:36:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java</file></files><comments><comment>Analysis: Add arabic, brazilian, czech to stemmer token filter language options, closes #1519.</comment></comments></commit></commits></item><item><title>percolate response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1518</link><project id="" key="" /><description>I am mapping the percolate responses for NEST 

```
{
  "ok" : true,
  "_index" : "_percolator",
  "_type" : "nest_test_data",
  "_id" : "somename",
  "_version" : 2
}
```

I have the urge to map `_type` to `.Index` and `_id` to `.Name` on the response object. But  I am not entirely sure if that'll solve  or introduce more confusion.

Thoughts?
</description><key id="2441452">1518</key><summary>percolate response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2011-12-04T11:03:28Z</created><updated>2012-12-28T19:06:55Z</updated><resolved>2012-12-28T19:06:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Version missing a space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1517</link><project id="" key="" /><description>The version string should have a space before 'JVM'

```
bin/elasticsearch -v
ElasticSearch Version: 0.18.5JVM: 20.4-b02
```
</description><key id="2430381">1517</key><summary>Version missing a space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-02T17:00:03Z</created><updated>2011-12-04T10:48:45Z</updated><resolved>2011-12-04T10:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/Version.java</file></files><comments><comment>Version missing a space, closes #1517.</comment></comments></commit></commits></item><item><title>search missing from stats when using clear and search params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1516</link><project id="" key="" /><description>I am getting search stats back on:
http://127.0.0.1.:9200/_stats?pretty=true 
but if i clear the flags and add it back manually it'll come up empty:
http://127.0.0.1:9200/_stats?pretty=true&amp;clear=true&amp;search=true

```
{
    ok: true,
    _shards: {
    total: 60,
    successful: 30,
    failed: 0
},
_all: {
    primaries: { },
    total: { },
    indices: {
    80f4f2ae-edb2-423b-b033-f00a73ec2157: {
        primaries: { },
        total: { }
    },
    db7bf88d-877c-4e5a-822a-e3fe8f38be1e: {
        primaries: { },
        total: { }
    },
    e62a8cfa-37c6-46c3-9a20-b3ad6b25417b: {
        primaries: { },
        total: { }
    },
    edc9c9bb-eb23-4949-83b9-fd5a9fdd925c: {
        primaries: { },
        total: { }
    },
    nest_test_data_clone: {
        primaries: { },
        total: { }
    },
    nest_test_data: {
        primaries: { },
        total: { }
    }
}
}
}
```
</description><key id="2419619">1516</key><summary>search missing from stats when using clear and search params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>bug</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-12-01T20:46:51Z</created><updated>2011-12-02T10:33:08Z</updated><resolved>2011-12-02T10:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-02T10:32:28Z" id="2988525">Thats a bug, you can use `/_stats/search` for now.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file></files><comments><comment>search missing from stats when using clear and search params, closes #1516.</comment></comments></commit></commits></item><item><title>Bug Slf4j with log4j-over-slf4j in ESLoggerFactory =&gt; java.lang.NoSuchMethodError: org.apache.log4j.Logger.setLevel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1515</link><project id="" key="" /><description>Hello, 

With this combination of slf4j-api +   logback + log4j-over-slf4j,
 the ESLoggerFactory was select (as wrong)  an Log4jESLoggerFactory.

As you known  log4j-over-slf4j,  is just a wrapper of log4j api in logback logger,  to catch it. 
That not good to log by log4j redirection, in this case.

In more, the result is an exception with method Log4jESLogger.setLevel(Log4jESLogger.java:48 

Why did you open the isssue https://github.com/elasticsearch/elasticsearch/issues/1265 , that create this bug on  August 20, 2011 ?  

Could you revert the issue a83c45b ?

cf exception
14:17:26.781 [elasticsearch[Colonel America]clusterService#updateTask-pool-11-thread-1] WARN  org.elasticsearch.indices.cluster - [Colonel America] [phonetic][0] failed to create shard
org.elasticsearch.index.shard.IndexShardCreationException: [phonetic][0] failed to create shard
    at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:300) [elasticsearch-0.18.5.jar:na]
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:532) [elasticsearch-0.18.5.jar:na]
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:501) [elasticsearch-0.18.5.jar:na]
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:173) [elasticsearch-0.18.5.jar:na]
    at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:271) [elasticsearch-0.18.5.jar:na]
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_27]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_27]
    at java.lang.Thread.run(Thread.java:662) [na:1.6.0_27]
Caused by: java.lang.NoSuchMethodError: org.apache.log4j.Logger.setLevel(Lorg/apache/log4j/Level;)V
    at org.elasticsearch.common.logging.log4j.Log4jESLogger.setLevel(Log4jESLogger.java:48) [elasticsearch-0.18.5.jar:na]
    at org.elasticsearch.index.search.slowlog.ShardSlowLogSearchService.&lt;init&gt;(ShardSlowLogSearchService.java:144) [elasticsearch-0.18.5.jar:na]
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) [na:1.6.0_27]

Thank you

Jerome
</description><key id="2414513">1515</key><summary>Bug Slf4j with log4j-over-slf4j in ESLoggerFactory =&gt; java.lang.NoSuchMethodError: org.apache.log4j.Logger.setLevel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmorille</reporter><labels /><created>2011-12-01T13:38:45Z</created><updated>2013-06-26T16:08:38Z</updated><resolved>2013-06-26T16:08:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-01T14:22:11Z" id="2975150">The reason for the change to default to log4j was that users were using log4j (config and all), and were using slf4j adapter to log4j, not log4j to slf4j.
</comment><comment author="jmorille" created="2011-12-01T16:51:51Z" id="2977026">II don't understand

If a user choose :
- slf4j-api + logback logger adapter  implementation =&gt; you have to select Slf4jESLoggerFactory (with all other implementation wrapper)
- slf4j-api + log4j-over-slf4j + logback logger adapter  implementation =&gt; you have to select Slf4jESLoggerFactory (with all other implementation wrapper) because Is a logger step more.
- just Log4j logger adapter  implementation =&gt; you have to select Log4jESLoggerFactory
- slf4j-api + log4j logger adapter implementation =&gt; you have to select Slf4jESLoggerFactory

In the slf4j-api + log4j adapter config, is more normal to not use the shortcut in logging (directly log to log4j) if the users wish this configuration (in the libs) by the indirection to slf4j.

And it create a Bug with my wishes config by  blocking  the index creation, in embeded elastic mode and in the unit testing.

Do you agree with my point of view ?
</comment><comment author="kimchy" created="2011-12-01T17:19:43Z" id="2977417">I think the more common case is to have slf4j to log4j, with log4j config, and in this case, elasticsearch should use log4j directly and not go over to slf4j.
</comment><comment author="jmorille" created="2011-12-08T10:38:10Z" id="3060691">I am not sure that log4j was popular any more. 
ie some talk thread found on google
http://stackoverflow.com/questions/4165558/best-practices-for-using-markers-in-slf4j-logback
http://stackoverflow.com/questions/178836/should-new-projects-use-logback-instead-of-log4j
http://stackoverflow.com/questions/178215/log4j-vs-logback

For me I switch to logback 3 years ago, http://logback.qos.ch/reasonsToSwitch.html
Mainly because logback speaks SLF4J natively (so for performance issue, even I don't do a benchmark to proove it)
and for some cool feature that I don't have in log4j
Ceki Gülcü, leave log4j project in order to refactor as logback.
The project log4j, seams to not have a lot of activity now, and for me is a deprecated project.

But is not very important the logger implementation that you or me prefer.
The point is that in the configuration (slf4j-api + logback + log4j-over-slf4j) I have a bug with your optimisation of logging directly in log4j with sl4f-api, when you call the method  Log4jESLogger.setLevel(Log4jESLogger.java:48.
that not available in my configuration.

To correct this bug,
-  my preference go to rollback issue a83c45b, is appear to me more logic.
  But another way, is to 
- find another way to detect log4j implementation
- or to introduce a new parameter in ES configuration to force the one or another solution (Log4jESLoggerFactory or Slf4jESLoggerFactory)

This simple bug, is a blocking bug for me in order to introduce ES as search solution in the company framework.

What solution do you prefer ?

I could send you a small junit testing to reproduce the bug, if you think that was necessary ?
</comment><comment author="kimchy" created="2011-12-08T11:47:28Z" id="3061428">Why is it a blocking bug? You can work around it by setting the logging factory yourself to slf4j in your code. The mentioned change is not going to be rolled back because of the reason I gave you. I would say the problem is in slf4j that is not wrapping log4j properly. 

And, on the question of logging. I wish slf4j would never have happened, but it has, and people seem to use it, and good for them. logback is not BSD but LGPL, I am not going to rely on it in elasticsearch. Will do my best to help integrate with slf4j, but the ability to set the level is important (and lost with slf4j) since it allows to dynamically set, in runtime, logging level through an API.
</comment><comment author="karussell" created="2011-12-14T13:12:39Z" id="3139254">&gt; logback

it is also EPL

&gt; I am not sure that log4j was popular any more. 

BTW: there is even log4j v2 :) from Ralph Goers

http://logging.apache.org/log4j/2.0/index.html

http://www.mail-archive.com/dev@commons.apache.org/msg23522.html
</comment><comment author="kimchy" created="2011-12-26T14:29:18Z" id="3274801">I've improved the internal logging detection to detect if the case is that slf4j is wrapping log4j (thus reducing features) and then default to using slf4j.
</comment><comment author="jmorille" created="2012-01-12T14:44:47Z" id="3464186">Sorry the laps, just come back for holiday.

To answers to you, Yes it is a blocking bug for us.
I going to look you're improvement.
</comment><comment author="kimchy" created="2012-01-12T22:07:51Z" id="3471442">Why is it blocking? You can easily configure elasticsearch to use slf4j.
</comment><comment author="spinscale" created="2013-06-26T16:08:38Z" id="20059756">Closing. Happy to reopen with more information of what can be improved or how to help elsewhere.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix escaping of arguments with paths in the bash script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1514</link><project id="" key="" /><description>The bash script does not currently escape the paths it passes to the Java executable. This causes unexpected results when the path contains special characters (e.g. whitespace). This patch quotes `$es_parms` and `$ES_CLASSPATH` to pass the paths properly.

For example, if the path to the directory where we've extracted elasticsearch is `/some path/leading to/elasticsearch`, we get an error like this (note how the path gets split up at the spaces):

```
Exception in thread "main" java.lang.NoClassDefFoundError: path/leading
Caused by: java.lang.ClassNotFoundException: path.leading
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: path/leading.  Program will exit.
```
</description><key id="2407702">1514</key><summary>Fix escaping of arguments with paths in the bash script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrtorrent</reporter><labels /><created>2011-11-30T21:45:33Z</created><updated>2014-07-03T07:01:00Z</updated><resolved>2011-12-01T15:06:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-12-01T15:06:04Z" id="2975672">The fix was problematic (have you tested it)? I pushed a better fix.
</comment><comment author="mrtorrent" created="2011-12-01T15:32:52Z" id="2975993">I did test it and actually first tried the approach you've taken in your commit, but it doesn't work. What problems did you see with the fix I went with?

Here's the exception thrown with your fix (a bit different than before):

```
Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/Version
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.Version
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.elasticsearch.Version.  Program will exit.
```
</comment><comment author="mrtorrent" created="2011-12-01T15:36:35Z" id="2976052">Sorry, that exception is of course with the `-v` switch. When actually running the app, the exception is:

```
Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/ElasticSearch
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.bootstrap.ElasticSearch
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.elasticsearch.bootstrap.ElasticSearch.  Program will exit.
```
</comment><comment author="mrtorrent" created="2011-12-01T15:51:26Z" id="2976243">Right, very sorry, this was just me being stupid -- I was testing your fix there in a fresh checkout and forgot to build the JARs. Your fix looks good, thank you.
</comment><comment author="kimchy" created="2011-12-01T17:20:41Z" id="2977429">No problem :), the problem that I had was that es_params was not being passed correctly with you quote it.
</comment><comment author="magedmakled" created="2012-03-24T03:00:59Z" id="4671269">I just install it using homebrew version 0.19.0 and I got the above error, any suggestions ? 
</comment><comment author="kimchy" created="2012-03-25T18:23:49Z" id="4683377">@magedmakled Can you add an "echo" to the elasticsearch script where it executes the java command, and gist it / post it here?
</comment><comment author="magedmakled" created="2012-03-28T21:17:25Z" id="4789732">@kimchy where exactly would you like that? I think the line that is causing that is  

exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS $es_parms -Des.path.home="$ES_HOME" -cp "$ES_CLASSPATH" $props \
                org.elasticsearch.bootstrap.ElasticSearch
</comment><comment author="rtlong" created="2012-03-31T02:30:36Z" id="4854677">@magedmakled I would suggest upgrading to 0.19.1. It was released recently. No guarantee that it will help, but it might.
</comment><comment author="magedmakled" created="2012-04-02T01:46:45Z" id="4871284">same results with 0.19.1
</comment><comment author="rtlong" created="2012-04-02T02:08:09Z" id="4871409">@magedmakled Try what @kimchy said then. I'd agree that the line you showed is 'where it executes the java command'. He needs you to add an echo, there, to see how the shell is interpreting that command. Add `echo` to the start of the line and try to start ES again, then paste the output here.
</comment><comment author="magedmakled" created="2012-04-02T02:21:24Z" id="4871487">exec /Library/Java/Home/bin/java -Xms256m -Xmx1g -Xss128k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.foreground=yes -Des.path.home=/usr/local/Cellar/elasticsearch/0.19.0 -cp :/usr/local/Cellar/elasticsearch/0.19.0/libexec/_:/usr/local/Cellar/elasticsearch/0.19.0/libexec/sigar/_ -Des.config=./config/development/elasticsearch/elasticsearch.yml org.elasticsearch.bootstrap.ElasticSearch
</comment><comment author="rtlong" created="2012-04-02T02:48:36Z" id="4871671">@kimchy Here's the config file ([./config/development/elasticsearch/elasticsearch.yml](https://gist.github.com/2280127)) @magedmakled and I are using, which was based on a 0.18.\* configuration; I haven't had time to verify it's perfectly compatible with 0.19.\* yet. That said, ElasticSearch 0.19.1, using that configuration, runs perfectly fine for me on Ubuntu. 

```
$ java -version
java version "1.7.0_01"
Java(TM) SE Runtime Environment (build 1.7.0_01-b08)
Java HotSpot(TM) 64-Bit Server VM (build 21.1-b02, mixed mode)
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve highlighting perf (a bit) by reusing some constructs across hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1513</link><project id="" key="" /><description /><key id="2404482">1513</key><summary>Improve highlighting perf (a bit) by reusing some constructs across hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-11-30T17:15:00Z</created><updated>2011-11-30T17:15:44Z</updated><resolved>2011-11-30T17:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/FetchSubPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Improve highlighting perf (a bit) by reusing some constructs across hits, closes #1513.</comment></comments></commit></commits></item><item><title>CouchDB river : Add support for views</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1512</link><project id="" key="" /><description>Real implementation of pull request #1258, as there was a problem with GitHub.

In CouchDB, you can retrieve docs by `GET`, `_changes` API and views.
CouchDB river uses `_changes` API to get documents.

I would like to be able to get documents that changed (getting ID with the _changes API) using a view with parameter `key="DOCID"`.

As views return a collection of results (aka rows), we will index in ES each row with an id like DOCID_seq where seq is the sequence number of each row.
If you get back 3 rows for one single change for document with ID=1234, the river will index 3 documents :
- 1234_1
- 1234_2
- 1234_3

To use it, you have to define a view in couchDB. For instance, `_design/vues/_view/test_dpi` with 

``` javascript
function(doc) {
listArt=doc.document.articles;

for(var i=0; i&lt;listArt.length;i++) {  
 var artJson = {};
 artJson = { 'docid' : doc._id, 'num' : listArt[i].numeroArticle };
 artJson =  JSON.stringify( artJson );
 emit(doc._id, eval('('+artJson+')') );
};
}
```

You can use it in your couchDb river as follow :

``` javascript
{
  "type":"couchdb",
  "couchdb": {
    "host":"localhost",
    "port":"5984",
    "db":"dau_test",
    "view":"vues/_view/test_dpi",
    "viewIgnoreRemove":false
  }
}
```

New options :
- `view` : if not null, couchDB river will not fetch content from `_changes` API but only IDs and then will use the view to retrieve rows using the ID as a key. By default : null
- `viewIgnoreRemove` : ask the river to ignore removal of rows if there is less rows after a document update. By default : false so non existing rows will be removed from elastic search.

For example, with the 3 rows described earlier, if you push a new version of the document 1234 in couchDB with only 2 docs, 

If `viewIgnoreRemove` is false (default), then
- 1234_1 will be updated
- 1234_2 will be updated
- 1234_3 will be removed

If `viewIgnoreRemove` is true, then
- 1234_1 will be updated
- 1234_2 will be updated
- 1234_3 will not be updated

BTW, I will push an update to the pull request when the `ids_prefix` filter will be available to make code more efficient. (See issue #1259).

Thanks
David
</description><key id="2394391">1512</key><summary>CouchDB river : Add support for views</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-11-29T22:21:07Z</created><updated>2011-12-05T19:26:15Z</updated><resolved>2011-12-05T19:26:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2011-12-05T19:26:15Z" id="3021717">Moved here : https://github.com/elasticsearch/elasticsearch-river-couchdb/issues/1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: Bool filter does not take should clauses properly into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1511</link><project id="" key="" /><description>If all should clauses do not match any doc, it will cause the filter to not match any doc, but instead, ignore them.
</description><key id="2390789">1511</key><summary>Query DSL: Bool filter does not take should clauses properly into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.6</label><label>v0.19.0.RC1</label></labels><created>2011-11-29T19:23:18Z</created><updated>2011-11-29T19:23:58Z</updated><resolved>2011-11-29T19:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file></files><comments><comment>Query DSL: Bool filter does not take should clauses properly into account, closes #1511.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file></files><comments><comment>Query DSL: Bool filter does not take should clauses properly into account, closes #1511.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file></files><comments><comment>Query DSL: Bool filter does not take should clauses properly into account, closes #1511.</comment></comments></commit></commits></item><item><title>Weird behaviour and unexpected exceptions with nested queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1510</link><project id="" key="" /><description>I've met with some exceptions trying to execute some nested queries. It's an erratic bug, because sometimes it works fine and other it fails. I've tried it in a fresh ElasticSearch version 0.18.5.

The following commands will reproduce the exceptions sometimes. Most of the times the last two queries (nested query and mixed query) work fine, but suddenly they will fail.
# First we delete indices

curl -XDELETE 'http://localhost:9200/users/'
curl -XDELETE 'http://localhost:9200/entities/'
# First index creation (users)

curl -XPUT 'http://localhost:9200/users/' -d '
index :
    number_of_shards : 10
    number_of_replicas : 0
'
# Insert document in first index (users)

curl -XPUT http://localhost:9200/users/user/1 -d '{
    "name": "John",
    "surname": "Doe"
}'
# Second index creation (entities)

curl -XPUT 'http://localhost:9200/entities/' -d '
index :
    number_of_shards : 10
    number_of_replicas : 0
'
# Create Nested Mapping in "entities" index called "nestedtest"

curl -XPUT 'http://localhost:9200/entities/nestedtest/_mapping' -d '
{
    "tweet" : {
        "properties" : {
            "name" : {"type" : "string", "store" : "yes"},
            "tweets": {"type": "nested"}
        }
    }
}
'
# Created Not-Nested Mapping in "entities" index called "notnestedtest"

curl -XPUT 'http://localhost:9200/entities/notnestedtest/_mapping' -d '
{
    "tweet" : {
        "properties" : {
            "name" : {"type" : "string", "store" : "yes"},
            "text" : {"type" : "string", "store" : "yes"}
        }
    }
}
'
# Insert document of type nestedtest

curl -XPUT http://localhost:9200/entities/nestedtest/1 -d '{
    "name": "document1",
    "tweets": [{"message": "this is a test message", "author":"author1"}, 
               {"message": "second message", "author":"author2"}]
}'
# Insert document of type notnestedtest

curl -XPUT http://localhost:9200/entities/notnestedtest/2 -d '{
    "name": "document2",
    "text": "Text of document2"
}'
# Nested query

curl -XGET http://localhost:9200/entities/nestedtest/_search?pretty=true -d '{
    "query" : {
        "nested" : {
            "path" : "tweets",
            "score_mode" : "avg",
            "query" : {
                "bool" : {
                    "must" : [
                        {
                            "text" : {"tweets.message" : "message"}
                        },
                        {
                            "text" : {"author" : "author2"}
                        }
                    ]
                }
            }
        }
    }
}'
# Mixed nested - not nested query

curl -XGET http://localhost:9200/entities/_search?pretty=true -d '{
    "query" : {
                "bool" : {
                    "should" : [
                        {
                            "text" : {"name" : "document2"}
                        },
                        {
                    "nested" : {
                            "path" : "tweets",
                            "score_mode" : "avg",
                            "query" : {
                                "bool" : {
                                    "must" : [
                                        {
                                            "text" : {"tweets.message" : "second"}
                                        },
                                        {
                                            "text" : {"tweets.message" : "message"}
                                        },
                                        {
                                            "text" : {"author" : "author2"}
                                        }
                                    ]
                                }
                            }
                    }
                        }
                    ]
                }
    }
}'
# Exceptions

The resulting exceptions are similar to the next:
...
    "failures" : [ {
      "index" : "entities",
      "shard" : 4,
      "status" : 500,
      "reason" : "QueryPhaseExecutionException[[entities][4]: query[+text:texto +BlockJoinQuery (filtered(+tweets.tweet_lists:[2 TO 2] +tweets.tweet_author:ogrisel)-&gt;FilterCacheFilterWrapper(_type:__tweets))],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: "
    } ]
  },
...
# Errors returned by ElasticSearch are similar to the next:

[2011-11-29 16:43:55,544][DEBUG][action.search.type       ] [eiffel1] [entities][4], node[Dzm9UI1NQHCeDiwRuVfkvA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@2c2166f2]
org.elasticsearch.search.query.QueryPhaseExecutionException: [entities][4]: query[+text:texto +BlockJoinQuery (filtered(+tweets.tweet_lists:[2 TO 2] +tweets.tweet_author:ogrisel)-&gt;FilterCacheFilterWrapper(_type:__tweets))],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:221)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:238)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.query.NestedQueryParser$LateBindingParentFilter.getDocIdSet(NestedQueryParser.java:171)
        at org.elasticsearch.index.search.nested.BlockJoinQuery$BlockJoinWeight.scorer(BlockJoinQuery.java:171)
        at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:298)
        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:116)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:198)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:391)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:298)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:286)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:217)
</description><key id="2386156">1510</key><summary>Weird behaviour and unexpected exceptions with nested queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">314fc</reporter><labels /><created>2011-11-29T15:56:23Z</created><updated>2011-12-14T13:05:51Z</updated><resolved>2011-12-14T13:05:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-30T14:53:06Z" id="2960555">Hey, I ran this several times on a one node cluster, and never managed to break it as you get it... 
</comment><comment author="kimchy" created="2011-12-14T13:05:50Z" id="3139185">relates to #1536, closing (will be fixed shortly)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When _source is disabled, don't return it in realtime get fetching the document from the transaction log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1509</link><project id="" key="" /><description /><key id="2379568">1509</key><summary>When _source is disabled, don't return it in realtime get fetching the document from the transaction log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-29T07:13:58Z</created><updated>2011-11-29T07:40:10Z</updated><resolved>2011-11-29T07:40:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/get/ShardGetService.java</file></files><comments><comment>When _source is disabled, don't return it in realtime get fetching the document from the transaction log, closes #1509.</comment></comments></commit></commits></item><item><title>Fix incorrect key in config sample</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1508</link><project id="" key="" /><description>Just a small one:

"transport.tcp.port" is used to configure netty, not "transport.port"
</description><key id="2374846">1508</key><summary>Fix incorrect key in config sample</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2011-11-28T23:24:25Z</created><updated>2014-07-16T21:55:50Z</updated><resolved>2011-12-12T22:59:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-29T14:24:08Z" id="2930389">Right!, thanks.
</comment><comment author="kimchy" created="2011-12-12T22:59:51Z" id="3114606">Applied it a few weeks ago, closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NodeInfo ignores one of the variables in the constructor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1507</link><project id="" key="" /><description /><key id="2368107">1507</key><summary>NodeInfo ignores one of the variables in the constructor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2011-11-28T15:00:31Z</created><updated>2014-06-19T08:13:26Z</updated><resolved>2011-11-28T15:10:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-28T15:10:16Z" id="2897980">Pulled to master and 0.18, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to netty 3.2.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1506</link><project id="" key="" /><description>Upgrade to the netty networking lib with important bug fixes.
</description><key id="2366927">1506</key><summary>Upgrade to netty 3.2.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-28T12:48:32Z</created><updated>2011-11-28T12:49:02Z</updated><resolved>2011-11-28T12:49:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade to netty 3.2.7, closes #1506.</comment></comments></commit></commits></item><item><title>Registering a percolate query with additional "object" level metadata can fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1505</link><project id="" key="" /><description /><key id="2360347">1505</key><summary>Registering a percolate query with additional "object" level metadata can fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-27T15:44:13Z</created><updated>2011-11-27T16:06:32Z</updated><resolved>2011-11-27T16:06:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java</file></files><comments><comment>Registering a percolate query with additional "object" level metadata can fail, closes #1505.</comment></comments></commit></commits></item><item><title>Analysis: Support greek language in the stemmer filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1504</link><project id="" key="" /><description /><key id="2359594">1504</key><summary>Analysis: Support greek language in the stemmer filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-27T11:40:37Z</created><updated>2011-11-27T11:40:47Z</updated><resolved>2011-11-27T11:40:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-27T11:40:47Z" id="2887259">Pushed in a pull request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Add language setting to lowercase filter, supporting greek and turkish</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1503</link><project id="" key="" /><description /><key id="2359573">1503</key><summary>Analysis: Add language setting to lowercase filter, supporting greek and turkish</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-27T11:34:23Z</created><updated>2011-11-27T11:39:18Z</updated><resolved>2011-11-27T11:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file></files><comments><comment>Analysis: Add language setting to lowercase filter, supporting greek and turkish, closes #1503.</comment></comments></commit></commits></item><item><title>Upgrade to Lucene 3.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1502</link><project id="" key="" /><description /><key id="2359495">1502</key><summary>Upgrade to Lucene 3.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-27T10:57:00Z</created><updated>2011-11-27T10:57:42Z</updated><resolved>2011-11-27T10:57:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/OpenFilterClause.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/vectorhighlight/MarginFragListBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/IndexCommitDelegate.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/DocSets.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/nested/BlockJoinQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/deps/lucene/SimpleLuceneTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/deps/lucene/VectorHighlighterTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Upgrade to Lucene 3.5, closes #1502.</comment></comments></commit></commits></item><item><title>Support for greek stemming (GreekStemFilter)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1501</link><project id="" key="" /><description>While the GreekAnalyzer supports stemming, custom analyzers for the greek language cannot be build because there is no GreekStemmer (although lucene has one).

That still doesn't solve the problem since the GreekLowerCaseFilter is missing. It would be nice if there was a way to access tokenizers or filters that are present in Lucene but not supported by ES.
</description><key id="2359217">1501</key><summary>Support for greek stemming (GreekStemFilter)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bandito</reporter><labels /><created>2011-11-27T08:57:00Z</created><updated>2014-06-14T11:28:06Z</updated><resolved>2011-11-27T11:24:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-27T11:24:24Z" id="2887205">Pushed hte change to 0.18 and master. Can you open a matching issue?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Convenient rolling index method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1500</link><project id="" key="" /><description>Here is some code where a rolling index pattern is implemented. Imagine you have a logical index named 'tweets', now you want to create every day a new index to keep the indices small (Its a better scalable 'sharding', but only if you have time dependent data). Now, in the proposed code you will have to call rollIndex(maximumIndices) once a day.

Then the new indices are all 'tagged' with a 'tweets_roll' alias (for later retrieval -&gt; imrovable?), there is a group of indices for searching (tweets_search) and feeding (tweets_feed). Per default it creates a search alias on all indices and a feed alias only for the very latest. It separates the search and the roll alias as it could be the case that one wants to keep some older indices but do not want to search on it.

What do you think? Its rather simple but it works - a simple test below the code.

```
private static final String simpleDateString = "yyyy-MM-dd-HH-mm-ss";
public String rollIndex(int maxRollIndices) {
    return rollIndex(getIndexName(), maxRollIndices, maxRollIndices);
}

public String rollIndex(String indexName, int maxRollIndices, int maxSearchIndices) {
    String rollAlias = indexName + "_roll";
    SimpleDateFormat formatter = new SimpleDateFormat(simpleDateString);

    if (maxRollIndices &lt; 1 || maxSearchIndices &lt; 1)
        throw new RuntimeException("remaining indices, search indices and feeding indices must be at least 1");

    // get old aliases
    Map&lt;String, AliasMetaData&gt; allRollingAliases = getAliases(rollAlias);

    // always create new index and append aliases
    String searchAlias = getSearchIndexName();
    String feedAlias = getFeedIndexName();
    String newIndexName = indexName + "_" + formatter.format(new Date());

    createIndex(newIndexName);        
    addAlias(newIndexName, searchAlias);
    addAlias(newIndexName, rollAlias);
    String oldFeedIndexName = null;

    if (allRollingAliases.isEmpty()) {
        // do nothing for now
    } else {
        TreeMap&lt;Long, String&gt; sortedIndices = new TreeMap&lt;Long, String&gt;(reverseSorter);
        String[] concreteIndices = getConcreteIndices(allRollingAliases.keySet());
        //logger.info("aliases:" + allRollingAliases + ", indices:" + Arrays.toString(concreteIndices));
        for (String index : concreteIndices) {
            int pos = index.indexOf("_");
            if (pos &lt; 0)
                throw new IllegalStateException("index " + index + " is not in the format " + simpleDateString);

            String indexDateStr = index.substring(pos + 1);
            Long timeLong;
            try {
                timeLong = formatter.parse(indexDateStr).getTime();
            } catch (Exception ex) {
                throw new IllegalStateException("index " + index + " is not in the format " + simpleDateString + " error:" + ex.getMessage());
            }
            String old = sortedIndices.put(timeLong, index);
            if (old != null)
                throw new IllegalStateException("indices with the identical date are not supported " + old + " vs. " + index);
        }
        int counter = 1;
        Iterator&lt;String&gt; indexIter = sortedIndices.values().iterator();

        while (indexIter.hasNext()) {
            String currentIndexName = indexIter.next();
            if (counter &gt;= maxRollIndices) {
                deleteIndex(currentIndexName);
                // delete all the older indices
                continue;
            }

            if (counter == 1)
                oldFeedIndexName = currentIndexName;

            if (counter &gt;= maxSearchIndices)
                removeAlias(currentIndexName, searchAlias);

            counter++;
        }
    }
    if(oldFeedIndexName != null)
        moveAlias(oldFeedIndexName, newIndexName, feedAlias);
    else
        addAlias(newIndexName, feedAlias);

    return newIndexName;
}

public String getSearchIndexName() {
    return getIndexName() + "_search";
}

public String getFeedIndexName() {
    return getIndexName() + "_feed";
}

public void createIndex(String indexName) {
    client.admin().indices().create(new CreateIndexRequest(indexName).settings(createIndexSettings())).actionGet(timeout);
}

public XContentBuilder createIndexSettings() {
    if (createIndexSettings == null) {
        try {
            createIndexSettings = JsonXContent.contentBuilder().startObject().
                    field("index.number_of_shards", createIndexShards).
                    field("index.number_of_replicas", createIndexReplicas).
                    field("index.refresh_interval", "10s").
                    field("index.merge.policy.merge_factor", 10).endObject();
        } catch (IOException ex) {
            throw new RuntimeException(ex);
        }
    }
    return createIndexSettings;
}

public void deleteIndex(String indexName) {
    client.admin().indices().delete(new DeleteIndexRequest(indexName)).actionGet();
}

public void addAlias(String indexName, String alias) {
    client.admin().indices().aliases(new IndicesAliasesRequest().addAlias(indexName, alias)).actionGet();
}

public void removeAlias(String indexName, String alias) {
    client.admin().indices().aliases(new IndicesAliasesRequest().removeAlias(indexName, alias)).actionGet();
}

public void moveAlias(String oldIndexName, String newIndexName, String alias) {
    client.admin().indices().aliases(new IndicesAliasesRequest().addAlias(newIndexName, alias).
            removeAlias(oldIndexName, alias)).actionGet();
}

public Map&lt;String, AliasMetaData&gt; getAliases(String index) {
    Map&lt;String, AliasMetaData&gt; md = client.admin().cluster().state(new ClusterStateRequest()).
            actionGet().getState().getMetaData().aliases().get(index);
    if (md == null)
        return Collections.emptyMap();

    return md;
}
private static Comparator&lt;Long&gt; reverseSorter = new Comparator&lt;Long&gt;() {

    @Override
    public int compare(Long o1, Long o2) {
        return -o1.compareTo(o2);
    }
};

public String[] getConcreteIndices(Set&lt;String&gt; set) {
    return client.admin().cluster().state(new ClusterStateRequest()).actionGet().getState().
            getMetaData().concreteIndices(set.toArray(new String[set.size()]));
}
```

TEST

```
@Test public void rollingIndex() throws Exception {
    search.setClient(createTestClient());
    search.setIndexName("tweets");
    String rollIndexTag = search.getIndexName() + "_roll";
    String searchIndex = search.getIndexName() + "_search";
    String feedIndex = search.getIndexName() + "_feed";
    search.rollIndex(4);        
    assertEquals(1, search.getAliases(rollIndexTag).size());
    assertEquals(1, search.getAliases(searchIndex).size());
    assertEquals(1, search.getAliases(feedIndex).size());

    Thread.sleep(1000);
    search.rollIndex(4);        
    assertEquals(2, search.getAliases(rollIndexTag).size());
    assertEquals(2, search.getAliases(searchIndex).size());
    assertEquals(1, search.getAliases(feedIndex).size());

    Thread.sleep(1000);
    search.rollIndex(4);        
    assertEquals(3, search.getAliases(rollIndexTag).size());
    assertEquals(3, search.getAliases(searchIndex).size());
    assertEquals(1, search.getAliases(feedIndex).size());

    Thread.sleep(1000);
    search.rollIndex(4);        
    assertEquals(4, search.getAliases(rollIndexTag).size());
    assertEquals(4, search.getAliases(searchIndex).size());
    assertEquals(1, search.getAliases(feedIndex).size());

    Thread.sleep(1000);
    search.rollIndex(4);        
    assertEquals(4, search.getAliases(rollIndexTag).size());
    assertEquals(4, search.getAliases(searchIndex).size());
    assertEquals(1, search.getAliases(feedIndex).size());

    Thread.sleep(1000);
    search.rollIndex(search.getIndexName(), 4, 3);        
    assertEquals(4, search.getAliases(rollIndexTag).size());
    assertEquals(3, search.getAliases(searchIndex).size());
    assertEquals(1, search.getAliases(feedIndex).size());
}
```
</description><key id="2352299">1500</key><summary>Convenient rolling index method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-11-25T21:24:58Z</created><updated>2014-05-26T15:51:28Z</updated><resolved>2014-05-26T15:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="otisg" created="2011-12-08T18:41:48Z" id="3067713">Didn't review the code, but like the rolling idea - Zoie has HourGlass for this sort of thing - maybe something can be used or learned from there - http://linkedin.jira.com/wiki/display/ZOIE/HourGlass+-+Forward-Rolling+Indexing
</comment><comment author="karussell" created="2011-12-08T21:01:45Z" id="3069930">One missing point in the code is to flush&amp;close the indices which are not searchable (Update: now implemented)
</comment><comment author="derrickburns" created="2012-01-19T01:11:05Z" id="3556343">I did read the code.   Nice and simple!

The rolling index concept is good. I would separate the mechanism for index creation from the mechanism for index deletion.   Then, one could separate different policies for each.

In thinking about this, I guess I'd like to see a policy agent within elasticsearch that listens to/intercepts requests directed at a given alias. Indexing requests (writes) to the alias would be intercepted and could result in new index creation, if the index creation policy says that a new index is needed.

Similarly, an index request of a query could trigger an index deletion or index migration to backing store.

Is it possible to install a plugin that listens to requests on a particular index?
</comment><comment author="karussell" created="2012-10-24T19:00:55Z" id="9751927">The code is wrapped in a plugin https://github.com/karussell/elasticsearch-rollindex
</comment><comment author="karussell" created="2014-05-26T15:51:28Z" id="44200046">It looks like curator is doing this now: https://github.com/elasticsearch/curator
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make the console bigger for a better log display under windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1499</link><project id="" key="" /><description>Patch for windows to display logs in a prettier format
</description><key id="2351844">1499</key><summary>Make the console bigger for a better log display under windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-11-25T19:46:24Z</created><updated>2014-07-16T21:55:51Z</updated><resolved>2011-11-27T17:24:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-27T14:03:57Z" id="2887795">Doesn't that remove the option to customize the size themself? Either generic global one, or for that "process"?
</comment><comment author="dadoonet" created="2011-11-27T17:24:05Z" id="2888771">You're perfectly right, as usual !!!

In fact, I was trying to make new batchs from where I launch elasticsearch.bat.
So, when I do something like start elasticsearch.bat, it opens a new window with a small (normal) size.
That's why I proposed that patch but it's not a nice idea !!!

Thanks Shay.
I close this pull request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Request to support statistical calculations in TermStatsFacet &amp; RangeFacet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1498</link><project id="" key="" /><description>From api it is evident that we can get statistical calculations like Standard Deviation, Variance and SumOfSquares using Statistical Facets. But same is not possible using TermStatsFacet and RangeFacet. Request to enable same support in TermStatsFace as well as RangeFacet.
</description><key id="2344351">1498</key><summary>Request to support statistical calculations in TermStatsFacet &amp; RangeFacet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rahulsharmacoder</reporter><labels /><created>2011-11-24T19:20:12Z</created><updated>2014-01-22T11:56:39Z</updated><resolved>2014-01-22T11:56:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="emorency" created="2012-10-26T14:45:38Z" id="9815357">Is it planned to be supported soon ?
</comment><comment author="jpountz" created="2014-01-22T11:56:39Z" id="33015196">This is supported by the new aggregations framework through the [`extended_facets`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-metrics-extendedstats-aggregation.html) aggregation.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Using _parent:123 in a query string query fails to fetch docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1497</link><project id="" key="" /><description /><key id="2339869">1497</key><summary>Using _parent:123 in a query string query fails to fetch docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-24T10:49:31Z</created><updated>2011-11-24T11:59:25Z</updated><resolved>2011-11-24T11:59:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/UidFilter.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Using _parent:123 in a query string query fails to fetch docs, closes #1497.</comment></comments></commit></commits></item><item><title>Support using _id:1234, or using term query/filter on _id even when _id is not indexed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1496</link><project id="" key="" /><description /><key id="2339728">1496</key><summary>Support using _id:1234, or using term query/filter on _id even when _id is not indexed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-24T10:31:28Z</created><updated>2011-11-24T10:32:04Z</updated><resolved>2011-11-24T10:32:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/simple/SimpleSearchTests.java</file></files><comments><comment>Support using _id:1234, or using term query/filter on _id even when _id is not indexed, closes #1496.</comment></comments></commit></commits></item><item><title>0.18 completes fix of issue 1478</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1495</link><project id="" key="" /><description>dealing with json array not json map so changed startObject closeObject to startArray,closeArray

completes fix of elasticsearch/elasticsearch#1478
</description><key id="2336369">1495</key><summary>0.18 completes fix of issue 1478</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ASutterWork</reporter><labels /><created>2011-11-24T00:43:48Z</created><updated>2014-07-16T21:55:52Z</updated><resolved>2011-11-24T08:04:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-24T07:49:45Z" id="2859144">Yea, missed that... . That what happens when I don't have big couch to test it :)
</comment><comment author="kimchy" created="2011-11-24T08:04:14Z" id="2859230">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Infinite loop in DynamicChannelBuffer.ensureWritableBytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1494</link><project id="" key="" /><description>Frames larger than 1GB can trigger an infinite loop in [DynamicChannelBuffer.ensureWritableBytes](https://github.com/netty/netty/blob/3.2/src/main/java/org/jboss/netty/buffer/DynamicChannelBuffer.java#L80). It happens because of integer overflow in newCapacity calculation for any minNewCapacity &gt; 1073741824.

To reproduce:

1) Start ES with enough memory to accomodate 1GB of data.

```
$ ES_MAX_MEM=2048m bin/elasticsearch -f
```

2) Execute the following python script 

``` Python
import socket
import struct

size = 1073741824
chunk = 1024
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.connect(("localhost", 9300))
# Send an integer equal to or larger than 1073741824 or large 
# so SizeHeaderFrameDecoder would start collecting data
s.send(struct.pack("!l",size))
# followed by at least 1GB of data to trigger overflow
for x in range(size / chunk):
    s.send(" " * chunk)
s.close()
```

3) Run jstack on elasticsearch pid

```
$ jstack 5475 | grep -A 20 -B 2 ensureWritableBytes 
"New I/O server worker #1-8" daemon prio=5 tid=112092000 nid=0x10d921000 runnable [10d920000]
   java.lang.Thread.State: RUNNABLE
    at org.elasticsearch.common.netty.buffer.DynamicChannelBuffer.ensureWritableBytes(DynamicChannelBuffer.java:81)
    at org.elasticsearch.common.netty.buffer.DynamicChannelBuffer.writeBytes(DynamicChannelBuffer.java:239)
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.writeBytes(AbstractChannelBuffer.java:457)
    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.writeBytes(AbstractChannelBuffer.java:450)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:213)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:81)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
```

This issue was observed on elasticsearch 0.17.7 during shard relocation.
</description><key id="2335834">1494</key><summary>Infinite loop in DynamicChannelBuffer.ensureWritableBytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-23T23:16:17Z</created><updated>2014-07-13T08:51:43Z</updated><resolved>2014-07-13T08:06:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-24T08:22:14Z" id="2859465">Interesting, I will take it up with trustin (from netty) and see what he thinks. The strange thing is that there is a limit on the size passed when recovering, both when moving file "chunks", and when transferring translog. The only reason I can think this might happen is there is a single document indexed that is bigger than 1gb, can that be the case?
</comment><comment author="imotov" created="2011-11-24T14:02:43Z" id="2864766">We have a limit for document sizes and relocation succeeded after the affected nodes were rebooted. So, it doesn't seem like a single large document problem. Could it be corrupted data in the network stream? I have seen errors like this before: https://gist.github.com/549a835dca90b71e58d3  They are quite infrequent, but they indicate that they might be possible. It's "in the cloud" after all, who knows what else is going on on this hardware. I am just thinking, if we don't expect to get such large frames and netty cannot handle them until this problem is fixed, wouldn't it make sense to discard them on the SizeHeaderFrameDecoder level?
</comment><comment author="kimchy" created="2011-11-24T18:07:58Z" id="2867867">It shouldn't be, thats strange... . I have enhanced the way decoding is done which should avoid the problem (https://github.com/elasticsearch/elasticsearch/commit/03c2e5ea528abf0029808c63f5ba6d5d803b1757), but its still interesting how this large message can happen if doc size is limited... . The limit btw for recovery size is in RecoverySource class.

How did you find out that its a big message?
</comment><comment author="imotov" created="2011-11-25T03:38:28Z" id="2871051">Yeah, it is strange. But I don't see any other way to trigger this issue. There are only two ways this loop can get stuck. One way requires newCapacity to be negative and another requires minNewCapacity to be larger than 1G. When I picked the first condition and started to work up the stack trying to find the way this condition could be met, I found dead-ends everywhere. When I did the same thing for the second condition I found that it's only possible to achieve by sending a large integer followed by gigabyte of something. And yes, it's really puzzling how this gigabyte could have been generated.
</comment><comment author="clintongormley" created="2014-07-08T12:43:48Z" id="48330745">@imotov can this issue be closed?
</comment><comment author="imotov" created="2014-07-13T08:06:37Z" id="48834480">@clintongormley I haven't seen this bug in the wild for more than 2 years. Closing. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DocumentMapper.java wrong order on build rootMappers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1493</link><project id="" key="" /><description>It was some problems when you restart the ES and noticed that some filters were not being applied on all field.
It was observed that the class AllFieldMapper, in the line 182
         &lt;pre&gt;     analyzer = context.analyzer(); &lt;/pre&gt;
this analyzer was null.

So I went deeper and noticed that the class DocumentMapper Map rootMappers The order of the objects changed, sometimes

&lt;pre&gt;
      this.rootMappers.put (AllFieldMapper.class, new AllFieldMapper());
came before the
     this.rootMappers.put (AnalyzerMapper.class, new AnalyzerMapper());
soon context.analyzer () class was null.
&lt;/pre&gt;

I changed the order of the puts and also changed the Map of rootMappers, I started using a LinkedHashMap.

Below is the change in class DocumentMapper.

Line: 136

&lt;pre&gt;
         private Map &lt;Class &lt;? extends RootMapper&gt;, RootMapper&gt; rootMappers = new LinkedHashMap &lt;Class &lt;? extends RootMapper&gt;, RootMapper &gt;();// Maps.newHashMap ();
&lt;/pre&gt;

Line: 171

&lt;pre&gt;
             this.rootMappers.put (SourceFieldMapper.class, new SourceFieldMapper());
             this.rootMappers.put (TypeFieldMapper.class, new TypeFieldMapper());
             this.rootMappers.put (AnalyzerMapper.class, new AnalyzerMapper());
             this.rootMappers.put (AllFieldMapper.class, new AllFieldMapper());
             this.rootMappers.put (BoostFieldMapper.class, new BoostFieldMapper());
             this.rootMappers.put (RoutingFieldMapper.class, new RoutingFieldMapper());
&lt;/pre&gt;
</description><key id="2333950">1493</key><summary>DocumentMapper.java wrong order on build rootMappers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavobmaia</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-23T20:03:24Z</created><updated>2011-11-24T12:20:22Z</updated><resolved>2011-11-24T07:58:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-24T07:54:45Z" id="2859168">What problem is it trying to solve? Are you using _analyzer mapping in the doc?
</comment><comment author="kimchy" created="2011-11-24T07:57:46Z" id="2859186">I'll push this change since its correct, but would like to understand what you are trying to solve specifically (just in case I missed something).
</comment><comment author="gustavobmaia" created="2011-11-24T12:20:22Z" id="2862572">Yes I use a specific mapping analyzer and some specific filters (Delimiter, stopWords, Stemmer....)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file></files><comments><comment>DocumentMapper.java wrong order on build rootMappers, closes #1493.</comment></comments></commit></commits></item><item><title>Query DSL: indices query to allow to set a `no_match_query`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1492</link><project id="" key="" /><description>The `indices` query will execute the query provided if it executes on an index matching the listed indices, and a `match_all` if the index does not match the listed indices. 

Add `no_match_query` field, can be set to `all` for `match_all`, `none` for not matching on any docs, or a full query.
</description><key id="2331914">1492</key><summary>Query DSL: indices query to allow to set a `no_match_query`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-23T17:00:57Z</created><updated>2011-11-23T17:01:29Z</updated><resolved>2011-11-23T17:01:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file></files><comments><comment>Query DSL: indices query to allow to set a `no_match_query`, closes #1492.</comment></comments></commit></commits></item><item><title>Request: Facet for Missing Term on TermStatsFacet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1491</link><project id="" key="" /><description>Currently only the count for the missing values for the TermStatsFacet can be retrieved. Something like:

FacetBuilders.termsStatsFacet(
               "foo")
                .keyField("foo")
                .valueField("bar)
                .size(100)

```
            .order(TermsStatsFacet.ComparatorType.COUNT)
            .allTerms();
```
</description><key id="2331303">1491</key><summary>Request: Facet for Missing Term on TermStatsFacet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikeasick</reporter><labels /><created>2011-11-23T16:16:30Z</created><updated>2014-01-22T11:39:16Z</updated><resolved>2014-01-22T11:39:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikeasick" created="2011-11-23T16:22:48Z" id="2850844">OOOPS fat fingered that and didn't include what I wanted. Something like:

FacetBuilders.termsStatsFacet(
"foo")
.keyField("foo")
.valueField("bar)
.size(100)
.addMissingFacet() // or includeMissingFacet or ... 
.order(TermsStatsFacet.ComparatorType.COUNT)
.allTerms();

That would request ES to include a TermsStatsFacet.Entry for the missing field with all stats included as if it were a proper faceted field.
</comment><comment author="jpountz" created="2014-01-22T11:39:16Z" id="33014250">Closing as this is possible with aggregations by putting a `stats` aggregation under a `missing` aggregation.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>AWS Plugin: Add more automatic region configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1490</link><project id="" key="" /><description>Add `us-west-2` and `ap-northeast` as region names (that automatically use the correct endpoint for ec2 and s3).
</description><key id="2331180">1490</key><summary>AWS Plugin: Add more automatic region configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-23T16:07:37Z</created><updated>2011-11-23T16:09:40Z</updated><resolved>2011-11-23T16:09:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java</file></files><comments><comment>AWS Plugin: Add more automatic region configuration, closes #1490.</comment></comments></commit></commits></item><item><title>Added a region for AwsEc2Service.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1489</link><project id="" key="" /><description>Just added this region as it was necessary for me (although it can done via ec2.endpoint)
</description><key id="2330596">1489</key><summary>Added a region for AwsEc2Service.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-11-23T15:16:06Z</created><updated>2014-07-15T13:40:51Z</updated><resolved>2011-11-23T16:48:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-23T16:48:10Z" id="2851229">Ha!, I just committed support for it myself :), see #1490.
</comment><comment author="karussell" created="2011-11-23T22:58:49Z" id="2856152">ah, cool. thanks :) !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scalable Sharding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1488</link><project id="" key="" /><description>The sharding that is currently possible with Elasticsearch is already great. But if you want to scale to really very big datasets, then you can still experience performance issues.

Suppose that you have 1 big index spread over 10 shards. In this case, Elastic still needs to perform searches on 10 shards (ideally on 10 different servers in parallel).

Another approach could be to have some kind of user configurable sharding in place. For example, in our case (we are a social media platform), we do not use Elastic sharding, but have our own custom sharding on top where we create indices for every topic and every month. This means that for a search of the last 30 days, at most, we need to search on the shard for this month and the shard for last month.

The same approach could be done on text fields. Suppose that we have setup 10 shards and do sharding on the field "contents". Instead of pushing the data to one of the shards, we could first perform an analysis step on the "contents" field to break up in words and then for each word, determine the servers responsible for that word. On those servers, we then index the document. The result of this would be that there would be more redundance on storing the documents (and as such adding documents will also take longer), but when searching, we can immediately determine based on the query what servers need to be searched (only those responsible for the data ranges that contain the analyzed words in the query). (key distribution could be done based on hashing and hashing ranges per server, similar to how for example Redis or Memcache work)

Multiple sharding keys should als be possible.

With this custom sharding, it should be possible to scale datasets to billions of documents. There will be redundant storage, but queries will be alot faster. For example in a 100 server setup, one search with 100 shards could potentially take a long time. With the custom sharding, only a few servers would need to be queried.
</description><key id="2328744">1488</key><summary>Scalable Sharding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">folke</reporter><labels /><created>2011-11-23T11:33:38Z</created><updated>2013-12-18T11:34:31Z</updated><resolved>2013-12-18T11:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="folke" created="2011-11-23T11:44:36Z" id="2847718">Some IRC responses :-)
- you could achieve a lot of what you mention using the routing parameter to search http://www.elasticsearch.org/guide/reference/api/search/
- for the simple cases you could use that indeed! But if you want to scale on text searches, then you need to index the same document on multiple nodes
- actually if it would be possible to define multiple routing values for indexing, then I guess what I suggest would be completely possible
- ok - i'm not an expert.  i do know that kimchy talked about sharding-on-terms as a failed experiment in the search world
- you can see the video of that talk here: http://www.elasticsearch.org/videos/2011/08/09/road-to-a-distributed-searchengine-berlinbuzzwords.html
- I wouldn't call Google as a failed experiment :-)
- another thing that would be awesome is dynamic sharding: instead of specifying the number of shards, in some cases it could be interesting to specify maximum documents per shard. If limit is hit, a new shard is created...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi field mapper with more than one extra mapping can cause endless re-sync'ing of mapping between nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1487</link><project id="" key="" /><description>More than one extra mapping (on top of the default one) does not get serialized in order, causing the serialized form of the mapping to differ.
</description><key id="2327170">1487</key><summary>Multi field mapper with more than one extra mapping can cause endless re-sync'ing of mapping between nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-23T07:04:11Z</created><updated>2011-11-23T07:04:41Z</updated><resolved>2011-11-23T07:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/multifield/MultiFieldMapper.java</file></files><comments><comment>Multi field mapper with more than one extra mapping can cause endless re-sync'ing of mapping between nodes, closes #1487.</comment><comment>Hello,</comment><comment>it looks like, although im running ES v0.18.7, I have the exact same problem.</comment><comment>I'm suspecting that my particular default-mapping.json file is not handled well: https://gist.github.com/3010783</comment></comments></commit></commits></item><item><title>Geo_Polygon Performance Issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1486</link><project id="" key="" /><description>I have been doing some rigorous testing to see if I can make ElasticSearch perform better than a relational database. In particular, I would like to be able to run a query that ultimately returns all the points within a complex polygon. To compare the two, I loaded the entire geonames allcountries.txt file in to Postgresql 9.0 (with latest PostGIS) on my Macbook Pro. I am also using the latest release version of ES on the same machine.

There are 7,836,496 point geometries in the geonames data set. Here are my stats from 3 different queries and note that the times are slower as expected but the number of returned results are much different which concerns me. The countries were chosen based on their size from small =&gt; medium =&gt; large.

Bahrain: PG
Time:   19      12      18      16ms
# 's :   210     210     210     210

Bahrain: ES
Time:   5923    1302    1186    1138ms
# 's :   219     219     219     219

Egypt: PG
Time:   5339    2812    3024    3312ms
# 's :   23203   23203   23203   23203

Egypt: ES
Time:   14754   14353   14380   14399ms
# 's :   10759   10759   10759   10759

China: PG
Time:   57617   57520   61826   58494
# 's :   252723  252723  252723  252723

China: ES
Time:   90999   170864  282611  289245ms
# 's :   150230  150230  150230  150230

The query I used in ES can be found here. https://gist.github.com/1385785 Due to size limitations in gist, I had to cut off the polygon so the full polygon data can be found here https://ogr2elasticsearch.googlecode.com/files/polygons.zip 

The postgresql query can be found here. https://gist.github.com/1385796 if someone else would like to try and replicate the test. I also noticed that ES is returning data from neighboring countries as well which means that it is not honoring the same "within" conditions that postgis uses.

Adam
</description><key id="2318482">1486</key><summary>Geo_Polygon Performance Issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geodawg</reporter><labels /><created>2011-11-22T14:42:47Z</created><updated>2013-04-05T16:04:11Z</updated><resolved>2013-04-05T16:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="geodawg" created="2011-11-28T19:46:55Z" id="2906143">All,

I just noticed that some of my geometries are MULTIPOLYGON types which may be the reason for the wonky results. How does ES address this?

Adam
</comment><comment author="kimchy" created="2011-12-04T13:50:06Z" id="3006564">There is no support for multi polygon, just polygon. Also, to improve perf, try and wrap the polygon with a bounding box. You can, possibly, build a several polygon filters and use `and`/`or`/`not` filters to combine them.
</comment><comment author="dsmiley" created="2012-01-03T21:19:33Z" id="3346341">FYI I am working on high performance geospatial filtering using Lucene.  The code is called "LSP" hosted here http://code.google.com/p/lucene-spatial-playground/ and I am aiming to pitch it as the replacement for Lucene's defunct spatial module.  For filtering alone (no geo sort / relevancy), there is no RAM requirement.  Polygon support is implemented via JTS, and a variable number of shapes of any kind is supported.  Shapes with area are also supported for indexing.  It'll probably be another month or so of me adding tests and plugging a few holes here and there before I'll pitch it for Lucene's spatial replacement.  That may be a good time for ElasticSearch to consider it.  It's using Lucene 4/trunk, not 3x, though.
</comment><comment author="kimchy" created="2012-01-04T11:57:13Z" id="3353635">@dsmiley looks interesting!, would love to integrate it with elasticsearch once its done and Lucene 4.0 is released :)
</comment><comment author="apatrida" created="2012-07-16T18:08:36Z" id="7014012">Is this still in the "waiting to use" stage?
</comment><comment author="nwohaibi" created="2013-03-01T11:14:49Z" id="14283962">@geodawg, great testing, you spared me a lot of time trying to accomplish the same thing.
I just want to know if you had run the same testing after ES implemented Spatial4j? I would love to see the results.
</comment><comment author="clintongormley" created="2013-04-05T16:04:11Z" id="15964725">Geoshapes now supported in v 0.90+
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IndicesQueryBuilder generates the wrong query name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1485</link><project id="" key="" /><description /><key id="2318060">1485</key><summary>IndicesQueryBuilder generates the wrong query name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-22T13:57:34Z</created><updated>2011-11-22T13:58:08Z</updated><resolved>2011-11-22T13:58:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file></files><comments><comment>IndicesQueryBuilder generates the wrong query name, closes #1485.</comment></comments></commit></commits></item><item><title>Thrift transport handling unexpected URI hangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1484</link><project id="" key="" /><description /><key id="2315665">1484</key><summary>Thrift transport handling unexpected URI hangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-22T08:43:49Z</created><updated>2011-11-22T08:44:25Z</updated><resolved>2011-11-22T08:44:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftRestImpl.java</file><file>plugins/transport/thrift/src/test/java/org/elasticsearch/thrift/test/SimpleThriftTests.java</file></files><comments><comment>Thrift transport handling unexpected URI hangs, closes #1484.</comment><comment>Wouldn't it be a lot better if this was logged as a warning or something similar, since if it ends up happening sometime in the future, figuring out what have happened is sort of impossible if the exception is caught and ignored silently</comment></comments></commit></commits></item><item><title>Shared Filesystem Gateway periodic snapshot cannot be successfully suspended</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1483</link><project id="" key="" /><description>As discussed on the mailing list:

http://elasticsearch-users.115913.n3.nabble.com/Gateway-Snapshot-settings-amp-unexpected-behaviour-maybe-bug-td3514819.html

We have a consistently reproducable case where attempting to suspend the periodic snapshotting fails.

Our test path goes something like this:

```
# cd into the location of the gateway snapshot location
$ cd /index/esgateway/qa4/indices

# do a recursive directory listing and save the output
$ ls -lR &gt; /tmp/before-any-changes

# suspend the gateway snapshotting by setting the interval to 0
$ echo '{"index":{"gateway":{"snapshot_interval":0}}}' | \
 curl -XPOST http://qa4es1.qa.acx:9200/snaptest/snaptype -d @-

# Do another recursive directory copy of the snapshot location, we're expecting nothing to have changed here, because we haven't modified the index, yet it looks like it snapshots at the time of resetting the snapshot to 0
$ ls -lR &gt; /tmp/after-disable-gateway-via-zero-snapshot-interval

# now actually modify the index
$ curl -XPOST http://qa4es1.qa.acx:9200/snaptest/snaptype -d @testdata
 (testdata is a file containing a json index update)

# Take another text copy of what the directory looks like, it really shouldn't have been touched right?
$ ls -lR &gt; /tmp/after-post-with-index-modification 

# Alas, we see differences...
$ diff /tmp/after-disable-gateway-via-zero-snapshot-interval \
  /tmp/after-post-with-index-modification
 ... [several differences reported] ...
```

We were expecting no differences at this point, up until we
reset the regular snapshot interval ... perhaps we are missing
something there though.
</description><key id="2311778">1483</key><summary>Shared Filesystem Gateway periodic snapshot cannot be successfully suspended</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tallpsmith</reporter><labels /><created>2011-11-22T00:31:53Z</created><updated>2011-11-27T22:27:56Z</updated><resolved>2011-11-27T22:27:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-22T09:47:08Z" id="2832362">Do you see it happen when you set the snapshot interval to `-1`. 
</comment><comment author="tallpsmith" created="2011-11-27T22:27:56Z" id="2890672">Forgot to post this last week, yes setting to -1 works a treat.  Be great to have that in the docs.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>By default, set http.compression to false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1482</link><project id="" key="" /><description>By default, don't send compressed content over http when the client denotes it supports it. Actually compressing (with gzip) is usually not worth it, and many clients are buggy when it comes to supporting it.
</description><key id="2307255">1482</key><summary>By default, set http.compression to false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-21T17:50:03Z</created><updated>2016-02-23T11:48:12Z</updated><resolved>2011-11-21T17:50:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file></files><comments><comment>By default, set http.compression to false, closes #1482.</comment></comments></commit></commits></item><item><title>read bulk_size/drop_threshold from twitter river configuration as documented</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1481</link><project id="" key="" /><description /><key id="2291895">1481</key><summary>read bulk_size/drop_threshold from twitter river configuration as documented</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tmm1</reporter><labels /><created>2011-11-20T10:34:47Z</created><updated>2014-07-16T21:55:52Z</updated><resolved>2011-11-20T10:39:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-20T10:39:24Z" id="2803321">Pushed to both 0.18 and master branches. thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>HTTPS support for CouchDB river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1480</link><project id="" key="" /><description>Add HTTPS support to CouchDB river. Also allow to disable Hostname checking, as many JVMs do not implement SNI, which is used by some CouchDB hosting solutions (e.g. cloudant).

Introduces 2 new configuration options:
- protocol: http/https
  -  Describes the protocol in use.
- no_verify: true/false
  - Allows to disable hostname verification for problematic hosts 

Also see associated pull request:

https://github.com/elasticsearch/elasticsearch/pull/1479
</description><key id="2291878">1480</key><summary>HTTPS support for CouchDB river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels><label>enhancement</label><label>v0.18.5</label><label>v0.19.0.RC1</label></labels><created>2011-11-20T10:32:39Z</created><updated>2011-11-20T10:49:56Z</updated><resolved>2011-11-20T10:49:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-20T10:49:56Z" id="2803340">Implemented in pull request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add HTTPS handling to CouchDB river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1479</link><project id="" key="" /><description>Hi, 

we had the need to index a cloudant DB using the CouchDB river using the HTTPS endpoint. As we found that the river does not allow using HTTPS, we came up with this patch.  

Introduces 2 new configuration options:
- couchProtocol: http/https
  - Describes the protocol in use.
- no_verify: true/false
  - Allows to disable hostname verification for hosts with problematic certs
</description><key id="2289388">1479</key><summary>Add HTTPS handling to CouchDB river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2011-11-19T18:58:19Z</created><updated>2014-07-16T21:55:53Z</updated><resolved>2011-11-20T09:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-20T09:38:13Z" id="2803147">Pushed to 0.18 and master branch. Can you open an issue to represent it?
</comment><comment author="skade" created="2011-11-20T10:33:10Z" id="2803306">Here you go. I hope this is correct:

https://github.com/elasticsearch/elasticsearch/issues/1480
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file></files><comments><comment>Multicast: Add discovery.zen.ping.multicast.send_ping setting to disable sending ping requests while still having multicast enabled, closes #1479.</comment></comments></commit></commits></item><item><title>BigCouch returns JSON array for sequence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1478</link><project id="" key="" /><description>BigCouch is returning:

```
 "last_seq":[136374,"g1AAAAIReJzLYWBg4MxgTmGQT8pMT84vTc5wKC5ITc5MzMksLtFLrkyqzNHLyU9OzMkBKmRKZMhjYZA-XiiTlcTAYOJDtD6gLhmgLiBl3vfYAqTZdTtIswRcc0FaMjZdFkDlQCo4NDQUpMuOgRQrQ4C6gFQ-0GqQZsfvpGguAOoCUt2PLfpAmm0jiXJvD1A5kFq-atUqkC5jFlKsXAHUBaQOF8ocB4fvEVI0HwHqAlL3gSEG0uygS5R7HwCVA6n_QADSZZqXBQCZGar4"]}
```

elasticsearch is requesting:

```
 /my_db/_changes?feed=continuous&amp;include_docs=true&amp;heartbeat=10000&amp;since=%5B136374%2C+g1AAAAIReJzLYWBg4MxgTmGQT8pMT84vTc5wKC5ITc5MzMksLtFLrkyqzNHLyU9OzMkBKmRKZMhjYZA-XiiTlcTAYOJDtD6gLhmgLiBl3vfYAqTZdTtIswRcc0FaMjZdFkDlQCo4NDQUpMuOgRQrQ4C6gFQ-0GqQZsfvpGguAOoCUt2PLfpAmm0jiXJvD1A5kFq-atUqkC5jFlKsXAHUBaQOF8ocB4fvEVI0HwHqAlL3gSEG0uygS5R7HwCVA6n_QADSZZqXBQCZGar4%5D
```

It looks like elasticsearch isn't expecting bigcouch's use of json arrays as a sequence identifier. The string doesn't appear to be quoted when it's passed back in the &amp;since= parameter.
</description><key id="2272313">1478</key><summary>BigCouch returns JSON array for sequence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">opie4624</reporter><labels /><created>2011-11-17T18:57:13Z</created><updated>2011-11-23T20:11:01Z</updated><resolved>2011-11-18T11:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-18T11:37:13Z" id="2788171">I pushed a fix for this in both 0.18 and master branch, it would be great if you can test it with bigcouch.
</comment><comment author="opie4624" created="2011-11-18T17:33:07Z" id="2791828">Pulling and compiling now... I'll let you know in a bit.
</comment><comment author="opie4624" created="2011-11-18T19:03:57Z" id="2792901">No dice. It can't consume the _changes stream at all:

```

[2011-11-18 11:00:45,540][WARN ][river.couchdb            ] [Ravage 2099] [couchdb][stream-data] failed to executefailure in bulk execution:
[100]: index [_river], type [stream-data], id [_seq], message [MapperParsingException[Failed to parse [couchdb.last_seq]]; nested: NumberFormatException[For input string: "g1AAAAH-eJyV0M0NgkAQBeDx52IHJpLYAVc52oNSwLKiLNkICJrQgiBtaGhDI1qFJhSijzXRgx7WvbzL-zKzI4mo53VmZDhiwYM198bhSmxY4po8dVJpyoAzKdFqM1p2aVBFhu8QtbIG9T9ozr8A6gbqiFFRW0pdtJSFOmJi27ZSZ70FQaYgiABzlcwbOXzLOHS5YFLEyS8cQiGy2ioU3motm6OO2JdlqdRVe9kDCOIYGZWSO615J9QRNxzpz3l3EMQD7_U__wndgqMv"]; ]
[2011-11-18 11:00:45,637][DEBUG][action.bulk              ] [Ravage 2099] [_river][0] failed to bulk item (index) index {[_river][stream-data][_seq], source[{"couchdb":{"last_seq":[3900,"g1AAAAH-eJzLYWBg4MxgTmGQScpMT84vTc5wKCjKLEssSdVLrkyqzNHLyU9OzMkBqmJKZMhjYZA-XiiTlcTAwDgVpEkCoSktGUMDULkMUDmQMu97bAHWdYUoXRZA5UAqODQ0FKzrEnEOBGoJAWoBUvlAe8E6p4F0ysN1FhekJmcm5mQWl2DTXADUBaS6H1v0keDFHqByILV81apVYF3XiHbsCqAWIHW4UOY4WOd0ouw7AlQOpO4DAwms6yrR9j0AagFS_4EArHNKFgA3I6OT"]}}]}
org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [couchdb.last_seq]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:308)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:577)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:565)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:435)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:433)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:475)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:416)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:302)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:136)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:487)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:400)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NumberFormatException: For input string: "g1AAAAH-eJzLYWBg4MxgTmGQScpMT84vTc5wKCjKLEssSdVLrkyqzNHLyU9OzMkBqmJKZMhjYZA-XiiTlcTAwDgVpEkCoSktGUMDULkMUDmQMu97bAHWdYUoXRZA5UAqODQ0FKzrEnEOBGoJAWoBUvlAe8E6p4F0ysN1FhekJmcm5mQWl2DTXADUBaS6H1v0keDFHqByILV81apVYF3XiHbsCqAWIHW4UOY4WOd0ouw7AlQOpO4DAwms6yrR9j0AagFS_4EArHNKFgA3I6OT"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
    at java.lang.Long.parseLong(Long.java:410)
    at java.lang.Long.parseLong(Long.java:468)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:68)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.parseCreateField(LongFieldMapper.java:231)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:295)
    ... 14 more
```
</comment><comment author="kimchy" created="2011-11-19T12:12:13Z" id="2798526">You need to delete the river first, since last_seq got identified as numeric (maybe you worked with local couchdb), and then changed to an array when you moved to bigcoudh.
</comment><comment author="opie4624" created="2011-11-19T18:15:06Z" id="2800074">I'm starting over with a fresh data directory and recreating the river
every time I test.
</comment><comment author="kimchy" created="2011-11-20T06:38:21Z" id="2802746">Can you set `river.couchdb` in logging.yml file to TRACE and run it? It seems like the seq stored under the `_river` index is identified as numeric for some reason. The trace level logging will show what gets indexed.
</comment><comment author="opie4624" created="2011-11-20T18:10:52Z" id="2806472">Log file is about 17MB: http://ge.tt/8QErZBA?c
</comment><comment author="opie4624" created="2011-11-22T21:09:03Z" id="2840708">https://gist.github.com/69c5cc6d2f214d3a5204

Looks like elasticsearch is parsing the json sequence info out properly for indexing, but is not storing it properly in the river/_seq document.

If I look at what's in elasticsearch there is no _seq document for the given river.
</comment><comment author="kimchy" created="2011-11-23T07:12:49Z" id="2845713">I see where the problem is, its because of the format of array, where it includes string and numeric types internally. Can I say again that this format BigCouch uses is quite atrocious...
</comment><comment author="kimchy" created="2011-11-23T07:21:19Z" id="2845767">ok, I pushed another effort in trying to solve it.
</comment><comment author="opie4624" created="2011-11-23T20:11:01Z" id="2854220">Getting the "failed to convert" error.
Log: https://gist.github.com/d215005dc57a77adee49
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file></files><comments><comment>another go at trying to solve: BigCouch returns JSON array for sequence #1478.</comment></comments></commit><commit><files><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file></files><comments><comment>BigCouch returns JSON array for sequence, closes #1478.</comment></comments></commit></commits></item><item><title>Fix binaryfieldfromtranslog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1477</link><project id="" key="" /><description>See issue #1476
</description><key id="2270605">1477</key><summary>Fix binaryfieldfromtranslog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2011-11-17T16:27:23Z</created><updated>2014-07-16T21:55:53Z</updated><resolved>2011-11-17T17:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-17T17:21:41Z" id="2778857">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get API: GetField.getValue() returns inconsistent result type for binary fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1476</link><project id="" key="" /><description>After a Get request, Java API GetField.getValue() of a binary field may return different type of value depending on whether fetching the document was done from the transaction log or from the index.

When fetching from the index, the result is a very convenient byte[], while when fetching from the transaction log, getValue() returns the base64 encoded representation as a String.

Both ways should return a byte[].
</description><key id="2268500">1476</key><summary>Get API: GetField.getValue() returns inconsistent result type for binary fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2011-11-17T13:06:22Z</created><updated>2011-11-18T11:37:49Z</updated><resolved>2011-11-18T11:37:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-18T11:37:49Z" id="2788178">Closing the issue, fixed by the mentioned pull request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Binary field compression causes wrong _source decoding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1475</link><project id="" key="" /><description>Having a compressed binary field in the properties of a given type causes the _source of the document to be unreadable on a Get operation right next after indexing the document.

The issue seems to be that in SourceLookup.java:80 the _source field is detected as a compressed field while it's not, leading to a Null content type and an NPE later on.

Full Exception:
https://gist.github.com/1372798

Test case:
https://gist.github.com/1372865
</description><key id="2267510">1475</key><summary>Binary field compression causes wrong _source decoding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2011-11-17T11:01:33Z</created><updated>2011-11-20T11:17:50Z</updated><resolved>2011-11-20T10:59:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-20T10:59:36Z" id="2803374">I pushed a fix for it, can you test?
</comment><comment author="ahfeel" created="2011-11-20T11:05:05Z" id="2803390">Just tried, works like a charm on my test case ! Thanks Shay !
</comment><comment author="kimchy" created="2011-11-20T11:17:50Z" id="2803434">Great, closing it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file></files><comments><comment>Binary field compression causes wrong _source decoding, closes #1475.</comment></comments></commit></commits></item><item><title>Broken plugin script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1474</link><project id="" key="" /><description>I downloaded today the 0.18.3 version, but the plugin script for Unix is broken.

The script last line contains the classpath as:

&lt;pre&gt;$ES_HOME/lib/*&lt;/pre&gt;


But instead, it should be:

&lt;pre&gt;:$ES_HOME/lib/*&lt;/pre&gt;


The problem happens because the shell expands the '*' in many files separated by space. But in the latter it doesn't.
</description><key id="2259902">1474</key><summary>Broken plugin script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">leogamas</reporter><labels><label>bug</label><label>v0.18.4</label><label>v0.19.0.RC1</label></labels><created>2011-11-16T18:33:30Z</created><updated>2011-11-24T07:48:21Z</updated><resolved>2011-11-16T19:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-16T19:00:00Z" id="2765954">Argh, right... . I will add `"` around it.
</comment><comment author="rmoriz" created="2011-11-23T22:47:42Z" id="2856045">FYI: If you use _homebrew_, the plugin script is still broken with 0.18.4 (due to homebrew-style directory changes)

```
exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/plugins/PluginManager
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.PluginManager
```

potential fix: https://github.com/mxcl/homebrew/pull/8760
</comment><comment author="kimchy" created="2011-11-24T07:48:20Z" id="2859136">@rmoriz: thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Broken plugin script, closes #1474.</comment></comments></commit></commits></item><item><title>Allow empty Strings to be null for Number's and don't autodetect empty string fields as string types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1473</link><project id="" key="" /><description>Request from thread: NumberFormatException issues caused by an empty string 
https://groups.google.com/group/elasticsearch/browse_thread/thread/0d30ae7b65084ab0/5d443b25a079db52#5d443b25a079db52

...."could please a feature be
introduced to have a flag at numbers like integer, float, etc, so we
can have ES make empty Strings a "null" value instead?
Currently an empty String "" ceates a
org.elasticsearch.index.mapper.MapperParsingException: Failed to parse
[field] exception; "....
</description><key id="2254994">1473</key><summary>Allow empty Strings to be null for Number's and don't autodetect empty string fields as string types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kbachl</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2011-11-16T10:59:43Z</created><updated>2011-11-21T16:02:25Z</updated><resolved>2011-11-21T16:02:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file></files><comments><comment>Allow empty Strings to be null for Number's and don't autodetect empty string fields as string types, closes #1473.</comment></comments></commit></commits></item><item><title>implement external_quiet version type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1472</link><project id="" key="" /><description>I wanted to change elasticsearch so it didn't throw a VersionConflictException if it received a create/index/delete request for a version less than the current index version. It returns the normal 'ok' response but the version number returned is the latest index version.

The reasoning behind this is; I am writing a very thin wrapper to elasticsearch, and any request into the wrapper after elasticsearch is put onto a messagequeue, if putting on that queue fails I return a specific 'temporary' error and the client is supposed to resend the request. When they resend the request, I didn't want it to fail at elasticsearch, hence this version type of external_quiet....

here is a pull request for my solution to this problem:- https://github.com/elasticsearch/elasticsearch/pull/1464
</description><key id="2254883">1472</key><summary>implement external_quiet version type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ianAndrewClark</reporter><labels /><created>2011-11-16T10:47:21Z</created><updated>2014-07-18T08:37:36Z</updated><resolved>2014-07-18T08:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-11-16T10:54:44Z" id="2758194">@ianAndrewClark so with your change, the update would happen, or it would be ignored?

If the former, then you could just not send version numbers.
If the latter, then I'm not sure this would be correct, because you've missed a change that possibly should be applied.

Perhaps I'm missing something?
</comment><comment author="tallpsmith" created="2011-11-16T11:10:32Z" id="2758301">I have to say this is prescient because we had to work around this in a case where we had a an application incrementally updating an index and another process rebuilding large chunks of the index.  The latter hits the DB to get all the results to index, and at that point it grabs the Version property (a lastupdated timestamp is our version), by the time the indexing process gets around to send a particular record, the timestamp/version value may have been changed, but the bulk indexing scrolling through a large result set still has the stale copy of the version.

We ended up catching the exception in this case and ignoring it, because in this use case the latest version IS already in the index and the bulk indexers copy of the state is stale and can be safely ignored.  

Just like we have this putIfAbsent behaviour with this versioning, it would be nice to have a putIfNewer behavior.
</comment><comment author="ianAndrewClark" created="2011-11-16T11:46:06Z" id="2758552">@clintongormley as discussed on irc, the latter it would ignore the change and the response would be just like the ok response but the version number would be the latest index version.

see this gist/script:- https://gist.github.com/1369898

I could wrap this error in my wrapping service, but in the case of bulk operations I'm trying to avoid parsing the response.
</comment><comment author="clintongormley" created="2014-07-18T08:37:34Z" id="49407891">After discussion, we've decided that this is something that should be handled client side.  The client can simply ignore the conflict errors.  Also, the exception includes the latest version number.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inner queries not resolved correctly in has_child filter when searching directly against the parent type (in the URI for example)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1471</link><project id="" key="" /><description>Reduction ([download](http://pastie.org/pastes/2869261/text)):

```
curl -XDELETE 'localhost:9200/test?pretty=true'
echo
curl -XPOST 'localhost:9200/test?pretty=true' -d '{}'
echo
curl -XPOST 'localhost:9200/test/blog/_mapping?pretty=true' -d '{"blog":{"properties":{"name":{"type":"string"}}}}'
echo
curl -XPOST 'localhost:9200/test/post/_mapping?pretty=true' -d '{"post":{"_parent":{"type":"blog"},"properties":{"priority":{"type":"integer"}}}}'
echo
curl -XPUT 'localhost:9200/test/blog/1?refresh=true&amp;pretty=true' -d '{"name":"Blog"}'
echo
curl -XPOST 'localhost:9200/test/post?parent=1&amp;refresh=true&amp;pretty=true' -d '{"priority":1}'
echo
curl -XPOST 'localhost:9200/test/post?parent=1&amp;refresh=true&amp;pretty=true' -d '{"priority":2}'
echo
echo "--- This correctly returns a document ---"
curl -XPOST 'localhost:9200/test/post/_search?pretty=true' -d '{"query":{"term":{"priority":2}}}'
echo
echo "--- This should return a document, but it doesn't ---"
curl -XPOST 'localhost:9200/test/blog/_search?pretty=true' -d '{"query":{"constant_score":{"filter":{"has_child":{"type":"post","query":{"term":{"priority":2}}}}}}}'
echo
echo "--- This correctly returns nothing ---"
curl -XPOST 'localhost:9200/test/post/_search?pretty=true' -d '{"query":{"range":{"priority":{"gt":5}}}}'
echo
echo "--- This should return nothing, but it returns something ---"
curl -XPOST 'localhost:9200/test/blog/_search?pretty=true' -d '{"query":{"constant_score":{"filter":{"has_child":{"type":"post","query":{"range":{"priority":{"gt":5}}}}}}}}'
echo
```
</description><key id="2250125">1471</key><summary>Inner queries not resolved correctly in has_child filter when searching directly against the parent type (in the URI for example)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels><label>bug</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-15T22:21:52Z</created><updated>2011-11-16T12:15:46Z</updated><resolved>2011-11-16T12:15:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file></files><comments><comment>Inner queries not resolved correctly in has_child filter when searching directly against the parent type (in the URI for example), closes #1471.</comment></comments></commit></commits></item><item><title>Couchdb River: since changes parameter (seq) is not url encoded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1470</link><project id="" key="" /><description /><key id="2249678">1470</key><summary>Couchdb River: since changes parameter (seq) is not url encoded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-15T21:40:20Z</created><updated>2011-11-17T10:35:14Z</updated><resolved>2011-11-15T21:41:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="opie4624" created="2011-11-16T21:07:31Z" id="2767925">BigCouch is returning:

```
 "last_seq":[136374,"g1AAAAIReJzLYWBg4MxgTmGQT8pMT84vTc5wKC5ITc5MzMksLtFLrkyqzNHLyU9OzMkBKmRKZMhjYZA-XiiTlcTAYOJDtD6gLhmgLiBl3vfYAqTZdTtIswRcc0FaMjZdFkDlQCo4NDQUpMuOgRQrQ4C6gFQ-0GqQZsfvpGguAOoCUt2PLfpAmm0jiXJvD1A5kFq-atUqkC5jFlKsXAHUBaQOF8ocB4fvEVI0HwHqAlL3gSEG0uygS5R7HwCVA6n_QADSZZqXBQCZGar4"]}
```

elasticsearch is requesting:

```
 /my_db/_changes?feed=continuous&amp;include_docs=true&amp;heartbeat=10000&amp;since=%5B136374%2C+g1AAAAIReJzLYWBg4MxgTmGQT8pMT84vTc5wKC5ITc5MzMksLtFLrkyqzNHLyU9OzMkBKmRKZMhjYZA-XiiTlcTAYOJDtD6gLhmgLiBl3vfYAqTZdTtIswRcc0FaMjZdFkDlQCo4NDQUpMuOgRQrQ4C6gFQ-0GqQZsfvpGguAOoCUt2PLfpAmm0jiXJvD1A5kFq-atUqkC5jFlKsXAHUBaQOF8ocB4fvEVI0HwHqAlL3gSEG0uygS5R7HwCVA6n_QADSZZqXBQCZGar4%5D
```

It looks like elasticsearch isn't handling bigcouch's use of json arrays as a sequence identifier. The string doesn't appear to be quoted when it's passed back in the &amp;since= parameter.
</comment><comment author="kimchy" created="2011-11-17T10:35:14Z" id="2774514">Yes, it looks like the problem is that I actually do not expect something different than a string as the result of seq. Can be fixed, though kindda annoying decision made by bigcouch to implement it like this. Open a new issue for it?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file></files><comments><comment>Couchdb River: since changes parameter (seq) is not url encoded, closes #1470.</comment></comments></commit></commits></item><item><title>Highlighting on term vector enabled field should not highlight filters by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1469</link><project id="" key="" /><description>Filters usually should not be highlighted, and the term vector based one is the only one that allows for it. The `highlight_filter` element can be set to `true` to enable it.
</description><key id="2242461">1469</key><summary>Highlighting on term vector enabled field should not highlight filters by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-15T11:41:38Z</created><updated>2013-01-17T06:41:12Z</updated><resolved>2011-11-15T11:42:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mahdeto" created="2013-01-17T06:41:12Z" id="12356368">Is there a way to access this from the Java API? I am on 0.19.12
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file></files><comments><comment>Highlighting on term vector enabled field should not highlight filters by default, closes #1469.</comment></comments></commit></commits></item><item><title>Indices Stats API: Providing groups as part of the HTTP API does not return stats for those groups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1468</link><project id="" key="" /><description /><key id="2241138">1468</key><summary>Indices Stats API: Providing groups as part of the HTTP API does not return stats for those groups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-15T08:29:57Z</created><updated>2011-11-15T08:32:06Z</updated><resolved>2011-11-15T08:32:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file></files><comments><comment>Indices Stats API: Providing groups as part of the HTTP API does not return stats for those groups, closes #1468.</comment></comments></commit></commits></item><item><title>Nested facet execution can fail with ArrayIndexOutOfBounds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1467</link><project id="" key="" /><description>Nested facet execution can fail with ArrayIndexOutOfBounds
</description><key id="2240836">1467</key><summary>Nested facet execution can fail with ArrayIndexOutOfBounds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.10</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-15T07:23:40Z</created><updated>2011-11-15T07:24:08Z</updated><resolved>2011-11-15T07:24:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/nested/NestedChildrenCollector.java</file></files><comments><comment>Nested facet execution can fail with ArrayIndexOutOfBounds, closes #1467.</comment></comments></commit></commits></item><item><title>Add counts of currently executing get operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1466</link><project id="" key="" /><description /><key id="2239780">1466</key><summary>Add counts of currently executing get operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-15T03:27:13Z</created><updated>2014-07-16T21:55:54Z</updated><resolved>2011-11-15T08:22:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-15T08:22:24Z" id="2742150">Thanks!, pushed to 0.18 and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>XContentBuilder to handle extended classes of java.util.Date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1465</link><project id="" key="" /><description>class: org.elasticsearch.common.xcontent.XContentBuilder
method: field(String name, Object value)

``` java
        } else if (type == Byte.class) {
            field(name, ((Byte) value).byteValue());
        } else if (type == Boolean.class) {
            field(name, ((Boolean) value).booleanValue());
        } else if (type == Date.class) {
            field(name, (Date) value);
        } else if (type == byte[].class) {
```

While it is ok to use `type == xxx.class` for final java classes I don't think it should be used for Date. Instead `instanceof` should be used so that the following method calls behave the same:

``` java
    xContentBuilder.field(field, (Date) dateValue)
    xContentBuilder.field(field, (Object) dateValue)
```
</description><key id="2234600">1465</key><summary>XContentBuilder to handle extended classes of java.util.Date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dawi</reporter><labels><label>enhancement</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-14T19:06:59Z</created><updated>2011-11-15T09:34:46Z</updated><resolved>2011-11-15T09:34:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-15T09:21:47Z" id="2742546">Is that because of classes that extend Date, like java.sql.Date? Cause it will only happen for that case, what you posted works well.
</comment><comment author="dawi" created="2011-11-15T09:27:09Z" id="2742592">Yes exactly. Forgot to mention that.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file></files><comments><comment>XContentBuilder to handle extended classes of java.util.Date, closes #1465.</comment></comments></commit></commits></item><item><title>implement external_relaxed versioning type - (Doesn't throw VersionConfl...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1464</link><project id="" key="" /><description>...ictExceptions)

I wanted to change elasticsearch so it didn't throw a VersionConflictException if it received a create/index/delete request for a version less than the current index version. It returns the normal 'ok' response but the version number returned is the latest index version.

The reasoning behind this is; I am writing a very thin wrapper to elasticsearch, and any request into the wrapper after elasticsearch is put onto a messagequeue, if putting on that queue fails I return a specific 'temporary' error and the client is supposed to resend the request. When they resend the request, I didn't want it to fail at elasticsearch, hence this version type of external_relaxed....

feedback appreciated.
</description><key id="2233102">1464</key><summary>implement external_relaxed versioning type - (Doesn't throw VersionConfl...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ianAndrewClark</reporter><labels /><created>2011-11-14T16:55:56Z</created><updated>2014-06-12T16:03:32Z</updated><resolved>2014-05-06T11:35:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-15T08:37:33Z" id="2742241">Looks good, the only thing that I would like to find is a better name for it... . Maybe external_quiet?
</comment><comment author="ianAndrewClark" created="2011-11-16T09:10:44Z" id="2757351">Cool, that's a much better name, I'll change that now.
</comment><comment author="ianAndrewClark" created="2011-11-16T10:47:53Z" id="2758124">I created an issue for it as well:- https://github.com/elasticsearch/elasticsearch/issues/1472
</comment><comment author="kimchy" created="2011-11-16T11:29:18Z" id="2758435">Heya, this implementation is problematic, because no exception is thrown, the operation will end up being replication, and we need to somehow prevent it in this case....
</comment><comment author="ianAndrewClark" created="2011-11-16T12:15:04Z" id="2758738">sorry to paraphrase what you said; as no exception is thrown in this operation on the primary shard it will be repeated on the replica shards, which is pointless and we should prevent this?

a naive/ugly approach would be to check that the primary's response version is the requested version and therefore that a change happened, erm I'll look into it now.
</comment><comment author="ianAndrewClark" created="2011-11-16T15:18:18Z" id="2760878">that last sentence was a naive comment from me, struggling to see how I can stop the action being performed on the replicas without an exception.... still looking.
</comment><comment author="kimchy" created="2011-11-18T11:42:40Z" id="2788217">Yes, its tricky..., not something that was expected when the code was written....
</comment><comment author="ianAndrewClark" created="2011-11-18T14:20:06Z" id="2789526">Yeah I see that now. I can think of two approaches, neither have easy solutions (er work).

a QuietVersionConflictEngineException that has a 200 response code, and trying to override it's output such that it looks like a normal response. A few problems with that XContentThrowableRestResponse doesn't look anything like a normal response, and I'd still have to fix up TransportShardBulkAction. Ugh.

or

somehow override ignoreReplicas() with a switch on those actions(ie:- TransportIndexAction) to stop the action on the replicas, but not sure how to do that in manner that conforms to the code in a thread-safe manner.

I'm not giving up, just haven't worked it out yet. Let me know if you have any ideas.
</comment><comment author="clintongormley" created="2014-05-06T11:35:14Z" id="42292107">Closed by #4993
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date Range: Inclusive upper range does not round up properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1463</link><project id="" key="" /><description>When using inclusive upper range for date range queries / filter, it does not round up as expected. For example, when using `2011-11-11` as the inclusive upper range, it will not including any hits in the `2011-11-11` day.

Since this is a braking change, the old behavior can be reverted by setting `index.mapping.date.parse_upper_inclusive` in the config file to `false`.
</description><key id="2231429">1463</key><summary>Date Range: Inclusive upper range does not round up properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-14T15:16:20Z</created><updated>2011-11-14T21:56:01Z</updated><resolved>2011-11-14T15:16:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ahfeel" created="2011-11-14T21:56:01Z" id="2737279">Wow many many many many thanks, this is really an awesome fix Shay ! You rock !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/simple/SimpleSearchTests.java</file></files><comments><comment>Date Range: Inclusive upper range does not round up properly, closes #1463.</comment></comments></commit></commits></item><item><title>Index shard search slow log (query and fetch)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1462</link><project id="" key="" /><description>Shard level slow search log allows to log slow search (query and fetch executions) into a dedicated log file.

Thresholds can be set for both the query phase of the execution, and fetch phase, here is a sample:

```
#index.search.slowlog.threshold.query.warn: 10s
#index.search.slowlog.threshold.query.info: 5s
#index.search.slowlog.threshold.query.debug: 2s
#index.search.slowlog.threshold.query.trace: 500ms

#index.search.slowlog.threshold.fetch.warn: 1s
#index.search.slowlog.threshold.fetch.info: 800ms
#index.search.slowlog.threshold.fetch.debug: 500ms
#index.search.slowlog.threshold.fetch.trace: 200ms
```

By default, none are enabled (set to `-1`). Levels (`warn`, `info`, `debug`, `trace`) allow to control under which logging level the log will be logged. Not all are required to be configured (for example, only `warn` threshold can be set). The benefit of several levels is the ability to quickly "grep" for specific thresholds breached.

The logging is done on the shard level scope, meaning the execution of a search request within a specific shard. It does not encompass the whole search request, which can be broadcast to several shards in order to execute. Some of the benefits of shard level logging is the association of the actual execution on the specific machine, compared with request level.

All settings are index level settings (and each index can have different values for it), and can be changed in runtime using the index update settings API.

The logging file is configured by default using the following configuration (found in `logging.yml`):

```
index_search_slow_log_file:
  type: dailyRollingFile
  file: ${path.logs}/${cluster.name}_index_search_slowlog.log
  datePattern: "'.'yyyy-MM-dd"
  layout:
    type: pattern
    conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"
```
</description><key id="2229925">1462</key><summary>Index shard search slow log (query and fetch)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-14T12:10:02Z</created><updated>2011-11-14T12:11:08Z</updated><resolved>2011-11-14T12:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/logging/Loggers.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/slowlog/ShardSlowLogSearchService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/stats/ShardSearchModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/stats/ShardSearchService.java</file></files><comments><comment>Index shard search slow log (query and fetch), closes #1462.</comment></comments></commit></commits></item><item><title>Add counts of currently executing index, delete, query and fetch operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1461</link><project id="" key="" /><description>Stats that were added in 0.18 are great. Sometimes, we also want to know what elasticsearch is actually doing at the moment. Could you add something like this?
</description><key id="2227536">1461</key><summary>Add counts of currently executing index, delete, query and fetch operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-14T06:23:26Z</created><updated>2014-07-16T21:55:55Z</updated><resolved>2011-11-15T03:29:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-14T08:26:28Z" id="2727715">Looks great!, I will pull this into 0.18 and master. How about completing the picture and adding current "get" operations as well?
</comment><comment author="imotov" created="2011-11-14T12:18:46Z" id="2729726">Sure, I will add it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add 'unique_terms' for terms facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1460</link><project id="" key="" /><description>Related to https://github.com/elasticsearch/elasticsearch/issues/1029.

It would be useful if we can get total number of unique terms for a particular facet.
One use case is that we store "id" to a field. If we get number of unique terms on the field, we know how many duplicates are included in result set.
</description><key id="2224007">1460</key><summary>Add 'unique_terms' for terms facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IoriH</reporter><labels /><created>2011-11-14T00:13:18Z</created><updated>2013-08-26T20:24:25Z</updated><resolved>2013-08-26T20:24:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2012-07-18T10:25:58Z" id="7062426">duplicate of #1044
</comment><comment author="javanna" created="2013-08-26T20:24:25Z" id="23291814">Closing as duplicate of #1044 . The mentioned count there is indeed the number of unique terms.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for field excerpt when no highlighting is possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1459</link><project id="" key="" /><description>The majority of search frontends usually display the highlighted chunks below the hit title. The problem is that on non highlighting queries, such as a numerical range, no highlighting chunk will be brought back in the search. If you don't want your display to look bad, you usually display an excerpt of the beginning of the file, or of the first contiguous paragraph, etc... The easy way to do that is to always ask for the full field content in your ES query, but this is a very costly solution in any case (unnecessary memory and bandwidth).

I would like to suggest an enhancement to ElasticSearch to add the possibility to get back a excerpt of the highlighted field in case the query doesn't highlight. The syntax would be:

```
"highlight" : {
    "fields" : {
        "content" : {
            "fragment_size" : 150,
            "number_of_fragments" : 3,
            "fallback" : "excerpt_basic" // or "excerpt_firstparagraph" or any other more advanced impl to come
        }
    }
}
```

Jérémie
</description><key id="2224004">1459</key><summary>Add support for field excerpt when no highlighting is possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2011-11-14T00:12:32Z</created><updated>2013-09-20T16:41:45Z</updated><resolved>2013-09-20T16:41:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-11-14T05:51:49Z" id="2726945">Easy workaround is to have a specific field of "no-highlighted" snippet of text (include_in_all = false if you use _all). You would return this field for every search hit and use for display only if no highlights are found. Given that snippet texts are not large this approach is probably not a big deal from the network transfer POW. WDYT?
</comment><comment author="ahfeel" created="2011-11-14T22:02:17Z" id="2737371">That would still be potentially a lot of wasted storage. Shay mentioned on IRC that bringing back this excerpt could be done at a relatively low cost, which sounds way better than having additional content sent to ES, stored and always transfered back.
</comment><comment author="lukas-vlcek" created="2011-11-14T23:27:44Z" id="2738510">That would be cool.
</comment><comment author="nik9000" created="2013-09-20T14:59:36Z" id="24816504">This is getting some attention under this issue:  #1171.  I propose we call this a duplicate of that issue.  This issue talks about a slightly different light than the other which is more concerned with sized based fragments rather than paragraph or sentence based ones but the substance of the problem is the same.  In the associated pull request @javanna has mentioned sentence based highlighting.
</comment><comment author="javanna" created="2013-09-20T16:41:45Z" id="24823677">Thanks @nik9000 I had missed this issue. Yes it looks like a duplicate of #1171 , where we are trying to add support for returning non highlighted fragments in case there's nothing to highlight. The tricky bit there is how to generate the excerpt. Returning the first sentence seems like a good idea but it should not be the only option available, as it wouldn't respect the expected length of the fragments.

Closing this one, I would suggest folks to have a look at #1171 and move the discussion there. Feedback is appreciated!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Using root object level mapping (_size, _source) can cause reparsing of the mapping on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1458</link><project id="" key="" /><description /><key id="2222436">1458</key><summary>Using root object level mapping (_size, _source) can cause reparsing of the mapping on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-13T18:27:35Z</created><updated>2011-11-13T18:56:33Z</updated><resolved>2011-11-13T18:56:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file></files><comments><comment>Using root object level mapping (_size, _source) can cause reparsing of the mapping on startup, close #1458.</comment></comments></commit></commits></item><item><title>Percolate / Analyzer API can hang if it fails to execute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1457</link><project id="" key="" /><description>This happens due to wrong prefer local handling, where it misses the "next" execution of a shard.
</description><key id="2222209">1457</key><summary>Percolate / Analyzer API can hang if it fails to execute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-13T17:30:36Z</created><updated>2011-11-13T17:54:26Z</updated><resolved>2011-11-13T17:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/single/custom/TransportSingleCustomOperationAction.java</file></files><comments><comment>Percolate / Analyzer API can hang if it fails to execute, closes #1457.</comment></comments></commit></commits></item><item><title>Mavenization to support development in NetBeans IDE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1456</link><project id="" key="" /><description>Hi Shay,

I've struggled a bit with git but now I have hopefully the correctly changed files to compile and test ElasticSearch from within NetBeans as there is no working gradle plugin available. Please ignore all the commits :) just have a look in the changed files. If depencencies changed, all the pom's can be automatically regenerated via gradlew and the script pkg/mavenize.sh

Important: I had to move 2 config files originally placed under java now under resources (to be maven compatible) - does this affect your build?

What I didn't understand: why is sigar not included in the gradle deps?

Regards,
Peter.
</description><key id="2217902">1456</key><summary>Mavenization to support development in NetBeans IDE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-11-12T17:27:36Z</created><updated>2014-06-25T11:07:24Z</updated><resolved>2011-12-07T21:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-11-12T18:05:00Z" id="2718588">I'll check if I can avoid moving the config/text files (as there are more missing).
</comment><comment author="dadoonet" created="2011-11-18T17:58:09Z" id="2792137">I just love this pull request !
Just wondering if your aim is to have a pom.xml file always available in github repository or if developpers will have to generate it ?

Thanks
David.
</comment><comment author="karussell" created="2011-11-18T20:11:07Z" id="2793686">Thanks :)

Well the aim is to always have a pom.xml in the repository :) 

But for new dependencies or new versions I wanted to automate the updating process of this pom.xml from the gradle file. Thats why the mavenize.sh (some parts are not necessary anymore e.g. moving the configs)
</comment><comment author="kimchy" created="2011-11-24T18:22:37Z" id="2867996">What I am hoping to eventually do is to move ann the plugins to their own repo, and then, there will be just one module in elasticsearch, which will simplify things a lot. Back then, the reason why I gave up on maven was its poor support for multi module projects.
</comment><comment author="karussell" created="2011-11-24T20:15:31Z" id="2868849">Ok. I would really go the gradle route if there would be better IDE support as with maven I really miss the flexibility of a real programming language in my build file (e.g. for deploying). 

So, do you think that it would be wiser to close this issue here and just keep it as a reference for others? Or will you go the (probably not that clear) route and add the poms to the project?

BTW: the last commits are my latest trials to submit several pull requests. I made the mistake that this pull request is from my master and now I cannot do further changes to it without popping up here :/
</comment><comment author="kimchy" created="2011-11-24T20:23:29Z" id="2868893">I think we can go a different route. I don't use any plugin in intellij for gradle, the intellij configuration is committed to the repo, and you can simply clone and import and it "just works". We can do something similar with netbeans, though now, with intellij community edition, the more people moving to use it, the better :)
</comment><comment author="karussell" created="2011-11-24T23:10:55Z" id="2869958">&gt; though now, with intellij community edition

ah, didn't know that it has also the gradle support. But the IntelliJ-only-one-project-open-at-a-time thing is too limiting for me (e.g. work vs. hobby projects ;))

&gt; We can do something similar with netbeans

but this would mean: either use a normal java project (done via ant under the hood) and manually adding the dependencies ... or using the pom.xml
</comment><comment author="dadoonet" created="2011-11-25T08:20:19Z" id="2872263">&gt; the reason why I gave up on maven was its poor support for multi module projects.

I'm afraid I have to disagree. I use every day maven for complex projects with several sub-modules and I don't have any bad feedback.
Yes, that's true that you can't easily do complex things, but should we ?
Yes, Gradle is more flexible. I see it as a mix between ant and maven.
I love maven because it gives me some guides and rules (even constraints). I love maven because when I come from a project to another one, I'm "at home". That's the same for all my coworkers.

Also, I think it's a really good idea to have gradle and/or maven and be able to setup our IDE with simple commands like 

```mvn eclipse:eclipse

```

We don't know what IDE developpers use. Perhaps some use only Notepad and the command line :-( 
IMO, I'm not sure it's a good idea to maintain settings for each IDE (I use eclipse - not sure we have to maintain a pom.xml, gradle settings, Intellij settings, netbean settings and .project file aka eclipse settings)... 


I started to mavenized ES project some months ago but I was stucked with the jarjar plugin. I was starting to get some nice results but I didn't have enough spare time to go deeper.

If I understood Peter's commit, I can see that we have to manually create the es-jarjar jar (with gradle) before being able to play with maven, right ? (I have to confess that I didn't try to get your source code and play with).
```
</comment><comment author="karussell" created="2011-11-25T08:35:16Z" id="2872373">&gt; Perhaps some use only Notepad and the command line :-( 

well then gradle is also fine. also Shay should use the things which he is most comfortable with :)

&gt; netbean settings

there is no such thing

&gt; before being able to play with maven, right?

yes, but this is just a java programm to create the jarjar. not entirely sure if it is necessary at all (Shay uses this to avoid version conflicts with guice, netty, ...)

&gt; (I have to confess that I didn't try to get your source code and play with).

not that difficult. just create the jarjar and use the (already generated) pom.xml of this pull request ...
</comment><comment author="jprante" created="2011-11-28T01:40:48Z" id="2891771">Hey, I did port several versions of Elasticsearch to Maven and Netbeans, I started several months ago. It's not complete with all the plugins though, just what I needed. The jarjar thing was reset to the original dependencies, and yes, I rebuild all dependencies from source in Maven by the way (well, almost everything).
Finally, besides building up a source-complete local repo in Subversion und Maven, I put the whole beast into Jenkins and Sonar with Findbugs for some fancy analysis.
If you like to have a look, visit http://demeter.hbz-nrw.de/sonar
I put up Lucene in Sonar for you as a bonus ;-) Cheers, Jörg
</comment><comment author="karussell" created="2011-11-28T07:45:39Z" id="2893928">&gt; I did port several versions of Elasticsearch to Maven and Netbeans

Do you need to do this for every version or do you have a script?

&gt; The jarjar thing was reset to the original dependencies

what do you mean here?
</comment><comment author="jprante" created="2011-11-28T08:25:12Z" id="2894230">Jarjar helps to ship a single jar and removes external jar dependencies by rewriting package names. In Elasticsearch, jarjar moves packages from e.g. Guice), Joda, GNU Trove etc. into something like org.elasticsearch.common.xyz. Shay tries to minimize external dependencies to obtain more reliability, I think. But I dropped the jarjar preprocessing and reset the original external dependencies in Maven's pom.xml.

The split into Maven tree-style src/{main,test}/java and src/{main,test}/resources has also to be done. I use a mixture of diff/patch und a simple find/replace script to apply the version changes. It can't be fully automated though. I use Maven tests (surefire) to validate the result. It takes some hours to edit a few number of patches for generating a working Elasticsearch version, it depends on the volume of Shay's changes.
</comment><comment author="karussell" created="2011-11-28T09:23:45Z" id="2894658">&gt; But I dropped the jarjar preprocessing and reset the original external dependencies in Maven's pom.xml

So you really changed every class in ElasticSearch to use the original dependency?

&gt; The split into Maven tree-style src/{main,test}/java and src/{main,test}/resources has also to be done

not necessarily, it can be done via configuring this in the pom.xml

&gt; It takes some hours to edit a few number of patches for generating a working Elasticsearch version, 
&gt; it depends on the volume of Shay's changes.

Hmmh, I think the changes are too intensive ... where can I view a resulting pom.xml in your repo? (didn't find it via browsing)
</comment><comment author="jprante" created="2011-11-28T12:32:37Z" id="2896280">I changed the package names to the original ones in all the java sources, yes.
The open source repo is at http://demeter.hbz-nrw.de/svn/os
Current ES parent pom is http://demeter.hbz-nrw.de/svn/os/elasticsearch/release/0.18.4/pom.xml
I need an approval system that our ES based search app is truely using open source licenses in all its dependencies, therefore I decided to collect and rebuild the code from groundup, together with the licenses. And, more technically, I gathered the whole sources also to have them ready for fixing them, when rebuilding the complete ES software stack using OpenJDK 1.7.0.
</comment><comment author="karussell" created="2011-12-07T21:56:22Z" id="3054149">Thx Shay :)

Kudos for all the refactoring work!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: Search requests execute by mistake on the networking http IO thread, causing other http operations to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1455</link><project id="" key="" /><description>Search: Search requests execute by mistake on the networking http IO thread, causing other http operations to hang
</description><key id="2213087">1455</key><summary>Search: Search requests execute by mistake on the networking http IO thread, causing other http operations to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.10</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-11T20:20:41Z</created><updated>2011-11-11T20:21:13Z</updated><resolved>2011-11-11T20:21:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/PlainShardsIterator.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file></files><comments><comment>Search: Search requests execute by mistake on the networking http IO thread, causing other http operations to hang, closes #1455.</comment></comments></commit></commits></item><item><title>CancelledKeyException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1454</link><project id="" key="" /><description>ElasticSearch 0.18.2, TransportClient

Sometimes have following lines in logs:

```
12:24:58,006  WARN New I/O client boss #11 NioClientSocketPipelineSink:internalWarn:105 - Unexpected exception in the selector loop.
java.nio.channels.CancelledKeyException
    at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73)
    at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:87)
    at java.nio.channels.SelectionKey.isConnectable(SelectionKey.java:336)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processSelectedKeys(NioClientSocketPipelineSink.java:353)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:276)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
```
</description><key id="2208893">1454</key><summary>CancelledKeyException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">splix</reporter><labels /><created>2011-11-11T12:28:14Z</created><updated>2013-04-05T16:06:29Z</updated><resolved>2013-04-05T16:06:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="felipehummel" created="2012-03-21T15:07:39Z" id="4619168">Any news on this issue? I'm also having. It started after I upgraded from 0.17.6 to 0.19.0
It appears on my indexing program (using the ES Java API).
</comment><comment author="clintongormley" created="2013-04-05T16:06:29Z" id="15964869">No further reports of this in one year - Assuming fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> Fix processing of regex patterns in large terms facet requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1453</link><project id="" key="" /><description>Regex patterns are ignored in large (more then 5000 facets) terms facet requests. For example:

```
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '{
    "size" : 0,
    "query" : { "query_string" : {"query" : "*:*"} },
    "facets" : {
        "message" : { 
            "terms" : {
                "field" : "message",
                "regex" : "elas[a-z]*",
                "size" : "20000"
            } 
       }
    }
}'
```
</description><key id="2204736">1453</key><summary> Fix processing of regex patterns in large terms facet requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-11T00:18:33Z</created><updated>2014-07-02T11:33:36Z</updated><resolved>2011-11-13T10:49:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-13T10:49:25Z" id="2721697">Pushed to master and 0.18 branch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set ES index to read only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1452</link><project id="" key="" /><description>It should be possible to set an Index in elasticsearch to readyOnly, so no new documents can be added.

Some more details can be found here in the forum post:
https://groups.google.com/forum/?pli=1#!searchin/elasticsearch/readonly/elasticsearch/ylsKoQhLstk/xtnrH_LCpuIJ
</description><key id="2202358">1452</key><summary>Set ES index to read only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ruflin</reporter><labels /><created>2011-11-10T20:30:27Z</created><updated>2012-01-01T12:29:35Z</updated><resolved>2011-12-27T18:36:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-11-13T15:29:35Z" id="2722746">What is the aim? Performance improvements, immutability for the sake of a cleaner system or better security?
</comment><comment author="ruflin" created="2011-11-13T15:46:08Z" id="2722819">The aim is immutability. Perhaps I can describe our use case a little bit more in detail.

If we change the structure of the index, the complete index has to be recreated. During recreating the new index, we don't want that new documents are added to the old index to make sure, all new documents are added to the new index after creating it. One option would be to disable indexing on all our servers. But we would prefer to be able to set this on the server side, so we can make sure that not servers add documents by accident and we don't have these documents in the new index.

Because of the fact that shay mentioned in the Forum, I assume there won't be any performance gain.
</comment><comment author="bbgordonn" created="2011-12-08T23:11:11Z" id="3071829">This would be useful for e.g. updating to a new major version (e.g. 0.18 to 0.19): mark all indices read-only, take half the cluster offline and upgrade it, restart, and repeat for the other half. Searches can proceed against both halves so availability is maintained but no new new documents or deletes can occur in half while they are disconnected.

This patch seems to basically work as a proof of concept. Of course, I don't know if this is the approach that should be taken--feel free to tell me it's completely wrong! It adds a new cluster block for the index when the index.read_only setting is true.

https://gist.github.com/f089f90787e5a691668d
</comment><comment author="bbgordonn" created="2011-12-09T14:50:19Z" id="3079979">Having thought about this more... the above is a simplified description of upgrading, but maybe too simple. For many applications action.auto_create_index would be needed. In the even more general case one would also need a way to disable changes to mappings, index templates, etc.
</comment><comment author="ruflin" created="2011-12-09T14:54:43Z" id="3080014">I would also prefer if the index would not allow any changes as you described in the second part. In our use case, there shouldn't be any changes like mappings etc. on the old index, but still it would be nice to have the guarantee, that nothing was changed during upgrading.
</comment><comment author="bbgordonn" created="2011-12-13T19:17:11Z" id="3127877">There is a ClusterBlockLevel.METADATA that prevents mapping updates, but it also prevents index exist and deletes calls. (In my case I'd like to be able to delete indices in case I want to split a cluster into two smaller ones).
</comment><comment author="bbgordonn" created="2011-12-13T21:39:34Z" id="3130165">https://gist.github.com/f089f90787e5a691668d is now updated with a cluster.read_only option.
</comment><comment author="ruflin" created="2011-12-14T10:29:30Z" id="3137820"> Nice, looks good.
</comment><comment author="karussell" created="2011-12-14T12:36:27Z" id="3138937">Why not add one or two tests and create a pull request from it?
</comment><comment author="bbgordonn" created="2011-12-14T22:43:07Z" id="3153239">Since kimchy OK'd my basic approach last night, I've been working on exactly that. I can't figure out how to inject initial values for the settings so I gave up on testing those (they would need to go into the cluster state, not the node state).
</comment><comment author="bbgordonn" created="2011-12-15T00:07:07Z" id="3154516">Pull request submitted for master.

kimchy said he didn't want it in 0.18, so I'm not submitting that version. It's in my fork if anyone wants it. I plan to use it during my eventual major version upgrade as a read-only cluster should allow me to work around the fact that a rolling upgrade to a new major version is usually not possible without risking split-brain issues from writes during the upgrade.
</comment><comment author="ruflin" created="2011-12-16T15:42:00Z" id="3178622">Thank you. I will use your fork to implement this functionality in Elastica.
</comment><comment author="kimchy" created="2011-12-27T18:36:24Z" id="3283978">See #1573. I don't think Elastic needs any special support to implement it, just expose the index update settings and cluster update settings API.
</comment><comment author="ruflin" created="2011-12-29T10:25:57Z" id="3299581">That is actually what I planned. I will add it to Elastica_Index_Settings and Elastica_Cluster. Thank you.
</comment><comment author="ruflin" created="2011-12-29T21:11:40Z" id="3305211">I would expect that the following code throws an exception:

curl -XPUT 'http://localhost:9200/elastica_test/'
curl -XPOST http://localhost:9200/elastica_test/_close -d '[]'
curl -XPUT http://localhost:9200/elastica_test/_settings -d '{"index":{"read_only":true}}'
curl -XPOST http://localhost:9200/elastica_test/_open -d '[]'
curl -XPUT http://localhost:9200/elastica_test/test/1 -d '{"hello":"world"}'

But after running this command on the current master branch, I receive a timeout error. Is this as expected? Is it necessary to close the index?
</comment><comment author="kimchy" created="2011-12-30T11:44:32Z" id="3312632">I've ran the command you posted, but none gets stuck for me. Regarding the read only, there is no need to close the index. You can set `index.blocks.read_only` setting to true using indices update settings to make a specific index read only. Or set `cluster.blocks.read_only` to `true` using the cluster update settings API to make the whole cluster read only.
</comment><comment author="ruflin" created="2011-12-30T12:48:57Z" id="3312900">Ok, I checked out the complete ES source again form github and rebuilt it. Now it works as expected. I had some "useless" changes in the code I completely forgot about. Sorry.

What is the difference between the following commands?

curl -XPUT http://localhost:9200/elastica_test/_settings -d '{"index":{"read_only":true}}'
curl -XPUT http://localhost:9200/elastica_test/_settings -d '{"index.read_only":true}'
curl -XPUT http://localhost:9200/elastica_test/_settings -d '{"index":{"blocks.read_only":true}}'
</comment><comment author="kimchy" created="2011-12-30T12:55:34Z" id="3312942">The setting is `index.blocks.read_only`, so the first two won't work (`index.read_only` setting _does not_ work). Settings wise, there is no difference between: `{ "index" : {"blocks.read_only" : true} }`, or `{ "index.blocks.read_only` : true}`. Actually, because its update _index_ settings, you don't even have to have the "index" prefix.
</comment><comment author="ruflin" created="2011-12-30T13:07:43Z" id="3313020">Good to know. Is there a specific reason that index.read_only does not throw an exception and is even stored in the settings?

curl -XPUT http://localhost:9200/elastica_test/_settings -d '{"index":{"doesnothing":"really"}}'
curl -XGET http://localhost:9200/elastica_test/_settings?pretty=true
</comment><comment author="ruflin" created="2012-01-01T12:29:35Z" id="3324384">In more thing I discovered:

`curl -XGET 'http://localhost:9200/_cluster/settings'`

Returns the following:

`{"persistent":{},"transient":{"cluster.blocks.read_only":"false"}}`

But I would expect:

`{"persistent":{},"transient":{"cluster.blocks.read_only":false}}`

Is it somehow possible that the type boolean instead of a string is returned?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>src/test/java/org/elasticsearch/test/integration/cluster/ClusterBlockTests.java</file></files><comments><comment>#1452 closed: block writes or metadata changes if {index,cluster}.read_only is set.</comment></comments></commit></commits></item><item><title>OverlappingFileLockException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1451</link><project id="" key="" /><description>ElasticSearch sometimes fails with error like:

```
Error injecting constructor, java.nio.channels.OverlappingFileLockException
  at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.env.NodeEnvironment
    for parameter 6 at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData
    for parameter 2 at org.elasticsearch.gateway.local.LocalGatewayAllocator.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.gateway.local.LocalGatewayAllocator
  while locating org.elasticsearch.cluster.routing.allocation.allocator.GatewayAllocator
    for parameter 1 at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators
    for parameter 2 at org.elasticsearch.cluster.routing.allocation.AllocationService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.cluster.routing.allocation.AllocationService
    for parameter 5 at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.cluster.metadata.MetaDataCreateIndexService
    for parameter 4 at org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction
    for parameter 6 at org.elasticsearch.action.index.TransportIndexAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.action.index.TransportIndexAction
    for parameter 3 at org.elasticsearch.client.node.NodeClient.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.client.node.NodeClient
  while locating org.elasticsearch.client.Client
    for parameter 1 at org.elasticsearch.rest.action.admin.cluster.node.shutdown.RestNodesShutdownAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.rest.action.admin.cluster.node.shutdown.RestNodesShutdownAction
Caused by: java.nio.channels.OverlappingFileLockException
    at sun.nio.ch.FileChannelImpl$SharedFileLockTable.checkList(FileChannelImpl.java:1166)
    at sun.nio.ch.FileChannelImpl$SharedFileLockTable.add(FileChannelImpl.java:1068)
    at sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:868)
    at java.nio.channels.FileChannel.tryLock(FileChannel.java:962)
    at org.apache.lucene.store.NativeFSLock.obtain(NativeFSLockFactory.java:216)
    at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:74)
    at sun.reflect.GeneratedConstructorAccessor31.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:56)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:103)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:823)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:49)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
```

At this case it displays this stacktrace infinitely, eats all resources and can be killed only by kill -9.

Happens under Ubuntu 11.04, both 32 and 64 bit. 

```
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) Server VM (build 20.1-b02, mixed mode)
```

and 

```
java version "1.6.0_23"
OpenJDK Runtime Environment (IcedTea6 1.11pre) (6b23~pre10-0ubuntu5)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)
```
</description><key id="2198520">1451</key><summary>OverlappingFileLockException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">splix</reporter><labels /><created>2011-11-10T14:49:43Z</created><updated>2015-08-04T16:21:06Z</updated><resolved>2013-04-05T16:07:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="splix" created="2011-11-10T15:51:13Z" id="2696838">It's 0.18.2
</comment><comment author="clintongormley" created="2013-04-05T16:07:04Z" id="15964906">No further reports of this in one year. Assuming fixed
</comment><comment author="synhershko" created="2013-09-17T20:10:36Z" id="24618255">@clintongormley this happens for me when trying to run an ES instance from SD card (don't ask)

ES won't even read indexes on SD. While I'll be happy to see that fixed, I'll completely understand a Won't Fix here :)
</comment><comment author="s1monw" created="2013-09-17T21:11:45Z" id="24622795">@synhershko dude! SD Card?
</comment><comment author="drewr" created="2013-09-17T21:14:59Z" id="24623022">I've run ES from SD and USB media. Helps in testing IO issues. :metal:
</comment><comment author="s1monw" created="2013-09-17T21:17:51Z" id="24623248">what happens if you use `"index.store.fs_lock" : "simple"` @synhershko  
</comment><comment author="kimchy" created="2013-09-17T21:20:42Z" id="24623457">Btw, seems like its coming from `NodeEnvironment`, where we try and lock a data directory for the node.
</comment><comment author="synhershko" created="2013-09-18T06:23:31Z" id="24643412">LOL

This is the stacktrace I see on failure - different than the one reported in this issue:

[2013-09-18 09:10:14,201][INFO ][cluster.metadata         ] [Woodgod] [wiki-enwiki] creating index, cause [api], shards [1]/[0], mappings [wikipage]
[2013-09-18 09:10:14,866][WARN ][indices.cluster          ] [Woodgod] [wiki-enwiki][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [wiki-enwiki][0] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:227)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.nio.channels.OverlappingFileLockException
    at sun.nio.ch.SharedFileLockTable.checkList(FileLockTable.java:255)
    at sun.nio.ch.SharedFileLockTable.add(FileLockTable.java:152)
    at sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:1056)
    at java.nio.channels.FileChannel.tryLock(FileChannel.java:1154)
    at org.apache.lucene.store.NativeFSLock.obtain(NativeFSLockFactory.java:217)
    at org.apache.lucene.store.Lock.obtain(Lock.java:72)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:672)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1377)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:255)
    at org.elasticsearch.index.shard.service.InternalIndexShard.start(InternalIndexShard.java:302)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:157)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)
    ... 3 more
[2013-09-18 09:10:14,873][WARN ][cluster.action.shard     ] [Woodgod] sending failed shard for [wiki-enwiki][0], node[bMTtkWo8Q8yGmupXMBEYYw], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[wiki-enwiki][0] failed recovery]; nested: OverlappingFileLockException; ]]
[2013-09-18 09:10:14,873][WARN ][cluster.action.shard     ] [Woodgod] received shard failed for [wiki-enwiki][0], node[bMTtkWo8Q8yGmupXMBEYYw], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[wiki-enwiki][0] failed recovery]; nested: OverlappingFileLockException; ]]

This happens to me when running ES on an SD card from a Mac. It also happens when I run it from an external FAT32 harddrive connected via USB.

Changing `index.store.fs_lock` as @s1monw suggested doesn't make any difference.
</comment><comment author="russ168" created="2015-01-13T08:43:10Z" id="69712038">I have the same issue when I run ES instance from SD card. The error like: NativeFSLock@/Volumes/elasticsearch-1.4.0.Beta1/data/elasticsearch/nodes/0/indices/test/1/index/write.lock: java.nio.channels.OverlappingFileLockException]; nested: OverlappingFileLockException;
</comment><comment author="Vijendra07Kulhade" created="2015-06-29T19:16:47Z" id="116801159">I am getting the same exception  any help will be appreciated.

Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'esClient': Cannot resolve reference to bean 'esNode' while setting bean property 'node'; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'esNode': Invocation of init method failed; nested exception is java.nio.channels.OverlappingFileLockException
        at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:326)
        at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:107)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1417)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1158)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458)
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:296)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:293)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
        at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:320)
        ... 28 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'esNode': Invocation of init method failed; nested exception is java.nio.channels.OverlappingFileLockException
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1512)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:521)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458)
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:296)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:293)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
        at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:320)
        ... 38 more
Caused by: java.nio.channels.OverlappingFileLockException
        at sun.nio.ch.SharedFileLockTable.checkList(FileLockTable.java:255)
        at sun.nio.ch.SharedFileLockTable.add(FileLockTable.java:152)
        at sun.nio.ch.FileChannelImpl.tryLock(FileChannelImpl.java:1057)
        at java.nio.channels.FileChannel.tryLock(FileChannel.java:1154)
        at org.apache.lucene.store.NativeFSLock.obtain(NativeFSLockFactory.java:217)
        at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:80)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:150)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166)
        at fr.pilato.spring.elasticsearch.ElasticsearchNodeFactoryBean.initialize(ElasticsearchNodeFactoryBean.java:122)
</comment><comment author="fvalmeida" created="2015-08-04T16:21:06Z" id="127663882">Hello, folks!

I guess figure out what might be happening.

The OverlappingFileLockException is an exception thrown by FileChannel.tryLock() method.

I'm running the ES from a SD too. (Mac OS X) 

When I read this: http://stackoverflow.com/questions/3530380/does-java-filechannel-trylock-work-on-mac-os-x?answertab=active#tab-top

I've changed the SD partition format from MS-FAT to Mac OS Extended.

Now it works like a charm.

Hope it helps.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Blank routing should be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1450</link><project id="" key="" /><description>A blank `routing` parameter should be treated in the same way as no `routing` parameter.  Currently it still has an effect:

```
# [Thu Nov 10 12:48:16 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1' 

# [Thu Nov 10 12:48:16 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Thu Nov 10 12:48:18 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XPUT 'http://127.0.0.1:9200/test/bar/1?pretty=1'  -d '
{
   "foo" : 1
}
'

# [Thu Nov 10 12:48:18 2011] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Thu Nov 10 12:48:21 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XPUT 'http://127.0.0.1:9200/test/bar/1?pretty=1&amp;routing='  -d '
{
   "foo" : 1
}
'

# [Thu Nov 10 12:48:21 2011] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Thu Nov 10 12:48:31 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XPOST 'http://127.0.0.1:9200/test/_refresh?pretty=1' 

# [Thu Nov 10 12:48:31 2011] Response:
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 10
#    }
# }

# [Thu Nov 10 12:48:35 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1' 

# [Thu Nov 10 12:48:35 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : 1
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "1",
#             "_type" : "bar"
#          },
#          {
#             "_source" : {
#                "foo" : 1
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "1",
#             "_type" : "bar"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```
</description><key id="2197123">1450</key><summary>Blank routing should be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>bug</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-10T11:50:23Z</created><updated>2011-11-13T10:04:19Z</updated><resolved>2011-11-13T10:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/delete/DeleteRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file></files><comments><comment>Blank routing should be ignored, closes #1450.</comment></comments></commit></commits></item><item><title>AbstractCompoundWordTokenFilterFactory should store the dictionary as a CharArraySet instance, not a Set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1449</link><project id="" key="" /><description>AbstractCompoundWordTokenFilterFactory stores the word list as Set&lt;String&gt;, but in Lucene, CompoundWordTokenFilterBase checks whether the dictionary is a CharArraySet, and if it's not, creates a new CharArraySet instance and populates it with the words from the input collection. This causes each TokenFilter to store a duplicate version of the dictionary in-memory.

The net effect is that ES ends up using a lot more memory than needed when using decompounding.

For a reproducible example, see https://gist.github.com/93e84d4fc412ae16d899, which fails if ES_MAX_MEM=1g, but works when ES_MAX_MEM=2g (on linux).
</description><key id="2197094">1449</key><summary>AbstractCompoundWordTokenFilterFactory should store the dictionary as a CharArraySet instance, not a Set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels><label>enhancement</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-10T11:45:24Z</created><updated>2011-11-10T18:12:49Z</updated><resolved>2011-11-10T18:12:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArabicAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArmenianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BasqueAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BrazilianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BulgarianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CatalanAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CjkAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CzechAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DanishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DutchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ElisionTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/EnglishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FinnishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FrenchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GalicianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GermanAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GreekAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/HindiAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/HungarianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/IndonesianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ItalianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/KeywordMarkerTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NorwegianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PatternAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PersianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PortugueseAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RomanianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RussianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SpanishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StandardAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StopAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StopTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SwedishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/TurkishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/AbstractCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>AbstractCompoundWordTokenFilterFactory should store the dictionary as a CharArraySet instance, not a Set, closes #1449.</comment><comment>Why are the asserts in AnalysisModuleTests commented out?</comment></comments></commit></commits></item><item><title>Log warning when application of an updated cluster state fails with an ex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1448</link><project id="" key="" /><description>When cluster state application fails on a non-master node, it might be very difficult to identify the reason for the failure. @TwP had a problem earlier today with an index that was missing a stopword file on a non-master node. The initial creation of the index on the master was successful but then missing stopword file was causing failure of state application on the non-master node. Because nothing was logged into the log file, it looked like some bizarre network failure that was preventing state from being published. Luckily, he was able to identify the root cause of the issue, which helped me to find missing logging for exceptions thrown in the submitStateUpdateTask method of InternalClusterService. I think that adding this logging would be quite helpful for troubleshooting of cluster state propagation issues. 
</description><key id="2194599">1448</key><summary>Log warning when application of an updated cluster state fails with an ex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-10T04:23:24Z</created><updated>2014-07-16T21:55:57Z</updated><resolved>2011-11-10T08:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-10T08:15:57Z" id="2692600">Make sense!, usually, better to have specific logging in the relevant listener, but we should have at least a catch all case with proper logging...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix reporting of total indexing stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1447</link><project id="" key="" /><description>It looks like there is a typo in the toXContent method that causes /_stats REST action to return primary stats instead of total stats.

On a relevant note, I have upgraded my notebook and now SearchStatsTests are failing almost every time. Fetches are occurring too fast and as a result fetchTimeInMillis() is almost always 0. Sometimes I am even getting 0 in queryTimeInMillis().
</description><key id="2186440">1447</key><summary>Fix reporting of total indexing stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-09T14:09:03Z</created><updated>2014-07-16T21:55:57Z</updated><resolved>2011-11-10T08:09:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-10T08:09:26Z" id="2692564">Applied the fix, thanks!. On SearchStatsTest, maybe we should increase the number of ops by a factor of 10? Does that work for you?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Improve applying guessed types on dynamic templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1446</link><project id="" key="" /><description>In 0.18, we changed the behavior to first check if there is a matched template before checking if the type is possibly a date (cause it matches a date pattern). We can improve this by not doing type based check for the first go, then check if it matches a date, and then do (string) type based check.
</description><key id="2183491">1446</key><summary>Mapping: Improve applying guessed types on dynamic templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-09T06:15:22Z</created><updated>2011-11-09T07:02:40Z</updated><resolved>2011-11-09T07:02:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file></files><comments><comment>Mapping: Improve applying guessed types on dynamic templates, closes #1446.</comment></comments></commit></commits></item><item><title>Provide documentation on index action settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1445</link><project id="" key="" /><description>Documentation seems to be missing on some index action settings. This request is based on a group discussion where the question was how to configure ES to _not_ automatically create indexes (upon indexing content). The answer was to use setting

action.auto_create_index: 0

which was nowhere to be found in the API documentation. There may be more settings of interest (with the 'action' prefix).
</description><key id="2172072">1445</key><summary>Provide documentation on index action settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfiedler</reporter><labels /><created>2011-11-08T08:38:56Z</created><updated>2013-04-05T16:07:21Z</updated><resolved>2013-04-05T16:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nkvoll" created="2011-11-08T20:42:59Z" id="2673268">This issue should probably be filed over at https://github.com/elasticsearch/elasticsearch.github.com :)
</comment><comment author="ppearcy" created="2012-06-13T18:59:02Z" id="6310159">This is documented:
http://www.elasticsearch.org/guide/reference/api/admin-indices-create-index.html

Can probably be closed. 
</comment><comment author="ppearcy" created="2012-11-19T22:08:54Z" id="10533409">Was just looking at this again and it is here: http://www.elasticsearch.org/guide/reference/api/index_.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi-term synonyms not highlighting correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1444</link><project id="" key="" /><description>It appears that when using a synonym filter and there are multi-term synonyms, highlighting doesn't always work correctly.

Details how to reproduce are here:
https://gist.github.com/1317149
</description><key id="2166150">1444</key><summary>Multi-term synonyms not highlighting correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2011-11-07T19:11:56Z</created><updated>2012-01-19T07:53:02Z</updated><resolved>2012-01-19T07:53:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2011-11-09T00:01:49Z" id="2675700">Hitting a different case, where instead of wrong highlighting the search errors out:
https://gist.github.com/1349777

I am going to see if I can dig into this. 
</comment><comment author="ppearcy" created="2011-11-10T18:02:24Z" id="2698684">Interestingly, for the last gist I posted, if the stemmer is removed, the error doesn't occur.
</comment><comment author="ppearcy" created="2011-11-10T19:33:20Z" id="2699948">Digging into the search failure I'm getting, the first thing that I see is that the TermOffset info looks incorrect for the document I've indexed. For the "vaccines" term, it lists the following offsets:
start = 25
end = 33

start = 0
end = 0

The 0/0 is causing the error. Trying to figure out why this is getting set this way. 
</comment><comment author="ppearcy" created="2012-01-17T17:19:46Z" id="3531618">Looks like this is addressed:
https://issues.apache.org/jira/browse/LUCENE-3668
</comment><comment author="ppearcy" created="2012-01-19T07:53:02Z" id="3559211">Closing in lieu of:
https://github.com/elasticsearch/elasticsearch/issues/1401
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster configuration EC2 not functioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1443</link><project id="" key="" /><description>After a week of requests on the IRC channel, attempts to figure it out with help in the community forums, I've had no joy getting a cluster to work on aws ec2 instances. I'm not sure if there are people using these instances or simply not around to see the rquests.

I can only assume since I've done everything as per the docs, this must not be working correctly, or there is an issue with the documentation missing some items...in either case logging this issue.

See reference to my config and the trace from startup as per this message thread on the group mailing list:

https://groups.google.com/group/elasticsearch/browse_thread/thread/14f1f3aeb0edd6a7

Any additional info I can provide let me know cheers,
</description><key id="2159205">1443</key><summary>Cluster configuration EC2 not functioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mehtryx</reporter><labels /><created>2011-11-07T02:31:37Z</created><updated>2012-01-09T05:14:28Z</updated><resolved>2012-01-09T05:14:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kalemeow" created="2011-11-08T23:34:41Z" id="2675413">I have experienced the same.  I had 2 EC2 instances running elasticsearch 17.6; after upgrading to 18.2, neither could speak to each other, receiving only EOFExceptions. Downgrading to 17.9 allowed the two nodes to speak to each other again so my cluster would work.  On 18.2, the two nodes could see each other but could not communicate.  You can see this behavior in these log snippets: https://gist.github.com/1349649
</comment><comment author="karussell" created="2011-11-26T15:13:20Z" id="2882014">@kalemeow: see the google groups thread. mehtryx problem is now solved. Regarding your problem: are all instances of the same version? Also I needed to set the region explicitely ...
</comment><comment author="karmi" created="2012-01-08T08:30:08Z" id="3400993">@mehtryx Since the issue appears to be solved on ML (wrong YML config), could you close this issue?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>boost field in multi_field mapping ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1442</link><project id="" key="" /><description>A global boost on a `multi_field` mapping doesn't apply to descendent fields. Maybe this shouldn't be allowed, but it's not throwing an error when this mapping is created.

```
    "title" : {
      "type": "multi_field",
      "boost": 2,
      "fields": {
        "title": {
          "type": "string",
          "analyzer": "standard"
        },
        "title_2": {
          "type": "string",
          "analyzer": "snowball"
        }
      }
    },
```
</description><key id="2150583">1442</key><summary>boost field in multi_field mapping ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmwilson</reporter><labels /><created>2011-11-05T03:41:10Z</created><updated>2013-04-05T16:07:37Z</updated><resolved>2013-04-05T16:07:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-13T13:38:55Z" id="2722309">Yea, you have to specify the boot on each inner field mapping... . It won't throw an error (since the mapping does not fail on unknown elements).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>deleting CouchDB river can leave some docs behind</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1441</link><project id="" key="" /><description>I'm running into a problem trying to atomically reindex a couchdb river. It seems that deleting the couchdb river can occasionally leave behind the `_seq` document. I believe this is happening because the river shutdown process isn't waiting for the couchdb's river indexer thread to exit before deleting all the couchdb river docs.

Here's the code to close the river:

```
@Override public void close() {
    if (closed) {
        return;
    }
    logger.info("closing couchdb stream river");
    slurperThread.interrupt();
    indexerThread.interrupt();
    closed = true;
}
```

After this function exits, ES goes off to delete all the river documents. However, because the plugin isn't joining on the subthreads, it's possible for the indexer thread to miss the interrupt until after it's updated the `_seq` doc;

```
           ....
           } catch (InterruptedException e) {
                if (closed) {
                    return;
                }
            }

            // thread halts here.

            if (lastSeq != null) {
                try {
                    if (logger.isTraceEnabled()) {
                        logger.trace("processing [_seq  ]: [{}]/[{}]/[{}], last_seq [{}]", riverIndexName, riverName.name(), "_seq", lastSeq);re
                    }
                    bulk.add(indexRequest(riverIndexName).type(riverName.name()).id("_seq")
                            .source(jsonBuilder().startObject().startObject("couchdb").field("last_seq", lastSeq).endObject().endObject()));
                } catch (IOException e) {
                    logger.warn("failed to add last_seq entry to bulk indexing");
                }
            }
            ...
```

If everything happens just right, ES will delete the plugin subdocuments, and this thread will recreate the `_seq` file.

It looks like the other river plugins use threads, but I haven't checked if they're also susceptible to this bug.
</description><key id="2149228">1441</key><summary>deleting CouchDB river can leave some docs behind</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">erickt</reporter><labels /><created>2011-11-04T22:32:18Z</created><updated>2013-10-07T09:24:26Z</updated><resolved>2013-10-07T09:24:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-07T09:24:26Z" id="25794841">Closed in favor of: https://github.com/elasticsearch/elasticsearch-river-couchdb/issues/39
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix possible NPE in TransportNodesListGatewayStartedShards operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1440</link><project id="" key="" /><description>When recovering cluster contains data nodes with no shards on them, the TransportNodesListGatewayStartedShards fails with the following exception:

```
[2011-11-04 10:43:28,611][DEBUG][gateway.local ] [Hulk 2099] failed to execute on node [gxylYRQ-Tp-EFlRRXgOgHQ] 
org.elasticsearch.transport.RemoteTransportException: [Sandman][inet[/10.1.10.148:9301]][/gateway/local/started-shards/node] 
Caused by: java.lang.NullPointerException 
at org.elasticsearch.gateway.local.TransportNodesListGatewayStartedShards.nodeOperation(TransportNodesListGatewayStartedShards.java:114) 
at org.elasticsearch.gateway.local.TransportNodesListGatewayStartedShards.nodeOperation(TransportNodesListGatewayStartedShards.java:49) 
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267) 
at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:260) 
at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:238) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:680) 
```

I am not sure if it affects recovery.

To reproduce:
- delete data directory 
- start one node with default settings
- create one index with default settings
- shut down the node
- add `gateway.recover_after_nodes: 3` to elasticsearch.yml 
- add `gateway: DEBUG` to logging.yml 
- start 3 nodes 
- observe error message above in the log file
</description><key id="2148374">1440</key><summary>Fix possible NPE in TransportNodesListGatewayStartedShards operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-04T21:00:28Z</created><updated>2014-06-13T07:33:12Z</updated><resolved>2011-11-10T08:05:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-10T08:05:00Z" id="2692537">It will not matter for recovery, but it should not happen :), thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>highlighting doesn't work with ip type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1439</link><project id="" key="" /><description>here is my mapping;

{
    "dst": {
        "type": "ip"
    },
    "srccountry": {
        "type": "string",
        "index": "not_analyzed"
    }
}

and my query is : 

{
  "size": 1,
  "query": {
    "query_string": {
      "query": "dst:192.168.1.100 srccountry:Indonesia",
      "lowercase_expanded_terms": false,
      "default_operator": "AND"
    }
  },
  "highlight": {
    "pre_tags": [
      "&lt; tag1 &gt;",
      "&lt; tag2 &gt;"
    ],
    "post_tags": [
      "&lt; /tag1 &gt;",
      "&lt; /tag2 &gt;"
    ],
    "fields": {
      "dst": {},
      "srccountry": {}
    }
  }
}
and highlight output is : 

highlight: {
    dst: [
        &lt; tag1 &gt;&lt; /tag1 &gt;192.168.1.100      &lt;==== PROBLEM
    ]
    srccountry: [
        &lt; tag1 &gt;Indonesia&lt; /tag1 &gt;
    ]
}
I think there is something that I missed with 'ip' type
</description><key id="2122603">1439</key><summary>highlighting doesn't work with ip type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fatagnus</reporter><labels /><created>2011-11-02T15:28:47Z</created><updated>2013-04-05T16:08:25Z</updated><resolved>2013-04-05T16:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T16:08:25Z" id="15964980">Yeah ,the IP type is a number, not a string, so highlighting won't work. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>highlighting doesn't work with ip type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1438</link><project id="" key="" /><description>here is my mapping;

{
    "dst": {
        "type": "ip"
    },
    "srccountry": {
        "type": "string",
        "index": "not_analyzed"
    }
}

and my query is : 

{
  "size": 1,
  "query": {
    "query_string": {
      "query": "dst:192.168.1.100 srccountry:Indonesia",
      "lowercase_expanded_terms": false,
      "default_operator": "AND"
    }
  },
  "highlight": {
    "pre_tags": [
      "&lt;tag1&gt;",
      "&lt;tag2&gt;"
    ],
    "post_tags": [
      "&lt;/tag1&gt;",
      "&lt;/tag2&gt;"
    ],
    "fields": {
      "dst": {},
      "srccountry": {}
    }
  }
}
and highlight output is : 

highlight: {
    dst: [
        &lt; tag1 &gt;&lt; /tag1 &gt;192.168.1.100      &lt;==== PROBLEM
    ]
    srccountry: [
        &lt; tag1 &gt;Indonesia&lt; /tag1 &gt;
    ]
}
I think there is something that I missed with 'ip' type
</description><key id="2122592">1438</key><summary>highlighting doesn't work with ip type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fatagnus</reporter><labels /><created>2011-11-02T15:27:22Z</created><updated>2011-11-02T15:28:19Z</updated><resolved>2011-11-02T15:28:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>highlighting doesn't work with ip type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1437</link><project id="" key="" /><description>here is my mapping;

{
    "dst": {
        "type": "ip"
    },
    "srccountry": {
        "type": "string",
        "index": "not_analyzed"
    }
}

and my query is : 

{
  "size": 1,
  "query": {
    "query_string": {
      "query": "srccountry: Indonesia dst:192.168.1.100",
      "lowercase_expanded_terms": false,
      "default_operator": "AND"
    }
  },
  "highlight": {
    "fields": {
      "srccountry": {},
      "dst": {}
    }
  }
}

and highlight output is : 

highlight: {
    dst: [
        &lt;em&gt;&lt;/em&gt;192.168.1.100                            &lt; ==== PROBLEM
    ]
    srccountry: [
        &lt;em&gt;Indonesia&lt;/em&gt;
    ]
}

I think there is something that I missed with 'ip' type
</description><key id="2122540">1437</key><summary>highlighting doesn't work with ip type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fatagnus</reporter><labels /><created>2011-11-02T15:22:35Z</created><updated>2011-11-02T15:22:52Z</updated><resolved>2011-11-02T15:22:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>data.path locations are unevenly filled (0.18.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1436</link><project id="" key="" /><description>here's my path configuration

```
path:
    data: /ebs/es1/data,/ebs/es2/data,/ebs/es3/data,/ebs/es4/data,/ebs/es5/data,/ebs/es6/data,/ebs/es7/data,/ebs/es8/data
```

unfortunately the disks are not being evenly filled:

```
es16a.: /dev/xvdl3             25G  173M   24G   1% /ebs/es3
es16a.: /dev/xvdl5             25G  173M   24G   1% /ebs/es5
es16a.: /dev/xvdl6             25G  173M   24G   1% /ebs/es6
es16a.: /dev/xvdl2             25G  173M   24G   1% /ebs/es2
es16a.: /dev/xvdl8             25G  173M   24G   1% /ebs/es8
es16a.: /dev/xvdl1             25G  508M   23G   3% /ebs/es1
es16a.: /dev/xvdl7             25G  173M   24G   1% /ebs/es7
es16a.: /dev/xvdl4             25G  173M   24G   1% /ebs/es4
```

but directories are beeing created on all drives, but the .nrm, .tis, .frq... files are only on the first disk.

https://gist.github.com/715b9dfe48d0c5fec197
</description><key id="2115650">1436</key><summary>data.path locations are unevenly filled (0.18.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jodok</reporter><labels><label>bug</label><label>v0.18.3</label><label>v0.19.0.RC1</label></labels><created>2011-11-01T21:54:44Z</created><updated>2011-11-02T21:44:17Z</updated><resolved>2011-11-02T21:44:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/Store.java</file></files><comments><comment>data.path locations are unevenly filled (0.18.2), closes #1436.</comment></comments></commit></commits></item><item><title>IllegalStateException when setting shard allocation awareness (0.18.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1435</link><project id="" key="" /><description>i'm setting:

```
node.zone: eu-west-1b
cluster.routing.allocation.awareness.attributes: zone
cluster.routing.allocation.awareness.force.zone: eu-west-1a,eu-west-1b,eu-west-1c
```

but after starting ES i get the following error:

&gt; {0.18.2}: Initialization Failed ...
&gt; 1) IllegalStateException[This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.]2) SettingsException[Failed to get setting group for [cluster.routing.allocation.awareness.force.] setting prefix and setting [cluster.routing.allocation.awareness.force.zone] because of a missing '.']

i used the same setting on 0.17.x and it worked without problems. probably something's different with 0.18.2?
</description><key id="2114541">1435</key><summary>IllegalStateException when setting shard allocation awareness (0.18.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jodok</reporter><labels /><created>2011-11-01T20:15:03Z</created><updated>2011-11-02T22:22:45Z</updated><resolved>2011-11-02T22:22:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-01T21:29:15Z" id="2596267">Grr, annoying failure message, but basically the "force" setting is wrong (and its wrong in the docs, I fixed it). It should be:

```
    cluster.routing.allocation.awareness.force.zone.values: eu-west-1a,eu-west-1b,eu-west-1c
```

btw, the reason why it works in 0.17 is because allocation awareness is new in 0.18...
</comment><comment author="kimchy" created="2011-11-02T22:22:45Z" id="2610679">Closing this, if it still does not work, ping...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aliases should be processed when index routing changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1434</link><project id="" key="" /><description>Found a bug in filtering aliases code. During initial recovery or shard relocations, IndicesClusterStateService doesn't process filtering aliases during cluster state update if metadata didn't change, even when an index with such aliases is allocated on the node. It manifests itself through InvalidAliasNameException when an unprocessed filtering alias is used in search queries.
</description><key id="2104342">1434</key><summary>Aliases should be processed when index routing changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-11-01T02:25:20Z</created><updated>2014-07-16T21:55:58Z</updated><resolved>2011-11-01T18:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-01T18:07:21Z" id="2593691">Cool, thanks. Pushed to 0.18 branch and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add the hostname to the node info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1433</link><project id="" key="" /><description>Node names are cool, inet address are useful too, but hostnames often get more interesting, at least in my case.
</description><key id="2097126">1433</key><summary>Add the hostname to the node info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-10-31T13:56:02Z</created><updated>2014-07-16T21:55:59Z</updated><resolved>2011-12-13T17:28:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nlalevee" created="2011-12-01T15:48:40Z" id="2976209">ping.
Any objection to accept this pull request ?
</comment><comment author="kimchy" created="2011-12-12T22:48:00Z" id="3114416">It does not apply cleanly to master anymore, can you fix it? Also, use NetworkUtils#getLocalAddress instead of going through the lookup of it each time.
</comment><comment author="nlalevee" created="2011-12-13T08:43:52Z" id="3120435">patch rebased and simplified
</comment><comment author="kimchy" created="2011-12-13T15:14:50Z" id="3124125">I think that initializing the hostname outside of node info in the actual action was better, can you do that one?
</comment><comment author="nlalevee" created="2011-12-13T15:33:26Z" id="3124476">patch updated accordingly
</comment><comment author="kimchy" created="2011-12-13T17:28:39Z" id="3126228">Pushed!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>plugin script fails to follow github redirect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1432</link><project id="" key="" /><description>The plugin installation script fails if GitHub decides not to serve out zip files directly but through a HTTP 302 re-direct.

````$ ./bin/plugin -install mobz/elasticsearch-head
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/downloads/mobz/elasticsearch-head/elasticsearch-head-0.18.1.zip...
Trying https://github.com/mobz/elasticsearch-head/zipball/v0.18.1...
Trying https://github.com/mobz/elasticsearch-head/zipball/master...
Failed to install mobz/elasticsearch-head, reason: failed to download

``````
The first two attempts would fail (because those files don't exist) but the master one should succeed. Trying with curl reveals that GitHub is in the mood to redirect requests to another server:

````$ curl -i 'https://github.com/mobz/elasticsearch-head/zipball/master'
HTTP/1.1 302 Found
Server: nginx/1.0.4
Date: Fri, 28 Oct 2011 10:40:08 GMT
Content-Type: text/html; charset=utf-8
Connection: keep-alive
Status: 302 Found
X-Frame-Options: deny
Location: https://nodeload.github.com/mobz/elasticsearch-head/zipball/master
X-Runtime: 8ms
Content-Length: 132
Set-Cookie: _gh_sess=BAh7BiIKZmxhc2hJQzonQWN0aW9uQ29udHJvbGxlcjo6Rmxhc2g6OkZsYXNoSGFzaHsABjoKQHVzZWR7AA%3D%3D--ed7eadd474fd37850b68bd3c08a5c55ad0c3c833; path=/; expires=Fri, 01 Jan 2021 00:00:00 GMT; secure; HttpOnly
Cache-Control: no-cache
Strict-Transport-Security: max-age=2592000
``````

Perhaps the plugin installation script should be follow the redirect.
</description><key id="2083776">1432</key><summary>plugin script fails to follow github redirect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">glynnbird</reporter><labels /><created>2011-10-28T21:05:25Z</created><updated>2014-06-24T13:29:08Z</updated><resolved>2013-04-05T16:08:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-15T11:07:19Z" id="2743460">Strange..., there is code that follows redirects, and installing it works well for me. Do you still have the problem?
</comment><comment author="glynnbird" created="2011-11-17T09:56:34Z" id="2774154">I tried again from a different network and it "just worked". If it can't be reproduced, then feel free to close the issue
</comment><comment author="rore" created="2011-12-19T08:54:41Z" id="3200700">Same thing just happened to me with bigdesk.
Trying to install from an EC2 instance.
</comment><comment author="merrellb" created="2011-12-21T06:29:21Z" id="3230229">I'm having the same problem on 0.18.5 running on a standalone Ubuntu 11.10 box.
</comment><comment author="kimchy" created="2011-12-21T08:29:51Z" id="3231030">Well, first, the download does follows redirects. We don't have proper for the actual failure, I will try and add it under a trace option or something similar.
</comment><comment author="merrellb" created="2011-12-21T19:36:13Z" id="3238413">My issue seems to have been permissions based.  Running sudo plugins -install seems to have resolved the problem.
</comment><comment author="rore" created="2011-12-21T19:39:16Z" id="3238451">Damn. Or should I say, Duh! How did I miss that.
</comment><comment author="josegonzalez" created="2012-02-28T20:46:42Z" id="4225080">Had this same exact issue just now, `sudo` seems to fix it

```
jose@elastic-ec2-01:/usr/share$ elasticsearch/bin/plugin -install mobz/elasticsearch-head
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/downloads/mobz/elasticsearch-head/elasticsearch-head-0.19.0.RC3.zip...
Trying https://github.com/mobz/elasticsearch-head/zipball/v0.19.0.RC3...
Trying https://github.com/mobz/elasticsearch-head/zipball/master...
Failed to install mobz/elasticsearch-head, reason: failed to download
jose@elastic-ec2-01:/usr/share$ sudo !!
sudo elasticsearch/bin/plugin -install mobz/elasticsearch-head
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/downloads/mobz/elasticsearch-head/elasticsearch-head-0.19.0.RC3.zip...
Trying https://github.com/mobz/elasticsearch-head/zipball/v0.19.0.RC3...
Trying https://github.com/mobz/elasticsearch-head/zipball/master...
Downloading ....................................DONE
Identified as a _site plugin, moving to _site structure ...
Installed head
jose@elastic-ec2-01
```
</comment><comment author="ghost" created="2012-03-07T15:38:10Z" id="4370218">I have the same problem and sudo doesn't help.  Does plugin use the system proxy settings (ie http_proxy)?

```
sudo bin/plugin -install elasticsearch/elasticsearch-river-couchdb
-&gt; Installing elasticsearch/elasticsearch-river-couchdb...
Trying https://github.com/downloads/elasticsearch/elasticsearch-river-couchdb/elasticsearch-river-couchdb-0.19.0.zip...
Trying https://github.com/elasticsearch/elasticsearch-river-couchdb/zipball/v0.19.0...
Trying https://github.com/elasticsearch/elasticsearch-river-couchdb/zipball/master...
Failed to install elasticsearch/elasticsearch-river-couchdb, reason: failed to download
```
</comment><comment author="dawei213" created="2012-05-15T19:49:05Z" id="5725448">The issue I was running into was our system proxy. Setting a system proxy didn't help so I edited the plugin script. I added the following to the script (in bold):

**PROXY_HOST="YOUR_PROXY_HOST"
PROXY_PORT=YOUR_PROXY_PORT"**

exec $JAVA **-Dhttps.proxyHost=$PROXY_HOST -Dhttps.proxyPort=$PROXY_PORT** -Xmx64m -Xms16m -Delasticsearch -Des.path.home="$ES_HOME" -cp "$ES_HOME/lib/_" org.elasticsearch.plugins.PluginManager $_
</comment><comment author="autoric" created="2012-06-06T03:12:52Z" id="6142152">dawei213 - thanks for this post, wasn't sure what to do. 

It's also worth noting that plugins on github and possibly others will actually redirect to a non-secure connection. So you need to take this a step further to make sure you can connect.

```
PROXY_HOST=proxy.mydomain.org
PROXY_PORT=80
S_PROXY_HOST=proxy.mydomain.org
S_PROXY_PORT=80

exec $JAVA -Dhttp.proxyHost=$PROXY_HOST -Dhttp.proxyPort=$PROXY_PORT -Dhttps.proxyHost=$S_PROXY_HOST -Dhttps.proxyPort=$S_PROXY_PORT -Xmx64m -Xms16m -Delasticsearch -Des.path.home=$
```
</comment><comment author="wires" created="2012-08-09T11:15:39Z" id="7611425">I confirm that this is a proxy issue; we had environment variables `http_proxy=http://host:port` and `https_proxy` setup, but there are not used.

Both HTTP and HTTPS seem required: the github `https://.../master` URL redirects to another https URL yet without a http proxy I still cannot download plugins?
</comment><comment author="arnelceledonio" created="2013-01-19T01:40:05Z" id="12448613">sudo elasticsearch/bin/plugin -install mobz/elasticsearch-head
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/mobz/elasticsearch-head/zipball/master... (assuming site plugin)
Failed to install mobz/elasticsearch-head, reason: failed to download out of all possible locations..., use -verbose to get detailed information
</comment><comment author="yatskevich" created="2013-03-21T15:29:21Z" id="15245330">Trying to run `bin/plugin -install` on EC2 VPC instance with Elastic IP:

```
sudo bin/plugin -install elasticsearch/elasticsearch-cloud-aws/1.11.0
-&gt; Installing elasticsearch/elasticsearch-cloud-aws/1.11.0...
Trying https://github.com/downloads/elasticsearch/elasticsearch-cloud-aws/elasticsearch-cloud-aws-1.11.0.zip...
Trying https://github.com/elasticsearch/elasticsearch-cloud-aws/zipball/v1.11.0...
Failed to install elasticsearch/elasticsearch-cloud-aws/1.11.0, reason: failed to download
```
</comment><comment author="dadoonet" created="2013-03-21T15:31:43Z" id="15245500">@yatskevich What is your ES version?
</comment><comment author="yatskevich" created="2013-03-21T15:34:44Z" id="15245701">@dadoonet, 

```
$ bin/elasticsearch -v
ElasticSearch Version: 0.19.9, JVM: 20.12-b01
```
</comment><comment author="dadoonet" created="2013-03-21T15:42:30Z" id="15246207">@yatskevich Update to 0.20.6 (see note: http://www.elasticsearch.org/blog/2012/12/27/0.20.2-released.html) or use:

``` sh
$ bin/plugin -url https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/elasticsearch-cloud-aws/1.11.0/elasticsearch-cloud-aws-1.11.0.zip -install elasticsearch-cloud-aws
```

Please use the mailing list next time.
</comment><comment author="yatskevich" created="2013-03-21T15:48:54Z" id="15246630">Thank you, @dadoonet !
We'll definitely consider an update to the latest and greatest. For now your proposed solution worked like a charm!
</comment><comment author="ibagui" created="2014-03-13T19:22:57Z" id="37575748">Hi,
I had the similar kind of problem and was getting the below error while installing the plugin.
"Failed to install mobz/elasticsearch-head, reason: failed to rename in order to copy to _site (rename to E:\elasticsearch\elasticsearch-1.0.1\plugins\head.tmp"

I stopped the elasticsearch and specified a node.name for my data node and started again. Problem solved.
</comment><comment author="SabareeshSS" created="2014-06-13T05:09:54Z" id="45976406">Hi,
Did any body found any workaround for this error?
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/mobz/elasticsearch-head/archive/master.zip...
Failed to install mobz/elasticsearch-head, reason: failed to download out of all
 possible locations..., use -verbose to get detailed information

Please help me, without this head plugin I am struggling a lot.
Thanks!
</comment><comment author="dadoonet" created="2014-06-13T05:24:14Z" id="45976966">Download it manually and unzip it in ./plugins/head/_site
</comment><comment author="clintongormley" created="2014-06-13T11:04:42Z" id="45999271">@dadoonet But we should follow redirects, no?  At least up to eg 5 redirects.
</comment><comment author="dadoonet" created="2014-06-13T11:07:34Z" id="45999490">@clintongormley Sure. My guess here is that for whatever reason he can not access this resource from his computer (download service down, proxy...). 
Do you mean that you can reproduce this issue?
</comment><comment author="clintongormley" created="2014-06-13T11:10:55Z" id="45999711">@dadoonet sorry, i just saw the original issue title and your response in my email, so I misunderstood.  Ignore me.
</comment><comment author="ymzong" created="2014-06-17T21:15:27Z" id="46366332">@dadoonet Hello, I am using ElasticSearch 1.2.1, and when I tried `plugin -install mobz/elasticsearch-head` today, the error as follows occurs:

```
Trying https://github.com/mobz/elasticsearch-head/archive/master.zip...
Failed: UnknownHostException[github.com]
Failed to install mobz/elasticsearch-head, reason: failed to download out of all
 possible locations..., use -verbose to get detailed information
```

Nonetheless, when I tried the link by myself on browser, it works.
</comment><comment author="dadoonet" created="2014-06-24T13:29:08Z" id="46970297">@ymzong You probably having a network issue. If your system is not able to find the domain github.com, this is not something we can solve on our side.
May be you are running behind a proxy or so?

I guess that if you try `curl http://github.com` from the command line, it won't work either, right?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: Fetching fields that end up extracted from source might fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1431</link><project id="" key="" /><description /><key id="2070971">1431</key><summary>Search: Fetching fields that end up extracted from source might fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.2</label><label>v0.19.0.RC1</label></labels><created>2011-10-27T18:18:15Z</created><updated>2011-10-27T18:28:48Z</updated><resolved>2011-10-27T18:28:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file></files><comments><comment>Search: Fetching fields that end up extracted from source might fail, closes #1431.</comment></comments></commit></commits></item><item><title>Merge Scheduler: Allow to set index.merge.scheduler.max_merge_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1430</link><project id="" key="" /><description>Defaults to the `max_thread_count` setting + 2.
</description><key id="2063252">1430</key><summary>Merge Scheduler: Allow to set index.merge.scheduler.max_merge_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.2</label><label>v0.19.0.RC1</label></labels><created>2011-10-27T00:13:46Z</created><updated>2015-10-21T21:48:23Z</updated><resolved>2011-10-27T00:14:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="l15k4" created="2015-10-21T21:48:23Z" id="150034754">Hey, is it possible for users to set `index.merge.scheduler.max_merge_count` ? I actually have this use case that I'm bulk indexing in cycles with 30 minutes long breaks and sometimes segment merging is falling behind and it starts to perform 10 times worse. The point is that I have 30 minutes time for performing merging after the indexing... disabling throttling and refresh interval doesn't help
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/scheduler/ConcurrentMergeSchedulerProvider.java</file></files><comments><comment>Merge Scheduler: Allow to set index.merge.scheduler.max_merge_count, closes #1430.</comment></comments></commit></commits></item><item><title>External data location configuration fails (path.data setting)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1429</link><project id="" key="" /><description /><key id="2062905">1429</key><summary>External data location configuration fails (path.data setting)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.1</label><label>v0.19.0.RC1</label></labels><created>2011-10-26T23:30:05Z</created><updated>2011-10-26T23:30:42Z</updated><resolved>2011-10-26T23:30:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalSettingsPerparer.java</file></files><comments><comment>External data location configuration fails (path.data setting), closes #1429.</comment></comments></commit></commits></item><item><title>TransportClient hangs when Elasticsearch Engine is running out of opened file descriptors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1428</link><project id="" key="" /><description>Description:

WhenElasticsearch Engine is running out of opened file descriptors, the TransportClient hangs indefinitely.

Steps to reproduce:
- set a lower value fie max opened files descriptors
- start elasticsearch engine
- connect to the elasticsearch engine using the java TransportClient (using port 9300)
- elasticsearch engine should raise IOException that it's running out of open file descriptors

Actual:
- transportclient hangs

Expected:
- the client shouldn't hang, but to throw some kind of exception.

Effect:
- if the client is initialized at application runtime, it will hang the entire application boot process.

Elasticsearch engine version: 0.17.8
Elasticsearch client version: 0.17.8
Java: 1.6.0_26
Scala (not sure how relevant is this for this specific situation though): 2.9.1.final
OS: 10.7.2

This is the thread dump taken from the moment when the problem appear:

2011-10-24 22:26:15
Full thread dump Java HotSpot(TM) 64-Bit Server VM (20.1-b02-383 mixed mode):

"elasticsearch[cached]-pool-1-thread-1" daemon prio=5 tid=7fe3a51e4800 nid=0x1175cc000 waiting for monitor entry [1175cb000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler.sample(TransportClientNodesService.java:253)
    - waiting to lock &lt;7f40c0048&gt; (a org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler)
    at org.elasticsearch.client.transport.TransportClientNodesService$ScheduledNodeSampler.run(TransportClientNodesService.java:243)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"New I/O client worker #1-4" daemon prio=5 tid=7fe3ab9ed800 nid=0x1174c9000 runnable [1174c8000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
    at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked &lt;7f300d450&gt; (a sun.nio.ch.Util$2)
    - locked &lt;7f300d440&gt; (a java.util.Collections$UnmodifiableSet)
    - locked &lt;7f300d238&gt; (a sun.nio.ch.KQueueSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:163)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"New I/O client worker #1-1" daemon prio=5 tid=7fe3ab9eb000 nid=0x1173c6000 runnable [1173c5000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
    at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked &lt;7f300ca00&gt; (a sun.nio.ch.Util$2)
    - locked &lt;7f300c9f0&gt; (a java.util.Collections$UnmodifiableSet)
    - locked &lt;7f300c7e8&gt; (a sun.nio.ch.KQueueSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:163)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"New I/O client worker #1-2" daemon prio=5 tid=7fe3ab9e9000 nid=0x1172c3000 runnable [1172c2000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
    at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked &lt;7f300bfb0&gt; (a sun.nio.ch.Util$2)
    - locked &lt;7f300bfa0&gt; (a java.util.Collections$UnmodifiableSet)
    - locked &lt;7f300bd98&gt; (a sun.nio.ch.KQueueSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:163)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"New I/O client worker #1-6" daemon prio=5 tid=7fe3aba52000 nid=0x1171c0000 runnable [1171bf000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
    at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked &lt;7f300b510&gt; (a sun.nio.ch.Util$2)
    - locked &lt;7f300b500&gt; (a java.util.Collections$UnmodifiableSet)
    - locked &lt;7f300b2f8&gt; (a sun.nio.ch.KQueueSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:163)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"New I/O client worker #1-3" daemon prio=5 tid=7fe3aba4f800 nid=0x1170bd000 runnable [1170bc000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
    at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked &lt;7f300aac0&gt; (a sun.nio.ch.Util$2)
    - locked &lt;7f300aab0&gt; (a java.util.Collections$UnmodifiableSet)
    - locked &lt;7f300a8a8&gt; (a sun.nio.ch.KQueueSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:163)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"New I/O client worker #1-5" daemon prio=5 tid=7fe3aba4d800 nid=0x116fba000 runnable [116fb9000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
    at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked &lt;7f300a070&gt; (a sun.nio.ch.Util$2)
    - locked &lt;7f300a060&gt; (a java.util.Collections$UnmodifiableSet)
    - locked &lt;7f3009e58&gt; (a sun.nio.ch.KQueueSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:163)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"New I/O client worker #1-7" daemon prio=5 tid=7fe3aba4b000 nid=0x116eb7000 runnable [116eb6000]
   java.lang.Thread.State: RUNNABLE
    at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
    at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
    at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    - locked &lt;7f3009620&gt; (a sun.nio.ch.Util$2)
    - locked &lt;7f3009610&gt; (a java.util.Collections$UnmodifiableSet)
    - locked &lt;7f3009408&gt; (a sun.nio.ch.KQueueSelectorImpl)
    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:163)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"elasticsearch[Black Mamba]transport_client_boss-pool-9-thread-1" daemon prio=5 tid=7fe3aba37800 nid=0x116db4000 waiting on condition [116db3000]
   java.lang.Thread.State: TIMED_WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;7f411fcd0&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
    at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:424)
    at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:323)
    at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:874)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:945)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
    at java.lang.Thread.run(Thread.java:680)

"elasticsearch[Black Mamba][scheduler]-pool-8-thread-1" daemon prio=5 tid=7fe3ac0cf000 nid=0x116c50000 waiting on condition [116c4f000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;7f40afc50&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    at java.util.concurrent.DelayQueue.take(DelayQueue.java:160)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:609)
    at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:602)
    at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
    at java.lang.Thread.run(Thread.java:680)

"elasticsearch[Black Mamba][timer]" daemon prio=5 tid=7fe3ac06e000 nid=0x116b4d000 waiting on condition [116b4c000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
    at java.lang.Thread.sleep(Native Method)
    at org.elasticsearch.threadpool.ThreadPool$EstimatedTimeThread.run(ThreadPool.java:285)

"Monitor Ctrl-Break" daemon prio=5 tid=7fe3ab8a1000 nid=0x116857000 runnable [116856000]
   java.lang.Thread.State: RUNNABLE
    at java.net.PlainSocketImpl.socketAccept(Native Method)
    at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:408)
    - locked &lt;7f40bf840&gt; (a java.net.SocksSocketImpl)
    at java.net.ServerSocket.implAccept(ServerSocket.java:462)
    at java.net.ServerSocket.accept(ServerSocket.java:430)
    at com.intellij.rt.execution.application.AppMain$1.run(AppMain.java:82)
    at java.lang.Thread.run(Thread.java:680)

"Low Memory Detector" daemon prio=5 tid=7fe3a612a800 nid=0x11627e000 runnable [00000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread1" daemon prio=9 tid=7fe3a612a000 nid=0x11617b000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"C2 CompilerThread0" daemon prio=9 tid=7fe3a6129000 nid=0x116078000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Signal Dispatcher" daemon prio=9 tid=7fe3a6128800 nid=0x115f75000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Surrogate Locker Thread (Concurrent GC)" daemon prio=5 tid=7fe3a6127800 nid=0x115e72000 waiting on condition [00000000]
   java.lang.Thread.State: RUNNABLE

"Finalizer" daemon prio=8 tid=7fe3a6123000 nid=0x115bae000 in Object.wait() [115bad000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on &lt;7f40bfc18&gt; (a java.lang.ref.ReferenceQueue$Lock)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
    - locked &lt;7f40bfc18&gt; (a java.lang.ref.ReferenceQueue$Lock)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
    at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

"Reference Handler" daemon prio=10 tid=7fe3a6122000 nid=0x115aab000 in Object.wait() [115aaa000]
   java.lang.Thread.State: WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on &lt;7f40a0100&gt; (a java.lang.ref.Reference$Lock)
    at java.lang.Object.wait(Object.java:485)
    at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
    - locked &lt;7f40a0100&gt; (a java.lang.ref.Reference$Lock)

"main" prio=5 tid=7fe3a6000800 nid=0x10dd35000 waiting on condition [10dd33000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;7f31ea4f0&gt; (a org.elasticsearch.common.util.concurrent.AbstractFuture$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
    at org.elasticsearch.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:249)
    at org.elasticsearch.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:78)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:45)
    at org.elasticsearch.transport.PlainTransportFuture.txGet(PlainTransportFuture.java:35)
    at org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler.sample(TransportClientNodesService.java:267)
    - locked &lt;7f40c0048&gt; (a org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler)
    at org.elasticsearch.client.transport.TransportClientNodesService.addTransportAddress(TransportClientNodesService.java:124)
    at org.elasticsearch.client.transport.TransportClient.addTransportAddress(TransportClient.java:200)
    at org.my.test.search.ElasticSearchClient.&lt;init&gt;(ElasticSearchClient.scala:31)
    at org.my.test.search.elasticsearch.ElasticSearchClient$.main(ElasticSearchClient.scala:107)
    at org.my.test.search.elasticsearch.ElasticSearchClient.main(ElasticSearchClient.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)

"VM Thread" prio=9 tid=7fe3a611d800 nid=0x1159a8000 runnable 

"Gang worker#0 (Parallel GC Threads)" prio=9 tid=7fe3a6002000 nid=0x11106e000 runnable 

"Gang worker#1 (Parallel GC Threads)" prio=9 tid=7fe3a6002800 nid=0x111171000 runnable 

"Gang worker#2 (Parallel GC Threads)" prio=9 tid=7fe3a6003000 nid=0x111274000 runnable 

"Gang worker#3 (Parallel GC Threads)" prio=9 tid=7fe3a6004000 nid=0x111377000 runnable 

"Gang worker#4 (Parallel GC Threads)" prio=9 tid=7fe3a6004800 nid=0x11147a000 runnable 

"Gang worker#5 (Parallel GC Threads)" prio=9 tid=7fe3a6005000 nid=0x11157d000 runnable 

"Gang worker#6 (Parallel GC Threads)" prio=9 tid=7fe3a6005800 nid=0x111680000 runnable 

"Gang worker#7 (Parallel GC Threads)" prio=9 tid=7fe3a6006800 nid=0x111783000 runnable 

"Concurrent Mark-Sweep GC Thread" prio=9 tid=7fe3a60e3800 nid=0x11564e000 runnable 
"Gang worker#0 (Parallel CMS Threads)" prio=9 tid=7fe3a60e2800 nid=0x114c48000 runnable 

"Gang worker#1 (Parallel CMS Threads)" prio=9 tid=7fe3a60e3000 nid=0x114d4b000 runnable 

"VM Periodic Task Thread" prio=10 tid=7fe3a613c800 nid=0x116381000 waiting on condition 

"Exception Catcher Thread" prio=10 tid=7fe3a6001800 nid=0x10de97000 runnable 
JNI global references: 1216

Heap
 par new generation   total 19136K, used 5188K [7f3000000, 7f44c0000, 7f44c0000)
  eden space 17024K,  22% used [7f3000000, 7f33cb928, 7f40a0000)
  from space 2112K,  61% used [7f40a0000, 7f41e5a80, 7f42b0000)
  to   space 2112K,   0% used [7f42b0000, 7f42b0000, 7f44c0000)
 concurrent mark-sweep generation total 63872K, used 0K [7f44c0000, 7f8320000, 7fae00000)
 concurrent-mark-sweep perm gen total 21248K, used 12223K [7fae00000, 7fc2c0000, 800000000)
</description><key id="2059949">1428</key><summary>TransportClient hangs when Elasticsearch Engine is running out of opened file descriptors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alinpopa</reporter><labels /><created>2011-10-26T18:37:34Z</created><updated>2013-04-05T16:09:05Z</updated><resolved>2013-04-05T16:09:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>delete by query not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1427</link><project id="" key="" /><description>When I try a delete by query request across multiple types, I get an error saying `InvalidTypeNameException[mapping type name [file,comment] should not include ',' in it` but when I issue the same query as a GET it works fine. According to the docs, the two APIs _appear_ to work exactly the same except for the HTTP method.
</description><key id="2058279">1427</key><summary>delete by query not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brianmario</reporter><labels /><created>2011-10-26T16:00:45Z</created><updated>2011-10-26T16:18:19Z</updated><resolved>2011-10-26T16:18:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brianmario" created="2011-10-26T16:01:15Z" id="2532180">Oh forgot to mention I'm on 0.17.9 in OSX
</comment><comment author="brianmario" created="2011-10-26T16:18:19Z" id="2532457">nevermind, found the issue on my end ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JMX/IndexName inconsistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1426</link><project id="" key="" /><description>Hi, I was just fooling around with ways to consistently name indexes and stumbled on the following:

ElasticSearch allows index names that JMX cannot register:

```
[2011-10-26 14:42:00,032][WARN ][jmx                      ] [M-Twins] Could not register object with name: service=indices,index=schema:1:test(Invalid character ':' in value part of property)
[2011-10-26 14:42:00,032][INFO ][cluster.metadata         ] [M-Twins] [schema:1:test] creating index, cause [api], shards [5]/[1], mappings []
```
</description><key id="2056262">1426</key><summary>JMX/IndexName inconsistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2011-10-26T12:50:16Z</created><updated>2013-04-05T16:09:41Z</updated><resolved>2013-04-05T16:09:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T16:09:41Z" id="15965047">JMX is no longer recommended with Elasticsearch. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting broken with custom_filters_score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1425</link><project id="" key="" /><description>Using a custom_filters_score query appears to break highlighting in the results.
</description><key id="2048598">1425</key><summary>Highlighting broken with custom_filters_score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmwilson</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-25T19:09:15Z</created><updated>2011-10-25T22:59:03Z</updated><resolved>2011-10-25T22:59:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-25T22:51:22Z" id="2524267">Tricky to support.... I will add proper support when term vectors are used, but when not, it will only support highlighting when the custom filters score is the top level one (which is the common case).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file></files><comments><comment>Highlighting broken with custom_filters_score query, closes #1425.</comment></comments></commit></commits></item><item><title>Silently discard a non-existant field if asked for and _source is not stored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1424</link><project id="" key="" /><description>If a document is searched for like so:

```
curl http://localhost:9200/myindex/_search -d '
{"fields": ["foo","bar"],
 "query": {"query_string": {"defaultField": "foo",
                            "query": "foo:*"}}}
```

If the `"bar"` field does not exist in the mapping and _source is not stored, ES throws an exception. ES should silently discard fields that don't exist if they are not stored and _source is not stored.
</description><key id="2047803">1424</key><summary>Silently discard a non-existant field if asked for and _source is not stored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-25T17:59:49Z</created><updated>2011-10-25T19:22:13Z</updated><resolved>2011-10-25T19:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-25T19:22:13Z" id="2521724">Implemented.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>indexing option ignored in index_stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1423</link><project id="" key="" /><description>With '`clear=true` and  `indexing=true`, should get indexing stats back, but it is ignored

```
curl -XGET 'http://127.0.0.1:9200/es_test_1/_stats?indexing=true&amp;types=type_1&amp;pretty=1&amp;clear=true' 

# [Tue Oct 25 19:09:14 2011] Response:
# {
#    "ok" : true,
#    "_all" : {
#       "primaries" : {},
#       "indices" : {
#          "es_test_1" : {
#             "primaries" : {},
#             "total" : {}
#          }
#       },
#       "total" : {}
#    },
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 10,
#       "total" : 10
#    }
# }
```
</description><key id="2047284">1423</key><summary>indexing option ignored in index_stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-10-25T17:11:13Z</created><updated>2011-10-25T18:20:52Z</updated><resolved>2011-10-25T18:20:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file></files><comments><comment>indexing option ignored in index_stats, closes #1423.</comment></comments></commit></commits></item><item><title>* Add support for LZF compression on binary fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1422</link><project id="" key="" /><description>Compressing on other field types is a complex subject with tricky consequences. On the other side, compression on binary fields is easy and has absolutely no side effects, so here's a patch to do it.

Syntax:

"properties" : {
  "data" : {
    "type" : "binary",
    "compress" : "true"
  }
}
</description><key id="2045672">1422</key><summary>* Add support for LZF compression on binary fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2011-10-25T14:45:06Z</created><updated>2014-07-16T21:55:59Z</updated><resolved>2011-11-13T13:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-13T13:37:40Z" id="2722300">Pushed to 0.18 branch and master, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Allow to include / exclude specific parts of it when storing it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1421</link><project id="" key="" /><description>The `_source` field is by default stored. Sometimes, not all of it is needed to be stored, and the ability to include / exclude parts of it is needed. The `_source` mapping should support `includes` and `excludes` array options, specifying which part of the document are to be stored. The format allows to do things like: `path1.*`, `path1.*.path2` using `*` as a wildcard. Here is an example:

```
{
    "my_type" : {
        "_source" : {
            "includes" : ["path1.*", "path2.*"],
            "excludes" : ["pat3.*"]
        }
    }
}
```
</description><key id="2039170">1421</key><summary>Mapping: Allow to include / exclude specific parts of it when storing it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-24T22:01:37Z</created><updated>2011-10-27T16:06:42Z</updated><resolved>2011-10-24T22:02:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-10-25T07:30:21Z" id="2513985">Does this mean that the `_source` field that is returned from (eg) a `get` may not be the same as what you put in?

Or have I misunderstood this.  What effect will this have?
</comment><comment author="kimchy" created="2011-10-25T19:14:38Z" id="2521621">Yes, it will not be the same, since what is actually stored is only part of the _source.
</comment><comment author="clintongormley" created="2011-10-27T16:06:42Z" id="2545475">I notice that the includes/excludes is only applied once the index is flushed.  While the doc is still in the translog, you get back the whole `_source`.  I'm wondering whether that matters or not.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file></files><comments><comment>Mapping: Allow to include / exclude specific parts of it when storing it, closes #1421.</comment></comments></commit></commits></item><item><title>minimum_number_should_match in a query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1420</link><project id="" key="" /><description>Just like you can use minimum_number_should_match in a boolean query: http://www.elasticsearch.org/guide/reference/query-dsl/bool-query.html .

It would be awsome if we could do the following:

```
"query" : {
    "query_string" : {
      "query" : "author",
      "fields" : [ "Smith" ],
      "use_dis_max" : true,
      "minimum_should_match": 3
    }
  }
```

Solr for example supports the following format: http://lucene.apache.org/solr/api/org/apache/solr/util/doc-files/min-should-match.html .
</description><key id="2037322">1420</key><summary>minimum_number_should_match in a query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MagmaRules</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-24T19:12:13Z</created><updated>2011-11-10T10:41:39Z</updated><resolved>2011-10-25T23:58:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-11-05T12:10:14Z" id="2640149">Why it will be written to a field minimum_should_write? And what is the default behaviour now?
</comment><comment author="MagmaRules" created="2011-11-07T10:26:06Z" id="2651878">karussell i think its a bug. 
It should be "minimum_should_match". If you send ' "minimum_should_match" : "3" ' for example it works as expected. 
The default behavior i believe its how it used to work. If any word matches then it is returned.
</comment><comment author="kimchy" created="2011-11-10T08:21:35Z" id="2692635">@karussell: where did you see minimum_should_write? The default behavior is the same as before.
</comment><comment author="karussell" created="2011-11-10T09:32:11Z" id="2693198">in the source of QueryStringQueryBuilder

if (minimumShouldMatch != null) {
            builder.field("minimum_should_write", minimumShouldMatch);
        }
</comment><comment author="kimchy" created="2011-11-10T10:13:42Z" id="2693557">Right, fixed.
</comment><comment author="MagmaRules" created="2011-11-10T10:41:39Z" id="2693796">Cool ty Kimchy. I tried to reopen this ticket but i apparently regular users cant.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/QueryParserSettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file></files><comments><comment>minimum_number_should_match in a query_string, closes #1420.</comment></comments></commit></commits></item><item><title>Bool Filter fails with ArrayIndexOutOfBoundsException </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1419</link><project id="" key="" /><description /><key id="2028972">1419</key><summary>Bool Filter fails with ArrayIndexOutOfBoundsException </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.10</label><label>v0.18.0</label></labels><created>2011-10-23T22:20:31Z</created><updated>2011-10-23T22:30:38Z</updated><resolved>2011-10-23T22:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/DocSets.java</file></files><comments><comment>Bool Filter fails with ArrayIndexOutOfBoundsException, closes #1419.</comment></comments></commit></commits></item><item><title>First indexing fails when indexing a type with an attachment field mapped using a dynamic template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1418</link><project id="" key="" /><description>When mapping fields that match a certain pattern as attachments using dynamic templates indexing will crash the first time a type is indexed with such a field. That is, for each type with such a field, the first request will fail. Subsequent indexing operations work fine.

I have yet to test this with versions higher than 0.17.6 but I've not seen any fix for it in the release notes so while I'm experiencing this with 0.17.6 I'm guessing it's the same in later versions.

To replicate the following steps can be performed. The POST will fail with  {"error":"NullPointerException[null]","status":500}

curl -XPUT 'http://localhost:9200/myindex/' -d 
'{
    "mappings": {
        "_default_": {
            "dynamic_templates": [
                {
                    "attachment_template": {
                        "match": "*$$attachment",
                        "mapping": {
                            "type": "attachment"
                        }
                    }
                }
            ]
        }
    }
}'

curl -XPOST 'http://localhost:9200/myindex/mytype' -d 
'{
    "Attachment$$attachment": "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"
}'
</description><key id="2023825">1418</key><summary>First indexing fails when indexing a type with an attachment field mapped using a dynamic template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joelabrahamsson</reporter><labels><label>bug</label><label>v0.18.0</label></labels><created>2011-10-22T18:08:57Z</created><updated>2011-10-24T23:40:36Z</updated><resolved>2011-10-24T23:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-24T23:31:45Z" id="2511058">I see where it happens, basically, in the auto detect part and applying the dynamic template. It seems like with binary fields, once getting them as text will cause trying to get it as binary will not return it (with jackson parser). Can improve it by doing the check early on...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file></files><comments><comment>First indexing fails when indexing a type with an attachment field mapped using a dynamic template, closes #1418.</comment></comments></commit></commits></item><item><title>ZeroMQ</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1417</link><project id="" key="" /><description>HIya

You remember you were thinking about adding another protocol to avoid http latency between client and server.  What about ZeroMQ - it has libraries available in most languages, it is async and you can send JSON/BSON etc with it.

http://www.zeromq.org/

What do you think?

clint
</description><key id="2022820">1417</key><summary>ZeroMQ</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-10-22T15:00:34Z</created><updated>2016-08-19T13:56:52Z</updated><resolved>2014-07-03T19:15:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-24T23:43:31Z" id="2511144">I think it makes sense. Last time I looked, I just couldn't figure out what a stable version of zeromq was, and whenever I see that in a project, I know its going to be a headache... . I think I saw a project on github that tried to have a zeromq transport...
</comment><comment author="mrgleeco" created="2012-02-18T06:23:34Z" id="4032329">since this last touch, looks like v2.1 stable was released in 201112.  Thoughts? Still relevent? 
</comment><comment author="clintongormley" created="2012-02-18T10:53:48Z" id="4033332">There is this plugin available: https://github.com/tlrx/transport-zeromq

It is a bit tricky figuring out where to insert all the moving parts (C library, Java library, etc), so I'm hoping that the packaging could be improved.

There were a couple of issues with the plugin, and a couple of issues with the Perl ZeroMQ library - hopefully both of these have improved. I know Lestratt is working on updating the Perl library to work with libzmq3.

Also, I have a working ES.pm transport backend which uses the above plugin, but I need to update it and check if the issues have been sorted before I release it
</comment><comment author="tlrx" created="2012-02-18T16:46:53Z" id="4035342">I agree with Clinton, zeromq and the Java binding are not easy to build and get working. More of that, thread management of the native library caused some ES server hang ups in the plugin.

Well, I'll try to update the transport plugin as soon as a stable version 3.x of zeromq is out.
</comment><comment author="timbunce" created="2012-08-30T16:40:59Z" id="8165676">Any news on this?
http://www.zeromq.org/topics:planning says version 3.2 is recommend for all new applications.
</comment><comment author="clintongormley" created="2014-07-03T19:15:52Z" id="47972788">ZeroMQ seems to have foundered. Closing
</comment><comment author="haf" created="2016-08-19T13:56:52Z" id="241025198">@clintongormley How do you make that out?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: Indices query type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1416</link><project id="" key="" /><description>The `indices` query type allows to execute a query only for shards that belong to the listed indices, otherwise resulting in a "match_all" behavior. For example:

```
`indices` : {
    "indices" : ["index1", "index_prefix_*"]
    "query" : {
        "term" : ...
    }
}
```

--- original request

Currently, searches only allow one set of indices to be specified across the entire query, but it would be helpful to allow `BoolQueryBuilder`-style subqueries to be able to specify their own indices as well, something Compass used to support, and for ES to deal with the aggregation. In other words, add `setIndices(String... indices)` to `QueryBuilder`, rather than just having it on `SearchRequestBuilder`. This would allow UNION-style queries to be built up, of the form:

"return all hits where property=X in (indexA,indexB) OR property=Y in (indexA,indexC)"
</description><key id="2014776">1416</key><summary>Query DSL: Indices query type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">poblish</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-21T12:11:18Z</created><updated>2012-03-17T16:11:41Z</updated><resolved>2011-10-22T23:35:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-22T23:32:27Z" id="2493212">Setting the indices to execute on each one is problematic, but we can introduce a new query type, called `indices` that will wrap another query and be executed when it matches the indices provided. I will update the issue to reflect that.
</comment><comment author="poblish" created="2011-11-22T13:04:16Z" id="2834050">Hi Shay,

Thanks for your work on this. I assumed it was working, but now I'm not so sure. My generated query is looking like:

```
"query": {
  "bool": {
    "should": {
      "custom_boost_factor": {
        "query": {
        "match_all": {}
    },
    "indices": [
        "simpleflag",
        "favouriteflag"
        ]
       }
    }
  }
}
```

... it doesn't seem that the inner query is actually being wrapped by an outer indices query. I'm using the right QueryBuilders.indicesQuery(...) method, though. Is this the expected output?

Also, I imagined that the result of running the above across _all indices would be to only return all records for the specificed indices only, but as I understand it I'm going to get all from those because of my match_all, _plus_ all from every other index too, because match_all is used for everything not in "indices: []".

I think that having something like "match_none" would make more sense for indices not specified in the subquery's list. That would allow me to run global-style queries across _all indices, but give the subqueries a real chance to filter down by index, without pulling in unwanted documents from the indices they didn't specify.

WDYT?
</comment><comment author="kimchy" created="2011-11-22T13:57:09Z" id="2834513">Heya, its a bug in the indices query builder, that uses the `custom_boost_factor` by mistake. You can build it yourself for now, I will push a fix. Here is the issue: https://github.com/elasticsearch/elasticsearch/issues/1485.
</comment><comment author="poblish" created="2011-11-22T22:58:11Z" id="2842207">Thanks, I'm trying a newly-built snapshot, but I'm not seeing any difference: the query certainly changes, but no difference in the results for any query I try.

```
curl -XGET '10.10.10.101:9200/_all/default/_search' -d '{
    "query": {
        "indices": {
            "query": {
                "match_all": {}
            },
            "indices": [
                "simpleflag", "favouriteflag", "articleversion", "articlerating","followrelationship", "groupevent", "assertionflag"
            ]
        }
    }
}'
```

So, with the above, I'm expecting the indices wrapper query to give me back a small number of results (every doc in _those few_ indices), but I'm actually getting every single doc in _all_ indices returned.

(I know I could rewrite the above to do without `indices{}`, but this is just the simplest possible case.)
</comment><comment author="kimchy" created="2011-11-23T07:10:05Z" id="2845693">The query you posted in this form does not make sense. The `indices` query will use the query you provided internally when its executed on one of the listed indices, and _match_all_ when it does not match one of those indices, which, if I remember, is what you were after.
</comment><comment author="poblish" created="2011-11-23T09:31:55Z" id="2846625">This takes us back to the first comment I left here yesterday. It looks like what I really need is not in fact _match_all_ for indices not in the `indices[]` section, but "match none". In the example above I _only_ care about the results within the `indices{}` query - I don't want anything else whatsoever. What Compass did was perfect for me :-)
</comment><comment author="kimchy" created="2011-11-23T10:10:47Z" id="2846951">When do you really need match_none? I mean, if its match_none, in your example, why not just query those indices (instead of using `_all`) and thats it?
</comment><comment author="poblish" created="2011-11-23T10:46:44Z" id="2847232">The above was the simplest possible case. In reality my app will be receiving custom, configurable queries like the following, and turning them (programatically) into suitably nested queries to pass to ES:
- get me all documents where { either John or Jane Smith} is mentioned in an article, performed an action, was tagged, etc.
- get me a list/count of all tags, flags, etc.

There are some cases where setting the overall indices helps, but in general I have to search against _all, relying on the individual bool() subqueries to control which (of the 50 or so) indices are used. Compass achieved this beautifully. One way of looking at this is that I want to use my subqueries to build up from _nothing_. What `indices{}` currently does is filter down from _everything_.
</comment><comment author="kimchy" created="2011-11-23T17:04:13Z" id="2851424">I still don't understand why you would need one that does not match, maybe in should clauses in a boolean query... . Opened #1492.
</comment><comment author="poblish" created="2011-11-25T09:19:15Z" id="2872672">Great, this is now working perfectly - many thanks!
</comment><comment author="folke" created="2012-03-16T08:45:36Z" id="4535913">Any chance "indices" could be added as a filter as well?
</comment><comment author="medcl" created="2012-03-17T16:11:41Z" id="4555008">++1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndicesQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file></files><comments><comment>Query DSL: Indices query type, closes #1416.</comment></comments></commit></commits></item><item><title>Use CJKAnalyzer instead of ChineseAnalyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1415</link><project id="" key="" /><description>As you can see,  ChineseAnalyzer is used for 'cjk' language analyzer. It should be CJKAnalyzer indeed.

Signed-off-by: Hiroyuki Ikezoe hiikezoe@gnome.org
</description><key id="2011625">1415</key><summary>Use CJKAnalyzer instead of ChineseAnalyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hiikezoe</reporter><labels /><created>2011-10-21T01:57:45Z</created><updated>2014-07-02T11:33:41Z</updated><resolved>2011-10-24T21:55:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-21T18:04:19Z" id="2484361">Right, thanks!, will pull.
</comment><comment author="kimchy" created="2011-10-24T21:55:25Z" id="2510076">Pulled.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use parameters given by user to launch ES in Windows Script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1414</link><project id="" key="" /><description>As discussed in the mailing list : https://groups.google.com/group/elasticsearch/browse_thread/thread/86a682574b589fc7/938083b4a43b390e?hl=fr%F3%A5%81%A3b4a43b390e

Parameters given by user should work also for windows users

Hope this helps
David
</description><key id="2008914">1414</key><summary>Use parameters given by user to launch ES in Windows Script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-10-20T20:14:23Z</created><updated>2014-06-19T17:01:34Z</updated><resolved>2011-11-20T10:56:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-11-16T14:16:33Z" id="2759795">I think this can cause problems, no? Parameters need to be escaped or something? Also, it means that always all the parameters are added to ES_PARAMS. Check the bootsap one, we actually parse based on it.

I am not terribly against this, it might hinder us in the future, just want to double check it will work well...
</comment><comment author="dadoonet" created="2011-11-16T19:04:14Z" id="2766015">I agree. So here is a new commit for it. Is that what you were looking for ?
I tested it with ES 0.17.6 and I'm now able to run ES with :

```
bin\elasticsearch.bat -Des.cluster.name=MyCluster
```

Thanks,
David.
</comment><comment author="dadoonet" created="2011-11-17T16:13:51Z" id="2777884">Sorry ! Don't merge my pull request. There is a failure in it.
I will make more testing before you can merge it.

Sorry for that !
</comment><comment author="dadoonet" created="2011-11-18T17:53:45Z" id="2792079">It's safe now :
Tested with

```
bin\elasticsearch.bat -Des.cluster.name=MyCluster
bin\elasticsearch.bat -Des.cluster.name=MyCluster -server
bin\elasticsearch.bat
```
</comment><comment author="kimchy" created="2011-11-20T10:56:10Z" id="2803360">Pushed to master, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ability to exclude fields in the _source mapping.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1413</link><project id="" key="" /><description>The code works as expected, but it may not be the most efficient way to do it, reviews appreciated :)

Usage:

"mappings" : {
"documents" : {
"_source" : {
"excludes" : [ "content", "foo.bar" ]
},
"properties" : {
....
</description><key id="2003294">1413</key><summary>Ability to exclude fields in the _source mapping.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2011-10-20T10:09:03Z</created><updated>2014-06-19T10:37:39Z</updated><resolved>2011-10-24T22:03:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-24T22:03:07Z" id="2510168">Missing some functionality, but implemented here #1421, check it out.
</comment><comment author="ahfeel" created="2011-10-24T22:09:27Z" id="2510242">Wow, looks pretty awesome. If I read the code well, it supports the same syntax but adds includes mode, that's perfect. Thanks Shay !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Implement "interesting terms" in More Like This handler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1412</link><project id="" key="" /><description>Solr implements the "interesting terms" for MLT queries: http://wiki.apache.org/solr/MoreLikeThisHandler

"One of: "list", "details", "none" -- this will show what "interesting" terms are used for the MoreLikeThis query. These are the top tf/idf terms. NOTE: if you select 'details', this shows you the term and boost used for each term. Unless mlt.boost=true all terms will have boost=1.0"
</description><key id="1999774">1412</key><summary>Implement "interesting terms" in More Like This handler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dcramer</reporter><labels><label>:More Like This</label></labels><created>2011-10-19T23:13:52Z</created><updated>2017-07-07T15:48:45Z</updated><resolved>2015-07-06T14:56:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2011-10-22T03:58:34Z" id="2488315">+1 nice feature
</comment><comment author="vinhphu1711" created="2012-06-12T08:56:02Z" id="6265868">+1 I'm excited to see this feature. Still no milestone yet?
</comment><comment author="dcroley" created="2012-07-06T20:06:24Z" id="6815036">+1 This would be really useful for my project too.
</comment><comment author="tkholopkin" created="2012-07-09T15:37:56Z" id="6850167">+100 Must have. Especially if projects migrate from Lucene to Elastic Search
</comment><comment author="mohsinh" created="2013-10-07T09:52:47Z" id="25796427">+1 to get list of "Interesting Terms"
</comment><comment author="alexanderjmitchell" created="2014-02-12T14:43:43Z" id="34874764">+1 this would be really useful
</comment><comment author="alexksikes" created="2014-05-09T08:58:42Z" id="42646072">There are two things that we may wish to achieve with "interesting terms" and More Like This.

1) Return the selected interesting terms that formed the More Like This query. These could be returned as an ordered list of (term, tf-idf, score) where score is the boosted score if activated.

2) Return the matched interesting terms of each document returned from the response.

The former is, I think, what Solr does with "mlt.interestingTerms", while the later could be achieved with "explain". However, "explain" is more for debugging purposes and so its output is very verbose. Perhaps, it would be more desirable to simply return a list of matched interesting terms. Any thoughts?

I haven't tested the "mlt.interestingTerms" Solr feature, would anyone know what the exact behavior and output are?
</comment><comment author="piyushrai" created="2014-11-11T11:53:21Z" id="62536968">+1 This is almost must have if one want to move more like this (with boosting) from SOLR to elasticsearch. Any updates on this?
</comment><comment author="alexksikes" created="2015-07-06T14:56:16Z" id="118880165">closed by https://github.com/elastic/elasticsearch/pull/10147
</comment><comment author="sabi0" created="2017-07-07T12:50:13Z" id="313672268">Sorry for bringing this old issue up.
I couldn't find a way to determine "interesting terms" for MLT query with an ad-hoc document.
`_validate/query?explain=true` returns just "filtered(like:[...here goes my text...]))".
~~Is such feature available?~~ I figured it out - just had to add `&amp;rewrite=true`.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ShardValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Validate API: support for verbose explanation of succesfully validated queries</comment></comments></commit></commits></item><item><title>Exit on metadata file corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1411</link><project id="" key="" /><description>Currently when ES encounters an error initializing from the metadata _state it continues on, overwriting whatever data was previously stored.  This eliminates any chance of fixing the problem and instantly destroys the data stored on the node.

A safer default behavior would be to exit in the event of an error.  This allows the user either correct the problem or delete the corrupted data explicitly.
</description><key id="1999629">1411</key><summary>Exit on metadata file corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merrellb</reporter><labels /><created>2011-10-19T22:54:18Z</created><updated>2014-07-08T12:40:04Z</updated><resolved>2014-07-08T12:40:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T12:40:04Z" id="48330402">Error handling has changed greatly since this ticket was opened, so I'm going to close this ticket. Please reopen if you think there is still an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>river-twitter needs better error handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1410</link><project id="" key="" /><description>I setup a river with

```
{
    "type" : "twitter",
    "twitter" : {
        "user" : "me",
        "password" : "1234",
        "index" : {
            "index" : "twitter-google",
            "type" : "status",
            "bulk_size" : 10
        },
        "filter" : {
            "tracks" : "google"
        }
    }
}
```

It kept getting disconnected, showing two problems:
- Lack of logging (see #1409)
- Lack of error handling.  If it keeps getting disconnected it should back off for a bit and wait a little between each connection.  I'm not sure if it got disconnected because it had two connections going (I think not, but maybe) or because it was getting an error. If the latter it wasn't being logged and the client wasn't behaving properly.
</description><key id="1996210">1410</key><summary>river-twitter needs better error handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abh</reporter><labels /><created>2011-10-19T17:20:34Z</created><updated>2013-04-05T16:10:35Z</updated><resolved>2013-04-05T16:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-19T21:14:44Z" id="2461326">I am using a library called twitter4j that does that handling. Which version are you using? Twitter changed their URL to get updates, and the twitter4j lib has not released an official lib release to reflect it. Both 0.17 branch and master now use a snapshot build of twitter4j for it to work.
</comment><comment author="abh" created="2011-10-19T21:20:59Z" id="2461408">I was trying with 0.17.8.
</comment><comment author="clintongormley" created="2013-04-05T16:10:35Z" id="15965114">No further info after 1 year - assuming resolved
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>river-twitter needs better diagnostics/logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1409</link><project id="" key="" /><description>I setup a river with:

```
{
    "type" : "twitter",
    "twitter" : {
        "user" : "me",
        "password" : "123",
        "index" : {
            "index" : "twitter-us",
            "type" : "status",
            "bulk_size" : 5
        },
        "filter" : {
            "locations" : "-160,18,-59.0,47"
        }
    }
}
```

It says it started, but didn't get any documents.   A log entry with the stream URL it started would be very helpful for debugging.
</description><key id="1996190">1409</key><summary>river-twitter needs better diagnostics/logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abh</reporter><labels /><created>2011-10-19T17:18:22Z</created><updated>2013-04-05T16:10:51Z</updated><resolved>2013-04-05T16:10:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Added Wordnet format into synonym filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1408</link><project id="" key="" /><description>- Allow to use `WordnetSynonymParser` via `format : wordnet` settings.
- `CustomSynonymParser` is removed and `SolrSynonymParser` used instead.
- Added synonym tests.
</description><key id="1995465">1408</key><summary>Added Wordnet format into synonym filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-10-19T16:07:43Z</created><updated>2014-07-10T08:21:37Z</updated><resolved>2011-10-19T21:45:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-19T21:45:04Z" id="2461663">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ability to exclude fields in the _source mapping.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1407</link><project id="" key="" /><description>The code works as expected, but it may not be the most efficient way to do it, reviews appreciated :)

Usage:

  "mappings" : {
    "documents" : {
      "_source" : {
        "excludes" : [ "content" ]
      },
      "properties" : {
....
</description><key id="1994415">1407</key><summary>Ability to exclude fields in the _source mapping.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels /><created>2011-10-19T14:30:05Z</created><updated>2014-07-16T21:56:02Z</updated><resolved>2011-10-20T10:09:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ahfeel" created="2011-10-20T10:09:26Z" id="2466868">Made the pull request from a dedicated branch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support default_value for fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1406</link><project id="" key="" /><description>Context:
https://groups.google.com/group/elasticsearch/browse_thread/thread/19a842e3f3f393c4

It would be useful to support a `default_value` field in addition to `null_value` and clarify the situations where they will be used.
</description><key id="1982856">1406</key><summary>Support default_value for fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmwilson</reporter><labels /><created>2011-10-18T23:17:08Z</created><updated>2013-04-05T16:11:43Z</updated><resolved>2013-04-05T16:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-18T23:19:59Z" id="2449182">I will break this issue into the case of `_boost` and other cases. Other custom fields are more difficult to support with how elasticsearch parsing works, but `_boost` should be simpler to implement.
</comment><comment author="clintongormley" created="2013-04-05T16:11:42Z" id="15965180">Default _boost supported
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DuplicateFilter in _search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1405</link><project id="" key="" /><description>Hi,

I have indexed documents that have a duplicate field. During the search I want to display only the document that has the highest score among its duplicate class (the group of documents that have the same field value).

There has been a discussion about deduplication in Lucene mailing-list:
http://www.mail-archive.com/java-user@lucene.apache.org/msg21437.html

Would it be possible to implement the Duplicate filter which is in contrib:
https://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/contrib/sandbox/src/test/org/apache/lucene/sandbox/queries/DuplicateFilterTest.java

So the request would look like:

```
{
    "query": {
        "term": { "name": "alexis" }
    },
    "filter": {
         "duplicate": { "field": "name" }
    }
}
```

Let me know if that makes sense...
</description><key id="1953509">1405</key><summary>DuplicateFilter in _search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexis779</reporter><labels /><created>2011-10-18T16:19:07Z</created><updated>2014-12-10T21:55:42Z</updated><resolved>2011-10-27T15:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-18T16:30:21Z" id="2444007">duplicate filter in Lucene does not work really well since Lucene moved to segment based searching, and, its problematic in distributed search as well...
</comment><comment author="alexis779" created="2011-10-27T15:22:54Z" id="2544849">Thanks for your quick reply.

We did not like to see documents that looked like duplicates in search results so we set up a process to detect whether a document is a duplicate or not at indexing time, discarding it from search results by seting _boost parameter as 0.
</comment><comment author="dustinboswell" created="2014-12-10T21:55:42Z" id="66531363">I have a similar need for removing duplicates, but can't remove them at indexing time for various reasons. Is there another recommended approach for this problem?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Note the rangeness of the port in the example config.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1404</link><project id="" key="" /><description>In `config/elasticsearch.yml`, it is noted that one can configure the inter-node communications port.

Two alterations to this note should be made.
- Note that this is the base port for a port range for inter-node communications, not a single port.
- Note the size of the port range.
</description><key id="1938490">1404</key><summary>Note the rangeness of the port in the example config.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yfeldblum</reporter><labels /><created>2011-10-18T11:28:29Z</created><updated>2013-04-05T16:12:50Z</updated><resolved>2013-04-05T16:12:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T16:12:50Z" id="15965240">Done
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: A failed search request might get overrun when trying another shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1403</link><project id="" key="" /><description>Search: A failed search request might get overrun when trying another shard
</description><key id="1935716">1403</key><summary>Search: A failed search request might get overrun when trying another shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.9</label><label>v0.18.0</label></labels><created>2011-10-18T02:49:52Z</created><updated>2011-10-18T02:50:28Z</updated><resolved>2011-10-18T02:50:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file></files><comments><comment>Search: A failed search request might get overrun when trying another shard, closes #1403.</comment></comments></commit></commits></item><item><title>Support field compression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1402</link><project id="" key="" /><description>Are there any plans to support field compression? I think solr 1.4 used to have it then they dropped it in 3.x for whatever reason. We had to stick to 1.4 for one of our indexes because it would be unmanageable if uncompressed. We're in the process of migrating to elasticsearch and will definitely need this.
</description><key id="1935660">1402</key><summary>Support field compression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brianmario</reporter><labels /><created>2011-10-18T02:33:53Z</created><updated>2013-03-03T20:01:08Z</updated><resolved>2013-03-03T20:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-18T21:04:59Z" id="2447795">For specific stored fields, there is no support for compression. But, you can configure the `_source` field to be compressed (its stored by default). See the source mapping in the docs.
</comment><comment author="brianmario" created="2011-10-18T21:12:33Z" id="2447914">Right, I'm wondering if support could be  added? I'd offer help but my Java skills are much more than rusty ;)

On Oct 18, 2011, at 2:05 PM, Shay Banonreply@reply.github.com wrote:

&gt; For specific stored fields, there is no support for compression. But, you can configure the `_source` field to be compressed (its stored by default). See the source mapping in the docs.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1402#issuecomment-2447795
</comment><comment author="kimchy" created="2011-10-18T22:56:55Z" id="2448997">Its tricky now in Lucene, it means having two fields, one that si indexed and not stored, and one that is stored with binary compressed data, but not indexed. We can possibly do that in elasticsearch by introducing compression on the binary type, have it also support text, and then use multi field mapping or something like that... . It will still be problematic when trying to highlight it for example, since you have one field name that is used for indexing, and one for storing...
</comment><comment author="brianmario" created="2011-10-18T23:32:42Z" id="2449299">Ah, I figured there was a reason solr dropped it too.

Thanks!

On Oct 18, 2011, at 3:56 PM, Shay Banonreply@reply.github.com wrote:

&gt; Its tricky now in Lucene, it means having two fields, one that si indexed and not stored, and one that is stored with binary compressed data, but not indexed. We can possibly do that in elasticsearch by introducing compression on the binary type, have it also support text, and then use multi field mapping or something like that... . It will still be problematic when trying to highlight it for example, since you have one field name that is used for indexing, and one for storing...
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1402#issuecomment-2448997
</comment><comment author="monken" created="2011-10-19T16:26:57Z" id="2457980">As a workaround, you could use a file system that does transparent compression for you.
</comment><comment author="ahfeel" created="2011-11-13T23:59:28Z" id="2725365">Just a quick update on this, compression to binary fields is added in master and 0.18 branch and should be available in upcoming 0.18.3.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Synonym filter not working correctly for multi-term synonyms.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1401</link><project id="" key="" /><description>I was trying to get my head around new multi term synonyms (LUCENE-3233) but it does not seem to work correctly based on my expectations. It seems that both synonym forms **[a b,c]** and **[a b =&gt; c]** suffer from specific issues. Both cases are discussed in detail below.
# Type of synonym rule: [a b =&gt; c]

Let's start with script that defines one simple multi-term synonym: "application server =&gt; eap". So whenever `application server` multi-term is found it is replaced with `eap`:

```
cat &lt;ES_HOME&gt;/config/analysis/synonym.txt
application server =&gt; eap
```

Now, let's use the following repro script:

```
curl -X DELETE 'localhost:9200/test/'
curl -X PUT 'localhost:9200/test/' -d '
{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0,
      "analysis" : {
        "analyzer" : {
          "ta" : {
            "type" : "custom",
            "tokenizer" : "standard",
            "filter" : [ "standard", "word_delimiter", "lowercase", "synonym", "stop", "kstem" ]
          }
        },
        "filter" : {
          "synonym" : {
            "type" : "synonym",
            "synonyms_path" : "analysis/synonym.txt"
          }
        }
      }
    }
  }
}'

curl -X PUT 'localhost:9200/test/test/_mapping' -d '
{
  "test" : {
    "properties" : {
      "plain" : { "type" : "string", "store" : "yes", "term_vector" : "no", "analyzer" : "ta" },
      "fvh" : { "type" : "string", "store" : "yes", "term_vector" : "with_positions_offsets", "analyzer" : "ta" }
    },
    "_all" : { "analyzer" : "ta" }
  }
}'

curl -X GET 'localhost:9200/_cluster/health?wait_for_status=green&amp;timeout=5s'

curl -X POST 'localhost:9200/test/test' -d '{"plain":"Hello EAP World!","fvh":"Hello EAP World!"}'
curl -X POST 'localhost:9200/test/test' -d '{"plain":"This is one really nice application server.","fvh":"This is one really nice application server."}'
curl -X POST 'localhost:9200/test/test' -d '{"plain":"Tomcat server is down.","fvh":"Tomcat server is down."}'
curl -X POST 'localhost:9200/test/test' -d '{"plain":"Application of drugs.","fvh":"Application of drugs."}'
curl -X POST 'localhost:9200/test/test' -d '{"plain":"This is ApplicationServer and EAP world.","fvh":"This is ApplicationServer and EAP world."}'

curl -X POST 'localhost:9200/test/_refresh'

echo "======================="
echo "application server"
curl 'localhost:9200/test/_search?pretty=1' -d '
{
  "query":{"query_string":{"query":"application server"}},
  "fields" : [ "plain","fvh"],
  "highlight" : { "fields" : { "plain" : {}, "fvh" : { "number_of_fragments":0 }}}
}'

echo "======================="
echo "eap"
curl 'localhost:9200/test/_search?pretty=1' -d '
{
  "query":{"query_string":{"query":"eap"}},
  "fields" : ["plain","fvh"],
  "highlight" : { "fields" : { "plain" : {}, "fvh" : { "number_of_fragments":0 }}}
}'

echo "======================="
echo "server"
curl 'localhost:9200/test/_search?pretty=1' -d '
{
  "query":{"query_string":{"query":"server"}},
  "fields" : ["plain","fvh"],
  "highlight" : { "fields" : { "plain" : {}, "fvh" : { "number_of_fragments":0 }}}
}'
```
## Problematic Queries
### Query: application server

No documents containing `application server` match. Instead I get only documents matching individual terms `application` or `server`. Analyzer itself works correctly, see:
    curl 'localhost:9200/test/_analyze?pretty=1&amp;text=application+server&amp;analyzer=ta'

Can the problem be in query parsing then?
### Query: eap

I am getting incorrect highlighting for two documents.

Original text: `This is ApplicationServer and EAP world.`
Highlighted text: `This is &lt;em&gt;Application&lt;/em&gt;Server and &lt;em&gt;EAP&lt;/em&gt; world.`

Original text: `This is one really nice application server.`
Highlighted text: `This is one really nice &lt;em&gt;application&lt;/em&gt; server.`

**Problem:** In both cases the word `server` should have been highlighted as well.
# Type of synonym rule: [a b, c]

Let's take the same script except modify definition of `synonym` analysis.

```
....
        "filter" : {
          "synonym" : {
            "type" : "synonym",
            "synonyms" : [ "application server, eap" ]
          }
        }
....
```

Or change content of the `&lt;ES_HOME&gt;/conf/analysis/synonym.txt` file accordingly.

Run the same repro script.
## Problematic Queries
### Query: application server

Original text: `This is ApplicationServer and EAP world.`
Highlighted text: `This is &lt;em&gt;Application&lt;/em&gt;&lt;em&gt;Server&lt;/em&gt; and &lt;em&gt;EAP&lt;/em&gt; &lt;em&gt;world&lt;/em&gt;.`

Original text: `Hello EAP World!`
Highlighted text: `Hello &lt;em&gt;EAP&lt;/em&gt; &lt;em&gt;World&lt;/em&gt;!`

In both cases the word `world` should not be highlighted.

Also both documents containing only individual terms from the multi-term synonym match as well (`application` or 'server'). But this time it is probably correct according to analyzer output:

```
curl 'localhost:9200/test/_analyze?pretty=1&amp;text=application+server&amp;analyzer=ta'
```

However, it would be great if there is an option how to avoid matching documents having partial synonym phrases only.
### Query: eap

The same output (and issues) as for the previous query.
### Query: server

Original text: `This is ApplicationServer and EAP world.`
Highlighted text: `This is Application&lt;em&gt;Server&lt;/em&gt; and EAP &lt;em&gt;world&lt;/em&gt;.`

Original text: `Hello EAP World!`
Highlighted text: `Hello EAP &lt;em&gt;World&lt;/em&gt;!`

**Problem**: the word `world` should not he highlighted.
</description><key id="1928486">1401</key><summary>Synonym filter not working correctly for multi-term synonyms.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>bug</label></labels><created>2011-10-17T13:39:13Z</created><updated>2013-02-18T22:41:12Z</updated><resolved>2013-02-18T22:41:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-10-17T13:42:32Z" id="2428880">Just to make it clear, for the **[a b =&gt; c]** synonym rule and the query `application server` there are documents that should be matching (not only those containing individual terms from the multi-term phrase).
</comment><comment author="kimchy" created="2011-10-17T23:31:02Z" id="2436174">I haven't run it yet, but, looking at what you posted, I think I can see where the problem is. The `query_string` query uses Lucene query parser, which will break the text by whitespace before feeding it into the analyzer. Does the same happen with the `text` query?
</comment><comment author="ppearcy" created="2011-11-11T03:41:28Z" id="2704640">Looks like I raised a similar issue:
https://github.com/elasticsearch/elasticsearch/issues/1444

This one is much better organized, though :)

I tried text queries for mine and got the same results, which from what I've seen makes sense since this appears to be an index time issue. It seems that some terms are getting set with incorrect TermOffsets. In the case I saw, start and end were both 0. I've been looking around to figure out why, but don't see it just yet. 
</comment><comment author="ppearcy" created="2011-11-23T17:17:51Z" id="2851700">I had come across this issue that might be related:
https://issues.apache.org/jira/browse/SOLR-2845

I tried setting the version to 3.3 on both my synonym filter and analyzer using it, which made no difference, though. 
</comment><comment author="edwardw" created="2011-12-29T16:03:50Z" id="3301935">I play with lukas-vlcek's example a little bit. If `synonym.txt` were defined the other way around:

``` bash
cat &lt;ES_HOME&gt;/config/analysis/synonym.txt
eap =&gt; application server
```

then the result becomes a little better. I.e., documents containing `application server` does match now. However, highlighting is still problematic. But that's probably a Lucene issue: see [LUCENE-1622](https://issues.apache.org/jira/browse/LUCENE-1622).
</comment><comment author="ppearcy" created="2012-01-17T06:17:00Z" id="3523707">Looks like there is a lucene fix:
https://issues.apache.org/jira/browse/LUCENE-3668

Trying to test it out...
</comment><comment author="ppearcy" created="2012-05-04T19:17:57Z" id="5518155">FYI, I just re-ran my test cases and with 19.3 (lucene 3.6) this is fixed. 

Lukas, do you want to confirm and close? 

Thanks,
Paul
</comment><comment author="s1monw" created="2013-02-18T22:41:12Z" id="13747183">this seems to be fixed. we can still re-open if not.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Array out-of-bounds exception with bool filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1400</link><project id="" key="" /><description>Hiya

Trying to upgrade from 0.16.5 to 0.17.8

A simple bool filter gives me an array out-of-bounds exception on 3 of the 5 shards.  If I change the bool filter to a bool query, or to an `and` filter, then all is OK:

https://gist.github.com/ca2da49500f4c55638a7

clint
</description><key id="1917410">1400</key><summary>Array out-of-bounds exception with bool filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.17.9</label><label>v0.18.0</label></labels><created>2011-10-15T12:52:17Z</created><updated>2011-10-15T13:35:27Z</updated><resolved>2011-10-15T13:35:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/DocSets.java</file></files><comments><comment>Array out-of-bounds exception with bool filter, closes #1400.</comment></comments></commit></commits></item><item><title>multi get ids shortcut should grab custom fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1399</link><project id="" key="" /><description>This patch lets the `{"ids": ["..."]}` form of a multi get use the url parameter "fields" like the longer form does.
</description><key id="1915425">1399</key><summary>multi get ids shortcut should grab custom fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2011-10-15T00:44:28Z</created><updated>2014-07-16T21:56:03Z</updated><resolved>2011-10-15T09:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-15T09:58:24Z" id="2414850">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow different source/target tokenizers for SynonymFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1398</link><project id="" key="" /><description>This is probably an edge case, but it would be great to be able to specify different tokenizers for the source and the target in the SynonymFilter. My particular use case is that my input is structured, multi-word text that I want to run through a KeywordAnalyzer to match multi-word tokens in the synonyms.txt source side; but I'd like for the output to be run through a StandardTokenizer. So it would be nice if the SynonymFilter provided a `source_tokenizer` and `target_tokenizer` option, with the `tokenizer` option just setting both to the same.
</description><key id="1911752">1398</key><summary>Allow different source/target tokenizers for SynonymFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels /><created>2011-10-14T16:01:11Z</created><updated>2016-09-28T01:49:15Z</updated><resolved>2014-07-08T12:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T12:38:55Z" id="48330292">Hi @outoftime 

I'm assuming you found some workaround to this?  There hasn't been any discussion on the ticket for 3 years, so I'm going to close it.
</comment><comment author="maxiruani" created="2016-09-28T01:49:15Z" id="250050605">I think I need to achieve something like this.

This is my analyzer and filter configuration:

``` json
{
  "analysis": {
    "filter": {
      "synonym_filter": {
        "type": "synonym",
        "synonyms": [
          "customer service =&gt; call center,telephone operator",
        ],
        "tokenizer": "keyword"
      }
    },
    "analyzer": {
      "synonym_analyzer": {
        "filter": [
          "lowercase",
          "synonym_filter"
        ],
        "tokenizer": "keyword"
      }
    }
  }
}
```

`http://localhost:9200/index/_analyze?analyzer=synonym_analyzer&amp;text=customer service&amp;pretty`

I need to match the whole keyword "customer service" to match the synonym "call center" or "telephone operator", but then I need the synonym to be standard tokenized, so I can be able to make a full text search.

Is there any way to achieve this?

Thanks in advance,

Maximiliano
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exception: TokenStream implementation classes or at least their incrementToken() implementation must be final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1397</link><project id="" key="" /><description>Sorry, I'm not able to provide a curl reproducer. What I did was to index System.currentTimeInMillis() in a test and then query for it. And I get this stack trace:

Exception in thread "elasticsearch[search]-pool-199-thread-5" java.lang.AssertionError: TokenStream implementation classes or at least their incrementToken() implementation must be final
        at org.apache.lucene.analysis.TokenStream.assertFinal(TokenStream.java:117)
        at org.apache.lucene.analysis.TokenStream.&lt;init&gt;(TokenStream.java:100)
        at org.apache.lucene.analysis.Tokenizer.&lt;init&gt;(Tokenizer.java:58)
        at org.elasticsearch.index.analysis.NumericTokenizer.&lt;init&gt;(NumericTokenizer.java:45)
        at org.elasticsearch.index.analysis.NumericLongTokenizer.&lt;init&gt;(NumericLongTokenizer.java:37)
        at org.elasticsearch.index.analysis.NumericLongAnalyzer.createNumericTokenizer(NumericLongAnalyzer.java:43)
        at org.elasticsearch.index.analysis.NumericLongAnalyzer.createNumericTokenizer(NumericLongAnalyzer.java:30)
        at org.elasticsearch.index.analysis.NumericAnalyzer.reusableTokenStream(NumericAnalyzer.java:45)
        at org.elasticsearch.index.analysis.NamedAnalyzer.reusableTokenStream(NamedAnalyzer.java:81)
        at org.elasticsearch.index.mapper.MapperService$SmartIndexNameSearchAnalyzer.reusableTokenStream(MapperService.java:696)
        at org.apache.lucene.search.FuzzyLikeThisQuery.addTerms(FuzzyLikeThisQuery.java:187)
        at org.apache.lucene.search.FuzzyLikeThisQuery.rewrite(FuzzyLikeThisQuery.java:264)
        at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:589)
        at org.elasticsearch.search.internal.ContextIndexSearcher.rewrite(ContextIndexSearcher.java:125)
        at org.apache.lucene.search.Searcher.createNormalizedWeight(Searcher.java:167)
        at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:661)
        at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:138)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:298)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:286)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:217)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:234)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:204)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:191)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:177)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
</description><key id="1910016">1397</key><summary>Exception: TokenStream implementation classes or at least their incrementToken() implementation must be final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cpesch</reporter><labels><label>bug</label><label>v0.17.9</label><label>v0.18.0</label></labels><created>2011-10-14T12:43:51Z</created><updated>2014-10-02T07:22:14Z</updated><resolved>2011-10-14T14:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cpesch" created="2011-10-14T12:44:16Z" id="2406176">version is 0.17.8
</comment><comment author="cpesch" created="2011-10-14T12:47:43Z" id="2406204">maybe I shouldn't have used

```
  builder = fuzzyLikeThisQuery(name).likeText(value).prefixLength(PREFIX_LENGTH).minSimilarity(MIN_SIMILARITY);
```

on a long - but the error message could be nicer
</comment><comment author="kimchy" created="2011-10-14T14:11:07Z" id="2406965">Yea, fuzzy does not mean much on a numeric field, but, I will fix the problem
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NumericTokenizer.java</file></files><comments><comment>Exception: TokenStream implementation classes or at least their incrementToken() implementation must be final, closes #1397.</comment></comments></commit></commits></item><item><title>Fix for #1393</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1396</link><project id="" key="" /><description>Updated the dependencies and code to handle twitter changes.

This should be changed to 2.2.5 once available

Pull request on the master branch (the other was on 0.17)
</description><key id="1901668">1396</key><summary>Fix for #1393</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmalves</reporter><labels /><created>2011-10-13T16:00:54Z</created><updated>2014-07-16T21:56:03Z</updated><resolved>2011-10-14T15:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-14T15:46:55Z" id="2408118">Pushed a fix with 2.2.5 snapshot
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>- Fix for [1393]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1395</link><project id="" key="" /><description>Updated the dependencies and code to handle twitter changes.

This should be changed to 2.2.5 once available
</description><key id="1901638">1395</key><summary>- Fix for [1393]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmalves</reporter><labels /><created>2011-10-13T15:58:15Z</created><updated>2014-07-16T21:56:04Z</updated><resolved>2011-10-14T15:47:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-14T15:47:13Z" id="2408123">PUshed a fix with 2.2.5 snapshot
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Comments are not allowed in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1394</link><project id="" key="" /><description>Hi,

I would like to be able to send comment in mapping as I managed mappings in files.
So by now, it's not allowed to send that kind of mapping :

```
curl -XPUT http://localhost:9200/twitter/tweet/_mapping -d @tweet.json
```

with tweet.json content

``` javascript
/* Test comment */
{
  "tweet":{
    "properties":{
      "user":{
        "type":"string" // This a comment on field
      }
    }
  }
}
```

You get an error like :

```
{"error":"MapperParsingException[Failed to parse mapping definition]; nested: JsonParseException[Unexpected character ('/' (code 47)): maybe a (non-standard) comment? (not recognized as one since Feature 'ALLOW_COMMENTS' not enabled for parser)\n at [Source: /* Test comment */{  \"tweet\":{    \"properties\":{      \"user\":{        \"type\":\"string\"      }    }  }}; line: 1, column: 2]]; ","status":400}
```

I think it's only need to set Jackson property as follow

``` java
objectMapper.configure(JsonParser.Feature.ALLOW_COMMENTS, true);
```

Could it have any side effects ?

Thanks.
David.
</description><key id="1899908">1394</key><summary>Comments are not allowed in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2011-10-13T13:00:14Z</created><updated>2016-08-30T09:35:18Z</updated><resolved>2013-01-07T03:21:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-14T16:14:55Z" id="2408419">That won't work, we might be able to parse it with comments (thanks to jackson, its not a valid json), but, the way elasticsearch works is that it parses the mappings into its own structure, and then serialize it based on it back to the json string representation (thus causing comments to be lost...).
</comment><comment author="karussell" created="2011-10-14T18:14:35Z" id="2409629">or probably skip lines beginning with # before parsing?
</comment><comment author="kimchy" created="2011-10-15T13:38:59Z" id="2415503">@karussell the problem is not with parsing, as that can be handled, its with how elasticsearch handles mapping. The json provided gets parsed into elasticsearch own mapping model, and then serialized back into the json (to form a consistent form of the mapping json, and only output whats applicable). There is no handling for comments in the mapping data structures so they can be serialized back.
</comment><comment author="dadoonet" created="2011-10-17T07:47:23Z" id="2425870">@kimchy : I don't need to get comments back from ES. It's okay to loose it.
I just need to be able to manage my mapping definition files in my VCS and I would like to share comments with other developpers.
</comment><comment author="kimchy" created="2011-10-18T21:05:32Z" id="2447808">I see, I can enable comments in json parsing, just want to check the parsing overhead costs...
</comment><comment author="dadoonet" created="2011-10-19T06:00:21Z" id="2452354">Nice. I can understand that you can not allow comments only for _mapping endpoint, so it can have side effects on other endpoints.
So, if there is any cost, as it's only a request feature for "confort" and it's not standard JSON, we can forget it !!!
</comment><comment author="apatrida" created="2011-12-22T00:55:38Z" id="3242100">I have similar issue with a really long mapping JSON file and without comments it is evil to manage.  Having them stripped on return is no problem, it is the source material that requires them, not echo'ing or retrieval of the result
</comment><comment author="Mpdreamz" created="2012-04-02T13:10:25Z" id="4878004">+1 for enabling comments. I am also ok with not getting them back from the _mapping endpoint.
</comment><comment author="kkrauth" created="2012-04-22T21:23:40Z" id="5271370">Same here. My mapping file is heavily commented so that I remember why certain fields are configured the way they are. Also no need to have the comments preserved in the ES internal representation as I assume most people would have the original mapping stored in a VCS.

For testing purposes, I use the following (using Jackson) to strip out comments from my config and apply it to ES

``` java
JsonFactory jf = new JsonFactory();
jf.enable(JsonParser.Feature.ALLOW_COMMENTS);
ObjectMapper om = new ObjectMapper(jf);
JsonNode root = om.readTree("{/*Some comment*/\"field\":\"value\"}");
String noComments = om.writeValueAsString(root);
assertEquals("{\"field\":\"value\"}", noComments);
```

Which gives me an idea, if enabling `JsonParser.Feature.ALLOW_COMMENTS` globally could incur a penalty overhead, why not enable it for mappings only? I would imagine mappings don't get changed often, and it seems that comments are most useful precisely in that scenario.
</comment><comment author="sppascha" created="2012-11-29T14:51:56Z" id="10849836">Putting in my +1. Mappings get lengthy and complicated pretty quickly (and often involve a lot of little compromises whose rationalizations are quickly lost to time). It would be very helpful to be able to annotate them inline.
</comment><comment author="simplechris" created="2013-01-05T13:04:52Z" id="11913742">+1 - It'd be really helpful to be able to annotate my mapping files inline.
</comment><comment author="lukas-vlcek" created="2013-01-20T15:08:54Z" id="12471814">Found an interesting article discussing comments in JSON. It might be useful to keep the link here.
http://bolinfest.com/essays/json.html
</comment><comment author="Hubbitus" created="2015-07-29T18:09:22Z" id="126044718">+1
</comment><comment author="knoxxs" created="2015-09-10T08:45:06Z" id="139169940">+1
</comment><comment author="sgeeroms" created="2016-08-30T09:35:18Z" id="243385922">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file></files><comments><comment>Comments are not allowed in mapping</comment><comment>checked jackson, there won't be an overhead in enabling comments. Added, with the caveat that when used with mappings, and calling "get mapping", the comments will not be returned</comment><comment>closes #1394</comment></comments></commit></commits></item><item><title>Twitter4j needs an update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1393</link><project id="" key="" /><description>Twitter updated its streaming endpoints. So twitter4j needs an update to 2.2.5-SNAPSHOT or change the endpoints by hand.

http://groups.google.com/group/twitter4j/browse_thread/thread/e6b5ca7c96dd1022
</description><key id="1897857">1393</key><summary>Twitter4j needs an update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels><label>enhancement</label><label>v0.17.9</label><label>v0.18.0</label></labels><created>2011-10-13T07:31:55Z</created><updated>2011-10-14T15:47:44Z</updated><resolved>2011-10-14T15:47:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pmalves" created="2011-10-13T15:32:00Z" id="2396030">Here's the pull request on the fix. I didn't find a repository for 2.2.5, so I'm putting the definitions to 2.2.4 and replacing the jar with the twitter-snapshot. Works now and should be changed to 2.2.5 when available

https://github.com/elasticsearch/elasticsearch/pull/1395
</comment><comment author="karussell" created="2011-10-13T19:09:17Z" id="2398780">2.2.5 SNAPSHOT is here http://twitter4j.org/maven2
</comment><comment author="kimchy" created="2011-10-14T15:47:43Z" id="2408131">Updated to 2.2.5 snapshot, once its out, will update to the formal version.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Synonym filter fails to properly load synonyms (since 0.17.7)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1392</link><project id="" key="" /><description>The upgrade to Lucene 3.4 caused the synonym filter to break and no longer load the synonym file properly.
</description><key id="1894183">1392</key><summary>Synonym filter fails to properly load synonyms (since 0.17.7)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.9</label><label>v0.18.0</label></labels><created>2011-10-12T22:17:31Z</created><updated>2011-10-12T22:18:18Z</updated><resolved>2011-10-12T22:18:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file></files><comments><comment>Synonym filter fails to properly load synonyms (since 0.17.7), closes #1392.</comment></comments></commit></commits></item><item><title>When searching against an index/type, use the type information to derive different search aspects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1391</link><project id="" key="" /><description>- Use the type provided to derive the field mapping for that type
- Searching against a type should automatically use the search_analyzer associated with it (if configured)
</description><key id="1893433">1391</key><summary>When searching against an index/type, use the type information to derive different search aspects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-12T20:53:34Z</created><updated>2011-10-17T22:38:40Z</updated><resolved>2011-10-17T22:38:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file></files><comments><comment>When searching against an index/type, use the type information to derive different search aspects, closes #1391.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/TextQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/CountDateHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/DateHistogramFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/ValueDateHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/ValueScriptDateHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/ValueGeoDistanceFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/BoundedCountHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/BoundedValueHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/BoundedValueScriptHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/CountHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/FullHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/ValueHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/ValueScriptHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/KeyValueRangeFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/statistical/StatisticalFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/statistical/StatisticalFieldsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/TermsByteFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/TermsByteOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/TermsFloatFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/TermsFloatOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/TermsIntFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/TermsIntOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/TermsIpFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/TermsIpOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/TermsShortFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/TermsShortOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/FieldsTermsStringFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/termsstats/TermsStatsFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/termsstats/doubles/TermsStatsDoubleFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/termsstats/longs/TermsStatsLongFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/termsstats/strings/TermsStatsStringFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file></files><comments><comment>When searching against an index/type, use the type information to derive different search aspects, closes #1391.</comment></comments></commit></commits></item><item><title>Text Query does not use search_analyzer on field that is prefixed with the type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1390</link><project id="" key="" /><description /><key id="1892300">1390</key><summary>Text Query does not use search_analyzer on field that is prefixed with the type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.0</label></labels><created>2011-10-12T20:13:42Z</created><updated>2011-10-12T20:50:29Z</updated><resolved>2011-10-12T20:50:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/TextQueryParser.java</file></files><comments><comment>Text Query does not use search_analyzer on field that is prefixed with the type name, closes #1390.</comment></comments></commit></commits></item><item><title>Root mapper analyzer/index_analyzer/search_analyzer with unregistered analyzer name won't fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1389</link><project id="" key="" /><description /><key id="1891185">1389</key><summary>Root mapper analyzer/index_analyzer/search_analyzer with unregistered analyzer name won't fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.0</label></labels><created>2011-10-12T19:30:12Z</created><updated>2011-10-12T19:41:43Z</updated><resolved>2011-10-12T19:41:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file></files><comments><comment>Root mapper analyzer/index_analyzer/search_analyzer with unregistered analyzer name won't fail, closes #1389.</comment></comments></commit></commits></item><item><title>Search / Get Preference: Add _only_node:[node_id] option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1388</link><project id="" key="" /><description>Add an option to restrict search only to be executed on shards allocated on a specific node (and if no relevant shards are allocated on the node, then the search will not be executed). For example, for a node with id `xyz`, the preference can be set to `_only_node:xyz`.
</description><key id="1890844">1388</key><summary>Search / Get Preference: Add _only_node:[node_id] option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-12T18:56:08Z</created><updated>2011-10-12T19:11:30Z</updated><resolved>2011-10-12T19:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file></files><comments><comment>Search / Get Preference: Add _only_node:[node_id] option, closes #1388.</comment></comments></commit></commits></item><item><title>Unclosed double quote causes error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1387</link><project id="" key="" /><description>When sending a query with an unclosed double quote, I get an error with a stack. Here's my log.

Caused by: org.apache.lucene.queryParser.ParseException: Cannot parse '"test': Lexical error at line 1, column 6.  Encountered: &lt;EOF&gt; after : "\"test"
    at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:216)
    at org.elasticsearch.index.query.QueryStringQueryParser.parse(QueryStringQueryParser.java:184)
    ... 38 more
Caused by: org.apache.lucene.queryParser.TokenMgrError: Lexical error at line 1, column 6.  Encountered: &lt;EOF&gt; after : "\"test"
    at org.apache.lucene.queryParser.QueryParserTokenManager.getNextToken(QueryParserTokenManager.java:1229)
    at org.apache.lucene.queryParser.QueryParser.jj_ntk(QueryParser.java:1748)
    at org.apache.lucene.queryParser.QueryParser.Modifiers(QueryParser.java:1192)
    at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:1236)
    at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:1226)
    at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:206)
    ... 39 more
</description><key id="1886891">1387</key><summary>Unclosed double quote causes error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fdv</reporter><labels /><created>2011-10-12T12:36:28Z</created><updated>2013-10-10T12:41:03Z</updated><resolved>2013-04-05T16:14:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-10-12T15:29:41Z" id="2380528">this is intented for e.g. query_string query. use a text query then this shouldn't be a problem.
</comment><comment author="nickhoffman" created="2011-10-16T20:27:24Z" id="2422854">Out of curiousity, why does ES fail on a double-quote that's escaped and unmatched?
</comment><comment author="karussell" created="2011-10-17T08:04:47Z" id="2426010">it's the lucene query parser which simply says you didn't expressed your query properly. its like your compiler complaining about a missing bracket
</comment><comment author="nickhoffman" created="2011-10-17T13:02:24Z" id="2428477">Hm, interesting. So is there a way to search for a double-quote using a query-string query?
</comment><comment author="karussell" created="2011-10-17T14:42:04Z" id="2429496">ah now I understand your usecase. when I'm doing a normal search you already need one escaping:

```
"query":"text:\"todo\""
```

so you'll need a double escaping to search e.g. for one quote

```
"query":"text:\\\""

where \\ creates one backslash ... not sure if there is an easier method
```
</comment><comment author="nickhoffman" created="2011-10-17T14:45:15Z" id="2429525">Makes sense. Thanks, mate.
</comment><comment author="kbond" created="2013-02-19T16:37:12Z" id="13782364">I am having this issue with users having the inch symbol (`"`) in their query - ie `19" rails`.  My quick and dirty solution was just to remove the quotes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix synonym handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1386</link><project id="" key="" /><description>Here's a quick fix for synonym handling. Not too pretty, feel free to hack up to your standards :-)
</description><key id="1885106">1386</key><summary>Fix synonym handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2011-10-12T07:44:52Z</created><updated>2014-07-16T21:56:04Z</updated><resolved>2011-10-12T22:25:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-12T22:18:21Z" id="2387751">The fix is actually simpler, see #1392, can you double check?
</comment><comment author="ppearcy" created="2011-10-12T22:25:29Z" id="2387798">Ah, yeah, that is much nicer :-)

Although, it does require the whole list to get pulled into memory twice, but that shouldn't matter. 

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Proposal for a new Super Hero ;-)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1385</link><project id="" key="" /><description>Just for fun ! ;-)
</description><key id="1867826">1385</key><summary>Proposal for a new Super Hero ;-)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-10-10T20:28:47Z</created><updated>2014-06-17T09:06:17Z</updated><resolved>2013-04-05T16:14:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-14T16:13:36Z" id="2408407">Ha! :)
</comment><comment author="dadoonet" created="2011-10-20T20:20:32Z" id="2473822">Thanks Peter... It's done ;-)
</comment><comment author="dadoonet" created="2011-11-28T22:48:48Z" id="2908818">So Shay ??? Are you going to accept this one before the next release ??? ;-)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested docs and the _boost field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1384</link><project id="" key="" /><description>A query run only on nested docs fails to take the value of the  `_boost` field into account - you have to run a query on the parent doc for it to be included.

Also, it should be possible to have a separate `_boost` field specified on a nested doc.
</description><key id="1852615">1384</key><summary>Nested docs and the _boost field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-10-08T15:03:50Z</created><updated>2014-07-03T19:15:24Z</updated><resolved>2014-07-03T19:15:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="faliev" created="2012-09-16T21:34:29Z" id="8599907">Is it possible to have a separate _boost field on nested docs?
</comment><comment author="clintongormley" created="2014-07-03T19:15:24Z" id="47972729">The _boost field is deprecated. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose matching nested docs to the query and sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1383</link><project id="" key="" /><description>Hiya

I have a use case for nested docs which, currently, can not be supported easily.

For instance, have a look at this doc:

```
{
  "location" : { "lat" : "8.31667", "lon" : "52.38333" },
  "parent_ids" : [ "2862926", "2921044" ],
  "contexts" : [ {
        "context" : "/en",
        "rank" : 2,
        "tokens" : [ 
            { "tokens" : [ "steinbrink" ] },
            { "tokens" : [ "steinbrink", "germany" ] }
        ],
        "label" : {
           "short" : "Steinbrink",
           "long" : "Steinbrink, Germany"
        }
     },
     {
        "context" : "/de",
        "rank" : 3,
        "tokens" : [ 
            { "tokens" : [ "steinbrink" ] },
            { "tokens" : [ "steinbrink", "deutschland" ] }
        ],
        "label" : {
           "short" : "Steinbrink",
           "long" : "Steinbrink, Deutschland"
        }
     }
  ]
}
```

`contexts` and `tokens` are both nested docs.

I would like to run query like:
- filter by `contexts.context == '/de'`
- sort by:
  - `contexts.context.rank desc`
  - `contexts.label.short asc`

but of course I have no way of pulling out the single matching context and using the values in that sub-doc.

I could use a script sort like the horribly inefficient:

```
{
   "sort" : [
      {
         "params" : {
            "name" : "/de"
         },
         "_script" : {
            "script" : "for (context: contexts) { if (context.context == name) { return context.rank }}",
            "order" : "desc",
            "type" : "number"
         }
      },
      {
         "_script" : {
            "params" : {
               "name" : "/de"
            },
            "script" : "for (context: contexts) { if (context.context == name) { return context.label.short }}",
            "order" : "asc",
            "type" : "string"
         }
      }
   ]
}
```

Or I may want to do:
- filter by `context=='/de'`
- find doc with tokens that best match `stein*`
- return the `tokens` that match best as a field

Or have a slightly different doc structure:

```
"tokens" : [ 
    { 
        "label": "Steinbrink",
        "tokens" : [ "steinbrink" ] 
    },
    { 
        "label": "Steinbrink, Germany",
        "tokens" : [ "steinbrink, germany" ] 
    },
],
```

and return the `label` associated with the `tokens` that best match. 

Even with a script, I couldn't find the best matching `tokens`.

In this example, I only have two contexts, and so could have changed the doc structure to something like:

```
contexts: {
  en: {....},
  de: {....}
}
```

But I have other use cases where I have hundreds of contexts, which would result in thousands of fields.

Would it be possible to add functionality to:
- expose the best matching child (or children, with the best matching first in an array) in a `_scope` so we could do something like:
  
  "sort": [
      { "_child.myscope[0].rank": "desc" }, 
      { "_child.myscope[0].label.short": "asc"}
  ],
  "fields": [ "_child.otherscope[0],tokens.label" ]
- return the scores for matching children in a `_scope` eg:
  
  { "children": 
      {
           "myscope": [5,3,0,2,0],
           "otherscope": [0,1,0]
      }
  }

Not sure how feasible this is, and if it is feasible, there is probably a much more elegant way of doing it.

thanks

clint
</description><key id="1851935">1383</key><summary>Expose matching nested docs to the query and sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-10-08T10:43:58Z</created><updated>2013-05-30T17:01:37Z</updated><resolved>2013-05-10T15:06:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2011-11-08T11:01:45Z" id="2666220">+1 for this.

We're having to do additional post processing on the result set to try and "guess" which of the nested documents matched so we can show relevant matches to our users. Being able to return just the matched nested documents (and be able to sort on them) would be a massive win.

Thanks,
Paul.
</comment><comment author="bryangreen" created="2011-12-20T02:12:45Z" id="3213188">+1
</comment><comment author="junckritter" created="2012-05-13T10:36:11Z" id="5675371">+1
</comment><comment author="jove4015" created="2012-05-14T03:19:34Z" id="5682021">+1
</comment><comment author="jnarowski" created="2012-05-19T01:25:24Z" id="5798693">+1
</comment><comment author="chrisyuska" created="2012-09-18T23:29:38Z" id="8674419">+1 as well (assuming this hasn't been added yet)
</comment><comment author="fbpj" created="2012-11-15T10:45:50Z" id="10404458">+1, any news about this ?
</comment><comment author="gebrits" created="2012-12-20T16:23:16Z" id="11579772">+1 opens up a lot of use-cases. A big general one: given a product / product-variant mapping -&gt; return only the matched product-variant
</comment><comment author="gebrits" created="2012-12-20T16:27:11Z" id="11579923">As for a more concrete example: the following (pretty general imho) usecase would be possible. 

http://stackoverflow.com/questions/13974277/modeling-parent-child-relationships-product-productvariant-in-elasticsearch
</comment><comment author="jannikks" created="2013-01-08T14:33:18Z" id="11999108">+1
</comment><comment author="bbansal" created="2013-02-22T22:34:49Z" id="13975742">+1
</comment><comment author="tommymonk" created="2013-05-10T13:55:06Z" id="17721392">+1
</comment><comment author="martijnvg" created="2013-05-10T15:06:46Z" id="17725564">I'm closing this one. Nested sorting has been implemented and I created an issue for retrieving the matching nested inner objects per hit #3022 
</comment><comment author="evandu" created="2013-05-30T15:16:30Z" id="18686978">+1 for this. 
How to filter nested array in response? 
</comment><comment author="chrisyuska" created="2013-05-30T17:01:37Z" id="18694079">@evandu, if you're looking for nested filtering, read this: http://www.elasticsearch.org/guide/reference/query-dsl/nested-filter/

If you're looking for nested sorting, read this: http://www.elasticsearch.org/guide/reference/api/search/sort/ (nested sorting is down the page a little bit)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Script sorting doesn't accept parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1382</link><project id="" key="" /><description>Hiya

There is a bug when providing params to a script sort.  Depending on the position of the `params` parameter in the JSON, you get 2 different error messages:

```
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1' 
curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "text" : "foo"
}
'
```
## Parse Failure [_script sorting requires setting the script to sort by]]; }]

```
curl -XGET 'http://127.0.0.1:9200/foo/_search?pretty=1'  -d '
{
   "sort" : {
      "_script" : {
         "params" : {
            "one" : 1
         },
         "script" : "one",
         "type" : "number"
      }
   }
}
'
```
## PropertyAccessException[[Error: unresolvable property or identifier: one]

```
curl -XGET 'http://127.0.0.1:9200/foo/_search?pretty=1'  -d '
{
   "sort" : {
      "_script" : {
         "script" : "one",
         "type" : "number",
         "params" : {
            "one" : 1
         }
      }
   }
}
'
```
</description><key id="1851828">1382</key><summary>Script sorting doesn't accept parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.17.9</label><label>v0.18.0</label></labels><created>2011-10-08T09:59:36Z</created><updated>2011-10-18T20:31:30Z</updated><resolved>2011-10-18T20:31:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file></files><comments><comment>Script sorting doesn't accept parameters, closes #1382.</comment></comments></commit></commits></item><item><title>Fix NPE in HighlightField serialization. Fixes #1380</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1381</link><project id="" key="" /><description>See #1380 for more information
</description><key id="1850303">1381</key><summary>Fix NPE in HighlightField serialization. Fixes #1380</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-10-07T23:25:05Z</created><updated>2014-07-16T21:56:06Z</updated><resolved>2011-10-08T18:56:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-08T18:56:17Z" id="2333286">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting fails with NPE for multifield and number_of_fragments:0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1380</link><project id="" key="" /><description>To reproduce, start two ES nodes and run the following script:

```
curl -XDELETE http://localhost:9200/testidx
curl -XPUT http://localhost:9200/testidx -d '{
    "settings" : {
        "index" : {
            "number_of_shards" : 1,
            "number_of_replicas" : 0
        }
    },
    "mappings" : {
        "rec" : {
            "_source" : { "enabled" : false },
            "properties" : {
            "from" : { "type": "string", "store": "yes" }
            }
        }
    }
}'
curl -XPUT http://localhost:9200/testidx/rec/1 -d '{
    "from" : ["user3@test.com","user2@test.com","user5@test.com"]
}'
echo
curl -XPOST http://localhost:9200/testidx/_refresh
echo
curl localhost:9200/testidx/_search -d '{
    "highlight": {
         "fields":{"from":{"number_of_fragments":0}}
    },
    "fields":["*"],
    "size":10,
    "sort":["_score"],
    "query":{
        "query_string":{
            "default_field":"from",
            "query":"*:*"
         }
    }
}' &amp;&amp; echo
curl localhost:9201/testidx/_search -d '{
    "highlight": {
         "fields":{"from":{"number_of_fragments":0}}
    },
    "fields":["*"],
    "size":10,
    "sort":["_score"],
    "query":{
        "query_string":{
            "default_field":"from",
            "query":"*:*"
         }
    }
}' &amp;&amp; echo
```

One of the search requests fails and the node where shard is allocated throws the following exception:

```
[2011-10-07 11:16:57,872][DEBUG][action.search.type       ] [Maha Yogi] [testidx][0], node[czKz17uBQiaTczE_OzQ0GA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@43233ac]
org.elasticsearch.transport.RemoteTransportException: [Mikhail Rasputin][inet[/10.0.1.8:9300]][search/phase/query+fetch]
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.io.stream.HandlesStreamOutput.writeUTF(HandlesStreamOutput.java:54)
    at org.elasticsearch.search.highlight.HighlightField.writeTo(HighlightField.java:110)
    at org.elasticsearch.search.internal.InternalSearchHit.writeTo(InternalSearchHit.java:574)
    at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:246)
    at org.elasticsearch.search.fetch.FetchSearchResult.writeTo(FetchSearchResult.java:101)
    at org.elasticsearch.search.fetch.QueryFetchSearchResult.writeTo(QueryFetchSearchResult.java:90)
    at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:136)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:74)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:66)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:502)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:492)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:238)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
```
</description><key id="1845043">1380</key><summary>Highlighting fails with NPE for multifield and number_of_fragments:0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.18.0</label></labels><created>2011-10-07T15:43:40Z</created><updated>2011-10-08T18:55:53Z</updated><resolved>2011-10-08T18:55:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file></files><comments><comment>Fix NPE in HighlightField serialization. Fixes #1380</comment></comments></commit></commits></item><item><title>Authentication &amp; Authorization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1379</link><project id="" key="" /><description>There are several points regarding security:
1. make sure that one can authenticate into ElasticSearch
2. make sure that sensitive data is encrypted when sending over the network and also between nodes
3. make sure that an authenticated user can see and change only his own 'things' (indices, data, node info)

Point 1 and 2 are already requested in issue #664. What I'm after is point 3. I wanted to ask you how you would implement point 1 and 3 (point 2 can be handled by someone else ;))

I've thought one could simply store user and password (as updateable settings) while creating an index. And when searching or indexing one needs to provide the user and pw. To keep it simple there is only one admin user which has access to the node and cluster health information etc. All other users are normal user and can only perform "CRUD" actions for indices and its data.

Now my problem is that when I intercept every request to authenticate &amp; authorize I would have to touch over 10 Request classes implementing ActionRequest.validate() for the transport client. Also there are no settings stored for those Requests.

Or how would you implement this?

And then for the rest client it looks a bit simpler because the settings are already in the request and I could then change the BaseRestHandler only to implement a validation within handleRequest. Is this correct?

Or is there a simpler or more powerful scenarios to implement my feature requests?
</description><key id="1843139">1379</key><summary>Authentication &amp; Authorization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-10-07T12:50:50Z</created><updated>2013-06-12T15:58:41Z</updated><resolved>2013-04-05T16:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="starfishmod" created="2011-10-27T12:14:17Z" id="2542614">+1 for this. Would great to be able to configure the access to various API's on a per index level
</comment><comment author="bryangreen" created="2011-12-20T02:16:56Z" id="3213234">+1
</comment><comment author="asafdav2" created="2012-02-22T20:27:33Z" id="4121197">+1
</comment><comment author="tarunjangra" created="2012-04-01T00:34:51Z" id="4863015">Absolutely +1. Really waiting for these features. Do we have any progresses on any of above?
</comment><comment author="karussell" created="2012-04-01T12:36:00Z" id="4866067">see https://github.com/sonian/elasticsearch-jetty
</comment><comment author="tarunjangra" created="2012-04-01T17:08:21Z" id="4867733">@karussell: Cool feature. Thank you for that.
</comment><comment author="pulkitsinghal" created="2012-05-30T11:54:01Z" id="6007817">For those using the jetty plugin:
https://github.com/sonian/elasticsearch-jetty 

You can also utilize the Chef cookbook to speed-up your AWS deployments:
https://github.com/pulkitsinghal/cookbook-elasticsearch
</comment><comment author="pannous" created="2012-09-25T09:41:47Z" id="8848682">+1
</comment><comment author="thejohnfreeman" created="2013-06-12T15:58:41Z" id="19335629">OP's needs may be addressed, but the general issue remains. The Java API communicates through the transport module which I'm guessing implements some custom protocol, unencrypted, over TCP.

Is there a setting to disallow Java API clients? That is, can a node disallow TCP connections from outside the cluster?

Is there a setting to encrypt network traffic among the cluster?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Return scores for matching nested docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1378</link><project id="" key="" /><description>I'm trying to do geolocation auto-complete. Each location can have multiple names associated with it, eg "London", "City of London", "London, United Kingdom", "Londres, Reino Unido" etc

I am storing these name variants as nested objects, but in order to display the correct name, I need to know which nested child matched (or the best matching child).

Consider this document:

```
{
    "location" : {
       "lat" : "12.88333",
       "lon" : "51.86667"
    },
    "rank" : 1
    "parent_ids" : ["2842565","2921044"],
    "langs": {
        "de": [
            { 
                "label": "Schadewalde", 
                "tokens": ["schadewalde"]
            },
            { 
                "label": "Schadewalde, Deutschland", 
                "tokens": ["schadewalde","deutschland"]
            },            
        ]

    }
}
```

If the user types in `schad` then I'd like to display `Schadewalde` but if they type in `schadewalde d` then I'd like to display `Schadewalde, Deutschland`.

Would there be any way of returning a score for each matching nested doc?

ta
</description><key id="1840814">1378</key><summary>Return scores for matching nested docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-10-07T06:24:47Z</created><updated>2014-07-03T19:14:52Z</updated><resolved>2014-07-03T19:14:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T19:14:52Z" id="47972686">Closing in favour of #3022 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>count doesn't work for empty query (discrepancy with search API)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1377</link><project id="" key="" /><description>The count API by its nature is very similar to search API, supposedly taking the 'query' part of a query and returning only the count of results (without the actual results). This works as expected in all cases except the empty query.

```
$ curl -XGET 'http://localhost:9200/gallery/item/_search?q=test'
... "hits":{"total":31 ...

$ curl -XGET 'http://localhost:9200/gallery/item/_count?q=test'
{"count":31 ...

$ curl -XGET 'http://localhost:9200/gallery/item/_search'
... "hits":{"total":11444 ...

$ curl -XGET 'http://localhost:9200/gallery/item/_count'
{"error":"No query to execute, not in body, and not bounded to 'q' parameter"}
```

I believe this discrepancy between search and count APIs needs to be fixed, and count for empty query should return the total count of the documents, like search already does.
</description><key id="1836488">1377</key><summary>count doesn't work for empty query (discrepancy with search API)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IlyaSemenov</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-06T18:34:48Z</created><updated>2011-10-06T19:57:37Z</updated><resolved>2011-10-06T19:57:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-06T19:37:16Z" id="2314070">Agreed, will fix it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/DocumentActionsTests.java</file></files><comments><comment>count doesn't work for empty query (discrepancy with search API), closes #1377.</comment></comments></commit></commits></item><item><title>Linux startup scripts fail to work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1376</link><project id="" key="" /><description>The scripts in master are saved in dos format.
They should be changed to unix format.
</description><key id="1825925">1376</key><summary>Linux startup scripts fail to work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haoniukun</reporter><labels /><created>2011-10-05T17:19:28Z</created><updated>2013-03-03T20:02:23Z</updated><resolved>2013-03-03T20:02:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-05T18:38:17Z" id="2301497">Is this the same person that asked the question on the mailing list? If so, I answered it there.
</comment><comment author="haoniukun" created="2011-10-05T18:51:48Z" id="2301667">:)
Thanks.

On Wed, Oct 5, 2011 at 11:38 AM, Shay Banon
reply@reply.github.com
wrote:

&gt; Is this the same person that asked the question on the mailing list? If so, I answered it there.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1376#issuecomment-2301497

## 

         牛坤
MSN:haoniukun@hotmail.com
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>REST Bulk API: Allow to execute _bulk against /{index}/_bulk and /{index}/{type}/_bulk endpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1375</link><project id="" key="" /><description>Allow to index data not just using `/_bulk`, but also against `/{index}/_bulk`, and `{index}/{type}/_bulk`, removing the need to repeat the index name and type for each bulk item action. The index and type can still be provided in each element, in which case they will override the URI based index / type.
</description><key id="1822600">1375</key><summary>REST Bulk API: Allow to execute _bulk against /{index}/_bulk and /{index}/{type}/_bulk endpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-05T11:02:50Z</created><updated>2011-10-05T11:03:44Z</updated><resolved>2011-10-05T11:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/bulk/BulkRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/action/bulk/BulkActionTests.java</file></files><comments><comment>REST Bulk API: Allow to execute _bulk against /{index}/_bulk and /{index}/{type}/_bulk endpoints, closes #1375.</comment></comments></commit></commits></item><item><title>Allow to specify a specific field in the clear cache API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1374</link><project id="" key="" /><description>This could be done by providing one or several field names instead of 'true' for the field_data parameter:

curl -XPOST 'http://localhost:9200/index/_cache/clear?fields=field1'

curl -XPOST 'http://localhost:9200/index/_cache/clear?fields=field1,field2'
</description><key id="1801171">1374</key><summary>Allow to specify a specific field in the clear cache API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">straux</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-03T09:51:27Z</created><updated>2011-10-03T10:13:16Z</updated><resolved>2011-10-03T10:13:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/field/data/FieldDataCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/field/data/none/NoneFieldDataCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/field/data/support/AbstractConcurrentMapFieldDataCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file></files><comments><comment>Allow to specify a specific field in the clear cache API, closes #1374.</comment></comments></commit></commits></item><item><title>Allow limiting the number of concurrent ES nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1373</link><project id="" key="" /><description>In many deployment scenarios it might be useful to limit the number of ES nodes that can run simultaneously on the same machine. For example, if a machine was configured to run a single ES node, the consequences of accidental start of the second ES node can be quite severe.

Usage: Adding the following line to elasticsearch.yml file will not allow more then one ES node to be started using the same data directory.

```node.max_local_storage_nodes: 1 

```
```
</description><key id="1799512">1373</key><summary>Allow limiting the number of concurrent ES nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-10-03T02:53:30Z</created><updated>2014-07-02T20:46:18Z</updated><resolved>2011-10-03T10:30:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-10-03T10:30:29Z" id="2270642">Make sense, pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Apache Tika 0.10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1372</link><project id="" key="" /><description>Upgrade Apache Tika in mapper-attachments plugin
http://www.apache.org/dist/tika/CHANGES-0.10.txt
</description><key id="1794657">1372</key><summary>Upgrade to Apache Tika 0.10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-10-02T07:12:04Z</created><updated>2011-10-05T13:22:08Z</updated><resolved>2011-10-05T13:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade to Apache Tika 0.10, closes #1372.</comment></comments></commit></commits></item><item><title>same attribute name with different mapping kind in different types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1371</link><project id="" key="" /><description>I'm creating a schema with those two types in it

``` javascript
{
   "albums": {
     "_source": { "enabled" : true },
     "properties": {
       "location": {"type": "geo_point"},
       "name": { "type": "string"}
     }
   }
}
```

``` javascript
{
 "users": {
   "_source" : { "enabled" : false },
   "properties" : {
     "name" : { "type": "string"},
     "location": {"type": "string"}
   }    
 }
}
```

It looks it's a very bad idea to have the same attribute (here 'location') with two different mapping kinds ('string' in the 'users' type, 'geopoint' in the 'albums' type).  It took me a long time to figure what the problem was because it perfectly works until we attempt to search with a `geo_bounding_box` filter.  
It should also be note, that no error pops if the type "users" stays empty.  Or, more precisely, as long as we don't push a user with a 'location' attribute (being a string).
If we swap the order of put mapping for the two different types, there is another behavior: it looks as if the 'albums' mapping did not have a field of kind 'geo_point'.  (the search query complains with `field [location] is not a geo_point field`)
Whatever the order of the put mapping is, if the do a GET mapping on the whole index, we get what we put (making the problem harder to diagnose)

Here is the complete script that reproduce the problem:

``` sh
#!/bin/sh

curl -XDELETE 'http://localhost:9200/foo'
echo 
curl -XPOST 'http://localhost:9200/foo' -d '{
  "settings" : {
      "number_of_shards" : 1
  }
 }'

# if we swap the two mapping update, the error changes at the end.
 echo
 curl -XPUT 'http://localhost:9200/foo/albums/_mapping' -d '
 {
   "albums": {
     "_source": { "enabled" : true },
     "properties": {
       "location": {"type": "geo_point"},
       "name": { "type": "string"}
     }
   }
 }
 '
echo
curl -XPUT 'http://localhost:9200/foo/users/_mapping' -d '
{
 "users": {
   "_source" : { "enabled" : false },
   "properties" : {
     "name" : { "type": "string"},
     "location": {"type": "string"}
   }    
 }
}
'

echo

# comment this and the error is gone...
curl -XPUT 'http://localhost:9200/foo/users/1?refresh=true' -d '{
 "name": "yo",
 "location": "Paris"
}'

echo

curl -XPUT 'http://localhost:9200/foo/albums/2?refresh=true' -d '{
   "name": "hi",
   "location": {"lat":42.3,"lon":2.1}
}'

echo
# echo sleeping
# sleep 3

echo

# This search query will generate an error:
# 
curl -XPOST 'http://localhost:9200/foo/albums/_search?pretty' -d '{
  "query": {
    "filtered": {
      "filter": {
        "geo_bounding_box": {
          "location": {
            "bottom_right": {
              "lon": 5,
              "lat": 30.1791692272601
            },
            "top_left": {
              "lon": -1.982220703125,
              "lat": 50.2658676477653
            }
          }
        }
            },
      "query": {
        "match_all": {
        }
      }
    }
  }
}
'
```

Here are the exceptions being logued

```
2011-09-29_12:50:02.29449 org.elasticsearch.search.query.QueryPhaseExecutionException: [foo][0]: query[ConstantScore(org.elasticsearch.index.search.geo.GeoBoundingBoxFilter@4136f531)],from[0],size[10]: Query Failed [Failed to execute main query]
2011-09-29_12:50:02.29449   at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:221)
2011-09-29_12:50:02.29450   at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:295)
2011-09-29_12:50:02.29450   at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:224)
2011-09-29_12:50:02.29451   at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:71)
2011-09-29_12:50:02.29451   at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:204)
2011-09-29_12:50:02.29452   at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:191)
2011-09-29_12:50:02.29453   at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:177)
2011-09-29_12:50:02.29453   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
2011-09-29_12:50:02.29454   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
2011-09-29_12:50:02.29454   at java.lang.Thread.run(Thread.java:636)
2011-09-29_12:50:02.29454 Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
2011-09-29_12:50:02.29455   at java.lang.String.substring(String.java:1949)
2011-09-29_12:50:02.29455   at org.elasticsearch.index.mapper.geo.GeoPointFieldData$StringTypeLoader.collectTerm(GeoPointFieldData.java:169)
2011-09-29_12:50:02.29456   at org.elasticsearch.index.field.data.support.FieldDataLoader.load(FieldDataLoader.java:54)
2011-09-29_12:50:02.29456   at org.elasticsearch.index.mapper.geo.GeoPointFieldData.load(GeoPointFieldData.java:152)
2011-09-29_12:50:02.29456   at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:51)
2011-09-29_12:50:02.29457   at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:34)
2011-09-29_12:50:02.29458   at org.elasticsearch.index.field.data.FieldData.load(FieldData.java:110)
2011-09-29_12:50:02.29458   at org.elasticsearch.index.cache.field.data.support.AbstractConcurrentMapFieldDataCache.cache(AbstractConcurrentMapFieldDataCache.java:112)
2011-09-29_12:50:02.29458   at org.elasticsearch.index.search.geo.GeoBoundingBoxFilter.getDocIdSet(GeoBoundingBoxFilter.java:65)
2011-09-29_12:50:02.29459   at org.apache.lucene.search.DeletionAwareConstantScoreQuery$DeletionConstantWeight.scorer(DeletionAwareConstantScoreQuery.java:53)
2011-09-29_12:50:02.29459   at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:116)
2011-09-29_12:50:02.29460   at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)
2011-09-29_12:50:02.29461   at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:198)
2011-09-29_12:50:02.29462   at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:391)
2011-09-29_12:50:02.29463   at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:298)
2011-09-29_12:50:02.29463   at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:286)
2011-09-29_12:50:02.29463   at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:217)
2011-09-29_12:50:02.29464   ... 9 more
```
</description><key id="1774579">1371</key><summary>same attribute name with different mapping kind in different types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fdejaeger</reporter><labels /><created>2011-09-29T12:52:05Z</created><updated>2013-04-05T16:16:00Z</updated><resolved>2013-04-05T16:16:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fdejaeger" created="2011-09-29T12:54:23Z" id="2237342">I forgot to say this happens with v0.17.7
</comment><comment author="clintongormley" created="2013-04-05T16:16:00Z" id="15965421">Closed in favor of #2860 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When refreshing, also execute the refresh operation on initializing shards to make sure we don't miss it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1370</link><project id="" key="" /><description /><key id="1755114">1370</key><summary>When refreshing, also execute the refresh operation on initializing shards to make sure we don't miss it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.8</label><label>v0.18.0</label></labels><created>2011-09-27T18:46:04Z</created><updated>2011-09-27T19:56:58Z</updated><resolved>2011-09-27T19:56:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file></files><comments><comment>When refreshing, also execute the refresh operation on initializing shards to make sure we don't miss it, closes #1370.</comment></comments></commit></commits></item><item><title>Fetch phase when searching might fail when mapping are updated with type missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1369</link><project id="" key="" /><description>Due to concurrent issue with replacing a type mapping, a fetch phase might not "see" an existing type mapping while it is being replaced.
</description><key id="1755096">1369</key><summary>Fetch phase when searching might fail when mapping are updated with type missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.8</label><label>v0.18.0</label></labels><created>2011-09-27T18:44:51Z</created><updated>2011-09-27T19:56:58Z</updated><resolved>2011-09-27T19:56:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file></files><comments><comment>Fetch phase when searching might fail when mapping are updated with type missing, closes #1369.</comment></comments></commit></commits></item><item><title>No need to reroute (check for possible shard allocations) when a new *non* data node is added to the cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1368</link><project id="" key="" /><description /><key id="1750926">1368</key><summary>No need to reroute (check for possible shard allocations) when a new *non* data node is added to the cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.8</label><label>v0.18.0</label></labels><created>2011-09-27T10:47:45Z</created><updated>2011-09-27T10:48:14Z</updated><resolved>2011-09-27T10:48:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file></files><comments><comment>No need to reroute (check for possible shard allocations) when a new *non* data node is added to the cluster, closes #1368.</comment></comments></commit></commits></item><item><title>Add an option to disallow deleting all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1367</link><project id="" key="" /><description>A setting called: `action.disable_delete_all_indices` (defaults to `false`) that can be set to `true` in order to disable deleting all indices in a single API call.

```
curl -X DELETE localhost:9200
```

It would, obviously, be a _very good idea_ to set this as `false` in a production  `elasticsearch.yml` :)

_EDIT: Updated wording_
</description><key id="1747241">1367</key><summary>Add an option to disallow deleting all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-09-26T22:45:14Z</created><updated>2013-12-20T15:19:51Z</updated><resolved>2011-09-27T08:54:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JustinTArthur" created="2013-05-30T23:04:21Z" id="18714089">You mean a good idea to set it to `true` in production, right?
</comment><comment author="Erni" created="2013-12-20T15:19:51Z" id="31016350">Why not add this option (and all the possible ones) in elasticsearch.yml? Even if they were commented by default, people could see that they are available.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file></files><comments><comment>Add an option to disallow deleting all indices, closes #1367.</comment></comments></commit></commits></item><item><title>Remove Infinity values for Range facets when no docs match the range</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1366</link><project id="" key="" /><description>Hi,

As discussed in the mailing list : http://groups.google.com/group/elasticsearch/browse_thread/thread/560c9b8954668931/d7b932e895bd7d4b

Just wondering if we must keep Infinity or not but just changing 

in https://github.com/elasticsearch/elasticsearch/blob/master/modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacet.java#L60

``` java
        double min = Double.POSITIVE_INFINITY;
        double max = Double.NEGATIVE_INFINITY;
```

to

``` java
        double min = Double.NEGATIVE_INFINITY;
        double max = Double.POSITIVE_INFINITY;
```
</description><key id="1746193">1366</key><summary>Remove Infinity values for Range facets when no docs match the range</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-09-26T20:59:39Z</created><updated>2011-09-26T21:42:09Z</updated><resolved>2011-09-26T21:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-26T21:15:31Z" id="2204049">That change won't help, you will simply see other "infinity values". In general, it can simply not output the min/max values if the count is 0.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/InternalRangeFacet.java</file></files><comments><comment>Remove Infinity values for Range facets when no docs match the range, closes #1366.</comment></comments></commit></commits></item><item><title>Add support for non-elasticsearch namespaces to Settings.getAsClass</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1365</link><project id="" key="" /><description>Elasticsearch provides great flexibility allowing to plug custom functionality and replace some built-in modules. However, if a plugin module is located in non org.elasticsearch namespace, it's kind of awkward to specify the plugin's module in the config file. 

For example, if I want to replace netty http transport with my own jetty-based implementation, I need to specify something like this in the config file:

`http.type: org.example.elasticsearch.http.jetty.JettyHttpServerTransportModule`

I would like to be able to simplify this to:

`http.type: org.example.elasticsearch.http.jetty`

It can be easily achieved by modifying `ImmutableSettings.getAsClass(String setting, Class&lt;? extends T&gt; defaultClazz, String prefixPackage, String suffixClassName)` logic a little bit. The current logic is 

1) try loading class specified in settings
2) try loading prefixPackage + Setting + suffixClassName
3) try loading prefixPackage + setting + "." + Setting + suffixClassName

I would like to add a check between 1 and 2 for the setting value. Currently it's assumed that the setting value is either resolved as a class name (1) or it doesn't contain any dots (2 and 3). I would like to add special logic that would check if a dot is present in the setting value. And if dot is present it would use the portion of the setting value before the last dot as a prefixPackage in 2) and 3). 

So, if I specify org.example.elasticsearch.http.jetty, I want it to try loading

1) org.example.elasticsearch.http.jetty
2) org.example.elasticsearch.http.JettyHttpServerTransportModule
3) org.example.elasticsearch.http.jetty.JettyHttpServerTransportModule

Since the setting value is capitalized, having a dot in the setting cannot produce an useful result right now. So the proposed solution is backward compatible with the current implementation and shouldn't break any existing functionality.

Does it make sense?
</description><key id="1745583">1365</key><summary>Add support for non-elasticsearch namespaces to Settings.getAsClass</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-09-26T19:48:27Z</created><updated>2014-06-18T17:10:50Z</updated><resolved>2011-09-26T20:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-26T20:51:03Z" id="2203736">Looks good, pushed. Btw, donno if jetty thingy is just an example or not, but, there is the transport wares plugin that integrates with web containers.
</comment><comment author="imotov" created="2011-09-27T14:55:53Z" id="2211733">Thanks! 

Yes, jetty plugin is more then just an example :-) I saw wares plugin. However, we didn't want to replace our existing ES infrastructure with jetty infrastructure. Moreover, repackaging ES into war is somewhat challenging task when you consider need for updatable config file and installation of ES plugins. I still cannot think of a good way to do it. 

So, we embedded jetty into ES instead. It gives us query logging, access control and SSL connector that we needed and cleanly installs as just another plugin. The rest of the system doesn't even have to know that something changed in ES. Thanks to wares servlet that I used as an example and extensible ES architecture it was pretty easy thing to do. I still need to work on a few configuration aspects and thoroughly test it. But, so far, it looks quite promising.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `cloud.node.auto_attributes` setting, when set to `true`, will automatically add aws ec2 related attributes to the node (like availability zone)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1364</link><project id="" key="" /><description /><key id="1742468">1364</key><summary>Add `cloud.node.auto_attributes` setting, when set to `true`, will automatically add aws ec2 related attributes to the node (like availability zone)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-26T18:01:07Z</created><updated>2011-09-26T18:01:36Z</updated><resolved>2011-09-26T18:01:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/node/Ec2CustomNodeAttributes.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java</file></files><comments><comment>Add `cloud.node.auto_attributes` setting, when set to `true`, will automatically add aws ec2 related attributes to the node (like availability zone), closes #1364.</comment></comments></commit></commits></item><item><title>Missing JUnit dependency in groovy plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1363</link><project id="" key="" /><description>`./gradlew test` fails in :plugins-lang-groovy:compileTestGroovy with the java.lang.NoClassDefFoundError: junit/framework/TestCase exception. See https://gist.github.com/1242765 for more information. It looks like junit dependency is needed after all. https://github.com/elasticsearch/elasticsearch/blob/master/plugins/lang/groovy/build.gradle#L35
</description><key id="1741690">1363</key><summary>Missing JUnit dependency in groovy plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-09-26T17:15:57Z</created><updated>2011-09-26T18:20:08Z</updated><resolved>2011-09-26T18:20:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-26T18:20:08Z" id="2201622">Weird, I don't understand why I don't get it..., will add it anyhow.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Return _parent with each object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1362</link><project id="" key="" /><description>I find that in almost every instance where an object has a _parent value specified  I need to have access to it.  Having to specify and extract it in _fields seems very strange for such an intrinsic value.  I think it would be much more logical to return this by default and not to imbed it in fields (ie it should be returned on the same level as _source)
</description><key id="1737218">1362</key><summary>Return _parent with each object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merrellb</reporter><labels /><created>2011-09-26T06:54:39Z</created><updated>2013-04-05T16:16:25Z</updated><resolved>2013-04-05T16:16:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="merrellb" created="2011-09-26T09:24:07Z" id="2196360">To clarify why this seems strange, every other "underscore field" (_id, _type, _index, _source) is returned at the root level and fields is usually reserved for a subsets of _source. Additionally in pyes  the code to create returned objects is the same for get and search and it ends up a bit hackish insuring that the _parent be added to the fields in a search (and extracted and treated differently from the other returned "fields") and to also insure that objects obtain the _parent from the get call correctly.  While adding _parent to every object might be slightly redundant I don't think it would be any more so than always returning _index, _type or in some cases even _id.  In almost every use case I've come across if _parent exists it is just as important to return as _source.
</comment><comment author="kimchy" created="2011-10-17T17:58:54Z" id="2432314">The problem with that is that it "costs" to fetch the _parent, and you might not want it all times (i.e. when you do a simple search). I did not fully understand the problem with specifying the _parent field if its needed...
</comment><comment author="merrellb" created="2011-10-17T18:54:57Z" id="2433093">Hey Shay,

Thanks for the reply.  I can certainly understand your hesitation, especially if the "costs" are non-trivial.  Here are the benefits as I see them:
1)  In almost all of my use cases if I set a parent, the value is important and would be beneficial,even in a simple search.
2)  Many frameworks (and pyes in particular) don't have a mechanism to introspect or otherwise know if a parent should be present.  To allow any retrieved object to be "updated" requires they fetch the parent anyway.
3)  It appears "get" can't return the parent, even when requested.  One could argue that one must know the parent for routing purposes anyway but that creates trouble in pyes as the same code parses the results of gets and queries (ie one requires the parent to be forwarded internally from the get operation and the other can retrieve it from the response)
4)  It just seems odd that a retrieved object doesn't, by default, return with enough information to be updated and put back into ES.
5)  Every other leading underscore field is retrieved at the "root" level except _parent which is relegated to "fields" which is a bit confusing as that is usually reserved for a subset of _source.  The code to insure that _parent is added to fields and then separated back out ends up looking a bit hackish in pyes.

If it is too costly to enable this by default it may warrant some additional optimization (not that you don't already do this) as I think many people will always be looking to retrieve parent.  If nothing else perhaps you could enable its retrieval similar to the way versions are enabled (most times if you want version you want parent so you can update)
</comment><comment author="kimchy" created="2011-10-18T21:04:04Z" id="2447785">Heya, answers per point:

1) Thats your use cases, I am still not sure that it covers most use cases. Sure, when you search and then update, then you need the parent, but there is also just the case where one searches and displays the results, or the updated data does not come "from elasticsearch".

2) I don't understand why introspection is needed. If one executes a search where a possible update happens afterwards, and parent needs to be used, then fetching both _source and _parent is simple by specifying them in the fields to get.

3) Get will return the _parent in upcoming 0.18, there was a problem in realtime get that it did not return the parent if requested.

4) Cause you don't always want to update it. And, btw, it works the same way as _routing.

5) I think you are putting to much into "underscore" fields to group them into fields that needs to be returned by default.
</comment><comment author="merrellb" created="2011-10-18T22:49:00Z" id="2448931">Thanks for the reply.

The default behavior already seems to be rather verbose, returning index, type, id and source (despite many scenarios where these are similarly not needed).  The philosophy seems to be returning everything specified by the index operation and then reducing this through fields if desired.  However _parent seems to be the exception, indexed but not returned unless explicitly added through the fields parameter. 

Frameworks like pyes are now creating heavier objects with save() methods but these break unless the user remembers to set the fields appropriately, knowing they later plan to update and realizing the type requires a parent.  There are certainly workarounds (introspection, query parameters to automatically add "parent" to the fields, etc) but it does hurt the abstraction a bit.

Anyway, I can understand where you are coming from since it sounds like _parent has additional cost not associated with the other fields returned by default (and default behavior is really more aesthetics and not a showstopper).  Thanks for your consideration on this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>World's smallest pull request - Very minor documentation correction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1361</link><project id="" key="" /><description>Just a minor, minor thing I saw in the commits. RAID is _striped_, not _stripped_. 

Cheers!
</description><key id="1734696">1361</key><summary>World's smallest pull request - Very minor documentation correction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pal</reporter><labels /><created>2011-09-25T19:14:14Z</created><updated>2014-07-16T21:56:08Z</updated><resolved>2011-09-25T19:17:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-25T19:17:55Z" id="2192469">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reformatted and amended the example configuration file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1360</link><project id="" key="" /><description>Edited elasticsearch.yml:
- Separated different sections (using headers)
- Added more information about nodes configuration
- Added more information about various index configurations and their effects
- Added information about setting network and HTTP configuration
- Reworded information on gateway, recovery, discovery

The example configuration file should allow operations stuff to quickly
get a sense of ElasticSearch features relevant for systems support,
and to understand how to configure node, cluster, network and discovery settings.

The aim here is to vaguely respect the most often changed configuration settings,
while having some top-to-bottom conceptual integrity.

Table of Contents:
- Cluster
- Node
- Index
- Paths
- Memory
- Network And HTTP
- Gateway
- Recovery Throttling
- Discovery
</description><key id="1730772">1360</key><summary>Reformatted and amended the example configuration file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2011-09-24T17:10:53Z</created><updated>2014-07-16T21:56:08Z</updated><resolved>2011-09-25T18:06:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-25T18:06:32Z" id="2192151">Pushed, thanks!, still more settings to add :), but, a great start.
</comment><comment author="karmi" created="2011-09-26T09:07:50Z" id="2196252">Thanks! Yeah, we should definitely continue working on it, but at the moment, it should give devops a kick-start :)
</comment><comment author="matthiasg" created="2011-09-26T09:16:37Z" id="2196322">great job ! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Malformed REST create index causes the index to still be created</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1359</link><project id="" key="" /><description /><key id="1727384">1359</key><summary>Malformed REST create index causes the index to still be created</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.8</label><label>v0.18.0</label></labels><created>2011-09-23T22:03:56Z</created><updated>2011-09-23T22:04:25Z</updated><resolved>2011-09-23T22:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file></files><comments><comment>Malformed REST create index causes the index to still be created, closes #1359.</comment></comments></commit></commits></item><item><title>Allow to disable shard allocations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1358</link><project id="" key="" /><description>Allow to suspend (replica) shard allocations using a specific setting. This can help with restarting a node. The setting will be updated using the cluster update settings API, and then the node can be restarted, and once it has joined back the cluster, enable it back. 

Two settings will be exposed:
- `cluster.routing.allocation.disable_allocation`: Allows to disable either primary or replica allocation. Note, a replica will still be promoted to primary if one does not exists.
- `cluster.routing.allocation.disable_replica_allocation`: Allows to disable only replica allocation. 

Both settings can be updated using the cluster update settings API.
</description><key id="1723557">1358</key><summary>Allow to disable shard allocations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-23T14:04:28Z</created><updated>2011-09-23T14:35:42Z</updated><resolved>2011-09-23T14:35:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java</file></files><comments><comment>Allow to disable shard allocations, closes #1358.</comment></comments></commit></commits></item><item><title>required _routing fails when path points to an integer field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1357</link><project id="" key="" /><description>`"_routing": { "required": true, "path":"customer_id" }` requires `customer_id` to be a string.

If the first document has an integer `customer_id` and no explicit field mapping, it'll raise a routing mismatch.

If `customer_id` is explicitly mapped as an integer field, it always raises a routing mismatch.

When `customer_id` is a string (whether implicitly in the first doc or explicitly in the mapping), it works.

Repro at https://gist.github.com/1236728 on ES 0.17.7
</description><key id="1720356">1357</key><summary>required _routing fails when path points to an integer field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeremy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-09-23T04:03:32Z</created><updated>2011-09-26T01:25:35Z</updated><resolved>2011-09-23T22:59:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeremy" created="2011-09-23T04:40:37Z" id="2175265">Seeing that @ https://github.com/elasticsearch/elasticsearch/blob/master/modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java#L155 the routing value is always a String
</comment><comment author="jeremy" created="2011-09-23T04:46:50Z" id="2175286">routing as string looks to be baked in fairly deeply. So perhaps instead of supporting other types, like integers, a type error should be raised when mapping _routing path to a non-string field.
</comment><comment author="kimchy" created="2011-09-23T22:18:05Z" id="2183415">Heya, yea, it kindda expects the routing to be of type string, though, we should still be able to handle numeric values. Lemme check how we can fix that...
</comment><comment author="jeremy" created="2011-09-26T01:25:35Z" id="2194067">Great; even better! Thanks @kimchy.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/routing/SimpleRoutingTests.java</file></files><comments><comment>required _routing fails when path points to an integer field, closes #1357.</comment><comment>Do I miss something here?</comment></comments></commit></commits></item><item><title>Allow to stripe the data location over multiple locations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1356</link><project id="" key="" /><description>Allow to stripe the data location over multiple locations. The striping is simple, placing whole files in one of the locations, and deciding where to place the file based on the location with greatest free space. Note, there is no multiple copies of the same data, in that, its similar to RAID 0. Though simple, it should provide a good solution for people that don't want to mess with raids and the like. Here is how it is configured:

```
   path.data: /mnt/first,/mnt/second
```

Or the in an array format:

```
   path.data: ["/mnt/first", "/mnt/second"]
```
</description><key id="1717755">1356</key><summary>Allow to stripe the data location over multiple locations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-22T20:36:13Z</created><updated>2015-02-13T14:04:22Z</updated><resolved>2011-09-22T21:36:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2011-09-23T02:03:40Z" id="2174601">hi,@kimchy, i was wondering after set the data location to multiple locations,can i change them lately,and does these location containing the same copy?
</comment><comment author="kimchy" created="2011-09-23T14:39:14Z" id="2179191">You can change them later, but requires to restart the node. Each location does not share the same copy, its striped ala RAID 0.
</comment><comment author="deinspanjer" created="2011-11-01T16:25:45Z" id="2592421">What is the expected failure mode if a disk dies or otherwise becomes inaccessible?  Will ES continue to write to the remaining volumes?  Will the data on the failed node be recognized and recovered by the cluster?
</comment><comment author="arsonak47" created="2015-02-13T14:04:22Z" id="74257643">I configured multiple folders in my elasticsearch.yaml as - 

path.data: /home/esdata/part1, /home/esdata/part2, /home/esdata/part3, /home/esdata/part4, /home/esdata/part5, /home/esdata/part6, /home/esdata/part7, /home/esdata/part8, /home/esdata/part9, /home/esdata/part10, /home/esdata/part11, /home/esdata/part12, /home/esdata/part13, /home/esdata/part14, /home/esdata/part15, /home/esdata/part16, /home/esdata/part17, /home/esdata/part18, /home/esdata/part19, /home/esdata/part20, /home/esdata/part21, /home/esdata/part22, /home/esdata/part23, /home/esdata/part24, /home/esdata/part25

After inserting huge amount of data (apparently around 7.4 GB), I checked my data directories to know the pattern. I got the following output
![screenshot](https://cloud.githubusercontent.com/assets/10289473/6188415/cfd68894-b3b6-11e4-95a7-49f34fb029b4.png)

I am using Elasticsearch-0.90.3. My Elasticsearch cluster has single node and my index has a single shard. Now it's clear from the screenshot that my data is unevenly distributed among directories. Is there any configuration option by which I can insure even data distribution among all the configured directories?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/env/Environment.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/fs/FsGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/Store.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/FsIndexStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/MmapFsDirectoryService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/NioFsDirectoryService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/SimpleFsDirectoryService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalSettingsPerparer.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/gateway/fs/AbstractSimpleIndexGatewayTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/indices/store/IndicesStoreTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/fullrestart/FullRestartStressTest.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/rollingrestart/RollingRestartStressTest.java</file></files><comments><comment>Allow to stripe the data location over multiple locations, closes #1356.</comment></comments></commit></commits></item><item><title>Put mapping on a single node with new mapping will not wait for the mapping to be applied</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1355</link><project id="" key="" /><description /><key id="1713708">1355</key><summary>Put mapping on a single node with new mapping will not wait for the mapping to be applied</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.8</label><label>v0.18.0</label></labels><created>2011-09-22T13:00:26Z</created><updated>2011-09-22T13:02:31Z</updated><resolved>2011-09-22T13:02:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file></files><comments><comment>Put mapping on a single node with new mapping will not wait for the mapping to be applied, closes #1355.</comment></comments></commit></commits></item><item><title>Even shard count distribution counts relocations as two</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1354</link><project id="" key="" /><description>This can cause more movements of shards until the cluster stabilizes....
</description><key id="1705475">1354</key><summary>Even shard count distribution counts relocations as two</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.0</label></labels><created>2011-09-21T22:00:06Z</created><updated>2011-09-21T22:01:05Z</updated><resolved>2011-09-21T22:01:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java</file></files><comments><comment>Even shard count distribution counts relocations as two, closes #1354.</comment></comments></commit></commits></item><item><title>Analyzer specified for _all field is not getting defaulted on searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1353</link><project id="" key="" /><description>This discussion should detail things:
http://elasticsearch-users.115913.n3.nabble.com/I-am-tired-of-continuously-trying-to-override-the-default-analyzer-and-tokanizer-settings-td3350150.html

Here is a gist recreation:
https://gist.github.com/1232772

Thanks,
Paul
</description><key id="1703114">1353</key><summary>Analyzer specified for _all field is not getting defaulted on searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2011-09-21T17:44:44Z</created><updated>2011-09-24T12:07:09Z</updated><resolved>2011-09-24T08:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="oravecz" created="2011-09-21T18:01:00Z" id="2158939">I don't think it is just the _all field. If you substitute the field "messages" for "_all" in Paul's gist, you get the same (incorrect) result.
</comment><comment author="kimchy" created="2011-09-22T11:20:51Z" id="2166304">Heya, regarding the gist from the issue on the `_all` field, its because the `_all` field mapping is placed in the wrong place. `_all` field is a root level mapping, so needs to be outside the `properties` (its not really a property of the json document). When I replace the index creation with this, it works:

```
curl -XPUT 'http://localhost:9200/my_twitter1/' -d '
{
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0
    },
    "mappings": {
        "tweet" : {
            "_all" : {"type" : "string", "null_value" : "na", "index" : "analyzed", "analyzer" : "whitespace"},
            "properties" : {
                "user" : {"type" : "string", "index" : "not_analyzed"},
                "message" : {"type" : "string", "null_value" : "na", "index" : "analyzed", "analyzer" : "whitespace"},
                "postDate" : {"type" : "date"}
            }
        }
    }
}'
```
</comment><comment author="kimchy" created="2011-09-22T11:21:51Z" id="2166312">@oravecz: thats because its `message`, not `messages`, seems to work when I use it :)
</comment><comment author="oravecz" created="2011-09-22T19:12:08Z" id="2171085">Yeah, sorry that was a typo on my part. For me the bug is still there.

Does this work for you?
https://gist.github.com/1235719
</comment><comment author="ppearcy" created="2011-09-24T08:22:27Z" id="2185443">Thanks Shay. 
</comment><comment author="oravecz" created="2011-09-24T12:04:27Z" id="2186072">I still see a bug with this issue Paul, can we reopen it?

Please see my comment above and this gist: https://gist.github.com/1235719
</comment><comment author="oravecz" created="2011-09-24T12:07:09Z" id="2186083">Ahh, sorry. Shay had replied on my gist to tell me my json was malformed. And of course, he is right. :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard allocation awareness (rack aware, zone aware, for example)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1352</link><project id="" key="" /><description>Cluster allocation awareness allows to configure shard and replicas allocation across generic attributes associated the nodes. Lets explain it through an example:

Assume we have several racks. When we start a node, we can configure an attribute called `rack_id` (any attribute name works), for example, here is a sample config:

```
node.rack_id: rack_one
```

The above sets an attribute called `rack_id` for the relevant node with a value of `rack_one`. Now, we need to configure the `rack_id` attribute as one of the awareness allocation attributes (set it on _all_ (master eligible) nodes config):

```
cluster.routing.allocation.awareness.attributes: rack_id
```

The above will mean that the `rack_id` attribute will be used to do awareness based allocation of shard and its replicas. For example, lets say we start 2 nodes with `node.rack_id` set to `rack_one`, and deploy a single index with 5 shards and 1 replica. The index will be fully deployed on the current nodes (5 shards and 1 replica each, total of 10 shards).

Now, if we start two more nodes, with `node.rack_id` set to `rack_two`, shards will relocate to even the number of shards across the nodes, but, a shard and its replica will not be allocated in the same `rack_id` value.

The awareness attributes can hold several values, for example:

```
cluster.routing.allocation.awareness.attributes: rack_id,zone
```

_NOTE_: When using awareness attributes, shards will not be allocated to nodes that don't have values set for those attributes.
## Forced Awareness

Sometimes, we know in advance the number of values an awareness attribute can have, and more over, we would like never to have more replicas then needed allocated on a specific group of nodes with the same awareness attribute value. For that, we can force awareness on specific attributes.

For example, lets say we have an awareness attribute called `zone`, and we know we are going to have two zones, `zone1` and `zone2`. Here is how we can force awareness one a node:

```
cluster.routing.allocation.awareness.force.zone: zone1,zone2
cluster.routing.allocation.awareness.attributes: zone
```

Now, lets say we start 2 nodes with `node.zone` set to `zone1` and create an index with 5 shards and 1 replica. The index will be created, but only 5 shards will be allocated (with no replicas). Only when we start more shards with `node.zone` set to `zone2` will the replicas be allocated.
## Automatic Preference When Searching / GETing

When executing a search, or doing a get, the node receiving the request will prefer to execute the request on shards that exits on nodes that have the same attribute values as the executing node.
## Realtime Settings Update

The settings can be updated using the cluster update settings API on a live cluster.
</description><key id="1700177">1352</key><summary>Shard allocation awareness (rack aware, zone aware, for example)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-21T15:25:43Z</created><updated>2015-06-24T18:46:08Z</updated><resolved>2011-09-21T15:26:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-09-22T06:47:25Z" id="2164641">Hiya - this sounds really useful. Question: is this exposed in the cluster state?  I'm thinking of the code that I use in ElasticSearch.pm to retrieve a list of live nodes for round-robin'ing in the client.  This should potentially be rack aware as well, so that a client in zone1 only (or preferentially) talks to nodes in zone1.
</comment><comment author="kimchy" created="2011-09-22T11:39:56Z" id="2166409">Yes, attributes are returned as part of the each node information in the cluster state. But, the settings (awareness settings), do not (as they will usually be set in the config file). But, I think in any case the user will have to tell you in the client construction which awareness attributes values the client runs on, and then you can use those to chooses which nodes based on their attributes?
</comment><comment author="finominfo" created="2015-01-14T08:10:56Z" id="69882356">Is it possible that one node belongs more than one zones? I mean:
node.zone: zone1, zone2
</comment><comment author="bleskes" created="2015-01-14T16:27:25Z" id="69943757">@finominfo any node attribute can have one value, so no. Why are you asking?
</comment><comment author="ml4spark" created="2015-06-22T15:21:16Z" id="114152281">@bleskes  Hi Boaz, in above explanation it is said that "a shard and its replica will not be allocated in the same rack_id value."  I see this working on my cluster, however It seems that this only applies to the primary shard and the replicas, but not among replicas (i.e. i use 2 replicas, these are sometimes allocated to the same rack_id) - using elasticsearch 1.6.  Can I force that no two copies of a shard (no matter if primary or replica) reside in the same rack_id?  Thanks!
</comment><comment author="bleskes" created="2015-06-24T15:08:56Z" id="114901541">@ml4spark do you have more shard copies (primary + replicas) than attribute values? you might want to use Forced Awareness.  Also see #11852 for a clarification of that sentence in the docs. I hope it helps.
</comment><comment author="ml4spark" created="2015-06-24T18:46:08Z" id="114977323">@bleskes thanks for the clarification. It turned out that my config file was incorrect and hence elastic didn't use the rack based routing at all. I got it right now and it works like a charm. Cheers
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java</file></files><comments><comment>Shard allocation awareness (rack aware, zone aware, for example), closes #1352.</comment></comments></commit></commits></item><item><title>Support external versioning for deletes arriving before initial update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1351</link><project id="" key="" /><description>The versioning feature works well for protecting the index against stale updates due to message reordering (e.g. in a messaging system). However, there is one case not handled properly yet which is when a delete (with external version number) arrives before the first update of the document (with external version number). 

Find more on that in this group thread (that asked to file this as enhancement request): https://groups.google.com/d/topic/elasticsearch/QXn1wK01THE/discussion 
</description><key id="1695555">1351</key><summary>Support external versioning for deletes arriving before initial update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfiedler</reporter><labels><label>enhancement</label><label>v0.17.8</label><label>v0.18.0</label></labels><created>2011-09-21T09:41:40Z</created><updated>2011-09-22T21:56:14Z</updated><resolved>2011-09-22T21:56:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-21T10:18:38Z" id="2154478">Already fixed here #1341 :)
</comment><comment author="jfiedler" created="2011-09-22T15:53:29Z" id="2168725">I moved to 0.17.7 but my little test case is still red. Not sure #1341 really is the same as this issue. This is my test case: I  trigger a delete with external version number 2 and then trigger an update with external version 1 (simulating message reordering). I am expecting the update to fail with a version conflict exception but it does succeed. When debugging I see the delete failing with a version conflict (current -1 but expected 2). The update then succeeds.
</comment><comment author="kimchy" created="2011-09-22T21:41:45Z" id="2172861">Yea, you are right, I missed that interaction model, will fix.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/versioning/SimpleVersioningTests.java</file></files><comments><comment>Support external versioning for deletes arriving before initial update, closes #1351.</comment></comments></commit></commits></item><item><title>Make it possible for the date field to accept fields that are in seconds instead of just milliseconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1350</link><project id="" key="" /><description>Not all systems have millisecond precision timestamps and would rather send their timestamp in seconds instead.

This pull request adds the following configuration option to the date field mapper: 

```
numeric_is_in_seconds: true|false
```

that tells ES which format numeric values should be treated as (seconds or milliseconds). The default value is false for backward compatability, and since this may be the most common.
</description><key id="1689642">1350</key><summary>Make it possible for the date field to accept fields that are in seconds instead of just milliseconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2011-09-20T16:55:45Z</created><updated>2014-07-06T15:50:17Z</updated><resolved>2011-09-21T21:30:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nkvoll" created="2011-09-20T16:59:17Z" id="2147264">I'm wondering whether "numeric_is_in_seconds" is a good name for the value, or whether something else should be used instead? Any tips?

I'd also like to write a unit test for this behaviour, but I can't seem to access the parsed/stored value from a unit test after a document has been parsed by the mapper. Any clues?
</comment><comment author="kimchy" created="2011-09-20T18:02:48Z" id="2147954">How about having a numeric_resolution field, which will get parsed into the available values in Java `TimeUnit`. Then, the time unit can be used to do `TimeUnit#toMillis(value)`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>edgeNgram tokenizer - StringIndexOutOfBoundsException on empty strings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1349</link><project id="" key="" /><description>Gist recreation below. Works fine with  standard nGram tokenizer, though.

```
curl -XDELETE localhost:9200/test_index?pretty; 
```

```
{
  "ok" : true,
  "acknowledged" : true
}

```

```
curl -XPOST localhost:9200/test_index?pretty -d '
{ 
  "analysis": {
    "analyzer" : {
      "test_analyzer": { 
        "type":"custom",
        "tokenizer":
        "edge_ngram_front",
        "filter":["standard","lowercase"]
      }
    },
    "tokenizer": { 
      "edge_ngram_front": { 
        "type":"edgeNGram",
        "min_gram":1,
        "max_gram":2,
        "side": "front"
      }
    }
  }
}'; 
```

```
{
  "ok" : true,
  "acknowledged" : true
}

```

```

curl -XPUT localhost:9200/test_index/test_type/_mapping?pretty -d '
{
  "test_type": {
    "properties": {
      "name": {
        "type": "string",
        "analyzer": "test_analyzer"
      }
    }
  }
}
'
```

```
{
  "ok" : true,
  "acknowledged" : true
}
```

```
curl -XPUT localhost:9200/test_index/test_type/1?pretty -d '{"name": ""}'
```

```
{
  "error" : "StringIndexOutOfBoundsException[String index out of range: -1]",
  "status" : 500
}

```
</description><key id="1688741">1349</key><summary>edgeNgram tokenizer - StringIndexOutOfBoundsException on empty strings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mwiercinski</reporter><labels /><created>2011-09-20T15:21:28Z</created><updated>2013-04-05T15:38:55Z</updated><resolved>2013-04-05T15:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-20T18:06:33Z" id="2147988">This is a bug in the edge ngram executed on an empty string. its basically in Lucene, want to open an issue there?
</comment><comment author="clintongormley" created="2013-04-05T15:38:55Z" id="15963145">Fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>include the path when serializing _id field mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1348</link><project id="" key="" /><description>The _id mapping supports being configured with a "path", which is where ES should find the actual id inside the document. This isn't currently returned when asking ES for the mapping. For example:

```
$ curl http://localhost:9200/test -XDELETE
{"ok":true,"acknowledged":true}

$ curl http://localhost:9200/test -XPOST
{"ok":true,"acknowledged":true}

$ curl http://localhost:9200/test/item/_mapping -XPUT -d '{
    "item" : {
        "_id": {
            "path": "foo.my_id", "store": true
        }
    }
}'
{"ok":true,"acknowledged"}

$ curl http://localhost:9200/test/item/_mapping?pretty
{
  "item" : {
    "_id" : {
      "store" : "yes"
    },
    "properties" : {
    }
  }
}

$ curl http://localhost:9200/test/item -XPOST -d '{"foo":{"my_id":"this_is_my_id"}, "bar": "bar text"}'
{"ok":true,"_index":"test","_type":"item","_id":"this_is_my_id","_version":1}

$ curl http://localhost:9200/test/item/this_is_my_id
{"_index":"test","_type":"item","_id":"this_is_my_id","_version":1,"exists":true, "_source" : {"foo":{"my_id":"this_is_my_id"}, "bar": "bar text"}}

$ curl http://localhost:9200/test/item/this_is_my_id?pretty
{
  "_index" : "test",
  "_type" : "item",
  "_id" : "this_is_my_id",
  "_version" : 1,
  "exists" : true, "_source" : {"foo":{"my_id":"this_is_my_id"}, "bar": "bar text"}
}
```

This pull request adds this variable to the outputted mapping, with an attempt at writing an unit test to prevent regressions.
</description><key id="1688358">1348</key><summary>include the path when serializing _id field mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2011-09-20T14:41:55Z</created><updated>2014-07-16T21:56:09Z</updated><resolved>2011-09-20T14:55:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-20T14:55:43Z" id="2145827">Pushed, thanks!. This is actually a bug as well since on restart, the path setting would not have been preserved...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve source based fields loading when searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1347</link><project id="" key="" /><description>When searching, and asking for specific `fields`, one can specify the actual path to the field, and if its not stored, it will be extracted from source. Currently, it changes the field to be loaded from source to be a script field.

There are several problems with this, the first is that it gets appended with a `_source.` prefix, which is annoying since what one asks for is not what you get back.

The second problem is that it does not work for objects. You have to explicitly specify `_source.obj1` to get an object named `obj1`.

The last problem is with inner arrays. Since its a script evaluation, inner arrays don't get "collapsed" automatically, causing failure to load the field.

This issue should fix all the above mentioned problems by not using scripts to extract the value when one specify a field named something like `obj1.obj2`, or `obj1.field1`, and extracting it in a smarter manner.

This is a breaking change since now `_source.` will not be added to the result of the field name.
</description><key id="1686952">1347</key><summary>Improve source based fields loading when searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.18.0</label></labels><created>2011-09-20T11:32:46Z</created><updated>2011-09-21T06:01:23Z</updated><resolved>2011-09-20T11:33:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mahendra" created="2011-09-21T06:01:23Z" id="2153023">+1 to this feature. This will simplify client code in my setup. I have clients querying CouchDB and ES together. For parsing fields in CouchDB response, they use the obj1.field1 notation. For ES, they use _source.obj1.field1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/FieldsParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceScoreOrderFragmentsBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceSimpleFragmentsBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/fields/SearchFieldsTests.java</file></files><comments><comment>Improve source based fields loading when searching, closes #1347.</comment></comments></commit></commits></item><item><title>Loss of shard after networking flap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1346</link><project id="" key="" /><description>Here's the logs from my cluster as the network flapped: https://raw.github.com/gist/1227273/20fad50a8f23dfba8687fb54ed5b9f4cf66f613b/gistfile1.txt

Afterwards, the cluster got stuck in this state:
{  
 "cluster_name" : "xsearch",  
 "status" : "red",  
 "timed_out" : false,  
 "number_of_nodes" : 29,  
 "number_of_data_nodes" : 29,  
 "active_primary_shards" : 14,  
 "active_shards" : 28,  
 "relocating_shards" : 0,  
 "initializing_shards" : 1,  
 "unassigned_shards" : 1  
}

Full cluster state after crash: https://gist.github.com/1227521

After restarting the failed node, new cluster state:
{  
│  "cluster_name" : "xsearch",  
│  "status" : "red",  
│  "timed_out" : false,  
│  "number_of_nodes" : 29,  
│  "number_of_data_nodes" : 29,  
│  "active_primary_shards" : 14,  
│  "active_shards" : 28,  
│  "relocating_shards" : 0,  
│  "initializing_shards" : 0,  
│  "unassigned_shards" : 2  
}

After full cluster restart, the cluster health returned to green.
</description><key id="1682694">1346</key><summary>Loss of shard after networking flap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amckinley</reporter><labels /><created>2011-09-19T21:29:51Z</created><updated>2013-04-05T15:39:44Z</updated><resolved>2013-04-05T15:39:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2011-10-10T02:23:12Z" id="2342430">Which version of ES did you use when this problem occurred? 
</comment><comment author="amckinley" created="2011-10-10T21:22:39Z" id="2352454">We're running the master branch, commit ed99a51406ab5fd8dcc626dc90f408559d57e3c1.
</comment><comment author="clintongormley" created="2013-04-05T15:39:44Z" id="15963191">No further info after one year - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>config/elasticsearch.yml: add a commented out node.name with docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1345</link><project id="" key="" /><description>Add a commented out node.name with docs. When setting up ElasticSearch
the feature of ElasticSearch of picking random names on startup has
repeatedly confused administrators and other people looking at it.

I only found out recently that you can override this by reading the
source for InternalSettingsPerparer.java. Add it to the example
elasticsearch.yml config file to help others discover this obscure
option.
</description><key id="1676680">1345</key><summary>config/elasticsearch.yml: add a commented out node.name with docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avar</reporter><labels /><created>2011-09-19T09:04:50Z</created><updated>2014-06-24T17:56:07Z</updated><resolved>2011-09-25T18:15:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-25T18:15:47Z" id="2192187">Added as part of more changes done to the config file, thanks!.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Realtime Get: Under high concurrent indexing and immediate get, a get might be missed while flushing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1344</link><project id="" key="" /><description /><key id="1672294">1344</key><summary>Realtime Get: Under high concurrent indexing and immediate get, a get might be missed while flushing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-18T10:43:06Z</created><updated>2011-09-18T10:44:20Z</updated><resolved>2011-09-18T10:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/get/GetStressTest.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/get/MGetStress1.java</file></files><comments><comment>Realtime Get: Under high concurrent indexing and immediate get, a get might be missed while flushing, closes #1344.</comment></comments></commit></commits></item><item><title>Bulk API: Properly retry execution on temporal state changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1343</link><project id="" key="" /><description>The index API has a retry mechanism on possible (initial) temporal state failure when executing on the primary shard. Have a similar logic in the bulk API as well.
</description><key id="1666824">1343</key><summary>Bulk API: Properly retry execution on temporal state changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-16T23:20:56Z</created><updated>2011-09-16T23:21:34Z</updated><resolved>2011-09-16T23:21:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file></files><comments><comment>Bulk API: Properly retry execution on temporal state changes, closes #1343.</comment></comments></commit></commits></item><item><title>handling NoRouteToHost correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1342</link><project id="" key="" /><description>an example of the behavior before this patch: https://raw.github.com/gist/1223276/d9c53584e072e23721cfdff49fcd63bd6d2a8da5/gistfile1.txt
</description><key id="1666804">1342</key><summary>handling NoRouteToHost correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amckinley</reporter><labels /><created>2011-09-16T23:14:18Z</created><updated>2014-06-13T14:45:04Z</updated><resolved>2013-04-05T15:40:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-18T07:34:42Z" id="2125731">I am not sure that we need to close the channel on this failure, since the failure is on connect, the channel should not be connected (and should also fail on the actual connect operation done). Did you see other failures similar to this one? Something wrapped with a `ConnectTransportException`?
</comment><comment author="amckinley" created="2011-09-19T19:25:01Z" id="2137404">So then shouldn't the if (!lifecycle.started()) clause at the start of the function handle this? That makes me surprised to see this error at all.
</comment><comment author="clintongormley" created="2013-04-05T15:40:30Z" id="15963241">No action after two years - Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Versioning: Delete on an already deleted document should still affect versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1341</link><project id="" key="" /><description>Versioning: Delete on an already deleted document should still affect versioning
</description><key id="1666050">1341</key><summary>Versioning: Delete on an already deleted document should still affect versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-16T21:49:03Z</created><updated>2011-09-16T21:49:34Z</updated><resolved>2011-09-16T21:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/versioning/SimpleVersioningTests.java</file></files><comments><comment>Versioning: Delete on an already deleted document should still affect versioning, closes #1341.</comment></comments></commit></commits></item><item><title>adding timeout to list of connection failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1340</link><project id="" key="" /><description>I saw a ton of these in my logs this morning: https://gist.github.com/1223180
</description><key id="1665817">1340</key><summary>adding timeout to list of connection failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amckinley</reporter><labels /><created>2011-09-16T21:20:06Z</created><updated>2014-07-16T21:56:10Z</updated><resolved>2011-09-16T21:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-16T21:52:06Z" id="2119181">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query parsing exception on 'erik\'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1339</link><project id="" key="" /><description>Not sure if the desired behavior is to throw an exception like this, but on the query string 'erik\', elasticsearch throws a parse exception. See attached gist: https://raw.github.com/gist/1223135/b19ad35a981b920f8b2422508f23fe3e73d5a7b7/gistfile1.txt
</description><key id="1665666">1339</key><summary>Query parsing exception on 'erik\'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amckinley</reporter><labels /><created>2011-09-16T21:01:24Z</created><updated>2013-04-05T15:40:45Z</updated><resolved>2013-04-05T15:40:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-26T18:12:52Z" id="2201530">Not sure how you query elasticsearch, but if its a `query_string` type query, then this is an illegal format of a query Lucene can parse.
</comment><comment author="amckinley" created="2011-09-26T23:08:21Z" id="2205199">I was just wondering if elastic search should do something more graceful with a parsing exception, but I guess returning the exception makes sense. Should the logging at least be changed to avoid dumping the entire stack trace?
</comment><comment author="kimchy" created="2011-09-27T22:03:51Z" id="2217130">Yea, it can certainly try and do something more graceful. Btw, if you don't want to expose the full query syntax of Lucene, just simple text based search, then using the text query will do the trick.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elastic search not in maven central</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1338</link><project id="" key="" /><description>I do see the artifacts properly hosted under the oss.sonatype.org server (thank you!).

But for most projects that oss server copies/synchronizes all artifacts with the maven central repository; so there should be no need for us to this repository to our pom.

Could you email your sonatype contact and ask them about it, maybe they forgot to turn on central sync for your project; it should be easy to fix. :)
</description><key id="1654881">1338</key><summary>elastic search not in maven central</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fern</reporter><labels /><created>2011-09-15T17:07:44Z</created><updated>2013-04-05T15:40:53Z</updated><resolved>2013-04-05T15:40:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-16T21:53:29Z" id="2119192">Yea, I'll try and chase them next week, see how it goes....
</comment><comment author="gaffo" created="2011-09-28T22:52:34Z" id="2231432">I can't seem to find it in any repo at the moment. Not in sonatype or in Fuse. Very frustrating!
</comment><comment author="karussell" created="2011-10-07T20:26:12Z" id="2327615">@gaffo did you actually search? Very frustrating ;) !

http://oss.sonatype.org/content/repositories/releases/org/elasticsearch/elasticsearch/

id: sonatype-releases
name: Sonatype Releases Repository
url: http://oss.sonatype.org/content/repositories/releases/
</comment><comment author="daspilker" created="2011-11-22T10:08:03Z" id="2832551">Any update on this issue?

BTW: Sonatype documented how to publish artifacts to central: http://www.sonatype.com/people/2011/10/publishing-your-artifacts-to-the-central-repository/
</comment><comment author="fern" created="2012-03-25T21:17:37Z" id="4684785">Hi. It does look like it's on the OSS repository, but it's not being synced to central still. is that right?

Were you able to talk to your Sonatype contact?  Maybe we should just create a ticket with them to make sure the central sync is setup for you:

https://issues.sonatype.org/browse/OSSRH
</comment><comment author="eskatos" created="2012-09-24T22:29:50Z" id="8837430">+1
</comment><comment author="dadoonet" created="2012-09-25T02:31:30Z" id="8841663">It's now in central :
http://repo1.maven.org/maven2/org/elasticsearch/elasticsearch/0.19.9/elasticsearch-0.19.9.pom

This issue should be closed now.
</comment><comment author="eskatos" created="2012-09-25T10:02:30Z" id="8849143">Thanks.
Please close the issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rest Delete API does not honor the `version_type` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1337</link><project id="" key="" /><description /><key id="1653199">1337</key><summary>Rest Delete API does not honor the `version_type` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-15T14:06:13Z</created><updated>2011-09-15T14:07:16Z</updated><resolved>2011-09-15T14:07:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java</file></files><comments><comment>Rest Delete API does not honor the `version_type` parameter, closes #1337.</comment></comments></commit></commits></item><item><title>Optimizing inactive (indexing wise) shard to only happen when there are no ongoing merges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1336</link><project id="" key="" /><description /><key id="1651332">1336</key><summary>Optimizing inactive (indexing wise) shard to only happen when there are no ongoing merges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-15T09:19:53Z</created><updated>2011-09-15T09:21:26Z</updated><resolved>2011-09-15T09:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryBufferController.java</file></files><comments><comment>Optimizing inactive (indexing wise) shard to only happen when there are no ongoing merges, closes #1336.</comment></comments></commit></commits></item><item><title>Upgrade to Lucene 3.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1335</link><project id="" key="" /><description /><key id="1651118">1335</key><summary>Upgrade to Lucene 3.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-15T08:47:00Z</created><updated>2011-09-15T08:51:16Z</updated><resolved>2011-09-15T08:51:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/synonym/SynonymFilter.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/synonym/SynonymMap.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/PublicTermsFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/DocSets.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/FixedBitDocSet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/SlicedOpenBitSet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/TermFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/uid/UidField.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractWeightedFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/FilterCacheValue.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/UidFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/child/ChildCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/child/HasChildFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/nested/BlockJoinQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/nested/NestedChildrenCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/nested/NonNestedDocsFilter.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/lucene/docset/SlicedOpenBitSetTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/deps/lucene/SimpleLuceneTests.java</file></files><comments><comment>Upgrade to Lucene 3.4.0, closes #1335.</comment></comments></commit></commits></item><item><title>Allow to filter geo bounding box or distance based on indexed lat lon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1334</link><project id="" key="" /><description>The way geo distance and bounding box filters work by default is by doing in memory checks to see if the document matches the provided parameters. Those in memory checks are fast, and with the new optimize bounding box option in geo distance (on by default), distance checks are also fast.

But, sometimes its faster to do the checks using indexed lat/lon values, especially when either _only_ doing the geo checks, or when the bounding box is "small".

The `geo_bounding_box` filter now allows to provide a `type`, which can be set to `indexed` (defaults to `memory`). When set to `indexed`, it will do checks based on indexed lat and lon.

The `geo_distance` and `geo_distance_range` filters allow to pass to the `optimize_bbox` parameter either `none`, `indexed`, or "memory" (defaults to `memory`).

Note, in order to use the `indexed` option, the `geo_point` mapping type must set `lat_lon` to `true` in its mapping in order to index them as well. Also, this option will not work properly for a document with multiple locations under the same field unless nested documents are used.
</description><key id="1642689">1334</key><summary>Allow to filter geo bounding box or distance based on indexed lat lon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-14T11:25:00Z</created><updated>2011-09-14T11:27:53Z</updated><resolved>2011-09-14T11:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistance.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/InMemoryGeoBoundingBoxFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/geo/GeoBoundingBoxTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/geo/GeoDistanceTests.java</file></files><comments><comment>Allow to filter geo bounding box or distance based on indexed lat lon, closes #1334.</comment></comments></commit></commits></item><item><title>Problems with setup tutorial</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1333</link><project id="" key="" /><description>I followed the tutorial for setting up Elastic Search on EC2:
- http://www.elasticsearch.org/tutorials/2011/08/22/elasticsearch-on-ec2.html
  Except for a couple of issues it works great, and I am really happy to see this kind of information readily available. 

I bumped into a couple of issues that I had to resolve: 
- setting 'bootstrap.mlockall to true' caused elastic search to hang during boot.
      \* this may be because I did not set ES_MIN_MEM and ES_MAX_MEM on the EC2 micro instance and ES ran out of memory.
      \* I just left this rule commented out for now.
- the recommended Red Hat distribution on EC2 has a firewall that I had to configure with system-config-firewall-tui to open the necessary ports.
- the security group need to open for port 9301 and 9302 in addition to the ports mentioned (22,9300,9200).

Thank you for a great guide.

Best,
Andy
</description><key id="1636727">1333</key><summary>Problems with setup tutorial</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">digitalplaywright</reporter><labels /><created>2011-09-13T18:37:32Z</created><updated>2013-04-05T15:41:20Z</updated><resolved>2013-04-05T15:41:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="greenmang0" created="2011-11-02T08:12:33Z" id="2600699">May I know the reason why 9301/02 needs to be open? I am running a cluster of 3 nodes with 9200 and 9300 open and didn't face any issues so far.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix bug when adding to BulkRequest with no TTL, add simple unit test for it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1332</link><project id="" key="" /><description /><key id="1634694">1332</key><summary>fix bug when adding to BulkRequest with no TTL, add simple unit test for it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-09-13T16:12:25Z</created><updated>2014-07-16T21:56:11Z</updated><resolved>2011-09-13T19:50:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-13T19:50:32Z" id="2085697">Pushed, thanks!.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>org.elasticsearch.index.mapper.MapperParsingException while adding Tweets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1331</link><project id="" key="" /><description>curl -XPUT http://localhost:9200/tweets/tweet/113429251348373500 -d ' {"contributors":{},"truncated":false,"text":"@SocialMediaCoop Americans Spend 23% of Online Time on Social Networks #socialmedia tweeps http://t.co/UVUOcA7 #facebook... via @DioFavatas","geo":{},"entities":{"urls":[{"indices":[91,110],"display_url":"mydio.me/o1jUPk","expanded_url":"http://mydio.me/o1jUPk","url":"http://t.co/UVUOcA7"}],"hashtags":[{"text":"socialmedia","indices":[71,83]},{"text":"facebook","indices":[111,120]}],"user_mentions":[{"indices":[0,16],"screen_name":"SocialMediaCoop","name":"Social Media Coop","id":188151799,"id_str":"188151799"},{"indices":[128,139],"screen_name":"DioFavatas","name":"Dio Favatas","id":16186656,"id_str":"16186656"}]},"favorited":false,"place":{},"coordinates":{},"source":"&lt;a href=\"http://www.evion.org/\" rel=\"nofollow\"&gt;Evion&lt;/a&gt;","in_reply_to_screen_name":"SocialMediaCoop","in_reply_to_user_id":188151799,"possibly_sensitive":false,"retweeted":false,"created_at":"Tue Sep 13 01:50:15 +0000 2011","in_reply_to_status_id_str":{},"user":{"contributors_enabled":false,"profile_background_image_url":"http://a0.twimg.com/images/themes/theme1/bg.png","show_all_inline_media":false,"geo_enabled":false,"profile_image_url":"http://a2.twimg.com/profile_images/1099763562/evion_logo_3_normal.png","profile_text_color":"333333","profile_image_url_https":"https://si0.twimg.com/profile_images/1099763562/evion_logo_3_normal.png","location":"","default_profile_image":false,"lang":"en","profile_background_image_url_https":"https://si0.twimg.com/images/themes/theme1/bg.png","profile_sidebar_fill_color":"DDEEF6","description":"Evion lets you discover interesting tweets from other people on Twitter as well as find interested new readers for your tweets.","screen_name":"evwo6","statuses_count":17950,"profile_background_tile":false,"default_profile":true,"followers_count":755,"follow_request_sent":{},"following":{},"notifications":{},"friends_count":720,"profile_link_color":"0084B4","verified":false,"created_at":"Fri Aug 06 07:59:25 +0000 2010","profile_sidebar_border_color":"C0DEED","protected":false,"favourites_count":0,"name":"Evion","is_translator":false,"profile_use_background_image":true,"id":175318654,"id_str":"175318654","listed_count":19,"time_zone":"Quito","utc_offset":-18000,"profile_background_color":"C0DEED","url":"http://evion.org"},"in_reply_to_status_id":{},"id":113429250345930750,"in_reply_to_user_id_str":"188151799","id_str":"113429250345930752","retweet_count":0,"updated_at":1315878615699}'

{"error":"MapperParsingException[object_mapper [tweet] tried to parse as object, but got EOF, has a concrete value been provided to it?]","status":400}

org.elasticsearch.index.mapper.MapperParsingException: object_mapper [in_reply_to_screen_name] tried to parse as object, but got EOF, has a concrete value been provided to it?
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:439)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:569)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:441)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:567)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:491)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:289)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:185)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:428)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:341)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
</description><key id="1629722">1331</key><summary>org.elasticsearch.index.mapper.MapperParsingException while adding Tweets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ramv</reporter><labels /><created>2011-09-13T01:55:36Z</created><updated>2013-10-17T20:27:13Z</updated><resolved>2013-04-05T15:42:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-13T10:16:48Z" id="2080208">This does get indexed, usually, this error comes from a document being indexed one way, and then indexed another. For example, one time having an `object` json for a field, and then having a value.
</comment><comment author="ramv" created="2011-09-13T14:10:39Z" id="2082069">I don't think the tweet is getting indexed. Here is an example

DEBUG: Elastical: the request failed {"method":"PUT","json":{"contributors":{},"truncated":false,"text":"@CMBInfo @ConstantContact - Over 6,000 #socialmedia regular users in study on brand #engagement by @SMG_London  http://t.co/hGkhqX4 #SMBI","geo":{},"entities":{"urls":[{"indices":[112,131],"display_url":"bit.ly/oQz5ar","expanded_url":"http://bit.ly/oQz5ar","url":"http://t.co/hGkhqX4"}],"hashtags":[{"text":"socialmedia","indices":[39,51]},{"text":"engagement","indices":[84,95]},{"text":"SMBI","indices":[132,137]}],"user_mentions":[{"indices":[0,8],"screen_name":"CMBInfo","name":"ChadwickMartinBailey","id":80942445,"id_str":"80942445"},{"indices":[9,25],"screen_name":"ConstantContact","name":"Constant Contact","id":25960305,"id_str":"25960305"},{"indices":[99,110],"screen_name":"SMG_London","name":"SMG London","id":38452444,"id_str":"38452444"}]},"favorited":false,"place":{},"coordinates":{},"source":"web","in_reply_to_screen_name":"CMBInfo","in_reply_to_user_id":80942445,"possibly_sensitive":false,"retweeted":false,"created_at":"Tue Sep 13 14:06:24 +0000 2011","in_reply_to_status_id_str":"113293957642977280","user":{"default_profile":true,"contributors_enabled":false,"profile_background_image_url":"http://a0.twimg.com/images/themes/theme1/bg.png","show_all_inline_media":false,"geo_enabled":false,"profile_image_url":"http://a2.twimg.com/profile_images/1281593358/Steve_Parker_normal.jpg","profile_text_color":"333333","profile_image_url_https":"https://si0.twimg.com/profile_images/1281593358/Steve_Parker_normal.jpg","location":"London","default_profile_image":false,"lang":"en","profile_background_image_url_https":"https://si0.twimg.com/images/themes/theme1/bg.png","profile_sidebar_fill_color":"DDEEF6","description":"","screen_name":"steveparkersmg","statuses_count":338,"profile_background_tile":false,"followers_count":232,"follow_request_sent":{},"following":{},"notifications":{},"friends_count":330,"profile_link_color":"0084B4","verified":false,"created_at":"Mon May 11 16:41:20 +0000 2009","profile_sidebar_border_color":"C0DEED","protected":false,"favourites_count":3,"name":"Steve Parker","is_translator":false,"profile_use_background_image":true,"id":39286278,"id_str":"39286278","listed_count":6,"time_zone":{},"utc_offset":{},"profile_background_color":"C0DEED","url":"http://emergingspaces.co.uk"},"in_reply_to_status_id":113293957642977280,"id":113614508500590600,"in_reply_to_user_id_str":"80942445","id_str":"113614508500590592","retweet_count":0,"updated_at":"2011-09-13T14:06:25.143Z"},"url":"http://127.0.0.1:9200/tweets/tweet/113614508500590600","timeout":10000,"encoding":"utf8"}

$ curl -XGET http://127.0.0.1:9200/tweets/tweet/113614508500590600
{"_index":"tweets","_type":"tweet","_id":"113614508500590600","exists":false}
</comment><comment author="kimchy" created="2011-09-13T14:15:52Z" id="2082118">If you get the failure you pasted in the issue, then it won't be indexed. As i said, its probably because you try to index a value into an object mapped json. i.e., you index one time something like this: `{ "obj1" : { "field1" : "value1" } }`, and then index this: `{ "obj1" : "value" }`.
</comment><comment author="uzquiano" created="2011-09-27T18:25:47Z" id="2214410">Would it be possible to have the error message indicate which field from the object mapped JSON is in conflict?
</comment><comment author="mehtryx" created="2011-10-12T19:24:09Z" id="2383306">I've encountered an issue here where the symptoms are the same...what I've discovered is that I have some data fields that are null and some which have a json date in them....and it is when I index a document that is different from whatever I had indexed first I get the error.

As an example the json for a record I might try to index contains:

"AvailableOn": "/Date(-2206274400000-0500)/",

however most of my documents had the following:

"AvailableOn": null,

When a null value is indexed it turns it into a null object inside elasticsearch, and then the documents that do not have null generated the error you reported and I'm experiencing.  My issue is very much related to how this date is presented and we can figure a work around, but I suspect the overall scenario may give people with the issue something to look for in their own data and determine how/why they are getting this.
</comment><comment author="mehtryx" created="2011-10-13T13:20:40Z" id="2394541">Further checking on other fields did not reproduce this....so while it was a theory and may have some relevence into what I am experiencing, it seems my other null fields with regular strings for data have no issue.

The fact still remains however that if I remove this field from my data contract or override it to always be null or always be a string representation like this:

item.AvailableOn = if item.AvailableOn?  then "#{item.AvailableOn}" else ""

Which I do in the code I have looping through the json docs and indexing...then it works.
</comment><comment author="markmacgillivray" created="2011-10-13T22:57:19Z" id="2401316">Hello there, great software by the way, have been using for a while but am now having trouble with this error too.

I have a mapping like this:

"record" : {
    "record" : {
        "dynamic_templates" : [
            {
                "default" : {
                    "match" : "*",
                    "mapping" : {
                        "type" : "multi_field",
                        "fields" : {
                            "{name}" : {"type" : "{dynamic_type}", "index" : "analyzed", "store" : "no"},
                            "exact" : {"type" : "{dynamic_type}", "index" : "not_analyzed", "store" : "yes"}
                        }
                    }
                }
            }
        ]
    }
}

but when I send it items like this:

[{'url': 'http://dx.doi.org/10.1007/3-540-34266-4_1'}]
[{'url': 'http://stat.berkeley.edu/users/pitman/AOP445.pdf', 'anchor': '[pdf]'}, {'url': 'http://projecteuclid.org/euclid.aop/1253539862', 'anchor': '[Project', 'format': 'Euclid]'}]
[{'url': 'http://arxiv.org/abs/0910.0405', 'anchor': 'arXiv'}, {'url': 'http://projecteuclid.org/euclid.bj/1297173851', 'anchor': 'Project', 'format': 'Euclid'}]
[{'url': 'http://www.bibkn.org/bibjson/index.html'}]

They index fine except for that last one. (These are a subset of examples from a batch of hundreds, some of which get indexed and others of which fail.)

Here is the error:

MapperParsingException: object_mapper [links] tried to parse as object, but got EOF, has a concrete value been provided to it?

I spent ages trying to strip out any odd characters that might be causing an EOF, but there are none...
</comment><comment author="markmacgillivray" created="2011-10-27T00:41:16Z" id="2538486">I have avoided this issue for my particular version of this problem by changing my dynamic mapping to apply to string types, thus avoiding the issue on objects (which still get mapped properly anyway). 
</comment><comment author="chrislovecnm" created="2012-07-26T14:43:55Z" id="7279513">I am having this issue as well. Has there been any progress?  The data that I am store does not have any objects inside it.
</comment><comment author="ramv" created="2012-07-26T15:01:57Z" id="7280016">This happens because the data format that twitter returns is not consistent. Sometimes the data contains an object, sometimes it is an array of objects. So you need to check which item in is causing the issue and fix it up before indexing.
On Jul 26, 2012, at 7:43 AM, Chris Love wrote:

&gt; I am having this issue as well. Has there been any progress?  The data that I am store does not have any objects inside it.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1331#issuecomment-7279513
</comment><comment author="chrislovecnm" created="2012-07-26T20:03:26Z" id="7288708">@ramv the data I have in it does not have ANY objects.... Ideas?
</comment><comment author="ramv" created="2012-07-26T20:05:45Z" id="7288777">can you share example data? It is very hard to debug without looking at the data you are trying to index.
</comment><comment author="chrislovecnm" created="2012-07-26T20:13:20Z" id="7288957">Happy too.  Maybe the dates are giving me grief ... Not too familiar with your JSON parsing :D  Would the mapping be helpful as well? How do I get that?

&lt;code&gt;
{
    "gateway": "Inspire",
    "state": "CO",
    "address1": "my address",
    "address2": null,
    "publisherCampaignGuid": "0886d40b-4f97-4193-a211-920dee663990",
    "gatewayResponseCode": null,
    "city": "Littleton",
    "amount": "100.00",
    "transactionState": "approved",
    "publisherName": null,
    "sponsorName": null,
    "totalAmount": null,
    "sponsorCampaignGuid": null,
    "gatewayResponseMessage": null,
    "firstName": "Christopher",
    "zip": "80125",
    "lastName": "Love",
    "nonProfitName": null,
    "gatewayTransactionId": "1664247231",
    "paymentType": null,
    "processingFeePercentage": "0.085",
    "lastUpdated": "2012-07-26T01: 30: 55.800Z",
    "nonProfitCampaignGuid": "7961eca6-69a5-49d4-8d51-4f5b680b422e",
    "country": "UnitedStates",
    "guid": "523ce9ee-3a43-4317-a5d7-27b1c03c5d69",
    "email": "wow@wow.com",
    "dateCreated": "2012-07-26T01: 30: 55.800Z",
    "sponsorMatchPercentage": "0"
}
&lt;/code&gt;
&lt;code&gt;
DEBUG [elasticsearch[Hurricane][bulk][T#3]] 2012-07-26 01:26:07,726 Log4jESLogger.java (line 99) [Hurricane] [com.igive][4] failed to execute bulk item (index) index {[com.igive][transaction][394], source[{"gateway":"Inspire","state":"CO","address1":"my address,"address2":null,"publisherCampaignGuid":"0886d40b-4f97-4193-a211-920dee663990","gatewayResponseCode":null,"city":"Littleton","amount":"100.00","transactionState":"approved","publisherName":null,"sponsorName":null,"totalAmount":null,"sponsorCampaignGuid":null,"gatewayResponseMessage":null,"firstName":"Christopher","zip":"80125","lastName":"Love","nonProfitName":null,"gatewayTransactionId":"1664247231","paymentType":null,"processingFeePercentage":"0.085","lastUpdated":"2012-07-26T01:30:55.800Z","nonProfitCampaignGuid":"7961eca6-69a5-49d4-8d51-4f5b680b422e","country":"United States                     ","guid":"523ce9ee-3a43-4317-a5d7-27b1c03c5d69","email":"wow@wow.com","dateCreated":"2012-07-26T01:30:55.800Z","sponsorMatchPercentage":"0"}]}
org.elasticsearch.index.mapper.MapperParsingException: object mapping for [transaction] tried to parse as object, but got EOF, has a concrete value been provided to it?
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:447)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:437)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:311)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:157)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
&lt;/code&gt;
</comment><comment author="ramv" created="2012-07-26T20:31:44Z" id="7289455">```
"address2": null,
```

...
   "publisherName": null,
    "sponsorName": null,
    "totalAmount": null,
    "sponsorCampaignGuid": null,
    "gatewayResponseMessage": null,

By assigning 'null' to these keys you are asserting that these keys are
objects.
Perhaps in an earlier index operation, you specified them to be strings or
integers? Can you try indexing the document after replacing the null values
with empty strings ""
ES creates a mapping by default when the first document is indexed. When
the type of keys changes in the subsequent index operations, ES will throw
this error.

## 

Best Regards,

Ram Viswanadha
</comment><comment author="chrislovecnm" created="2012-07-26T21:07:11Z" id="7290414">@ramv grrr ... it is the grails plugin not me :D Will take a look. DOH. How do I clear out the mapping?
</comment><comment author="ramv" created="2012-07-26T21:13:32Z" id="7290614">curl -XDELETE http://localhost:9200/{index}/{type}/_mapping
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rivers: Close rivers early allowing them to still do index operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1330</link><project id="" key="" /><description>Rivers: Close rivers early allowing them to still do index operations
</description><key id="1628869">1330</key><summary>Rivers: Close rivers early allowing them to still do index operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-12T22:59:59Z</created><updated>2011-09-12T23:16:03Z</updated><resolved>2011-09-12T23:16:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalNode.java</file></files><comments><comment>Rivers: Close rivers early allowing them to still do index operations, closes #1330.</comment></comments></commit></commits></item><item><title>Improbable NullPointerException when calling isDeleted on a closing ReadOnlySegmentReader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1329</link><project id="" key="" /><description>I can't beleive it happened to me, I thought this only happens in textbooks:

```
[2011-09-12 13:55:03,796][WARN ][index.cache.bloom.simple ] [Brain Cell] [yesdb] failed to load bloom filter for [_uid]
java.lang.NullPointerException
    at org.apache.lucene.index.ReadOnlySegmentReader.isDeleted(ReadOnlySegmentReader.java:34)
    at org.elasticsearch.index.cache.bloom.simple.SimpleBloomCache$BloomFilterLoader.run(SimpleBloomCache.java:191)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
```

It looks like the `deletedDocs` got removed in the middle of the statement. The fix could be wrapping `reader.isDeleted(termDocs.doc())` with something like  `try {...} catch (NullPointerException e) {throw new AlreadyClosedException()}`. I wanted to try to patch it but couldn't get gradle working (mysterious `ClassNotFoundException: groovy.lang.GroovyObject`).
</description><key id="1628065">1329</key><summary>Improbable NullPointerException when calling isDeleted on a closing ReadOnlySegmentReader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">losomo</reporter><labels /><created>2011-09-12T21:20:51Z</created><updated>2011-09-12T22:16:29Z</updated><resolved>2011-09-12T22:16:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-12T22:16:01Z" id="2076089">Actually, this can happen if the reader gets closed from under the feet of the async process of loading a bloom filter, which is certainly possible (not a bug). In 0.17.7 (current 0.17 branch) and master, a failure is only logged if the reader is still opened... .
</comment><comment author="kimchy" created="2011-09-12T22:16:29Z" id="2076094">I will close this issue since its fixed in master / 0.17 branch, comment if you think it does not make sense :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Build fails on Windows at :plugins-lang-groovy:compileGroovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1328</link><project id="" key="" /><description>I keep hitting this:
...
:plugins-lang-groovy:compileJava
Download http://repo1.maven.org/maven2/org/codehaus/groovy/groovy-all/1.8.1/groovy-all-1.8.1.pom
Download http://repo1.maven.org/maven2/org/codehaus/groovy/groovy-all/1.8.1/groovy-all-1.8.1.jar
:plugins-lang-groovy:compileGroovy
[ant:groovyc] URI has an authority component

FAILURE: Build failed with an exception.
- What went wrong:
  Execution failed for task ':plugins-lang-groovy:compileGroovy'.
  Cause: Forked groovyc returned error code: 1

Any idea what's going on here?
</description><key id="1626817">1328</key><summary>Build fails on Windows at :plugins-lang-groovy:compileGroovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">james0tucson</reporter><labels /><created>2011-09-12T19:04:35Z</created><updated>2011-09-13T21:01:32Z</updated><resolved>2011-09-13T19:48:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-12T22:24:11Z" id="2076162">Strange..., not sure what is going on... .
</comment><comment author="james0tucson" created="2011-09-12T22:44:47Z" id="2076322">We think that it may be a Gradle problem and not so much an ES problem.  It could be related to the maximum size of the environment that can be passed to a forked process (whatever Windows does for "fork").  I've tried building it under a Windows 7 Ent SP1 command shell using JDK 1.6.0_27, and have also tried it under a Cygwin shell (with the same result.)  Nearly all my system knowledge is in Linux and OSX, so I'm not the best person to troubleshoot a Windows issue.
Happy to provide more detail if you'd like.
</comment><comment author="kimchy" created="2011-09-13T10:19:46Z" id="2080229">It might be a gradle problem, but peopel should be able to build elasticsearch, so its elasticsearch problem as well :). How do you execute the build? Maybe you have another gradle version installed? Based on this issue: http://issues.gradle.org/browse/GRADLE-324, we should get more output since gradle version 0.7.
</comment><comment author="james0tucson" created="2011-09-13T16:40:08Z" id="2083719">Here's some more info and the tail of the log:

---

## Gradle 1.0-milestone-3

Gradle build time: Monday, 25 April 2011 5:40:11 PM EST
Groovy: 1.7.10
Ant: Apache Ant(TM) version 1.8.2 compiled on December 20 2010
Ivy: 2.2.0
JVM: 1.6.0_27 (Sun Microsystems Inc. 20.2-b06)
OS: Windows 7 6.1 amd64

...

09:28:31.220 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] Class org.codehaus.groovy.ant.Groovyc loaded from parent loader (parentFirst)
09:28:31.249 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter]  +Datatype groovyc org.codehaus.groovy.ant.Groovyc
09:28:31.291 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] Class org.apache.tools.ant.taskdefs.condition.Or loaded from parent loader (parentFirst)
09:28:31.292 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] Class org.apache.tools.ant.types.resources.selectors.Or loaded from parent loader (parentFirst)
09:28:31.293 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] Class org.apache.tools.ant.taskdefs.condition.And loaded from parent loader (parentFirst)
09:28:31.293 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] Class org.apache.tools.ant.types.resources.selectors.And loaded from parent loader (parentFirst)
09:28:31.425 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] fileset: Setup scanner in dir C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy with patternSet{ includes: [] excludes: [] }
09:28:31.446 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\client\GAdminClient.groovy added as org\elasticsearch\groovy\client\GAdminClient.class doesn't exist.
09:28:31.447 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\client\GClient.groovy added as org\elasticsearch\groovy\client\GClient.class doesn't exist.
09:28:31.447 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\client\GClusterAdminClient.groovy added as org\elasticsearch\groovy\client\GClusterAdminClient.class doesn't exist.
09:28:31.448 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\client\GIndicesAdminClient.groovy added as org\elasticsearch\groovy\client\GIndicesAdminClient.class doesn't exist.
09:28:31.448 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\action\GActionFuture.java skipped - don't know how to handle it
09:28:31.448 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\common\xcontent\GXContentBuilder.groovy added as org\elasticsearch\groovy\common\xcontent\GXContentBuilder.class doesn't exist.
09:28:31.449 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\node\GNode.groovy added as org\elasticsearch\groovy\node\GNode.class doesn't exist.
09:28:31.449 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\node\GNodeBuilder.groovy added as org\elasticsearch\groovy\node\GNodeBuilder.class doesn't exist.
09:28:31.450 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GAdminClient.groovy skipped - don't know how to handle it
09:28:31.450 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GClient.groovy skipped - don't know how to handle it
09:28:31.450 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GClusterAdminClient.groovy skipped - don't know how to handle it
09:28:31.450 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GIndicesAdminClient.groovy skipped - don't know how to handle it
09:28:31.451 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] org\elasticsearch\groovy\client\action\GActionFuture.java added as org\elasticsearch\groovy\client\action\GActionFuture.class doesn't exist.
09:28:31.451 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\common\xcontent\GXContentBuilder.groovy skipped - don't know how to handle it
09:28:31.451 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\node\GNode.groovy skipped - don't know how to handle it
09:28:31.451 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\node\GNodeBuilder.groovy skipped - don't know how to handle it
09:28:31.451 [INFO] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] Compiling 8 source files to C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\build\classes\main
09:28:31.453 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] Compilation arguments:
09:28:31.453 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] C:\Program Files\Java\jdk1.6.0_27\jre\bin\java
-classpath
C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\jarjar\build\libs\jarjar-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\lib\sigar\sigar-1.6.4.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-javadoc.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-sources.jar;\hqfile2\home$\coyote.gradle\cache\jline\jline\jars\jline-0.9.94.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-api\jars\slf4j-api-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-log4j12\jars\slf4j-log4j12-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\log4j\log4j\bundles\log4j-1.2.16.jar;\hqfile2\home$\coyote.gradle\cache\net.java.dev.jna\jna\jars\jna-3.2.7.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-core\jars\lucene-core-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-analyzers\jars\lucene-analyzers-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-queries\jars\lucene-queries-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-memory\jars\lucene-memory-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-highlighter\jars\lucene-highlighter-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.codehaus.groovy\groovy-all\jars\groovy-all-1.8.1.jar;C:\es-1328\gradle-1.0-milestone-3\lib\commons-cli-1.2.jar
-Dfile.encoding=Cp1252
org.codehaus.groovy.tools.FileSystemCompiler
--classpath
C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\jarjar\build\libs\jarjar-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\lib\sigar\sigar-1.6.4.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-javadoc.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-sources.jar;\hqfile2\home$\coyote.gradle\cache\jline\jline\jars\jline-0.9.94.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-api\jars\slf4j-api-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-log4j12\jars\slf4j-log4j12-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\log4j\log4j\bundles\log4j-1.2.16.jar;\hqfile2\home$\coyote.gradle\cache\net.java.dev.jna\jna\jars\jna-3.2.7.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-core\jars\lucene-core-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-analyzers\jars\lucene-analyzers-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-queries\jars\lucene-queries-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-memory\jars\lucene-memory-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-highlighter\jars\lucene-highlighter-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.codehaus.groovy\groovy-all\jars\groovy-all-1.8.1.jar;C:\es-1328\gradle-1.0-milestone-3\lib\commons-cli-1.2.jar
-j
-Fg
-Jsource=1.6
-Jtarget=1.6
-d
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\build\classes\main
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GAdminClient.groovy
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GClient.groovy
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GClusterAdminClient.groovy
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GIndicesAdminClient.groovy
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\common\xcontent\GXContentBuilder.groovy
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\node\GNode.groovy
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\node\GNodeBuilder.groovy
C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\action\GActionFuture.java
09:28:31.460 [DEBUG] [org.gradle.api.internal.project.ant.AntLoggingAdapter] Execute:Java13CommandLauncher: Executing 'C:\Program Files\Java\jdk1.6.0_27\jre\bin\java' with arguments:
'-classpath'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\jarjar\build\libs\jarjar-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\lib\sigar\sigar-1.6.4.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-javadoc.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-sources.jar;\hqfile2\home$\coyote.gradle\cache\jline\jline\jars\jline-0.9.94.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-api\jars\slf4j-api-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-log4j12\jars\slf4j-log4j12-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\log4j\log4j\bundles\log4j-1.2.16.jar;\hqfile2\home$\coyote.gradle\cache\net.java.dev.jna\jna\jars\jna-3.2.7.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-core\jars\lucene-core-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-analyzers\jars\lucene-analyzers-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-queries\jars\lucene-queries-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-memory\jars\lucene-memory-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-highlighter\jars\lucene-highlighter-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.codehaus.groovy\groovy-all\jars\groovy-all-1.8.1.jar;C:\es-1328\gradle-1.0-milestone-3\lib\commons-cli-1.2.jar'
'-Dfile.encoding=Cp1252'
'org.codehaus.groovy.tools.FileSystemCompiler'
'--classpath'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\jarjar\build\libs\jarjar-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\lib\sigar\sigar-1.6.4.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-javadoc.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6.jar;C:\es-1328\elasticsearch-elasticsearch-b555e52\modules\elasticsearch\build\libs\elasticsearch-0.17.6-sources.jar;\hqfile2\home$\coyote.gradle\cache\jline\jline\jars\jline-0.9.94.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-api\jars\slf4j-api-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\org.slf4j\slf4j-log4j12\jars\slf4j-log4j12-1.5.11.jar;\hqfile2\home$\coyote.gradle\cache\log4j\log4j\bundles\log4j-1.2.16.jar;\hqfile2\home$\coyote.gradle\cache\net.java.dev.jna\jna\jars\jna-3.2.7.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-core\jars\lucene-core-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-analyzers\jars\lucene-analyzers-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-queries\jars\lucene-queries-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-memory\jars\lucene-memory-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.apache.lucene\lucene-highlighter\jars\lucene-highlighter-3.3.0.jar;\hqfile2\home$\coyote.gradle\cache\org.codehaus.groovy\groovy-all\jars\groovy-all-1.8.1.jar;C:\es-1328\gradle-1.0-milestone-3\lib\commons-cli-1.2.jar'
'-j'
'-Fg'
'-Jsource=1.6'
'-Jtarget=1.6'
'-d'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\build\classes\main'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GAdminClient.groovy'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GClient.groovy'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GClusterAdminClient.groovy'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\GIndicesAdminClient.groovy'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\common\xcontent\GXContentBuilder.groovy'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\node\GNode.groovy'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\node\GNodeBuilder.groovy'
'C:\es-1328\elasticsearch-elasticsearch-b555e52\plugins\lang\groovy\src\main\groovy\org\elasticsearch\groovy\client\action\GActionFuture.java'

The ' characters around the executable and arguments are
not part of the command.
09:28:33.365 [WARN] [org.gradle.api.internal.project.ant.AntLoggingAdapter] [ant:groovyc] URI has an authority component
09:28:33.690 [DEBUG] [org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter] Finished executing task ':plugins-lang-groovy:compileGroovy'
09:28:33.696 [ERROR] [org.gradle.BuildExceptionReporter] 
09:28:33.697 [ERROR] [org.gradle.BuildExceptionReporter] FAILURE: Build failed with an exception.
09:28:33.697 [ERROR] [org.gradle.BuildExceptionReporter] 
09:28:33.697 [ERROR] [org.gradle.BuildExceptionReporter] \* What went wrong:
09:28:33.697 [ERROR] [org.gradle.BuildExceptionReporter] Execution failed for task ':plugins-lang-groovy:compileGroovy'.
09:28:33.698 [ERROR] [org.gradle.BuildExceptionReporter] Cause: Forked groovyc returned error code: 1
09:28:33.698 [ERROR] [org.gradle.BuildExceptionReporter] 
09:28:33.698 [ERROR] [org.gradle.BuildExceptionReporter] \* Exception is:
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter] org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':plugins-lang-groovy:compileGroovy'.
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:71)
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:48)
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:34)
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:55)
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:57)
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:41)
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:51)
09:28:33.699 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:52)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:42)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.AbstractTask.execute(AbstractTask.java:237)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.execution.DefaultTaskGraphExecuter.executeTask(DefaultTaskGraphExecuter.java:167)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.execution.DefaultTaskGraphExecuter.doExecute(DefaultTaskGraphExecuter.java:160)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.execution.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:78)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.execution.TaskNameResolvingBuildExecuter.execute(TaskNameResolvingBuildExecuter.java:113)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.execution.DelegatingBuildExecuter.execute(DelegatingBuildExecuter.java:54)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.execution.DelegatingBuildExecuter.execute(DelegatingBuildExecuter.java:54)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:158)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:112)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:80)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.RunBuildAction.execute(RunBuildAction.java:41)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.RunBuildAction.execute(RunBuildAction.java:27)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.ExceptionReportingAction.execute(ExceptionReportingAction.java:32)
09:28:33.700 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.ExceptionReportingAction.execute(ExceptionReportingAction.java:21)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.CommandLineActionFactory$WithLoggingAction.execute(CommandLineActionFactory.java:219)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.CommandLineActionFactory$WithLoggingAction.execute(CommandLineActionFactory.java:203)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.Main.execute(Main.java:55)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.Main.main(Main.java:40)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at java.lang.reflect.Method.invoke(Method.java:597)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.ProcessBootstrap.runNoExit(ProcessBootstrap.java:46)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.ProcessBootstrap.run(ProcessBootstrap.java:28)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.launcher.GradleMain.main(GradleMain.java:24)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter] Caused by: : Forked groovyc returned error code: 1
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.ant.Groovyc.compile(Groovyc.java:871)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.ant.Groovyc.execute(Groovyc.java:606)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
09:28:33.701 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at java.lang.reflect.Method.invoke(Method.java:597)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.util.AntBuilder.performTask(AntBuilder.java:260)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.util.AntBuilder.nodeCompleted(AntBuilder.java:220)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.project.ant.BasicAntBuilder.nodeCompleted(BasicAntBuilder.java:71)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at java.lang.reflect.Method.invoke(Method.java:597)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoCachedMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:229)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:52)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:54)
09:28:33.702 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:128)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.project.AntBuilderDelegate.nodeCompleted(DefaultIsolatedAntBuilder.groovy:137)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.util.BuilderSupport.doInvokeMethod(BuilderSupport.java:147)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.util.BuilderSupport.invokeMethod(BuilderSupport.java:64)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeOnDelegationObjects(ClosureMetaClass.java:403)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:349)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:44)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:153)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.compile.AntGroovyCompiler$_execute_closure1.doCall(AntGroovyCompiler.groovy:66)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at java.lang.reflect.Method.invoke(Method.java:597)
09:28:33.703 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.Closure.call(Closure.java:282)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.Closure.call(Closure.java:295)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.util.ConfigureUtil.configure(ConfigureUtil.java:61)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.util.ConfigureUtil.configure(ConfigureUtil.java:31)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.util.ConfigureUtil$configure.call(Unknown Source)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:128)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.project.DefaultIsolatedAntBuilder.execute(DefaultIsolatedAntBuilder.groovy:98)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.project.IsolatedAntBuilder$execute.call(Unknown Source)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
09:28:33.704 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:124)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.compile.AntGroovyCompiler.execute(AntGroovyCompiler.groovy:63)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.compile.IncrementalJavaSourceCompiler.execute(IncrementalJavaSourceCompiler.java:73)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.tasks.compile.GroovyCompile.compile(GroovyCompile.java:60)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at java.lang.reflect.Method.invoke(Method.java:597)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1058)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.BeanDynamicObject.invokeMethod(BeanDynamicObject.java:158)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.CompositeDynamicObject.invokeMethod(CompositeDynamicObject.java:93)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.tasks.compile.GroovyCompile_Decorated.invokeMethod(Unknown Source)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at groovy.lang.GroovyObject$invokeMethod.call(Unknown Source)
09:28:33.705 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.util.ReflectionUtil.invoke(ReflectionUtil.groovy:23)
09:28:33.706 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$2.execute(AnnotationProcessingTaskFactory.java:129)
09:28:33.706 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$2.execute(AnnotationProcessingTaskFactory.java:127)
09:28:33.706 [ERROR] [org.gradle.BuildExceptionReporter]    at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:63)
09:28:33.706 [ERROR] [org.gradle.BuildExceptionReporter]    ... 33 more
09:28:33.706 [ERROR] [org.gradle.BuildExceptionReporter] 
09:28:33.707 [LIFECYCLE] [org.gradle.BuildResultLogger] 
09:28:33.707 [LIFECYCLE] [org.gradle.BuildResultLogger] BUILD FAILED
09:28:33.708 [LIFECYCLE] [org.gradle.BuildResultLogger] 
09:28:33.708 [LIFECYCLE] [org.gradle.BuildResultLogger] Total time: 5 mins 8.517 secs
</comment><comment author="james0tucson" created="2011-09-13T19:48:42Z" id="2085681">This is resolved by setting GRADLE_USER_HOME to a shorter path.  I am firmly convinced that the problem is related to the length of the classpath that gets passed by gradle to build processes.  A shorter GRADLE_USER_HOME path fixes the build for us.
</comment><comment author="kimchy" created="2011-09-13T21:01:32Z" id="2086554">Thanks for chasing this up... . Will try and see if its something that can be fixed in gradle once I have some time :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pass custom uris params to a couchdb river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1327</link><project id="" key="" /><description>If it's possible to pass custom filters params it isn't possible yet to pass custom uri params to the _changes url. I've such a need for a custom auth handler based on these uri parameters.
</description><key id="1626065">1327</key><summary>Pass custom uris params to a couchdb river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benoitc</reporter><labels /><created>2011-09-12T17:46:52Z</created><updated>2013-04-05T15:42:13Z</updated><resolved>2013-04-05T15:42:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-14T14:30:17Z" id="2093671">There is an option to do so, explained here: http://www.elasticsearch.org/guide/reference/river/couchdb.html under filter section. Is that what you are after?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consistent response format for dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1326</link><project id="" key="" /><description>The response format of a date in a search that specifies `fields` varies based on whether the field is marked `store` or not. If it is not stored, it will read from the `source`, and thus be in the same format as the `source`. If it is `stored`, it will read from the stored field and cast it to the default date format.

Example:

```
$ curl http://localhost:9200/test/test/_mapping?pretty=true
{
  "test" : {
    "properties" : {
      "created_at2" : {
        "store" : "yes",
        "format" : "dateOptionalTime",
        "type" : "date"
      },
      "created_at" : {
        "format" : "dateOptionalTime",
        "type" : "date"
      }
    }
  }
}

$ curl -XPUT http://localhost:9200/test/test/1 -d '{"created_at":1315847893000,"created_at2":1315847893000}'
{"ok":true,"_index":"test","_type":"test","_id":"1","_version":1}


$ curl 'http://localhost:9200/test/test/_search?fields=created_at,created_at2&amp;pretty=true'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 1.0,
      "fields" : {
        "created_at" : 1315847893000,
        "created_at2" : "2011-09-12T17:18:13.000Z"
      }
    } ]
  }
}
```
</description><key id="1625856">1326</key><summary>Consistent response format for dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels /><created>2011-09-12T17:26:27Z</created><updated>2014-07-08T12:30:57Z</updated><resolved>2014-07-08T12:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="damienalexandre" created="2013-07-01T16:56:33Z" id="20294982">:+1:  (still happening in 0.9)
</comment><comment author="loris" created="2013-07-01T16:57:32Z" id="20295053">:+1: 
</comment><comment author="clintongormley" created="2014-07-08T12:30:56Z" id="48329559">This has been fixed in #3301 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indexation of document causes NullPointerException (on Linux) or ES process hanging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1325</link><project id="" key="" /><description>Affected ES v0.15.1, v0.16.2, v0.17.6.
Here is test case:

curl -XDELETE 'http://localhost:9200/test_contacts/'
curl -XPUT 'http://localhost:9200/test_contacts/'

curl -XPUT http://localhost:9200/test_contacts/contact/_mapping -d "{\"contact\": {\"_all\": {\"enabled\": false}, \"dynamic\": false, \"properties\": {\"any_fields\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"reverse_fields\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"last_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"twitter\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_id\": {\"index\": \"not_analyzed\", \"type\": \"string\"}, \"user_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"linkedin\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_id\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"street\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"skype\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"city\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"first_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"zip\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"title\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"state\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"leadSource\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"company_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"department\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"email\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"website\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"description\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"accountNumber\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"assistant\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"phone\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"facebook\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_id\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"leadType\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"dates\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"country\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"assistantPhone\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"reverse_any_fields\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"tag\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"created\": {\"index\": \"not_analyzed\", \"type\": \"date\", \"format\": \"yyyy-MM-dd'T'HH:mm:ssZ\"}, \"company_id\": {\"index\": \"not_analyzed\", \"type\": \"string\"}, \"fields\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"last_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"twitter\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_id\": {\"index\": \"not_analyzed\", \"type\": \"string\"}, \"user_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"linkedin\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_id\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"street\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"skype\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"city\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"first_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"zip\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"title\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"state\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"leadSource\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"company_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"department\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"email\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"website\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"description\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"accountNumber\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"assistant\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"phone\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"facebook\": {\"dynamic\": false, \"type\": \"object\", \"properties\": {\"profile\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_id\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"user_name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"leadType\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"dates\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"name\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"country\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}, \"assistantPhone\": {\"index\": \"not_analyzed\", \"type\": \"string\", \"omit_term_freq_and_positions\": \"true\"}}}, \"id\": {\"type\": \"string\", \"store\": \"yes\"}, \"is_account\": {\"index\": \"not_analyzed\", \"type\": \"boolean\"}, \"owner_id\": {\"index\": \"not_analyzed\", \"type\": \"string\"}}, \"_source\": {\"enabled\": false}}}"

curl -XPUT http://localhost:9200/test_contacts/contact/4e6dfd2dfa5bd81330000098 -d '{"reverse_any_fields": ["resu emos", "emos", "resu", "moc.niamod@rednaxela"], "any_fields": ["some user", "some", "user", "alexander@domain.com"], "created": "2011-09-12T12:38:05+0000", "reverse_fields": {"first_name": ["emos"], "last_name": ["resu"], "name": ["resu emos"], "email": ["moc.niamod@rednaxela"]}, "fields": {"first_name": ["some"], "last_name": ["user"], "name": ["some user"], "email": ["alexander@domain.com"]}, "owner_id": "4e6dfd2bfa5bd81330000002", "tag": [], "company_id": "4b619ddffa5bd81b71000002", "is_account": false, "id": "4e6dfd2dfa5bd81330000098"}'
</description><key id="1623819">1325</key><summary>Indexation of document causes NullPointerException (on Linux) or ES process hanging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">an2deg</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-12T13:48:21Z</created><updated>2011-09-12T18:36:11Z</updated><resolved>2011-09-12T18:36:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-12T18:31:23Z" id="2073721">This happens because `email` is mapped as `object`, but is provided with a value ,will push a change to fail it with a proper exception.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file></files><comments><comment>Indexation of document causes NullPointerException (on Linux) or ES process hanging, closes #1325.</comment></comments></commit></commits></item><item><title>XContentBuilder.field : StackOverflowError with Integer[] parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1324</link><project id="" key="" /><description>After upgrading to 0.17.x from a 0.15.x version in a grails project, I've encountered a StackOverflowError when trying to add a Integer array (Integer[]) using the field() method.

Code sample to reproduce (groovy) :

```
  def json = jsonBuilder().startObject()
  Integer[] listofInteger = [1, 2, 3, 4]
  json.field("test2", listofInteger)
  json.endObject()
```

Error returned : 

```
Caused by: java.lang.StackOverflowError
at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:568)
at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:601)
at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:601)
at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:601)
   (...)
```

It will also occured when using a Float[] or Double[].
Building the array manually with the array() method will not trigger the error :

```
  def json = jsonBuilder().startObject()
  Integer[] listofInteger = [1, 2, 3, 4]
  json.startArray("test2")
  for (Object o : listofInteger) {
      json.value(o);
  }
  json.endArray()
  json.endObject()
```

The line https://github.com/elasticsearch/elasticsearch/blob/master/modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java#L631 is probably the reason of the issue since an Integer[] will be recognized as a Object[], and will just infinitely call the field(String, Object) method.
</description><key id="1623439">1324</key><summary>XContentBuilder.field : StackOverflowError with Integer[] parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mstein</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-12T12:56:54Z</created><updated>2011-09-12T19:45:08Z</updated><resolved>2011-09-12T19:45:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file></files><comments><comment>XContentBuilder.field : StackOverflowError with Integer[] parameters, closes #1324.</comment></comments></commit></commits></item><item><title>Nested Mapping: Nested object with a null value causes wrong indexing structure (resulting in wrong search responses)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1323</link><project id="" key="" /><description>Nested Mapping: Nested object with a null value causes wrong indexing structure (resulting in wrong search responses)
</description><key id="1621907">1323</key><summary>Nested Mapping: Nested object with a null value causes wrong indexing structure (resulting in wrong search responses)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-12T08:22:59Z</created><updated>2011-09-12T08:23:33Z</updated><resolved>2011-09-12T08:23:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/nested/NestedMappingTests.java</file></files><comments><comment>Nested Mapping: Nested object with a null value causes wrong indexing structure (resulting in wrong search responses), closes #1323.</comment></comments></commit></commits></item><item><title>Allow merging shard replicas based on document versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1322</link><project id="" key="" /><description>Would it be possible to allow shard replicas to be merged together after a network partition based on the highest version numbers? 

Clearly this would be optional, as versions would collide or become out-of-sequence in a default configuration. However, for people who desire this functionality, it's easy to generate globally unique, sequential version numbers from the content that won't collide (microsecond timestamp + truncated hash of the doc) on the client side and feed these as version numbers into ElasticSearch.

The only caveat I can think of is that deletions aren't as straightforward. It seems like the system Cassandra uses, which replaces the stored value with a "tombstone" that represents the deletion. The tombstones get cleaned up after a configurable grace time  period. I believe the grace period in Cassandra is 10 days by default -- clearly a very conservative value. This system seems to work well. The delete API already takes a version number, so that could be used to version the tombstones. Again, I think this should be totally optional as keeping tombstones might effect the performance &amp; data size of some existing use cases that don't need this feature. If it isn't optional or doesn't need to be, perhaps the default grace period could be very short, like an hour?

Mergeable replicas would allow the an ElasticSearch cluster to operate as an eventually consistent "AP" system and could make working with multiple datacenters very feasible out-of-the-box. Currently it works as a "CP" system, so the quorum side of a network partition takes precedence and the other side(s) become effectively read-only. It would be nice to be able to continue to receive writes on the non-quorum side(s) and merge these writes into the cluster once it is healed.

Currently my strategy involves building a completely isolated ES cluster at each datacenter and distributing index updates using a message queue / worker system. Each datacenter has it's own isolated MQ and worker group. Index updates get inserted into a fan-out exchange which inserts an update message in each individual datacenter's outbound queue, which the workers pick up and perform the index update. This is complicated and was error-prone to start, and also introduces adds non-trivial extra cost and latency to the process.
</description><key id="1616032">1322</key><summary>Allow merging shard replicas based on document versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">rbranson</reporter><labels /><created>2011-09-10T22:54:57Z</created><updated>2015-09-19T16:54:53Z</updated><resolved>2015-09-19T16:54:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T16:54:53Z" id="141687478">I've left this issue open for a long time awaiting discussion, and I've chatted about it internally a bit, but found little support for this.  I think we've gone firmly down our current path and are unlikely to change tack, especially with the added complexity this would require.

I'm going to close this issue.  Who knows, maybe in the future once our current plans are implemented, we might rethink this but for now I don't think it is going anywhere.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Thread Pool: Blocking thread pool type configuration fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1321</link><project id="" key="" /><description>Configuring `blocking` thread pool fails. Fix configuration parameter to properly read (and set) a `queue_size` parameter (defaults to `1000`).
</description><key id="1615910">1321</key><summary>Thread Pool: Blocking thread pool type configuration fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-10T22:06:19Z</created><updated>2011-09-10T22:07:07Z</updated><resolved>2011-09-10T22:07:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file></files><comments><comment>Thread Pool: Blocking thread pool type configuration fails, closes #1321.</comment></comments></commit></commits></item><item><title>Query with stopwords executed directly against a type fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1320</link><project id="" key="" /><description>Hiya

This query on the analyzed sub-field of a multi-field throws an NPE when the query includes stopwords:

https://gist.github.com/1108986
</description><key id="1614075">1320</key><summary>Query with stopwords executed directly against a type fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-10T12:21:03Z</created><updated>2011-09-10T21:53:53Z</updated><resolved>2011-09-10T21:53:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/SearchContextException.java</file></files><comments><comment>Query with stopwords executed directly against a type fails, closes #1320.</comment></comments></commit></commits></item><item><title>Add support for RackSpace via jclouds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1319</link><project id="" key="" /><description>Cloud provider integration via jclouds was removed and replaced with dedicated AWS cloud support due to unknown reasons. Support for at least RackSpace is needed and doing this through jclouds would enable other cloud providers to be used as well.

Any details you may be able to share on why the jclouds-based integration was removed would be great, as this would also prevent/warn people that would like to fork and contribute back a jclouds or other solution.

See also:

https://groups.google.com/group/elasticsearch/browse_thread/thread/f64f4319a6330646

https://github.com/elasticsearch/elasticsearch/issues/733
https://github.com/elasticsearch/elasticsearch/issues/163
</description><key id="1607846">1319</key><summary>Add support for RackSpace via jclouds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshdevins</reporter><labels /><created>2011-09-09T14:12:57Z</created><updated>2014-07-08T12:28:13Z</updated><resolved>2014-07-08T12:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="joshdevins" created="2011-09-12T08:28:26Z" id="2068744">Note: as mentioned on the newsgroup, this is an optimization. Using unicast discovery and local gateway will work just fine on any cloud provider.
</comment><comment author="grkvlt" created="2011-09-16T04:54:47Z" id="2111880">I am interested in helping to get jclouds integration working, let me know if there is anything I can do
</comment><comment author="bryangreen" created="2011-12-20T02:16:50Z" id="3213232">+1
</comment><comment author="clintongormley" created="2014-07-08T12:28:13Z" id="48329300">No discussion on this issue for 3 years. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add default ttl value support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1318</link><project id="" key="" /><description /><key id="1606606">1318</key><summary>add default ttl value support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-09-09T10:32:34Z</created><updated>2014-07-16T21:56:11Z</updated><resolved>2011-09-09T12:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-09T12:35:20Z" id="2050732">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add time value definition of ttl inside source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1317</link><project id="" key="" /><description /><key id="1606319">1317</key><summary>add time value definition of ttl inside source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-09-09T09:35:25Z</created><updated>2014-07-16T21:56:12Z</updated><resolved>2011-09-09T10:10:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Per document TTL support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1316</link><project id="" key="" /><description>A lot of documents naturally come with an expiration date and it can be painful to handle yourself the task of deleting all expired documents regularly. 

Therefore a new field, called `_ttl` has been added. 
## enabled

By default it is disabled, in order to enable it, the following mapping should be defined:

``` javascript
{
    "tweet" : {
        "_ttl" : { "enabled" : true }
    }
}
```
## default

You can provide a per index/type default ttl value as follows:

``` javascript
{
    "tweet" : {
        "_ttl" : { "enabled" : true, "default" : "1d" }
    }
}
```

If you don't provide `_ttl` in your query or in the `_source` all tweets will have a TTL of 1 day.
## How it works

The `_ttl` field allows to attach a TTL to your documents. Note that the expiration date that will be set for a document with a provided TTL is relative to the `_timestamp` of the document, meaning it can be based on the time of indexing or on any time provided inside your `_source` document. Note also that the `_timestamp` field need not to be enabled in order for `_ttl` to work.

The provided `_ttl` value must be strictly positive and can be a number (in milliseconds) or any valid time value as shown in the following examples.

You can provided the `_ttl` externally in your indexing order or inside the `_source`:

&lt;pre&gt;
// One day  in ms
curl -XPUT 'http://localhost:9200/twitter/tweet/1?ttl=86400000' -d '{
    "user": "kimchy",
    "message": "Trying out elasticsearch, so far so good?"
}'
&lt;/pre&gt;


&lt;pre&gt;
// One day using a time value format
curl -XPUT 'http://localhost:9200/twitter/tweet/1?ttl=1d' -d '{
    "user": "kimchy",
    "message": "Trying out elasticsearch, so far so good?"
}'
&lt;/pre&gt;


&lt;pre&gt;
// One day inside the _source
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "_ttl": "1d",
    "user": "kimchy",
    "message": "Trying out elasticsearch, so far so good?"
}'
&lt;/pre&gt;


You can display the `_ttl` field value as follow:

&lt;pre&gt;
curl -XGET 'http://localhost:9200/twitter/tweet/_search?q=user:kimchy&amp;pretty=1&amp;fields=_source,_ttl'
&lt;/pre&gt;


Giving this output:

``` javascript
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.30685282,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 0.30685282, "_source" : {
    "_ttl": 86400000,
    "user": "kimchy",
    "message": "Trying out elasticsearch, so far so good?"
},
      "fields" : {
        "_ttl" : 86394472 // Note the _ttl value here returned in ms
      }
    } ]
  }
}
```

The `_ttl` value can also be displayed using realtime get, for example:

&lt;pre&gt;
curl -XGET 'http://localhost:9200/twitter/tweet/1?pretty=1&amp;fields=_source,_ttl'
&lt;/pre&gt;

## Documents expiration

Expired documents will be automatically deleted regularly.  You can dynamically set the `indices.ttl.interval` to fit your needs. The default value is `60s`.

The deletion orders are processed by bulk. You can set `indices.ttl.bulk_size` to fit your needs. The default value is `10000`.

Note that the expiration procedure handle versioning properly so if a document is updated between the collection of documents to expire and the delete order, the document won't be deleted.
</description><key id="1606249">1316</key><summary>Per document TTL support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-09T09:25:17Z</created><updated>2013-11-27T06:38:24Z</updated><resolved>2011-09-09T14:24:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-09T14:24:32Z" id="2051659">Implemented over several commits.
</comment><comment author="mikeasick" created="2011-10-27T16:05:41Z" id="2545463">Shay - it would be great if we could register for _ttl notifications since ES is taking the initiative to manage this. There may be other systems we might want to clear. 
</comment><comment author="kimchy" created="2011-10-27T19:41:42Z" id="2548330">Right, it would be nice. There isn't a mechanism in elasticsearch to "notify things". We will need to think about at one point or another, and once we have something built, then adding notifications on expirations is definitely possible.
</comment><comment author="mikeasick" created="2011-10-27T20:08:58Z" id="2548622">Makes sense - I've not dived into the percolation API's but on reading the description I thought there was a notification there when qualifying content was submitted. 

As a bit of an aside, do you offer support subscriptions for ES? I'm working with it now and expect to propose it as a solution in other situations You're doing a great job of releasing fixes and features, but there are some companies that insist on paying.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting - does not work with custom_score or boosting, closes #1314</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1315</link><project id="" key="" /><description>A bit messy but should be working fine. Needs to be reviewed.

Idea: Going forward it might be useful to add some interface that would be used by highlighting code. The interface would force ES specific Query implementations to return Query that can be understood by highlighting framework. For example it would be subQuery for some queries or the query that is returned by rewrite method. Then the messy logic based on `instanceof` could have been reduced.
</description><key id="1602700">1315</key><summary>Highlighting - does not work with custom_score or boosting, closes #1314</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-09-08T21:38:54Z</created><updated>2014-06-15T23:58:05Z</updated><resolved>2011-10-25T22:50:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-09-12T07:58:05Z" id="2068532">Though the pull request does not look very systematic there is probably not a better way how to handle this problem at the Lucene side: http://markmail.org/thread/tjeqb5g543khhwia
</comment><comment author="kimchy" created="2011-10-25T22:50:42Z" id="2524258">Yea, not for regular highlighter, but we hack it nicely in the FastVectorHighlighter. I will munge with this one a bit, and add support for more queries in the FVH one.
</comment><comment author="nkvoll" created="2011-11-05T20:57:10Z" id="2642413">This means highlighting when using the custom_score_filter query type will not work, with or without the FVH?

Is there any ticket open for getting highlighting to work with the rest of the query types?
</comment><comment author="kimchy" created="2011-11-10T08:18:42Z" id="2692617">@nkvoll it got fixed for all current types of queries, though when not using FVH and using plain highlighter, it will only work for the custom ones (like custom_filters_score) when they are the outmost ones (which is the common case).
</comment><comment author="awopython" created="2011-12-08T15:17:48Z" id="3063600">What version did this fix get merged into?
</comment><comment author="kimchy" created="2011-12-08T15:34:47Z" id="3063817">@awopython Its in latest, 0.18.5.
</comment><comment author="awopython" created="2011-12-08T16:45:36Z" id="3064848">thanks for the quick response.  just to verify, the above fix should allow highlighting for the following search result, correct?

{"highlight": 
    {"pre_tags": ["&lt;awo&gt;"], 
    "fragment_size": 250, 
    "number_of_fragments": 1, 
    "fields": {"content": {}, "tease": {}}, 
    "post_tags": ["&lt;/awo&gt;"]}, 
"query": 
    {"custom_score": 
        {"lang": "mvel", 
        "query": 
            {"query_string": {"query": "steak"}}, 
        "script": "_score*(doc.pub_date.longValue)"}}, "
facets": {
    "sites": {"terms": {"field": "sites", "size": 10}}}, 
"fields": ["title", "tease", "pub_date", "django_ct", "django_id", "type"]}

thanks again for your help!  After playing w/ swish-e for several years and indextank for a few months, we are really enjoying developing our search solution using elasticsearch.  Great work on a nice product!
</comment><comment author="kimchy" created="2011-12-08T17:06:01Z" id="3065215">The fix eventually done is not exactly what we have here, but yes, 0.18.5 should highlight this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting - does not work with custom_score or boosting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1314</link><project id="" key="" /><description>reported by: https://github.com/nkvoll

Highlighting does not seem to work with custom_score or boosting (and maybe others, but these were the one I just tested).

Example (from https://gist.github.com/9f003179c02ebe9b69ae ):

```
curl -XDELETE http://localhost:9200/test?pretty
curl -XPOST 'http://localhost:9200/test/my_type/1?pretty=1&amp;refresh=true' -d '
{
    "foo": {
        "bar": "this is bar"
    }
}'

# works as expected:
curl http://localhost:9200/test/_search?pretty -XPOST -d '{
    "query": {"query_string": {"query":"bar"}},
    "highlight": {"fields": {"foo.bar":{}}}
}'


# does not highlight:
curl http://localhost:9200/test/_search?pretty -XPOST -d '{
    "query": {
        "boosting": {
            "positive": {
                "query_string": {
                    "query":"bar"
                }
            },
            "negative": {"term": {"foo.bar": "baz"}},
            "negative_boost": 0.9
        }
    },
    "highlight": {"fields": {"foo.bar":{}}}
}'


# does not highlight either:
curl http://localhost:9200/test/_search?pretty -XPOST -d '{
    "query": {
        "custom_score": {
            "query": {
                "query_string": {
                    "query":"bar"
                }
            },
            "script": "_score"
        }
    },    
    "highlight": {"fields": {"foo.bar":{}}}
}'    
```
</description><key id="1602581">1314</key><summary>Highlighting - does not work with custom_score or boosting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>bug</label></labels><created>2011-09-08T21:25:59Z</created><updated>2013-03-11T09:31:13Z</updated><resolved>2013-03-11T09:31:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TwP" created="2012-09-28T18:49:37Z" id="8988146">Just want to report that highlighting does not work with a `boosting` query. It does not matter if `term_vectors` are used with the field or not. I'll work up an example and gist it in a bit.
</comment><comment author="s1monw" created="2013-02-18T22:47:55Z" id="13747460">FYI - this will be fixed once Lucene 4.2 is released with master. I fixed this in [LUCENE-4728](https://issues.apache.org/jira/browse/LUCENE-4728) by rewriting the query against the highlight reader. I will close this once 4.2 is available or once we fix this in master.
</comment><comment author="TwP" created="2013-02-20T03:57:56Z" id="13814656">@s1monw Thank you! :heart::sparkles:
</comment><comment author="jtreher" created="2013-03-01T15:04:58Z" id="14293269">@s1monw Yes, thanks a million. This will be a great fix.
</comment><comment author="s1monw" created="2013-03-07T11:47:54Z" id="14555548">FYI - I added tests for this issue in the 4.2 upgrade branch that I created to test the 4.2 RC candidate and all tests pass. so once 4.2 is out I merge this and we are good to go here!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Add tests for highlighting boost query.</comment></comments></commit></commits></item><item><title>CTRL+C is not working on windows XP and windows 7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1313</link><project id="" key="" /><description>See in mailing list :
https://groups.google.com/forum/?hl=en#!topic/elasticsearch/y4lNXh_APpQ
</description><key id="1602012">1313</key><summary>CTRL+C is not working on windows XP and windows 7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-09-08T20:25:39Z</created><updated>2014-07-16T21:56:13Z</updated><resolved>2011-09-09T07:31:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2011-09-08T20:35:32Z" id="2044352">Don't know if it was a good idea to add the line because I just saw that Shay suppress it with this commit : https://github.com/elasticsearch/elasticsearch/commit/845104dabc2e3a7bb5f93371eaaf9b6ae7a1dc28

So Is the automatic detection is not working only on Windows ?

David.
</comment><comment author="kimchy" created="2011-09-09T07:29:38Z" id="2048868">Strange, I will get this in, and double check whats going on once I get my hands on a windows box.
</comment><comment author="kimchy" created="2011-09-09T07:31:12Z" id="2048871">Pushed, thanks!.
</comment><comment author="kimchy" created="2011-09-09T08:01:06Z" id="2049050">Committed a different fix just now, without the need for setting the special flag...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Example config is missing the path.plugins directive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1312</link><project id="" key="" /><description>The example config in `config/elasticsearch.yml` is missing the `path.plugins` directive as an example:

``` yaml
#path.plugins: /path/to/plugins
```

ElasticSearch makes use of this parameter, but there is no documentation anywhere. :-)
</description><key id="1600177">1312</key><summary>Example config is missing the path.plugins directive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bascht</reporter><labels /><created>2011-09-08T16:48:50Z</created><updated>2011-09-09T14:25:33Z</updated><resolved>2011-09-09T14:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-09T14:25:33Z" id="2051674">Added, thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster / Index level allocation filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1311</link><project id="" key="" /><description>Allow to control allocation if indices on nodes based on include/exclude filters. The filters can be set both on the index level and on the cluster level. Lets start with an example of setting it on the cluster level:

Lets say we have 4 nodes, each has specific attribute called `tag` associated with it (the name of the attribute can be any name). Each node has a specific value associated with `tag`. Node 1 has a setting `node.tag: value1`, Node 2 a setting of `node.tag: value2`, and so on.

We can create an index that will only deploy on nodes that have `tag` set to `value1` and `value2` by setting `index.routing.allocation.include.tag` to `value1,value2`. For example:

```
curl -XPUT localhost:9200/test -d '{
    "index.routing.allocation.include.tag" : "value1,value2"
}'
```

On the other hand, we can create an index that will be deployed on all nodes except for nodes with a `tag` of value `value3` by setting `index.routing.allocation.exclude.tag` to `value3`. For example:

```
curl -XPUT localhost:9200/test -d '{
    "index.routing.allocation.exclude.tag" : "value3"
}'
```

The `include` and `exclude` values can have generic simple matching wildcards, for example, `value1*`. A special attribute name called `_ip` can be used to match on node ip values.

Obviously a node can have several attributes associated with it, and both the attribute name and value are controlled in the setting. For example, here is a sample of several node configurations:

```
node.group1: group1_value1
node.group2: group2_value4
```

In the same manner, `include` and `exclude` can work against several attributes, for example:

```
curl -XPUT localhost:9200/test -d '{
    "index.routing.allocation.include.group1" : "xxx"
    "index.routing.allocation.include.group2" : "yyy",
    "index.routing.allocation.exclude.group3" : "zzz",
}'
```

The provided settings can also be updated in real time using the update settings API, allowing to "move" indices (shards) around in realtime.

Cluster wide filtering can also be defined, and be updated in real time using the cluster update settings API. This setting can come in handy for things like decommissioning nodes (even if the replica count is set to 0). Here is a sample of how to decommission a node based on `_ip` address:

```
curl -XPUT localhost:9200/_cluster/_settings -d '{
    "transient" : {
        "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
    }
}'
```
</description><key id="1597551">1311</key><summary>Cluster / Index level allocation filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-08T11:28:29Z</created><updated>2011-09-08T14:46:13Z</updated><resolved>2011-09-08T11:29:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeFilters.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocators.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocationTests.java</file></files><comments><comment>Cluster / Index level allocation filtering, closes #1311.</comment></comments></commit></commits></item><item><title>Allowing default value for more methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1310</link><project id="" key="" /><description /><key id="1578794">1310</key><summary>Allowing default value for more methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-09-06T14:51:06Z</created><updated>2014-07-16T21:56:13Z</updated><resolved>2011-09-06T17:29:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-06T17:29:13Z" id="2019155">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: When specifying empty array of the stopwords, the default (English) is being used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1309</link><project id="" key="" /><description>Example : https://gist.github.com/1195120

Seems like regression of #230
</description><key id="1577803">1309</key><summary>Analysis: When specifying empty array of the stopwords, the default (English) is being used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mwiercinski</reporter><labels /><created>2011-09-06T12:22:13Z</created><updated>2014-01-31T23:46:12Z</updated><resolved>2013-03-03T20:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nbrownus" created="2014-01-31T23:01:12Z" id="33851507">Looks like this is back in 0.90.7
</comment><comment author="s1monw" created="2014-01-31T23:05:10Z" id="33851789">@nbrownus can you provide an example of the problem you are seeing?
</comment><comment author="nbrownus" created="2014-01-31T23:13:29Z" id="33852323">Config that should work but doesn't

```
index:
    mapping:
        ignore_malformed: true
    analysis:
        analyzer:
            default:
                type: standard
                stopwords: []
```

```
curl -XGET 'http://localhost:9200/test/_analyze?pretty' -d'this is a test'
{
  "tokens" : [ {
    "token" : "test",
    "start_offset" : 10,
    "end_offset" : 14,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 4
  } ]
}
```

Unfortunate config that does work

```
index:
    mapping:
        ignore_malformed: true
    analysis:
        analyzer:
            default:
                type: standard
                stopwords: ["somethinglong"]
```

```
curl -XGET 'http://localhost:9200/test/_analyze?pretty' -d'this is a test'
{
  "tokens" : [ {
    "token" : "this",
    "start_offset" : 0,
    "end_offset" : 4,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  }, {
    "token" : "is",
    "start_offset" : 5,
    "end_offset" : 7,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 2
  }, {
    "token" : "a",
    "start_offset" : 8,
    "end_offset" : 9,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 3
  }, {
    "token" : "test",
    "start_offset" : 10,
    "end_offset" : 14,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 4
  } ]
}
```
</comment><comment author="nbrownus" created="2014-01-31T23:46:12Z" id="33854398">Looks like using `stopwords: _none_` works though. The docs folder has that buried away but the website does not. Would be nice to give that some love.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix some typos in the index/analysis module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1308</link><project id="" key="" /><description>hypennation_decompunder -&gt; hyphenation_decompounder
only_longest_max -&gt; only_longest_match

I could not find any references to these strings after searching through the rest of the source code so it should be safe to update. http://www.elasticsearch.org/guide/reference/index-modules/analysis/compound-word-tokenfilter.html even refers to the "hyphenation_decompounder".
</description><key id="1577799">1308</key><summary>Fix some typos in the index/analysis module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2011-09-06T12:20:12Z</created><updated>2014-07-16T21:56:14Z</updated><resolved>2011-09-06T12:24:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-06T12:24:28Z" id="2014959">Pushed, thanks!.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>allow global setting of operation preference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1307</link><project id="" key="" /><description>The operation preference can be set globally using :

cluster.routing.operation.preference: _primary

Usually this parameter is used on each request.

If the parameter is given in the request this has precedence over the global setting.

Use case:
The field cache is used for each shard. If you are using replicas the field cache is used for each shard/replica which needs lots of memory. To be able to use replicas without the need of high memory machines it is now possible to use only the primary shards for the queries.
</description><key id="1576619">1307</key><summary>allow global setting of operation preference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jukart</reporter><labels /><created>2011-09-06T10:27:29Z</created><updated>2014-07-04T09:19:09Z</updated><resolved>2014-07-04T09:16:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T09:16:34Z" id="48023790">After discussion, we've decided that we prefer requiring the operation to be set on each request instead.
</comment><comment author="clintongormley" created="2014-07-04T09:19:09Z" id="48023983">Closed in favour of #6725 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix clear for FieldLookup values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1306</link><project id="" key="" /><description>When using FieldLookup.getValues only the first document provides values.

This is due to wrong clear of the "valuesLoaded" flag.
</description><key id="1576571">1306</key><summary>fix clear for FieldLookup values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jukart</reporter><labels /><created>2011-09-06T10:16:30Z</created><updated>2014-07-16T21:56:16Z</updated><resolved>2011-09-06T17:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-06T17:15:30Z" id="2018974">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>terms facet gives wrong count with n_shards &gt; 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1305</link><project id="" key="" /><description>I'm working with nested documents and have noticed that my faceted search interface is giving the wrong counts when I have more than one shard. To be more specific, I'm working with RDF triples (entity &gt; attribute &gt; value) and I'm nesting the attributes (called predicates in my example):

```
{
  "_id" : "512a2c022f0b4e3daa341e6c8bcf6c2f",
  "url": "http://dbpedia.org/resource/Alan_Shepard",
  "predicates": [
    {
      "type": "type",
      "string_value": ["thing", "person", "astronaut"]
    }, {
      "type": "label",
      "string_value": ["Alan Shepard"]
    }, {
      "type": "time in space",
      "float_value": [216.950]
    },
    ... lots more
  ]
}
```

I've created a shell script (https://gist.github.com/1196986) that recreates the problem with a fresh index. The created data set has these totals:
- thing (30)
- creative work (20)
- video game (10)
- tv show (10)
- people (10)

With only **one shard** the following query gives the correct counts no matter what the size parameter is set to:

```
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "facets": {
    "type_counts": {
      "terms": {
        "field": "string_value",
        "size": 5
      },
      "nested": "predicates",
      "facet_filter": {
        "term": {
          "type": "type"
        }
      }
    }
  }
}
```

However, with **more than one shard** the size parameter affects the accuracy of the counts. If it is equal to or greater than the number of terms returned by the facet query (5 in this case) then it works fine. However, the terms at the bottom of the list start to display low counts as you reduce the size parameter:

With "size" : 4
- thing (30)
- creative work (20)
- video game (10)
- **tv show (9)**

With "size" : 3
- thing (30)
- **creative work (15)**
- **video game (9)**

With "size" : 2
- thing (30)
- **creative work (15)**

So it looks like the sub-totals from some of the shards aren't being included for some reason. BTW I'm on ubuntu and the problem seems to affect all versions of ES I've tried (17.0, 17.1 and 17.6). Any ideas...?

P.S. absolutely loving ES - it's made my life a lot easier :)
</description><key id="1576320">1305</key><summary>terms facet gives wrong count with n_shards &gt; 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmchambers</reporter><labels><label>enhancement</label><label>high hanging fruit</label></labels><created>2011-09-06T09:32:06Z</created><updated>2015-07-17T12:18:31Z</updated><resolved>2015-07-14T20:29:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="losomo" created="2011-10-17T15:48:52Z" id="2430492">+1 for this bug. I have reproduced the problem using just documents with one field. 
Complete test (as run on version 0.17.8):
as Perl script: https://gist.github.com/1292897
generated shell test: https://gist.github.com/1292912
The first error can be seen on line 951.
Expected:

```
"terms" : [
 {
 "count" : 10,
 "term" : "user 10"
 }
 ],
```

Got:

```
"terms" : [
 {
 "count" : 7,
 "term" : "user 9"
 }
 ],
```
</comment><comment author="losomo" created="2011-10-17T18:02:37Z" id="2432372">After more experimenting I'd say that it's caused by naïve top-N facet merging. Something like Phase three here should be added:

http://wiki.apache.org/solr/DistributedSearchDesign#Phase_3:_REFINE_FACETS_.28only_for_faceted_search.29

Or something smarter: http://netcins.ceid.upatras.gr/papers/Klee_VLDB.pdf
</comment><comment author="kimchy" created="2011-10-18T00:17:47Z" id="2436533">Right, the way top N facets work now is by getting the top N from each shard, and merging the results. This can give inaccurate results. The phase 3 thingy is not really a solution, will read the paper though :)
</comment><comment author="losomo" created="2011-10-18T08:01:05Z" id="2439057">Right. The "phase 3 thingy" only solves the second problem:
1. The recall is not 100% (some values may be simply missing)
2. The numbers are often lower than they should be.

But while the first problem can easily stay unnoticed, the second one leads to quite a lousy user experience in very common faceting scenarios: The web displays top 10 commenters with number of their comments in parentheses. You click the last one with 30 comments and Voilà, it shows her 48 comments. This is hard not to notice.

**Meanwhile a slow and not-really-always-working workaround:**
Set `size` to what you want + 150 or another magical constant when requesting the facets and then filter out the top `size`  facets on the client side.
</comment><comment author="karussell" created="2011-10-19T07:23:21Z" id="2452807">&gt; The phase 3 thingy is not really a solution

@kimchy: why do you think so? isn't it a similar approach as query then fetch? now only for facets and its counts instead of docs and its score? (it even could be implemented in the second query to avoid traffic, no?)

Another workaround would be 'routing': every facet value should go to the same shard. But then the data should have **a lot facet values** and all of them should have not a too high count to avoid wrong balancing ...
</comment><comment author="agnellvj" created="2012-05-24T20:09:28Z" id="5916250">Is this on the roadmap? We discovered this same problem with regular fields and large datasets with multiple nodes and shards. This is problematic for us since the faceted counts can fluctuate wildly based on what filters we apply. Is there a workaround?
</comment><comment author="piskvorky" created="2012-06-03T15:21:26Z" id="6086296">I still see this bug in 19.4, and the wrong counts are also a show-stopper for us. 

Adding "150 or another magical constant" only helps for tiny datasets; does there exist a more robust workaround until this is fixed properly?
</comment><comment author="jmchambers" created="2012-06-03T16:44:53Z" id="6086914">+1 for a better workaround or fix

The only completely robust workaround I know of is to limit yourself to one shard. Obviously that pretty much takes the 'elastic' out of 'elasticsearch', but if accurate counts are critical to your app and your index isn't too big you might get away with it...
</comment><comment author="tarunjangra" created="2012-06-24T20:33:21Z" id="6536142">Any update on this? I do have an application where counts are critically important. As you are saying to limit to one shard for a particular entity which suppose to undergo such queries.
Does this make sense to route entities by entity type in corresponding shards?
</comment><comment author="karmi" created="2012-07-17T07:11:56Z" id="7028254">&gt; The only completely robust workaround I know of is to limit yourself to one shard. Obviously that pretty much takes the 'elastic' out of 'elasticsearch' (...)

@jmchambers Not really -- you can slice the data into many one-shard indices, with a weekly (daily, hourly, ...) rotation. You can use aliases (or wildcards, ...) to query them in a reasonable way from your application. Would mulitple one-shard indices work around the facet problem?
</comment><comment author="piskvorky" created="2012-07-17T08:07:53Z" id="7029069">@karmi: only if the indices are large enough. To my knowledge, the scoring stats (IDF etc) are computed per-index (actually, per-shard by default). So comparing relevancy scores across indices is like comparing apples to oranges, even with `dfs_query_then_fetch`.

But once the indices are large enough, this doesn't matter anymore as the stats converge (assuming identical doc/word distribution in each index, for the stats to actually converge).
</comment><comment author="ajhalani" created="2012-08-09T20:59:22Z" id="7626873">+1 for a fix. The workaround (one-shard, multiple one-shard index, etc.) don't sound convincing.
</comment><comment author="Downchuck" created="2012-08-27T01:50:49Z" id="8045484">I'm a bit confused on this issue: I have five shards and the top value occurs on each shard as #1, by a large margin. Still, when I facet, I get a 20% smaller number than when I simply query for the value directly.
</comment><comment author="jrydberg" created="2012-09-01T19:21:26Z" id="8215719">@karmi 

&gt; you can slice the data into many one-shard indices, with a weekly (daily, hourly, ...) rotation

Won't you run into the same problem when doing a faceted count over multiple indices?
</comment><comment author="giamma" created="2012-10-29T13:10:40Z" id="9865474">Any news about this issue? 
</comment><comment author="tgruben" created="2012-11-08T12:55:10Z" id="10186909">Any hope of this being fixed in the near future?
</comment><comment author="danfairs" created="2012-11-22T09:29:56Z" id="10628263">For what it's worth, the multiple index/single shard approach is working for us. Our application has a new index per week of data anyway, so it's not actually too painful for our current usage.
</comment><comment author="markwaddle" created="2012-11-27T08:16:14Z" id="10749416">+1 for a fix
My application has 7m+ docs of varying sizes (600GB index) and growing so 1 shard is not feasible.
For my application I am willing to trade performance (and/or hardware resources) for accurate facet counts.
</comment><comment author="webmusing" created="2013-04-05T07:14:47Z" id="15942013">+1 for a fix

Terms Stats Facet seems to be affected with the same issue as well.
</comment><comment author="piskvorky" created="2013-04-07T11:40:29Z" id="16013643">It seems there is no fix forthcoming; how about at least making the most common scenario less annoying/more palatable?

What I mean by that: when ES returns let's say 20 facets (with the wrong counts...), it could automatically run a second request against all shards asking for an accurate count **only for these exact 20 facets**. Then add up these accurate counts, and only return that as a result. Would that make sense?
</comment><comment author="vincentpoon" created="2013-05-10T18:46:04Z" id="17737633">+1 for a fix.  Incorrect facet counts is resulting in a bad user experience
</comment><comment author="songday" created="2013-07-05T08:56:46Z" id="20507606">Seems this bug did not fix completely

I did a facet query and sort by count, the results were right:
      "terms" : [ {
        "term" : "AAA",
        "count" : 59,
        "total_count" : 59,
        "min" : 1.0,
        "max" : 54.0,
        "total" : 391.0,
        "mean" : 6.627118644067797
      }, {
        "term" : "BBB",
        "count" : 55,
        "total_count" : 55,
        "min" : 1.0,
        "max" : 17.0,
        "total" : 154.0,
        "mean" : 2.8
      }]

but if sort by total (same query), the results dose not right:
      "terms" : [ {
        "term" : "AAA",
        "count" : 56,  //this is not right
        "total_count" : 56,
        "min" : 1.0,
        "max" : 54.0,
        "total" : 388.0,
        "mean" : 6.928571428571429
      }, {
        "term" : "BBB",
        "count" : 56,  //this is not right
        "total_count" : 56,
        "min" : 1.0,
        "max" : 17.0,
        "total" : 171.0,
        "mean" : 3.0535714285714284
      }]
</comment><comment author="tommymonk" created="2013-07-23T09:42:17Z" id="21403347">+1
</comment><comment author="HeyBillFinn" created="2013-08-02T14:20:42Z" id="22008596">Hi,

I am trying to run a search query using multiple facets, and I want the facet numbers to update in the same way as the site Zappos does. For example, when I search for Nike in the men's shoe section, I get facets for Brand and Color.

[cid:93D592E2-8A16-49EC-81A2-5F5804ED1836]

When I narrow my search by Brand (by selecting 'Nike'), the numbers within the Brand facet do not change, but the numbers within the Color facet change to reflect the narrowed search results.
[cid:8C58B26B-E7AC-44F3-8DDA-23A0BBD0AD0B]

I can widen my search by selecting 'Nike Action', and still no numbers within the Brand facet are updated, but numbers in the Color facet have been updated to reflect the additional results.

[cid:56423869-2538-47FA-8F43-D6355DC5AA48]

I can see the same expected results if I select a term within the Color facet:

[cid:BDC2DCEB-B7A5-4E86-BFF6-3DDD68E7ADED]

I can think of two ways to do this using Elastic Search, and I'm looking for any guidance/suggestions as to the best way to implement this.
1. Filtered query using fairly complex facet filters within each facet, including global = true flag.
2. Top-level filter (which I understand does not affect the facet results) with slightly less complex facet filters within each facet

Which of the two options would perform better? Is there a better option that I'm not thinking of? I can add example JSON if it will help explain my thoughts.

Thanks!

Bill
</comment><comment author="danfairs" created="2013-08-02T14:26:50Z" id="22008976">@finn1317, this is something you should ask on the elasticsearch mailing list. I don't think it's related to the issue being discussed here.
</comment><comment author="peakx" created="2013-08-08T13:42:07Z" id="22324098">+1 for a fix. 
</comment><comment author="SanderDemeester" created="2013-08-09T09:14:10Z" id="22383739">Question, could this _bug_ be related to the following result i'm getting.
If i request the first json document where the value for field `x` is `y` i get a result back.  
But when If i request all distict values for field `x` i get a list back with values. Byt 'y' is not included. 

query uses the `all_terms` and i match use `match_all`.
</comment><comment author="kaliseo" created="2013-08-18T15:39:09Z" id="22832651">+1

Same issue : 
ElasticSearch Version: 0.90.1
</comment><comment author="seti123" created="2013-08-21T20:09:13Z" id="23046067">I noticed wrong TermsFacet counts after updating documents (means I put new doc with same _id). Using ES 0.90.2. _version counts up.
A few docs got  more than 11 versions and the facet counter seems to count each version (at least it is growing with the updates I do). When I query I get only latest doc (as far I know ES is holding only last Doc and no history - on the other hand I wished to query for _version-1 to compare changes ...).

My application is reading the docs and depnding on other information it might write a new version to the index with additional information. But the TermsFace counts get then completely wrong ... (e.g. counting docs with same title 11 times instead of 1 time).

Any advice? 
</comment><comment author="spancer" created="2013-09-11T00:50:34Z" id="24205983">+1

I got this issue on ElasticSearch Version: 0.90.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Peer recovery: Allow to throttle recovery based on "size per sec"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1304</link><project id="" key="" /><description>Allow to set `indices.recovery.max_size_per_sec` setting allowing to throttle recovery (on node level). Defaults to no throttling (aside from the default concurrent streams and number of concurrent shard allowed to be recovered). Can be set, for example, to:

```
 indices.recovery.max_size_per_sec: 50mb
```

The setting can also be updated using the cluster update settings API.
</description><key id="1576200">1304</key><summary>Peer recovery: Allow to throttle recovery based on "size per sec"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-06T09:02:46Z</created><updated>2011-09-06T09:42:11Z</updated><resolved>2011-09-06T09:40:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file></files><comments><comment>Peer recovery: Allow to throttle recovery based on "size per sec", closes #1304.</comment></comments></commit></commits></item><item><title>Recovery Settings: Change settings (still support old settings) and allow for more dynamic settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1303</link><project id="" key="" /><description>Change the recovery settings names to a better descriptive names (they are not on shard level), from `index.shard.recovery` to `indices.recovery` prefix, and allow to change all using cluster update settings API. The settings are: `indices.recovery.file_chunk_size`, `indices.recovery.translog_ops`, `indices.recovery.translog_size`, `indices.recovery.compress`, `indices.recovery.concurrent_streams`.
</description><key id="1575954">1303</key><summary>Recovery Settings: Change settings (still support old settings) and allow for more dynamic settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-09-06T08:21:01Z</created><updated>2011-12-08T15:35:32Z</updated><resolved>2011-09-06T08:21:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aguereca" created="2011-12-07T21:16:29Z" id="3053560">The page http://www.elasticsearch.org/guide/reference/index-modules/translog.html mention some defaults for index.translog.flush_threshold_size and index.translog.flush_threshold_ops 
Those are the same as indices.recovery.translog_ops and indices.recovery.translog_size ??
If so, then why when ES starts I see smaller values : translog_size [100kb], translog_ops [1000]

From my log: 
[2011-12-07 14:44:56,098][DEBUG][index.shard.recovery     ] [De La Fontaine, Valentina Allegra] using concurrent_streams [5], file_chunk_size [100kb], translog_size [100kb], translog_ops [1000], and compress [true]
</comment><comment author="kimchy" created="2011-12-08T09:28:45Z" id="3059738">@aguereca which version are you using? it will print it under indices.recovery. The settings you pointed to in translog index module control when its being flushed, while the these settings control how its being recovered.
</comment><comment author="aguereca" created="2011-12-08T15:35:32Z" id="3063827">@kimchy Thanks, I just update to the last version, now is printing under indices.recovery. Thanks for the clarification, now is clear to me the difference on both, I stumble with those settings because I'm trying to track a pesky error: "this writer hit an OutOfMemoryError; cannot commit" that have been happening "randomly" each few days on one of the indexes, and one of the messages that I found in the log after some "failed engine" and before the "Flush failed" was a : "failed to flush shard on translog threshold", so I was wondering is some tweaking of this settings could make the trick.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/RateLimiter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/IgnoreRecoveryException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoverFilesRecoveryException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryFailedException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryFileChunkRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryFilesInfoRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryFinalizeRecoveryRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryPrepareForTranslogOperationsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/RecoveryTranslogOperationsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java</file></files><comments><comment>Recovery Settings: Change settings (still support old settings) and allow for more dynamic settings, closes #1303.</comment></comments></commit></commits></item><item><title>File system river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1302</link><project id="" key="" /><description>Hello there,

on the past week I was trying to use Lucene to automatically index a few text files inside a given directory on the file system. Without much success and with a change of requirements I then took a better look on elasticsearch and read about the river concept.

Today I created this river that monitors a given folder on the file system, and automatically indexes/remove files added/updated or deleted. It, in fact, worked quietly nice :D

There is one problem though, that is when I shutdown the server and start it up again. Seems like the river stops to work. Could you please advise me about it?

And later, if it's of your interest, accepted this pull request?

Thanks in advance.

ps.: here is a brief manual on how to use it:

After installing the plugin and running the server, create a river with:

``` bash

curl -XPUT localhost:9200/_river/my_fs_river/_meta -d '
{
    "type" : "fs",
    "fs" : {
        "path" : "/home/user/tmp/files/",
        "check_interval" : "10000"
    }
}'
```

By the way, it expects the dir to be already created. In this example that's in /home/user/tmp/files/

If you know create a file, like this:

``` bash

cat &lt;&lt;EOF &gt; /home/user/tmp/files/
hello world
EOF
```

... in 10 seconds or less (based on the check_interval), the file will be indexed. I know java 7 has better ways to monitor for file changes, but since that's not very well adopted I preferred to not use those features. You can check that the file is there with:

``` bash

curl -XGET localhost:9200/my_fs_river/status/a.txt
```

And, finally, search for the term:

``` bash

curl -XGET http://localhost:9200/my_fs_river/status/_search -d '{
    "query" : {
        "term" : { "contents": "hello" }
    }
}'
```
</description><key id="1563879">1302</key><summary>File system river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lucastorri</reporter><labels /><created>2011-09-04T22:38:09Z</created><updated>2014-07-16T21:56:16Z</updated><resolved>2013-03-22T07:23:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-06T18:40:47Z" id="2020769">Hey,

  How about creating an external project and have this plugin hosted on github (including the download)? I would like to get to a place where all plugins are no longer "embedded" in elasticsearch. More info here: https://groups.google.com/forum/#!topic/elasticsearch/IZMUJXRbmrw.
</comment><comment author="kimchy" created="2011-09-15T10:56:28Z" id="2103202">Also, a file system based river is problematic, since the river can be allocated on any node (though you can force it to run on a specific node).
</comment><comment author="dadoonet" created="2013-03-22T07:23:44Z" id="15284203">FYI, there is now a FSRiver project outside ES: https://github.com/dadoonet/fsriver
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices / Node Stats: Shard level search stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1301</link><project id="" key="" /><description>Aggregate shard level search stats, both "query" and "fetch" level stats (count and time) executing on the shard level. The total stats are aggregated and returned on node level stats, and on indices level stats.

When executing a search, custom stats groups can be provided allowing to further aggregated on different, user defined level stats aggregation (regardless of the indices / types search on). The stats can be provided as a comma separated list `stats` url parameter, or within the request body, for example:

```
{
    "query" : {
        "match_all" : {}
    },
    "stats" : ["group1", "group2"]
}
```

Getting back group level stats can be done on the indices stats API by providing it with the `groups` parameter (a comma separated list of the groups to return stats for), or, using the specific search stats endpoint. Here are some examples:

```
# "total" search level stats across all indices
curl 'localhost:9200/_stats?clear=true&amp;search=true`

# a short hand more specific option for the above
curl 'localhost:9200/_stats/search'

# "total" level stats for 'my_index'
curl 'localhost:9200/my_index/_stats?clear=true&amp;search=true'

# a short hand more specific option for the above
curl 'localhost:9200/my_index/_stats/search'

# search stats for group1 and group2 (several options)
curl 'localhost:9200/_stats?groups=group1,group2'
curl 'localhost:9200/_stats/search/group1,group2

# same as above, but only for 'my_index'
curl 'localhost:9200/my_index/_stats?groups=group1,group2'
curl 'localhost:9200/my_index/_stats/search/group1,group2
```
</description><key id="1563536">1301</key><summary>Indices / Node Stats: Shard level search stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-09-04T20:55:11Z</created><updated>2011-11-15T08:30:27Z</updated><resolved>2011-09-04T20:55:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tallpsmith" created="2011-09-05T04:26:50Z" id="1995585">kimchyThanks++; // note: this is a 64-bit value obviously... he needs that many thanks.
</comment><comment author="lindstromhenrik" created="2011-11-14T18:36:59Z" id="2734490">Running through the example it seems like the requests for groups when passed in the URL (i.e localhost:9200/my_index/_stats/search/group1,group2 and localhost:9200/_stats/search/group1,group2) is not working in 0.18.2.
</comment><comment author="kimchy" created="2011-11-15T08:30:27Z" id="2742201">@lindstromhenrik: right, recreated and opened an issue:  https://github.com/elasticsearch/elasticsearch/issues/1468
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/search/SearchRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/stats/SearchStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/stats/ShardSearchModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/stats/ShardSearchService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/stats/StatsGroupsParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/SearchService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/stats/SearchStatsTests.java</file></files><comments><comment>Indices / Node Stats: Shard level search stats, closes #1301.</comment></comments></commit></commits></item><item><title>Support setting ctx._parent in CouchDB-River</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1300</link><project id="" key="" /><description>It's already possible to set ctx._type in river-scripts to specify the type on a per-document base.
This feature seems to be missing for the _parent field which makes it kind of difficult to implement hierarchy-based searches when using CouchDB-River.
</description><key id="1561596">1300</key><summary>Support setting ctx._parent in CouchDB-River</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reihu</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-04T14:28:07Z</created><updated>2013-03-08T23:09:25Z</updated><resolved>2011-09-04T14:29:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Ganican" created="2013-03-08T23:09:25Z" id="14651501">I used the following script to set parent for my couchdb doc with a field
"parent":

_if(ctx.doc.parent) ctx._parent = ctx.doc.parent_

But some how I didnt see any field "_parent" in the actual index. Is the
above script right syntax?

Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file></files><comments><comment>Support setting ctx._parent in CouchDB-River, closes #1300.</comment></comments></commit></commits></item><item><title>preliminary work for per doc ttl support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1299</link><project id="" key="" /><description>As planned I send this first pull request on per doc ttl support so that you can have a first look at it.

What is missing basically is:
- an optimized purge method to delete expired docs the current one is quite naive
- a `hasTTL` flag on MapperService and DocumentMapper similar to `hasNested`
- maybe a way to hide expired docs of search if necessary
</description><key id="1552511">1299</key><summary>preliminary work for per doc ttl support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-09-02T15:37:16Z</created><updated>2014-07-16T21:56:17Z</updated><resolved>2011-09-08T22:22:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-08T22:22:21Z" id="2045674">Pushed with some minor changes and fixes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix small bug in parent handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1298</link><project id="" key="" /><description>fix bug in parent handling where the parsed parent value was not set in the SourceToParse
</description><key id="1549712">1298</key><summary>fix small bug in parent handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-09-02T10:09:33Z</created><updated>2014-07-16T21:56:17Z</updated><resolved>2011-09-04T14:39:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-04T14:39:36Z" id="1991513">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Negative total cpu time reported by the node stats REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1297</link><project id="" key="" /><description>We came across this little issue in using the node stats API, where the total CPU time values are reported as negative 1 while the user and kernel CPU times are non-zero.  The command used was:

  curl -XGET http://&lt;host&gt;:9200/_cluster/nodes/stats
  ...
        "process": {
        "timestamp": 1314943670627,
        "open_file_descriptors": 112,
        "cpu": {
          "percent": 1,
          "sys": "343 milliseconds",
          "sys_in_millis": 343,
          "user": "3 seconds and 226 milliseconds",
          "user_in_millis": 3226,
          "total": "-1 milliseconds",
          "total_in_millis": -1
        },

This is on the 0.18 series of elasticsearch, running on a Mac OS X 10.6.8 machine with multiple CPUs.

cheers.
</description><key id="1548762">1297</key><summary>Negative total cpu time reported by the node stats REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">natoscott</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-02T07:06:38Z</created><updated>2011-09-02T07:13:43Z</updated><resolved>2011-09-02T07:13:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/process/SigarProcessProbe.java</file></files><comments><comment>Negative total cpu time reported by the node stats REST API, closes #1297.</comment></comments></commit></commits></item><item><title>Rapidly concurrent deleting/creating an index leaves index inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1296</link><project id="" key="" /><description>I tried to delete an index, and less than a second later, another machine attempted to perform an index. That recreated the index before the deletion propagated cleanly, leaving the cluster inconsistent. Example log output: https://gist.github.com/1187311
</description><key id="1546167">1296</key><summary>Rapidly concurrent deleting/creating an index leaves index inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amckinley</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-01T21:18:43Z</created><updated>2011-09-02T06:36:41Z</updated><resolved>2011-09-02T06:36:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexIT.java</file><file>core/src/test/java/org/elasticsearch/action/admin/indices/template/put/MetaDataIndexTemplateServiceTests.java</file></files><comments><comment>Remove MetaDataSerivce and it's semaphores</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataService.java</file></files><comments><comment>Rapidly concurrent deleting/creating an index leaves index inconsistent, closes #1296.</comment></comments></commit></commits></item><item><title>Escape html</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1295</link><project id="" key="" /><description>added option to escape html by adding SimpleHtmlEncoder. rest ; escape_html: true
</description><key id="1542599">1295</key><summary>Escape html</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">locojay</reporter><labels /><created>2011-09-01T16:39:28Z</created><updated>2014-06-17T17:43:02Z</updated><resolved>2011-09-01T19:15:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="locojay" created="2011-09-01T16:42:27Z" id="1970042">this seems to have been requested in https://github.com/elasticsearch/elasticsearch/issues/1120
</comment><comment author="kimchy" created="2011-09-01T17:33:02Z" id="1970501">Looks good, one thing to note: can you change the flag called encode_html to a general one called encoder, that has two (string) values: `default` and `html`. We can possibly have others later on.

Also, git wise, can you squash your commits into one, and make sure you are sync'ed with master, github here indicates that the pull request can't be merged automatically.
</comment><comment author="locojay" created="2011-09-01T17:36:13Z" id="1970538">sure will do later . i did a boolean one since lucene just had Default and SimpleHtml encoder.
</comment><comment author="kimchy" created="2011-09-01T17:40:19Z" id="1970580">Right understood. Btw, I think the encoder should not be set per field, its an overkill, it should be set globally on the highlighter element. I don't see a case where different encoders will be used for different fields.
</comment><comment author="kimchy" created="2011-09-01T19:15:01Z" id="1972271">PUshed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Using 57744018578214912 as an id can cause failure to route to the correct shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1294</link><project id="" key="" /><description>This is because of the hashing logic we do..., Math.abs on Integer.MIN_VALUE.
</description><key id="1541515">1294</key><summary>Using 57744018578214912 as an id can cause failure to route to the correct shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-09-01T15:12:47Z</created><updated>2011-09-01T17:52:54Z</updated><resolved>2011-09-01T15:33:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jodok" created="2011-09-01T17:52:54Z" id="1970720">awesome :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file></files><comments><comment>Using 57744018578214912 as an id can cause failure to route to the correct shard, closes #1294.</comment></comments></commit></commits></item><item><title>negative from parameter yields undescriptive error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1293</link><project id="" key="" /><description>A minor quibble:

If the `from` parameter is accidentally set to a negative value (for example -10), the following happens:

   [2011-09-01 11:59:55,517][DEBUG][action.search.type       ] [Quentin Quire] Index Shard [cp_45678][2]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3c168b6c] while moving to second phase
java.lang.NullPointerException
    at org.elasticsearch.search.controller.SearchPhaseController.docIdsToLoad(SearchPhaseController.java:249)

It would be nice if ElasticSearch would send a nicer error message indicating the client misbehavior.
</description><key id="1539541">1293</key><summary>negative from parameter yields undescriptive error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-09-01T10:01:53Z</created><updated>2011-09-01T16:25:16Z</updated><resolved>2011-09-01T16:25:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-01T16:24:47Z" id="1969829">Minor, but important!, will push a fix.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/FromParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/SizeParseElement.java</file></files><comments><comment>negative from parameter yields undescriptive error message, closes #1293.</comment></comments></commit></commits></item><item><title>Java API: ActionFuture#actionGet to automatically unwrap failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1292</link><project id="" key="" /><description>Currently, using `ActionFuture#actionGet` will cause an `ElasticSearchException` to be thrown, but, it can be wrapped, for example, with a `RemoteTransportException`. The wrapping can be important, as it provides a "trace" to the origin of the failure.

We can automatically unwrap to the actual failure when throwing a failure from `actionGet`, and add to `ActionFuture` a helper method called `getRootFailure` which provides the actual (possibly wrapped) exception.
</description><key id="1535466">1292</key><summary>Java API: ActionFuture#actionGet to automatically unwrap failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-31T20:24:34Z</created><updated>2011-08-31T21:10:29Z</updated><resolved>2011-08-31T21:10:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/ActionFuture.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>plugins/lang/groovy/src/main/groovy/org/elasticsearch/groovy/client/action/GActionFuture.java</file></files><comments><comment>Java API: ActionFuture#actionGet to automatically unwrap failures, closes #1292.</comment></comments></commit></commits></item><item><title>Java API: XContentFactory creation of XContentBuilder to always be "safe"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1291</link><project id="" key="" /><description>Currently, we do some thread local caching of buffers in order to reduce buffer copying when using the typical `XContentFactory#jsonBuilder` (for example). This can create dangerous bugs of overlapping buffers if the builder is not passed to the API right when its created. Remove this behavior, and optimize in other manners in order to reduce buffer copying.

Note, this means that the "safe" methods will be removed from the `XContentFactory`.
</description><key id="1534366">1291</key><summary>Java API: XContentFactory creation of XContentBuilder to always be "safe"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-31T18:10:20Z</created><updated>2011-08-31T18:11:05Z</updated><resolved>2011-08-31T18:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/BytesStream.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/CachedStreams.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/FastByteArrayOutputStream.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/Serializers.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/stream/CachedStreamOutput.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContent.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContent.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/blobstore/BlobStoreGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/blobstore/BlobStoreIndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BaseQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/XContentRestResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/NettyTransportChannel.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/support/TransportStreams.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/xcontent/builder/BuilderRawFieldTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/DocumentActionsTests.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/netty/MemcachedRestChannel.java</file></files><comments><comment>Java API: XContentFactory creation of XContentBuilder to always be "safe", closes #1291.</comment></comments></commit></commits></item><item><title>add path support for id field, fix issue 1245</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1290</link><project id="" key="" /><description /><key id="1525815">1290</key><summary>add path support for id field, fix issue 1245</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-08-30T21:37:52Z</created><updated>2014-06-27T19:15:22Z</updated><resolved>2011-08-30T22:03:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-30T22:03:06Z" id="1950139">Pushed, cool!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support fetching _routing, _parent, _timestamp using realtime get when stored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1289</link><project id="" key="" /><description>Support fetching _routing, _parent, _timestamp using realtime get when stored
</description><key id="1523641">1289</key><summary>Support fetching _routing, _parent, _timestamp using realtime get when stored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-30T19:34:53Z</created><updated>2011-08-30T19:35:28Z</updated><resolved>2011-08-30T19:35:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/engine/AbstractSimpleEngineTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/timestamp/SimpleTimestampTests.java</file></files><comments><comment>Support fetching _routing, _parent, _timestamp using realtime get when stored, closes #1289.</comment></comments></commit></commits></item><item><title>Node / Indices Stats: Add get stats (including missing / exists)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1288</link><project id="" key="" /><description>Node / Indices Stats: Add get stats (including missing / exists)
</description><key id="1522604">1288</key><summary>Node / Indices Stats: Add get stats (including missing / exists)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-08-30T17:56:25Z</created><updated>2011-08-30T17:56:54Z</updated><resolved>2011-08-30T17:56:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/get/GetStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/indices/stats/SimpleIndexStatsTests.java</file></files><comments><comment>Node / Indices Stats: Add get stats (including missing / exists), closes #1288.</comment></comments></commit></commits></item><item><title>Don't include CLASSPATH env var in elasticsearch.bat script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1287</link><project id="" key="" /><description>Don't include the generalized `CLASSPATH` in the default classpath process.
</description><key id="1518690">1287</key><summary>Don't include CLASSPATH env var in elasticsearch.bat script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-30T12:40:28Z</created><updated>2011-08-30T12:41:15Z</updated><resolved>2011-08-30T12:41:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Don't include CLASSPATH env var in elasticsearch.bat script, closes #1287.</comment></comments></commit></commits></item><item><title>Invalid conversion from long to date in script field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1286</link><project id="" key="" /><description>Using 0.18.0-SNAPSHOT take the following script:

```
curl -X DELETE 'localhost:9200/index'

curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314651089690 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314651087102 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314651082795 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650999543 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650999252 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650998558 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650990049 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650989434 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650988908 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650944381 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650944055 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650943401 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650932802 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650932211 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650931720 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650922622 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650922335 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650921635 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650917717 }'
curl -X POST 'localhost:9200/index/type/' -d '{ "date" : 1314650917152 }'

curl -X POST 'localhost:9200/_refresh'

echo

curl -X GET 'localhost:9200/index/type/_search?pretty=true' -d '
{
  "size" : 20,
  "query" : {
    "match_all" : {}
  },
  "sort" : {
    "_script" : {
      "script" : "doc.date.value",
      "type" : "number",
      "order" : "desc"
    }
  },
  "script_fields" : {
    "original_millis" : {
      "script" : "doc.date.value"
    },
    "script_millis" : {
      "script" : "doc.date.date.millis"
    },
    "script_date" : {
      "script" : "doc.date.date"
    }
  }
}'
```

Now, taking only script fields from the returned hits we can get the following sequence:

```
orig_millis      script_millis    script_date

1314651089690    1314651089690    2011-08-29T20:48:41.635Z
1314651087102    1314651087102    2011-08-29T20:48:37.717Z
1314651082795    1314651082795    2011-08-29T20:48:42.335Z
1314650999543    1314650999543    2011-08-29T20:48:42.622Z
1314650999252    1314650999252    2011-08-29T20:48:37.152Z
1314650998558    1314650998558    2011-08-29T20:48:42.622Z    
1314650990049    1314650990049    2011-08-29T20:48:37.152Z
1314650989434    1314650989434    2011-08-29T20:48:37.717Z
1314650988908    1314650988908    2011-08-29T20:48:41.635Z
1314650944381    1314650944381    2011-08-29T20:48:37.717Z
1314650944055    1314650944055    2011-08-29T20:48:42.622Z
1314650943401    1314650943401    2011-08-29T20:48:37.152Z
1314650932802    1314650932802    2011-08-29T20:48:41.635Z
1314650932211    1314650932211    2011-08-29T20:48:42.622Z
1314650931720    1314650931720    2011-08-29T20:48:37.152Z
1314650922622    1314650922622    2011-08-29T20:48:42.622Z
1314650922335    1314650922335    2011-08-29T20:48:42.335Z
1314650921635    1314650921635    2011-08-29T20:48:41.635Z
1314650917717    1314650917717    2011-08-29T20:48:37.717Z
1314650917152    1314650917152    2011-08-29T20:48:37.152Z
```

As we can see the order by original long value is correct but the date values generated from them are obviously not sorted correctly - check sec values. Interestingly, millis value generated from the date value is equal to the original millis value from the document (so is it that the problem can be only be in converting the correct millis value into string date representation?).
</description><key id="1517646">1286</key><summary>Invalid conversion from long to date in script field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-08-30T09:28:57Z</created><updated>2014-03-26T09:07:43Z</updated><resolved>2014-03-26T09:07:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-31T14:02:16Z" id="1957478">Because the date is cached, if you want to return some sort of representation of it, you need to build it in the script, for example, calling toString on it. Otherwise, values will get garbled...
</comment><comment author="lukas-vlcek" created="2014-03-26T09:07:43Z" id="38662108">Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Timestamp field support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1285</link><project id="" key="" /><description>A new field, called `_timestamp` has been added to the existing fields usable in mapping.

The `timestamp` field allows to automatically index the timestamp of a document. It can be provided externally via the index request or in the `_source`. If it is not provided externally it will be automatically set to the date the document was processed by the indexing chain.
## enabled

By default it is disabled, in order to enable it, the following mapping should be defined:

``` javascript
{
    "tweet" : {
        "_timestamp" : { "enabled" : true }
    }
}
```
## store / index

By default the `_timestamp` field has `store` set to `no` and `index` set to `not_analyzed`. It can be queried as a standard date field.
## path

The timestamp value can be provided as an external value when indexing. But, it can also be automatically extracted from the document to index based on a `path`. For example, having the following mapping:

``` javascript
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "path" : "post_date"
        }
    }
}
```

Will cause `2009-11-15T14:12:12` to be used as the timestamp value for:

``` javascript
{
    "message" : "You know, for Search",
    "post_date" : "2009-11-15T14:12:12"
}
```

Note, using `path` without explicit timestamp value provided require an additional (though quite fast) parsing phase.
## format

You can define the date format used to parse the provided timestamp value. For example:

``` javascript
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "path" : "post_date",
            "format" : "YYYY-MM-dd"
        }
    }
}
```

Note, the default format is `dateOptionalTime`. The timestamp value will first be parsed as a number and if it fails the format will be tried.
</description><key id="1509196">1285</key><summary>Timestamp field support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-08-29T12:52:50Z</created><updated>2011-08-29T14:46:48Z</updated><resolved>2011-08-29T14:46:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-29T14:46:48Z" id="1931558">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A new river may not be found immediately</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1284</link><project id="" key="" /><description>Hiya

In my test suite, I'm creating a river then checking for its existence (in a 3 node cluster).

This test frequently fails, unless I sleep for 3 seconds.

Note that I'm querying each node in turn:

```
curl -XPUT 'http://127.0.0.1:9200/_river/foo/_meta?pretty=1'  -d '
{
   "type" : "dummy"
}
'
curl -XGET 'http://127.0.0.1:9202/_cluster/health?pretty=1&amp;timeout=30s&amp;wait_for_status=green' 
curl -XPOST 'http://127.0.0.1:9201/_refresh?pretty=1' 
curl -XGET 'http://127.0.0.1:9200/_river/foo/_meta?pretty=1' 
curl -XGET 'http://127.0.0.1:9202/_river/foo/_status?pretty=1' 
```

Here is a typical result:

```
# [Sun Aug 28 12:05:15 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/_river/foo/_meta?pretty=1'  -d '
{
   "type" : "dummy"
}
'

# [Sun Aug 28 12:05:15 2011] Response:
# {
#    "ok" : true,
#    "_index" : "_river",
#    "_id" : "_meta",
#    "_type" : "foo",
#    "_version" : 1
# }

# [Sun Aug 28 12:05:15 2011] Protocol: http, Server: 127.0.0.1:9202
curl -XGET 'http://127.0.0.1:9202/_cluster/health?pretty=1&amp;timeout=30s&amp;wait_for_status=green' 

# [Sun Aug 28 12:05:15 2011] Response:
# {
#    "number_of_data_nodes" : 3,
#    "relocating_shards" : 0,
#    "active_shards" : 2,
#    "status" : "green",
#    "cluster_name" : "es_test",
#    "active_primary_shards" : 1,
#    "timed_out" : false,
#    "initializing_shards" : 0,
#    "number_of_nodes" : 3,
#    "unassigned_shards" : 0
# }

# [Sun Aug 28 12:05:15 2011] Protocol: http, Server: 127.0.0.1:9201
curl -XPOST 'http://127.0.0.1:9201/_refresh?pretty=1' 

# [Sun Aug 28 12:05:15 2011] Response:
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 2
#    }
# }

# [Sun Aug 28 12:05:15 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/_river/foo/_meta?pretty=1' 

# [Sun Aug 28 12:05:15 2011] Response:
# {
#    "_source" : {
#       "type" : "dummy"
#    },
#    "_index" : "_river",
#    "_id" : "_meta",
#    "_type" : "foo",
#    "exists" : true,
#    "_version" : 1
# }

# [Sun Aug 28 12:05:15 2011] Protocol: http, Server: 127.0.0.1:9202
curl -XGET 'http://127.0.0.1:9202/_river/foo/_status?pretty=1' 

# [Sun Aug 28 12:05:15 2011] Response:
# {
#    "_index" : "_river",
#    "_id" : "_status",
#    "_type" : "foo",
#    "exists" : false
# }
```
</description><key id="1503300">1284</key><summary>A new river may not be found immediately</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-08-28T10:12:29Z</created><updated>2014-07-03T19:13:43Z</updated><resolved>2014-07-03T19:13:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-08-04T11:28:43Z" id="7501123">This is still failing in 0.19.8  
http://travis-ci.org/#!/clintongormley/ElasticSearch.pm/jobs/2001801/L667
</comment><comment author="otisg" created="2013-07-01T15:06:42Z" id="20287700">Confirming that we see this issue even with 0.90.2.
Are there any known work-arounds?
</comment><comment author="otisg" created="2013-07-03T03:27:33Z" id="20393528">@clintongormley - this appears to be related to having multiple nodes.  It doesn't happen when one has just 1 ES node.  So in your case you had to sleep a little and that is probably because the request for the River that you made after creating the River got routed to the wrong shard, the one that didn't yet know about the River.
</comment><comment author="clintongormley" created="2014-07-03T19:13:43Z" id="47972556">Closing as no longer relevant
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ignoreAttachements option for couchDb river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1283</link><project id="" key="" /><description>When couchDB river reads _changes API, _changes always returns attachments for each document.

Here is a simple patch to help users to disable this default feature.

It adds a new option to the couchDb river called : `ignore_attachments` :
- **false** (default) : attachments will be pushed into Elastic Search
- **true** : attachments will be ignored when pushing to Elastic Search

You can use it as follows :

``` javascript
{
  "type":"couchdb",
  "couchdb": {
    "host":"localhost",
    "port":"5984",
    "db":"mytest",
    "ignore_attachments":true
  }
}
```

BTW, I added a small test case.

Any comments are welcomed.

I will add a pull request to update the couchDb river guide to describe this new option also.
</description><key id="1496765">1283</key><summary>Add ignoreAttachements option for couchDb river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-08-26T17:22:12Z</created><updated>2014-06-17T05:39:30Z</updated><resolved>2011-08-29T14:55:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2011-08-26T17:55:29Z" id="1915249">Here is the documentation pull request : https://github.com/elasticsearch/elasticsearch.github.com/pull/72
</comment><comment author="kimchy" created="2011-08-29T14:55:27Z" id="1931671">Pushed, with some cleanups..., thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Luke: Unknown format version: -11</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1282</link><project id="" key="" /><description>elasticsearch 0.17.6

We are unable to view a lucene index using Luke. We receive "unknown format version: -11". Per the thread "luke not recognizing index from ES git master", we tried setting "index.compound_format: true" in the ES config yml file. We tried removing the checksum file in the index directory. We are able to get hits back in our app against the single field we have indexed so far. 

an index path: elasticsearch-0.17.6/data/elasticsearch/nodes/0/indices/profiles/0/index/
</description><key id="1495681">1282</key><summary>Luke: Unknown format version: -11</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adkron</reporter><labels /><created>2011-08-26T15:00:53Z</created><updated>2013-04-05T15:46:38Z</updated><resolved>2013-04-05T15:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="blueguitarenator" created="2011-08-26T15:07:01Z" id="1913399">me too
</comment><comment author="alexis779" created="2011-11-02T14:56:09Z" id="2604355">Luke manages to load the Lucene index in my case.
It is located at ~/linux/elasticsearch-0.17.7/data/elasticsearch/nodes/0/indices/myStuff/0/index and corresponds to shard 0 of myStuff "index".

These are the versions:
elasticsearch 0.17.7
luke 3.4.0_1

I have issues to visualize all the fields. It looks like _source contains all the data in binary format.

Any luck?
</comment><comment author="clintongormley" created="2013-04-05T15:46:38Z" id="15963592">Lucene versions have moved on - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi Get: Allow to specify fields to fetch in the URI, and apply it automatically to all docs to get without explicit fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1281</link><project id="" key="" /><description>Multi Get: Allow to specify fields to fetch in the URI, and apply it automatically to all docs to get without explicit fields
</description><key id="1495481">1281</key><summary>Multi Get: Allow to specify fields to fetch in the URI, and apply it automatically to all docs to get without explicit fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-26T14:37:01Z</created><updated>2011-08-26T14:42:35Z</updated><resolved>2011-08-26T14:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file></files><comments><comment>Multi Get: Allow to specify fields to fetch in the URI, and apply it automatically to all docs to get without explicit fields, closes #1281.</comment></comments></commit></commits></item><item><title>Tiered merge policy setting: `max_merge_segment` misnamed and should be `max_merged_segment`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1280</link><project id="" key="" /><description>Support both, but the correct setting should be `max_merged_segment`, this is the realtime setting.
</description><key id="1495280">1280</key><summary>Tiered merge policy setting: `max_merge_segment` misnamed and should be `max_merged_segment`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-26T14:16:38Z</created><updated>2011-09-12T18:29:09Z</updated><resolved>2011-08-26T14:17:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java</file></files><comments><comment>Tiered merge policy setting: `max_merge_segment` misnamed and should me `max_merged_segment`, closes #1280.</comment></comments></commit></commits></item><item><title>Add support for a _timestamp field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1279</link><project id="" key="" /><description>This patch add support for a _timestamp field (issue #491).

The _timestamp can be provided externally in the index request or in a _source field (via a path) or its value is automatically set. It is a classical date field with customizable format. The timestamp value can be given using the following values:
- a number
- a number inside string
- a string conformed with the defined format

By default the _timestamp field is disabled and not stored.

When the _timestamp is automatically generated it is set with the current time when the index query was processed by the transport layer.

This patch should work properly with transactions log and replicates.

It will hopefully serve as a basis for the per doc TTL feature I am working on and other features like rolling indices.
</description><key id="1493727">1279</key><summary>Add support for a _timestamp field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-08-26T09:23:09Z</created><updated>2014-06-24T19:57:32Z</updated><resolved>2011-08-29T14:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-29T14:46:38Z" id="1931555">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Performance improvement when creating a multi-value field cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1278</link><project id="" key="" /><description /><key id="1493698">1278</key><summary>Performance improvement when creating a multi-value field cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acerb</reporter><labels /><created>2011-08-26T09:17:56Z</created><updated>2014-07-16T21:56:20Z</updated><resolved>2011-08-29T15:55:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="acerb" created="2011-08-26T09:19:37Z" id="1911117">Hi,

We've a 10 millions document database which took 10seconds to load in cache (facetting) instead of 20 with this patch, might worth take a look.

Regards,

Chris.
</comment><comment author="kimchy" created="2011-08-29T15:55:10Z" id="1932645">looks good!, pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for a _timestamp field (same player plays again)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1277</link><project id="" key="" /><description>This patch add support for a _timestamp field (issue #491).

The _timestamp can be provided externally in the index request or in a _source field (via a path) or its value is automatically set. It is a classical date field with customizable format. The timestamp value can be given using the following values:
- a number
- a number inside string
- a string conformed with the defined format

By default the _timestamp field is disabled and not stored.

When the _timestamp is automatically generated it is set with the current time when the index query was processed by the transport layer.

This patch should work properly with transactions log and replicates.

It will hopefully serve as a basis for the per doc TTL feature I am working on and other features like rolling indices.
</description><key id="1485995">1277</key><summary>Add support for a _timestamp field (same player plays again)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-08-25T21:06:15Z</created><updated>2014-07-16T21:56:21Z</updated><resolved>2011-08-26T09:17:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Node Stats: Certain indices level stats to retain stats even when shard relocates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1276</link><project id="" key="" /><description>Currently, we return the aggregated stats from all the shards. This means that those values will be changed as shards relocate from the node. This make sense to some stats (docs, store), but, its better to retain aggregated values for other stats (merge, refresh, flush, indexing).
</description><key id="1484392">1276</key><summary>Node Stats: Certain indices level stats to retain stats even when shard relocates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-25T17:19:50Z</created><updated>2011-08-25T17:20:29Z</updated><resolved>2011-08-25T17:20:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/service/NodeService.java</file></files><comments><comment>Node Stats: Certain indices level stats to retain stats even when shard relocates, closes #1276.</comment></comments></commit></commits></item><item><title>Geofacet with boundingbox</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1275</link><project id="" key="" /><description>Create a geo facet that uses a boundingbox instead of distance. 

``` javascript
{
  "geo_bbox": {
       "bbox": [-180,90,180,-90],
       "spacing": {
           "lat": 0.0006993,
           "lon": 0.0006993
       },       
       "key_field": "location",
       "value_field": "propery_value"
  }
}
```

The result is expanded to:

``` javascript
"geotest" : {
    "_type" : "geo_bbox",
    "missing" : 0,
    "bbox": [-180,90,180,-90],
    "rows": 200,
    "cols": 200,
    "data" : [
        {
          "bbox": [ 72, 18, 72.5, 17.5 ],
          "center": { 
               "lat":  17.75,
               "lon": 72.25
          },
         "count": 15168,
         "total_count": 14897,
         "min": 6500,
         "max": 225000000,
         "total": 37108379266,
         "mean": 2490996.795730684
    } 
    ]
  }

```

The idea is to provide a boundingbox and a gridsize/spacing. The spacing and outer boundingbox is expanded to {row} x {column} number of boundingboxes which is used to test the key against. The value_field (script_field etc) is used for calculations in the same manner as in term/term_stat facet.
</description><key id="1481171">1275</key><summary>Geofacet with boundingbox</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">matsjg</reporter><labels /><created>2011-08-25T08:49:30Z</created><updated>2013-07-03T12:03:44Z</updated><resolved>2013-07-03T12:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-07T15:10:13Z" id="19112885">Do you think you can solve your issue with the bounding box filter and a terms facet or did I get your sample wrong?
</comment><comment author="spinscale" created="2013-07-03T12:03:44Z" id="20410906">Closing due to lack of feedback. Happy to reopen if this is still an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Object types inside a document aren't returned when requested with fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1274</link><project id="" key="" /><description>I have an object nested inside a document that I'm trying to pull out with `fields`, but it can only be accessed trough `_source`. I've tried it stored or not stored, with the same results.

```
$ curl -s -XDELETE http://127.0.0.1:9200/test | json
{
    "acknowledged": true,
    "ok": true
}

$ curl -s -XPUT http://127.0.0.1:9200/test | json
{
    "acknowledged": true,
    "ok": true
}

$ curl -s -XPUT http://127.0.0.1:9200/test/test/_mapping -d '{"test": {"properties": {"obj": {"dynamic": true, "type": "object", "store": true}}}}' | json
{
    "acknowledged": true,
    "ok": true
}

$ curl -s -XPOST http://localhost:9200/test/test -d '{"obj": {"x": 1, "y": 2}, "id": 1}' | json
{
    "_id": "FbC8ButqScqXVwKY_BL4Yw",
    "_index": "test",
    "_type": "test",
    "_version": 1,
    "ok": true
}

$ curl -s -XGET http://127.0.0.1:9200/test/test/_search -d '{}' | json{
    "_shards": {
        "failed": 0,
        "successful": 5,
        "total": 5
    },
    "hits": {
        "hits": [
            {
                "_id": "FbC8ButqScqXVwKY_BL4Yw",
                "_index": "test",
                "_score": 1.0,
                "_source": {
                    "id": 1,
                    "obj": {
                        "x": 1,
                        "y": 2
                    }
                },
                "_type": "test"
            }
        ],
        "max_score": 1.0,
        "total": 1
    },
    "timed_out": false,
    "took": 25
}

$ curl -s -XGET http://127.0.0.1:9200/test/test/_search -d '{"fields": ["obj"]}' | json
{
    "_shards": {
        "failed": 0,
        "successful": 5,
        "total": 5
    },
    "hits": {
        "hits": [
            {
                "_id": "FbC8ButqScqXVwKY_BL4Yw",
                "_index": "test",
                "_score": 1.0,
                "_type": "test"
            }
        ],
        "max_score": 1.0,
        "total": 1
    },
    "timed_out": false,
    "took": 8
}
```
</description><key id="1478378">1274</key><summary>Object types inside a document aren't returned when requested with fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jbalogh</reporter><labels /><created>2011-08-24T21:52:38Z</created><updated>2013-04-05T15:47:24Z</updated><resolved>2013-04-05T15:47:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-24T22:45:46Z" id="1895271">Can you please add questions on the mailing list, and once we verify its an issue, we can open one. Its hard to maintain two user forums, and you have less eyeballs here...

Regarding your problem, you should prepend `_source` to the field, so ti should be `_source.obj`. Having a `store` element on `object` is meaningless by the way, it only applies to fields which get explicitly stored. By default, though, the _source is stored, and when you ask for `_source.obj` it will load it, parse it, and just return the `obj` part.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices Stats API + indexing statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1273</link><project id="" key="" /><description>A new indices stats API, allow to retrieve specific indices level stats. Also, add indexing stats providing statistics on indexing operation (delete/index).

The following returns high level aggregation and index level stats for all indices:

```
curl localhost:9200/_stats
```

Specific index stats can be retrieved using:

```
curl localhost:9200/index1,index2/_stats
```

By default, `docs`, `store`, and `indexing` stats are returned, other stats can be enabled as well:
- `docs`: The number of docs / deleted docs (docs not yet merged out). Note, affected by refreshing the index.
- `store`: The size of the index.
- `indexing`: Indexing statistics, can be combined with a comma separated list of `types` to provide document type level stats.
- `merge`: merge stats.
- `flush`: flush stats.
- `refresh`: refresh stats.
- `clear`: Clears default set flags (`docs`, `store`, and `indexing`).

Here are some samples:

```
# Get back stats for merge and refresh on top of the defaults
curl 'localhost:9200/_stats?merge=true&amp;refresh=true'
# Get back stats just for flush
curl 'localhost:9200/_stats?clear=true&amp;flush=true'
# Get back stats for type1 and type2 documents for the my_index index
curl 'localhost:9200/my_index/_stats?clear=true&amp;indexing=true&amp;types=type1,type2
```

The stats returned are aggregated on the index level, with `primaries` and `total` aggregations. In order to get back shard level stats, set the `level` parameter to `shards`.
## Specific stats endpoints

Instead of using flags to indicate which stats to return, specific REST endpoints can be used, for example:

```
# Merge stats across all indices
curl localhost:9200/_stats/merge
# Merge stats for the my_index index
curl localhost:9200/my_index/_stats/merge
# Indexing stats for my_index
curl localhost:9200/my_index/_stats/indexing
# Indexing stats for my_index for my_type1 and my_type2
curl localhost:9200/my_index/_stats/indexing/my_type1,my_type2
```
</description><key id="1472680">1273</key><summary>Indices Stats API + indexing statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-08-24T08:24:06Z</created><updated>2011-08-24T21:11:29Z</updated><resolved>2011-08-24T08:25:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2011-08-24T10:24:20Z" id="1888489">cool inside~
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActions.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexShardStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/IndexStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/ShardStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/status/IndicesStatusRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/node/NodeIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/ClientTransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/admin/indices/stats/ClientTransportIndicesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/Directories.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/metrics/MeanMetric.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/ToXContent.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/indexing/IndexingOperationListener.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/indexing/IndexingStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/indexing/ShardIndexingModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/DocsStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/Store.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/StoreStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/support/AbstractStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/NodeIndicesStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/get/RestGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/support/AbstractRestRequest.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/indices/stats/SimpleIndexStatsTests.java</file></files><comments><comment>Indices Stats API + indexing statistics, closes #1273.</comment></comments></commit></commits></item><item><title>Stopwords are not being removed from wildcard query_string queries. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1272</link><project id="" key="" /><description>Hi, 

If you run query_string request including of the stopword followed by "*" wildcard, Elasticsearch will act like it has not filtered the stopword off. This is regardless of analyze_wildcard and whether the searched field is being analyzed or not (set through mappings API) .

An example: 

Clean the index: 

```
$ curl -XDELETE localhost:9200/test_index?pretty
{
  "ok" : true,
  "acknowledged" : true
}
```

Populate with **The Times**. 

```
$ curl -XPUT localhost:9200/test_index/test_type/1?pretty -d ' {"name": "The Times" } ' 
{
  "ok" : true,
  "_index" : "test_index",
  "_type" : "test_type",
  "_id" : "1",
  "_version" : 1
}
```

Query for **the times** does work correctly: 

```
$ curl -XGET localhost:9200/test_index/test_type/_search?pretty -d '
{
  "query": {
    "query_string": {
      "query": "the times"
    }
  }
} 
'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.2169777,
    "hits" : [ {
      "_index" : "test_index",
      "_type" : "test_type",
      "_id" : "1",
      "_score" : 0.2169777, "_source" :  {"name": "The Times" } 
    } ]
  }
}

```

Same using **default_operator** set to AND.  

```
$ curl -XGET localhost:9200/test_index/test_type/_search?pretty -d '
{
  "query": {
    "query_string": {
      "query": "the times",
      "default_operator": "AND"
    }
  }
} 
'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.2169777,
    "hits" : [ {
      "_index" : "test_index",
      "_type" : "test_type",
      "_id" : "1",
      "_score" : 0.2169777, "_source" :  {"name": "The Times" } 
    } ]
  }
}
```

If I now add the wildcard to **the**, I will not get any results back: 

```
$ curl -XGET localhost:9200/test_index/test_type/_search?pretty -d '
{
  "query": {
    "query_string": {
      "query": "the* times",
      "default_operator": "AND"
    }
  }
} 
'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

, however does work with wildcard on **times**: 

```
$ curl -XGET localhost:9200/test_index/test_type/_search?pretty -d '
{
  "query": {
    "query_string": {
      "query": "the times*",
      "default_operator": "AND"
    }
  }
} 
'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test_index",
      "_type" : "test_type",
      "_id" : "1",
      "_score" : 1.0, "_source" :  {"name": "The Times" } 
    } ]
  }
}

```
</description><key id="1468534">1272</key><summary>Stopwords are not being removed from wildcard query_string queries. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mwiercinski</reporter><labels /><created>2011-08-23T18:15:30Z</created><updated>2013-04-05T15:50:22Z</updated><resolved>2013-04-05T15:50:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T15:50:22Z" id="15963785">Apparently using `analyze_wildcard` does work now:

```
curl -XGET 'http://127.0.0.1:9200/_all/_validate/query?pretty=1&amp;explain=true'  -d '
{
   "query_string" : {
      "query" : "the* times",
      "analyze_wildcard" : 1
   }
}
'

# {
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 3,
#       "total" : 3
#    },
#    "explanations" : [
#       {
#          "index" : "test",
#          "explanation" : "_all:times",
#          "valid" : true
#       },
#       {
#          "index" : "foo",
#          "explanation" : "_all:times",
#          "valid" : true
#       },
#       {
#          "index" : "test_index",
#          "explanation" : "_all:times",
#          "valid" : true
#       }
#    ],
#    "valid" : true
# }
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs - broken link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1271</link><project id="" key="" /><description>The destination of the link named "rewrite" on this page http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query.html, doesn't exist.
</description><key id="1465077">1271</key><summary>Docs - broken link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-08-23T10:04:44Z</created><updated>2011-08-23T10:17:24Z</updated><resolved>2011-08-23T10:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-08-23T10:17:24Z" id="1878522">Thanks - fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing count in range facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1270</link><project id="" key="" /><description>Hi, 

terms facets include missing count, but seems like range facet does not contain this field in response. Would it be possible to add it? And also to all other facets that don't contain such info?

It's possible to get this info by a workaround at application level by adding another terms facet on same field, but it's a pain.

Thanks for the great work btw.
</description><key id="1464393">1270</key><summary>Missing count in range facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsuchal</reporter><labels /><created>2011-08-23T08:02:20Z</created><updated>2014-01-22T10:55:59Z</updated><resolved>2014-01-22T10:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jsuchal" created="2013-08-26T13:29:30Z" id="23262259">Any update on this?
</comment><comment author="jpountz" created="2014-01-22T10:55:59Z" id="33011476">We made it more consistent with aggregations: terms/histogram/range aggregations don't include missing counts in the response but there is a dedicated [`missing`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-missing-aggregation.html) aggregation that can be used to compute missing counts.

I'm closing this issue as we don't plan to modify facets anymore in order to focus on aggregations.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CouchDB River: Add throttling when indexing does not keep up with fetching _changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1269</link><project id="" key="" /><description>Add automatic throttling when fetching _changes is considerably faster compared to indexing data (can happen when fully reindexing a new db). By default, the throttling "queue" size will be 5 times the `bulk_size` parameter, and can be set using `throttling_size`.
</description><key id="1460319">1269</key><summary>CouchDB River: Add throttling when indexing does not keep up with fetching _changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-22T19:15:44Z</created><updated>2011-08-22T19:18:24Z</updated><resolved>2011-08-22T19:18:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file></files><comments><comment>CouchDB River: Add throttling when indexing does not keep up with fetching _changes, closes #1269.</comment></comments></commit></commits></item><item><title>Fix small bug in routing parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1268</link><project id="" key="" /><description>Pretty much self explanatory. Not a big deal :)
</description><key id="1459193">1268</key><summary>Fix small bug in routing parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-08-22T16:33:57Z</created><updated>2014-06-25T14:18:23Z</updated><resolved>2011-08-24T17:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-24T17:19:00Z" id="1892000">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature Request: decommissioning nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1267</link><project id="" key="" /><description>Hi

It would be nice to be able to decommission specific nodes like it is possible for example in HDFS (http://developer.yahoo.com/hadoop/tutorial/module2.html#decommission).

This would allow to keep the replication factor low (or even zero) and to remove machines from the cluster without loosing data. This is usefull for example to handle peaks of load like bulk indexing with many ec2 instances and then terminate instances after the indexing. Without the ability to decommission the nodes which should be terminated, the data stored on those machines will be lost if there is no replication. 

So for speed of indexing we need to set the replication factor low. Then after indexing we need to increase it to at least 1 and then slowly remove machines from the cluster, so that all shards get replicated to other machines before the node goes down.

I can imagine that this feature could help in a lot scenarios, especially on clout environments. It would even make it possible to use spot-instances on ec2 with a smal number of replicas.

An even better approach would be to be able to declare specific instances in a cluster as non-persistent . Which would mean that shards stored on those machines should not count towards the replication factor. So that at least one replica on another machine must exist, but the local data on the non-persistent machine can still be used for searches. This would make it possible to dynamically grow or shrink the cluster depending on traffic.

Regards, Bernd
</description><key id="1455205">1267</key><summary>Feature Request: decommissioning nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobe</reporter><labels /><created>2011-08-22T04:34:39Z</created><updated>2012-01-19T07:36:35Z</updated><resolved>2012-01-19T07:36:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="prashanthellina" created="2011-11-02T11:40:57Z" id="2602310">I would love to have this feature so I can shrink my cluster without replication enabled.
</comment><comment author="nebulabug" created="2011-11-02T11:44:31Z" id="2602337">+1 very useful for speed indexing. 
</comment><comment author="dobe" created="2012-01-19T07:36:35Z" id="3559084">just for the records: this can now be achieved as described here http://www.elasticsearch.org/guide/reference/index-modules/allocation.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster Update Settings API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1266</link><project id="" key="" /><description>Allow to update cluster wide specific settings. Settings updated can either be persistent (applied cross restarts) or transient (will not survive a full cluster restart). Here is an example:

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}'
```

Or:

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}'
```

There is a specific list of settings that can be updated, those include:
- `discovery.zen.minimum_master_nodes`
- `index.shard.recovery.concurrent_streams`
- `cluster.routing.allocation.node_initial_primaries_recoveries`, `cluster.routing.allocation.node_concurrent_recoveries`
- `indices.cache.filter.size`

Logger values can also be updated by setting `logger.` prefix. More settings will be allowed to be updated.

Cluster wide settings can be returned using `curl -XGET localhost:9200/_cluster/settings`.
</description><key id="1448383">1266</key><summary>Cluster Update Settings API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-08-20T01:00:11Z</created><updated>2015-03-19T07:11:17Z</updated><resolved>2011-08-20T01:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ritu2p" created="2015-03-18T22:31:12Z" id="83209647">What are the various "logger." settings? I am looking to turn on log level to DEBUG for a particular class and route the debug messages to a different appender. 
I am aware of the "logger.cluster.service" : "DEBUG" setting ... however, I don't want to set the log level of the entire cluster to "debug".

Is this possible?
</comment><comment author="javanna" created="2015-03-19T07:11:17Z" id="83376438">Hi @ritu2p I'd suggest to ask this type of questions on the mailing list rather than commenting on this issue that was closed years ago. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActions.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/Requests.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/cluster/settings/ClusterUpdateSettingsRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/node/NodeClusterAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/ClientTransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/admin/cluster/settings/ClientTransportClusterUpdateSettingsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/NodeAllocations.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ShardsAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ThrottlingNodeAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/settings/ClusterSettingsService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/logging/ESLogger.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/logging/jdk/JdkESLogger.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/logging/log4j/Log4jESLogger.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/logging/slf4j/Slf4jESLogger.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/recovery/RecoverySource.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/XContentRestResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterGetSettingsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/cluster/settings/RestClusterUpdateSettingsAction.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java</file></files><comments><comment>Cluster Update Settings API, closes #1266.</comment></comments></commit></commits></item><item><title>Java Logging: Automatically default to log4j before slf4j logging if log4j in the classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1265</link><project id="" key="" /><description>Java Logging: Automatically default to log4j before slf4j logging if log4j in the classpath
</description><key id="1448301">1265</key><summary>Java Logging: Automatically default to log4j before slf4j logging if log4j in the classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-20T00:26:34Z</created><updated>2013-06-26T16:08:38Z</updated><resolved>2011-08-20T01:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/logging/ESLoggerFactory.java</file></files><comments><comment>Java Logging: Automatically default to log4j before slf4j logging if log4j in the classpath, closes #1265.</comment></comments></commit></commits></item><item><title>Geo: Automatically normalize lat/lon on search components</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1264</link><project id="" key="" /><description>The other part of normalization that happens on the indexing side, should be done on all relevant search components that handle geo location. All of them by default normalize location now, this can be turned off by setting `normalize` to `false` on any of them.
</description><key id="1445217">1264</key><summary>Geo: Automatically normalize lat/lon on search components</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-19T17:19:34Z</created><updated>2011-08-19T17:20:02Z</updated><resolved>2011-08-19T17:20:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoUtils.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/LatLng.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file></files><comments><comment>Geo: Automatically normalize lat/lon on search components, closes #1264.</comment><comment>Glad you fixed the bug on `index.mapper.geo.GeoPointFieldMapper` causing infinite loops for lng = -360-180 = -540 (among other) because of the while test comparison being non strict.</comment><comment>However -180 and 180 can still be returned, they should instead be equal to the same value. Idem for -0.0 and 0.0.</comment><comment>Moreover, this implementation can be **very slow** for huge positive or negative values.</comment></comments></commit></commits></item><item><title>NPE in nested queries with range clauses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1263</link><project id="" key="" /><description>Hiya

Using range clauses in nested queries throws an NPE, but the same query as a term query doesn't:

```
# [Fri Aug 19 09:55:23 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "mappings" : {
      "location" : {
         "properties" : {
            "hours" : {
               "include_in_root" : 1,
               "type" : "nested",
               "properties" : {
                  "open" : {
                     "type" : "integer"
                  },
                  "close" : {
                     "type" : "integer"
                  },
                  "day" : {
                     "index" : "not_analyzed",
                     "type" : "string"
                  }
               }
            },
            "name" : {
               "type" : "string"
            }
         }
      }
   }
}
'

# [Fri Aug 19 09:55:23 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Fri Aug 19 09:55:27 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XPOST 'http://127.0.0.1:9200/foo/location?pretty=1'  -d '
{
   "hours" : [
      {
         "open" : 9,
         "close" : 12,
         "day" : "monday"
      },
      {
         "open" : 13,
         "close" : 17,
         "day" : "monday"
      }
   ],
   "name" : "Test"
}
'

# [Fri Aug 19 09:55:27 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "GjuRCXsVR3qZwNciqA1yuA",
#    "_type" : "location",
#    "_version" : 1
# }

# [Fri Aug 19 09:55:29 2011] Protocol: http, Server: 127.0.0.2:9200
curl -XGET 'http://127.0.0.1:9200/foo/location/_search?pretty=1'  -d '
{
   "query" : {
      "nested" : {
         "query" : {
            "bool" : {
               "must" : [
                  {
                     "text" : {
                        "hours.day" : "monday"
                     }
                  },
                  {
                     "range" : {
                        "hours.close" : {
                           "gte" : 10
                        }
                     }
                  },
                  {
                     "range" : {
                        "hours.open" : {
                           "lte" : 10
                        }
                     }
                  }
               ]
            }
         },
         "path" : "hours"
      }
   }
}
'

# [Fri Aug 19 09:55:29 2011] Response:
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : null,
#       "total" : 0
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failures" : [
#          {
#             "index" : "foo",
#             "status" : 500,
#             "reason" : "QueryPhaseExecutionException[[foo][1]: qu
# &gt;             ery[BlockJoinQuery (filtered(+hours.day:monday +hou
# &gt;             rs.close:[10 TO *] +hours.open:[* TO 10])-&gt;FilterCa
# &gt;             cheFilterWrapper(_type:__hours))],from[0],size[10]:
# &gt;              Query Failed [Failed to execute main query]]; nest
# &gt;              ed: ",
#             "shard" : 1
#          }
#       ],
#       "failed" : 1,
#       "successful" : 4,
#       "total" : 5
#    },
#    "took" : 4
# }
```

And in the logs:

```
[09:57:37,516][DEBUG][action.search.type       ] [Andrew Gervais] [foo][4], node[Dk6W4_9GQMisu1sTfo3Zyg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7e383efa]
org.elasticsearch.search.query.QueryPhaseExecutionException: [foo][4]: query[BlockJoinQuery (filtered(+hours.day:monday +hours.close:[10 TO *] +hours.open:[* TO 10])-&gt;FilterCacheFilterWrapper(_type:__hours))],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:221)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:234)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:204)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:191)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:177)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.query.NestedQueryParser$LateBindingParentFilter.getDocIdSet(NestedQueryParser.java:171)
    at org.elasticsearch.index.search.nested.BlockJoinQuery$BlockJoinWeight.scorer(BlockJoinQuery.java:171)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:116)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:198)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:391)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:298)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:286)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:217)
    ... 9 more
```
</description><key id="1439254">1263</key><summary>NPE in nested queries with range clauses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-08-19T07:58:51Z</created><updated>2011-12-14T13:05:26Z</updated><resolved>2011-08-25T10:02:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-24T20:34:54Z" id="1894001">Can't seem to recreate it on master (and it should be the same on 0.17 branch). Tried with 1 and 2 nodes. Which version does it happen for you on? Can you try on master?
</comment><comment author="clintongormley" created="2011-08-25T10:02:02Z" id="1898598">Hmm, bizarre.  This was failing for me every time that I ran it - I tried several versions of the range clause, and it just wouldn't work.  

Since then I have upgraded to 0.17.6, and it works.  I have gone back to version 0.17.4 and 0.17.5 (which I think was what i was using when I had this issue) and it still works.

Thanks for looking, I'll close this issue
</comment><comment author="deverton" created="2011-10-27T05:29:25Z" id="2539869">Hrm, I'm getting this issue on 0.18.1. I'll see if I can boil it down to a test case.
</comment><comment author="deverton" created="2011-10-27T05:39:33Z" id="2539924">Sorry, I meant 0.17.8. It fails for me with the exact test case above. Just upgraded to 0.18.1 and can't reproduce it.
</comment><comment author="deverton" created="2011-12-08T05:41:00Z" id="3058200">Okay, I am still seeing this on 0.18.4. It's intermittent and only affects some shards sometimes, but it's there. The test case for this issue doesn't seem to tickle it and the query I'm using is pretty big and complicated. It also uses a facet with a filter but the stack trace is exactly the same.

``` java
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.query.NestedQueryParser$LateBindingParentFilter.hashCode(NestedQueryParser.java:159)
    at org.elasticsearch.index.search.nested.BlockJoinQuery.hashCode(BlockJoinQuery.java:423)
    at org.apache.lucene.search.QueryWrapperFilter.hashCode(QueryWrapperFilter.java:72)
    at java.util.AbstractList.hashCode(AbstractList.java:527)
    at org.elasticsearch.common.lucene.search.AndFilter.hashCode(AndFilter.java:72)
```
</comment><comment author="kimchy" created="2011-12-14T13:05:26Z" id="3139180">relates to #1536
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo Type Mapping: Add normalize flag (default to true), and default validate to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1262</link><project id="" key="" /><description>Add a `normalize` mapping to `geo_type` that automatically normalized longitude (to value between `-180` and `180`) and latitude (to values between `-90` and `90`). Specific `normalize_lat` and `normalize_lon` can be set. 

Also, by default, perform validation on lat and lon provided (since we normalize by default, its fine to do).
</description><key id="1438589">1262</key><summary>Geo Type Mapping: Add normalize flag (default to true), and default validate to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-19T04:23:48Z</created><updated>2011-08-19T04:24:52Z</updated><resolved>2011-08-19T04:24:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/geopoint/LatLonMappingGeoPointTests.java</file></files><comments><comment>Geo Type Mapping: Add normalize flag (default to true), and default validate to true, closes #1262.</comment></comments></commit></commits></item><item><title>Geo Distance Filter Bounding Box Optimization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1261</link><project id="" key="" /><description>Automatically use bounding box around geo distance filters in order to reduce amount of distance based computations needed. Also, reuse computed values for the consistent source lat/lon used to check against docs.

The optimization can be disabled by settings `optimize_bbox` to `false.
</description><key id="1438443">1261</key><summary>Geo Distance Filter Bounding Box Optimization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-19T03:42:47Z</created><updated>2011-08-19T04:07:12Z</updated><resolved>2011-08-19T04:07:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/geo/GeoDistanceSearchBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointDocFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoBoundingBoxFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistance.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/Point.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/search/geo/GeoDistanceTests.java</file></files><comments><comment>Geo Distance Filter Bounding Box Optimization, closes #1261.</comment></comments></commit></commits></item><item><title>Query Parser caching does not take parsed query rewrite method into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1260</link><project id="" key="" /><description>This results in changing the rewrite method to not take affect.
</description><key id="1436071">1260</key><summary>Query Parser caching does not take parsed query rewrite method into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-18T22:57:00Z</created><updated>2011-08-18T22:57:19Z</updated><resolved>2011-08-18T22:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/QueryParserSettings.java</file></files><comments><comment>Query Parser caching does not take parsed query rewrite method into account, closes #1260.</comment></comments></commit></commits></item><item><title>Query DSL: Add ids_prefix query and filter to fetch docs by prefix ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1259</link><project id="" key="" /><description>Relative to issue #865

This allows to not have the _id field indexed, but still be able to fetch docs by prefix id. 

The query and filter format could be :

``` javascript
{
    "ids_prefix" : {
        "type" : "my_type",
        "values" : ["1", "4", "10"]
    }
}
```

or

``` javascript
{
    "ids_prefix" : {
        "type" : "my_type",
        "prefix" : "1"
    }
}
```
</description><key id="1435773">1259</key><summary>Query DSL: Add ids_prefix query and filter to fetch docs by prefix ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-08-18T22:11:07Z</created><updated>2012-01-29T19:09:55Z</updated><resolved>2012-01-29T19:09:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-29T19:09:55Z" id="3710368">Implemented in #1648. No need for explicit query / filter, the current `prefix` filter/query will support it automatically.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CouchDB river : add support for views</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1258</link><project id="" key="" /><description>In CouchDB, you can retrieve docs by `GET`, `_changes` API and views.
CouchDB river uses `_changes` API to get documents.

I would like to be able to get documents that changed (getting ID with the _changes API) using a view with parameter `key="DOCID"`.

As views return a collection of results (aka rows), we will index in ES each row with an id like DOCID_seq where seq is the sequence number of each row.
If you get back 3 rows for one single change for document with ID=1234, the river will index 3 documents :
- 1234_1
- 1234_2
- 1234_3

To use it, you have to define a view in couchDB. For instance, `_design/vues/_view/test_dpi` with 

``` javascript
function(doc) {
  listArt=doc.document.articles;
  var numeroArticle;
  var codeMarchandise;
  var designationCommerciale;
  var mb;
  var nombreColis; 

  for(var i=0; i&lt;listArt.length;i++) {  
   var artJson = {};
   numeroArticle=listArt[i].numeroArticle; 
   codeMarchandise=listArt[i].codeMarchandise;
   designationCommerciale=listArt[i].designationCommerciale;
   mb=listArt[i].masseBrute;   
   nombreColis=listArt[i].nombreColis;   
   artJson = { 'numeroArticle' : numeroArticle , 'codeMarchandise' : codeMarchandise , 'designationCommerciale' : designationCommerciale ,'masseBrute' : mb , 'nombreColis' : nombreColis };
   artJson =  JSON.stringify( artJson );

   emit(doc._id, eval('('+artJson+')') );
  };
}
```

You can use it in your couchDb river as follow :

``` javascript
{
  "type":"couchdb",
  "couchdb": {
    "host":"localhost",
    "port":"5984",
    "db":"dau_test",
    "view":"vues/_view/test_dpi",
    "viewIgnoreRemove":false
  }
}
```

New options :
- `view` : if not null, couchDB river will not fetch content from `_changes` API but only IDs and then will use the view to retrieve rows using the ID as a key. By default : null
- `viewIgnoreRemove` : ask the river to ignore removal of rows if there is less rows after a document update. By default : false so non existing rows will be removed from elastic search.

For example, with the 3 rows described earlier, if you push a new version of the document 1234 in couchDB with only 2 docs, 

If `viewIgnoreRemove` is false (default), then
- 1234_1 will be updated
- 1234_2 will be updated
- 1234_3 will be removed

If `viewIgnoreRemove` is true, then
- 1234_1 will be updated
- 1234_2 will be updated
- 1234_3 will not be updated

I hope I wrote it in the right way. Any comments are welcome...

BTW, I will push an update when the `ids_prefix` filter will be available to make code more efficient. (See issue #1259)

Thanks
</description><key id="1435371">1258</key><summary>CouchDB river : add support for views</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-08-18T21:14:03Z</created><updated>2014-06-12T07:13:44Z</updated><resolved>2011-10-05T21:35:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2011-08-19T08:33:15Z" id="1849189">Ooouch. I'm not a Git expert so I added the commit 270d7e0 to this pull request instead of opening a new pull request for the attachement bypass option...

So, what can I do now ? Is there anyway to remove the last commit to this pull request ?
Or do I complete my pull request by giving some details about the new option ?

Thanks (and sorry ;-) )
</comment><comment author="kimchy" created="2011-08-24T20:23:48Z" id="1893919">I think we should have two different pull requests for those. Lemme first also work on the ids prefix filter so we can have the better solution baked right in. Not sure I fully followed what it does though :)
</comment><comment author="dadoonet" created="2011-08-26T17:25:44Z" id="1914890">Ok. Pull request #1283 created for the ignoreAttachements new option. I will try to update this one (or will close it and open another one if I don't suceed).
</comment><comment author="jdzurik" created="2011-10-06T01:44:52Z" id="2305602">Do you think the functionality of pulling from a view will be added to the couchdb river?
</comment><comment author="dadoonet" created="2011-10-06T19:33:39Z" id="2314022">Hi there,

Not sure of what happened with my pull request : https://github.com/elasticsearch/elasticsearch/commit/fc0e03cd3f34d6765c1934540a15cfb8770b5c55

I think that I did a stupid thing yesterday with git and my elasticsearch fork...
I need more training about git ! Shame on me !

Do I have to create a new pull request for this issue ?
</comment><comment author="jdzurik" created="2011-11-28T19:25:05Z" id="2905739">I was looking through the release notes for 18 and I don't the ability to create a couchdb river for a view is this something that's not getting implemented or is it just being worked on from other angles?
</comment><comment author="dadoonet" created="2011-11-28T19:52:58Z" id="2906244">No. It's not here.
I make some mess with Github (and raise an issue at GitHub support) so, the pull request seems to be closed but in fact, it's not there.
I'm not sure that I can find my code again :-( I will try and make a new pull request for it, although I'm waiting for issue #1259 to be solved.
Cheers
David.
</comment><comment author="dadoonet" created="2011-11-30T21:51:14Z" id="2967047">@jdzurik : I worked on it again. You can give it a try and let me know.
Source code is here : https://github.com/dadoonet/elasticsearch/tree/couchdbriver_views
Documentation is updated here : https://github.com/dadoonet/elasticsearch.github.com/tree/couchdbriver_views
See https://github.com/dadoonet/elasticsearch.github.com/commit/a7fac2940cc01e43f27f4ec65c3ff346e7cb1bbf

Please let me know if it answers to your needs. If so, I will send a pull request for it.

David.
</comment><comment author="benjelloun23" created="2014-04-29T15:13:44Z" id="41689071">Hello David,

I installed ElasticSearch, its work good i can index and search xml and json content using Dev HTTP Client.
I need your help to index binary files in elasticsearch then search for them by content.
I added mapper-attachements to elastic search but what i dont know is how to specify the folder of pdf or docx files to index it. something like base64 or i dont know.
Thanks for helping me.

sincerely,
</comment><comment author="dadoonet" created="2014-04-29T15:28:30Z" id="41691065">I think you misunderstood what I answered to your private email.

If you need to ask public question, please use the mailing list. You can have more details on how to do it here: http://www.elasticsearch.org/help/

But please, don't hijack issues or pull requests.
Thanks
</comment><comment author="benjelloun23" created="2014-04-29T15:38:13Z" id="41692344">ok i'm sorry for misunderstanding you and thanks for help you are a good man(Professional)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>dynamic templates might cause elasticsearch to keep resync mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1257</link><project id="" key="" /><description>dynamic templates might cause elasticsearch to keep resync mappings
</description><key id="1434111">1257</key><summary>dynamic templates might cause elasticsearch to keep resync mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-18T18:12:48Z</created><updated>2011-08-18T18:13:21Z</updated><resolved>2011-08-18T18:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file></files><comments><comment>dynamic templates might cause elasticsearch to keep resync mappings, closes #1257.</comment></comments></commit></commits></item><item><title>fix hightlight score ordering for a field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1256</link><project id="" key="" /><description>Just by reading the code I noticed a issue with the parser for highlighting. Here is the fix. Not tested.
</description><key id="1433286">1256</key><summary>fix hightlight score ordering for a field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-08-18T16:16:42Z</created><updated>2014-07-16T21:56:23Z</updated><resolved>2011-08-18T17:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>replication rack awareness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1255</link><project id="" key="" /><description>It should be possible to be able to have replicas assigned to nodes in different racks/groups.

This could be done by providing a node setting like "rack" or "group" and then assign replicas to groups not matching the own group.

In case of an aws datacenter going down this would help to survive with only a minimum of replicas.
</description><key id="1425051">1255</key><summary>replication rack awareness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jukart</reporter><labels /><created>2011-08-17T18:08:22Z</created><updated>2011-09-23T19:06:42Z</updated><resolved>2011-09-23T14:29:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-18T20:00:32Z" id="1842414">Agreed, its _very_ high on the list of features to implement.
</comment><comment author="kimchy" created="2011-09-23T14:29:14Z" id="2179101">Implemented in #1352.
</comment><comment author="jukart" created="2011-09-23T19:06:42Z" id="2181782">Thanks a lot, great work
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enhance transport stats to include rx and tx counters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1254</link><project id="" key="" /><description>The NetworkService module exposes some network level data, but doesn't really show bandwidth related statistics that elasticsearch is dealing with.

What would be sensational would be to expose:
- A nodes inbound/outbound traffic that is going from/to elasticsearch clients
- top level 'node requests' as a counter would be good.  ie as each request hits a node from a client, a counter would increment.  One would get course grained information on overall combined search/index activity from this.
- Internal node traffic (intra-cluster comms)

slight divergence, but 
- Gateway snaphotting as it wrote bytes out would be interesting
</description><key id="1422788">1254</key><summary>Enhance transport stats to include rx and tx counters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tallpsmith</reporter><labels /><created>2011-08-17T13:08:02Z</created><updated>2011-09-01T22:09:07Z</updated><resolved>2011-08-20T18:37:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-20T18:08:37Z" id="1860371">About to push this, it will include rx/tx invocation count and bytes count. Also, there will be a new `NodeService` that can be injected to plugins where you can simply call `#info` or `#stats` on them to get the node level info and stats.
</comment><comment author="tallpsmith" created="2011-08-29T02:37:50Z" id="1926886">thanks muchly Shay, I've been in Vietnam all week (alas, work), so will endeavour to integrate this this week once I get through the tsunami of emails.  appreciate it!
</comment><comment author="tallpsmith" created="2011-09-01T06:24:24Z" id="1965347">Shay, i'm getting an NPE early during initialisation because part of the stats isn't setup with an object stat just yet.  It's failing here:

org.elasticsearch.transport.TransportService#stats

Line:  112
    public TransportStats stats() {
        return new TransportStats(transport.serverOpen(), adapter.rxMetric.count(), adapter.rxMetric.sum(), adapter.txMetric.count(), adapter.txMetric.sum());
    }

one of these is null early on during the initialisation of the plugin.  The stack failure is:

https://gist.github.com/1185581

Even if I move my initialisation to the doStart() method of the plugin, I'm still getting an exception, except the stack is being swallowed:

[2011-09-01 16:20:51,082][INFO ][com.custardsource.parfait.pcp.PcpMonitorBridge] PCP monitoring bridge started for writer [PcpMmvWriter[byteBufferFactory=FileByteBufferFactory[file=/private/var/tmp/mmv/elasticsearch-server.mmv]]]
[2011-09-01 16:20:51,089][ERROR][bootstrap                ] [Atom Bob] {elasticsearch/0.18.0-SNAPSHOT/2011-09-01T02:38:23}: Startup Failed ...
- RuntimeException[java.lang.NullPointerException]

paul:elasticsearch-0.18.0-SNAPSHOT paulsmith$ 
</comment><comment author="kimchy" created="2011-09-01T16:19:12Z" id="1969774">@tallpsmith: ok, I think I fixed it with the latest commit, can you check? btw, why do you need the stats on startup? I would imagine you need it periodically
</comment><comment author="tallpsmith" created="2011-09-01T22:09:07Z" id="1974055">We grab an initial value on startup and then poll. I originally thought I
was dumb to put it in the constrictor init. but even in te doStartup method
which I think it realm should go didn't work. After a plugin has seen the
doStart event I would have thought it's safe to query any object it has been
initialized with by injection?

Thanks I will check out the latest push.

On Friday, 2 September 2011, kimchy &lt;
reply@reply.github.com&gt;
wrote:

&gt; @tallpsmith: ok, I think I fixed it with the latest commit, can you check?
&gt; btw, why do you need the stats on startup? I would imagine you need it
&gt; periodically
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1254#issuecomment-1969774
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/counter/SimpleCounterBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/NodeModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/Transport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>plugins/transport/memcached/src/main/java/org/elasticsearch/memcached/MemcachedServer.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftServer.java</file></files><comments><comment>Enhance transport stats to include rx and tx counters, closes #1254.</comment></comments></commit></commits></item><item><title>Docs - Snowball depricated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1253</link><project id="" key="" /><description>The docs should mention that Snowball is depricated and that the Language analyzers should be used instead.

Discussion: https://groups.google.com/d/topic/elasticsearch/cLiWGiHvoQE/discussion
</description><key id="1421645">1253</key><summary>Docs - Snowball depricated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-08-17T09:11:33Z</created><updated>2013-04-05T14:59:39Z</updated><resolved>2013-04-05T14:59:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T14:59:39Z" id="15960541">Snowball isn't deprecated, so much as the less preferred option. Still has its uses, so I think things are fine as they are.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo Type Mapping: Add validation options to validate lat and lon values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1252</link><project id="" key="" /><description>Allow to set in the mapping:
- `validate_lat`: Validates that latitude is between `-90` and `90`.
- `validate_lon`: Validates that longitude is between `-180` and `180`.
- `validate`: Validates both lat and lon values.

By default, no validation is done for backward comp. 
</description><key id="1420123">1252</key><summary>Geo Type Mapping: Add validation options to validate lat and lon values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-17T02:21:06Z</created><updated>2011-08-17T02:55:06Z</updated><resolved>2011-08-17T02:55:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/geopoint/LatLonMappingGeoPointTests.java</file></files><comments><comment>Geo Type Mapping: Add validation options to validate lat and lon values, closes #1252.</comment></comments></commit></commits></item><item><title>Update Settings: Allow to dynamically set index.gc_deletes setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1251</link><project id="" key="" /><description>Allow to dynamically set this setting, allowing to increase it, for example, when doing scan based reindexing of data.
</description><key id="1419508">1251</key><summary>Update Settings: Allow to dynamically set index.gc_deletes setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-16T23:48:30Z</created><updated>2011-08-16T23:59:00Z</updated><resolved>2011-08-16T23:59:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>Update Settings: Allow to dynamically set index.gc_deletes setting, closes #1251.</comment></comments></commit></commits></item><item><title>Documents with invalid lat/lon completely breaks spatial search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1250</link><project id="" key="" /><description>It seems that some documents with invalid lat/lon made it into ElasticSearch. Now when I execute a spatial query, I get complete failure.

My query:

```
{'query': {'filtered': {'filter': {'geo_distance': {'distance': '25km', 'geometry.coordinates': {'lat': Decimal('47.60856'), 'lon': Decimal('-122.34135')}}}, 'query': {'match_all': {}}}}}
```

Result:

SearchPhaseExecutionException: Failed to execute phase [query], total failure; shardFailures {[dxd69J4eRp-Zu7PFkcGwhg][places][3]: QueryPhaseExecutionException[[places][3]: query[ConstantScore(org.elasticsearch.index.search.geo.GeoDistanceFilter@b8291ad0)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalArgumentException[Illegal latitude value -111.746345]; }{[ONA_8qrXTKyCsLceDCaXSQ][places][0]: RemoteTransportException[[Kogar][inet[/10.162.115.15:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[places][0]: query[ConstantScore(org.elasticsearch.index.search.geo.GeoDistanceFilter@37b59344)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalArgumentException[Illegal latitude value -92.604507]; }{[FtfYWzHySJe8iVdWIgJt8Q][places][2]: RemoteTransportException[[Astron][inet[/10.176.17.192:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[places][2]: query[ConstantScore(org.elasticsearch.index.search.geo.GeoDistanceFilter@ab3aa48f)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalArgumentException[Illegal latitude value -98.237867]; }{[ONA_8qrXTKyCsLceDCaXSQ][places][1]: RemoteTransportException[[Kogar][inet[/10.162.115.15:9300]][search/phase/query]]; nested: QueryPhaseExecutionException[[places][1]: query[ConstantScore(org.elasticsearch.index.search.geo.GeoDistanceFilter@37b59344)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalArgumentException[Illegal latitude value -117.518814]; }{[dxd69J4eRp-Zu7PFkcGwhg][places][4]: QueryPhaseExecutionException[[places][4]: query[ConstantScore(org.elasticsearch.index.search.geo.GeoDistanceFilter@b8291ad0)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalArgumentException[Illegal latitude value 120.927147]; }

One or both of two solutions would be good:
1. Refuse to index documents where lat/lon are out of bounds. Solr does this.
2. Skip the documents with invalid coordinates when `IllegalArgumentException` is raised when filtering, rather than giving up and breaking the whole search.
</description><key id="1419418">1250</key><summary>Documents with invalid lat/lon completely breaks spatial search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ieure</reporter><labels /><created>2011-08-16T23:28:40Z</created><updated>2013-07-01T10:46:16Z</updated><resolved>2013-07-01T10:46:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-17T02:21:46Z" id="1822502">Opened #1252 for option 1, which is validation. Lets keep this issue open for option 2 as well.
</comment><comment author="spinscale" created="2013-07-01T10:46:16Z" id="20274601">This problem is fixed. See this example, which correctly returns one valid result on the search.

```
curl -X DELETE localhost:9200/geo
curl -X PUT localhost:9200/geo
curl -X PUT localhost:9200/geo/test/_mapping -d '{ "test" : { "properties" : { "point" : { "type":"geo_point" } } } }'
curl localhost:9200/geo/test/_mapping

curl -X PUT 'localhost:9200/geo/test/1?refresh=true' -d '{ "point" : { lat:"a", lon:"b" } }'
curl -X PUT 'localhost:9200/geo/test/2?refresh=true' -d '{ "point" : { lat:-200, lon:-200 } }'
curl -X PUT 'localhost:9200/geo/test/3?refresh=true' -d '{ "point" : { lat:40, lon:-70 } }'

curl -X POST localhost:9200/geo/test/_search -d '{ "filter" : { "geo_distance" : { "point": { "lat": 40, "lon":-70 }, "distance":"10km" } } }'
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Single node rolling restart into a new node can cause metadata loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1249</link><project id="" key="" /><description>A specific scenario, where rolling restart of a single node, without any metadata change, can cause loss of metadata. For example, starting one, working against it, then starting another node, shutting down the first node, shutting down the second node (without any metadata changes, like new indices / updated mappings), and starting up the second node, can cause metadata loss.
</description><key id="1415413">1249</key><summary>Single node rolling restart into a new node can cause metadata loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-16T14:17:35Z</created><updated>2011-08-16T14:18:37Z</updated><resolved>2011-08-16T14:18:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/gateway/local/SimpleRecoveryLocalGatewayTests.java</file></files><comments><comment>Single node rolling restart into a new node can cause metadata loss, closes #1249.</comment></comments></commit></commits></item><item><title>docs for query_string query should mention shorthand AND and OR</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1248</link><project id="" key="" /><description>The query_string docs don't mention that + and - can be used with keywords.  That would be handy (for noobs like me) to know. Same thing with quoting phrases for exact matches.  It would be nice to have it listed that query_string supports all those things.  Examples would make it even nicer.  Thanks!

http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query.html
</description><key id="1410560">1248</key><summary>docs for query_string query should mention shorthand AND and OR</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zahna</reporter><labels /><created>2011-08-15T20:58:30Z</created><updated>2013-04-05T15:33:16Z</updated><resolved>2013-04-05T15:33:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Silex" created="2012-04-27T07:09:15Z" id="5375781">Yeah it'd mention a link to http://lucene.apache.org/core/3_6_0/queryparsersyntax.html
</comment><comment author="clintongormley" created="2013-04-05T15:33:16Z" id="15962831">Full docs are being added. Written, just need to be published
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tiered merge policy: Change the default max_merge_segment from 5gb to 20gb</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1247</link><project id="" key="" /><description>The default lucene value for `tiered` merge policy seems too low, increase the default value.
</description><key id="1409606">1247</key><summary>Tiered merge policy: Change the default max_merge_segment from 5gb to 20gb</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label></labels><created>2011-08-15T18:36:11Z</created><updated>2011-09-12T18:39:55Z</updated><resolved>2011-08-15T18:36:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-09-12T18:39:55Z" id="2073826">Revert the change just not to create confusion.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java</file></files><comments><comment>Tiered merge policy: Change the default max_merge_segment from 5gb to 20gb, closes #1247.</comment></comments></commit></commits></item><item><title>Make installed plugins available over the node API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1246</link><project id="" key="" /><description>I would like to be able to check over the node API if a node has a certain plugin installed or retrieve the list of installed plugins.

For more details see google groups post:

https://groups.google.com/forum/#!topic/elasticsearch/QVRR9Q6VuG0
</description><key id="1407845">1246</key><summary>Make installed plugins available over the node API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ruflin</reporter><labels /><created>2011-08-15T14:27:21Z</created><updated>2013-04-05T15:35:06Z</updated><resolved>2013-04-05T15:35:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T15:35:06Z" id="15962928">Duplicate of #2668 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Option to use _id path to extract in place of auto-id generation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1245</link><project id="" key="" /><description>Elasticsearch needs to know the id in order to control the routing of the document indexed to a shard. If there is no id value specified in the IndexRequest, an id is auto-generated. If that auto id differs from the _id value in the document an error occurs. 

Allow for a mapping setting to "search" for the id (something like id path, similar in nature to routing path).

The only reason it makes sense to have this option is because ES is parsing the bytes to a JSON-equivalent structure to check the _id value of the document. There is little sense to do this in the client if ES is already doing it. 

The `path` element can be set on the `_id` mapping, allowing to set a path for it (similar to routing), if no id is provided, and path is set, the path will be used to extract the id from the source. This does require another (quick) round of parsing the source document.
</description><key id="1404646">1245</key><summary>Option to use _id path to extract in place of auto-id generation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oravecz</reporter><labels><label>feature</label><label>v0.18.0</label></labels><created>2011-08-14T22:37:51Z</created><updated>2011-08-30T22:05:05Z</updated><resolved>2011-08-30T22:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-30T22:05:05Z" id="1950161">Implemented by #1290.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>wrong method signature: RangeFilterBuilder and NumericRangeFilterBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1244</link><project id="" key="" /><description>I think NumericRangeFilterBuilder.lte(String to) should be changed to .lte(Object to) to be consistent with the other methods.

public NumericRangeFilterBuilder gt(Object from);
public NumericRangeFilterBuilder gte(Object from);
public NumericRangeFilterBuilder lt(Object to);
public NumericRangeFilterBuilder lte(String to); &lt;- seems to be not intended

This problem also exists in RangeFilterBuilder.

Regards, Daniel
</description><key id="1402698">1244</key><summary>wrong method signature: RangeFilterBuilder and NumericRangeFilterBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dawi</reporter><labels><label>bug</label><label>v0.17.7</label><label>v0.18.0</label></labels><created>2011-08-14T10:31:36Z</created><updated>2011-08-14T21:06:36Z</updated><resolved>2011-08-14T21:06:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NumericRangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file></files><comments><comment>wrong method signature: RangeFilterBuilder and NumericRangeFilterBuilder, closes #1244.</comment></comments></commit></commits></item><item><title>Unknown language causes snowball analyzer to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1243</link><project id="" key="" /><description>I used 'en' instead of 'English':

```
# [Sat Aug 13 18:34:08 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "settings" : {
      "analysis" : {
         "analyzer" : {
            "en" : {
               "language" : "en",
               "type" : "snowball"
            }
         }
      }
   }
}
'

# [Sat Aug 13 18:34:08 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Aug 13 18:34:10 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/foo/_analyze?pretty=1&amp;text=The+Brown-Cow's+Part_No.+%23A.BC123-456+joe%40bloggs.com&amp;analyzer=en' 
```
</description><key id="1400347">1243</key><summary>Unknown language causes snowball analyzer to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-08-13T16:35:45Z</created><updated>2013-04-05T15:36:32Z</updated><resolved>2013-04-05T15:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T15:36:32Z" id="15963012">Fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changes API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1242</link><project id="" key="" /><description>There should be an integration point for ES and external application where the external applications should be notified of any document changes or updates that happens in ES.

CouchDB have a good implementation on it and it would be great if ES can also incorporate something similar or same.

CouchDB change notification feature - http://guide.couchdb.org/draft/notifications.html
</description><key id="1400233">1242</key><summary>Changes API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>:Changes API</label><label>feature</label><label>high hanging fruit</label><label>stalled</label></labels><created>2011-08-13T15:45:14Z</created><updated>2017-03-27T08:36:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rufuspollock" created="2011-10-16T14:47:43Z" id="2421223">Hi, I want to register a big +1 on this.With the versioning system now in place in ES I imagine this should be possible and would make a lot of stuff a lot easier (from the simple such as generating RSS/Atom feeds to the more complex such as syncing between distinct federated ES clusters).

Some questions for implementation:
- By default changes would be per index (e.g. i'd have /twitter/tweet/_changes) but we may also want to get all changes per "database" e.g. /twitter/_changes (all changes for all indexes under /twitter)
- Changes need an incrementing unique id to make sync possible (or need to be timestamped in a consistent way). E.g. I need to be able to say: give me a list of all changed documents since change {X}. (Otherwise I have to pull _all_ changes and scan them them to check which documents are affected)
</comment><comment author="kimchy" created="2011-10-16T21:25:30Z" id="2423126">@rgrp: agreed on the need, versioning plays a part in this, but there is still a lot to be implemented to make this happen. A note on what you said regarding changes, I agree that there should be a _changes feed for an index, and across all the cluster. But, what you noted was _changes feed per _type_ (`/twitter/tweet` - `twitter` is the index, and `tweet` is the type), and one per index (`/twitter/`).
</comment><comment author="Vineeth-Mohan" created="2011-10-21T15:29:04Z" id="2482590">Dependent on issue #1077
</comment><comment author="derryx" created="2011-10-25T15:34:49Z" id="2518713">I would prefer a solution where I can hook in and get informed by Elasticsearch about events rather than polling on a _changes URL.
</comment><comment author="Vineeth-Mohan" created="2011-10-25T15:37:56Z" id="2518759">Hope this is similar to what you are looking for - http://guide.couchdb.org/draft/notifications.html#continuous
</comment><comment author="rufuspollock" created="2011-10-26T08:59:34Z" id="2528143">@kimchy: thanks for correction on terminology :-) and appreciate this may not be straightforward (big thank-you for all your great work so far).

@derryx (and @Vineeth-Mohan): agreed that one wants push rather than pull notifications like continuous notification in couch. However, this may be harder to do with a java-based backend rather than an erlang one as in erlang it's not really a problem to keep a permanent http connection open with the client.
</comment><comment author="derryx" created="2011-10-26T09:26:01Z" id="2528341">Tomcat has something similar for Ajax push to the browser. They call it "comet-call" because of the long "tail":
http://tomcat.apache.org/tomcat-7.0-doc/aio.html#Comet_support

So it should be no problem to support this with Java.
</comment><comment author="derryx" created="2012-02-27T20:09:33Z" id="4201854">I have coded a plugin that provides change information. It is a first start and will be extended in the future. You can find it here: https://github.com/derryx/elasticsearch-changes-plugin
</comment><comment author="Vineeth-Mohan" created="2012-02-28T02:16:58Z" id="4208163">@derryx  - thanks a ton man. this looks cool.
</comment><comment author="jprante" created="2012-03-30T16:00:14Z" id="4845405">If you consider client connections to a _changes API for notifications, a performant, scalable alternative to Comet is  WebSocket. Implemented already in netty, and Elasticsearch uses netty :)
</comment><comment author="derryx" created="2012-04-04T12:56:08Z" id="4952697">The cool thing about websockets is that they are bidirectional. This is not needed here. A persistent HTTP-connection is good enough. The problems currently are more that the current HTTP-transport of ES does not support persistent connections and to get all the changes from ES.
</comment><comment author="kimchy" created="2012-04-04T13:33:55Z" id="4953385">@jprante the websockets part is cool, and can definitely possibly be used as way to stream changes, but the harder part is building the whole changes infrastructure...
</comment><comment author="jprante" created="2012-04-05T11:41:37Z" id="4974916">One more thought. WebSocket is also available via XMPP, and XMPP is a robust solution for a distributed notification infrastructure. So how about including a simple lightweight websocket client into each ES node for sending notifications via XMPP? Maybe with the help of Atmosphere https://github.com/Atmosphere/atmosphere ? API doc for an example Websocket pubsub can be found here http://atmosphere.github.com/atmosphere/apidocs/org/atmosphere/samples/pubsub/WebSocketPubSub.html
</comment><comment author="rualatngua" created="2012-07-01T14:02:43Z" id="6693702">+1
</comment><comment author="slorber" created="2012-07-06T23:40:30Z" id="6818599">+1
</comment><comment author="JohnnyMarnell" created="2012-10-05T03:21:15Z" id="9164378">+1
</comment><comment author="adorr" created="2013-01-16T20:44:18Z" id="12338976">+1
</comment><comment author="mbbx6spp" created="2013-01-16T21:19:00Z" id="12340634">:+1: 
</comment><comment author="otisg" created="2013-01-20T06:53:14Z" id="12466915">+1 for @jprante's websocket idea: https://github.com/elasticsearch/elasticsearch/issues/1242#issuecomment-4974916
</comment><comment author="Spredzy" created="2013-04-05T15:57:14Z" id="15964290">+1
</comment><comment author="slorber" created="2013-04-05T16:10:10Z" id="15965088">Btw just to understand: what's the benetifs of using websockets? Isn't a "normal socket" enough?

Do you need to receive the notifications in the browser?
Does this mean that your ElasticSearch http port is open to anyone?
</comment><comment author="jprante" created="2013-04-05T19:13:22Z" id="15974947">@slorber Websocket is a transparent protocol extension of HTTP that upgrades HTTP into a "normal socket" where you can do communication in async / realtime mode and push style instead of poll. You can serve both HTTP and Websocket on one port, because clients send upgrade requests to let the communication switch from HTTP to Websocket. 

Note, Websocket is part of HTML5 http://www.w3.org/TR/websockets/

In the browser you use Websocket with Javascript very easy with something like `var socket = new WebSocket("ws://host:port/path");` and you receive notifications with `onopen`, `onmessage` etc.

Because Websocket uses the same port as HTTP, your Elasticsearch HTTP port would not be different to the current behavior.
</comment><comment author="slorber" created="2013-04-05T23:27:08Z" id="15986172">I understand that, but do you really want to receive change notifications from your JS stack? 
This means the http port of elasticsearch should be opened to the outside world? Or one should implement it server-side with NodeJS?
Ok, I remember having seen a Java websocket client some times ago. 

What I mean is: if the standart usecase is to receive change updates on the server side, why do we need to use WebSocket instead of a non-HTML event transmission technology?
</comment><comment author="jprante" created="2013-04-05T23:57:05Z" id="15986972">ES has a transport protocol layer (Java binary format) so change notifications could be implemented with Java straight forward, for example by using a pubsub technology (where Websocket with Netty is also an option).

HTTP is meant for easy consuming ES requests and responses by REST, using languages / technologies which are not using the internal Java transport protocol. It is enabled by default, but is optional for ES. Upgrading HTTP to Websocket would be a very easy method to help implementing a change notification service also consumable by Ruby, Python, Perl, Javascript etc. just like in native Java transport protocol. I think ES API should follow this polyglot approach.

In most situations, ES production is placed in a private network / behind a firewall / reverse proxy / load balancer so delivering services to the Web is out of the scope of ES. This is also true for change notifications, but the communication mode will get bidirectional. There should be external application logic that can process the raw ES change events in the requests and responses for disseminating them to the web. But, if you prefer, you can also pass external Web requests and responses transparently to ES.

Can you be more specific about "non-HTML event transmission technology?" Websocket is not a HTML technology, it's just a raw TCP/IP socket usable by web applications in bi-directional mode, and this was embraced by W3C.
</comment><comment author="slorber" created="2013-04-06T11:42:10Z" id="15994808">I think ES should follow the polyglot approach too.

Since ES is placed on a private network, I guess the browsers won't consume that change stream, and I wonder if there's not another polyglot event-transmission technology which could be more appropriate than websocket.

I don't know these stuff so much but AMQP, Thrift, Protobuf and polyglot stuff like that aren't eligible as well for the implementation of this feature? Isn't there any non-HTML technology that solved this problem efficiently before websockets? 
</comment><comment author="brusic" created="2013-04-06T15:15:07Z" id="15997784">Thrift and Protobuf are more for message serialization and not for app communication. There actually is a Thrift plugin for ElasticSearch. Most queuing system rely on an additional application to be installed and maintained. 

The challenge in finding a solution is crafting one that supports every client (language) platform. Raw sockets are tough. Websockets might be non-HTML, but I haven't seen any uses outside of browser communication. Then again, I haven't looked into it much.
</comment><comment author="jprante" created="2013-04-06T15:36:13Z" id="15998194">@slorber It is very desirable to receive ES change notifications in the browser. Many ES programmers are active in web development, they live inside the browser, and that is very good. I love the Chrome Sense Elasticsearch plugin for example. Think of dynamic updates with jQuery, AngularJS, and the like. You can set up transparent Websocket proxies for routing change notification requests and responses easily.

AMQP is a message queue protocol. You may have noticed that ES already offers a RabbitMQ river. I can't see how  extra message queues could be a base technology for ramping up ES change notification streams. It depends on the implementation but I do not see the advantage how an extra message queue system can keep up the performance when hundreds or thousands of ES nodes send notifications. Even the events of one single node may overwhelm external message queue systems. I think, just to create and receive change notifications from ES, an extra message queue implementation is just overhead. For consolidation, you have already the ES cluster model with the client node that waits for the response to the requests sent. The client should decide per parameter if changes should be received from the local node, from the nodes of a specific index, or from the nodes of the whole cluster.

There is already an ES Thrift plugin to replace the HTTP transport. Thrift is a data type language for cross-language RPC services, like Protobuf and Avro. For creating a language you must specify an RPC service for change notifications, and this will substitute more or less the JSON and the REST on the wire. In summary, HTTP, Websocket, Thrift, Protobuf, Avro are just transport technologies. They are exchangeable, so they should be not specific about how ES change notification are implemented. My point was, Netty HTTP is already in ES, and that's why Netty Websocket is an interesting option. I've already implemented Websocket as an ES transport some months ago :)
</comment><comment author="yannnis" created="2013-06-02T23:35:14Z" id="18816419">+1
</comment><comment author="cravergara" created="2013-06-24T16:52:57Z" id="19919820">Hello all!

I'm currently working on replacing Netty 3.6 in ElasticSearch with Netty 4.0. The goal is to first recreate all original functionality in ES's current HTTP implementation, then follow up by adding WebSockets (which is made far easier in 4.0)

A few things: 4.0 is not yet fully released. I don't intend on merging until:

A. everything I do has been tested
B. Netty 4.0 reaches stable. 

I may make a fork before merging.

I've fixed the following packages:

org.elasticsearch.bulk.udp
org.elasticsearch.common.bytes
org.elasticsearch.common.compress
org.elasticsearch.common.compress.lzf

To do:

org.elasticsearch.common.netty
org.elasticsearch.http.netty
org.elasticsearch.transport.netty

The packages that were complete were minor modifications. The ones currently up next are more difficult, so would take me more time.

-Cris
</comment><comment author="cravergara" created="2013-06-26T15:56:47Z" id="20058896">Hello again!

I found a way to accomplish streaming, although it really only fits my use case. It requires that your data is being streamed by a river, and you're using a pub/sub system:

When you make an IndexRequest, you can specify a percolate field, and it will return matching percolators. Exploiting this, I altered the CouchDB river (where my data is coming from) to include a percolate field in the IndexRequest, take the percolation results, and to forward to a pub/sub system (in my case, Redis). From there, it pipes into my Node.js instance that's using ES, which distributes it via WebSockets to the appropriate clients.

The advantage of this is that I didn't have to alter ES in any way to support it. I just made a very specialized adapter. The disadvantage is that the publishing system has to remove the percolator when nobody is subscribed to it, so I had to alter Redis to send a DELETE to the appropriate percolator when a channel has no more subscribers.

This also doesn't solve the issue for most use cases, it just happens to fit perfectly into my infrastructure.

I have been trying to replace the Netty library on ES with a newer implementation of it, but I'm still not entirely sure how it will handle WebSockets. What is clear is that a lot of changes would have to be made to support it.

-Cris
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date detection for `YYYY/MM/dd` or `YYYY/MM/dd HH:mm:ss` without timezones now broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1241</link><project id="" key="" /><description>Issue #1181 has broken detection of dates without timezones:

Couldn't the default format be:  `"yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z||yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"`

```
# [Sat Aug 13 10:20:10 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/twitter/tweet?pretty=1'  -d '
{
   "date_2" : "2011/08/13 11:00:00 +00",
   "date_1" : "2011/08/13 11:00:00"
}
'

# [Sat Aug 13 10:20:11 2011] Response:
# {
#    "ok" : true,
#    "_index" : "twitter",
#    "_id" : "WYTlC0okR4itAoLeAsZ39w",
#    "_type" : "tweet",
#    "_version" : 1
# }

# [Sat Aug 13 10:20:13 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/twitter/_mapping?pretty=1' 

# [Sat Aug 13 10:20:13 2011] Response:
# {
#    "twitter" : {
#       "tweet" : {
#          "properties" : {
#             "date_2" : {
#                "format" : "yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z",
#                "type" : "date"
#             },
#             "date_1" : {
#                "type" : "string"
#             }
#          }
#       }
#    }
# }
```
</description><key id="1399388">1241</key><summary>Date detection for `YYYY/MM/dd` or `YYYY/MM/dd HH:mm:ss` without timezones now broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>v0.17.6</label><label>v0.18.0</label></labels><created>2011-08-13T08:22:00Z</created><updated>2011-09-04T17:06:43Z</updated><resolved>2011-08-13T10:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-13T09:55:29Z" id="1796941">will revert the issue for now. Need to think of a better way to support this for 0.18.
</comment><comment author="izap" created="2011-09-04T17:06:43Z" id="1992727">I have one more requirement for date format, Actually in my application, i need to capture contents from facebook account. So it is usual when some user has data like "Working since 2002, 09 to till now", Facebook returning 0000, 00 for "till now" values. And when i tried to save the same value in ElasticSearch, It was generating date format exception. That is right, But what is the other way to consider that value till now?. I believe, it should allow such exception in date.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Date detection for `YYYY/MM/dd` or `YYYY/MM/dd HH:mm:ss` without timezones now broken, closes #1241.</comment></comments></commit></commits></item><item><title>Log4j River</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1240</link><project id="" key="" /><description>A river that listens for Log4j LoggingEvents on a socket. Works with the out-of-the-box Log4j SocketAppender, so no changes to logging source applications is required.  Borrows heavily (entirely?) from OOTB Log4j SocketServer class - notably, per-host configuration files enable ES-side filtering of received log events.
</description><key id="1398037">1240</key><summary>Log4j River</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">worrel</reporter><labels /><created>2011-08-12T22:32:04Z</created><updated>2014-07-02T14:38:42Z</updated><resolved>2013-04-05T15:37:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="otisg" created="2012-06-21T18:53:12Z" id="6491290">This looks cool. +1
</comment><comment author="clintongormley" created="2013-04-05T15:37:25Z" id="15963054">New rivers should be implemented as plugins. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log4j River</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1239</link><project id="" key="" /><description>It would be useful to have a Log4J River to use ES to aggregate &amp; search logs from various distributed components.  I think I'll create one.....OK, there I did.  I'll share.
</description><key id="1397854">1239</key><summary>Log4j River</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">worrel</reporter><labels /><created>2011-08-12T21:50:14Z</created><updated>2013-04-05T15:37:47Z</updated><resolved>2013-04-05T15:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-12T22:13:06Z" id="1795214">I might be missing something, but it does not sound like a river, no? Maybe a log4j appender?
</comment><comment author="worrel" created="2011-08-12T22:36:41Z" id="1795334">Hi Shay.  Certainly could be done with a custom Appender, but I wanted an approach where I don't have to touch the source applications (for various reasons).  ElasticSearch itself is an example: I'd have to update LogConfigurator to know about my new appender in order to use it.  The main reason is 3rd-party software or apps developed by other teams where organizational friction makes this the easier approach :-)
</comment><comment author="kimchy" created="2011-09-06T18:41:51Z" id="2020782">I see, then it possibly make sense, though, since rivers can move between nodes in the cluster, I guess you will need to pin them down. Is there a chance you can created it as an external plugin? Discussed a bit here: https://groups.google.com/forum/#!topic/elasticsearch/IZMUJXRbmrw.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Peer recovery process can sometimes not reuse the same index files allocated on a possible node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1238</link><project id="" key="" /><description>Peer recovery process can sometimes not reuse the same index files allocated on a possible node, this can cause for longer deployments since shards will perform full recovery.
</description><key id="1391902">1238</key><summary>Peer recovery process can sometimes not reuse the same index files allocated on a possible node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-12T00:46:06Z</created><updated>2011-08-12T01:06:35Z</updated><resolved>2011-08-12T01:06:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/support/AbstractStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file></files><comments><comment>Peer recovery process can sometimes not reuse the same index files allocated on a possible node, closes #1238.</comment></comments></commit></commits></item><item><title>Updating `index.auto_expand_replicas` might not be applied correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1237</link><project id="" key="" /><description>Updating `index.auto_expand_replicas` might not be applied correctly
</description><key id="1391466">1237</key><summary>Updating `index.auto_expand_replicas` might not be applied correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-11T23:02:27Z</created><updated>2011-08-11T23:22:00Z</updated><resolved>2011-08-11T23:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/indices/settings/UpdateNumberOfReplicasTests.java</file></files><comments><comment>Updating `index.auto_expand_replicas` might not be applied correctly, closes #1237.</comment></comments></commit></commits></item><item><title>Add support for a _timestamp field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1236</link><project id="" key="" /><description>This patch add support for a _timestamp field (issue #491).

The _timestamp can be provided in the _source field or its value is automatically set. It is a classical date field with customizable format.  The timestamp value can be given using the following values:
- a number
- a number inside string
- a string conformed with the defined format

If the provided timestamp cannot be parsed or is null the _timestamp will be automatically generated.

By default the _timestamp field is disabled and not stored.

When the _timestamp is automatically generated it is set with the current time when the index query was received.

This patch should work properly with transactions log and replicates.

It will hopefully serve as a basis for the per doc TTL feature I am working on.
</description><key id="1390986">1236</key><summary>Add support for a _timestamp field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-08-11T21:50:06Z</created><updated>2014-07-16T21:56:23Z</updated><resolved>2011-08-25T21:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2011-08-25T21:06:52Z" id="1903989">Closed from here and moved with new patch to https://github.com/elasticsearch/elasticsearch/pull/1277.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix copy-pasted javadoc error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1235</link><project id="" key="" /><description>Not a big deal of course but still an improvement ;)
</description><key id="1390619">1235</key><summary>Fix copy-pasted javadoc error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2011-08-11T21:12:33Z</created><updated>2014-06-16T11:16:12Z</updated><resolved>2011-08-13T13:10:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-13T13:10:15Z" id="1797345">Applied.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failure to reduce geo distance sorting (class case exception)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1234</link><project id="" key="" /><description>When using geo distance based sorting, sometimes (depends on the order of execution and on which nodes), it can fail to reduce the result of the search across nodes.
</description><key id="1390269">1234</key><summary>Failure to reduce geo distance sorting (class case exception)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-11T20:24:56Z</created><updated>2014-05-28T07:22:15Z</updated><resolved>2011-08-11T20:37:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/function/sort/DoubleFieldsFunctionDataComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/function/sort/StringFieldsFunctionDataComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceDataComparator.java</file></files><comments><comment>Failure to reduce geo distance sorting (class case exception), closes #1234.</comment></comments></commit></commits></item><item><title>Mapping: Allow to enable automatic numeric types detection new fields with a string value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1233</link><project id="" key="" /><description>Json has native types for numeric values. Sometimes though, numeric values are provided within a string field. Allow to configure on the root level type mapping automatic numeric detection:

```
{
    "tweet" : {
        "numeric_detection" : true,
        "properties" : {
            "message" : {"type" : "string"}
        }
    }
}
```

Naturally, this can be set across all types by settings it on the `_default` type for an index, using it in index templates, and so on.
</description><key id="1390152">1233</key><summary>Mapping: Allow to enable automatic numeric types detection new fields with a string value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-11T20:10:31Z</created><updated>2016-09-22T18:13:47Z</updated><resolved>2011-08-11T20:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nellicus" created="2016-09-22T14:55:06Z" id="248927912">@kimchy amazingly I've discovered this just now.... 

is there a case for enabling this by default?

AFAIK when we send 

```
{"field":"1.0"} 
```

to Elasticsearch , we want to map that as a float 95% of the cases, not as a string

I'm raising this in the context of https://github.com/logstash-plugins/logstash-patterns-core/issues/173
</comment><comment author="kimchy" created="2016-09-22T18:13:46Z" id="248983306">@nellicus the reason that it is not enabled by default is that it might by mistake identify a field as a number when it is a string, and then other messages will fail to index.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java</file></files><comments><comment>Mapping: Allow to enable automatic numeric types detection new fields with a string value, closes #1233.</comment></comments></commit></commits></item><item><title>Thrift Transport: Uses wrong array offset into the underlying buffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1232</link><project id="" key="" /><description>Thrift Transport: Uses wrong array offset into the underlying buffer
</description><key id="1387058">1232</key><summary>Thrift Transport: Uses wrong array offset into the underlying buffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-11T12:30:01Z</created><updated>2011-08-11T12:30:28Z</updated><resolved>2011-08-11T12:30:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftRestRequest.java</file></files><comments><comment>Thrift Transport: Uses wrong array offset into the underlying buffer, closes #1232.</comment></comments></commit></commits></item><item><title>Build ElasticSearch as Debian package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1231</link><project id="" key="" /><description>Hi Shay,

I'd like to submit a patch to the elascticsearch project for building elasticsearch as a debian package.

The debian package is built with jdeb (https://github.com/tcurdt/jdeb) and is integrated with the gradle script (task named "deb").

The debian package installation has the following structure : 

/usr/share/elasticsearch         : elasticsearch home directory
/etc/elasticsearch                  : elasticsearch configuration directory
/etc/default/elasticsearch       : default service configuration file
/etc/init.d/elasticsearch          : service file (sysvinit)
/var/lib/elasticsearch              : elasticsearch data directory (for indices)
/var/log/elasctisearch             : elasticsearch logs directory

Regards,

Nicolas
</description><key id="1386161">1231</key><summary>Build ElasticSearch as Debian package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nhuray</reporter><labels /><created>2011-08-11T09:26:28Z</created><updated>2014-06-14T21:19:15Z</updated><resolved>2011-09-27T22:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-11T09:52:03Z" id="1780528">Hey, looks good. Can you move this into a pkg top level directory, and move the jdeb jar file into a lib folder under pkg/lib? If we have other packaging, we can place it there.

Also, some notes:
1. elasticsearch requires java 6, so listing java 5 locations should not be done.
2. I think that if the sun jdk is installed, it should come before the openjdk one.
</comment><comment author="nhuray" created="2011-08-11T10:18:42Z" id="1780674">Hi Shay,

Ok no problem to move the 'debian' directory to 'pkg/debian' and the 'jdeb' jar to 'pkg/lib'.

However what do you mean about  "elasticsearch requires java 6, so listing java 5 locations should not be done." ?

For package dependencies (Depends header in "control" file), the "sun-java6-jre" package is defined as an alternative to the "openjdk-6-jre-headless".  If any one of the alternative packages is installed, that part of the dependency is considered to be satisfied. For more information show http://www.debian.org/doc/debian-policy/ch-relationships.html.

Regards,

Nicolas
</comment><comment author="kimchy" created="2011-08-11T18:21:37Z" id="1784413">Regarding the jdk versions, I was referring to this one: https://github.com/elasticsearch/elasticsearch/pull/1231/files#L8R49.
</comment><comment author="kimchy" created="2011-08-18T17:26:50Z" id="1841017">Cam you squash the commits into a single one? simpler to apply
</comment><comment author="nhuray" created="2011-08-19T09:55:42Z" id="1849665">Hi Shay

You can merge the pull request now.

Nicolas
</comment><comment author="dadoonet" created="2011-08-19T14:45:37Z" id="1851500">Hi all,

Just a question about the stop script. Is it the best way to stop a node ? I mean that is not a `curl -XPOST 'http://IP:PORT/_cluster/nodes/_local/_shutdown'` command safer ?

I suppose that `start-stop-daemon --stop` is also safe, right ?

Thanks
</comment><comment author="nhuray" created="2011-08-19T15:42:46Z" id="1852183">Hi dadoonet

I suppose that "start-stop-daemon --stop" kill the process safely, it send a SIGTERM signal.

See this discussion : http://elasticsearch-users.115913.n3.nabble.com/how-to-shut-down-a-node-td451493.html

It's possible to send a SIGKILL signal (like CTRL+C) with the --signal option : http://man.cx/start-stop-daemon(8)

Regards,

Nicolas
</comment><comment author="dadoonet" created="2011-08-19T15:55:58Z" id="1852310">Thanks.
</comment><comment author="kimchy" created="2011-08-29T15:01:55Z" id="1931732">Heya, sorry for coming back to this late... . One last change, at least in terms of build structure, can we fetch jdeb from maven central as a dependency and not add it as a jar file? Not sure if its possible with gradle, I think it should be...
</comment><comment author="nhuray" created="2011-08-29T17:01:56Z" id="1933575">Hi Shay

I don't know if it's possible with Gradle to do it. Anyway I'm on holiday
for now so I'll couldn't do it for the next three weeks.
</comment><comment author="kimchy" created="2011-09-01T16:20:19Z" id="1969784">@nhuray, I think we should be able to do it, I just don't want to have the jar file in the git repo... . When you come back from the vacation, ping me I will try and help.
</comment><comment author="purem" created="2011-09-09T14:57:16Z" id="2051990">Why /var/lib/elasticsearch for data?
</comment><comment author="nhuray" created="2011-09-12T17:08:10Z" id="2072819">Actually it's a linux convention to store persistent data : http://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard
</comment><comment author="kimchy" created="2011-09-27T22:17:32Z" id="2217277">Gerat, thanks!. Pushed the changes. Hoping for more eyeballs on this and tests, as I am not an expert on formal deb packaging...
</comment><comment author="medcl" created="2011-09-28T10:07:31Z" id="2223265">cool stuff~
</comment><comment author="nickhoffman" created="2011-10-07T01:06:32Z" id="2316982">Hey guys. How does one use what's in this awesome commit to build an ES Debian package?
</comment><comment author="nhuray" created="2011-10-07T07:04:07Z" id="2318660">Hi Nick

You can build the project with the gradle wrapper  `./gradlew` or with gradle if it was installed on your system with `gradle -q deb`.

The package will be created under `build/distributions`.
</comment><comment author="nickhoffman" created="2011-10-07T13:05:34Z" id="2321110">Thanks, @nhuray. Much appreciated!
</comment><comment author="nickhoffman" created="2011-10-07T13:54:40Z" id="2321571">BTW, I just noticed that this commit isn't in the 0.17 branch. Will it be included in the next major or minor release or tagged version, or will we have to use HEAD?
</comment><comment author="kimchy" created="2011-10-08T21:26:24Z" id="2333950">@nickhoffman: its there now in the repo, when there is a release, its tagged, so you can always switch to the tagged source and build it. I am still not sure if it will be provided as a "download" in the download page, or people will simply close the repo, and use the tag to build it. I guess time will tell on how best to provide it.
</comment><comment author="nickhoffman" created="2011-10-09T00:44:21Z" id="2334601">@kimchy Am I correct in assuming that this commit will be in 0.17.9 (or 0.18.0 if that's the next tagged release)?

It can't really hurt to provide an i386 build of the Debian package on elasticsearch.org . If people want it built for a different platform, they can always clone the repo and build it themselves, as you said. I would imagine that most people run ES on i386 due to the extra memory usage on x64.
</comment><comment author="kimchy" created="2011-10-13T16:08:06Z" id="2396472">@nickhoffman: its in master now, which means when 0.18 will be tagged (and any future versions). I think we should let it bake a bit before providing it as a full download, and have clear instructions on how to build it for now.
</comment><comment author="nickhoffman" created="2011-10-13T17:02:55Z" id="2397183">@kimchy Sounds good!
</comment><comment author="gabbyz" created="2012-08-07T10:15:57Z" id="7548921">i can't connect the elasticsearch in mysql database using cakephp

do i missing something?.. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve how versioning are read to reduce (deleted) open file handles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1230</link><project id="" key="" /><description>Don't use a point in time searcher when a flush happens to read version data from the index, instead, obtain a searcher reference each time, which will be a refreshed one if a refresh was called between flushes.
</description><key id="1386082">1230</key><summary>Improve how versioning are read to reduce (deleted) open file handles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-11T09:13:24Z</created><updated>2011-08-11T09:26:12Z</updated><resolved>2011-08-11T09:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>Improve how versioning are read to reduce (deleted) open file handles, closes #1230.</comment></comments></commit></commits></item><item><title>Java API TransportClient can fail on remote node shutdown instead of retrying the next connected node under heavy load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1229</link><project id="" key="" /><description>Java API TransportClient can fail on remote node shutdown instead of retrying the next connected node under heavy load
</description><key id="1385854">1229</key><summary>Java API TransportClient can fail on remote node shutdown instead of retrying the next connected node under heavy load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-11T08:31:37Z</created><updated>2011-08-11T08:32:17Z</updated><resolved>2011-08-11T08:32:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file></files><comments><comment>when fixing #1229, we should also handle a case where the node is closing when connected from another node</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/client/ClientFailover.java</file></files><comments><comment>Java API TransportClient can fail on remote node shutdown instead of retrying the next connected node under heavy load, closes #1229.</comment></comments></commit></commits></item><item><title>Highlight - cutting words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1228</link><project id="" key="" /><description>Highlight cutting the word in the middle.

my text: 
     Just to be clear - they're playing! I do not dress up my dog ​​- I only did so because my friend sent me a shirt from his company and I was.

Query is dog.

Highlight: 
     ss up my dog - I only did so because my friend sent me a shirt from

note: cut the word "dress" -&gt; ss up my dog.....
</description><key id="1381429">1228</key><summary>Highlight - cutting words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavobmaia</reporter><labels /><created>2011-08-10T17:44:35Z</created><updated>2012-05-16T16:07:28Z</updated><resolved>2012-01-22T22:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gustavobmaia" created="2011-08-10T18:26:51Z" id="1775292">I'am using java API:

i would like to came all text:
Just to be clear - they're playing! I do not dress up my &lt;em&gt;dog&lt;/em&gt; ​​- I only did so because my friend sent me a shirt from his company and I was.

 searchBuilder.addHighlightedField(fieldToSnippet.getFieldName(),5000, 1);

Highlight came:
ss up my dog ​​- I only did so because my friend sent me a shirt from his company and I was.
</comment><comment author="gustavobmaia" created="2011-08-10T18:48:31Z" id="1775504">Hi i see some problem...
When a set this option in the field i have some problem with the Highlight. The text came cut.
TermVector.WITH_POSITIONS_OFFSETS

I remove it and the text came correct.
</comment><comment author="jsuchal" created="2011-08-23T08:06:14Z" id="1877799">When you use WITH_POSITIONS_OFFSETS fast highlighter is used which has this problem. Stay with simple highlighter (a bit slower) until this is somehow fixed.
</comment><comment author="ppearcy" created="2011-12-14T17:05:09Z" id="3144481">FYI, support for this feature is now in lucene:
https://issues.apache.org/jira/browse/LUCENE-1824

Thanks!
</comment><comment author="pulse00" created="2012-01-20T21:41:21Z" id="3591056">Is this already available in es 0.18.7?
</comment><comment author="ppearcy" created="2012-01-21T21:22:41Z" id="3599058">Yeah, should be since it is off of Lucene 3.5. 
</comment><comment author="kimchy" created="2012-01-22T22:00:15Z" id="3606622">Yea, its part of 0.18.7. I submitted a patch where beginning of text is still cut down sometimes to Lucene, but already fixed it in master (upcoming 0.19).
</comment><comment author="shairontoledo" created="2012-05-16T16:07:28Z" id="5744797">I was wondering if the patch is in 0.19.3. I'm getting truncated result in highlight 

https://gist.github.com/23154602fe943dca973d

I search by "pdf" in a indexed field with content "The Scala Collections API.pdf", I get "Collections API.&lt;em&gt;pdf&lt;/em&gt;". 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed shard recovery can cause shard data to be deleted (replicas will still work)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1227</link><project id="" key="" /><description>A failed shard recovery (for example, because of OOM and the like) can cause shard data to be delete from disk. Though replicas can be there to recover from, it should not happen.
</description><key id="1381347">1227</key><summary>Failed shard recovery can cause shard data to be deleted (replicas will still work)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-10T17:29:05Z</created><updated>2011-08-11T17:49:17Z</updated><resolved>2011-08-10T17:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abh" created="2011-08-11T17:11:09Z" id="1783646">We're looking forward to a release with this.  

We "lucked out" yesterday that enough of our nodes died with an out-of-heap error within a few minutes and we lost 3/4 of our shards (been furiously rebuilding on a new cluster since then).
</comment><comment author="bcoe" created="2011-08-11T17:49:17Z" id="1784063">I believe I've run into this issue as well.

The behaviour I've observed:
- a shard blows up (either out of memory, or in one case out of file descriptors).
- Upon restarting the shard never recovers state with errors resembling (org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException) being thrown endlessly.

I wil be eagerly waiting for this patch to make it into the next stable release :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Failed shard recovery can cause shard data to be deleted (replicas will still work), closes #1227.</comment></comments></commit></commits></item><item><title>Full field paths not respected in sort parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1226</link><project id="" key="" /><description>When sorting by `type.field` instead of `field`, the sort parameter seems to be ignored

```
# [Wed Aug 10 15:28:14 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "mappings" : {
      "bar" : {
         "properties" : {
            "text" : {
               "index" : "not_analyzed",
               "type" : "string"
            }
         }
      }
   }
}
'

# [Wed Aug 10 15:28:14 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Wed Aug 10 15:28:25 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "text" : "foo"
}
'

# [Wed Aug 10 15:28:25 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "K91kSFDQRUW7IYszBQVHjQ",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Wed Aug 10 15:28:28 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "text" : "bar"
}
'

# [Wed Aug 10 15:28:28 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "ZiQZ_FbQSxGXLY0YwvjWgw",
#    "_type" : "bar",
#    "_version" : 1
# }
```

This works:

```
# [Wed Aug 10 15:28:34 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/foo/bar/_search?pretty=1'  -d '
{
   "sort" : {
      "text" : "asc"
   }
}
'

# [Wed Aug 10 15:28:34 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "text" : "bar"
#             },
#             "sort" : [
#                "bar"
#             ],
#             "_score" : null,
#             "_index" : "foo",
#             "_id" : "ZiQZ_FbQSxGXLY0YwvjWgw",
#             "_type" : "bar"
#          },
#          {
#             "_source" : {
#                "text" : "foo"
#             },
#             "sort" : [
#                "foo"
#             ],
#             "_score" : null,
#             "_index" : "foo",
#             "_id" : "K91kSFDQRUW7IYszBQVHjQ",
#             "_type" : "bar"
#          }
#       ],
#       "max_score" : null,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```

This doesn't:

```
# [Wed Aug 10 15:28:38 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/foo/bar/_search?pretty=1'  -d '
{
   "sort" : {
      "bar.text" : "asc"
   }
}
'

# [Wed Aug 10 15:28:38 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "text" : "foo"
#             },
#             "sort" : [
#                null
#             ],
#             "_score" : null,
#             "_index" : "foo",
#             "_id" : "K91kSFDQRUW7IYszBQVHjQ",
#             "_type" : "bar"
#          },
#          {
#             "_source" : {
#                "text" : "bar"
#             },
#             "sort" : [
#                null
#             ],
#             "_score" : null,
#             "_index" : "foo",
#             "_id" : "ZiQZ_FbQSxGXLY0YwvjWgw",
#             "_type" : "bar"
#          }
#       ],
#       "max_score" : null,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```
</description><key id="1379636">1226</key><summary>Full field paths not respected in sort parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-10T13:30:59Z</created><updated>2011-08-10T13:52:21Z</updated><resolved>2011-08-10T13:52:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file></files><comments><comment>Full field paths not respected in sort parameters, closes #1226.</comment></comments></commit></commits></item><item><title>CouchDB River: Custom script does not convert the modified doc properly to be indexed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1225</link><project id="" key="" /><description>The `doc` can be modified by the script, but, it ends up not being converted back from javascript native constructs to ones that can be indexed properly.
</description><key id="1379063">1225</key><summary>CouchDB River: Custom script does not convert the modified doc properly to be indexed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-10T11:45:19Z</created><updated>2011-08-10T12:07:08Z</updated><resolved>2011-08-10T12:07:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsSearchHitPhase.java</file><file>plugins/lang/javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java</file><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file></files><comments><comment>CouchDB River: Custom script does not convert the modified doc properly to be indexed, closes #1225.</comment></comments></commit></commits></item><item><title>Missing docs for index exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1224</link><project id="" key="" /><description /><key id="1378716">1224</key><summary>Missing docs for index exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-08-10T10:26:55Z</created><updated>2011-08-10T14:14:50Z</updated><resolved>2011-08-10T14:14:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-10T14:14:50Z" id="1772869">Done, and open the issue in the elasticsearch.github.com repo :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting: For fast term vector highlighting, allow to control the fragment offset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1223</link><project id="" key="" /><description>A new parameter, called `fragment_offset` added to control the margin to start highlighting from. This is implemented by #1207 pull request.
</description><key id="1372656">1223</key><summary>Highlighting: For fast term vector highlighting, allow to control the fragment offset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-09T14:24:50Z</created><updated>2011-08-09T14:25:30Z</updated><resolved>2011-08-09T14:25:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Rare deadlock when introducing new mapping fields/objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1222</link><project id="" key="" /><description>Rare deadlock when introducing new mapping fields/objects
</description><key id="1371949">1222</key><summary>Rare deadlock when introducing new mapping fields/objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-09T12:26:34Z</created><updated>2011-08-09T12:31:28Z</updated><resolved>2011-08-09T12:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/multifield/MultiFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file></files><comments><comment>Rare deadlock when introducing new mapping fields/objects, closes #1222.</comment></comments></commit></commits></item><item><title>Deadlock in Elastic Search 0.17.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1221</link><project id="" key="" /><description>After upgrade from ES 0.16.2 to 0.17.4 I usually get a deadlock when running my tests.
Sample stack traces during deadlock are available here: https://gist.github.com/1133740
</description><key id="1371944">1221</key><summary>Deadlock in Elastic Search 0.17.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Avatah</reporter><labels /><created>2011-08-09T12:25:41Z</created><updated>2011-08-09T12:28:26Z</updated><resolved>2011-08-09T12:28:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-09T12:28:26Z" id="1762840">This does not apply only to 0.17.4, and its present also in earlier versions, its a matter of timing... . There is another issue for this opened: #1222, so I will close this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Invalidate cache stats when clearing the cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1220</link><project id="" key="" /><description>Since b9387848cbb80967873f71de4b07962323208964, index cache stats are cached for one second.

This cache should be cleared when the index caches are cleared via the API
</description><key id="1371230">1220</key><summary>Invalidate cache stats when clearing the cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-09T09:37:25Z</created><updated>2011-08-09T12:40:50Z</updated><resolved>2011-08-09T12:40:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file></files><comments><comment>Invalidate cache stats when clearing the cache, closes #1220.</comment></comments></commit></commits></item><item><title>Allow setting _index and _type with CouchDB river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1219</link><project id="" key="" /><description>Suppose two types of documents are stored in CouchDB. The document types can be detected somehow, e.g. by reading a field's valud. The user wants to index the different document types as different types in a single ElasticSearch index, or in multiple ElasticSearch indices, or both.

Currently, the only way to do this is to setup two rivers, one for each type, with appropriate filters to filter the corresponding document types from CouchDB. This hits performance, as CouchDB will have to read all documents twice. If there are more than two document types, this gets even worse.

So I'm suggesting to allow to somehow set the _type and _index values so that the indexing could be accomplished with a single river.

We will allow for a custom script to set `_type` and `_index` (probably based on the document content), and if set, those values will be used instead of the default ones.

And, while we are at it, allow for `_routing` as well.
</description><key id="1371197">1219</key><summary>Allow setting _index and _type with CouchDB river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">akheron</reporter><labels><label>enhancement</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-09T09:28:00Z</created><updated>2012-12-14T22:22:20Z</updated><resolved>2011-08-09T12:53:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Ganican" created="2012-12-14T22:22:05Z" id="11394646">Is there way to use any other field in the CouchDB document to cast type. Or it should be defined exactly that way as "_type"?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/river/couchdb/src/main/java/org/elasticsearch/river/couchdb/CouchdbRiver.java</file></files><comments><comment>Allow setting _index and _type with CouchDB river, closes #1219.</comment></comments></commit></commits></item><item><title>Discovery Durability Increase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1218</link><project id="" key="" /><description>This fixes issue #1217.  Adds one function and modifies two.  The idea is to add every node in the cluster to the discovery list.  This will increase durability as clients will remain connected even if all originally listed nodes have gone away.
</description><key id="1369317">1218</key><summary>Discovery Durability Increase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coffeepac</reporter><labels /><created>2011-08-09T00:13:42Z</created><updated>2014-07-16T21:56:25Z</updated><resolved>2011-09-26T22:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-09T13:03:58Z" id="1763079">This solution is problematic in a case where nodes come and go, and when they come back up, they don't have the same IP address. This will cause the list of nodes to grow indefinitely. I see two possible solutions for this:
1. Maintain a separate list of "dynamic" nodes, with a timestamp of when last they were connected and got results from. If enough time passes, remove those nodes from the list.
2. Always iterate over the last discovered nodes + the seed nodes when pinging. Not perfect, but simpler to implement.
</comment><comment author="coffeepac" created="2011-08-09T17:35:37Z" id="1765516">Who doesn't love a little infinitely growing list?  :)
1.  isn't the dynamic list the same as the sniffed list but with a timestamp field?  And how would I remove a seed node if it went away?
2.  I like this one a lot.  It allows for modification of the discovery list.  Where is the pinging code?
</comment><comment author="coffeepac" created="2011-08-09T19:24:47Z" id="1766517">Found the pinging code, its in the same file.  I got confused, I've been away from this for a bit.  This patch was blocked by day-job stuff for awhile.  

I'll try and get a impl for 2 done this week.
</comment><comment author="coffeepac" created="2011-08-21T18:20:21Z" id="1864121">Sorry for the delay, I'm going to work on this nowish.  Day job is for HP, not sure if you saw but we've been having some fun in the personal services group :)  Oh TouchPad, we hardly new ye...
</comment><comment author="coffeepac" created="2011-08-22T04:42:06Z" id="1866516">Updated request with two changes:
1. only add data nodes to the discovery list.  I don't know if this is good or bad but it seemed to be what you were doing originally
2. if any nodes fail the ping check they are removed from the discovery list, this will prevent the infinitely growing list
</comment><comment author="kimchy" created="2011-09-26T22:25:07Z" id="2204794">If I got the change right, its still problematic. Since not reaching a node removes it from the listedNodes, it means that on a full disconnection, we will loose all the nodes to talk to, and then we won't be able to reconnect. Keeping the listed nodes as the base nodes to connect to, and adding the newly discovered nodes would do the trick. Lemme work on it a bit...
</comment><comment author="kimchy" created="2011-09-26T22:29:27Z" id="2204830">ok, pushed a fix on #1217. Closing this, thanks!.
</comment><comment author="coffeepac" created="2011-09-26T22:32:51Z" id="2204856">Nice catch!  Thank you!

Oh sweet, sweet durability...

Pat

On Mon, Sep 26, 2011 at 3:29 PM, Shay Banon &lt;
reply@reply.github.com&gt;wrote:

&gt; ok, pushed a fix on #1217. Closing this, thanks!.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/pull/1218#issuecomment-2204830
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transport Client: When `sniff` is enabled, use the sniffed nodes to be the list fo nodes to ping as well as the provided nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1217</link><project id="" key="" /><description>If a client is connected to the ES cluster using a TransportClient with `sniff` enabled, it should maintain its connection even if all originally listed ES nodes have gone away but there are discovered nodes still running. 

This also means that you can provide one ES address on startup and still have durability if that single node goes down.
</description><key id="1369294">1217</key><summary>Transport Client: When `sniff` is enabled, use the sniffed nodes to be the list fo nodes to ping as well as the provided nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coffeepac</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-08-09T00:08:08Z</created><updated>2011-09-26T22:29:15Z</updated><resolved>2011-09-26T22:29:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="coffeepac" created="2011-08-09T00:08:59Z" id="1759520">Submitting pull request to effect this change.  Pull request also removes duplicate nodes from discovery list if, for some reason, you listed the same node twice at startup.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file></files><comments><comment>Transport Client: When `sniff` is enabled, use the sniffed nodes to be the list fo nodes to ping as well as the provided nodes, closes #1217.</comment></comments></commit></commits></item><item><title>date histogram facet &amp; nested objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1216</link><project id="" key="" /><description>Hi !

Histogram facets seems to return no entries when two nested objects share the same date field name :

Here is  the curl recreation (with 0.17.4) : 

index creation

```
curl -XPOST 'http://localhost:9200/foo'
```

a bar object with two nested collections

```
curl -XPUT 'http://localhost:9200/foo/bar/_mapping?&amp;pretty=true' -d '{
    "bar": {
      "properties": {
        "nested_1": {
          "type": "nested",
          "properties": {
            "date": {
              "type": "date"
            },
            "value": {
              "type": "long"
            }
          }
        }, 
        "nested_2": {
          "type": "nested",
          "properties": {
            "date": {
              "type": "date"
            },
            "value": {
              "type": "long"
            }
          }
        }
      }
    }
  }'
```

some test data 

```
curl -XPUT 'http://localhost:9200/foo/bar/123' -d '{
  nested_1: [{
    "date": "2010-06-20",
    "value": 999
  }],
  nested_2: [{
    "date": "2010-06-20",
    "value": 666
  }]
}'
```

Querying with two histogram facets on distinct nested object

```
curl -XPOST 'http://localhost:9200/foo/bar/_search?pretty=true' -d '{
  facets: {
    nested_1: {
      nested: "nested_1",
      date_histogram: {
        "field": "nested_1.date",
        "interval": "day",
        "value_field":"nested_1.value"
      }
    },
    nested_2: {
      nested: "nested_2",
      date_histogram: {
        "field": "nested_2.date",
        "interval": "day",
        "value_field":"nested_2.value"
      }
    }
  }
}'
```

Give me the following result : 

```
"facets" : {
    "nested_1" : {
      "_type" : "date_histogram",
      "entries" : [ {
        "time" : 1276992000000,
        "count" : 1,
        "min" : 999.0,
        "max" : 999.0,
        "total" : 999.0,
        "total_count" : 1,
        "mean" : 999.0
      } ]
    },
    "nested_2" : {
      "_type" : "date_histogram",
      "entries" : [ ]
    }
```

when i expect : 

```
"facets" : {
    "nested_1" : {
      "_type" : "date_histogram",
      "entries" : [ {
        "time" : 1276992000000,
        "count" : 1,
        "min" : 999.0,
        "max" : 999.0,
        "total" : 999.0,
        "total_count" : 1,
        "mean" : 999.0
      } ]
    },
    "nested_2" : {
      "_type" : "date_histogram",
      "entries" : [ {
        "time" : 1276992000000,
        "count" : 1,
        "min" : 666.0,
        "max" : 666.0,
        "total" : 666.0,
        "total_count" : 1,
        "mean" : 666.0
      }  ]
    }
```

looking for more details now in order to help...
</description><key id="1365729">1216</key><summary>date histogram facet &amp; nested objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dstendardi</reporter><labels /><created>2011-08-08T15:02:02Z</created><updated>2011-08-08T15:25:34Z</updated><resolved>2011-08-08T15:25:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dstendardi" created="2011-08-08T15:25:34Z" id="1755054">oops, this is not really an issue... the path should be full (and not relative)

```
      nested: "nested_2",
      date_histogram: {
        "field": "date",
        "interval": "day",
        "value_field":"value"
      }
```

must be replaced by 

```
      nested: "nested_2",
      date_histogram: {
        "field": "nested_2.date",
        "interval": "day",
        "value_field":"nested_2.value"
      }
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support range facet with value from nested document field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1215</link><project id="" key="" /><description>It would be good to support at least getting the value_field of a range facet from properties of a nested document. See this thread for more details https://groups.google.com/group/elasticsearch/browse_thread/thread/bcba354ede58c99
</description><key id="1364151">1215</key><summary>Support range facet with value from nested document field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">deverton</reporter><labels /><created>2011-08-08T09:53:49Z</created><updated>2014-01-22T11:54:45Z</updated><resolved>2014-01-22T11:54:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-22T11:54:45Z" id="33015098">This is possible with the [`nested`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-nested-aggregation.html) aggregation.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested query docs don't mention _scope</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1214</link><project id="" key="" /><description>Issue #1098 talks about using `_scope` in nested queries, but the live docs don't: http://www.elasticsearch.org/guide/reference/query-dsl/nested-query.html
</description><key id="1355695">1214</key><summary>Nested query docs don't mention _scope</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label></labels><created>2011-08-06T10:12:58Z</created><updated>2013-04-05T15:38:05Z</updated><resolved>2013-04-05T15:38:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T15:38:05Z" id="15963096">No longer supported
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>shard allocated for local recovery (post api), should exists, but doesn't</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1213</link><project id="" key="" /><description>After creating 2414 indexes and submitting 86723 documents, my logs started to fill up with these:

```
[2011-08-05 19:22:03,225][WARN ][indices.cluster          ] [Riot] [silly_6102][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [silly_6102][1] shard allocated for 
local recovery (post api), should exists, but doesn't
```

there were only a few MB of such messages on most nodes, but the master had additional routing_table dumps that cumulatively consumed several GB.

I did see several errors like this one earlier in the logs:

```
[2011-08-05 18:43:11,545][WARN ][transport.netty          ] [Spellbinder] Exception caught on netty layer [[id: 0x44449afc]]
java.net.NoRouteToHostException: No route to host
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.connect(NioClientSocketPipelineSink.java:384)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processSelectedKeys(NioClientSocketPipelineSink.java:354)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:276)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
```

All six nodes seem to be able to reach each other right now.

Sorry for the lack of detail... anywhere else I can look?
</description><key id="1354570">1213</key><summary>shard allocated for local recovery (post api), should exists, but doesn't</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bbgordonn</reporter><labels /><created>2011-08-05T23:29:24Z</created><updated>2013-04-05T13:33:19Z</updated><resolved>2013-04-05T13:33:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="br0kend" created="2011-10-23T13:36:19Z" id="2495335">Has this been resolved yet? I'm observing the same behaviour and looking for a cause.
</comment><comment author="kcm" created="2012-05-15T16:03:42Z" id="5719739">Seeing some of this on a 0.18.6 cluster, trying to figure out why now.  Looks like it was instigated partially by another node going off the air probably during index creation time.  
</comment><comment author="clintongormley" created="2013-04-05T13:33:19Z" id="15955666">This has finally been resolved in v 0.20.6 and fixed in Lucene 4.2.1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>missing doc for associating char filter with an analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1212</link><project id="" key="" /><description>the example at url http://www.elasticsearch.org/guide/reference/index-modules/analysis/ is missing the syntax to add a char filter.  maybe put it in on the myAnalyzer2 definition?  thanks!
</description><key id="1353072">1212</key><summary>missing doc for associating char filter with an analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zahna</reporter><labels /><created>2011-08-05T18:56:00Z</created><updated>2013-04-05T13:36:08Z</updated><resolved>2013-04-05T13:36:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T13:36:08Z" id="15955809">Examples are provided in the mapping and html_strip pages
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unique count for a field per shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1211</link><project id="" key="" /><description>Would be great if in some way we could get a unique count for a given field in an index per shard. Especially in the case where only 1 shard per index is used, this would allow for real unique counts.
</description><key id="1347949">1211</key><summary>Unique count for a field per shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">folke</reporter><labels /><created>2011-08-04T22:27:45Z</created><updated>2014-03-13T18:29:26Z</updated><resolved>2014-03-13T18:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-03-13T18:29:26Z" id="37569612">I think #5426 addresses this requirement?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Store content type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1210</link><project id="" key="" /><description>My patch should resolve Issue #1209
</description><key id="1347843">1210</key><summary>Store content type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fehmicansaglam</reporter><labels /><created>2011-08-04T22:07:19Z</created><updated>2014-06-24T16:55:34Z</updated><resolved>2011-08-09T14:54:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-05T20:04:55Z" id="1738578">I think we can take this change a step forward..., how about, if the content type is not provided when indexing, using Tika to guess the content type and then index it? That way we have automatic content type detection and storage as well.
</comment><comment author="fehmicansaglam" created="2011-08-05T20:09:35Z" id="1738616">What if the user wants to set the content type to another value? I cannot propose a use case now but it may sometimes make sense. My patch uses the user provided content type first(I think so:)). If the content type is not provided then content type is detected using Tika.
</comment><comment author="kimchy" created="2011-08-05T23:11:40Z" id="1739822">The content type is passed to Tika, so I would assume Tika would fail parsing the document if the content type is different than the actual content type of the doc (fail in Tika that is).

So, I think that if the content type is not provided, use Tika to extract it (using detect method), and then use the result of it as the content type indexed / stored.
</comment><comment author="fehmicansaglam" created="2011-08-06T04:48:54Z" id="1740766">I am not sure but I think I see your point. If content type is not provided my patch works as you said. But if content type is provided and is wrong Tika would fail(I haven't tried actually). What about this? Let Tika always extract the content type and parse the doc according to that type. Then if content type is provided store that, if not store the type extracted by Tika.
</comment><comment author="kimchy" created="2011-08-09T14:40:08Z" id="1763839">Ahh, yes, I see what you mean. Missed the part where the content type is taken from the metadata, which Tika will populate. Will push this change, I think that if its provided by hte user, let it be that one.
</comment><comment author="kimchy" created="2011-08-09T14:54:31Z" id="1763986">Pushed change.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Store/index content type </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1209</link><project id="" key="" /><description>The content type is not indexed when using attachment type. In order to retreive the content type later it should be indexed in a similar manner to title and author (under the `content_type` sub field).
</description><key id="1347102">1209</key><summary>Store/index content type </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fehmicansaglam</reporter><labels><label>enhancement</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-08-04T20:14:57Z</created><updated>2011-08-09T14:54:51Z</updated><resolved>2011-08-09T14:54:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-09T14:54:51Z" id="1763990">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/mapper/attachments/src/main/java/org/elasticsearch/index/mapper/attachment/AttachmentMapper.java</file></files><comments><comment>fix issue #1209</comment></comments></commit></commits></item><item><title>Stored fields with multiple values might not return in search request asking for them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1208</link><project id="" key="" /><description>Stored fields with multiple values might not return in search request asking for them
</description><key id="1346869">1208</key><summary>Stored fields with multiple values might not return in search request asking for them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.4</label><label>v0.18.0</label></labels><created>2011-08-04T19:34:18Z</created><updated>2011-08-04T19:34:54Z</updated><resolved>2011-08-04T19:34:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/document/SingleFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/selector/FieldMappersFieldSelector.java</file></files><comments><comment>Stored fields with multiple values might not return in search request asking for them, closes #1208.</comment></comments></commit></commits></item><item><title>Added fragment offset parameter to highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1207</link><project id="" key="" /><description>Second time lucky...
</description><key id="1344469">1207</key><summary>Added fragment offset parameter to highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">purem</reporter><labels /><created>2011-08-04T13:52:49Z</created><updated>2014-06-26T15:58:09Z</updated><resolved>2011-08-09T14:25:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="locojay" created="2011-08-08T14:37:08Z" id="1754615">This is a great feature. have been looking for a while at something which offsets the fragments. Thanks. merge to master?
</comment><comment author="kimchy" created="2011-08-09T14:25:20Z" id="1763704">Applied, opened #1223 issue to explain it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Queries: Optimize single clause boolean query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1206</link><project id="" key="" /><description>Single must clause boolean, or a should (with minimum match &gt; 0) can be optimized down to the clause query, with proper boosting.
</description><key id="1342985">1206</key><summary>Queries: Optimize single clause boolean query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-04T08:35:39Z</created><updated>2011-08-04T08:36:19Z</updated><resolved>2011-08-04T08:36:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file></files><comments><comment>Queries: Optimize single clause boolean query, closes #1206.</comment></comments></commit></commits></item><item><title>Query DSL: custom_filters_score - add score_mode to control filters matching scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1205</link><project id="" key="" /><description>The default in `custom_filters_score` is to use the first filter that matches to compute the score. Add a `score_mode`, with a default value of `first` (similar to default behavior currently), but also with the following values:
- `first`: Use the first filter matching scoring script / boost.
- `total`: Use the total scores computed across the matching filters.
- `avg`: Use the average scores of the matching filters.
- `max`: Use the max score computed across the matching filters.

As usual, if no filter matches, use the default score.
</description><key id="1341479">1205</key><summary>Query DSL: custom_filters_score - add score_mode to control filters matching scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-04T00:30:58Z</created><updated>2011-08-04T00:31:27Z</updated><resolved>2011-08-04T00:31:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryParser.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/customscore/CustomScoreSearchTests.java</file></files><comments><comment>Query DSL: custom_filters_score - add score_mode to control filters matching scoring, closes #1205.</comment></comments></commit></commits></item><item><title>Query DSL: custom_filters_score allow to associate boost on filter instead of script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1204</link><project id="" key="" /><description>Allow to also just associate a boost on a filter in a custom_filters_score which will improve perf compared to a script.
</description><key id="1341249">1204</key><summary>Query DSL: custom_filters_score allow to associate boost on filter instead of script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-03T23:50:42Z</created><updated>2011-08-03T23:51:09Z</updated><resolved>2011-08-03T23:51:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryParser.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/customscore/CustomScoreSearchTests.java</file></files><comments><comment>Query DSL: custom_filters_score allow to associate boost on filter instead of script, closes #1204.</comment></comments></commit></commits></item><item><title>Provide online Javadocs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1203</link><project id="" key="" /><description>A hosted version of the Javadocs will greatly help out on the mailing-list/forum when trying to point out a class/method to someone.  The online documentation is great for illustrating concepts, but something at the class level would also help
</description><key id="1340830">1203</key><summary>Provide online Javadocs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">brusic</reporter><labels><label>feature</label></labels><created>2011-08-03T22:17:49Z</created><updated>2016-04-05T19:27:48Z</updated><resolved>2016-03-31T17:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-04T14:07:54Z" id="1727205">Why not just point to the source in github? It includes both the javadoc and the source.
</comment><comment author="brusic" created="2011-08-04T17:38:45Z" id="1728929">True.  However, Javadocs allow hyperlinking to the related classes.  Browsing with github does not instant access to other class, especially if navigating across different class hierarchies or modules.

Of course, my request is very minor.  Since the site is served from github itself, it might be annoying to redeploy Javadocs, especially since new versions are constantly being pushed out.
</comment><comment author="gzsombor" created="2012-03-12T14:29:45Z" id="4452884">Yes, that would be nice, and the javadoc should linked from the Guide/Java API ...
</comment><comment author="domguinard" created="2012-04-13T09:02:59Z" id="5111032">I would love to have that as well, would also help better understanding the methods of the Java API as the documentation is mainly about the DSL and sometimes the mapping DSL Java API isn't 100% easy to guess...
</comment><comment author="benqua" created="2012-04-24T17:18:47Z" id="5310164">yes, please, make javadoc easily available. 
Googling for quickly shows the need for it!
</comment><comment author="rajish" created="2012-09-06T07:46:00Z" id="8324885">@kimchy I think you assume everyone uses some kind of IDE like Eclipse or InteliJ. There are still Emacs, vi, etc. users out there.

If there is a problem with setting up an on-line javadoc, it would be nice to have at least a maven target to generate it locally.
</comment><comment author="rajish" created="2012-09-06T08:09:11Z" id="8325412">Although no [Maven Javadoc Plugin](http://maven.apache.org/plugins/maven-javadoc-plugin/) seems to be installed the `javadoc:javadoc` goal seems to work, The generated javadocs are placed in `target/site/apidocs`.
</comment><comment author="divideby0" created="2012-11-16T15:45:56Z" id="10451131">I set up a Jenkins job to rebuild head and the latest Javadocs should now always be available here:

http://elasticsearch.spantree.net/elasticsearch/HEAD/apidocs/

I'll work on the groovydocs for the Groovy plugin next.
</comment><comment author="divideby0" created="2013-02-07T03:56:46Z" id="13220314">Both the Java and Groovy plugin apidocs are now available at: http://elasticsearch.spantree.net/
</comment><comment author="p0lar" created="2014-02-21T22:51:42Z" id="35782312">Generated Java API Documentation for almost all released versions can be found at: http://javadoc.kyubu.de/elasticsearch/
</comment><comment author="ConnorDoyle" created="2014-05-06T18:03:54Z" id="42337315">API docs are still impossible to find from the official site.  The linked issue does not point to Javadoc.  Why is this closed?
</comment><comment author="divideby0" created="2014-05-06T18:05:58Z" id="42337560">I believe the Javadocs for head should be available at both these links:

http://elasticsearch.spantree.net/elasticsearch/HEAD/
http://javadoc.kyubu.de/elasticsearch/
</comment><comment author="jplehmann" created="2014-07-03T16:52:55Z" id="47955619">Appreciate those who posted Javadoc online.

Upvote for more prominently linking to this and/or making it more official.  It's not a top hit of search engine results and frustrating to have to search for.
</comment><comment author="clintongormley" created="2014-07-04T10:11:15Z" id="48027949">Also, we should use annotations to label classes as public or internal.

@mrsolo - could you do this from Jenkins?
</comment><comment author="mrsolo" created="2014-07-07T16:57:09Z" id="48205834">Docs can be hosted on jenkins.elasticsearch.org
</comment><comment author="clintongormley" created="2014-07-07T18:12:21Z" id="48217353">@mrsolo what do we need to do to make this happen?  also, can we publish javadocs per released version (plus master &amp; 1.x)?
</comment><comment author="mrsolo" created="2014-07-07T18:16:43Z" id="48218126">@clintongormley We can do direct url for master for start which can be updated per commit.  I assume some sort of nice looking landing page is in order to serve others.
</comment><comment author="p0lar" created="2014-07-08T22:04:52Z" id="48405358">@clintongormley javadocs for all reased versions are already published at http://javadoc.kyubu.de/elasticsearch/ since 0.19.4. If needed, I can add older versions, too. I just assumed there's no need for javadocs prior to 0.19.4.
</comment><comment author="clintongormley" created="2014-07-09T08:43:47Z" id="48444099">hi @p0lar 

it's awesome that you're hosting the javadocs. very generous of you, thank you.  that said, I think people probably expect that elasticsearch, as a company, should provide javadocs under our own domain somewhere and push updates as part of the release process. if you're the only one responsible for the javadocs then there is a bus number of 1 eg if you're on holiday and people complain that the docs haven't been updated, there is nothing that we can do about it. for this reason, i'm wanting to find a more permanent solution.
</comment><comment author="p0lar" created="2014-07-09T17:59:08Z" id="48510789">@clintongormley Thanks. I (and I am sure many others) will be very happy to see this kind of documentation hosted officially.
</comment><comment author="clintongormley" created="2014-07-18T08:24:17Z" id="49406945">@mrsolo We'd like Javadocs for all supported releases, plus docs for the nightlies would be nice to have.  Apparently Jenkins can publish the docs to another website, so that viewing the docs is not impacted by the speed of Jenkins?

@drewr anywhere that we can host these docs?
</comment><comment author="mrsolo" created="2014-07-18T17:44:27Z" id="49459422">@mcmesser what are the current supported releases?  I assume that support in this context is development support.
</comment><comment author="clintongormley" created="2014-07-18T18:01:23Z" id="49461326">@mrsolo i think that last comment was meant for me?  I think if we start with the 1.0.0 release and above, we'll be good.
</comment><comment author="mrsolo" created="2014-07-18T18:09:19Z" id="49462164">That was actually directed to @mcmesser   This should be inline with the support matrix.
</comment><comment author="mcmesser" created="2014-07-18T18:45:49Z" id="49465992">0.90.x is supported until October. 

&gt; On Jul 18, 2014, at 12:09 PM, Bill Hwang notifications@github.com wrote:
&gt; 
&gt; That was actually directed to @mcmesser This should be inline with the support matrix.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="emanuelecasadio" created="2014-10-05T09:40:38Z" id="57931261">Javadoc for ElasticSearch 1.3.4 is missing.
</comment><comment author="p0lar" created="2014-10-29T20:44:31Z" id="61001522">@emanuelecasadio I've added all javadoc versions up to 1.4.0.Beta1.
</comment><comment author="joschi" created="2015-11-09T15:22:34Z" id="155094282">Publishing the Javadoc artifacts on Maven Central along with the main artifacts would enable services like [Javadoc.io](http://www.javadoc.io/) to be used and to view the Elasticsearch Javadoc online: http://www.javadoc.io/doc/org.elasticsearch/elasticsearch/1.7.3
</comment><comment author="joschi" created="2015-11-15T11:08:12Z" id="156802460">@drewr Are there any plans to start publishing the Javadoc artifacts of Elasticsearch on Maven Central again?
</comment><comment author="digitalrinaldo" created="2015-11-25T13:05:01Z" id="159601876">What is the status of java doc support? Can you consider adding a second download link with each release that includes the javadocs. I can trivially go produce the docs and put them up myself but do we really need everyone to publish the java docs. Are you aware that tools like Eclipse and Netbeans have built in support for java docs and that the entire code recommender support and refactoring doesn't work well with Javadocs and that at any point in time a developer may have multiple versions of Elasticsearch so having all the javadocs versioned to match the sources is needed for serious developers.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rest API: Add an `X-Opaque-Id` header, to be returned in the response if exists in the request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1202</link><project id="" key="" /><description>The `X-Opaque-Id` header, when provided on the request header, will be returned as a header in the response. This allows to track possibly track calls, or implement more efficient concurrent clients.
</description><key id="1338272">1202</key><summary>Rest API: Add an `X-Opaque-Id` header, to be returned in the response if exists in the request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-03T16:11:17Z</created><updated>2013-10-23T20:40:26Z</updated><resolved>2011-08-03T16:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tallpsmith" created="2011-08-04T02:42:01Z" id="1723941">Hey, this would be sensational to use this ID as part of slf4j(log4j, logback) MDC concept (http://www.slf4j.org/manual.html#mdc).

If all logs in Elasticsearch were tagged with this id coming from the client then one has fantastic log correlation across the cluster.
</comment><comment author="tallpsmith" created="2011-08-04T02:42:36Z" id="1723943">that implies that all intra-cluster-node communication passes this header on as part of the requests too.
</comment><comment author="kimchy" created="2011-08-04T09:12:28Z" id="1725549">&gt; that implies that all intra-cluster-node communication passes this header on as part of the requests too.

Yep, and thats harder :)
</comment><comment author="tallpsmith" created="2011-08-04T09:46:53Z" id="1725707">ahhh... come on.... "It's wafer thin.. "?  :)

This would be great for Google Dapper like retrospective analysis!
</comment><comment author="tallpsmith" created="2012-03-01T05:09:59Z" id="4252560">BTW, is there anyway to set this header in the Java api when making a search request?  I'd like to be able to set this header with our unique requestId on our app side, and then have the elasticsearch-jetty plugin log this (see https://github.com/sonian/elasticsearch-jetty/issues/16).

that way the logs on ES would have the requestID being passed across.
</comment><comment author="kimchy" created="2012-03-01T12:48:24Z" id="4257423">@tallpsmith No, there isn't...
</comment><comment author="tallpsmith" created="2012-03-01T23:03:41Z" id="4272443">if you had a pointer of an approximate location in the ES Code that could be tweaked to add this feature, I might get a pull request ready, but only if you think it's feasible to do (and you'd be happy to accept the idea)?
</comment><comment author="kimchy" created="2012-03-02T19:29:20Z" id="4292387">@tallpsmith it makes sense, but adding it is a bit of a big task. The transport level APIs do not have the concept of metadata passing when executing an operation (or headers). So it needs to be added (in the most non intrusive manner as possible) and then used through the system. Requires some thinking on how best to do it...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/HttpHelper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/support/RestUtils.java</file><file>plugins/transport/wares/src/main/java/org/elasticsearch/wares/NodeServlet.java</file></files><comments><comment>Rest API: Add an `X-Opaque-Id` header, to be returned in the response if exists in the request, closes #1202.</comment></comments></commit></commits></item><item><title>Cache Stats: Computing the filter stats using the default weighted filter cache is expensive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1201</link><project id="" key="" /><description>Cache Stats: Computing the filter stats using the default weighted filter cache is expensive
</description><key id="1337686">1201</key><summary>Cache Stats: Computing the filter stats using the default weighted filter cache is expensive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-03T14:43:20Z</created><updated>2011-08-03T14:43:58Z</updated><resolved>2011-08-03T14:43:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractWeightedFilterCache.java</file></files><comments><comment>Cache Stats: Computing the filter stats using the default weighted filter cache is expensive, closes #1201.</comment></comments></commit></commits></item><item><title>Index Cache Stats / JVM Stats: Add a refresh_interval to control when it gets refreshed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1200</link><project id="" key="" /><description>Currently, we compute the stats for each call, but that can be expensive if many clients call the stats. 

Add a `index.cache.stats.refresh_interval` setting, that will only refresh the cache stats once the interval expires. Default it to `1s`.

The jvm stats setting is `monitor.jvm.refresh_interval` and also defaults to `1s`.

Also, change process and os to have `1s` refresh interval by default (compared to `5s` now)
</description><key id="1337080">1200</key><summary>Index Cache Stats / JVM Stats: Add a refresh_interval to control when it gets refreshed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-03T13:07:24Z</created><updated>2011-08-03T14:43:58Z</updated><resolved>2011-08-03T14:43:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/jvm/JvmService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/os/OsService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/process/ProcessService.java</file></files><comments><comment>Index Cache Stats / JVM Stats: Add a refresh_interval to control when it gets refreshed, closes #1200.</comment></comments></commit></commits></item><item><title>Automatic index creation can still cause "index missing" failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1199</link><project id="" key="" /><description>Because of timing issues in waiting for the fact that an index has been created across the cluster, the client might get a response that the index has been created and when indexing, there will be an index missing failure.
</description><key id="1336535">1199</key><summary>Automatic index creation can still cause "index missing" failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-03T11:32:12Z</created><updated>2011-08-03T11:32:45Z</updated><resolved>2011-08-03T11:32:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Automatic index creation can still cause "index missing" failures, closes #1199.</comment></comments></commit></commits></item><item><title>Delete By Query wrongly persisted to translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1198</link><project id="" key="" /><description>Delete By Query wrongly persisted to translog
</description><key id="1335624">1198</key><summary>Delete By Query wrongly persisted to translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-03T08:10:45Z</created><updated>2011-08-03T08:30:13Z</updated><resolved>2011-08-03T08:30:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/Translog.java</file></files><comments><comment>Delete By Query wrongly persisted to translog, closes #1198.</comment></comments></commit></commits></item><item><title>Multiple boolean query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1197</link><project id="" key="" /><description /><key id="1334338">1197</key><summary>Multiple boolean query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavobmaia</reporter><labels /><created>2011-08-03T00:41:20Z</created><updated>2011-08-03T01:04:03Z</updated><resolved>2011-08-03T01:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Add a `kstem` name option to `stemmer` token filter (on top of the default kstem option)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1196</link><project id="" key="" /><description>Analysis: Add a `kstem` name option to `stemmer` token filter (on top of the default kstem option). Mainly for completeness. 
</description><key id="1332956">1196</key><summary>Analysis: Add a `kstem` name option to `stemmer` token filter (on top of the default kstem option)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-02T20:30:35Z</created><updated>2011-08-02T20:31:18Z</updated><resolved>2011-08-02T20:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java</file></files><comments><comment>Analysis: Add a `kstem` name option to `stemmer` token filter (on top of the default kstem option), closes #1196.</comment></comments></commit></commits></item><item><title>Allow creation of empty docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1195</link><project id="" key="" /><description>Per conversation on IRC, there's no reason not to allow creation of empty documents. Currently ES will throw an error both on  a `PUT` with no data or a `PUT` with an empty JSON object as the data:

```
$ curl -XPUT http://localhost:9200/test/test/1 -d '{}'
{"error":"MapperParsingException[Malformed content, after first object, either the type field or the actual properties should exist]","status":400}
$ curl -XPUT http://localhost:9200/test/test/1
{"error":"ElasticSearchParseException[Failed to derive xcontent from []]","status":500}
```
</description><key id="1332377">1195</key><summary>Allow creation of empty docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-02T18:55:41Z</created><updated>2011-08-03T11:01:17Z</updated><resolved>2011-08-03T09:36:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-03T09:19:00Z" id="1716543">PUT with no data is problematic to how ES works, and not sure it makes sense, but one with empty json '{}' should be supported.
</comment><comment author="outoftime" created="2011-08-03T11:01:17Z" id="1717095">Sure! Was just suggesting that it would be nice for one of the to to work : )

On Wed, Aug 3, 2011 at 05:19, kimchy
reply@reply.github.com
wrote:

&gt; PUT with no data is problematic to how ES works, and not sure it makes sense, but one with empty json '{}' should be supported.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1195#issuecomment-1716543
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file></files><comments><comment>Allow creation of empty docs, closes #1195.</comment></comments></commit></commits></item><item><title>warn if mlockall is not working once enabled in settings (instead of debug logging)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1194</link><project id="" key="" /><description>see http://elasticsearch-users.115913.n3.nabble.com/Confirming-mlockall-is-working-td3204069.html for reference.

If the mlockall setting is working a Debug output should be produced like "mlockall now working".
If it is not working then a warning message should be outputted.
</description><key id="1329053">1194</key><summary>warn if mlockall is not working once enabled in settings (instead of debug logging)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dav-rob</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-02T10:40:50Z</created><updated>2014-03-31T18:36:34Z</updated><resolved>2011-08-02T11:07:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/jna/CLibrary.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/jna/Natives.java</file></files><comments><comment>warn if mlockall is not working once enabled in settings (instead of debug logging), closes #1194.</comment></comments></commit></commits></item><item><title>Unicast Discovery: When providing a comma separated list of addresses, trim them from whitespaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1193</link><project id="" key="" /><description>Unicast Discovery: When providing a comma separated list of addresses, trim them from whitespaces
</description><key id="1328997">1193</key><summary>Unicast Discovery: When providing a comma separated list of addresses, trim them from whitespaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-02T10:30:09Z</created><updated>2011-08-02T10:42:49Z</updated><resolved>2011-08-02T10:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Unicast Discovery: When providing a comma separated list of addresses, trim them from whitespaces, closes #1193.</comment></comments></commit></commits></item><item><title>`highlight.fields` should accept field names with the full path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1192</link><project id="" key="" /><description>When specifying fields to highlight, you have to specify the simple field name - you can't specify the full path eg `body` not `foo.body`:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "foo" : {
         "properties" : {
            "text" : {
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/foo?pretty=1'  -d '
{
   "text" : "the quick brown fox jumped over the lazy dog"
}
'
```

Simple field name:

```
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "query" : {
      "text" : {
         "text" : "brown"
      }
   },
   "highlight" : {
      "fields" : {
         "text" : {}
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "text" : "the quick brown fox jumped over the lazy
# &gt;                 dog"
#             },
#             "_score" : 0.11506981,
#             "_index" : "test",
#             "_id" : "SA4FtDBxTq6Gk-k8uqGcag",
#             "_type" : "foo",
#             "highlight" : {
#                "text" : [
#                   "the quick &lt;em&gt;brown&lt;/em&gt; fox jumped over the l
# &gt;                   azy dog"
#                ]
#             }
#          }
#       ],
#       "max_score" : 0.11506981,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```

Full path:

```
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "query" : {
      "text" : {
         "foo.text" : "brown"
      }
   },
   "highlight" : {
      "fields" : {
         "foo.text" : {}
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "text" : "the quick brown fox jumped over the lazy
# &gt;                 dog"
#             },
#             "_score" : 0.11506981,
#             "_index" : "test",
#             "_id" : "SA4FtDBxTq6Gk-k8uqGcag",
#             "_type" : "foo"
#          }
#       ],
#       "max_score" : 0.11506981,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```
</description><key id="1328624">1192</key><summary>`highlight.fields` should accept field names with the full path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-02T08:59:44Z</created><updated>2011-08-02T09:55:27Z</updated><resolved>2011-08-02T09:55:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file></files><comments><comment>`highlight.fields` should accept field names with the full path, closes #1192.</comment></comments></commit></commits></item><item><title>Sorting on _score in the URI format / Java API is reversed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1191</link><project id="" key="" /><description>When using the URI search format, for example, "_score:asc", the order is of sorting is reversed.
</description><key id="1326847">1191</key><summary>Sorting on _score in the URI format / Java API is reversed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-01T23:52:45Z</created><updated>2011-08-02T05:40:34Z</updated><resolved>2011-08-02T00:08:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/SortOrder.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file></files><comments><comment>Sorting on _score in the URI format is reversed, closes #1191.</comment></comments></commit></commits></item><item><title>Merge Policy type setting fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1190</link><project id="" key="" /><description>Setting `index.merge.policy.type` to `log_byte_size` fails to load the relevant setting. One must set the full class name for it.
</description><key id="1325470">1190</key><summary>Merge Policy type setting fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-01T20:10:26Z</created><updated>2011-08-01T20:11:03Z</updated><resolved>2011-08-01T20:11:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/MergePolicyModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/scheduler/MergeSchedulerModule.java</file></files><comments><comment>Merge Policy type setting fails, closes #1190.</comment></comments></commit></commits></item><item><title>error when creating and deleting many indexes?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1189</link><project id="" key="" /><description>The below Perl program for the bulk API fails non-deterministically after a few minutes during most runs. I've tried tinkering with a lot of the variables--refresh time, number of indexes, number of servers--to make this simpler to reproduce, but it's still a pretty complex case. Sorry.

The only other thing I've noticed is a lot of errors in the logs like:

```
[2011-08-01 11:47:35,224][WARN ][cluster.action.shard     ] [Mother Night] sending failed shard for [silly_9921][1], node[hHogDRzJRx-kK35d5FuCiQ], relocating [rXUNQERpSY6NrCwVgb3qvQ], [R], s[INITIALIZING], reason [Failed to start shard, message [RecoveryFailedException[Index Shard [silly_9921][1]: Recovery failed from [Anelle][aUg0SyLgQ8WeDeQhaW6KMw][inet[/10.2.75.191:9300]] into [Mother Night][hHogDRzJRx-kK35d5FuCiQ][inet[/10.2.75.193:9300]]]; nested: RemoteTransportException[[Anelle][inet[/10.2.75.191:9300]][index/shard/recovery/startRecovery]]; nested: RecoveryEngineException[[silly_9921][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[silly_9921][1] Failed to transfer [1] files with total size of [58b]]; nested: FileNotFoundException[/var/lib/elasticsearch/elasticsearch/nodes/0/indices/silly_9921/1/index/segments_1 (No such file or directory):
impleFSDirectory.java:90)
        at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.&lt;init&gt;(NIOFSDirectory.java:91)
        at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:78)
        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:345)
        at org.elasticsearch.index.store.support.AbstractStore$StoreDirectory.openInput(AbstractStore.java:328)
        at org.elasticsearch.index.shard.recovery.RecoverySource$1$1.run(RecoverySource.java:162)
        ... 3 more
[2011-08-01 11:47:35,224][WARN ][cluster.action.shard     ] [Mother Night] sending failed shard for [silly_9921][1], node[hHogDRzJRx-kK35d5FuCiQ], relocating [rXUNQERpSY6NrCwVgb3qvQ], [R], s[INITIALIZING], reason [Failed to start shard, message [RecoveryFailedException[Index Shard [silly_9921][1]: Recovery failed from [Anelle][aUg0SyLgQ8WeDeQhaW6KMw][inet[/10.2.75.191:9300]] into [Mother Night][hHogDRzJRx-kK35d5FuCiQ][inet[/10.2.75.193:9300]]]; nested: RemoteTransportException[[Anelle][inet[/10.2.75.191:9300]][index/shard/recovery/startRecovery]]; nested: RecoveryEngineException[[silly_9921][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[silly_9921][1] Failed to transfer [1] files with total size of [58b]]; nested: FileNotFoundException[/var/lib/elasticsearch/elasticsearch/nodes/0/indices/silly_9921/1/index/segments_1 (No such file or directory)]; ]]
```

The error I get:

```
[ERROR] ** ElasticSearch::Error::Missing at lib/ElasticSearch/Transport/HTTPLite.pm line 56 :
IndexMissingException[[silly_8089] missing]

With vars:{
  'request' =&gt; {
    'qs' =&gt; {},
    'as_json' =&gt; undef,
    'cmd' =&gt; '/_bulk',
    'data' =&gt; \'{"index":{"_index":"silly_9901","_id":"1311677474-91-19527-316-22","_type":"eeee"}}
{"id":"1311677474-91-19527-316-22","domain id":"9901"}
{"index":{"_index":"silly_8089","_id":"1311677474-92-19527-316-22","_type":"eeee"}}
{"id":"1311677474-92-19527-316-22","domain id":"8089"}
',
    'method' =&gt; 'POST'
  },
  'status_code' =&gt; 404,
  'server' =&gt; '10.2.75.191:9200',
  'status_msg' =&gt; 'Not Found
'
}
```

The program:

``` perl
#!/usr/bin/perl
use strict;
use warnings;

use Data::Dumper;
use ElasticSearch;

my $es = ElasticSearch-&gt;new(
    'transport' =&gt; 'httplite',
    'servers' =&gt;  [
        '10.2.75.191:9200',
        '10.2.75.193:9200',
        '10.2.72.64:9200',
        '10.2.72.65:9200',
    ],
    'trace_calls' =&gt; 0,
    'timeout' =&gt; 300,
    'max_requests' =&gt; 10_000,
    'no_refresh' =&gt; 0,
);

recreate_indexes();

my $i = 0;
while (1) {
    $i++;
    do_index([
        {
            'id' =&gt; ('1311677474-91-19527-316-' . $i),
            'domain id' =&gt; int(rand(10000)),
        },
        {
            'id' =&gt; ('1311677474-92-19527-316-' . $i),
            'domain id' =&gt; int(rand(10000)),
        },
    ]);
}

sub do_index {
    my ($documents) = @_;
    my $n_msgs = scalar(@{$documents});

    my @req;
    for my $doc (@{$documents}) {
        my $index = 'silly_' . $doc-&gt;{'domain id'};
        my $doc_req = {
            'index' =&gt; $index,
            'type' =&gt; 'eeee',
            'id' =&gt; $doc-&gt;{'id'},
            'data' =&gt; $doc,
        };
        push(@req, { 'index' =&gt; $doc_req });
    }

    print STDERR join(' ',
                      'Submitting', $n_msgs,
                      'pid =', $$,
                      'server =', $es-&gt;transport()-&gt;current_server(),
                      "\n");
    my $result = $es-&gt;bulk(\@req);
}

sub recreate_indexes {
    print STDERR "DELETE index _all\n";
    $es-&gt;delete_index(
        'index' =&gt; '_all',
    );

    print STDERR "DELETE index template silly\n";
    $es-&gt;delete_index_template(
        'name' =&gt; 'silly',
        'ignore_missing' =&gt; 1,
    );

    print STDERR "CREATE index template silly\n";
    $es-&gt;create_index_template(
        'name' =&gt; 'silly',
        'template' =&gt; 'silly_*',
        'settings' =&gt; {
            "index" =&gt; {
                "refresh_interval" =&gt; "60s",
                "number_of_replicas" =&gt; 1,
                "number_of_shards" =&gt; 3,
            },
        },
        'mappings' =&gt; {
            'eeee' =&gt; {
                "_source" =&gt; { "enabled" =&gt; 0 },
            },
        },
    );
}
```
</description><key id="1323953">1189</key><summary>error when creating and deleting many indexes?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bbgordonn</reporter><labels /><created>2011-08-01T16:13:24Z</created><updated>2013-04-05T13:46:00Z</updated><resolved>2013-04-05T13:46:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-01T17:49:20Z" id="1702383">Are there any other exceptions in the log? Are you sure you are not running out of file handles? If you use the latest big couch it has a realtime graph for open / max file handles: https://github.com/lukas-vlcek/bigdesk.
</comment><comment author="bbgordonn" created="2011-08-01T18:18:03Z" id="1702630">No, I had the file handle problem last week. :) No errors like that since.

There is an java.net.UnknownHostException because my hostnames are not really in DNS. I have another test program that creates a few thousand indexes as quickly as it can. While this is running, CPU on one of my nodes will occasionally spike to 100% of one core for a few minutes. This occasionally causes that node to be removed from the cluster and re-added later.
</comment><comment author="bbgordonn" created="2011-08-01T18:37:45Z" id="1702786">Could it be that issuing a delete index _all is not always completely done when it returns "ok"? I.e., that it could still be deleting indexes after that request is finished, causing a race condition when submitting documents. Or, there may be two different issues here.

IndexMissingException[[silly_8349] missing]

With vars:{
  'request' =&gt; {
    'qs' =&gt; {},
    'as_json' =&gt; undef,
    'cmd' =&gt; '/_bulk',
    'data' =&gt; \'{"index":{"_index":"silly_8980","_id":"1311677474-91-19527-316-23","_type":"eeee"}}
{"id":"1311677474-91-19527-316-23","domain id":"8980"}
{"index":{"_index":"silly_8349","_id":"1311677474-92-19527-316-23","_type":"eeee"}}
{"id":"1311677474-92-19527-316-23","domain id":"8349"}
',
    'method' =&gt; 'POST'
  },
  'status_code' =&gt; 404,
  'server' =&gt; '10.2.72.64:9200',
  'status_msg' =&gt; 'Not Found

Here's the log file from 10.2.72.64:

https://gist.github.com/1118720

and the log from 10.2.72.65, which shows interleaved creates and deletes:

https://gist.github.com/1118718

I found a similar thread in the google group:

http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/4cb889197af5a990/07651e5946c56a51?lnk=raot
</comment><comment author="kimchy" created="2011-08-01T21:54:16Z" id="1704583">Do you issue a delete all before or while you index the data? If its before, and it might be a timing issue, can you try and sleep for something like 20-30 seconds before you start the indexing?
</comment><comment author="bbgordonn" created="2011-08-01T22:47:43Z" id="1704984">I never delete while indexing. As in the script above, I always issue DELETE with an index of _all, delete an index template, then create the same index template, then start bulk indexing.

Just tried sleeping for 30s after the delete. I saw "deleting index" lines for each individual index just like the previous gist, and can confirm that they stopped long before the sleep finished and "creating index" lines started appearing.

The last "creating index" line was the same one mentioned in the 404 error.
</comment><comment author="kimchy" created="2011-08-02T08:32:17Z" id="1707400">I think I managed to get confused by the two issues. The first one was the recovering failure, and I suspect it was because of the interleaving delete and index requests. Does that still happen?

The second one is the index missing failure, and you still get that, right?
</comment><comment author="bbgordonn" created="2011-08-02T14:39:58Z" id="1709566">I do not know why the "No such file or directory" error occurs. I included it for completeness. What you say makes sense. I have not gotten it in the last few tests or noticed any particular correlation between that error and the failures received by the client, which makes me suspect it is not related after all. Sorry for the confusion!

I get the index missing error regularly unless I explicitly create indexes before doing a bulk submit (still using the template).
</comment><comment author="bbgordonn" created="2011-08-02T21:33:18Z" id="1713401">Just got the IndexMissingException using the rabbitmq river when no indexes had been deleted. I tweaked the issue title to (hopefully) be more clear.
</comment><comment author="kimchy" created="2011-08-03T11:34:56Z" id="1717257">ok, I think I managed to track this one down, opened #1199 as a separate issue and pushed a fix, it should fix your case (I managed to create it locally, and this fix fixes it). If you want to make sure, you can build es from source, or jump on IRC, I can provide you with a custom build.
</comment><comment author="bbgordonn" created="2011-08-03T15:47:14Z" id="1719077">It seems to happen a less often now but is still reproducible. Bulk requests that result in index creation are now slightly but noticeably slower, which I take as a good sign that the patch is working.

I did run into a situation I didn't have before: IndexMissingException during the DELETE. May be because I accidentally ran two manual DELETEs against _all simultaneously (my script never does this).

[root@el6-75-191 ~]# curl -s -XDELETE 'http://localhost:9200/_all/'
{"error":"RemoteTransportException[[Maverick][inet[/10.2.72.64:9300]][indices/deleteIndex]]; nested: IndexMissingException[[silly_3611] missing]; ","status":404}
</comment><comment author="kimchy" created="2011-08-04T09:14:15Z" id="1725556">Strange..., will try and reproduce it again...
</comment><comment author="bbgordonn" created="2011-08-04T14:28:16Z" id="1727398">It's now very difficult to produce with a limited example like the above.

My big test tries to index ~9 million documents with ~2500 indexes with 600 documents per bulk request. This fails after five or ten requests of 600.
</comment><comment author="clintongormley" created="2013-04-05T13:46:00Z" id="15956347">Above test no longer seems to produce failure. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: ids filter/parse to automatically use types provided as part of the search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1188</link><project id="" key="" /><description>When types are provided as part of the search request (for example, within the search uri), then use them automatically as the provided types in the ids filter.
</description><key id="1321930">1188</key><summary>Query DSL: ids filter/parse to automatically use types provided as part of the search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-08-01T10:05:43Z</created><updated>2011-08-01T10:06:16Z</updated><resolved>2011-08-01T10:06:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IdsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file></files><comments><comment>Query DSL: ids filter/parse to automatically use types provided as part of the search, closes #1188.</comment></comments></commit></commits></item><item><title>Boost shorter fields at query time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1187</link><project id="" key="" /><description>We discussed this on IRC - I'm not sure what the outcome was, but I thought I'd open this issue in case this is fixable.

The problem is: field length norm is stored in a single byte, which loses precision for very short fields (eg &lt; 5 terms).  The result is that "foo bar" and "foo bar baz" would have the same score when searching for "foo bar".

Would it be possible to implement a check at query time for (eg) fields with fewer than 5 terms, to boost them slightly based on the inverse of their length?
</description><key id="1318297">1187</key><summary>Boost shorter fields at query time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-07-31T12:56:58Z</created><updated>2013-04-05T13:48:53Z</updated><resolved>2013-04-05T13:48:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T13:48:52Z" id="15956503">This seems to have been resolved in 0.90. On an index with 1 shard:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "match" : {
         "x" : "foo bar"
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "x" : "foo bar"
#             },
#             "_score" : 0.76735055,
#             "_index" : "test",
#             "_id" : "F39tg4v9SEeTXKwuD1rTEg",
#             "_type" : "test"
#          },
#          {
#             "_source" : {
#                "x" : "foo bar baz"
#             },
#             "_score" : 0.6138804,
#             "_index" : "test",
#             "_id" : "irPKzHAZRx-gQAywCB989g",
#             "_type" : "test"
#          },
#          {
#             "_source" : {
#                "x" : "foo"
#             },
#             "_score" : 0.20663503,
#             "_index" : "test",
#             "_id" : "yGc5laMvSa6iyhSywLkreQ",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 0.76735055,
#       "total" : 3
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "took" : 49
# }
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: Allow to control how all multi term queries are rewritten</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1186</link><project id="" key="" /><description>Allow to control how multi term queries rewrite, all relevant queries (`wildcard`, `prefix`), and query parsers (`query_string` and `field`, when using wildcard characters) support a new parameter called `rewrite`, with the following values:
- When not set, or set to `constant_score_default`, defaults to automatically choosing either `constant_score_boolean` or `constant_score_filter` based on query characteristics.
- `scoring_boolean`: A rewrite method that first translates each term into a should clause in a boolean query, and keeps the scores as computed by the query.  Note that typically such scores are meaningless to the user, and require non-trivial CPU to compute, so it's almost always better to use `constant_score_default`. This rewrite method will hit too many clauses failure if it exceeds the boolean query limit (defaults to 1024).
- `constant_score_boolean`: Similar to `scoring_boolean` except scores are not computed.  Instead, each matching document receives a constant score equal to the query's boost. This rewrite method will hit too many clauses failure if it exceeds the boolean query limit (defaults to 1024).
- `constant_score_filter`: A rewrite method that first creates a private Filter by visiting each term in sequence and marking all docs for that term.  Matching documents are assigned a constant score equal to the query's boost.
- `top_terms_N`: A rewrite method that first translates each term into should clause in boolean query, and keeps the scores as computed by the query. This rewrite method only uses the top scoring terms so it will not overflow boolean max clause count. The `N` controls the size of the top scoring terms to use.
- `top_terms_boost_N`: A rewrite method that first translates each term into should clause in boolean query, but the scores are only computed as the boost. This rewrite method only uses the top scoring terms so it will not overflow the boolean max clause count. The `N` controls the size of the top scoring terms to use.
</description><key id="1312856">1186</key><summary>Query DSL: Allow to control how all multi term queries are rewritten</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-29T21:36:12Z</created><updated>2014-02-09T12:25:03Z</updated><resolved>2011-07-29T21:36:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/QueryParserSettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file></files><comments><comment>Query DSL: Allow to control how all multi term queries are rewritten, closes #1186.</comment><comment>This will fail for "top_terms_boost_N" rewrite method. Cause it will try to parse "boost_N" to int.</comment></comments></commit></commits></item><item><title>Analysis: Unique token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1185</link><project id="" key="" /><description>A `unique` token filter filters out only unique tokens. It can also only filter out unique tokens on the same position when setting `only_on_same_position` to `true`.
</description><key id="1312193">1185</key><summary>Analysis: Unique token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-29T19:44:11Z</created><updated>2011-07-29T19:45:37Z</updated><resolved>2011-07-29T19:45:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/miscellaneous/UniqueTokenFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/UniqueTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file><file>modules/elasticsearch/src/test/java/org/apache/lucene/analysis/miscellaneous/UniqueTokenFilterTests.java</file></files><comments><comment>Analysis: Unique token filter, closes #1185.</comment></comments></commit></commits></item><item><title>WildcardQuery - dont use boost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1184</link><project id="" key="" /><description>In the classe MapperQueryParser have the default value.

setMultiTermRewriteMethod(MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT);

when use wildcard i need to change this value to

setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);

I know it is slow search. But is very important for me because i need do similary search like quora.com.
</description><key id="1311965">1184</key><summary>WildcardQuery - dont use boost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavobmaia</reporter><labels /><created>2011-07-29T19:06:29Z</created><updated>2011-07-29T21:49:32Z</updated><resolved>2011-07-29T21:49:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-29T21:37:38Z" id="1685408">Added #1186, should solve your case and more, I think.
</comment><comment author="gustavobmaia" created="2011-07-29T21:49:27Z" id="1685581">Thanks, Great. :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Renamed root object `date_formats` to `dynamic_date_formats` (old value still works)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1183</link><project id="" key="" /><description>This better reflects how they are actually used, since they only apply for dynamically created fields.
</description><key id="1311381">1183</key><summary>Mapping: Renamed root object `date_formats` to `dynamic_date_formats` (old value still works)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-29T17:16:42Z</created><updated>2011-07-29T18:11:08Z</updated><resolved>2011-07-29T18:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file></files><comments><comment>Mapping: Renamed root object `date_formats` to `dynamic_date_formats` (old value still works), closes #1183.</comment></comments></commit></commits></item><item><title>Stop Token Filter - allow to set enable_position_increments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1182</link><project id="" key="" /><description>hello,
I was having problem with the filter StopFilter using enable_position_increments token = false; When I did a search the result was not even setting enable_position_increments = false in the query.

Began to look at the code of the elastic i saw some problem with enable_position_increments in object StopFilter on the class StopTokenFilterFactory. So I changed it by adding the following line:

public class StopTokenFilterFactory extends AbstractTokenFilterFactory {

```
private final Set&lt;?&gt; stopWords;

private final boolean ignoreCase;
```

NEW LINE ***\*    private final boolean enablePositionIncrements;

```
@Inject public StopTokenFilterFactory(Index index, @IndexSettings Settings indexSettings, Environment env, @Assisted String name, @Assisted Settings settings) {
    super(index, indexSettings, name, settings);
    this.stopWords = Analysis.parseStopWords(env, settings, StopAnalyzer.ENGLISH_STOP_WORDS_SET);
    this.ignoreCase = settings.getAsBoolean("ignore_case", false);
  NEW LINE ****  this.enablePositionIncrements = settings.getAsBoolean("enable_position_increments", true);
}

@Override public TokenStream create(TokenStream tokenStream) {
NEW LINE ****   StopFilter stopFilter = new StopFilter(version, tokenStream, stopWords, ignoreCase);
NEW LINE ****   stopFilter.setEnablePositionIncrements(enablePositionIncrements);
    return stopFilter;
}

public Set&lt;?&gt; stopWords() {
    return stopWords;
}

public boolean ignoreCase() {
    return ignoreCase;
}
```

}
</description><key id="1311305">1182</key><summary>Stop Token Filter - allow to set enable_position_increments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavobmaia</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-29T17:01:05Z</created><updated>2011-07-29T18:32:05Z</updated><resolved>2011-07-29T18:32:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-29T18:31:23Z" id="1683539">Thanks, will add this option to 0.17 and master branch.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StopTokenFilterFactory.java</file></files><comments><comment>Stop Token Filter - enable_position_increments, closes #1182.</comment></comments></commit></commits></item><item><title>Mapping: Root object non ISO date formats to support timezone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1181</link><project id="" key="" /><description>Change `yyyy/MM/dd HH:mm:ss||yyyy/MM/dd` to `yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z`
</description><key id="1311302">1181</key><summary>Mapping: Root object non ISO date formats to support timezone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-07-29T16:59:41Z</created><updated>2011-08-13T09:55:10Z</updated><resolved>2011-07-29T17:01:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-13T09:32:02Z" id="1796905">We need to also support non time zone option...., thats a bug....
</comment><comment author="kimchy" created="2011-08-13T09:55:10Z" id="1796939">Will revert this, as its tricky without changing default behavior now that I think about it. Need to think of a better way to suport optional timezone in this case.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Mapping: Root object non ISO date formats to support timezone, closes #1181.</comment></comments></commit></commits></item><item><title>When flushing, old transaction log is not removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1180</link><project id="" key="" /><description>When flushing, old transaction log is not removed
</description><key id="1310393">1180</key><summary>When flushing, old transaction log is not removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-07-29T14:30:01Z</created><updated>2011-08-10T13:22:56Z</updated><resolved>2011-07-29T16:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-08-10T13:22:56Z" id="1772408">This issue was wrongly applied to the 0.17.3 branch, reapplied properly to 0.17.5.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file></files><comments><comment>When flushing, old transaction log is not removed, closes #1180.</comment></comments></commit></commits></item><item><title>More Like This API: Allow to provide `search_size` and `search_from`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1179</link><project id="" key="" /><description>Allow to provide `search_size` and `search_from` in the more like this API to control the size/from of the search executed.
</description><key id="1308526">1179</key><summary>More Like This API: Allow to provide `search_size` and `search_from`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-29T07:43:09Z</created><updated>2011-07-29T07:46:11Z</updated><resolved>2011-07-29T07:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/Client.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/mlt/MoreLikeThisRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/mlt/RestMoreLikeThisAction.java</file></files><comments><comment>More Like This API: Allow to provide `search_size` and `search_from`, closes #1179.</comment></comments></commit></commits></item><item><title>Search API: URI request allow to additional query_string parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1178</link><project id="" key="" /><description>Add `analyze_wildcard` and `lowercase_expanded_terms`.
</description><key id="1308339">1178</key><summary>Search API: URI request allow to additional query_string parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-29T06:47:45Z</created><updated>2011-07-29T06:48:20Z</updated><resolved>2011-07-29T06:48:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file></files><comments><comment>Search API: URI request allow to additional query_string parameters, closes #1178.</comment></comments></commit></commits></item><item><title>HTTP: Disable automatic cookie parsing and resetting, allow to enable it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1177</link><project id="" key="" /><description>Currently, all the cookies passed are passed and re SET in the response. Seems problematic with #1172, and really, does not seem necessary to do it, even with plugin sites installed. One can reneable it using: `http.reset_cookies`.
</description><key id="1305039">1177</key><summary>HTTP: Disable automatic cookie parsing and resetting, allow to enable it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-28T19:02:15Z</created><updated>2011-07-28T19:02:52Z</updated><resolved>2011-07-28T19:02:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/HttpRequestHandler.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file></files><comments><comment>HTTP: Disable automatic cookie parsing and resetting, allow to enable it, closes #1177.</comment></comments></commit></commits></item><item><title>Running as a service on Window 2008 64bit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1176</link><project id="" key="" /><description>When trying to install elastic on Windows 2008 SP2 64 bit I get the following error. When I look in github and my directory the exe does not exists. Is this intentionally missing?

D:\tools\elasticsearch-0.16.2\bin\service&gt;elasticsearch.bat install
Unable to locate a Wrapper executable using any of the following names:
D:\tools\elasticsearch-0.16.2\bin\service\exec\elasticsearch-windows-x86-64.exe

D:\tools\elasticsearch-0.16.2\bin\service\exec\elasticsearch.exe
Press any key to continue . . .
</description><key id="1304814">1176</key><summary>Running as a service on Window 2008 64bit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hvandenb</reporter><labels /><created>2011-07-28T18:33:02Z</created><updated>2014-02-22T11:42:57Z</updated><resolved>2014-02-22T11:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-30T11:48:08Z" id="1687887">Sadly, the service wrapper used does not come with 64bit windows binaries... . We probably need to work on another solution for services under windows.
</comment><comment author="jdzurik" created="2012-01-10T18:34:24Z" id="3434483">I created a repository to help with installing the windows service and setting some of the properties. 

It is based off the .bat but runs java from C# and output's the console to a log file.  

https://github.com/jdzurik/ESWindowsInstaller
</comment><comment author="spinscale" created="2014-02-22T11:42:57Z" id="35800706">You can register elasticsearch pretty easy as a service under windows now, see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-service-win.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Groovy Plugins: Upgrade to 1.8.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1175</link><project id="" key="" /><description /><key id="1304477">1175</key><summary>Groovy Plugins: Upgrade to 1.8.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-28T17:40:57Z</created><updated>2011-07-28T17:41:25Z</updated><resolved>2011-07-28T17:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Groovy Plugins: Upgrade to 1.8.1, closes #1175.</comment></comments></commit></commits></item><item><title>Http Transport: Allow to configure `max_header_size`, `max_initial_line_length`, and `max_chunk_size`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1174</link><project id="" key="" /><description>Allow to configure `http.max_header_size` (defaults to `8k`), `http_max_initial_line_length` (defaults to `4kb`), and `http.max_chunk_size` (defaults to `8kb`).
</description><key id="1303632">1174</key><summary>Http Transport: Allow to configure `max_header_size`, `max_initial_line_length`, and `max_chunk_size`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-28T15:32:08Z</created><updated>2014-01-27T10:56:55Z</updated><resolved>2011-07-28T15:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file></files><comments><comment>Http Transport: Allow to configure `max_header_size`, `max_initial_line_length`, and `max_chunk_size`, closes #1174.</comment></comments></commit></commits></item><item><title>Enhance Jackson's errors output when parsing Json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1173</link><project id="" key="" /><description>Map the field `"integer"` to the type `integer`, index the following document:

```
{"integer":9223372036854775807}
```

In the logs, you'll get the error described at: https://gist.github.com/1111244
And your (curl) request will receive the following answer:

```
{
  "error":
    "MapperParsingException[Failed to parse [integer]];
     nested: JsonParseException[Numeric value (9223372036854775807) out of range of int\n
     at [Source: [B@3ad6a0e0; line: 1, column: 31]]; ",
  "status":400
}
```

The part `[Source: [B@3ad6a0e0; line: 1, column: 31]]` could be enhanced to output the actual source, as it is done in many (if not all) other places.
</description><key id="1301631">1173</key><summary>Enhance Jackson's errors output when parsing Json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-07-28T09:06:49Z</created><updated>2013-04-05T13:54:05Z</updated><resolved>2013-04-05T13:54:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-28T09:21:52Z" id="1671174">I actually wonder if it should not be in jackson itself, I think its the best place for it (the Source... is coming from jackson because a byte[] is passed). Will check with Tatu.
</comment><comment author="kimchy" created="2011-07-28T09:24:31Z" id="1671190">Opened an issue on jackson, lets see how it goes: http://jira.codehaus.org/browse/JACKSON-642. Thats the correct place to fix this.
</comment><comment author="clintongormley" created="2013-04-05T13:54:05Z" id="15956812">Over to Jackson - closing the issue here
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>StackOverflowError with plain 0.17.2 when HTTP querying for _status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1172</link><project id="" key="" /><description>Unpack the new 0.17.2

Configure
cluster.name: NAME
discovery.zen.ping.multicast.enabled: false

and then open http://localhost:9200/_status/ in the web browser

The log:

[2011-07-27 17:20:24,336][INFO ][node                     ] [Enigma] {elasticsearch/0.17.2}[5792]: initializing ...
[2011-07-27 17:20:24,343][INFO ][plugins                  ] [Enigma] loaded [], sites []
[2011-07-27 17:20:26,136][INFO ][node                     ] [Enigma] {elasticsearch/0.17.2}[5792]: initialized
[2011-07-27 17:20:26,136][INFO ][node                     ] [Enigma] {elasticsearch/0.17.2}[5792]: starting ...
[2011-07-27 17:20:26,250][INFO ][transport                ] [Enigma] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/X.X.X.X:9300]}
[2011-07-27 17:20:29,314][INFO ][cluster.service          ] [Enigma] new_master [Enigma][JxL5e_bZQQGFRbRJ__qHOA][inet[/X.X.X.X:9300]], reason: zen-disco-join (elected_as_master)
[2011-07-27 17:20:29,351][INFO ][discovery                ] [Enigma] NAME/JxL5e_bZQQGFRbRJ__qHOA
[2011-07-27 17:20:29,383][INFO ][gateway                  ] [Enigma] recovered [0] indices into cluster_state
[2011-07-27 17:20:29,416][INFO ][http                     ] [Enigma] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/X.X.X.X:9200]}
[2011-07-27 17:20:29,417][INFO ][node                     ] [Enigma] {elasticsearch/0.17.2}[5792]: started
[2011-07-27 17:21:51,340][WARN ][http.netty               ] [Enigma] Caught exception while handling client http traffic, closing connection
java.lang.StackOverflowError
    at java.lang.Character.codePointAt(Character.java:2335)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3344)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.match(Pattern.java:4295)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)
    at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Loop.matchInit(Pattern.java:4314)
    at java.util.regex.Pattern$Prolog.match(Pattern.java:4251)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3366)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3366)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Branch.match(Pattern.java:4114)
    at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)
    at java.util.regex.Pattern$Curly.match0(Pattern.java:3782)
    at java.util.regex.Pattern$Curly.match(Pattern.java:3744)
    at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)
    at java.util.regex.Pattern$Curly.match0(Pattern.java:3789)
    at java.util.regex.Pattern$Curly.match(Pattern.java:3744)
    at java.util.regex.Pattern$Loop.matchInit(Pattern.java:4316)
    at java.util.regex.Pattern$Prolog.match(Pattern.java:4251)
    at java.util.regex.Pattern$Start.match(Pattern.java:3055)
    at java.util.regex.Matcher.search(Matcher.java:1105)
    at java.util.regex.Matcher.find(Matcher.java:561)
    at org.elasticsearch.common.netty.handler.codec.http.CookieDecoder.extractKeyValuePairs(CookieDecoder.java:192)
    at org.elasticsearch.common.netty.handler.codec.http.CookieDecoder.decode(CookieDecoder.java:68)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:132)
    at org.elasticsearch.rest.action.admin.indices.status.RestIndicesStatusAction$1.onResponse(RestIndicesStatusAction.java:77)
    at org.elasticsearch.rest.action.admin.indices.status.RestIndicesStatusAction$1.onResponse(RestIndicesStatusAction.java:68)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.start(TransportBroadcastOperationAction.java:153)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:75)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:47)
    at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:61)
    at org.elasticsearch.client.node.NodeIndicesAdminClient.status(NodeIndicesAdminClient.java:179)
    at org.elasticsearch.rest.action.admin.indices.status.RestIndicesStatusAction.handleRequest(RestIndicesStatusAction.java:68)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:88)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:118)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:84)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:270)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:40)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:83)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:100)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
    at org.elasticsearch.common.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:104)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:302)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:65)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
</description><key id="1296587">1172</key><summary>StackOverflowError with plain 0.17.2 when HTTP querying for _status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cpesch</reporter><labels /><created>2011-07-27T15:26:45Z</created><updated>2011-08-22T09:09:49Z</updated><resolved>2011-08-22T09:09:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-27T16:59:36Z" id="1665553">Strange, this does not happen to me. It seems to fail on cookie decoding within netty (the http server). Nothing changed there... .
- Which browser do you use? 
- Is this new in 0.17.2 and worked in 0.17.1? 
- What happens when you switch browsers?
- What happens when you use curl?
</comment><comment author="cpesch" created="2011-07-28T07:36:41Z" id="1670678">I can reproduce it with Firefox 4 with 0.17.1 and 0.17.2, I could not reproduce it with curl, Chrome 12 and IE 9.

Since Firefox is my primary browser and I'm developing a lot, it has a lot of cookies stored for localhost. These are the HTTP headers as shown by Firebug:

Host    localhost:9200
User-Agent  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1
Accept  text/html,application/xhtml+xml,application/xml;q=0.9,_/_;q=0.8
Accept-Language en,de;q=0.5
Accept-Encoding gzip, deflate
Accept-Charset  ISO-8859-1,utf-8;q=0.7,*;q=0.7
Keep-Alive  115
Connection  keep-alive
Cookie  com.coremedia.cms.editor.locale=en; cmKeywordCookie="eyJfX21heF9fIjozNS4wLCJpbnN1cmFuY2UiOjQuMCwiUG9ydGFsIjoyLjAsInBvcnRhbGludGVncmF0aW9uIjoyLjAsIlNBUCI6Mi4wLCJNb2JpbGUiOjM1LjAsIlBlcnNvbmFsaXNpZXJ1bmciOjIxLjAsIkNvbnRlbnQiOjMuMCwiaW5kdXN0cnkiOjExLjAsIkNvcmVNZWRpYSI6My4wLCJEZWxpdmVyeSI6My4wLCJNZWRpYSI6My4wLCJQcm9kdWN0IjozLjAsImFkYXB0aXZlIjoxOS4wLCJtb2JpbCI6MTQuMCwiTW9iaWwiOjIxLjAsInNldmVudmFsIjoxNC4wLCJfX3RvdGFsX18iOjM2My4wLCJpcGhvbmUiOjE0LjAsIlBvcnRhbGludGVncmF0aW9uIjoyLjAsInRlbGNvIjoxLjAsIkNsaWVudHMiOjEwLjAsIldlYiI6MzUuMCwiTVRQIjoxNC4wLCJDVFAiOjE0LjAsImZpbmFuY2UiOjQuMCwiS29udGFrdCI6MTQuMCwiSW50cmFuZXQiOjIuMCwicHVibGljIjo1LjAsIlRlY2huaXNjaCI6MTQuMCwibWVkaWEiOjYuMCwiUHJvZHVrdCI6MjMuMCwiY2xpZW50cyI6MjYuMCwibW9iaWxlIjoxOS4wfQ=="; cmExplicitCookie="e30="; guid=b61b71ba-cd13-4a61-acf7-9a071444d6cd+4550f76013625a8b07a9d0f98bf36cc9dee829a5ed8d0277aef4e790cf24a6693943e7d1205b9372d49d74407771ee7c34d74bd18be1ce01891102cc0d48ef14; cmSubjectTaxonomiesCookie="e30="; cmLocationTaxonomiesCookie="e30="; cmLastVisited="e30="; __utma=111872281.1768004830.1311339006.1311339006.1311347537.2; __utmz=111872281.1311339006.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); JSESSIONID=hmk31r04042613y3i8gwftrgv
Cache-Control   max-age=0

Could that cause the problem with the cookie decoding?
</comment><comment author="kimchy" created="2011-07-28T08:18:38Z" id="1670870">I took the cookie value, and ran it through the way netty parses the cookie, and it seems to work fine. Which jvm version are you using, maybe its a jvm bug?

Here is the (Java) code I ran:

```
public class Test {

    private final static Pattern PATTERN =
        Pattern.compile("(?:\\s|[;,])*\\$*([^;=]+)(?:=(?:[\"']((?:\\\\.|[^\"])*)[\"']|([^;,]*)))?(\\s*(?:[;,]+\\s*|$))");

    public static void main(String[] args) {

        String value = "com.coremedia.cms.editor.locale=en; cmKeywordCookie=\"eyJfX21heF9fIjozNS4wLCJpbnN1cmFuY2UiOjQuMCwiUG9ydGFsIjoyLjAsInBvcnRhbGludGVncmF0aW9uIjoyLjAsIlNBUCI6Mi4wLCJNb2JpbGUiOjM1LjAsIlBlcnNvbmFsaXNpZXJ1bmciOjIxLjAsIkNvbnRlbnQiOjMuMCwiaW5kdXN0cnkiOjExLjAsIkNvcmVNZWRpYSI6My4wLCJEZWxpdmVyeSI6My4wLCJNZWRpYSI6My4wLCJQcm9kdWN0IjozLjAsImFkYXB0aXZlIjoxOS4wLCJtb2JpbCI6MTQuMCwiTW9iaWwiOjIxLjAsInNldmVudmFsIjoxNC4wLCJfX3RvdGFsX18iOjM2My4wLCJpcGhvbmUiOjE0LjAsIlBvcnRhbGludGVncmF0aW9uIjoyLjAsInRlbGNvIjoxLjAsIkNsaWVudHMiOjEwLjAsIldlYiI6MzUuMCwiTVRQIjoxNC4wLCJDVFAiOjE0LjAsImZpbmFuY2UiOjQuMCwiS29udGFrdCI6MTQuMCwiSW50cmFuZXQiOjIuMCwicHVibGljIjo1LjAsIlRlY2huaXNjaCI6MTQuMCwibWVkaWEiOjYuMCwiUHJvZHVrdCI6MjMuMCwiY2xpZW50cyI6MjYuMCwibW9iaWxlIjoxOS4wfQ==\"; cmExplicitCookie=\"e30=\"; guid=b61b71ba-cd13-4a61-acf7-9a071444d6cd+4550f76013625a8b07a9d0f98bf36cc9dee829a5ed8d0277aef4e790cf24a6693943e7d1205b9372d49d74407771ee7c34d74bd18be1ce01891102cc0d48ef14; cmSubjectTaxonomiesCookie=\"e30=\"; cmLocationTaxonomiesCookie=\"e30=\"; cmLastVisited=\"e30=\"; __utma=111872281.1768004830.1311339006.1311339006.1311347537.2; __utmz=111872281.1311339006.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); JSESSIONID=hmk31r04042613y3i8gwftrgv";

        Matcher m = PATTERN.matcher(value);
        int pos = 0;
        while (m.find(pos)) {
            pos = m.end();
        }
    }
}
```
</comment><comment author="cpesch" created="2011-07-28T12:21:45Z" id="1672114">Your test runs fine with both 32- and 64-bit Java 6 on Windows 7 64-bit:

C:&gt; 'C:\Program Files\Java\jdk1.6.0_26\bin\java.exe' -version
java version "1.6.0_26"
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)
C:&gt; 'C:\Program Files\Java\jdk1.6.0_26\bin\java.exe' Test
C:&gt; 'C:\Program Files (x86)\Java\jdk1.6.0_26\bin\java.exe' -version
java version "1.6.0_26"
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) Client VM (build 20.1-b02, mixed mode, sharing)
C:&gt; 'C:\Program Files (x86)\Java\jdk1.6.0_26\bin\java.exe' Test
C:&gt;
</comment><comment author="kimchy" created="2011-07-28T17:04:51Z" id="1674402">ok, I can see where the problem is..., its basically because elasticsearch sets in the `elasticsearch.in.sh` file the size for the thread stack size to `128k`. If I set it to `1024k`, then it works. Damn Java regex... . 
</comment><comment author="kimchy" created="2011-07-28T19:03:23Z" id="1675518">worked on it a bit more, and opened #1177, this should solve it. you can verify on master or 0.17 branch if you want.
</comment><comment author="cpesch" created="2011-08-22T09:09:46Z" id="1868345">problem does not occur with 0.17.6 on my machine
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting returning an excerpt even with no highlights</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1171</link><project id="" key="" /><description>Highlighting is great as providing an _highlighted excerpt_, but if you run no query against one field, you will have no output at all.
Highlighting could provide a non highlighted excerpt (some number of characters from the beginning of the value) as a fallback.
This would be yield shorted answers than always asking for the whole field content (that may be quite long).

``` json
{
  "highlight" : {
    "fields" : {
      "text" : {
        "number_of_fragments" : 3,
        "size_of_fragments" : 150,
        "no_results" : VALUE
      }
    }
  }
}
```

Where _VALUE_ could take: `hide` (default), `show`, or a character count.
Where `show` would be equivalent to setting the character count to `number_of_fragments * size_of_fragments`.
</description><key id="1295679">1171</key><summary>Highlighting returning an excerpt even with no highlights</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ofavre</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2011-07-27T13:17:07Z</created><updated>2013-11-12T14:36:27Z</updated><resolved>2013-10-24T12:57:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2012-03-13T19:47:14Z" id="4484346">While not ideal, a temp workaround is to use a script field to always grab a chunk of text and choose between that one if highlights don't come back. Something like this:

"script_fields":
{"data.fragment":
  {"script":"if (_source[fieldName] == null || _source[fieldName].length() &lt; fragmentSize) return _source[fieldName];\nreturn _source[fieldName].substring(0, fragmentSize);",
   "params":{"fragmentSize":300,"fieldName":"data"}
  }
}
</comment><comment author="nik9000" created="2013-09-03T14:52:00Z" id="23718387">I opened and then just closed 3609 which is a dupe of this issue.  This issue is better written any way.

The script isn't really a great workaround for me because it doesn't respect `boundary_chars` and `boundary_max_scan`.
</comment><comment author="javanna" created="2013-09-03T14:57:28Z" id="23718832">@nik9000 Are you familiar with highlighting? Would you like to send a pull request for this?
</comment><comment author="nik9000" created="2013-09-03T15:04:59Z" id="23719433">The only thing in ES I think I come close to understanding is phrase suggestions.  I'll see if I can figure it out though.
</comment><comment author="nik9000" created="2013-09-03T18:48:34Z" id="23737173">That last commit should work for this.  I'd love for someone to point out a better way to do this for the plain highlighter - currently that one works pretty much just like the script above.  The FVH though does all its normal FVH things including `boundary_chars` and `boundary_max_scan`.  The api is literally exactly as proposed by @ofavre two years ago.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Minor changes to no_match_size highlight parameter and highlight tests (#1171)</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Highlighting can return excerpt with no highlights</comment></comments></commit></commits></item><item><title>Seeded random ordering (Feature request)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1170</link><project id="" key="" /><description>Implementation of random result ordering with optional seed allowing for recreation of random order i.e. across "pages" when using pagination (from, size).
</description><key id="1295051">1170</key><summary>Seeded random ordering (Feature request)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pawpro</reporter><labels><label>feature</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2011-07-27T11:04:35Z</created><updated>2014-05-16T23:18:20Z</updated><resolved>2013-08-17T20:58:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattiassvedhem" created="2012-03-20T01:59:40Z" id="4587811">I'm really interested in this, did someone try to implement this?
</comment><comment author="pawpro" created="2012-03-21T13:52:23Z" id="4617594">I discussed this briefly with kimchy at the time and his feedback was that it wouldn't be hard to do. Definitely a great feature candidate.
I have implemented this by storing set of "random" numbers e.g. each object has say 20 additional integers (each one of these is a sorting field named as rand1, rand2, rand3 etc). For each user I select (calculate based on date and  UA)  number  1...20 which is used to select which "random" field will be used. When new objects are added each simply receives set of random. Umbers. You decide how many random states you need. Remember that reverse order in this case doubles the number of random states. This solution is all about perception.
</comment><comment author="mattiassvedhem" created="2012-03-22T00:08:28Z" id="4630073">Cool! I'll try that.
</comment><comment author="Evan-R" created="2013-04-18T04:50:12Z" id="16557455">Has this been implemented into the current version of ES yet?
</comment><comment author="s1monw" created="2013-04-18T06:07:36Z" id="16559448">@uboness can we pull your latest commit in for this?
</comment><comment author="emarthinsen" created="2013-05-10T03:02:10Z" id="17702043">This would be great if it were added.
</comment><comment author="missinglink" created="2013-07-22T16:19:04Z" id="21355960">BUMP++
</comment><comment author="uboness" created="2013-08-17T21:05:50Z" id="22819670">random order is now supported as part of the new function score queries. request example:

``` bash
curl -XGET 'localhost:9200/_search' -d '{
  "query": {
    "function_score" : {
      "query" : { "match_all": {} },
      "random_score" : {}
    }
  }
}';
```

or with a seed (for near-consistent pagination):

``` bash
curl -XGET 'localhost:9200/_search' -d '{
  "query": {
    "function_score" : {
      "query" : { "match_all": {} },
      "random_score" : { "seed" : 1376773391128418000 }
    }
  }
}';
```

Check out more on function score queries here: #3423
</comment><comment author="emptyflask" created="2013-08-20T15:26:28Z" id="22953169">This is immensely helpful. I hope we'll see this in v0.90.4 in a matter of days...
</comment><comment author="s1monw" created="2013-08-20T15:27:46Z" id="22953303">@emptyflask can you tell us a bit about your actual usecase for this? I am very curious and it would be good to have a couple of usecases on the reference documenation
</comment><comment author="emptyflask" created="2013-08-20T15:45:15Z" id="22954580">@s1monw I'm currently doing this with Solr, and it's the one thing that's tying up my migration to ES on that app. There are a few thousand user-submitted ideas that receive votes from other users, so to give every idea an equal chance of showing up on the first page, we want the default view to be randomly sorted, yet paginated and consistent for each user. Using the current user's id as a random seed solves this problem nicely.
</comment><comment author="missinglink" created="2013-08-20T15:54:20Z" id="22955239">:+1: 10 points for @uboness
</comment><comment author="brupm" created="2013-08-30T21:14:01Z" id="23589644">:+1: Will this make it into 0.90.4? 
</comment><comment author="s1monw" created="2013-08-31T06:58:02Z" id="23601830">@brupm it's already pushed to 0.90 branch so yes it will be in 0.90.4
</comment><comment author="damienalexandre" created="2013-09-02T15:09:00Z" id="23665431">Awesome feature, thx!

Another use-case is a Wikipedia-like random page - coupled to some search criteria (to avoid low interest pages) it's much faster than querying the DB for a random ID.
</comment><comment author="brupm" created="2013-09-03T21:46:54Z" id="23749656">@s1monw Any idea when 0.90.4 will be released?  Thanks!
</comment><comment author="kimchy" created="2013-09-03T21:48:52Z" id="23749776">we plan to release 0.90.4 early next week.
</comment><comment author="dctdct" created="2014-03-01T17:22:57Z" id="36430411">   "sort": [
      {
         "name": {
            "order": "random"
         }
      }
   ], 

Is not working :-(
</comment><comment author="emptyflask" created="2014-03-01T17:53:00Z" id="36431264">@dctdct This is the way you should do it: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html
</comment><comment author="dctdct" created="2014-03-03T19:11:25Z" id="36546824">Maybe this is a bug with the php client but passing this query as array doesn't work.

$json = '{
            "query": {
              "function_score": {
                 "functions": [
                    {
                       "random_score": {

```
                   }
                }
             ],
             "boost_mode": "replace",
             "query": {
                "match_all": {}
             }
          }
       }
    }';

    $qry = array(
        'query' =&gt; array(
            'function_score' =&gt; array(
                'functions' =&gt; array(
                    array('random_score' =&gt; array())
                ),
                'query' =&gt; array(
                    array('match_all' =&gt; array())
                )
            )
        )
    );

    $searchParams['body'] = $qry;

    $retDoc = $elastic-&gt;search($searchParams);
```
</comment><comment author="yao23" created="2014-05-16T21:27:48Z" id="43381486">How to iterate every items in each page with the random score function?
</comment><comment author="emptyflask" created="2014-05-16T23:18:20Z" id="43389163">Same way you would iterate over any other set of results. The function
score only affects the score attribute of each document, which reorders the
results. Everything else remains the same.
On May 16, 2014 4:28 PM, "Yao Li" notifications@github.com wrote:

&gt; How to iterate every items in each page with the random score function?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/1170#issuecomment-43381486
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreModule.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionBuilder.java</file><file>src/test/java/org/elasticsearch/test/integration/search/functionscore/RandomScoreFunctionTests.java</file><file>src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Added support for random_score function:</comment></comments></commit></commits></item><item><title>Combining multiple fields into one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1169</link><project id="" key="" /><description>Adding the possibility to map a property into one (or more) common field.
Should permit multiple properties into one common field, and one property to multiple fields.
</description><key id="1294851">1169</key><summary>Combining multiple fields into one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-07-27T10:21:45Z</created><updated>2013-11-08T17:59:00Z</updated><resolved>2012-11-19T23:17:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2011-07-27T10:23:27Z" id="1662725">Original mailing thread by OlgaT:
http://elasticsearch-users.115913.n3.nabble.com/mapping-configuration-tt3199690.html
</comment><comment author="ofavre" created="2011-07-27T10:56:34Z" id="1662908">Another thing that would be cool:
The field could either combine the values or the analyzed terms, depending on some mapping.

Say you have multiple shallow fields for the only sake of settings analyzers, say each has the translation of some word in multiple languages, you have thing_en, thing_fr, thing_de, etc. that are analyzed with the according analyzers (english, french, german, etc.). You will combine all those fields into a common field thing_lang_dependent.
At query time you want to query against the combined field along with the classic text fields, using the analyzer of the user's language setting.

Or say the terms are complicated to extract, as from an attachment, or a special company-specific encoding, when combining the fields, you can have multiple different such encodings. You don't want to write another custom analyzer for this mixed field.
Assuming you can search using a unique analyzer at search time.
</comment><comment author="slorber" created="2012-10-29T21:46:42Z" id="9886748">May be related: http://stackoverflow.com/questions/13123555/equivalent-of-copyfield-of-solr-on-elasticsearch/
</comment><comment author="ofavre" created="2012-11-19T23:17:49Z" id="10535844">It has been implemented as a plugin for ElasticSearch ≥ 0.19.
(It's been 8 month now! thanks to @jprante, maybe I should close that issue now...)
Please find it at : https://github.com/yakaz/elasticsearch-analysis-combo.
</comment><comment author="khituras" created="2013-06-09T07:37:55Z" id="19162181">While the elasticsearch-analysis-combo is a quite nice plugin, it doesn't solve the original issue here of combining multiple fields to one.
The combination of multiple fields in the JSON document to Lucene fields with the exact same name would be a very useful feature after all. If the feature would do the merging (sorting the analysis-result tokens by offset) it could even do what the elasticsearch-analysis-combo plugin does (and circumvent the analyser cloning).

More importantly, it could even be used as a pre-analysed field content feature similar to Solr's PreAnalyzed field type and PreAnalyzedUpdateProcessor: When you could specify multiple fields in the JSON document to end haveing the same Lucene document name (that part is important), you could have one JSON field stored, but not analyzed with the original text and another field not stored and delivered with a serialized version of a custom TokenStream created externally (e.g. as in the Solr case: http://wiki.apache.org/solr/JsonPreAnalyzedParser)).
If the resulting Lucene fields had the same name, even highlighting should work correctly, similarly as when using the synonym filter: You search for a synonym but not only find documents containing the original word but also have those highlighted.

This does not work when mapping to JSON fields using the "index_name" property. Example:
{
"text":{"type": "string","index": "no","store":"yes"},
"text_analyzed":{"type": "string","index": "analyzed","term_vector" : "with_positions_offsets", "index_name":"text"}
}
Note the "index_name":"text" part of the "text_analyzed" field.
This approach does not work with highlighting. I'm not exactly sure why, but most probably because internally we still have to fields "text" and "text_analyzed" where the second only gets an alias to "text".
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: _default_ mapping type with root level date_formats can cause recursive addition of them to the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1168</link><project id="" key="" /><description>Mapping: _default_ mapping type with root level date_formats can cause recursive addition of them to the mapping.
</description><key id="1294378">1168</key><summary>Mapping: _default_ mapping type with root level date_formats can cause recursive addition of them to the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-27T08:29:10Z</created><updated>2011-07-27T08:41:50Z</updated><resolved>2011-07-27T08:41:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file></files><comments><comment>Mapping: _default_ mapping type with root level date_formats can cause recursive addition of them to the mapping, closes #1168.</comment></comments></commit></commits></item><item><title>Get API: a get for a document that does not exists can cause open file handles leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1167</link><project id="" key="" /><description>Get API: a get for a document that does not exists can cause open file handles leak
</description><key id="1291148">1167</key><summary>Get API: a get for a document that does not exists can cause open file handles leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-26T19:42:58Z</created><updated>2011-07-26T19:57:46Z</updated><resolved>2011-07-26T19:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>Get API: a get for a document that does not exists can cause open file handles leak, closes #1167.</comment></comments></commit></commits></item><item><title>Document what mapping updates are allowed </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1166</link><project id="" key="" /><description>Currently, change of type is not allowed when updating a mapping.  What are the other updates are not possible?
</description><key id="1289112">1166</key><summary>Document what mapping updates are allowed </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels /><created>2011-07-26T14:28:58Z</created><updated>2012-07-20T00:30:37Z</updated><resolved>2012-07-20T00:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="caravone" created="2011-09-12T18:47:37Z" id="2073904">You also cannot change a field from not indexed to indexed. (http://groups.google.com/group/elasticsearch/browse_thread/thread/b426978237d41555)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: 'truncate' token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1165</link><project id="" key="" /><description>`truncate` token filter truncates tokens. The `length` parameter can control its length, defaults to `10` (so you can use it as is, without "defining" it, and it will truncate to 10).

It'd be useful to be able to truncate tokens to a max length for sorting purposes.

For instance, I index a `name` field and i want it analyzed (so that i can search on it) but have it as a multi-field with a not-analyzed or keyword version to sort on, but I don't need to store all 100 characters, just the first 10 characters.

Of course, it would need to take unicode into account so that it doesn't split up a multicode character
</description><key id="1288143">1165</key><summary>Analysis: 'truncate' token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-26T11:43:54Z</created><updated>2011-07-29T20:36:15Z</updated><resolved>2011-07-29T20:36:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/TruncateTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file><file>modules/elasticsearch/src/test/java/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilterTests.java</file></files><comments><comment>Analysis: 'truncate' token filter, closes #1165.</comment></comments></commit></commits></item><item><title>Regression: fields= no longer works</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1164</link><project id="" key="" /><description>Doing a get with `fields=` has been broken by commit 8c9dffc235a940e1f9cccbd62333795868e68149
</description><key id="1287800">1164</key><summary>Regression: fields= no longer works</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-07-26T10:15:01Z</created><updated>2011-07-26T14:12:46Z</updated><resolved>2011-07-26T14:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/GetActionTests.java</file></files><comments><comment>improvement to string splitting caused fields= on get to return the source back, fix it and also optimize this case when using realtime get, closes #1164.</comment></comments></commit></commits></item><item><title>Index Settings: Add `index.recovery.initial_shards` controlling the number of shards to exists when using local gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1163</link><project id="" key="" /><description>The `index.recovery.initial_shards` allow to control the number of shards expected to be found on full cluster restart per index. The values are: `quorum`, `quorum-1`, `full`, `full-1`, and a numeric value.

This setting is a dynamic setting and can be set using the update settings API.
</description><key id="1287340">1163</key><summary>Index Settings: Add `index.recovery.initial_shards` controlling the number of shards to exists when using local gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-26T08:36:05Z</created><updated>2011-07-26T08:46:25Z</updated><resolved>2011-07-26T08:46:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGatewayNodeAllocation.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/gateway/local/QuorumLocalGatewayTests.java</file></files><comments><comment>Index Settings: Add `index.recovery.initial_shards` controlling the number of shards to exists when using local gateway, closes #1163.</comment></comments></commit></commits></item><item><title>Update Settings: Changing the number of replicas does not cause allocation / deallocation of shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1162</link><project id="" key="" /><description>The shards do not get allocated / deallocated when changing the number of replicas until another state changing event happens... (like creating an index, adding / removing a node)
</description><key id="1287208">1162</key><summary>Update Settings: Changing the number of replicas does not cause allocation / deallocation of shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-26T08:02:49Z</created><updated>2011-08-28T09:55:59Z</updated><resolved>2011-07-26T08:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file></files><comments><comment>Update Settings: Changing the number of replicas does cause allocation / deallocation of shards, closes #1162.</comment></comments></commit></commits></item><item><title>Server crashes when posting documents and searching them immediately afterwards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1161</link><project id="" key="" /><description>I have a test that does a bulk post of documents into 3 indices and immediately afterwards does a _search.

It fails due to that the documents are not indexed yet for search and the server also crashes:

[2011-07-26 09:55:33,174][INFO ][node                     ] [Orphan] {elasticsearch/0.17.1}[619]: initializing ...
[2011-07-26 09:55:33,181][INFO ][plugins                  ] [Orphan] loaded [], sites []
[2011-07-26 09:55:35,240][INFO ][node                     ] [Orphan] {elasticsearch/0.17.1}[619]: initialized
[2011-07-26 09:55:35,240][INFO ][node                     ] [Orphan] {elasticsearch/0.17.1}[619]: starting ...
[2011-07-26 09:55:35,366][INFO ][transport                ] [Orphan] bound_address {inet[/0.0.0.0:9301]}, publish_address {inet[/10.0.1.3:9301]}
[2011-07-26 09:55:38,489][INFO ][cluster.service          ] [Orphan] detected_master [Ecstasy][CW7W5ET5Spm4Wug76WCcYQ][inet[/10.0.1.3:9300]], added {[Ecstasy][CW7W5ET5Spm4Wug76WCcYQ][inet[/10.0.1.3:9300]],}, reason: zen-disco-receive(from master [[Ecstasy][CW7W5ET5Spm4Wug76WCcYQ][inet[/10.0.1.3:9300]]])
[2011-07-26 09:55:38,495][INFO ][discovery                ] [Orphan] elasticsearch/7pVYKvdARNaHqhPk0rbczA
[2011-07-26 09:55:38,504][INFO ][http                     ] [Orphan] bound_address {inet[/0.0.0.0:9201]}, publish_address {inet[/10.0.1.3:9201]}
[2011-07-26 09:55:38,505][INFO ][node                     ] [Orphan] {elasticsearch/0.17.1}[619]: started
[2011-07-26 09:56:02,342][WARN ][index.shard.service      ] [Orphan] [apple.com][0] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [apple.com][0] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:762)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:384)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:613)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.FileNotFoundException: /Volumes/Private/johnnyluu/Documents/Development/third_party/elasticsearch-0.17.1/data/elasticsearch/nodes/1/indices/apple.com/0/index/_0.frq (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.&lt;init&gt;(RandomAccessFile.java:216)
    at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.&lt;init&gt;(SimpleFSDirectory.java:69)
    at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.&lt;init&gt;(SimpleFSDirectory.java:90)
    at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.&lt;init&gt;(NIOFSDirectory.java:91)
    at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:78)
    at org.elasticsearch.index.store.support.AbstractStore$StoreDirectory.openInput(AbstractStore.java:344)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:85)
    at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:114)
    at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:702)
    at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:660)
    at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:157)
    at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:38)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:459)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:407)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:414)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:427)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:392)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:745)
    ... 5 more
[2011-07-26 09:56:02,356][WARN ][index.shard.service      ] [Orphan] [apple.com][2] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [apple.com][2] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:762)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:384)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:613)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.FileNotFoundException: _0.fdt
    at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:284)
    at org.elasticsearch.index.store.support.AbstractStore$StoreDirectory.fileLength(AbstractStore.java:310)
    at org.elasticsearch.index.store.support.AbstractStore$StoreIndexOutput.close(AbstractStore.java:428)
    at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:118)
    at org.apache.lucene.index.FieldsWriter.close(FieldsWriter.java:127)
    at org.apache.lucene.index.StoredFieldsWriter.flush(StoredFieldsWriter.java:52)
    at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:59)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:581)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3543)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3508)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:458)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:407)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:414)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:427)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:392)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:745)
    ... 5 more

However, if I wait 1 sec after I posted the documents I get the search results as usual.
</description><key id="1287197">1161</key><summary>Server crashes when posting documents and searching them immediately afterwards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2011-07-26T07:59:44Z</created><updated>2011-07-26T08:39:24Z</updated><resolved>2011-07-26T08:39:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-26T08:06:09Z" id="1652431">Please, post questions in the mailing lists, and lets open issues if there really are issues. That exception does look interesting, but we should discuss it on the mailing list.
</comment><comment author="ghost" created="2011-07-26T08:30:09Z" id="1652532">I've browsed through issues and I can't tell what are issues and questions, eg:

https://github.com/elasticsearch/elasticsearch/issues/988
https://github.com/elasticsearch/elasticsearch/issues/916
https://github.com/elasticsearch/elasticsearch/issues/912

I don't think I posted a question this time but a direct bug.

I thought these kind of problems were issues and should be posted here.
</comment><comment author="kimchy" created="2011-07-26T08:37:54Z" id="1652576">I can't control what people open, but I do prefer to have discussions on the mailing list, and then open special issues that can spawn out of them. What you ask here is a direct question, its basically relates to the refresh interval in es, which is a question. Also, I prefer to have a discussion on what you run, how you run it, and so on, on the mailing list. Issues are explicit problems identified.
</comment><comment author="ghost" created="2011-07-26T08:39:24Z" id="1652581">Okay then fair enough.

I'll post it on the mailing list and will stick to it in the future.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Local Gateway: Allow to set gateway.local.initial_shards to `quorum-1`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1160</link><project id="" key="" /><description>Add another value to `gateway.local.initial_shards` of value `quorum-1`.
</description><key id="1287183">1160</key><summary>Local Gateway: Allow to set gateway.local.initial_shards to `quorum-1`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-26T07:55:32Z</created><updated>2011-07-26T07:56:08Z</updated><resolved>2011-07-26T07:56:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGatewayNodeAllocation.java</file></files><comments><comment>Local Gateway: Allow to set gateway.local.initial_shards to `quorum-1`, closes #1160.</comment></comments></commit></commits></item><item><title>Unicast Discovery: Clusters under different cluster names can cause failed discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1159</link><project id="" key="" /><description>Basically, the logic to filter the nodes from different clusters is not correct, instead of filtering just that node, it can (depending on the order), filter the whole aggregated response.
</description><key id="1286556">1159</key><summary>Unicast Discovery: Clusters under different cluster names can cause failed discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-26T04:50:03Z</created><updated>2011-07-26T04:50:46Z</updated><resolved>2011-07-26T04:50:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Unicast Discovery: Clusters under different cluster names can cause failed discovery, closes #1159.</comment></comments></commit></commits></item><item><title>Improve peer recovery of index files to reduce chances of corruption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1158</link><project id="" key="" /><description>When copying over index files into a node that already has them, improve the logic in order to reduce chances of corruption of that shard on that node in case of a full cluster shutdown.
</description><key id="1286471">1158</key><summary>Improve peer recovery of index files to reduce chances of corruption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-26T04:18:15Z</created><updated>2011-07-26T04:18:57Z</updated><resolved>2011-07-26T04:18:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferDirectory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/recovery/RecoveryStatus.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/recovery/RecoveryTarget.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/Store.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/FsStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/memory/ByteBufferStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/ram/RamStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/support/AbstractStore.java</file></files><comments><comment>Improve peer recovery of index files to reduce chances of corruption, closes #1158.</comment></comments></commit></commits></item><item><title>_mget into _bulk API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1157</link><project id="" key="" /><description>I'm confused about why there is _mget for reading documents and _bulk for creating, indexing and deleting documents.

It would be better if there is only _bulk for all actions
</description><key id="1285346">1157</key><summary>_mget into _bulk API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2011-07-25T22:59:13Z</created><updated>2011-07-26T03:11:57Z</updated><resolved>2011-07-26T03:11:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-26T03:11:57Z" id="1651309">Please, questions on the mailing list, and the reason for that is that they have a completely different structure and optimized to do different things.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk API: _version on delete actions is not honored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1156</link><project id="" key="" /><description>The `delete` action does not uses the `_version` provided properly, effectively ignoring it.
</description><key id="1285321">1156</key><summary>Bulk API: _version on delete actions is not honored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels><label>bug</label><label>v0.16.5</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-25T22:53:51Z</created><updated>2011-07-26T04:35:43Z</updated><resolved>2011-07-26T04:35:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-26T03:13:26Z" id="1651312">It does support versioning, read the page you link to again. And again, questions in the mailing list please.
</comment><comment author="ghost" created="2011-07-26T03:17:07Z" id="1651326">I tried deleting versions that were not there and succeeded. I'll try again.

And sorry for posting questions in here. Thought this one was a missing feature or a bug.
</comment><comment author="kimchy" created="2011-07-26T03:27:13Z" id="1651364">Ahh, I checked the code, and I see the problem, will push a fix.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file></files><comments><comment>Bulk API: _version on delete actions is not honored, closes #1156.</comment></comments></commit></commits></item><item><title>Add polish language analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1155</link><project id="" key="" /><description>Elastic Search lacks support for polish language out of box. An analyzer is available here:
http://lucene.apache.org/java/3_3_0/api/contrib-stempel/index.html

Including it in the package would be useful for polish ES users.
</description><key id="1281182">1155</key><summary>Add polish language analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pkonieczny</reporter><labels /><created>2011-07-25T11:21:19Z</created><updated>2015-11-24T14:56:15Z</updated><resolved>2012-01-19T14:04:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2011-07-27T04:42:31Z" id="1661221">FYI, in lucene we now have 2 polish analyzers, the other is based on Morfologik https://issues.apache.org/jira/browse/LUCENE-2341

Note: this one relies upon Java6, so its only currently in Lucene 4.0
</comment><comment author="MaciejZ" created="2011-11-16T12:17:29Z" id="2758756">I'd really love to see support for Polish language. Thanks in advance for your effort!
</comment><comment author="bonpensiero" created="2011-11-22T09:54:17Z" id="2832419">A lot of my development contains European languages, polish included. I'd would really appreciate seeing polish analyzer added to Elastic Search.

Kind Regards,
Ignacy
</comment><comment author="tkloc" created="2012-01-16T21:36:40Z" id="3518718">+1
</comment><comment author="kimchy" created="2012-01-19T14:04:50Z" id="3562993">A stempel plugin added: https://github.com/elasticsearch/elasticsearch-analysis-stempel.
</comment><comment author="pkonieczny" created="2012-02-01T16:21:25Z" id="3761582">Thanks, much appreciated!
</comment><comment author="marcing" created="2015-11-24T14:56:15Z" id="159292391">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search API: REST endpoint should use default operation_threading of thread_per_shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1154</link><project id="" key="" /><description>`operation_threading`, which defaults to `thread_per_shard` on the Java API, should default to it also in the REST API (it defaults to `single_thread` now). This only applies for search executed on the node the request was received on.
</description><key id="1278154">1154</key><summary>Search API: REST endpoint should use default operation_threading of thread_per_shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-24T17:51:30Z</created><updated>2011-07-24T17:52:00Z</updated><resolved>2011-07-24T17:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchOperationThreading.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file></files><comments><comment>Search API: REST endpoint should use default operation_threading of thread_per_shard, closes #1154.</comment></comments></commit></commits></item><item><title>Get API: Will always prefer first local execution, regardless of the preference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1153</link><project id="" key="" /><description>Get API: Will always prefer first local execution, regardless of the preference
</description><key id="1278101">1153</key><summary>Get API: Will always prefer first local execution, regardless of the preference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-24T17:32:51Z</created><updated>2011-07-24T17:33:59Z</updated><resolved>2011-07-24T17:33:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file></files><comments><comment>Get API: Will always prefer first local execution, regardless of the preference, closes #1153.</comment></comments></commit></commits></item><item><title>Search / Broadcast concurrency bug can result in response corruption / errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1152</link><project id="" key="" /><description>Concurrency bug when trying to aggregate results from different endpoints can cause response corruption, or other type of failures like error notices that a message can't be sent twice for the same http request, or search context missing failures.
</description><key id="1277361">1152</key><summary>Search / Broadcast concurrency bug can result in response corruption / errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.5</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-24T12:39:31Z</created><updated>2011-07-24T12:40:15Z</updated><resolved>2011-07-24T12:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/PlainShardsIterator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/ShardsIterator.java</file></files><comments><comment>Search / Broadcast concurrency bug can result in response corruption / errors, closes #1152.</comment></comments></commit></commits></item><item><title>Debug message: "failed to load sigar"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1151</link><project id="" key="" /><description>In ES 0.17.0 and 0.17.1 (running in embedded mode) I'm getting a debug message that sigar could not be loaded.
It doesn't seem to have any negative effect, but is a little annoying.

**Debug message:**

```
2011-07-23 00:16:17, DEBUG: org.elasticsearch.common.logging.slf4j.Slf4jESLogger.internalDebug.74: failed to load sigar
java.lang.ClassNotFoundException: org.hyperic.sigar.Sigar
    at java.net.URLClassLoader$1.run(URLClassLoader.java:202) ~[na:1.6.0_21]
    at java.security.AccessController.doPrivileged(Native Method) ~[na:1.6.0_21]
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190) ~[na:1.6.0_21]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:307) ~[na:1.6.0_21]
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) ~[na:1.6.0_21]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:248) ~[na:1.6.0_21]
    at org.elasticsearch.monitor.MonitorModule.configure(MonitorModule.java:75) ~[elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:59) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:210) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:91) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:142) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:103) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:58) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:147) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159) [elasticsearch-0.17.1.jar:na]
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166) [elasticsearch-0.17.1.jar:na]
    at at.erpel.registry.config.BaseSpringConfig.elasticSearchNode(BaseSpringConfig.java:264) [classes/:na]
    at at.erpel.testing.registry.config.SpringTestConfig$$EnhancerByCGLIB$$58667108.CGLIB$elasticSearchNode$12(&lt;generated&gt;) [cglib-nodep-2.2.jar:na]
    at at.erpel.testing.registry.config.SpringTestConfig$$EnhancerByCGLIB$$58667108$$FastClassByCGLIB$$2c2e3d52.invoke(&lt;generated&gt;) [cglib-nodep-2.2.jar:na]
    at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:215) [cglib-nodep-2.2.jar:na]
    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:210) [spring-context-3.0.3.RELEASE.jar:3.0.3.RELEASE]
    at at.erpel.testing.registry.config.SpringTestConfig$$EnhancerByCGLIB$$58667108.elasticSearchNode(&lt;generated&gt;) [cglib-nodep-2.2.jar:na]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_21]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_21]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_21]
    at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_21]
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:145) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:570) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:983) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:879) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:206) [spring-context-3.0.3.RELEASE.jar:3.0.3.RELEASE]
    at at.erpel.testing.registry.config.SpringTestConfig$$EnhancerByCGLIB$$58667108.elasticSearchNode(&lt;generated&gt;) [cglib-nodep-2.2.jar:na]
    at at.erpel.registry.config.BaseSpringConfig.client(BaseSpringConfig.java:240) [classes/:na]
    at at.erpel.testing.registry.config.SpringTestConfig$$EnhancerByCGLIB$$58667108.CGLIB$client$15(&lt;generated&gt;) [cglib-nodep-2.2.jar:na]
    at at.erpel.testing.registry.config.SpringTestConfig$$EnhancerByCGLIB$$58667108$$FastClassByCGLIB$$2c2e3d52.invoke(&lt;generated&gt;) [cglib-nodep-2.2.jar:na]
    at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:215) [cglib-nodep-2.2.jar:na]
    at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:210) [spring-context-3.0.3.RELEASE.jar:3.0.3.RELEASE]
    at at.erpel.testing.registry.config.SpringTestConfig$$EnhancerByCGLIB$$58667108.client(&lt;generated&gt;) [cglib-nodep-2.2.jar:na]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_21]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_21]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_21]
    at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_21]
    at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:145) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:570) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:983) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:879) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:844) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:786) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:703) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:474) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:84) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:282) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1074) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:517) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:844) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:786) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:703) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:474) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.annotation.InjectionMetadata.inject(InjectionMetadata.java:84) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:282) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1074) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:517) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:580) [spring-beans-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:895) [spring-context-3.0.3.RELEASE.jar:3.0.3.RELEASE]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:425) [spring-context-3.0.3.RELEASE.jar:3.0.3.RELEASE]
    at at.erpel.testing.registry.config.ERPELContextLoader.loadContext(ERPELContextLoader.java:112) [test-classes/:na]
    at org.springframework.test.context.TestContext.loadApplicationContext(TestContext.java:280) [spring-test-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.test.context.TestContext.getApplicationContext(TestContext.java:304) [spring-test-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.injectDependencies(DependencyInjectionTestExecutionListener.java:109) [spring-test-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.test.context.support.DependencyInjectionTestExecutionListener.prepareTestInstance(DependencyInjectionTestExecutionListener.java:75) [spring-test-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:321) [spring-test-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at org.springframework.test.context.testng.AbstractTestNGSpringContextTests.springTestContextPrepareTestInstance(AbstractTestNGSpringContextTests.java:133) [spring-test-3.0.5.RELEASE.jar:3.0.5.RELEASE]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_21]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_21]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_21]
    at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_21]
    at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:76) [testng-6.0.1.jar:na]
    at org.testng.internal.Invoker.invokeConfigurationMethod(Invoker.java:525) [testng-6.0.1.jar:na]
    at org.testng.internal.Invoker.invokeConfigurations(Invoker.java:202) [testng-6.0.1.jar:na]
    at org.testng.internal.Invoker.invokeConfigurations(Invoker.java:130) [testng-6.0.1.jar:na]
    at org.testng.internal.TestMethodWorker.invokeBeforeClassMethods(TestMethodWorker.java:173) [testng-6.0.1.jar:na]
    at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:105) [testng-6.0.1.jar:na]
    at org.testng.TestRunner.runWorkers(TestRunner.java:1147) [testng-6.0.1.jar:na]
    at org.testng.TestRunner.privateRun(TestRunner.java:749) [testng-6.0.1.jar:na]
    at org.testng.TestRunner.run(TestRunner.java:600) [testng-6.0.1.jar:na]
    at org.testng.SuiteRunner.runTest(SuiteRunner.java:317) [testng-6.0.1.jar:na]
    at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:312) [testng-6.0.1.jar:na]
    at org.testng.SuiteRunner.privateRun(SuiteRunner.java:274) [testng-6.0.1.jar:na]
    at org.testng.SuiteRunner.run(SuiteRunner.java:223) [testng-6.0.1.jar:na]
    at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52) [testng-6.0.1.jar:na]
    at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86) [testng-6.0.1.jar:na]
    at org.testng.TestNG.runSuitesSequentially(TestNG.java:1039) [testng-6.0.1.jar:na]
    at org.testng.TestNG.runSuitesLocally(TestNG.java:964) [testng-6.0.1.jar:na]
    at org.testng.TestNG.run(TestNG.java:900) [testng-6.0.1.jar:na]
    at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:73) [surefire-testng-2.6.jar:2.6]
    at org.apache.maven.surefire.testng.TestNGXmlTestSuite.execute(TestNGXmlTestSuite.java:94) [surefire-testng-2.6.jar:2.6]
    at org.apache.maven.surefire.Surefire.run(Surefire.java:169) [surefire-api-2.6.jar:2.6]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_21]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_21]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_21]
    at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_21]
    at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350) [surefire-booter-2.6.jar:2.6]
    at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021) [surefire-booter-2.6.jar:2.6]
```
</description><key id="1273392">1151</key><summary>Debug message: "failed to load sigar"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xeraa</reporter><labels /><created>2011-07-22T23:37:36Z</created><updated>2011-07-27T23:51:10Z</updated><resolved>2011-07-27T14:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-24T02:04:16Z" id="1639516">Its in debug level, whats the problem?
</comment><comment author="xeraa" created="2011-07-24T12:16:59Z" id="1640632">Right, it's in debug level, but I'd assume a `java.lang.ClassNotFoundException` shouldn't happen nevertheless?!

As Sigar is the only library directly included in _lib/_, could there be a minor issue of how the library is included when running in embedded mode on Windows (32-bit) - or why am I getting a `java.lang.ClassNotFoundException`?
</comment><comment author="kimchy" created="2011-07-24T12:36:00Z" id="1640659">sigar is under `lib/sigar` and it provides native libs to get mainly OS level stats. It is not required, but, its still handy to understand if it _is_ included, why its not working.
</comment><comment author="xeraa" created="2011-07-27T14:19:39Z" id="1664178">Thanks for the feedback, we'll simply ignore this message in the future.
</comment><comment author="kimchy" created="2011-07-27T15:12:15Z" id="1664598">Thinking about it a bit more, I guess we can reduce the logging level for this to TRACE. It still needs to be logged in case you do include sigar and for some reason it does not work (in which case, the exception will help debug it).
</comment><comment author="xeraa" created="2011-07-27T23:51:10Z" id="1668950">Thanks for https://github.com/elasticsearch/elasticsearch/commit/36e6102a1bb9912f9c0547e96797d0833023b81d!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Minor(?) scripting bug(?): (caching-related?) odd behavior when changing languages for the same script code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1150</link><project id="" key="" /><description>(Unlike the other scripting issues I've been reporting, this one is not causing my any problems at all, I just wanted to report this to get fixed during some idle moment - I think just adding "lang" to the cache key should fix it?)

I noticed this when trying to workaround the mvel bugs reported elsehwere (cough-hint-cough j/k). Here's the steps:

1] Run an mvel script that doesn't work in JS:

  "script_fields":{
    "test1":{
      "script":"toRadians(0)",
    }
  },

2] Switch the language to JS 

  "script_fields":{
    "test1":{
      "script":"toRadians(0)",
      "lang":"js"
    }
  },

This returns exactly the same response

3] Now make a trivial change to the script:

  "script_fields":{
    "test1":{
      "script":"0+toRadians(0)",
      "lang":"js"
    }
  },

This fails with the expected "EcmaError[ReferenceError: "toRadians" is not defined. (Script3.js#1)]" error

So it looks like in step [2] it's just re-submitted the original cached? mvel query

4]  if I now change lang to be mvel, ie same script:

  "script_fields":{
    "test1":{
      "script":"0+toRadians(0)",
      "lang":"mvel"
    }
  },

It continues to fail, ie the (erroring) script has been cached.
</description><key id="1271946">1150</key><summary>Minor(?) scripting bug(?): (caching-related?) odd behavior when changing languages for the same script code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-22T19:11:58Z</created><updated>2011-07-30T12:13:14Z</updated><resolved>2011-07-30T12:13:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-30T11:59:08Z" id="1687915">Yea, thats exactly what happens, a script is only cached based on its content, and not the lang as well...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/ScriptService.java</file></files><comments><comment>Minor(?) scripting bug(?): (caching-related?) odd behavior when changing languages for the same script code, closes #1150.</comment></comments></commit></commits></item><item><title>Failed to load uid from the index in match_all query with parent/child and _source disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1149</link><project id="" key="" /><description>Issue:  After indexing parent/child documents, in which the _source is disabled on the child document, a match_all query can fail with:

[2011-07-12 18:57:18,122][DEBUG][action.search.type       ] [Nital, Adri] [257386] Failed to execute fetch phase org.elasticsearch.search.fetch.FetchPhaseExecutionException: [temp] [0]: query[ConstantScore(_:_)],from[0],size[10]: FetchFailed [Failed to loaduidfrom the index]

The program that repros the problem is here:
       https://gist.github.com/1096154

This appears to be a concurrency problem with parent/child documents.

Please note that the _source for the child document must be disabled
for this problem to show up.  The repro program "puts" the mapping programatically and thus
sets this.
</description><key id="1271237">1149</key><summary>Failed to load uid from the index in match_all query with parent/child and _source disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmader</reporter><labels><label>bug</label><label>v0.17.3</label><label>v0.18.0</label></labels><created>2011-07-22T17:07:32Z</created><updated>2011-07-30T19:29:24Z</updated><resolved>2011-07-30T19:29:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/document/ResetFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/document/SingleFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/selector/AllButSourceFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/selector/FieldMappersFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/selector/UidAndSourceFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/selector/UidFieldSelector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file></files><comments><comment>Failed to load uid from the index in match_all query with parent/child and _source disabled, closes #1149.</comment></comments></commit></commits></item><item><title>Fail shard (recovery) allocation on a node when the index does not exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1148</link><project id="" key="" /><description>Fail shard (recovery) allocation on a node when the index does not exists, this can happen if the data got deleted by mistake, for example (and we should go and try and allocate it to a different node).
</description><key id="1267925">1148</key><summary>Fail shard (recovery) allocation on a node when the index does not exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-22T03:55:50Z</created><updated>2011-07-22T03:58:24Z</updated><resolved>2011-07-22T03:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayRecoveryException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/blobstore/BlobStoreIndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/none/NoneIndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Fail shard (recovery) allocation on a node when the index does not exists, closes #1148.</comment></comments></commit></commits></item><item><title>File#mkdirs gets stuck, might be concurrency issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1147</link><project id="" key="" /><description>In 0.16, only create it under a global mutex lock. In 0.17, lets try and be smarted and detect that its stuck....
</description><key id="1266250">1147</key><summary>File#mkdirs gets stuck, might be concurrency issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.5</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-21T22:00:29Z</created><updated>2011-07-21T22:01:50Z</updated><resolved>2011-07-21T22:01:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/MmapFsStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/NioFsStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/SimpleFsStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/dump/SimpleDumpGenerator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file></files><comments><comment>File#mkdirs gets stuck, might be concurrency issue, closes #1147.</comment></comments></commit></commits></item><item><title>Hash filter cache keys to reduce memory usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1146</link><project id="" key="" /><description>If a filter is large, hash the filter using SHA1 to use as a cache key, instead of the filter itself
</description><key id="1264921">1146</key><summary>Hash filter cache keys to reduce memory usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-07-21T18:20:44Z</created><updated>2013-04-05T13:59:33Z</updated><resolved>2013-04-05T13:59:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T13:59:33Z" id="15957099">Won't be supported automatically
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scripts: arrays: ".multiValued" returns true even when ".values" fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1145</link><project id="" key="" /><description>(See also https://github.com/elasticsearch/elasticsearch/issues/1144)

I now have a dataset with a multi-valued field ("locs" from the previous issue, the type happens to be a geo-point; I don't know if that's significant but I suspect not) that sometimes has a single value, eg

"fields": {
   "locs": "40.3278286374,-74.511843005"
}

and sometimes many values:

"fields": {
   "locs": [
      "40.3278286374,-74.511843005",
      "40.3278286374,-74.511843005"
   ]
}

Two related issues (the second of which looks like a definite bug)

Both of these are issues with mvel (tried js and got a separate problem, described in [3])

1] Ideally I'd like to be able to use "doc['locs'].values" in both cases, eg with "doc['locs'].values.length==1" in the first case and "==2" in the second.

In practice, if I try to access any properties of "values" in the first case (eg "n = doc['locs'].values.length"), I get an error like:

"reason": "RuntimeException[cannot invoke getter: getValues [declr.class: org.elasticsearch.index.mapper.xcontent.geo.GeoPointDocFieldData; act.class: org.elasticsearch.index.mapper.xcontent.geo.GeoPointDocFieldData](see trace)]; nested: nested: ArrayIndexOutOfBoundsException[0]; "

2] Since the above happened, I wanted to test on ".multiValued" and have 2 different clauses to handle the two cases. Unfortunately, "doc['locs'].multiValued" returns true in both cases.

(As an aside: If there's any mvel workaround which would allow me to keep the code in mvel with 0.16.2 that would be awesome, I use a bunch of trig functions and wotnot later on in the script, and I'm not particularly excited to rewrite it in JS or mess around with native scripts)

3] Perhaps I'm going mad, but with "lang":"js", then "doc['locs'].values" always failed for me (with the same ArrayIndexOutOfBoundsException[0]), ie even with 'locs' having 2 or 3 values.

(Conversely, doc['locs'].lats and doc['locs'].lons worked in JS)
</description><key id="1264822">1145</key><summary>Scripts: arrays: ".multiValued" returns true even when ".values" fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels><label>bug</label><label>v0.17.5</label><label>v0.18.0</label></labels><created>2011-07-21T18:02:07Z</created><updated>2011-08-09T15:50:15Z</updated><resolved>2011-08-09T15:50:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Alex-Ikanow" created="2011-07-22T19:48:13Z" id="1634542">Updated: edited original issue, edited title and added fact it didn't work for js either (also added then removed what I thought was a workaround but wasn't in fact)
</comment><comment author="kimchy" created="2011-07-30T12:19:10Z" id="1687957">Heya, can you gist a recreation? Something like indexing some sample data using curl, and then issuing the offending scripts? This sounds like a bug, will be simpler to fix it then. 
</comment><comment author="Alex-Ikanow" created="2011-08-01T14:01:43Z" id="1700438">The GIST is here:

https://gist.github.com/1118155

In the simple reproduction above, "mvel" and "js" behaved consistently, ie "values" always failed even for a single record with multiple values. I didn't investigate any further (ie why mvel did work in my original, more complex, reproduction) since it's almost certainly the same issue anyway

Also, with regard to [1], do you intend for "values" to return an array of length 1 for single-value entries (ie multiValued==false tells you if you can use "value", but you can always use "values")?

As noted above, it would be great if it did work this way (at least if the field ever contained multiple values, though that's presumably harder than making it always behave that way)
</comment><comment author="Alex-Ikanow" created="2011-08-01T14:04:31Z" id="1700459">(sorry closed by mistake - not a big fan of the big "comment &amp; close" button github gives you!)
</comment><comment author="kimchy" created="2011-08-09T15:49:37Z" id="1764517">This is a bug in how the cached array is constructed to support this type. Will push a fix shortly.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/MultiValueGeoPointFieldData.java</file></files><comments><comment>Scripts: arrays: ".multiValued" returns true even when ".values" fails, closes #1145.</comment></comments></commit></commits></item><item><title>Mvel: ".lats" and ".lons" fails in array of geopoints (.values[x].lat/lon works)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1144</link><project id="" key="" /><description>From http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/95d0a4a9a11a3730/c66c31c72280be7d#c66c31c72280be7d:

I have a set of documents with an array of geo-points (the mapping is enforced, and the dataset happens to guarantee all documents have 2+ values in the array ... see also the next issue I'm about to create!). 

I am trying to run some mvel scripts that generate various fields based on the lat/long values in the array. 

1] This test case works, I do 

"i = 0; d = 0; if (doc['locs'].values.length &gt; 0) { d = doc['locs'].lons[0]; } d;" 

https://gist.github.com/1092577 

2] This trivial change causes it to fail: 

"i = 0; d = 0; if (doc['locs'].values.length &gt; 0) { d = doc['locs'].lons[i]; } d;" 

https://gist.github.com/1092581 

(ie I change the index of the lons array to be "i" vs 0; It also fails  with i declared as int, long, Integer, or Long; or the array index  being (int)0, (Integer)0, (long)0, (Long)0) 

3] The following was a workaround:   

"i = 0; d = 0; if (doc['locs'].values.length &gt; 0) { d = doc['locs'].values[i].lat; } d;" 

ie it looks like there's a bug with the ".lats" and ".lons" shortcuts.
</description><key id="1264732">1144</key><summary>Mvel: ".lats" and ".lons" fails in array of geopoints (.values[x].lat/lon works)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels /><created>2011-07-21T17:49:31Z</created><updated>2011-08-10T17:12:53Z</updated><resolved>2011-08-10T17:12:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-30T12:21:37Z" id="1687965">Can you gist a full recreation for this (curl that index some sample data, and then issue the failing scripts), it will speedup nailing down the problem.
</comment><comment author="Alex-Ikanow" created="2011-08-01T12:52:10Z" id="1699990">Hey, sorry for sloppy bug reporting :)

Here's the simplest GIST I could reconstruct, let me know if this is good enough:

https://gist.github.com/1118064
</comment><comment author="kimchy" created="2011-08-09T15:44:51Z" id="1764452">I tested on master / 0.17 branch, and it seems to work. They do include a new version of mvel, which might have fixed it... . Can you give it a go? Maybe it also fixes the other issue?
</comment><comment author="Alex-Ikanow" created="2011-08-10T16:52:27Z" id="1774346">Thanks for playing around, I'm in the middle of deploying based on 0.16.x at the moment (having worked around both these bugs using .lats and .lons in JS), I'll make a note to go back and try the mvel version when i move up a version for my next release. Feel free to close this, and I'll reopen if it's still an issue then?
</comment><comment author="kimchy" created="2011-08-10T17:12:53Z" id="1774543">Sounds good. Thanks for your patience and excellent recreations.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reusing query parser in WrapperQueryParser produces invalid queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1143</link><project id="" key="" /><description>I was surprised to see that when I used the WrapperQuery, my query returned the source field instead of the fields I was asking for. Also, some queries combining wrapper queries where invalid (na) without any good explanation. I eventually found that writing this :

``` javascript
{
  "from" : 0,
  "size" : 10,
  "query" : {
    "wrapper" : {
      "query" : "ewogICAgInRlcm0iOiB7ImZpZWxkMSI6InZhbHVlMV8xIn0KfQo="
    }
  },
  "fields" : [ "field2" ]
}
```

Returns the "field2" field value :

``` javascript
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"test","_type":"type1","_id":"1","_score":0.30685282,"fields":{"field2":"value2_1"}}]}}
```

, while writing this :

``` javascript
{
  "fields" : [ "field2" ],
  "from" : 0,
  "size" : 10,
  "query" : {
    "wrapper" : {
      "query" : "ewogICAgInRlcm0iOiB7ImZpZWxkMSI6InZhbHVlMV8xIn0KfQo="
    }
  }
}
```

doesn't and returns the source object :

``` javascript
{"took":4479,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"test","_type":"type1","_id":"1","_score":0.30685282, "_source" : {"field1":"value1_1","field2":"value2_1"}}]}}
```

. I guess there is something breaking the internal state of the parser when reusing it with `SearchContext.current().queryParserService().parse(qSourceParser).query();`
</description><key id="1263206">1143</key><summary>Reusing query parser in WrapperQueryParser produces invalid queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">melix</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-21T14:13:10Z</created><updated>2011-07-21T17:21:39Z</updated><resolved>2011-07-21T17:21:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="melix" created="2011-07-21T15:03:05Z" id="1624124">It seems that creating a temporary QueryParseContext fixes the problem :

``` java
   @Override public Query parse(QueryParseContext parseContext) throws IOException, QueryParsingException {
        XContentParser parser = parseContext.parser();

        XContentParser.Token token = parser.nextToken();
        assert token == XContentParser.Token.FIELD_NAME;
        String fieldName = parser.currentName();
        assert fieldName.equals("query");
        parser.nextToken();

        byte[] querySource = parser.binaryValue();
        XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource);
        try {
            final QueryParseContext context = new QueryParseContext(parseContext.index(), parseContext.indexQueryParser);
            context.reset(qSourceParser);
            Query result = context.parseInnerQuery();
            parser.nextToken();
            return result;
        } finally {
            qSourceParser.close();
        }
    }
```

It fixes both problems initially reported : broken queries and missing fields.
</comment><comment author="kimchy" created="2011-07-21T17:20:59Z" id="1625178">Yea, thats the problem!, good catch. Will push a fix to both master and 0.17 branch.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file></files><comments><comment>Reusing query parser in WrapperQueryParser produces invalid queries, closes #1143.</comment></comments></commit></commits></item><item><title>Query DSL: Allow to associate a custom cache key with a filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1142</link><project id="" key="" /><description>Filtres, when cached, use the filter itself as the cache key. The filter itself can be quite big, memory wise, for example, when using a terms filter with many terms. Allow to associate a custom cache key (that should be unique based on the content of the filter) that will be used instead of the actual filter, thus reserving memory. For example, a user based filter of friends can be cached like this (for a user named kimchy):

```
{
    "terms" : {
        "friends" : ["first", "second", "third"],
        "_cache_key" : "kimchy_friends"
    }
}
```

Will cache the filter under `kimchy_friends`, instead of using the filter itself (which includes the whole list of friends).

The `_cache_key` can be placed on all filters, in the same level as `_cache` and `_name`.
</description><key id="1261797">1142</key><summary>Query DSL: Allow to associate a custom cache key with a filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-21T08:50:49Z</created><updated>2011-07-21T18:21:05Z</updated><resolved>2011-07-21T08:51:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-07-21T09:23:51Z" id="1622365">Hiya - is this the best way to do this?  I can see issues arising when people reuse the _cache_key incorrectly.  Wouldn't it be better just to make a SHA1 of the filter, and use that as a unique ID?
</comment><comment author="kimchy" created="2011-07-21T17:24:24Z" id="1625209">@clintongormley People will always make mistakes..., it does not mean we should not expose features because of that. The downside of using SHA1 / MD5 on the data is the cost that comes with calculating it. Not saying that we shouldn't do it, possibly even trying to guess the size vs. cost automatically, but there should be an option for people to provide the cache key.
</comment><comment author="clintongormley" created="2011-07-21T17:41:50Z" id="1625334">@kimchy My question is: what functional purpose exists for exposing this in the API?  

All people care about is whether a filter is cached or not, they shouldn't care what name you use to cache the filter internally. 

Re cost, you could just say: if length $cache_id &gt; $max, then $cache_id = sha1($cache_id), so you'll be hashing the minority of cached filters
</comment><comment author="kimchy" created="2011-07-21T18:06:29Z" id="1625519">&gt; All people care about is whether a filter is cached or not, they shouldn't care what name you use to cache the filter internally.

Now :), there are more things that will happen down the road where this makes sense. Just a simple sample is invalidating specific filters based on cache key patterns. Other as well, though, its too early to really (nothing baked) to write about...

&gt; Re cost, you could just say: if length $cache_id &gt; $max, then $cache_id = sha1($cache_id), so you'll be hashing the minority of cached filters

Agreed, thats what I meant by doing size vs. cost. As to the fact that its a minority or not, it really depends on the app..., so you want to allow them to control it. This size vs. cost thingy can actually be the default for something like terms filter (the obvious heavy one), maybe open an issue for it?
</comment><comment author="clintongormley" created="2011-07-21T18:21:05Z" id="1625638">Opened: https://github.com/elasticsearch/elasticsearch/issues/1146
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/ExistsFieldQueryExtension.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MissingFieldQueryExtension.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractWeightedFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/CacheKeyFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/AndFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NotFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NumericRangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NumericRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/OrFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/OrFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ScriptFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermsFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Query DSL: Allow to associate a custom cache key with a filter, closes #1142.</comment></comments></commit></commits></item><item><title>Java API: BoostingQueryBuilder does not build the query correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1141</link><project id="" key="" /><description>Java API: BoostingQueryBuilder does not build the query correctly
</description><key id="1261269">1141</key><summary>Java API: BoostingQueryBuilder does not build the query correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-21T06:17:38Z</created><updated>2011-07-21T06:18:27Z</updated><resolved>2011-07-21T06:18:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BaseQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file></files><comments><comment>Java API: BoostingQueryBuilder does not build the query correctly, closes #1141.</comment></comments></commit></commits></item><item><title>Query DSL: custom_filters_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1140</link><project id="" key="" /><description>A `custom_filters_score` query allows to execute a query, and if the hit matches a provided filter (ordered), use the script associated with it to compute the score. Here is an example:

```
{
    "custom_filters_score" : {
        "query" : {
            "match_all" : {}
        },
        "filters" : [
            {
                "filter" : { "range" : { "age" : {"from" : 0, "to" : 10} } },
                "script" : "_score * 3"
            },
            {
                "filter" : { "range" : { "age" : {"from" : 10, "to" : 20} } },
                "script" : "_score * 2"
            }
        ]
    }
}
```

This can considerably simplify and increase performance for parameterized based scoring since filters are easily cached for faster performance, and the script is considerably simpler.

The query allows (on the same level as `query` and `filters`) to provide `boost`, and the common scripting elements `params` and `lang`.
</description><key id="1261229">1140</key><summary>Query DSL: custom_filters_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.2</label><label>v0.18.0</label></labels><created>2011-07-21T06:04:12Z</created><updated>2011-07-21T06:04:51Z</updated><resolved>2011-07-21T06:04:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomFiltersScoreQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomScoreQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/customscore/CustomScoreSearchTests.java</file></files><comments><comment>Query DSL: custom_filters_score, closes #1140.</comment></comments></commit></commits></item><item><title>Scan Search: Take track_scores into account, if set, return also scores per doc when scanning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1139</link><project id="" key="" /><description>Scan Search: Take track_scores into account, if set, return also scores per doc when scanning
</description><key id="1258905">1139</key><summary>Scan Search: Take track_scores into account, if set, return also scores per doc when scanning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-20T20:10:16Z</created><updated>2011-07-20T20:10:49Z</updated><resolved>2011-07-20T20:10:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file></files><comments><comment>Scan Search: Take track_scores into account, if set, return also scores per doc when scanning, closes #1139.</comment></comments></commit></commits></item><item><title>Analyze API: Failure to read full message over the wire</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1138</link><project id="" key="" /><description>Analyze API: Failure to read full message over the wire
</description><key id="1258558">1138</key><summary>Analyze API: Failure to read full message over the wire</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-20T19:05:07Z</created><updated>2012-03-14T11:57:01Z</updated><resolved>2011-07-20T19:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="anhthu" created="2012-03-14T08:58:41Z" id="4494349">It seems that the problem occured again in version 0.19.0
</comment><comment author="kimchy" created="2012-03-14T11:57:01Z" id="4496693">@anhthu yea, its fixed in upcoming 0.19 branch and master with issue #1770.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java</file></files><comments><comment>Analyze API: Failure to read full message over the wire, closes #1138.</comment></comments></commit></commits></item><item><title>Snapshotting to S3 occasionally fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1137</link><project id="" key="" /><description>On 0.16.1 we occasionally get the following stack trace:

20110718-08:01:49 [elasticsearch[snapshot]-pool-11-thread-1] ERROR com.amazonaws.http.HttpClient - Unable to unmarshall error response (Premature end of file.): null
org.xml.sax.SAXParseException: Premature end of file.
        at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:239)
        at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:283)
        at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:124)
        at com.amazonaws.util.XpathUtils.documentFrom(XpathUtils.java:67)
        at com.amazonaws.services.s3.internal.S3ErrorResponseHandler.handle(S3ErrorResponseHandler.java:63)
        at com.amazonaws.services.s3.internal.S3ErrorResponseHandler.handle(S3ErrorResponseHandler.java:38)
        at com.amazonaws.http.HttpClient.handleErrorResponse(HttpClient.java:508)
        at com.amazonaws.http.HttpClient.execute(HttpClient.java:215)
        at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:396)
        at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:368)
        at org.elasticsearch.cloud.aws.blobstore.AbstractS3BlobContainer.listBlobsByPrefix(AbstractS3BlobContainer.java:105)
        at org.elasticsearch.cloud.aws.blobstore.AbstractS3BlobContainer.listBlobs(AbstractS3BlobContainer.java:122)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.doSnapshot(BlobStoreIndexShardGateway.java:173)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshot(BlobStoreIndexShardGateway.java:151)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:236)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:231)
        at org.elasticsearch.index.engine.robin.RobinEngine.snapshot(RobinEngine.java:817)
        at org.elasticsearch.index.shard.service.InternalIndexShard.snapshot(InternalIndexShard.java:434)
        at org.elasticsearch.index.gateway.IndexShardGatewayService.snapshot(IndexShardGatewayService.java:231)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$SnapshotRunnable.run(IndexShardGatewayService.java:329)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
20110718-08:01:49 [elasticsearch[snapshot]-pool-11-thread-1] WARN org.elasticsearch.index.gateway - [Skullcrusher] [ugc][1] failed to snapshot (scheduled)
org.elasticsearch.index.gateway.IndexShardGatewaySnapshotFailedException: [ugc][1] Unable to unmarshall error response (Premature end of file.): null
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshot(BlobStoreIndexShardGateway.java:161)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:236)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:231)
        at org.elasticsearch.index.engine.robin.RobinEngine.snapshot(RobinEngine.java:817)
        at org.elasticsearch.index.shard.service.InternalIndexShard.snapshot(InternalIndexShard.java:434)
        at org.elasticsearch.index.gateway.IndexShardGatewayService.snapshot(IndexShardGatewayService.java:231)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$SnapshotRunnable.run(IndexShardGatewayService.java:329)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: com.amazonaws.AmazonClientException: Unable to unmarshall error response (Premature end of file.): null
        at com.amazonaws.http.HttpClient.handleErrorResponse(HttpClient.java:514)
        at com.amazonaws.http.HttpClient.execute(HttpClient.java:215)
        at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:396)
        at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:368)
        at org.elasticsearch.cloud.aws.blobstore.AbstractS3BlobContainer.listBlobsByPrefix(AbstractS3BlobContainer.java:105)
        at org.elasticsearch.cloud.aws.blobstore.AbstractS3BlobContainer.listBlobs(AbstractS3BlobContainer.java:122)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.doSnapshot(BlobStoreIndexShardGateway.java:173)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshot(BlobStoreIndexShardGateway.java:151)
        ... 9 more
Caused by: org.xml.sax.SAXParseException: Premature end of file.
        at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:239)
        at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:283)
        at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:124)
        at com.amazonaws.util.XpathUtils.documentFrom(XpathUtils.java:67)
        at com.amazonaws.services.s3.internal.S3ErrorResponseHandler.handle(S3ErrorResponseHandler.java:63)
        at com.amazonaws.services.s3.internal.S3ErrorResponseHandler.handle(S3ErrorResponseHandler.java:38)
        at com.amazonaws.http.HttpClient.handleErrorResponse(HttpClient.java:508)
        ... 16 more 

I'm guessing ES just tries again to snapshot later and we don't see the stack for some time, but thought it worth bringing up just in case it is an issue.
</description><key id="1255248">1137</key><summary>Snapshotting to S3 occasionally fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keteracel</reporter><labels /><created>2011-07-20T09:36:39Z</created><updated>2013-04-05T14:00:08Z</updated><resolved>2013-04-05T14:00:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-20T16:34:51Z" id="1616819">This smells like a bug in the aws sdk? Which version of elasticsearch are you using?
</comment><comment author="keteracel" created="2011-07-20T16:45:48Z" id="1616898">0.16.1
</comment><comment author="kimchy" created="2011-07-20T16:54:21Z" id="1616946">I upgraded to a newer version of the aws sdk in 0.17 (you can upgrade it yourself in the 0.16 version by just replacing the jar files). For now, I would not be too worried, since yes, ES will continue and try to snapshot it.
</comment><comment author="clintongormley" created="2013-04-05T14:00:07Z" id="15957135">No further reports of this in 2 years, and S3 gw is deprecated. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException for invalid faceted query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1136</link><project id="" key="" /><description>I stuffed up while writing a simple faceted query for the top three terms and got an NPE from Elasticsearch rather than the expected "invalid query" error.

[Stack Trace](https://gist.github.com/1094125)
[Test case](https://gist.github.com/1094119)

This is on Elasticsearch 0.17.0. On ES 0.16.0 I get

``` json
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[_na_][testindex][0]: No active shards}{[_na_][testindex][1]: No active shards}{[_na_][testindex][2]: No active shards}{[_na_][testindex][3]: No active shards}{[_na_][testindex][4]: No active shards}]",
  "status" : 500
}

instead.
```
</description><key id="1253639">1136</key><summary>NullPointerException for invalid faceted query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">deverton</reporter><labels><label>enhancement</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-20T01:09:46Z</created><updated>2011-07-20T02:54:18Z</updated><resolved>2011-07-20T02:54:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-20T02:53:41Z" id="1612518">It fails because the facet is not formatted correctly, I will push a better failure message in this case. Here is how it should look like (note the `terms` type):

```
curl -XPOST http://localhost:9200/testindex/_search?pretty=true -d '{
    "query" : { "match_all" : {} },
    "facets" : {
        "tags" : {
            "terms" : {
                "field" : "tag",
                "size" : 3
            }
        }
    }
}'
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetParseElement.java</file></files><comments><comment>NullPointerException for invalid faceted query, closes #1136.</comment></comments></commit></commits></item><item><title>Java client nodes using multicast discovery connect to one another</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1135</link><project id="" key="" /><description>Really, they shouldn't... (not 0.17 specific)
</description><key id="1253540">1135</key><summary>Java client nodes using multicast discovery connect to one another</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-20T00:37:53Z</created><updated>2011-07-20T00:38:25Z</updated><resolved>2011-07-20T00:38:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file></files><comments><comment>Java client nodes using multicast discovery connect to one another, closes #1135.</comment></comments></commit></commits></item><item><title>Network: Default (back) network.tcp.connect_timeout to 30s</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1134</link><project id="" key="" /><description>It was reduced to `2s` in 0.17, but seems to cause problems, make it `30s` back.
</description><key id="1252637">1134</key><summary>Network: Default (back) network.tcp.connect_timeout to 30s</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-19T21:36:44Z</created><updated>2011-07-19T21:37:40Z</updated><resolved>2011-07-19T21:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/network/NetworkService.java</file></files><comments><comment>Network: Default (back) network.tcp.connect_timeout to 30s, closes #1134.</comment></comments></commit></commits></item><item><title>IndicesExistsRequest throws IndexMissingException </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1133</link><project id="" key="" /><description>The IndexExistsRequest is throwing an IndexMissingException instead of setting exists to false in an IndicesExistsResponse object.
</description><key id="1251887">1133</key><summary>IndicesExistsRequest throws IndexMissingException </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danpolites</reporter><labels><label>bug</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-19T20:05:52Z</created><updated>2011-07-20T16:33:28Z</updated><resolved>2011-07-19T20:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danpolites" created="2011-07-20T13:01:39Z" id="1615140">I'm also seeing that sometimes a RemoteTransportException is being thrown instead of the IndexMissingException. I had to update our code to check for IndexMissingException and the cause of RemoteTransportException. It appears that your fix should take care of any type of excepting being thrown, but I've also seen this behavior when sending a create index request when that index already exists. It will throw a RemoteTransportException or IndicesAlreadyExistsException. 
</comment><comment author="kimchy" created="2011-07-20T16:33:28Z" id="1616814">When it comes over the wire, it will have a remote exception type to indicate when ti came from... . You can catch an ElasticSearchException and call #unwrapCause()
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/exists/TransportIndicesExistsAction.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/DocumentActionsTests.java</file></files><comments><comment>IndicesExistsRequest throws IndexMissingException, closes #1133.</comment></comments></commit></commits></item><item><title>Bug fix of the maxExpansion of a prefix query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1132</link><project id="" key="" /><description>My elasticsearch was hitting some errors "TooManyClauses[maxClauseCount is set to 1024]" for some prefix search queries. So I tried to set maxExpansion to 1024 on the phrase query, but it was still happening. Setting to 1023 fixed my errors. After some debugging it appears to be a simple bug in elasticsearch, see my patch.

You'll also see an another commit which add debugging info about query errors. It helped to see the stack trace of where exactly happend the error.
</description><key id="1248967">1132</key><summary>Bug fix of the maxExpansion of a prefix query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-07-19T12:20:37Z</created><updated>2014-07-16T21:56:26Z</updated><resolved>2011-07-19T17:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-19T17:11:21Z" id="1608765">applied, changed debug to trace logs, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Realtime Get fails when using compression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1131</link><project id="" key="" /><description>After indexing a document with a compressed _source, ES returns the following error:

`{"error":"IOException[EOF in 477 byte (compressed) block: could only read 453 bytes]","status":500}`

Here is a script that reproduces the error:
https://gist.github.com/1091985
</description><key id="1248709">1131</key><summary>Realtime Get fails when using compression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monken</reporter><labels><label>bug</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-19T11:21:15Z</created><updated>2011-07-19T19:01:58Z</updated><resolved>2011-07-19T19:01:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/GetActionTests.java</file></files><comments><comment>Realtime Get fails when using compression, closes #1131.</comment></comments></commit></commits></item><item><title>mget doesn't work with aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1130</link><project id="" key="" /><description>```
# [Tue Jul 19 10:43:16 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPUT 'http://127.0.0.1:9200/foo_1/foo/1?pretty=1'  -d '
{
   "foo" : "bar"
}
'

# [Tue Jul 19 10:43:16 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo_1",
#    "_id" : "1",
#    "_type" : "foo",
#    "_version" : 1
# }

# [Tue Jul 19 10:43:18 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/_aliases?pretty=1'  -d '
{
   "actions" : [
      {
         "add" : {
            "index" : "foo_1",
            "alias" : "foo"
         }
      }
   ]
}
'

# [Tue Jul 19 10:43:18 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Tue Jul 19 10:43:20 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/foo/_mget?pretty=1'  -d '
{
   "docs" : [
      {
         "_id" : 1
      }
   ]
}
'

# [Tue Jul 19 10:43:20 2011] Response:
# {
#    "status" : 404,
#    "error" : "IndexMissingException[[foo] missing]"
# }
```
</description><key id="1248048">1130</key><summary>mget doesn't work with aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.17.1</label><label>v0.18.0</label></labels><created>2011-07-19T08:44:26Z</created><updated>2011-07-19T16:58:44Z</updated><resolved>2011-07-19T16:58:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file></files><comments><comment>mget doesn't work with aliases. closes #1130.</comment></comments></commit></commits></item><item><title>ES hanging on flush/refresh/bad index_exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1129</link><project id="" key="" /><description>Start 3 nodes and run this script.  The last statement causes the cluster to hang.

```
# [Tue Jul 19 10:37:34 2011] Protocol: http, Server: 127.0.0.1:9201
curl -XPUT 'http://127.0.0.1:9200/es_test_1/?pretty=1' 

# [Tue Jul 19 10:37:34 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Tue Jul 19 10:37:34 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XHEAD 'http://127.0.0.1:9200/foo?pretty=1' 

# [Tue Jul 19 10:37:34 2011] Protocol: http, Server: 127.0.0.1:9202
curl -XPOST 'http://127.0.0.1:9200/_refresh?pretty=1' 

# [Tue Jul 19 10:37:34 2011] Response:
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 0,
#       "total" : 10
#    }
# }

# [Tue Jul 19 10:37:34 2011] Protocol: http, Server: 127.0.0.1:9201
curl -XPOST 'http://127.0.0.1:9200/_refresh?pretty=1' 

# [Tue Jul 19 10:37:34 2011] Response:
# {
#    "ok" : true,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 10
#    }
# }

# [Tue Jul 19 10:37:34 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPOST 'http://127.0.0.1:9200/_flush?pretty=1&amp;refresh=true' 
```
</description><key id="1248029">1129</key><summary>ES hanging on flush/refresh/bad index_exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-07-19T08:39:18Z</created><updated>2011-07-30T16:36:12Z</updated><resolved>2011-07-30T16:36:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-19T16:52:16Z" id="1608589">Heya, I don't get it hanging... . Started 3 nodes, and ran the script, seems to work fine... . Its pretty fast, so the refresh / flush ends up being executed just on 1-2 shards because those are the only ones allocated, maybe thats the reason?
</comment><comment author="kimchy" created="2011-07-30T13:07:23Z" id="1688072">I think its fixed now?
</comment><comment author="clintongormley" created="2011-07-30T16:36:12Z" id="1688729">It is indeed 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Merging the terms from multiple sub-analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1128</link><project id="" key="" /><description>Multi-field is great, but searching with multiple analyzers against only one field is simpler/better.
If you have a multi-lingual index, where each document has its source language, you can analyze the text fields using a special analyzer, based on the detected language (maybe even using the [`_analyzer.path`](http://www.elasticsearch.org/guide/reference/mapping/analyzer-field.html) functionality).
But what happens when you misdetected the language somehow, either at index- or at query-time? Some aggressive stemming can have devastating effects.

In such a scenario, having the original words indexed in parallel to the stemmed one would help. Be they in the same field would even letting phrase/slop queries work properly.
The only way to get multiple terms at the same position with ElasticSearch is through the [synonym token filter](http://www.elasticsearch.org/guide/reference/index-modules/analysis/synonym-tokenfilter.html), useless for stemming.

I've been working on a way to merge the terms that multiple analyzers output.
Say you want both to use a [simple analyzer](http://www.elasticsearch.org/guide/reference/index-modules/analysis/simple-analyzer.html), and any of the specialized [language-specific analyzer](http://www.elasticsearch.org/guide/reference/index-modules/analysis/lang-analyzer.html), or anything.
My plugin can make it as simple as the following index setting:

```
index:
  analysis:
    analyzer:
      # An analyzer using both the "simple" analyzer and the sophisticated "english" analyzer, combining the resulting terms
      combo_en:
        type: combo
        sub_analyzers: [simple, english]
```

Here is a simple example of what is does:

``` bash
# What the "simple" analyzer outputs
curl -XGET 'localhost:9200/yakaz/_analyze?pretty=1&amp;analyzer=simple' -d 'An example'
{
  "tokens" : [ {
    "token" : "an",
    "start_offset" : 0,
    "end_offset" : 2,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "example",
    "start_offset" : 3,
    "end_offset" : 10,
    "type" : "word",
    "position" : 2
  } ]
}
# What the "english" analyzer outputs
curl -XGET 'localhost:9200/yakaz/_analyze?pretty=1&amp;analyzer=english' -d 'An example'
{
  "tokens" : [ {
    "token" : "exampl",
    "start_offset" : 3,
    "end_offset" : 10,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 2
  } ]
}

# Now what our combined analyzer outputs
curl -XGET 'localhost:9200/yakaz/_analyze?pretty=1&amp;analyzer=combo_en' -d 'An example'
{
  "tokens" : [ {
    "token" : "an",
    "start_offset" : 0,
    "end_offset" : 2,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "example",
    "start_offset" : 3,
    "end_offset" : 10,
    "type" : "word",
    "position" : 2
  }, {
    "token" : "exampl",
    "start_offset" : 3,
    "end_offset" : 10,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 2
  } ]
}
```

Terms are sorted by position, then by start/end offset, so that it's easier to use its output under reasonable assumptions of using a classical analyzer.

Here is the good news! You can find my implementation here: https://github.com/ofavre/elasticsearch/tree/combo-analyzer-v0.16.4 (based on released ElasticSearch version 0.16.4).

**EDIT:** It is finally available as a plugin, thanks to jprante: https://github.com/yakaz/elasticsearch-analysis-combo.
</description><key id="1242480">1128</key><summary>Merging the terms from multiple sub-analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-07-18T15:46:19Z</created><updated>2017-03-13T09:33:25Z</updated><resolved>2013-02-01T10:31:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2011-07-18T15:47:27Z" id="1596325">Consider this issue as a pre- pull-request.

The current implementation should be independent of the used sub-analyzers.
However, I used some tricks in order to clone a Reader in some optimized ways. I think part of those hacks should somehow be integrated within Lucene's core, the combo-analyzer being a contrib, and having some wrapper in ElasticSearch.
</comment><comment author="ofavre" created="2011-08-26T15:18:39Z" id="1913491">Here is the patch I proposed to the Lucene community:
https://issues.apache.org/jira/browse/LUCENE-3392

We'll see how it goes.
</comment><comment author="jprante" created="2011-11-28T16:15:49Z" id="2899445">This is the solution to a multilingual "_all" field. Can't wait for it. 
</comment><comment author="slorber" created="2012-07-03T22:59:30Z" id="6748898">This seems nice.

Is the analyzer field/path available with your combo plugin?
</comment><comment author="ofavre" created="2012-07-05T08:46:06Z" id="6773952">It is available as a standalone plugin now, see: https://github.com/yakaz/elasticsearch-analysis-combo.

@slorber: Of course! The analyzer used for a field can be controlled using the [analyzer field](http://www.elasticsearch.org/guide/reference/mapping/analyzer-field.html), then the analyzer is called, fed with some data. So any analyzer can be used with this feature.
And this technique is effectively useful when you have a language field, and want to combine a language dependent analyzer (english, spanish, french, etc.), as well as a language agnostic analyzer (simple, whitespace, etc.), just in case you misdetected the language in the first place.
</comment><comment author="slorber" created="2012-07-05T13:09:39Z" id="6778242">Hello,

Thanks, yes it's obvious it can be used for the _analyzer field since your combo is... an analyzer... Thus i guess i just need to create a combo analyzer for each langage instead of the classic "one analyzer per langage".

Btw i've had quite an appropriate result using multi fields but i think it's a pain and you noticed that.
Perhaps you can tell me how it works with multi fields? I think it's not "obvious" about how store and highlights work on a multifield.

Here's my mapping:
https://gist.github.com/3053540

The pain is:
- I need to use a boolean/text search on these 3 fields
- I need to store=yes for all of them or i can't get any highlight
- I need to add highlights for the 3 fields or i only get the highlights when it was matched by the field analyzer
- My highlight map has now 3 fields and i must select/merge the most appropriate one (exact match &gt; stemming &gt; ngrams for me)

Do you also noticed that?
When using store=yes for all subfields, are they stored as duplicates in ES?

And how does your combo analyzer solve these problems? 
- I will have only 1 field so only one store=true -&gt; nice
- But what will be the behavior of highlights?
- What if 2 analyzers are producing the same tokens, are they consumming 2 \* the token space on my index or are merged?
- How will search behave? What kind of analysis will be performed on the search text for that field before trying to find matches?

And the most important:
- Would you use that in production
- How "hacky" is your solution and is there an elegant integration with Lucene/ES planned? 
</comment><comment author="jprante" created="2012-07-05T13:42:19Z" id="6778923">Storing a field means storing the original content. This content is then available for display (hightlighting). This has not much to do with the combo analyzer.

Yes, the tokens take, if they get repeated by the combo analyzer, more space - but only for referencing, positions, frequency for scoring, and the like, not in the dictionary (the index is inverted!) so this is neglectable.

During a Lucene search, the query words are transformed into tokens for matching documents in the index by the analyzer for the field. It is always recommended to use the same analyzer for indexing and for search. Otherwise your search results are getting unpredictable. This holds also for the combo analyzer. The situation is more relaxed, as you will mostly get results if you just use a subanalyzer on the combo analyzed field.

If you like to follow up, I would recommend asking questions on the Elasticsearch mailing list, because not everybody will be able to monitor the github issue tracking system for interesting discussions. More info: https://groups.google.com/group/elasticsearch
</comment><comment author="slorber" created="2012-07-09T19:54:16Z" id="6857625">Thanks.

By chance do you know if it's possible to embed your plugin in unit tests?
</comment><comment author="jprante" created="2012-07-09T19:58:32Z" id="6857737">Plugins can be tested, sure, with testng/surefire/junit... the jar and the deps must be on the classpath.
</comment><comment author="slorber" created="2012-07-09T20:05:15Z" id="6857923">Thanks, didn't know it was so easy, i though we would have to deal with the plugin path property or something...
</comment><comment author="nickminutello" created="2014-07-16T09:19:07Z" id="49140227">So, this was closed because it is never being implemented in elastic?
Or because its solved via the plugin?
</comment><comment author="ofavre" created="2014-07-16T09:55:34Z" id="49144061">The proposed patch has never been integrated into Lucene.
The feature has been implemented as a plugin. [Get it here!](https://github.com/yakaz/elasticsearch-analysis-combo#installation)
</comment><comment author="clintongormley" created="2014-07-16T10:12:28Z" id="49145804">@nickminutello the reason we never implemented it was that we think it is a bad idea to mix analysis chains like this.
</comment><comment author="slorber" created="2014-07-16T10:55:16Z" id="49149966">@nickminutello note that we are using the plugin in production since 2012 and it works well until now
</comment><comment author="jprante" created="2014-07-16T10:59:10Z" id="49150406">The combo analyzer is also here in production since 2012 and we could not live without it. 

At least Elasticsearch uses the KeywordRepeatFilter https://github.com/elasticsearch/elasticsearch/issues/2753
which is a kind of a lightweight version of the combo analyzer, since it handles the combination of stemmed/unstemmed tokens. So the idea of combining token streams is not a bad idea.
</comment><comment author="clintongormley" created="2014-07-16T11:00:32Z" id="49150550">@jprante the existence of a feature doesn't make it a good idea: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/stemming-in-situ.html#stemming-in-situ
</comment><comment author="jprante" created="2014-07-16T11:25:38Z" id="49152610">I see the points, but there are workarounds:
- boosting fields in the index is not the only boosting that is available. There is also query term boosting/weighting, or document boosting by function score
- if tokens appear more than once in a field they can be radically filtered out by the unique filter (why only_on_same_position? phrase search is no longer reliable anyway when token streams are mixed)
- skewed IDF is also a challenge when using multiple fields instead of just one field. The effect is small for short text input and BM25 Okapi which has some tunables

So, when strategies exist to work around the effects, mixing tokens from multiple analyzers is still a good idea, especially for multi language search. Many applications here use this, with success.
</comment><comment author="apatrida" created="2017-03-12T17:06:29Z" id="285958431">@jprante what are you doing now in the 5.x versions, since that original yakaz plugin was never updated?</comment><comment author="jprante" created="2017-03-12T18:29:37Z" id="285963884">@apatrida in the meantime, I could reorganize my simple use case to a more complex token filter chain, and I dropped multiple language analysis support in favor of ICU case folding, which is not a full substitution though. 

After the `language_to` feature https://github.com/jprante/elasticsearch-langdetect/issues/49#issuecomment-274655727 , I plan to extend my `langdetect` plugin by a new query similar to `simple_query_string` which tries to detect the language in a query and set the appropriate language field before the query is executed on the cluster.

But if analyzer chaining is still the only possible method for some use cases, I maybe find time to try to implement such an über-analyzer for ES 5.x.</comment><comment author="apatrida" created="2017-03-12T23:18:56Z" id="285986032">@jprante I'm in the same now, filter chains, but do run into issues like someone mentioned on one of your projects where you might want to protect a keyword from the next link in the chain, and yet want the rest of the chain to process that token.  (really just need to add exception lists to some of the plugins would solve this, like the decompounder).  I'll hop over to your `langdetect`and see where you are headed and see if I can help out anywhere.  thanks</comment><comment author="s1monw" created="2017-03-13T00:04:34Z" id="285989256">Lucene has a `KeywordAttribute` that can be set by `KeywordMarkerFilter` and is respected by stemmers etc. for exactly that reason to prevent certain terms to be modified by the next link in the chain. maybe that is useful and already available.</comment><comment author="apatrida" created="2017-03-13T00:08:59Z" id="285989534">@s1monw but that blocks all future items in the chain from processing it, not just the next link in the chain yes?  The issue I was referring too would be better solved with an exclude list in his decompounder because the rest of the chain needs to process the token, just not the decompounder.</comment><comment author="s1monw" created="2017-03-13T00:26:32Z" id="285990648">the way token filters work is that you can chain them so you can also add one that resets keyword attributes. I think stuff like this should be addressed in a pluggable fashion otherwise you just end up with legacy issues. Also it seems not related to ES so I wonder if you wanna discuss this on the repos where that langdetect is maintained?</comment><comment author="apatrida" created="2017-03-13T09:33:25Z" id="286056432">@s1monw sure, I was writing here to get alternatives written that you might use instead of what was originally presented (sub-analyzers), then rejected, in this issue.  Google leads here, and now this topic gives some alternatives from some of those who originally backed that idea.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Phonetic filter `replace` flag is inverted, fix (changes default behavior now, to follow docs)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1127</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "name_metaphone" : {
               "replace" : false,
               "encoder" : "metaphone",
               "type" : "phonetic"
            }
         },
         "analyzer" : {
            "my_analyzer" : {
               "filter" : [
                  "standard",
                  "name_metaphone"
               ],
               "type" : "custom",
               "tokenizer" : "standard"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.1:9200/foo/_analyze?pretty=1&amp;text=robert&amp;analyzer=my_analyzer' 

# [Mon Jul 18 12:30:23 2011] Response:
# {
#    "tokens" : [
#       {
#          "end_offset" : 6,
#          "position" : 1,
#          "start_offset" : 0,
#          "type" : "&lt;ALPHANUM&gt;",
#          "token" : "RBRT"
#       }
#    ]
# }
```
</description><key id="1240819">1127</key><summary>Analysis: Phonetic filter `replace` flag is inverted, fix (changes default behavior now, to follow docs)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>bug</label><label>v0.17.0</label></labels><created>2011-07-18T10:30:40Z</created><updated>2011-07-18T18:29:37Z</updated><resolved>2011-07-18T18:29:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-18T18:29:27Z" id="1600686">The flag setting is inverted, will fix it...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticTokenFilterFactory.java</file></files><comments><comment>Analysis: Phonetic filter `replace` flag is inverted, fix (changes default behavior now, to follow docs), closes #1127.</comment></comments></commit></commits></item><item><title>Allow to configure node name using `node.name` (on top of current `name`)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1126</link><project id="" key="" /><description>Allow to configure node name using `node.name` (on top of current `name`)
</description><key id="1233476">1126</key><summary>Allow to configure node name using `node.name` (on top of current `name`)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-16T00:35:22Z</created><updated>2011-07-16T00:35:48Z</updated><resolved>2011-07-16T00:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalSettingsPerparer.java</file></files><comments><comment>Allow to configure node name using `node.name` (on top of current `name`), closes #1126.</comment></comments></commit></commits></item><item><title>Nodes APIs: All node APIs to allow to match on nodes based on addresses, names, and attributes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1125</link><project id="" key="" /><description>Address:

```
curl localhost:9200/_cluster/nodes/10.0.0.3,10.0.0.4
curl localhost:9200/_cluster/nodes/10.0.0.*
```

Names: 

```
curl localhost:9200/_cluster/nodes/node_name_goes_here
curl localhost:9200/_cluster/nodes/node_name_goes_*
```

Attributes:

In the config, add something like `node.rack: 2`, and then:

```
curl localhost:9200/_cluster/nodes/rack:2
curl localhost:9200/_cluster/nodes/ra*:2
curl localhost:9200/_cluster/nodes/ra*:2*
```
</description><key id="1233410">1125</key><summary>Nodes APIs: All node APIs to allow to match on nodes based on addresses, names, and attributes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-07-16T00:01:49Z</created><updated>2011-07-16T00:27:50Z</updated><resolved>2011-07-16T00:27:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/Actions.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/transport/DummyTransportAddress.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/transport/InetSocketTransportAddress.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/transport/LocalTransportAddress.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/transport/TransportAddress.java</file></files><comments><comment>Nodes APIs: All node APIs to allow to match on nodes based on addresses, names, and attributes, closes #1125.</comment></comments></commit></commits></item><item><title>Scroll API skipping results when search_type=query_and_fetch and more than one shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1124</link><project id="" key="" /><description>I've tried performing a scolling search, and had a few surprises.
Some results seem to be skipped or forgotten when using search_type `QUERY_AND_FETCH`, in case of a sharded index.

Here is a short recreation that reproduces this bug:
https://gist.github.com/1084738/e961096516e8386eb553d907004801a26e396979
It creates a 4 shards index, indexes 8 docs, and performs a query_and_fetch search with scrolling.
You can see that the documents with id 5 is returned by the normal query, and forgotten with the scrolled search.
(Using more that 8 documents also bugs, with a different doc id missing, I didn't test very far though)
</description><key id="1230748">1124</key><summary>Scroll API skipping results when search_type=query_and_fetch and more than one shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-07-15T15:01:38Z</created><updated>2013-04-05T14:00:43Z</updated><resolved>2013-04-05T14:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-15T23:24:45Z" id="1584001">I haven't run the recreation, but I think you don't use the scroll id provided by the previous call in your next request to scroll.
</comment><comment author="ofavre" created="2011-07-16T00:22:01Z" id="1584195">I checked in the console output, they're all equal: the first returned scroll_id, the provided scroll_id parameter, and the scroll_id returned by the other responses.
</comment><comment author="clintongormley" created="2013-04-05T14:00:43Z" id="15957162">No further reports of this. Scroll being used successfully - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>problematic queries with normal search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1123</link><project id="" key="" /><description>that happends under version 0.16.2

works fine：

curl -XDELETE localhost:9200/index2
curl -XPOST http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/0a219cd6a07b410fa73eb11b47e3d6a9 -d'{  "_tenantid": 100001}'
curl -XPOST http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/0045e39df84b47adb154644ee1b1c513 -d'{  "_tenantid": 100001}'
curl -XPOST http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/0cf2b835275a48ef8769a5eac7569756 -d'{  "_tenantid": 100001}'
curl -XGET http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/_search?q=_tenantid:100001
{"took":10,"timed_out":false,"_shards":{"total":3,"successful":3,"failed":0},"hi
ts":{"total":3,"max_score":1.0,"hits":[{"_index":"index2","_type":"bc8c7a80c2fc4
d629de3ec6b9f59ba95","_id":"0045e39df84b47adb154644ee1b1c513","_score":1.0, "_so
urce" : {  "_tenantid": 100001}},{"_index":"index2","_type":"bc8c7a80c2fc4d629de
3ec6b9f59ba95","_id":"0cf2b835275a48ef8769a5eac7569756","_score":1.0, "_source"
: {  "_tenantid": 100001}},{"_index":"index2","_type":"bc8c7a80c2fc4d629de3ec6b9
f59ba95","_id":"0a219cd6a07b410fa73eb11b47e3d6a9","_score":1.0, "_source" : {  "
_tenantid": 100001}}]}}

and here are the problematic queries：

curl -XDELETE localhost:9200/index2

curl -XPOST http://localhost:9200/index2/e3ebce3f49624eae9993031e391ae877/05a1dd8eb70a4e01908a836d14929dfe -d'{  "_tenantid": "201276"}'
curl -XPOST http://localhost:9200/index2/7544eeae89f945af9e34299f657130b0/8def98cc942b46f9bff4e50c16178ba3 -d'{  "_tenantid": "201276"}'
curl -XPOST http://localhost:9200/index2/e3ebce3f49624eae9993031e391ae877/a61f84b9c2254e4ea9bda2b0585f180e -d'{  "_tenantid": 100001}'

curl -XPOST http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/0a219cd6a07b410fa73eb11b47e3d6a9 -d'{  "_tenantid": 100001}'
curl -XPOST http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/0045e39df84b47adb154644ee1b1c513 -d'{  "_tenantid": 100001}'
curl -XPOST http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/0cf2b835275a48ef8769a5eac7569756 -d'{  "_tenantid": 100001}'

curl -XGET  http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/_search?q=_tenantid:100001
{"took":0,"timed_out":false,"_shards":{"total":3,"successful":3,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}

http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/_mapping
{"bc8c7a80c2fc4d629de3ec6b9f59ba95":{"_source":{"compress":true},"properties":{"_tenantid":{"type":"long"}}}}

note, i just index some docs with another type,but the previous query(should works fine) failed to get right hits.that's a little weird ~

if i query with:
 curl -XGET  http://localhost:9200/index2/bc8c7a80c2fc4d629de3ec6b9f59ba95/_search?q=_tenantid:*
will return all result,seems query is fine
</description><key id="1228805">1123</key><summary>problematic queries with normal search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2011-07-15T07:12:11Z</created><updated>2011-07-18T01:47:58Z</updated><resolved>2011-07-18T01:47:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-15T18:13:24Z" id="1581708">The _tenatntid is a string at the start, and then a number...
</comment><comment author="medcl" created="2011-07-17T04:31:49Z" id="1589124">i think they are not the in the same type? does that affects
</comment><comment author="kimchy" created="2011-07-17T18:45:01Z" id="1590990">Yes, it affects things. You can prefix the field name with the type name, that would work. I do need to add the ability to automatically see that its "filtered by type" already, and use that, but even then, you will have problems later if you want to facet on it (facets field types are not type aware, and probably won't be for performance reasons)...
</comment><comment author="medcl" created="2011-07-18T01:47:58Z" id="1592355">ye,i see,thinks kimchy
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index / Delete API when causing auto index creation can cause buffer data overrun</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1122</link><project id="" key="" /><description>Index / Delete API when causing auto index creation can cause buffer data overrun
</description><key id="1219431">1122</key><summary>Index / Delete API when causing auto index creation can cause buffer data overrun</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.4</label><label>v0.17.0</label></labels><created>2011-07-14T00:04:41Z</created><updated>2011-07-14T00:05:08Z</updated><resolved>2011-07-14T00:05:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file></files><comments><comment>Index / Delete API when causing auto index creation can cause buffer data overrun, closes #1122.</comment></comments></commit></commits></item><item><title>Installing local plugins was broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1121</link><project id="" key="" /><description /><key id="1218190">1121</key><summary>Installing local plugins was broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amckinley</reporter><labels /><created>2011-07-13T20:26:24Z</created><updated>2014-07-16T21:56:27Z</updated><resolved>2011-07-13T21:30:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-13T21:30:49Z" id="1566306">pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to configure encoder when highlighting with default and html (escape) options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1120</link><project id="" key="" /><description>Allow to set `encoder` within the `highlight` element, with possible values of `default` and `html`.

--- Original Request
Hi there,

seems like default encoder is used in ES https://github.com/elasticsearch/elasticsearch/blob/master/modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java#L56

This is normally not a problem, since you probably don't want any malicious html/js in your documents anyway. But imagine a text document (e.g. html tutorial in plain text) which contains some html tags, javascript...

Example:

```
There are headline tags like &lt;h1&gt; which...
```

Now getting highlights from such document is unsafe since you will get this when searching for `tags like`

```
There are headline &lt;em&gt;tags like&lt;/em&gt;&lt;h1&gt; which...
```

Any chance to make the default SimpleHTMLEncoder or at least  provide an option to make it customizable?
</description><key id="1218065">1120</key><summary>Allow to configure encoder when highlighting with default and html (escape) options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsuchal</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-07-13T20:03:29Z</created><updated>2011-10-25T18:45:15Z</updated><resolved>2011-10-25T18:45:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="locojay" created="2011-09-01T16:42:59Z" id="1970052">have a look : https://github.com/elasticsearch/elasticsearch/pull/1295
</comment><comment author="kimchy" created="2011-10-25T18:45:15Z" id="2521217">closing...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CountResponse to implement ToXContent interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1119</link><project id="" key="" /><description>Pls, I need to stream CountResponse to client from servlet.

```
ServletOutputStream os = response.getOutputStream();

CountResponse countResponse = ...

XContentBuilder builder = XContentFactory.jsonBuilder(os);
builder.startObject();
countResponse.toXContent(builder, ToXContent.EMPTY_PARAMS);
builder.endObject();
builder.close();

os.close();
```
</description><key id="1215514">1119</key><summary>CountResponse to implement ToXContent interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-07-13T13:28:00Z</created><updated>2013-07-04T12:55:16Z</updated><resolved>2013-07-04T12:55:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Native (java) process memory leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1118</link><project id="" key="" /><description>An internal memory leak when using GarbageCollectorMXBean#getLastGcInfo in the JVM. Disable using it...
</description><key id="1212130">1118</key><summary>Native (java) process memory leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.4</label><label>v0.17.0</label></labels><created>2011-07-12T22:46:03Z</created><updated>2011-07-18T12:31:56Z</updated><resolved>2011-07-12T22:46:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2011-07-18T09:26:45Z" id="1593914">Is it really fixed, or is it simply disabled?
_(What did solve it? doing the reflection accessibility trick just once?)_

Also, should issue #1075 be closed?
</comment><comment author="ThePixelDeveloper" created="2011-07-18T12:31:56Z" id="1594839">I was the starter of that ticket, I've closed it now. Thanks ofavre.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/leaks/GenericStatsLeak.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/leaks/JvmStatsLeak.java</file></files><comments><comment>Native (java) process memory leak, closes #1118.</comment><comment>Did you mean to spell that monitory.jvm.enable_last_gc or should it be monitor.jvm.enable_last_gc?</comment></comments></commit></commits></item><item><title>Overriding default mapping breaks river plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1117</link><project id="" key="" /><description>Placing a **default-mapping.json** file containing `"_source": { "enabled": false }` in config/ overrides default mapping for all indices including those of the _river type. Thus when a create river request is sent to the cluster, a NPE is thrown. More specifically:

```[WARN ] [river.routing            ] [NODE_NAME] failed to get/parse _meta for [RIVER_NAME]
          java.lang.NullPointerException
      at org.elasticsearch.river.routing.RiversRouter$1.execute(RiversRouter.java:106)...

```

This issue was discussed on the [ES users group](http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/8a2b4d5223e80582/2c61341f79e67a9b).

```
</description><key id="1210294">1117</key><summary>Overriding default mapping breaks river plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">albogdano</reporter><labels /><created>2011-07-12T17:55:15Z</created><updated>2014-07-08T12:25:33Z</updated><resolved>2014-07-08T12:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T14:13:21Z" id="15957879">No longer throws NPE
</comment><comment author="clintongormley" created="2013-04-05T14:51:32Z" id="15960009">Actually my assessment was incorrect - this does still throw an NPE - just in the logs. Reopening
</comment><comment author="clintongormley" created="2014-07-08T12:25:33Z" id="48329047">Rivers will be removed in a future version of Elasticsearch. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adds a JSONQueryBuilder which allows using a JSON query string through the Java builder API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1116</link><project id="" key="" /><description>This pull request adds support for a JSONQueryBuilder. It is useful when you want to combine the Java builder API with JSON queries (strings). Example usage :

```
BoolQueryBuilder bool = new BoolQueryBuilder();
bool.must(new JSONQueryBuilder("{\"term\": {\"field\":\"value\"}}");
bool.must(new TermQueryBuilder("field2","value2");
```

In my use case, I have a custom JSON query format where I want the user to be able to put ElasticSearch "native" json queries at some points. Hence I retrieve the user query json string and use the JSON builder to let ES build the query.
</description><key id="1208177">1116</key><summary>Adds a JSONQueryBuilder which allows using a JSON query string through the Java builder API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">melix</reporter><labels /><created>2011-07-12T12:41:31Z</created><updated>2014-07-10T06:59:34Z</updated><resolved>2011-07-13T00:39:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="melix" created="2011-07-12T15:47:34Z" id="1555241">Sorry I'm still not a GitHub ninja. I see. The second commit is about adding support for field() and type() to the analyzer requests, so that you can test your mappings.
</comment><comment author="kimchy" created="2011-07-13T00:39:43Z" id="1559147">Heya, pulled your changes and made two changes:
1. Analysis, you don't have to provide the type, you can just provide a field prefixed by the type (like `document.field_name`).
2. Renamed the json query to wrapper query, and optimized it to not do parsing twice.
</comment><comment author="melix" created="2011-07-13T06:31:28Z" id="1560460">Excellent ! Thank you !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node Stats: Add number of server open channels for transport and http</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1115</link><project id="" key="" /><description>Node Stats: Add number of server open channels for transport and http
</description><key id="1205207">1115</key><summary>Node Stats: Add number of server open channels for transport and http</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-11T23:40:01Z</created><updated>2011-07-11T23:41:33Z</updated><resolved>2011-07-11T23:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/netty/OpenChannelsHandler.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/HttpInfo.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/HttpServerTransport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/HttpStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/info/RestNodesInfoAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/Transport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/fullrestart/FullRestartStressTest.java</file></files><comments><comment>Node Stats: Add number of server open channels for transport and http, closes #1115.</comment></comments></commit></commits></item><item><title>Add timeout into CountRequestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1114</link><project id="" key="" /><description>Would it be possible to implement ${subject} before 0.17.0 release?
</description><key id="1200292">1114</key><summary>Add timeout into CountRequestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-07-11T08:27:40Z</created><updated>2013-04-05T14:13:51Z</updated><resolved>2013-04-05T14:13:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-07-11T12:10:11Z" id="1545965">And please implement toString() for CountRequestBuilder as well (this is useful for unit testing of business logic)
</comment><comment author="clintongormley" created="2013-04-05T14:13:51Z" id="15957911">This can be handled by using search_type=count in the search API
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node Info / Stats: Add max_file_descriptors (info) and open_file_descriptors (stats)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1113</link><project id="" key="" /><description>When available, add the file descriptor information to node info and stats.
</description><key id="1196483">1113</key><summary>Node Info / Stats: Add max_file_descriptors (info) and open_file_descriptors (stats)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-10T02:12:59Z</created><updated>2011-07-10T02:13:46Z</updated><resolved>2011-07-10T02:13:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/process/JmxProcessProbe.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/process/ProcessInfo.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/process/ProcessStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/process/SigarProcessProbe.java</file></files><comments><comment>Node Info / Stats: Add max_file_descriptors (info) and open_file_descriptors (stats), closes #1113.</comment></comments></commit></commits></item><item><title>RabbitMQ River: Upgrade to rabbitmq driver 2.5.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1112</link><project id="" key="" /><description>RabbitMQ River: Upgrade to rabbitmq driver 2.5.1
</description><key id="1196415">1112</key><summary>RabbitMQ River: Upgrade to rabbitmq driver 2.5.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-10T01:22:02Z</created><updated>2011-07-10T01:22:31Z</updated><resolved>2011-07-10T01:22:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>RabbitMQ River: Upgrade to rabbitmq driver 2.5.1, closes #1112.</comment></comments></commit></commits></item><item><title>Cluster re-allocation lock option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1111</link><project id="" key="" /><description>There is a performance hit when we remove a machine from the cluster due to shard 
reallocation and cold shards coming online. I currently have shard 
allocations throttled, with concurrent_recoveries: 1 and 
concurrent_streams: 1. 

Our response times in a steady state are 25-30ms with maxes at 1-2 
seconds. During shard reallocation, this increases by 10x, which isn't 
bad, but my clients have gotten spoiled. 

What I'd like to do is send a command to the cluster to tell it not to 
re-allocate any shards for some amount of time while a server is taken 
offline for maint, since I know that any re-allocations will pretty 
much be wasted work. Is this doable in 16.2 and if not would others 
consider this a viable feature? 

This would mitigate the performance hit when the machine is removed, 
but I'd still expect some hit when it is added back. 

Thanks!
</description><key id="1196013">1111</key><summary>Cluster re-allocation lock option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2011-07-09T21:42:35Z</created><updated>2011-09-24T08:22:54Z</updated><resolved>2011-09-24T08:22:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2011-09-24T08:22:54Z" id="2185445">Looks like this is addressed:
https://github.com/elasticsearch/elasticsearch/issues/1358

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RabbitMQ river does not support amqps, ssl and certificates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1110</link><project id="" key="" /><description>Currently what is supported is this:

```
"rabbitmq" : {
    "host" : "localhost", 
    "port" : 5672,
    "user" : "guest",
    "pass" : "guest",
    "vhost" : "/",
    "queue" : "elasticsearch",
    "exchange" : "elasticsearch",
    "routing_key" : "elasticsearch",
    "exchange_type" : "direct",
    "exchange_durable" : true,
    "queue_durable" : true,
    "queue_auto_delete" : false
}
```

would be nice if SSL/TLS was supported (it is in rabbitmq), something like this?

```
"rabbitmq" : {
    "host" : "localhost", 
    "port" : 5671,
    "user" : "guest",
    "pass" : "guest",
    "vhost" : "/",
    "queue" : "elasticsearch",
    "exchange" : "elasticsearch",
    "routing_key" : "elasticsearch",
    "exchange_type" : "direct",
    "exchange_durable" : true,
    "queue_durable" : true,
    "queue_auto_delete" : false,
    "ssl" : {
      "cert_chain_file" : "/some/file",
      "private_key_file" : "/some/other/file",
      "verify_peer" : true|false
    }
}
```
</description><key id="1193275">1110</key><summary>RabbitMQ river does not support amqps, ssl and certificates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnae</reporter><labels /><created>2011-07-08T23:33:28Z</created><updated>2014-07-08T12:25:09Z</updated><resolved>2014-07-08T12:25:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T12:25:09Z" id="48329013">Rivers will be removed from a future version of Elasticsearch.  Closing this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JMS River</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1109</link><project id="" key="" /><description>ElasticSearch has a river for [RabbitMQ](http://www.elasticsearch.org/guide/reference/river/rabbitmq.html). However, it would be nice to have a generic river that leverages JMS. This way JMS providers such as MQ, TIBCO EMS etc can be indexed. 

Ideally, it should support the same format as the bulk api. However, ideally an XML representation would be great. Alternatively just being able to index the JMS message directly is another option. Support for TOPICS and QUEUES will be needed. 

Be glad to help develop this solution.
</description><key id="1192128">1109</key><summary>JMS River</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">hvandenb</reporter><labels /><created>2011-07-08T19:41:29Z</created><updated>2013-06-05T10:10:22Z</updated><resolved>2013-06-05T10:10:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-08T20:43:57Z" id="1535246">Helping out would be great. I guess the logic should be similar to the rabbitmq one...
</comment><comment author="hvandenb" created="2011-07-08T22:28:52Z" id="1535822">The main logic should be the same. We would have to deal with some vendor specific libs.
</comment><comment author="kimchy" created="2011-07-09T00:08:18Z" id="1536283">Yea, that would be the tricky part, vendor specific connection creation and the like...
</comment><comment author="tblachowicz" created="2011-07-20T22:56:39Z" id="1619804">The implementation could rely on the JMS and JNDI APIs. So it could get the ConnectionFactory remotely from JNDI. Using this approach the challenge is to create initial JNDI context (set the properties, provide the specific libraries) and then it all goes along the standard JMS API.
</comment><comment author="hvandenb" created="2011-07-21T14:23:51Z" id="1623866">That's the right approach, we'll have to be able to do JNDI &amp; JMS as some JMS systems don't handle JNDI well or in some configurations it's desired to not use JNDI. For example, if an organization does not have JNDI/LDAP for the connection factories. Additionally, we would need to store security info for JNDI &amp; JMS as they might be different.
</comment><comment author="YannBrrd" created="2013-02-14T08:20:52Z" id="13537384">What is the status of this issue ? 
</comment><comment author="YannBrrd" created="2013-02-19T12:45:55Z" id="13770665">Would this help ? I don't know much about JMS...

http://docs.oracle.com/javaee/1.4/tutorial/doc/JMS5.html 
</comment><comment author="qotho" created="2013-04-29T13:50:02Z" id="17167579">I have put the code for a JMS river plugin in GitHub at https://github.com/iooab23/elasticsearch-river-jms.  I am still in the process of unit testing the river, but I have successfully submitted bulk requests using Web Logic.  As suggested above, I have used the standard JMS and JNDI APIs to keep the code vendor neutral.  I am however having a problem figuring out how to generically handle the unit testing.  Any suggestions?
</comment><comment author="qotho" created="2013-04-29T13:57:29Z" id="17167990">@hvandenb: Regarding your comments about using JMS instead of JNDI.  I initially attempted to use session.createQueue and session.createTopic to provide a non-JNDI option, but doing this uses a vendor-specific syntax to identify the queue or topic which I have had a hard time getting to work with WebLogic.  For now the plugin is using JNDI.
</comment><comment author="hvandenb" created="2013-04-30T16:20:27Z" id="17237533">Typically you use a driver url or name for these items so that the driver is loaded dynamically. I can look at the code that you have currently and I'll checkout what I can contribute.
</comment><comment author="spinscale" created="2013-05-28T07:32:59Z" id="18535244">@qotho thanks a lot for creating a river!

Do you consider it stable enough, so we can add it to the documentation on elasticsearch.org?
</comment><comment author="spinscale" created="2013-06-05T10:10:22Z" id="18966637">Added your PR from https://github.com/elasticsearch/elasticsearch.github.com/pull/418 - closing this ticket
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PatternReplaceFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1108</link><project id="" key="" /><description>This adds the Pattern Replace Filter to Elasticsearch.

It allows to easily handle string replacement during the analysis process using the power of regular expression. 

The regular expression pattern has to be specified with the setting "pattern".

The replacement expression has to be specified with the setting "replacement".

&lt;pre&gt;

index:
    analysis:
        analyzer:
            default:
                tokenizer: standard
                filter: [pattern_replace]
        filter:
            pattern_replace:
                type: pattern_replace
                pattern: (?&amp;lt;=[\d])(,)(?=[\d])

&lt;/pre&gt;


By default the replacement expression is an empty string.
</description><key id="1191693">1108</key><summary>PatternReplaceFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">belevian</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-08T18:26:07Z</created><updated>2014-06-12T16:55:55Z</updated><resolved>2011-07-08T19:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-08T19:04:55Z" id="1534650">Seems like it got bundled with #1108, so I will push both. I will also make it require parameters to be set.
</comment><comment author="kimchy" created="2011-07-08T19:28:56Z" id="1534802">Pushed, thanks!
</comment><comment author="clintongormley" created="2011-07-19T08:58:13Z" id="1605502">You mention the `replacement` setting, but don't give an example of how to use it.  Does it have to be a literal, can it contain back references?
</comment><comment author="belevian" created="2011-07-19T16:01:22Z" id="1608140">Yes the replacement string may contain back references. Its behavior is the same as in the Java Matcher class. 

For more details : http://download.oracle.com/javase/6/docs/api/java/util/regex/Matcher.html
</comment><comment author="timbunce" created="2012-12-21T15:38:59Z" id="11617182">The docs at http://www.elasticsearch.org/guide/reference/index-modules/analysis/pattern_replace-tokenfilter.html are very limited.

My current question: Is an empty replacement a reasonable way to "remove" a token, or does it have undesirable side effects?

When I analyze I see the 'removed' tokens are still present, just empty e.g. "[:6-&gt;10:word]" but I don't know if this will cause any problems.

If it will cause problems would using a Token Length Filter with min:1 be a good fix?
</comment><comment author="belevian" created="2013-01-15T08:59:03Z" id="12258342">I don't recommend you to use the Pattern Replace Filter for removing tokens, others exist just for that purpose, for example Stop and Length.

In my opinion, it should be used only when you want to apply a transformation to a tokens based on a pattern.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MappingCharFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1107</link><project id="" key="" /><description>This adds the Mapping Char Filter to Elasticsearch.

It allows to easily handle char mapping during the analysis process. 

The char mapping list has to be specified with the setting "mappings".

&lt;pre&gt;
index:
    analysis:
        analyzer:
            default:
                tokenizer: standard
                filter: [my_mapping]
        char_filter:
            my_mapping:
                type: mapping
                mappings: [ph=&gt;f, qu=&gt;q] 
&lt;/pre&gt;


Otherwise the setting "mappings_path" can specify a file where you can put the list of char mapping :

&lt;pre&gt;
ph =&gt; f
qu =&gt; k
&lt;/pre&gt;
</description><key id="1191568">1107</key><summary>MappingCharFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">belevian</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-08T18:05:11Z</created><updated>2014-06-20T23:19:46Z</updated><resolved>2011-07-08T19:28:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-08T19:28:48Z" id="1534800">Pushed, thanks!
</comment><comment author="phoet" created="2012-02-24T07:49:50Z" id="4152958">this is not in the docs, isn't it? should i put this in under "Char Filter" ?
</comment><comment author="kimchy" created="2012-02-27T14:26:20Z" id="4194303">Yea, its not doc'ed, can you add it?
</comment><comment author="phoet" created="2012-02-27T16:41:17Z" id="4197301">done https://github.com/elasticsearch/elasticsearch.github.com/pull/150
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MappingCharFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1106</link><project id="" key="" /><description>This adds the Mapping Char Filter to Elasticsearch.

It allows to easily handle char mapping during the analysis process. 

The char mapping list has to be specified with the setting "mappings".

&lt;pre&gt;
index:
    analysis:
        analyzer:
            default:
                tokenizer: standard
                filter: [my_mapping]
        char_filter:
            my_mapping:
                type: mapping
                mappings: [ph=&gt;f, qu=&gt;q] 
&lt;/pre&gt;


Otherwise the setting "mappings_path" can specify a file where you can put the list of char mapping :

&lt;pre&gt;
ph =&gt; f
qu =&gt; k
&lt;/pre&gt;
</description><key id="1190316">1106</key><summary>MappingCharFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">belevian</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-07-08T14:39:10Z</created><updated>2014-06-27T11:21:21Z</updated><resolved>2011-07-08T17:32:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-08T15:27:42Z" id="1533064">Looks good!, but it fails to apply... because of ffaa17e...
</comment><comment author="klimchuk" created="2013-01-17T04:26:19Z" id="12354002">Interesting, why I can't use space as a replacement?
For example: "X=&gt; . " I was X to be replace with space-dot-space
As a result I have dot without spaces.
</comment><comment author="imotov" created="2013-01-20T18:11:30Z" id="12474510">Spaces are removed because they are optional part of the syntax. If you need X to be replaced with space-dot-space, add the following rule:

`X=&gt;\\u0020.\\u0020`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation does not match code setting for compression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1105</link><project id="" key="" /><description>The documentation for the HTTP module (http://www.elasticsearch.org/guide/reference/modules/http.html) refers to a setting called _http.compress_. However, the code is using _http.compression_. So a minor documentation update needs to be done.

The change for compression was introduced in 
https://github.com/elasticsearch/elasticsearch/commit/8a5dd90885347de6b765d9269dc5e99bbbdb3c25
</description><key id="1190259">1105</key><summary>Documentation does not match code setting for compression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hvandenb</reporter><labels /><created>2011-07-08T14:28:11Z</created><updated>2011-07-08T19:29:26Z</updated><resolved>2011-07-08T19:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-08T19:29:26Z" id="1534807">Fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TCP: Change the default connect timeout to 2s</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1104</link><project id="" key="" /><description>The `transport.tcp.connect_timeout` set to `2s` (lower from `30s`, its really too high).
</description><key id="1187928">1104</key><summary>TCP: Change the default connect timeout to 2s</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-08T05:10:37Z</created><updated>2011-07-08T05:36:58Z</updated><resolved>2011-07-08T05:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/network/NetworkService.java</file></files><comments><comment>TCP: Change the default connect timeout to 2s, closes #1104.</comment></comments></commit></commits></item><item><title>Uneven distribution of search requests across shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1103</link><project id="" key="" /><description>Under certain conditions, and depending on the execution type path and concurrency, there might be an uneven distribution of search requests across shards and replicas in a in a 1 replica case.
</description><key id="1187536">1103</key><summary>Uneven distribution of search requests across shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-07-08T02:32:41Z</created><updated>2011-07-08T02:33:50Z</updated><resolved>2011-07-08T02:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-08T02:33:50Z" id="1529840">Fixed, wrong close commit message...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: nested filter support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1102</link><project id="" key="" /><description>Similar to `nested` query, add `nested` filter. It has the same structure as the query, with two additional flags:
- `_cache`: defaults to `false`, controlling if the filter will be cached or not.
- `_name`: Allows to set a name for it, so it will be shown in the matched filter response.
</description><key id="1186082">1102</key><summary>Query DSL: nested filter support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-07-08T00:02:30Z</created><updated>2011-07-08T00:03:19Z</updated><resolved>2011-07-08T00:03:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/nested/SimpleNestedTests.java</file></files><comments><comment>Query DSL: nested filter support, closes #1102.</comment></comments></commit></commits></item><item><title>Improve cleanup of unused shards when index is completely relocated from </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1101</link><project id="" key="" /><description>Improve cleanup of unused shards when index is completely relocated from a node. 

Current logic that cleans unused shards seems to skip shards of indices that were completely relocated from the node. It's relatively easy to reproduce with an index that has one shard and one replica (see enclosed test). I also took a shot at fixing this problem.
</description><key id="1185380">1101</key><summary>Improve cleanup of unused shards when index is completely relocated from </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-07-07T21:41:50Z</created><updated>2014-07-16T21:56:29Z</updated><resolved>2011-07-07T22:36:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-07T22:36:16Z" id="1526897">Looks good. Will pull and change things a bit (some checks are not needed) and push back.
</comment><comment author="kimchy" created="2011-07-07T22:53:11Z" id="1526982">ok, pushed a new version of it out, have a look
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file></files><comments><comment>more work on Improve cleanup of unused shards when index is completely relocated from (#1101)</comment></comments></commit></commits></item><item><title>Add missing testng dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1100</link><project id="" key="" /><description>./gradlew clean test  was returning java.lang.ClassNotFoundException without these dependencies on my machine. Not sure if they were removed intentionally or not.
</description><key id="1185195">1100</key><summary>Add missing testng dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-07-07T21:11:47Z</created><updated>2014-07-16T21:56:30Z</updated><resolved>2011-07-07T21:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-07T21:56:22Z" id="1526621">strange, I thought that it was embedded in the new testng jar..., will fix it/
</comment><comment author="imotov" created="2011-07-07T22:05:37Z" id="1526699">I thought testng 6.1.1 is using jcommander:1.13 https://github.com/cbeust/testng/blob/testng-6.1.1/build.properties#L33
</comment><comment author="kimchy" created="2011-07-07T22:11:37Z" id="1526746">Not based on the maven repo, well, annoying (as usual with this). Seems to work ok with that pesky jcommander (god knows why a testing library needs deps at all...). Its only really used for running it from the command line.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for scripts in key_field of terms_stats facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1099</link><project id="" key="" /><description>When we use terms_stats facet, we are able to group by only one key_field. It would be nice if we could also group by composite fields or use a script for the key_field.

e.g, something like
"key_field" : "doc['field1'].value + \" _ \" + doc['field2'].value"
</description><key id="1179978">1099</key><summary>Support for scripts in key_field of terms_stats facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hshankar</reporter><labels /><created>2011-07-07T04:48:40Z</created><updated>2014-03-05T17:21:46Z</updated><resolved>2014-01-22T11:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kindly" created="2012-04-05T01:19:20Z" id="4966813">This would definitely be useful so you could do pivot tables using the stats (sum average).  Currently you can do counts with the terms facet using script_field.
</comment><comment author="ashdamle" created="2012-12-04T07:23:56Z" id="10986895">Any ETA on this? :)

Thanks

Ash
</comment><comment author="jpountz" created="2014-01-22T11:11:21Z" id="33012547">I'm closing this issue as aggregations make it possible to use a script as a key.
</comment><comment author="ashdamle" created="2014-03-05T17:21:46Z" id="36768146">Thanks!

Ash

On Wed, Jan 22, 2014 at 3:11 AM, Adrien Grand notifications@github.comwrote:

&gt; I'm closing this issue as aggregations make it possible to use a script as
&gt; a key.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/1099#issuecomment-33012547
&gt; .

## 

Cheers

Ash

Ash Damle
CEO, Founder | Lumiata
ash@lumiata.com | @ashdamle | 617.283.0226

www.lumiata.com
optimizing care, elevating health
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested Objects Facets Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1098</link><project id="" key="" /><description>Support faceting on nested documents. It includes both faceting on child documents matching a specific nested query (using scopes), and faceting on all nested documents of the matching root documents.

Why is it good for? First of all, this is the only way to use facets on nested documents once they are used (possibly for other reasons). But, there is also facet specific reason why nested documents can be used, and thats the fact that facets working on different key and value field (like `term_stats`, or `histogram`) can now support cases where both are multi valued properly.

The following will use this same mapping:

```
{
    "type1" : {
        "properties" : {
            "obj1" : {
                "type" : "nested"
            }
        }
    }
}
```

And, here is a sample data:

```
{
    "obj1" : [
        {
            "name" : "blue",
            "count" : 4
        },
        {
            "name" : "green",
            "count" : 6
        }
    ]
}
```
## Nested Query Facets

Any `nested` query allows to specify a `_scope` associated with it. Any `facet` allows for a scope to be defined on it controlling the scope it will execute against. For example, the following `facet1` terms stats facet will only run on documents matching the nested query associated with `my_scope`.

```
{
    "query" : {
        "nested" : {
            "_scope" : "my_scope"
            "path" : "obj1",
            "score_mode" : "avg"
            "query" : {
                "bool" : {
                    "must" : [
                        "text" : {"obj1.name" : "blue"},
                        "range" : {"obj1.count" : {"gt" : 5}}
                    ]
                }
            }
        }
    },
    "facets" : {
        "facet1" : {
            "terms_stats" : {
                "key_field" : "obj1.name",
                "value_field" : "obj1.count",
            },
            "scope" : "my_scope"
        }
    }
}
```
## All Nested Matching Root Documents

Another option is to run the facet on all the nested documents matching the root objects that the main query will end up producing. For example:

```
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "facet1" : {
            "terms_stats" : {
                "key_field" : "name",
                "value_field" : "count",
            },
            "nested" : "obj1"
        }
    }
}
```

The `nested` element provides the path to the nested document (can be a multi level nested docs) that will be used.
</description><key id="1178960">1098</key><summary>Nested Objects Facets Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-07-07T01:20:11Z</created><updated>2013-01-30T09:57:06Z</updated><resolved>2011-07-07T01:21:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2011-07-08T01:49:56Z" id="1529366">that's exactly what i need.thx shay.
</comment><comment author="erikgraf" created="2013-01-30T09:57:06Z" id="12882188">In the above example query for "All Nested Matching Root Documents" there is an extra "," after "count" that I had to remove to make the JSON valid. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/nested/BlockJoinQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/nested/NestedChildrenCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/AbstractFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/DateHistogramFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/filter/FilterFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramScriptFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/query/QueryFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeScriptFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/statistical/StatisticalFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/statistical/StatisticalScriptFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/nested/SimpleNestedTests.java</file></files><comments><comment>Nested Objects Facets Support, closes #1098.</comment></comments></commit></commits></item><item><title>Mapping: non-string type field level boosting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1097</link><project id="" key="" /><description>This is an add-on to [Issue 920](https://github.com/elasticsearch/elasticsearch/issues/920) requesting non-string type (double, long, boolean, date, etc) field level boosting which is currently not available in ElasticSearch. 

Specifically, it should be supported for all numeric types and date type.
</description><key id="1175309">1097</key><summary>Mapping: non-string type field level boosting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jrawlings</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-07-06T17:12:49Z</created><updated>2011-07-06T19:35:40Z</updated><resolved>2011-07-06T19:35:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/boost/CustomBoostMappingTests.java</file></files><comments><comment>Mapping: non-string type field level boosting, closes #1097.</comment></comments></commit></commits></item><item><title>Minimize startup time.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1096</link><project id="" key="" /><description>We are migrating from HibernateSearch to ES. I have noticed an increase in our application startup time from ES initialization.

This has minimal impact in production but when running unit tests all day long, this can impact our development throughput.

Suggestion:

Use lazy initialization in ES in as many places as possible. This will allow unit test runs that rely on only one ES feature to run much faster.

Many thanks!
</description><key id="1174536">1096</key><summary>Minimize startup time.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2011-07-06T15:22:43Z</created><updated>2011-07-06T15:25:12Z</updated><resolved>2011-07-06T15:25:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-06T15:25:12Z" id="1512523">This is really not an issue, but a general question / suggestion. If you want to speed up tests when running embedded elasticsearch, ask how on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested Object/Docs Mapping and Searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1095</link><project id="" key="" /><description>Nested objects/documents allow to map certain sections in the document indexed as nested allowing to query them as if they are separate docs joining with the parent owning doc.

Note, this feature is experimental and might require reindexing the data if using it.

One of the problems when indexing inner objects that occur several times in a doc is that "cross object" search match will occur, for example:

```
{
    "obj1" : [
        {
            "name" : "blue",
            "count" : 4
        },
        {
            "name" : "green",
            "count" : 6
        }
    ]
}
```

Searching for `name` set to `blue` and `count` higher than `5` will match the doc, because in the first element the `name` matches `blue`, and in the second element, `count` matches "higher than `5`".
## Nested Mapping

Nested mapping allow to map certain inner objects (usually multi instance ones), for example:

```
{
    "type1" : {
        "properties" : {
            "obj1" : {
                "type" : "nested"
            }
        }
    }
}
```

The above will cause all `obj1` to be indexed as a nested doc. The mapping is similar in nature to setting `type` to `object`, except that its `nested`.

The `nested` object fields can also be automatically added to the immediate parent by setting `include_in_parent` to `true`, and also included in the root object by setting `include_in_root` to `true`.

Nested docs will also automatically use the root doc `_all` field. 
## Nested Queries

Nested queries allow to search within nested docs, resulting in the root parent doc (join), for example:

```
{
    "nested" : {
        "path" : "obj1",
        "score_mode" : "avg"
        "query" : {
            "bool" : {
                "must" : [
                    "text" : {"obj1.name" : "blue"},
                    "range" : {"obj1.count" : {"gt" : 5}}
                ]
            }
        }
    }
}
```

The query `path` points to the nested object path, and the `query` (or `filter`) includes the query that will run on the nested docs matching the direct path, and joining with the root parent docs.

The `score_mode` allows to set how inner children matching affects scoring of parent. It defaults to `avg`, but can be `total`, `max` and `none`.

Multi level nesting is automatically supported, and detected, resulting in an inner nested query to automatically match the relevant nesting level (and not root) if it exists within another nested query.
## Internal Implementation

Internally, nested objects are indexed as additional documents, but, since they can be guaranteed to be indexed within the same "block", it allows for extremely fast joining with parent docs.

Those internal nested documents are automatically masked away when doing operations against the index (like searching with a `match_all` query), and they bubble out when using the nested query.
## Left Overs

There are many things to still do to have it as a complete. The most important one is have a good story around faceting and nested docs (it will really enhance it as well, especially for key and value based facets).
</description><key id="1174351">1095</key><summary>Nested Object/Docs Mapping and Searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-07-06T14:53:18Z</created><updated>2015-07-29T10:04:30Z</updated><resolved>2011-07-06T14:54:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-07-06T15:02:31Z" id="1512321">This is awesome! 

Perhaps clarify that `root_and_nested` only differs from `object_and_nested` when there are multiple levels to the hierarchy.  

Maybe `parent_and_nested` is a better name than `object_and_nested`? Or even just have `nested` and have a flag inside the mapping like

```
 {foo: { 
    type: "nested", 
    include_in_parent: true/false, 
    include_in_root: true/false
}}
```
</comment><comment author="kimchy" created="2011-07-06T17:52:07Z" id="1513772">I like the include thingy, updated the issue and pushing support for that.
</comment><comment author="ppearcy" created="2011-07-06T20:35:59Z" id="1515079">Very cool!!! 

Curious, will this be able to support the popularity use case? This would be where each document has nested popularity values where these values can be updated independent of the parent doc? Or is it required to update the parent and nested docs at the same time? 

At first glance, I don't think so, since I don't see how you can refer to the _id of the nested doc. 

Thanks for all the awesome work!
</comment><comment author="kimchy" created="2011-07-06T22:08:37Z" id="1515877">No, it won't support that, since it only works from a single doc, and you need to reindex the document. Thats another problem that parent child mapping tries to solve a bit, though not perfectly.
</comment><comment author="jprante" created="2011-07-11T00:24:43Z" id="1543733">This is a good thing. I know the separation of regions within a document as "scope search", seen in a product of a former norwegian enterprise search vendor. But the solution provided here seems more elegant and with even less limitations.

Will nested queries with "bool should" instead of "bool must" work as expected?  
</comment><comment author="kimchy" created="2011-07-11T00:30:35Z" id="1543742">@jprante: Yes, any type of query can work "inside" a nested query.
</comment><comment author="jprante" created="2011-07-11T00:55:34Z" id="1543812">A nested query use case might be author name search in a library catalog. Imagine a book title written by two authors, "Smith, Joe" and "Jones, Peter". Searching for "joe jones" or "peter smith" will no longer return false hits. Instead, "joe smith" or "peter jones" will deliver the correct results when using a nested query on the parent of the forename/surname pair.
</comment><comment author="gdiphil" created="2013-03-21T19:21:11Z" id="15259762">Was curious if this feature is still experimental?  In what scenarios might one need to reindex data when using this feature?

Thanks!
</comment><comment author="1106944911" created="2014-09-24T03:55:18Z" id="56622055">Hi all .,why not create mapping ,could anyone help me ?
{
    "resume": {
        "properties": {
            "his_data": {
                "type": "nested",
                "properties": {
                    "com": {
                        "type": "nested",
                        "properties": [
                            {
                                "date_s": {
                                    "type": "string"
                                },
                                "position": {
                                    "type": "string",
                                    "analyzer": "ik"
                                },
                                "name": {
                                    "type": "string"
                                }
                            }
                        ]
                    },
                    "edu": {
                        "type": "nested",
                        "properties": [
                            {
                                "major": {
                                    "type": "string",
                                    "analyzer": "ik"
                                }
                            }
                        ]
                    }
                }
            }
        }
    }
}
</comment><comment author="dadoonet" created="2014-09-24T05:04:16Z" id="56625270">@1106944911 please use the mailing list. We can help you there. See http://www.elasticsearch.org/help
</comment><comment author="pushkargarg" created="2015-07-29T10:03:46Z" id="125904396">@kimchy @dadoonet 
I have an already existing MYSQL DB. My doubt is regarding indexing nested objects. How do I import multiple nested objects into the root object? For eg, I have a users table which has_many schools, how to import the users and all their schools in nested format. 
I have a Rails env.
Would really appreciate an answer.
Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/uid/UidField.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/ObjectMapperListener.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/ObjectMappers.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/internal/AnalyzerMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/multifield/MultiFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/nested/BlockJoinQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/nested/NestedDocsFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/nested/NestedQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/nested/NestedQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/nested/NonNestedDocsFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/filter/FilterFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/query/QueryFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/compound/CompoundTypesTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTempalteTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/geopoint/GeohashMappingGeoPointTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/geopoint/LatLonAndGeohashMappingGeoPointTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/geopoint/LatLonMappingGeoPointTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/nested/NestedMappingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/object/NullValueObjectMappingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/parent/ParentMappingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/typelevels/ParseDocumentTypeLevelsTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/nested/SimpleNestedTests.java</file><file>plugins/mapper/attachments/src/main/java/org/elasticsearch/index/mapper/attachment/AttachmentMapper.java</file><file>plugins/mapper/attachments/src/test/java/org/elasticsearch/index/mapper/xcontent/SimpleAttachmentMapperTests.java</file></files><comments><comment>Nested Object/Docs Mapping and Searching, closes #1095.</comment></comments></commit></commits></item><item><title>java.lang.ArrayIndexOutOfBoundsException when indexing a doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1094</link><project id="" key="" /><description>The issue was described on a mailing list here:
http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/108019d6c0091abf#

The fallowing exception occurs while creating index for a specific index type and key:

&lt;pre&gt;&lt;code&gt;
[2011-07-06 05:34:33,187][DEBUG][action.bulk              ] [Astra] [201009][4]: Failed to execute bulk item (index) [index {[201009][message][211d779c-4e6c-4f9b-b74e-5cc502854732], source[{"messageId":["211d779c-4e6c-4f9b-b74e-5cc502854732"],"sourceEndpoint":["81940f97-e84c-42f8-b215-3809d08d69d8"],"destinationEndpoint":["EA217E80-2CE4-11DA-8F53-5BDB61D1C73E"],"dateCreated":"2010-09-24T01:30:31.305Z","messageState":[20],"documentId":["FV/13025/10/KARCZ01"],"messageType":[7]}]}]
java.lang.ArrayIndexOutOfBoundsException: -48
        at org.elasticsearch.index.engine.robin.RobinEngine.dirtyLock(RobinEngine.java:948)
        at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:388)
        at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:374)
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:292)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:131)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:418)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.access$100(TransportShardReplicationOperationAction.java:233)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:331)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
&lt;/code&gt;&lt;/pre&gt;


It happens when the hashCode() of &lt;b&gt;Term class&lt;/b&gt; returns Integer.MIN_VALUE. 
Math.abs(Integer) of this value returns the same (negative value).

JavaDoc:

&gt; &lt;b&gt;int java.lang.Math.abs(int a)&lt;/b&gt;
&gt; 
&gt; Returns the absolute value of an int value. If the argument is not negative, the argument is returned. If the argument is negative, the negation of the argument is returned. 
&gt; 
&gt; &lt;b&gt;Note that if the argument is equal to the value of Integer.MIN_VALUE, the most negative representable int value, the result is that same value, which is negative.&lt;/b&gt;

So be careful using &lt;b&gt;Math.abs() &lt;/b&gt;

;-) 

Thanks for interesting.
</description><key id="1172696">1094</key><summary>java.lang.ArrayIndexOutOfBoundsException when indexing a doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scoro</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-07-06T08:53:11Z</created><updated>2011-07-06T15:57:19Z</updated><resolved>2011-07-06T15:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>java.lang.ArrayIndexOutOfBoundsException when indexing a doc, closes #1094.</comment></comments></commit></commits></item><item><title>added include_lower and include_upper properties to RangeFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1093</link><project id="" key="" /><description>Hi Shay,

based on the conversation we had on the mailing list, I field the following issue:
https://github.com/elasticsearch/elasticsearch/issues/1092) 
and provided a patch for it. 

The difficult part was to get eclipse configured correctly with gradle. I basically ended up using the eclipse plugin from gradle that generated config files and it worked like a charm! 

When I get a change I will add a section to the doc for those who want to use Eclipse with ES source code
</description><key id="1172483">1093</key><summary>added include_lower and include_upper properties to RangeFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephane-bastian</reporter><labels /><created>2011-07-06T07:53:47Z</created><updated>2014-06-12T09:03:22Z</updated><resolved>2014-03-31T09:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-06T14:59:59Z" id="1512309">Heya, looks good. The one thing is that we don't need to use the boolean checks for each doc, instead, change the from and to values to reflect it (increment by 1 for integers, use hacks for floats). You can check NumericRangeFieldDataFilter for an example.
</comment><comment author="dadoonet" created="2012-05-14T12:21:58Z" id="5688765">Heya,
I just ran into this issue also.
Any news about it ?
</comment><comment author="brusic" created="2013-04-08T22:29:24Z" id="16083179">Just like David, I have come across this issue long after the issue was created. My local workaround is simple, but it would be great to have consistent range support between queries, filters and facets.
</comment><comment author="clintongormley" created="2013-04-09T11:54:16Z" id="16108167">Yes I agree.  The `from` `to` params in range facets are the equivalent of `gte` and `lt`, while in range query/filter, they are `gte` and `lte`, which is confusing.

These should definitely be consistent
</comment><comment author="s1monw" created="2013-04-09T12:29:00Z" id="16109521">now is the time - we can break bw compat here and make it consistent. I'd do `gte` and `lte` all over the place?
</comment><comment author="clintongormley" created="2013-04-09T12:36:57Z" id="16109847">The benefit of `gt`, `gte`, `lt`, `lte` is that they are explicit. I don't like the `include_upper` and `include_lower` options at all.  I do like the concept of `from` and `to` (where `to` is `lt`, not `lte`), but the problem is that they are not explicit.

So yes, I agree with @s1monw - support `gt`,`gte`,`lt`,`lte` everywhere.  Either deprecate (or at least undocument) `from` and `to` or make them consistent ie change range query/filter to use `to` = `lt`
</comment><comment author="clintongormley" created="2014-03-31T09:42:35Z" id="39070441">This PR is no longer relevant as the `range` filter now supports the `gt*`/`lt*` operators.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RangeFacet: add include_lower and include_upper properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1092</link><project id="" key="" /><description>Currently the RangeFacet does not offer as many options than its RangeFilter couterpart. More specifically, by default the 'from' value is included and the 'to' value is excluded from facet calculations. In most cases this is the expected result.However sometimes you do not want the 'from' or the 'to' value to be included in the facet calculation.
To fix this shortcoming and better mimic the RangeFilter, it would be nice to support the following properties:
- include_lower
- include_upper
</description><key id="1172405">1092</key><summary>RangeFacet: add include_lower and include_upper properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephane-bastian</reporter><labels /><created>2011-07-06T07:34:54Z</created><updated>2013-04-05T14:17:09Z</updated><resolved>2013-04-05T14:17:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T14:17:09Z" id="15958091">Closed in favour of #1093
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo Point Type: .lat/.lon vs .latValue/.lonValue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1091</link><project id="" key="" /><description>Unlike suggested by these pages:

"Usage Scripts" in
http://www.elasticsearch.org/guide/reference/mapping/geo-point-type.html 

"Document Fields" in
http://www.elasticsearch.org/guide/reference/modules/scripting.html

an indexed geo point (with "lat_lon": true) currently exposes .lat/.lon (.lats/.lons) properties instead of .latValue/.lonValue (.latValue/.lonValues).

I mentioned this issue in the queue for elasticsearch.github.com
https://github.com/elasticsearch/elasticsearch.github.com/pull/44

but then I realized the proper fix can either be:
- change the docs
- change the ES code to make use of latValue/lonValue as it was likely the first intention. 
</description><key id="1170375">1091</key><summary>Geo Point Type: .lat/.lon vs .latValue/.lonValue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aferreira</reporter><labels /><created>2011-07-05T21:45:39Z</created><updated>2011-07-10T20:11:39Z</updated><resolved>2011-07-10T20:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-06T01:20:08Z" id="1508763">Yea, the docs are wrong, and it should be `doc[...].lat`, but, you don't have to set lat_lon in the mapping for this, is this what you saw?
</comment><comment author="kimchy" created="2011-07-06T01:22:48Z" id="1508770">Fixed for now the `doc` access...
</comment><comment author="aferreira" created="2011-07-06T02:35:23Z" id="1509020">&gt;  but, you don't have to set lat_lon in the mapping for this, is this what you saw?

I think I tried some combinations of @doc[...].value.lat@, @doc[...].value.latValue@ and skipped the obvious @doc[...].lat@ – it really works without needing lat_lon to be set.

But then there is still 

doc['field_name'].latValue
doc['field_name'].lonValue
doc['field_name'].latValues
doc['field_name'].lonValues

in Scripting (http://www.elasticsearch.org/guide/reference/modules/scripting.html) which should be 

doc['field_name'].lat
doc['field_name'].lon
doc['field_name'].lats
doc['field_name'].lons

And just for checking, even though we have   .lat / .lon for a geo_point with or without "lat_lon" enabled in the field mapping, it is expected that access is faster with "lat_lon" enabled, right? This is valid for scripts and also for the geo_\* filters/facets and the like?
</comment><comment author="kimchy" created="2011-07-06T03:30:36Z" id="1509214">Fixed scripting as well... . And no, access will not be faster with lat_lon is enabled, it will be the same speed (its not really used for that).
</comment><comment author="kimchy" created="2011-07-10T20:11:38Z" id="1542967">Closing, I think we fixed all the places....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolate a MoreLikeThis query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1090</link><project id="" key="" /><description>This is a feature request to be able to use the percolate feature for a more like this query. An example API might be:

curl -XPUT localhost:9200/_percolator/test/mlt_perc -d '{
    "mlt" : {
        "min_doc_freq":"1",
        "fields":"text"
    }
}'

The get request for the percolate on the test index would return the matching MoreLikeThis query and also the MoreLikeThis results.
</description><key id="1169036">1090</key><summary>Percolate a MoreLikeThis query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bjfish</reporter><labels /><created>2011-07-05T18:24:34Z</created><updated>2013-04-05T14:18:52Z</updated><resolved>2013-04-05T14:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T14:18:52Z" id="15958194">This wouldn't work with the way percolators are implemented, as percolators only have access to the single doc being examined.  closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow changing of highlight offset for highlighted terms in fragments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1089</link><project id="" key="" /><description /><key id="1165920">1089</key><summary>Allow changing of highlight offset for highlighted terms in fragments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">purem</reporter><labels /><created>2011-07-05T09:07:02Z</created><updated>2014-07-16T21:56:31Z</updated><resolved>2011-08-04T13:34:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-05T11:03:49Z" id="1502886">Looks interesting!, can you write a test for it as well, just to make sure future changes don't break it...
</comment><comment author="kimchy" created="2011-07-27T15:29:42Z" id="1664751">Heya, had another look, and I see that the frag list build is the same as the Simple one, except that it exposes some constants as parameters. Can you add it to the documentation of the Merge one, just so we know where this code came from in the future? Once its done, I will pull it.
</comment><comment author="purem" created="2011-07-27T15:46:10Z" id="1664892">Is this just a matter of adding / changing a comment?

On 27 July 2011 16:29, kimchy &lt;
reply@reply.github.com&gt;wrote:

&gt; Heya, had another look, and I see that the frag list build is the same as
&gt; the Simple one, except that it exposes some constants as parameters. Can you
&gt; add it to the documentation of the Merge one, just so we know where this
&gt; code came from in the future? Once its done, I will pull it.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/pull/1089#issuecomment-1664751
</comment><comment author="kimchy" created="2011-07-27T16:57:16Z" id="1665540">Yes, and rebasing against current master.
</comment><comment author="purem" created="2011-07-27T20:33:11Z" id="1667316">Wasn't sure how to deal with that last commit, never had to rebase before.
</comment><comment author="purem" created="2011-07-27T20:35:35Z" id="1667331">Also "Close Pull Request" != Comment. Apologies if that messes anything up.

Seems I rebased wrong. 137 Commits!
</comment><comment author="kimchy" created="2011-07-29T06:44:42Z" id="1679164">Yea :), lets get it in, once the pull request is clean, I will merge it in.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removed unused class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1088</link><project id="" key="" /><description>This class was backported with KStem but since ES is using Lucene 3.3 now this class should be removed.
</description><key id="1161822">1088</key><summary>Removed unused class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-07-04T12:17:06Z</created><updated>2014-07-16T21:56:31Z</updated><resolved>2011-07-06T14:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-07-05T21:46:59Z" id="1507657">Added fix for plugins description.
</comment><comment author="kimchy" created="2011-07-06T14:55:55Z" id="1512281">Pushed, thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>index: no should also disable include_in_all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1087</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "document" : {
         "properties" : {
            "title_cn" : {
               "index" : "no",
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/document?pretty=1'  -d '
{
   "title_cn" : "bar"
}
'

curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "query" : {
      "field" : {
         "_all" : "bar"
      }
   }
}
'

# [Sat Jul  2 11:37:01 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "title_cn" : "bar"
#             },
#             "_score" : 0.13561106,
#             "_index" : "test",
#             "_id" : "Zshx_UXmRVWkaHYHP3_PDw",
#             "_type" : "document"
#          }
#       ],
#       "max_score" : 0.13561106,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```
</description><key id="1154560">1087</key><summary>index: no should also disable include_in_all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.18.0</label></labels><created>2011-07-02T09:38:35Z</created><updated>2013-06-04T17:38:46Z</updated><resolved>2011-08-19T16:51:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file></files><comments><comment>index: no should also disable include_in_all, closes #1087.</comment></comments></commit></commits></item><item><title>mvel's pow() truncates decimals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1086</link><project id="" key="" /><description>I'm trying to score documents by a blend of text match and distance from a lat, lon by adding the reciprocal of the distance from the origin point to the score. Unfortunately, `pow()` discards all decimal places, which leads to anything within 1km going to Infinity since `pow(0, -1)` -&gt; Infinity.

I added these `script_fields` to debug this issue:

```
"script_fields": {
    "_distance_recip": {"script": "pow(doc['geometry.coordinates'].distanceInKm(lat, lon), -1)"},
    "params": {"lat": 37.803460000000001, "lon": -122.44002},
    "_distance": {"script": "doc['geometry.coordinates'].distanceInKm(lat, lon)",
                  "params": {"lat": 37.803460000000001, "lon": -122.44002},}}
```

And I got these results back:

```
"_distance_recip": "Infinity", 
"_distance": 0.68799850202738555

"_distance_recip": "Infinity", 
"_distance": 0.52009691901279231

"_distance_recip": 1.0, 
"_distance": 1.90754297084453

"_distance_recip": 1.0, 
"_distance": 1.275160615266868

"_distance_recip": 0.5, 
"_distance": 2.5103734638225164

"_distance_recip": 0.5, 
"_distance": 2.4968355466598258
```

All these values are consistent with truncating the float to an int, which is not what I would expect at all.
</description><key id="1153132">1086</key><summary>mvel's pow() truncates decimals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ieure</reporter><labels /><created>2011-07-01T21:28:21Z</created><updated>2013-04-05T14:20:44Z</updated><resolved>2013-04-05T14:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ieure" created="2011-07-01T21:39:11Z" id="1487345">Adding, this behavior appears to be triggered by the type of the exponent passed to `pow()`. If I call `pow(0.68799850202738555, -1.0)`, I get the correct value of 1.453491536759473.

However, I continue to believe that this behavior is strange and unexpected.
</comment><comment author="clintongormley" created="2013-04-05T14:20:44Z" id="15958287">Yeah, mvel is type aware (and sometimes not).  The long term plan is to move to using Javascript as the default scripting language, once Nashorn is available.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE with mget</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1085</link><project id="" key="" /><description>Start 3 nodes, run the script below, and you will get an NPE with this error in the logs:

```
Message not fully read (response) for [133] handler org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$2@1405ac5f, error [false], resetting
```

Script:

```
curl -XPUT 'http://127.0.0.1:9200/es_test_1/?pretty=1'  -d '
{
   "mappings" : {
      "type_1" : {
         "properties" : {
            "num" : {
               "store" : "yes",
               "type" : "integer"
            },
            "date" : {
               "format" : "yyyy-MM-dd HH:mm:ss",
               "type" : "date"
            },
            "text" : {
               "type" : "string"
            }
         }
      },
      "type_2" : {
         "properties" : {
            "num" : {
               "store" : "yes",
               "type" : "integer"
            },
            "date" : {
               "format" : "yyyy-MM-dd HH:mm:ss",
               "type" : "date"
            },
            "text" : {
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XPUT 'http://127.0.0.1:9200/es_test_2/?pretty=1'  -d '
{
   "mappings" : {
      "type_1" : {
         "properties" : {
            "num" : {
               "store" : "yes",
               "type" : "integer"
            },
            "date" : {
               "format" : "yyyy-MM-dd HH:mm:ss",
               "type" : "date"
            },
            "text" : {
               "type" : "string"
            }
         }
      },
      "type_2" : {
         "properties" : {
            "num" : {
               "store" : "yes",
               "type" : "integer"
            },
            "date" : {
               "format" : "yyyy-MM-dd HH:mm:ss",
               "type" : "date"
            },
            "text" : {
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.1:9200/es_test_1/type_1/_mget?pretty=1'  -d '
{
   "docs" : [
      {
         "_id" : 1
      }
   ]
}
'
```
</description><key id="1152278">1085</key><summary>NPE with mget</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2011-07-01T19:19:23Z</created><updated>2011-07-02T12:39:50Z</updated><resolved>2011-07-02T12:39:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetShardResponse.java</file></files><comments><comment>NPE with mget, closes #1085.</comment></comments></commit></commits></item><item><title>mget doesn't return docs with duplicate IDs if type not specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1084</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/test/foo/1?pretty=1'  -d '
{
   "text" : "foo"
}
'
curl -XPUT 'http://127.0.0.1:9200/test/bar/1?pretty=1'  -d '
{
   "text" : "foo"
}
'

curl -XGET 'http://127.0.0.1:9200/test/_mget?pretty=1'  -d '
{
   "docs" : [
      {
         "_id" : 1
      }
   ]
}
'

# [Fri Jul  1 19:47:43 2011] Response:
# {
#    "docs" : [
#       {
#          "_source" : {
#             "text" : "foo"
#          },
#          "_index" : "test",
#          "_id" : "1",
#          "_type" : "foo",
#          "exists" : true,
#          "_version" : 1
#       }
#    ]
# }
```
</description><key id="1151796">1084</key><summary>mget doesn't return docs with duplicate IDs if type not specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-07-01T17:49:48Z</created><updated>2013-04-05T14:21:12Z</updated><resolved>2013-04-05T14:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T14:21:12Z" id="15958309">Expected behaviour. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 3.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1083</link><project id="" key="" /><description>Upgrade to Lucene 3.3
</description><key id="1150946">1083</key><summary>Upgrade to Lucene 3.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-07-01T15:29:43Z</created><updated>2011-07-01T15:31:01Z</updated><resolved>2011-07-01T15:31:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData1.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData2.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData3.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData4.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData5.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData6.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData7.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData8.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemFilter.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemmer.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/ShardFieldDocSortedHitQueue.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/FieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/ByteFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/DoubleFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/FloatFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/IntFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/LongFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/ShortFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/strings/StringFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/controller/ShardFieldDoc.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file></files><comments><comment>Upgrade to Lucene 3.3. closes #1083.</comment></comments></commit></commits></item><item><title>wildcard query failed,when indexing uppercased value </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1082</link><project id="" key="" /><description>curl localhost:9200/a/b -d'{short:"NP"}'
curl localhost:9200/a/b -d'{short:"NPL"}'
curl localhost:9200/a/b -d'{short:"NPLP"}'
curl localhost:9200/a/b -d'{short:"NPNP"}'
curl localhost:9200/a/b -d'{short:"LNP"}'
curl localhost:9200/a/b -d'{short:"NLNP"}'
curl localhost:9200/a/b -d'{short:"NNP"}'
curl localhost:9200/a/b -d'{short:"NPP"}'

curl localhost:9200/a/b -d'{short:"np"}'
curl localhost:9200/a/b -d'{short:"nppp"}'

curl localhost:9200/a/b/_search?q=short:NP*&amp;pretty=true

{"took":0,"timed_out":false,"_shards":{"total":3,"successful":3,"failed":0},"hit
s":{"total":4,"max_score":1.0,"hits":[{"_index":"a","_type":"b","_id":"-yA-bArNQ
X-lL5uJfIKm8w","_score":1.0, "_source" : {short:"nppp"}},{"_index":"a","_type":"
b","_id":"IUA2G1cmRvmtrlDpI56KOQ","_score":1.0, "_source" : {short:"nppp"}},{"_i
ndex":"a","_type":"b","_id":"EJhRGOq3TdGsTXgr68O9JQ","_score":1.0, "_source" : {
short:"np"}},{"_index":"a","_type":"b","_id":"q56A76_GROO2j_YCsTL1NA","_score":1
.0, "_source" : {short:"np"}}]}}'pretty' is not recognized as an internal or ext
ernal command,
operable program or batch file.

the result contains lowcased value,but uppercased can't b found,and i set the default analyzer to keyword:
index.analysis.analyzer.default.type : "keyword" 

curl localhost:9200/a/b/_search?q=short:np*

{"took":0,"timed_out":false,"_shards":{"total":3,"successful":3,"failed":0},"hit
s":{"total":4,"max_score":1.0,"hits":[{"_index":"a","_type":"b","_id":"-yA-bArNQ
X-lL5uJfIKm8w","_score":1.0, "_source" : {short:"nppp"}},{"_index":"a","_type":"
b","_id":"IUA2G1cmRvmtrlDpI56KOQ","_score":1.0, "_source" : {short:"nppp"}},{"_i
ndex":"a","_type":"b","_id":"EJhRGOq3TdGsTXgr68O9JQ","_score":1.0, "_source" : {
short:"np"}},{"_index":"a","_type":"b","_id":"q56A76_GROO2j_YCsTL1NA","_score":1
.0, "_source" : {short:"np"}}]}}

lowercase works fine,but not what i wanted.
</description><key id="1147863">1082</key><summary>wildcard query failed,when indexing uppercased value </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2011-07-01T01:04:28Z</created><updated>2011-07-28T10:56:40Z</updated><resolved>2011-07-28T10:56:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-01T09:15:12Z" id="1481660">By default, the query_string query (which gets automatically created when using `?q=` will lowercase wildcard and prefix queries, thats why it works. You can disable this flag if you explicitly use the `query_string` query: http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query.html.
</comment><comment author="medcl" created="2011-07-02T12:22:02Z" id="1489205">if i dont' wanna use queryDSL,and just wanna use the get style,what should i do?
i just tried to add the parameter:lowercase_expanded_terms,but didn't work,
here is what i try：curl localhost:9200/a/b/_search?q=short:NP*&amp;lowercase_expanded_terms=false
am i missed something?
</comment><comment author="medcl" created="2011-07-28T10:56:40Z" id="1671661">i use this config to solve my problem.

index:
  analysis:
    analyzer:  
      default:
          type: custom
          filter: [lowercase]
          tokenizer: keyword      
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Without pre-creating index, heavy write rates OOM with too many threads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1081</link><project id="" key="" /><description>I observed this with logstash.

Recreate:
- start with a fresh elasticsearch (no data dir)
- start indexing fast (1000s per second)
- a few seconds later, OOM due to thread creation explosion

This only seems to occur when heavy write rates hit an index that isn't created yet, causing ES to create the index and that seems to stall out writes and threads balloon waiting for that to happen.
</description><key id="1145577">1081</key><summary>Without pre-creating index, heavy write rates OOM with too many threads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels><label>bug</label><label>v0.17.0</label></labels><created>2011-06-30T18:20:53Z</created><updated>2011-06-30T18:22:37Z</updated><resolved>2011-06-30T18:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/exists/TransportIndicesExistsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/TransportGatewaySnapshotAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/settings/TransportUpdateSettingsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file></files><comments><comment>Without pre-creating index, heavy write rates OOM with too many threads, closes #1081.</comment></comments></commit></commits></item><item><title>text_phrase query not behaving as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1080</link><project id="" key="" /><description>According to the docs: "A phrase query maintains order of the terms up to a configurable slop (which defaults to 0)."

However, this isn't true: See the second last query in https://gist.github.com/ad4ea0fde12e2cfdfcae which has `slop: 5` - it succeeds even though the order of the words is different
</description><key id="1144232">1080</key><summary>text_phrase query not behaving as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-06-30T14:09:25Z</created><updated>2013-04-05T14:21:43Z</updated><resolved>2013-04-05T14:21:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T14:21:43Z" id="15958334">Expected behaviour - slop acts as an edit distance
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Zen Discovery: Add `minimum_master_nodes` setting helping with split brains</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1079</link><project id="" key="" /><description>A master node is a node that can become master, not an elected master node in a specific cluster. By default, a node can be elected to become master in a cluster unless `node.master` is set to `false`.

Add `zen.discovery.minimum_master_nodes` setting, controlling how many total master nodes (a node can be explicitly set to be, or not to be, a master node) should be in the cluster a specific node "sees". Below this number, the relevant node will get back into trying to form a cluster with enough master nodes. A cluster will not be formed if a node does not see `minimum_master_node` or above.

For example, in a 5 node cluster, one can set the `minimum_master_nodes` to 3, meaning that if a network disconnection happens of a node (or 2), then they will not form their own cluster, but instead, get back into trying to join the bigger cluster.

This can also help in a cluster where there are specific master nodes (lets say 3), and the rest are just data nodes (non master). Setting then the `minimum_master_nodes` to 2 can help with the master nodes not forming their own cluster, and the data nodes making sure they are connected to at least 2 master nodes.
</description><key id="1144188">1079</key><summary>Zen Discovery: Add `minimum_master_nodes` setting helping with split brains</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-30T14:01:49Z</created><updated>2011-07-11T04:47:19Z</updated><resolved>2011-06-30T14:02:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2011-07-11T01:56:07Z" id="1543945">shay,does these "maste node"  synchronized metadata automaticly?
</comment><comment author="kimchy" created="2011-07-11T04:47:19Z" id="1544338">@medcl: yes, its the same way that it works in previous versions. In local gateway scenario, master nodes also store the cluster metadata (indices created, mappings, ...)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/elect/ElectMasterService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/GatewayService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/cluster/MinimumMasterNodesTests.java</file></files><comments><comment>Zen Discovery: Add `minimum_master_nodes` setting helping with split brains, closes #1079.</comment></comments></commit></commits></item><item><title>[Feature Request] Regexp for termsstats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1078</link><project id="" key="" /><description>As discussed today on IRC:
We love Elasticsearch,
We love regexp on terms, 
We love termsstats, 
We miss regexp on termsstats.
</description><key id="1143246">1078</key><summary>[Feature Request] Regexp for termsstats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anhtuangai</reporter><labels /><created>2011-06-30T10:13:04Z</created><updated>2014-02-21T15:06:27Z</updated><resolved>2014-02-21T15:06:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-21T15:06:27Z" id="35738028">Hey,

closing this, as you can do it with aggregations, finally :-)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Feature Request] Add a river to ElasticSearch instance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1077</link><project id="" key="" /><description>As discussed in the mailing list : http://elasticsearch-users.115913.n3.nabble.com/How-to-reindex-an-ES-index-tp3089964p3089964.html

It would be nice to be able to reindex data from an ES instance using the `_source` field of previously stored documents.

With it, we could :
- Modify the mapping and ask for reindexing (even in the same cluster) documents stored in oldindex to a newindex index. The new mapping will be defined in newindex.
- Migrate easily from an ES version to another if needed
- Do many cool things that I can't imagine right now ;-)

Thanks
</description><key id="1142917">1077</key><summary>[Feature Request] Add a river to ElasticSearch instance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-06-30T08:41:08Z</created><updated>2016-03-07T22:22:06Z</updated><resolved>2013-04-05T14:23:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="otisg" created="2011-07-01T00:36:07Z" id="1480005">+1
</comment><comment author="apinkin" created="2011-07-09T03:47:01Z" id="1537143">+1
</comment><comment author="Vineeth-Mohan" created="2011-08-13T15:49:43Z" id="1797776">Similar feature request - https://github.com/elasticsearch/elasticsearch/issues/1242
</comment><comment author="gsf" created="2011-09-23T06:00:40Z" id="2175598">+1
</comment><comment author="neogenix" created="2011-10-05T03:00:57Z" id="2293996">+1
</comment><comment author="dadoonet" created="2011-10-05T06:39:24Z" id="2295081">Hi Shay,

What would you prefer for this plugin ? Would like it to be part of elasticsearch project or outside ?
Thanks
</comment><comment author="dadoonet" created="2011-10-05T06:40:02Z" id="2295089">Sorry closed by error...
</comment><comment author="Vineeth-Mohan" created="2011-10-05T06:43:18Z" id="2295105">As talked in the one of the related thread ,

There should be some version system for an entire index which i believe its not implemented in ES.

Reference - https://github.com/elasticsearch/elasticsearch/issues/1242
</comment><comment author="Yegoroff" created="2011-10-18T20:50:59Z" id="2447600">+1
</comment><comment author="HugoMag" created="2011-10-21T10:55:58Z" id="2479937">+1
</comment><comment author="mikegrassotti" created="2011-10-25T14:59:43Z" id="2518206">+1
</comment><comment author="outoftime" created="2011-10-26T21:04:21Z" id="2536430">I've got a need for this as well -- I think to keep things simple I'm going to try to implement a plugin that provides API endpoints for the following:
- Copy one document from one index to another
- Copy the entire contents from one index to another

Presumably the latter call, by default, should not be blocking.

As far as re-mapping, etc., I think it's reasonable enough to still deal with that separately in the client. The main motivation I've got for wanting a full-index-copy is to get rid of the HTTP overhead of transmitting documents to/from a client just to move them from one index to another.
</comment><comment author="Vineeth-Mohan" created="2011-10-27T03:53:06Z" id="2539426">The issue to this solution as talked by Shay in a thread is that - if one of the endpoint goes down , and it want to resume from its last stopped point , how will it do that ?
Suggested solution was to have a index versioning and we can request changes since that version number. For this versioning have to be implemented at ES side.
</comment><comment author="dadoonet" created="2011-10-27T10:18:04Z" id="2541812">100% agree with Vineeth-Mohan. So I have to wait to issue #1242 before writing the river.
That's make sense and that's why I only start to create a java batch to pull and push a full index content to another one.

I will be able to move it to a river when the "change like" API will be in ES.
</comment><comment author="outoftime" created="2011-10-27T11:36:49Z" id="2542375">Ah -- having something basic working would be very useful for me even if it didn't have the capability to resume after a node failure, so I'll probably pursue that as a plugin.
</comment><comment author="drawks" created="2011-12-27T19:42:25Z" id="3284527">+1 being able to apply new mappings without having to write a client and implement "schema versioning" in one of the various language api interfaces would be a big win operationally. Doing things like changing the type of a field on an index already on disk should be a single call affair that is handled without having to fetch the _source for every document and then sending it back to an index call.
</comment><comment author="TeRq" created="2012-03-30T14:19:06Z" id="4843322">+1
</comment><comment author="emilis" created="2012-04-04T16:56:42Z" id="4958006">+1
</comment><comment author="llonchj" created="2012-04-12T02:00:44Z" id="5083184">+1
</comment><comment author="benmccann" created="2012-04-15T17:48:31Z" id="5140952">+1
</comment><comment author="darklow" created="2012-04-24T06:33:06Z" id="5298347">+1
</comment><comment author="piskvorky" created="2012-04-27T18:11:10Z" id="5387111">+1
</comment><comment author="niuage" created="2012-04-30T22:35:55Z" id="5429551">+1
</comment><comment author="radostyle" created="2012-05-31T22:21:42Z" id="6049164">+1
</comment><comment author="occ" created="2012-07-05T15:46:19Z" id="6782290">+1
</comment><comment author="atkinson" created="2012-07-19T01:19:36Z" id="7087412">+1 

I'd like to push changes out via sockjs, so an evented API, or even a RESTful webhook would be awesome
</comment><comment author="davekhor" created="2012-09-28T17:06:20Z" id="8984061">+1

My need is simpler - I was hoping to use ES-to-ES river as a 'tee' (for those who still remember Unix :))
</comment><comment author="anatolyg" created="2012-09-28T17:20:04Z" id="8984465">+1

I'd like to use this as a feeder for a backup elastic search instance 
</comment><comment author="JohnnyMarnell" created="2012-10-05T03:22:02Z" id="9164394">+1

This would be really useful for tweaking mappings and queries, especially in early stages of development
</comment><comment author="jantoniucci" created="2012-10-29T17:00:16Z" id="9876469">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support Hierarchical Facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1076</link><project id="" key="" /><description>This seems to be a popular request.  This feature would allow hierarchical facets on fields that are not necessarily numeric, thus allowing faceting that looks like this, for example:

Media Type Facets:
- twitter (20)
  - source: foo (12)
  - source: bar (8)
- facebook (14)
  - source: foo (10)
  - source: bar (4)
</description><key id="1140552">1076</key><summary>Support Hierarchical Facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">otisg</reporter><labels /><created>2011-06-29T20:56:13Z</created><updated>2013-12-27T14:12:43Z</updated><resolved>2013-12-27T14:12:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="otisg" created="2011-07-03T03:27:15Z" id="1491281">Maybe this newly committed Lucene faceting module will provide the Hierarchical Faceting, so ES doesn't have to reinvent its own variant?
https://issues.apache.org/jira/browse/LUCENE-3079

See also: https://issues.apache.org/jira/browse/SOLR-2412
</comment><comment author="richardsyeo" created="2011-07-20T20:19:27Z" id="1618554">+1
</comment><comment author="phobos182" created="2011-07-25T13:06:03Z" id="1645562">+1
</comment><comment author="mobilemike" created="2011-08-12T23:17:39Z" id="1795489">+1
</comment><comment author="juneym" created="2011-08-17T12:55:11Z" id="1826035">+1 on this. I really need this feature for my project. 
</comment><comment author="mrgautamsam" created="2011-09-18T07:48:49Z" id="2125757">+1 Looking forward to this feature!!
</comment><comment author="eulerfx" created="2011-10-20T00:17:21Z" id="2463132">+1
</comment><comment author="acerb" created="2011-11-03T13:55:25Z" id="2617197">+1
</comment><comment author="tfreitas" created="2011-11-04T15:37:58Z" id="2632359">+1
</comment><comment author="leogamas" created="2011-12-29T18:38:43Z" id="3303635">+1
</comment><comment author="nhuray" created="2011-12-29T21:03:30Z" id="3305145">+1
</comment><comment author="Vineeth-Mohan" created="2011-12-30T14:55:18Z" id="3313731">+1
</comment><comment author="tfreitas" created="2011-12-30T15:22:15Z" id="3313951">+1
</comment><comment author="gjb83" created="2012-01-04T19:15:50Z" id="3359457">+1
</comment><comment author="guiferrpereira" created="2012-01-05T18:54:10Z" id="3374395">+1
</comment><comment author="ahfeel" created="2012-01-16T14:05:30Z" id="3511042">+1
</comment><comment author="tfreitas" created="2012-02-07T18:45:07Z" id="3853614">+1
</comment><comment author="AnAppAMonth" created="2012-04-10T02:56:46Z" id="5038303">+1
</comment><comment author="hbrunsting" created="2012-04-12T13:51:34Z" id="5091359">+1
</comment><comment author="jzelez" created="2012-04-12T14:28:19Z" id="5092165">+1
</comment><comment author="chendo" created="2012-05-21T06:04:16Z" id="5815870">+1
</comment><comment author="rauanmayemir" created="2012-05-31T11:22:47Z" id="6033393">+1
</comment><comment author="bhagwat" created="2012-06-08T08:16:28Z" id="6196708">+1
</comment><comment author="himanshuseth" created="2012-06-08T08:19:01Z" id="6196769">+1
</comment><comment author="gbgmian" created="2012-06-08T08:21:01Z" id="6196828">+1
</comment><comment author="ghost" created="2012-06-18T15:57:44Z" id="6400374">+1
</comment><comment author="pwoestelandt" created="2012-06-18T16:06:41Z" id="6400674">+1
</comment><comment author="tgautier-silicon" created="2012-06-18T18:17:14Z" id="6404487">+1
</comment><comment author="mailsurfie" created="2012-06-28T01:50:37Z" id="6620127">+1
</comment><comment author="jgoelen" created="2012-07-02T11:12:36Z" id="6705547">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Memory leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1075</link><project id="" key="" /><description>I had a long discussion about this on the mailing list. The solutions tried have had no effect so I'm considering this a bug.
## System Information
- Total memory = 469M
- vm.swappiness = 10
### Java

``` bash
Vendor:       Sun Microsystems Inc.
Name:         Java HotSpot(TM) Client VM
Version:      19.1-b02
Java version: 1.6.0_24
```
### Elastic

``` bash
Version: 0.16.2
```
#### Indicies

``` bash
Number of documents: 3888, Store size: 1.8mb (1925915 B)
Field cache evictions: 0, Field cache size: 125.9kb, Filter cache size: 18.3kb
Merges: Current: 0, Total: 0, Took: 0s
```
### Graphs

![](http://localhostr.com/file/LDs4Yad/localhost.localdomain-memory-day.png)
![](http://localhostr.com/file/O6YDxPU/localhost.localdomain-memory-week.png)
![](http://localhostr.com/file/uypPCsE/localhost.localdomain-memory-month.png)

&lt;a href="http://localhostr.com/file/6xvX3fN/Selection_029.jpeg"&gt;&lt;img src="http://localhostr.com/file/6xvX3fN/Selection_029.jpeg" width="495" /&gt;&lt;/a&gt;
## Solutions tried
### Locking memory

No effect. The following option was set for the elastisearch user: 

```
ulimit -l unlimited
```
### Swapped from OpenJDK to Oracle's Java

No effect
### Next action

I'll be trying to monitor the JVM at a lower level using a tool like visualgc to see what I can gain from it.
</description><key id="1137846">1075</key><summary>Memory leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ThePixelDeveloper</reporter><labels /><created>2011-06-29T14:05:04Z</created><updated>2011-07-18T12:31:26Z</updated><resolved>2011-07-18T12:31:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AdrianRossouw" created="2011-07-11T12:20:38Z" id="1546021">I have been experiencing the same, and it was confirmed by kimchy that this is a a problem he is debugging.

### Description

The JVM process steadily consuming more memory until the system becomes starved of resources and falls over. This additional memory consumption is not tied to the reported heap/non-heap memory consumption from BigDesk or the nodes/stats API.

While initially considered ubuntu specific, there have been reports of it happening on other linux distributions, but the low memory configuration of the 'problem systems' makes the issue more pronounced, and these systems have been running ubuntu so far.

### Solutions tried

**switch to oracle jre**

no effect.

**stop using the servicewrapper**

no effect.

**determine if open files/sockets cause the problem**

Debugging with lsof and netstat return reasonable results, showing that the pipes are being closed correctly according to the operating system.

**remove native libraries to check for leaks in them**

Upon kimchy's instructions i removed the sigar, jna and jline libraries from /lib, and restarted the server. [These are the results after 4 days](https://gist.github.com/2dbd3d8eb77cb5fd01a4)
</comment><comment author="AdrianRossouw" created="2011-07-12T11:41:09Z" id="1553617">I have cloned one of the environments and made different changes to each clone.
1. upgraded the clone 'spin1' to ubuntu 10.10 from 10.04.
2. upgraded the clone 'spin2' to the latest jre (update 26).

Initial states:  [spin1](https://gist.github.com/a87c2e6cd7922aba1550) and [spin2](https://gist.github.com/171b42d6bd5636204f2e)
12 hours later: [spin1](https://gist.github.com/793db965da0be8d139f8) and [spin2](https://gist.github.com/cea58b0a966689973c3c)

This is too early to tell what is going wrong, but on telling difference has shown up.
Namely, the tcp connection count on spin1 is incredibly high, much higher than
spin2 with the newer JRE. These figures are provided by sigar, and a connection
reference leak is a possible cause for a memory leak of this nature.

```
  # from spin2 

    "tcp" : {
      "active_opens" : 72,
      "passive_opens" : 2980,
      "curr_estab" : 29,
      "in_segs" : 130486,
      "out_segs" : 98255,
      "retrans_segs" : 69,
      "estab_resets" : 6,
      "attempt_fails" : 7,
      "in_errs" : 0,
      "out_rsts" : 6
    }

    # from spin1
    "tcp" : {
      "active_opens" : 10256,
      "passive_opens" : 14670,
      "curr_estab" : 29,
      "in_segs" : 205680,
      "out_segs" : 204016,
      "retrans_segs" : 108,
      "estab_resets" : 59,
      "attempt_fails" : 7,
      "in_errs" : 0,
      "out_rsts" : 44
    }
```
</comment><comment author="ThePixelDeveloper" created="2011-07-12T12:45:42Z" id="1553941">Since my last post I've updated to the latest JRE (update 26) and still experience the same issues. 

![](http://i.imgur.com/Vlg62.png)

Here are node stats after it's been running for a few days:

https://gist.github.com/754639add60d4196e8f5

Stats after an elasticsearch restart:

https://gist.github.com/17587c560f58a363e5ad

It should also be noted that my network stats are mostly the same with regard to active opens.
</comment><comment author="kimchy" created="2011-07-12T23:25:51Z" id="1558773">Heya, I think I found the issue, which is a leak in the JVM itself when using GarbageCollectorMXBean#getLastGcInfo. It get called periodically in elasticsearch (every 1 second), and also on node stats API.

I have pushed a change to disable it into both 0.16 branch and master.
</comment><comment author="AdrianRossouw" created="2011-07-14T13:31:05Z" id="1571469">Adding -Des.monitor.jvm.enabled=false to the command line solved the issue.

After 1 day, the test environment is using 160mb of memory, while the control is using 200mb.
</comment><comment author="aloiscochard" created="2011-07-14T13:45:00Z" id="1571575">kimchy.karma += 100k
</comment><comment author="ThePixelDeveloper" created="2011-07-14T13:46:18Z" id="1571587">I've been running off 0.16 head for the past day and can confirm the memory leak has been fixed

![](http://i.imgur.com/ox031.png)
![](http://i.imgur.com/z8Vk0.png)
</comment><comment author="kimchy" created="2011-07-14T16:47:37Z" id="1573049">cool, I will release 0.16.4 today that disables that by default, and also does not have the leak when calling node stats.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Please include the Smart Chinese Analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1074</link><project id="" key="" /><description>I have been using Smart Chinese Analyzer with Lucene with fine results. Recently I moved to using Elasticsearch, and would like to continue to use the same language analyzer. I understand that currently I cannot configure ES to use this analyzer and would like the analyzer to be added.

Elasticsearch version: 0.16.2. 
</description><key id="1135850">1074</key><summary>Please include the Smart Chinese Analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">voltaire-in</reporter><labels /><created>2011-06-29T05:08:30Z</created><updated>2011-06-29T05:10:08Z</updated><resolved>2011-06-29T05:09:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="voltaire-in" created="2011-06-29T05:10:08Z" id="1464854">just saw this is duplicate with https://github.com/elasticsearch/elasticsearch/issues/1073
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding SmartChineseAnalyzer to ElasticSearch </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1073</link><project id="" key="" /><description>Feature request: adding SmartChineseAnalyzer:

http://lucene.apache.org/java/2_9_1/api/contrib-smartcn/org/apache/lucene/analysis/cn/smart/package-summary.html
</description><key id="1131290">1073</key><summary>Adding SmartChineseAnalyzer to ElasticSearch </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">selforganized</reporter><labels /><created>2011-06-28T16:20:33Z</created><updated>2011-12-29T12:35:39Z</updated><resolved>2011-12-29T10:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="voltaire-in" created="2011-07-17T06:07:30Z" id="1589276">I also would like to use this analyzer. 
</comment><comment author="thmttch" created="2011-09-03T07:15:58Z" id="1985662">Update: It's now a plugin:

https://github.com/thmttch/elasticsearch

I created a fork that adds this as a built-in:

https://github.com/thmttch/elasticsearch/tree/smartchinese

I'm not sure if it's appropriate to make a merge request into master, but you can try it out.
</comment><comment author="hau" created="2011-12-28T14:11:38Z" id="3291062">I tried following the instructions on https://github.com/thmttch/elasticsearch. Installation was fine.   When I tried the example:
curl -XPUT localhost:9200/test1 -d '{"analysis": { "analyzer": { "default": { "type": "SmartChinese" }}}}'

I got:
{"error":"IndexCreationException[[test1] failed to create index]; nested: NoSuchMethodError[org.apache.lucene.analysis.WordlistLoader.getWordSet(Ljava/io/Reader;Ljava/lang/String;)Ljava/util/HashSet;]; ","status":500}

On the server, it had the same error.

I'm using the latest stable version of elasticsearch 0.18.6, on 64bit linux with Sun JRE 1.6.0.29.  Could you help please?
</comment><comment author="hau" created="2011-12-28T16:18:55Z" id="3292268">I tried using elasticsearch-0.18.0.  

curl -XPUT localhost:9200/test1 -d '{"analysis": { "analyzer": { "default": { "type": "SmartChinese" }}}}'

worked without errors.  Maybe the lucene api was changed after 0.18.0.  but when I did this:

curl -XGET localhost:9200/test1/analyze -d '{ "body" : "我说世界好!" }'

it did not show me the expected results, but it returned this:

No handler found for uri [/test1/analyze] and method [GET]%
</comment><comment author="thmttch" created="2011-12-29T08:24:42Z" id="3298942">Yes, ES upgraded to the newest Lucene around 0.18.5 or so, so I think it needs to be recompiled. Will look into it soon. Thanks for reporting!
</comment><comment author="kimchy" created="2011-12-29T10:09:21Z" id="3299475">I added a plugin to provide smart chinese analyzer and other: https://github.com/elasticsearch/elasticsearch-analysis-smartcn. Closing this issue.
</comment><comment author="hau" created="2011-12-29T10:27:28Z" id="3299594">Thank you!  It installed fine on elasticsearch-0.18.6.
However, when I tried the example described in https://github.com/thmttch/elasticsearch, more specifically
curl -XPUT localhost:9200/test1 -d '{"analysis": { "analyzer": { "default": { "type": "smartcn" }}}}'
It got an error 

Caused by: org.elasticsearch.common.settings.NoClassSettingsException: Failed to load class setting [type] with value [smartcn]
    at org.elasticsearch.common.settings.ImmutableSettings.getAsClass(ImmutableSettings.java:230)
    at org.elasticsearch.index.analysis.AnalysisModule.configure(AnalysisModule.java:306)
    ... 13 more
Caused by: java.lang.ClassNotFoundException: smartcn
    at java.net.URLClassLoader$1.run(Unknown Source)
    at java.net.URLClassLoader$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at org.elasticsearch.common.settings.ImmutableSettings.getAsClass(ImmutableSettings.java:214)
    ... 14 more

I have a feeling that I'm not doing it correctly.  Could you point me to a place that describes how to configure analyzer, tokenizer and token filter?
</comment><comment author="kimchy" created="2011-12-29T10:45:53Z" id="3299690">@hau `smart_chinese` will work instead of `smartcn`.
</comment><comment author="hau" created="2011-12-29T12:35:39Z" id="3300281">Great.  That worked!  Continuing on the original example, I got

curl -XGET localhost:9200/test1/_analyze -d '{ "body" : "我说世界好!" }'
{"tokens":[{"token":"bodi","start_offset":3,"end_offset":7,"type":"word","position":3},{"token":"我","start_offset":12,"end_offset":13,"type":"word","position":7},{"token":"说","start_offset":13,"end_offset":14,"type":"word","position":8},{"token":"世界","start_offset":14,"end_offset":16,"type":"word","position":9},{"token":"好","start_offset":16,"end_offset":17,"type":"word","position":10}]}

Looks like it parsed the content correctly.  Thank you very much!

Could you explain a little bit how the tokenizer and token filter are used?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fragment_size doesn't work with quoted phrase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1072</link><project id="" key="" /><description>I nave query with highlight like this: (Here is curl recreation https://gist.github.com/1032233 )
Look at issued result ( item4.description ) (you can see all doc also in  _source ), and mapping.

if i supply query:
two words
item.description returns only 128 chars, as expected. But if
"two words"
it returns the same 128 chars and all remainder of that field. Bit strange.

ES (v 0.16.0)

Please tell, can I do the trick with highlight fields like ["item_.title", "item_.description"]  in query fields?

I understand, it looks bit strange. I’m try to explain:
All my docs contains 10 items with title and description (and so on, not important). I need to have 1 match per field, so my first version with just array of items wont work. If i setup number_of_fragments to 1 it returns _only_ 1 result for all 10 items. If i setup number_of_fragments to 0 it returns all description concatenated in 1 highlight. So solution is to make item1, item2 etc.
Is it best solution?

PS No any reaction on my 3 messages after first at mailing list. I believe this is a bug, so i created this issue.
</description><key id="1130835">1072</key><summary>fragment_size doesn't work with quoted phrase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">yark</reporter><labels><label>bug</label><label>v0.90.0</label></labels><created>2011-06-28T15:00:37Z</created><updated>2014-03-12T13:05:59Z</updated><resolved>2013-04-27T15:51:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yark" created="2011-06-28T15:24:55Z" id="1456276">ES v 0.16.2 also vulnerable
</comment><comment author="sirmarlo" created="2012-11-30T17:45:08Z" id="10897622">Has there been any progress with this? I too am seeing this issue and am wondering if there was a resolution.
</comment><comment author="clintongormley" created="2013-04-05T14:48:37Z" id="15959840">This is still present in 0.90.0.RC1, I've rewritten the recreation into something runnable:

https://gist.github.com/clintongormley/5319822

At the bottom are two queries - one phrase query which shows the problem in item4.description, and one non-phrase query which shows expected behaviour.

@s1monw this looks like another lucene highlighter problem
</comment><comment author="s1monw" created="2013-04-05T16:38:23Z" id="15966777">[LUCENE-4899](https://issues.apache.org/jira/browse/LUCENE-4899) seems to fix this problem as well. At least the test I added does the right thing. I will keep this open until we upgraded to Lucene 4.3
</comment><comment author="s1monw" created="2013-04-27T15:51:36Z" id="17118314">Closed via https://github.com/elasticsearch/elasticsearch/commit/355f80adc92cc69ed35cbc4936f0f8334ee55307
</comment><comment author="MagmaRules" created="2013-07-18T10:06:38Z" id="21173908">I seem to be having the same problem in some cases: https://gist.github.com/MagmaRules/6028216
I'm running ES 0.90.1 (has lucene 4.3 in the lib).
</comment><comment author="ajhalani" created="2013-10-25T15:59:50Z" id="27104218">I am still seeing the issue v0.90.5. Is this fix only for fastVectorHighlighter, not the plain highlighter?
</comment><comment author="hdkeeper" created="2014-03-12T13:05:59Z" id="37405518">I'm experiencing the same problem. Elasticsearch 0.90.12.
Don't you know, will the issue be fixed somewhen?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IOException while indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1071</link><project id="" key="" /><description>Inserting documents into elasticsearch is causing random IOExceptions.

FWIW, I was having similar issues with solr 1.4.1 and 3.2.0 on both java 1.6.0_22 and 1.6.0_26.

```
[2011-06-27 20:08:46,976][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: initializing ...
[2011-06-27 20:08:46,979][INFO ][plugins                  ] [Scorcher] loaded []
[2011-06-27 20:08:50,198][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: initialized
[2011-06-27 20:08:50,199][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: starting ...
[2011-06-27 20:08:50,380][INFO ][transport                ] [Scorcher] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/172.17.0.144:9300]}
[2011-06-27 20:08:53,459][INFO ][cluster.service          ] [Scorcher] new_master [Scorcher][LMbJrnMPQE-LJP2lQsSLaw][inet[/172.17.0.144:9300]], reason: zen-disco-join (elected_as_master)
[2011-06-27 20:08:53,576][INFO ][discovery                ] [Scorcher] es_github_cluster/LMbJrnMPQE-LJP2lQsSLaw
[2011-06-27 20:08:53,587][INFO ][http                     ] [Scorcher] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/172.17.0.144:9200]}
[2011-06-27 20:08:53,588][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: started
[2011-06-27 20:08:53,605][INFO ][gateway                  ] [Scorcher] recovered [0] indices into cluster_state
[2011-06-27 20:09:23,835][INFO ][cluster.metadata         ] [Scorcher] [github] creating index, cause [auto(bulk api)], shards [5]/[1], mappings []
[2011-06-27 20:09:24,324][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issue-comments] (dynamic)
[2011-06-27 20:09:24,329][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:09:24,394][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issue-comments] (dynamic)
[2011-06-27 20:09:24,404][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:09:25,589][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issue-comments] (dynamic)
[2011-06-27 20:09:35,082][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:09:46,366][WARN ][index.shard.service      ] [Scorcher] [github][0] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][0] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Device or resource busy
    at sun.nio.ch.FileDispatcher.preClose0(Native Method)
    at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
    at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
    at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
    at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.close(NIOFSDirectory.java:106)
    at org.apache.lucene.index.SegmentTermEnum.close(SegmentTermEnum.java:214)
    at org.apache.lucene.index.TermInfosReader.&lt;init&gt;(TermInfosReader.java:130)
    at org.apache.lucene.index.SegmentReader$CoreReaders.&lt;init&gt;(SegmentReader.java:121)
    at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:578)
    at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:684)
    at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:642)
    at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:155)
    at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:38)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:455)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:11:15,391][WARN ][index.shard.service      ] [Scorcher] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ArrayIndexOutOfBoundsException: bit=58 size=39
    at org.apache.lucene.util.BitVector.getAndSet(BitVector.java:76)
    at org.apache.lucene.index.SegmentReader.doDelete(SegmentReader.java:890)
    at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1068)
    at org.apache.lucene.index.BufferedDeletes.applyDeletes(BufferedDeletes.java:327)
    at org.apache.lucene.index.BufferedDeletes.applyDeletes(BufferedDeletes.java:286)
    at org.apache.lucene.index.BufferedDeletes.applyDeletes(BufferedDeletes.java:191)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3358)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3296)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:454)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:11:19,736][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:11:34,476][DEBUG][action.search.type       ] [Scorcher] [github][3], node[LMbJrnMPQE-LJP2lQsSLaw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@ec3615b]
org.elasticsearch.search.query.QueryPhaseExecutionException: [github][3]: query[ConstantScore(*:*)],from[0],size[50]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ArrayIndexOutOfBoundsException
[2011-06-27 20:11:36,543][DEBUG][action.search.type       ] [Scorcher] [github][3], node[LMbJrnMPQE-LJP2lQsSLaw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3a7109eb]
org.elasticsearch.search.query.QueryPhaseExecutionException: [github][3]: query[ConstantScore(*:*)],from[0],size[50]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ArrayIndexOutOfBoundsException
[2011-06-27 20:11:36,893][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:11:51,377][INFO ][cluster.metadata         ] [Scorcher] [github] update_mapping [issue-comments] (dynamic)
[2011-06-27 20:14:53,493][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: stopping ...
[2011-06-27 20:14:53,571][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: stopped
[2011-06-27 20:14:53,571][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: closing ...
[2011-06-27 20:14:53,579][INFO ][node                     ] [Scorcher] {elasticsearch/0.16.2}[443]: closed
[2011-06-27 20:14:59,199][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: initializing ...
[2011-06-27 20:14:59,202][INFO ][plugins                  ] [Solara] loaded []
[2011-06-27 20:15:01,507][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: initialized
[2011-06-27 20:15:01,508][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: starting ...
[2011-06-27 20:15:01,574][INFO ][transport                ] [Solara] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/172.17.0.144:9300]}
[2011-06-27 20:15:04,614][INFO ][cluster.service          ] [Solara] new_master [Solara][zswXDV3zSEmuLw0wOhYtxA][inet[/172.17.0.144:9300]], reason: zen-disco-join (elected_as_master)
[2011-06-27 20:15:04,674][INFO ][discovery                ] [Solara] es_github_cluster/zswXDV3zSEmuLw0wOhYtxA
[2011-06-27 20:15:04,764][INFO ][http                     ] [Solara] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/172.17.0.144:9200]}
[2011-06-27 20:15:04,819][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: started
[2011-06-27 20:15:05,756][INFO ][gateway                  ] [Solara] recovered [1] indices into cluster_state
[2011-06-27 20:20:59,914][DEBUG][action.search.type       ] [Solara] [github][4], node[zswXDV3zSEmuLw0wOhYtxA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4f5b571e]
org.elasticsearch.search.query.QueryPhaseExecutionException: [github][4]: query[_all:ffi repo_id:[3 TO 3]],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: read past EOF
    at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:163)
    at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:213)
    at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:92)
    at org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:181)
    at org.apache.lucene.index.SegmentTermDocs.next(SegmentTermDocs.java:112)
    at org.apache.lucene.index.SegmentTermPositions.next(SegmentTermPositions.java:102)
    at org.apache.lucene.search.spans.TermSpans.next(TermSpans.java:49)
    at org.apache.lucene.search.spans.SpanScorer.&lt;init&gt;(SpanScorer.java:46)
    at org.elasticsearch.common.lucene.all.AllTermQuery$AllTermWeight$AllTermSpanScorer.&lt;init&gt;(AllTermQuery.java:78)
    at org.elasticsearch.common.lucene.all.AllTermQuery$AllTermWeight.scorer(AllTermQuery.java:67)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:298)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:116)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:517)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:177)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:384)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:291)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:279)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:211)
    ... 9 more
[2011-06-27 20:22:40,672][DEBUG][action.search.type       ] [Solara] [github][4], node[zswXDV3zSEmuLw0wOhYtxA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1d94a7ec]
org.elasticsearch.search.query.QueryPhaseExecutionException: [github][4]: query[_all:ffi repo_id:[3 TO 3]],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 30
    at org.apache.lucene.util.OpenBitSet.fastSet(OpenBitSet.java:247)
    at org.apache.lucene.search.PublicTermsFilter.getDocIdSet(PublicTermsFilter.java:87)
    at org.elasticsearch.index.cache.filter.support.AbstractWeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(AbstractWeightedFilterCache.java:181)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:120)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:517)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:177)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:384)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:291)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:279)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:211)
    ... 9 more
[2011-06-27 20:22:41,276][WARN ][index.shard.service      ] [Solara] [github][0] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][0] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Device or resource busy
    at sun.nio.ch.FileDispatcher.preClose0(Native Method)
    at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
    at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
    at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
    at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.close(NIOFSDirectory.java:106)
    at org.apache.lucene.index.FieldInfos.&lt;init&gt;(FieldInfos.java:94)
    at org.apache.lucene.index.SegmentReader$CoreReaders.&lt;init&gt;(SegmentReader.java:118)
    at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:578)
    at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:684)
    at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:642)
    at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:155)
    at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:38)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:455)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:22:45,627][DEBUG][action.search.type       ] [Solara] [github][3], node[zswXDV3zSEmuLw0wOhYtxA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@5143a680]
org.elasticsearch.search.query.QueryPhaseExecutionException: [github][3]: query[_all:ffi repo_id:[3 TO 3]],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Bad file descriptor
    at sun.nio.ch.FileDispatcher.pread0(Native Method)
    at sun.nio.ch.FileDispatcher.pread(FileDispatcher.java:31)
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:199)
    at sun.nio.ch.IOUtil.read(IOUtil.java:175)
    at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:612)
    at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:161)
    at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:139)
    at org.apache.lucene.index.SegmentReader$Norm.bytes(SegmentReader.java:445)
    at org.apache.lucene.index.SegmentReader$Norm.bytes(SegmentReader.java:424)
    at org.apache.lucene.index.SegmentReader.getNorms(SegmentReader.java:1050)
    at org.apache.lucene.index.SegmentReader.norms(SegmentReader.java:1057)
    at org.elasticsearch.common.lucene.all.AllTermQuery$AllTermWeight.scorer(AllTermQuery.java:67)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:298)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:116)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:517)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:177)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:384)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:291)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:279)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:211)
    ... 9 more
[2011-06-27 20:22:45,629][DEBUG][action.search.type       ] [Solara] [github][4], node[zswXDV3zSEmuLw0wOhYtxA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@5143a680]
org.elasticsearch.search.query.QueryPhaseExecutionException: [github][4]: query[_all:ffi repo_id:[3 TO 3]],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 30
    at org.apache.lucene.util.OpenBitSet.fastSet(OpenBitSet.java:247)
    at org.apache.lucene.search.PublicTermsFilter.getDocIdSet(PublicTermsFilter.java:87)
    at org.elasticsearch.index.cache.filter.support.AbstractWeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(AbstractWeightedFilterCache.java:181)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:120)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:517)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:177)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:384)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:291)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:279)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:211)
    ... 9 more
[2011-06-27 20:22:48,303][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.SegmentReader$Norm.clone(SegmentReader.java:502)
    at org.apache.lucene.index.SegmentReader.reopenSegment(SegmentReader.java:751)
    at org.apache.lucene.index.SegmentReader.clone(SegmentReader.java:663)
    at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:644)
    at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:155)
    at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:38)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:455)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:22:56,330][WARN ][index.shard.service      ] [Solara] [github][4] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][4] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Bad file descriptor
    at java.io.RandomAccessFile.writeBytes(Native Method)
    at java.io.RandomAccessFile.write(RandomAccessFile.java:482)
    at org.apache.lucene.store.FSDirectory$FSIndexOutput.flushBuffer(FSDirectory.java:469)
    at org.apache.lucene.store.BufferedIndexOutput.flushBuffer(BufferedIndexOutput.java:99)
    at org.apache.lucene.store.BufferedIndexOutput.flush(BufferedIndexOutput.java:88)
    at org.apache.lucene.store.BufferedIndexOutput.close(BufferedIndexOutput.java:113)
    at org.apache.lucene.store.FSDirectory$FSIndexOutput.close(FSDirectory.java:478)
    at org.elasticsearch.index.store.support.AbstractStore$StoreIndexOutput.close(AbstractStore.java:422)
    at org.apache.lucene.index.NormsWriter.flush(NormsWriter.java:170)
    at org.apache.lucene.index.DocInverter.flush(DocInverter.java:73)
    at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:59)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:552)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3331)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3296)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:454)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:22:56,331][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.SegmentReader$Norm.clone(SegmentReader.java:502)
    at org.apache.lucene.index.SegmentReader.reopenSegment(SegmentReader.java:751)
    at org.apache.lucene.index.SegmentReader.clone(SegmentReader.java:663)
    at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:644)
    at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:155)
    at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:38)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:455)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:23:04,357][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.SegmentReader$Norm.clone(SegmentReader.java:502)
    at org.apache.lucene.index.SegmentReader.reopenSegment(SegmentReader.java:751)
    at org.apache.lucene.index.SegmentReader.clone(SegmentReader.java:663)
    at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:644)
    at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:155)
    at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:38)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:455)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:23:18,384][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
[2011-06-27 20:23:24,410][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
[2011-06-27 20:23:30,438][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
[2011-06-27 20:23:37,463][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
[2011-06-27 20:23:43,486][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
[2011-06-27 20:23:50,513][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
[2011-06-27 20:23:57,541][WARN ][index.shard.service      ] [Solara] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
[2011-06-27 20:25:01,091][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: stopping ...
[2011-06-27 20:25:01,202][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: stopped
[2011-06-27 20:25:01,203][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: closing ...
[2011-06-27 20:25:01,214][INFO ][node                     ] [Solara] {elasticsearch/0.16.2}[3325]: closed
[2011-06-27 20:39:28,396][INFO ][node                     ] [Reignfire] {elasticsearch/0.16.2}[1391]: initializing ...
[2011-06-27 20:39:28,399][INFO ][plugins                  ] [Reignfire] loaded []
[2011-06-27 20:39:30,686][INFO ][node                     ] [Reignfire] {elasticsearch/0.16.2}[1391]: initialized
[2011-06-27 20:39:30,686][INFO ][node                     ] [Reignfire] {elasticsearch/0.16.2}[1391]: starting ...
[2011-06-27 20:39:30,755][INFO ][transport                ] [Reignfire] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/172.17.1.26:9300]}
[2011-06-27 20:39:33,795][INFO ][cluster.service          ] [Reignfire] new_master [Reignfire][w5JiddqYTn2_gAuAX6WOxg][inet[/172.17.1.26:9300]], reason: zen-disco-join (elected_as_master)
[2011-06-27 20:39:33,852][INFO ][discovery                ] [Reignfire] es_github_cluster/w5JiddqYTn2_gAuAX6WOxg
[2011-06-27 20:39:33,865][INFO ][http                     ] [Reignfire] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/172.17.1.26:9200]}
[2011-06-27 20:39:33,865][INFO ][gateway                  ] [Reignfire] recovered [0] indices into cluster_state
[2011-06-27 20:39:33,865][INFO ][node                     ] [Reignfire] {elasticsearch/0.16.2}[1391]: started
[2011-06-27 20:41:16,217][INFO ][cluster.metadata         ] [Reignfire] [github] creating index, cause [auto(bulk api)], shards [5]/[1], mappings []
[2011-06-27 20:41:16,682][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:41:16,687][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issue-comments] (dynamic)
[2011-06-27 20:41:16,703][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:41:16,819][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issue-comments] (dynamic)
[2011-06-27 20:41:21,749][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:41:25,926][WARN ][http.netty               ] [Reignfire] Caught exception while handling client http traffic, closing connection
java.io.IOException: Device or resource busy
    at sun.nio.ch.FileDispatcher.preClose0(Native Method)
    at sun.nio.ch.SocketDispatcher.preClose(SocketDispatcher.java:41)
    at sun.nio.ch.SocketChannelImpl.implCloseSelectableChannel(SocketChannelImpl.java:677)
    at java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:201)
    at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.close(NioWorker.java:580)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:742)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:60)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:568)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:747)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:350)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:272)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:568)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:563)
    at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:720)
    at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:208)
    at org.elasticsearch.common.netty.channel.ChannelFutureListener$1.operationComplete(ChannelFutureListener.java:46)
    at org.elasticsearch.common.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:381)
    at org.elasticsearch.common.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:372)
    at org.elasticsearch.common.netty.channel.DefaultChannelFuture.setSuccess(DefaultChannelFuture.java:316)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.write0(NioWorker.java:486)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.writeFromSelectorLoop(NioWorker.java:399)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:286)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
[2011-06-27 20:41:35,838][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:41:52,580][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issues] (dynamic)
[2011-06-27 20:42:06,109][INFO ][cluster.metadata         ] [Reignfire] [github] update_mapping [issue-comments] (dynamic)
[2011-06-27 20:43:44,401][WARN ][index.shard.service      ] [Reignfire] [github][4] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][4] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Device or resource busy
    at sun.nio.ch.FileDispatcher.preClose0(Native Method)
    at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
    at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
    at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
    at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.close(NIOFSDirectory.java:106)
    at org.apache.lucene.index.FieldInfos.&lt;init&gt;(FieldInfos.java:94)
    at org.apache.lucene.index.SegmentReader$CoreReaders.&lt;init&gt;(SegmentReader.java:118)
    at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:578)
    at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:684)
    at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:642)
    at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:155)
    at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:38)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:455)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:43:44,419][WARN ][index.shard.service      ] [Reignfire] [github][3] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [github][3] Refresh failed
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:640)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:403)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:628)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Bad file descriptor
    at java.io.RandomAccessFile.writeBytes(Native Method)
    at java.io.RandomAccessFile.write(RandomAccessFile.java:482)
    at org.apache.lucene.store.FSDirectory$FSIndexOutput.flushBuffer(FSDirectory.java:469)
    at org.apache.lucene.store.BufferedIndexOutput.flushBuffer(BufferedIndexOutput.java:99)
    at org.apache.lucene.store.BufferedIndexOutput.flush(BufferedIndexOutput.java:88)
    at org.apache.lucene.store.BufferedIndexOutput.close(BufferedIndexOutput.java:113)
    at org.apache.lucene.store.FSDirectory$FSIndexOutput.close(FSDirectory.java:478)
    at org.elasticsearch.index.store.support.AbstractStore$StoreIndexOutput.close(AbstractStore.java:422)
    at org.apache.lucene.index.FormatPostingsDocsWriter.close(FormatPostingsDocsWriter.java:124)
    at org.apache.lucene.index.FormatPostingsTermsWriter.close(FormatPostingsTermsWriter.java:71)
    at org.apache.lucene.index.FormatPostingsFieldsWriter.finish(FormatPostingsFieldsWriter.java:70)
    at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:135)
    at org.apache.lucene.index.TermsHash.flush(TermsHash.java:109)
    at org.apache.lucene.index.DocInverter.flush(DocInverter.java:72)
    at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:59)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:552)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3331)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3296)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:454)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:403)
    at org.apache.lucene.index.DirectoryReader.doReopenFromWriter(DirectoryReader.java:405)
    at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:418)
    at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:383)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:623)
    ... 5 more
[2011-06-27 20:44:35,078][INFO ][node                     ] [Reignfire] {elasticsearch/0.16.2}[1391]: stopping ...
```
</description><key id="1127303">1071</key><summary>IOException while indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tmm1</reporter><labels /><created>2011-06-28T07:40:49Z</created><updated>2014-02-09T21:37:02Z</updated><resolved>2011-06-28T07:45:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-28T07:45:26Z" id="1452914">questions on the mailing list, and provide a link to a gist of the failure.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Misbehaving "missing" field in facet results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1070</link><project id="" key="" /><description>As discussed on the mailing list:
http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/bddbe33e2ce91797/5db4766740a3596c

Full gist recreation: https://gist.github.com/e6e703b44e71bb80c641
</description><key id="1126516">1070</key><summary>Misbehaving "missing" field in facet results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrisberkhout</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-28T03:19:25Z</created><updated>2011-06-28T09:38:33Z</updated><resolved>2011-06-28T09:38:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-28T09:26:28Z" id="1453395">I can recreate the bug, will fix it. A few notes on your mapping / document: Setting a field to be not analyzed, you should set `index` to `not_analyzed` (and not the way you do it now). Also, there is no need to index `id`, you already get that for free (under `_id`).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/MultiValueByteFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/MultiValueDoubleFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/MultiValueFloatFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/MultiValueIntFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/MultiValueLongFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/MultiValueShortFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/strings/MultiValueStringFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/geo/MultiValueGeoPointFieldData.java</file></files><comments><comment>Misbehaving "missing" field in facet results, closes #1070.</comment></comments></commit></commits></item><item><title>Working GIST example for geo_distance filter, consider for documentation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1069</link><project id="" key="" /><description>As a newbie to ES, I found the API documentation confusing and would like to encourage building a set of working examples to add to the documentation.

Here's one I created based on examples I've seen in the mailing list for the geo_distance filter: https://gist.github.com/1050346
</description><key id="1126306">1069</key><summary>Working GIST example for geo_distance filter, consider for documentation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">john-bai</reporter><labels /><created>2011-06-28T02:16:59Z</created><updated>2013-04-05T14:50:09Z</updated><resolved>2013-04-05T14:50:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T14:50:09Z" id="15959929">Hiya - thanks for the geo example. It's been ignored for so long because this was opened in the wrong repository.  We are in the process of completely rewriting the docs in a different format, which will include such examples. 

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi GET API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1068</link><project id="" key="" /><description>Multi GET API allows to get multiple documents based on an index, type (optional) and id (and possibly routing). The response includes a `doc` array with all the documents each element similar in structure to the `get` API. Here is an example:

```
curl localhost:9200/_mget?pretty=1 -d '{
    "docs" : [
        {
            "_index" : "test",
            "_type" : "type",
            "_id" : "1"
        },
        {
            "_index" : "test",
            "_type" : "type",
            "_id" : "2"
        }
    ]
}'
```

The `mget` endpoint can also be used against an index (in which case its not required in the body):

```
curl localhost:9200/test/_mget?pretty=1 -d '{
    "docs" : [
        {
            "_type" : "type",
            "_id" : "1"
        },
        {
            "_type" : "type",
            "_id" : "2"
        }
    ]
}'
```

And a type:

```
curl localhost:9200/test/type/_mget?pretty=1 -d '{
    "docs" : [
        {
            "_id" : "1"
        },
        {
            "_id" : "2"
        }
    ]
}'
```

In which case, the `ids` element can directly be used to simplify the request:

```
curl localhost:9200/test/type/_mget?pretty=1 -d '{
    "ids" : ["1", "2"]
}'
```
</description><key id="1124223">1068</key><summary>Multi GET API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-27T18:59:09Z</created><updated>2011-06-27T19:24:07Z</updated><resolved>2011-06-27T19:24:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActions.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetItemResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetShardRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/MultiGetShardResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/Client.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/get/MultiGetRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/ClientTransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/get/ClientTransportMultiGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/OperationRouting.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/GetActionTests.java</file></files><comments><comment>Multi GET API, closes #1068.</comment></comments></commit></commits></item><item><title>Search Trends - Most Searched</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1067</link><project id="" key="" /><description>This is a feature request to track top searches/queries for an index. For example, the result would return the most popular queries and a count for each.
</description><key id="1123867">1067</key><summary>Search Trends - Most Searched</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bjfish</reporter><labels /><created>2011-06-27T18:00:40Z</created><updated>2014-07-08T12:23:58Z</updated><resolved>2014-07-08T12:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T12:23:58Z" id="48328904">This would be best implemented in your application, where you can group queries as make sense to your use case.  You could still use Elasticsearch to store and query the logs that your application produces. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sponsored Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1066</link><project id="" key="" /><description>This is a feature request for a "Sponsored Search" feature similar to the Query Elevation component in Solr: http://wiki.apache.org/solr/QueryElevationComponent.
</description><key id="1123780">1066</key><summary>Sponsored Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bjfish</reporter><labels /><created>2011-06-27T17:42:27Z</created><updated>2012-09-17T18:35:09Z</updated><resolved>2012-09-17T18:35:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="selforganized" created="2011-06-28T16:22:30Z" id="1456798">I'd like to have this feature too.
</comment><comment author="kimchy" created="2011-06-30T00:27:42Z" id="1471748">Not a big fan of static files representing elevation, would like to solve it in a different manner (still thinking on it). Though, this specific feature should not be too difficult to implement, also as a plugin with a custom native script that does that.
</comment><comment author="bjfish" created="2011-06-30T15:28:54Z" id="1475571">I am also not a fan of static files and was hoping it could be configured similar to the settings api:
$ curl -XPUT 'http://localhost:9200/${index}/' -d '{
"sponsor" : {
    "${queryText}" : ["${result1}","${result2}"]
}
}'

with a corresponding GET. Others may want a per type level configuration too.
</comment><comment author="spinscale" created="2012-09-07T14:21:40Z" id="8366642">You can actually achieve this behaviour already if you want. Add a field for some sort of rating (with values -1, -2, 1, 2) for rating up and change your query a bit... (might not be the most performant solution but works for us)

BoolQueryBuilder query = QueryBuilders.boolQuery();
query.should(QueryBuilders.customBoostFactorQuery(QueryBuilders.fieldQuery("yourField", "2")).boostFactor(HIGH_UP_BOOST));
query.should(QueryBuilders.customBoostFactorQuery(QueryBuilders.fieldQuery("yourField", "1")).boostFactor(UP_BOOST));
query.should(QueryBuilders.customBoostFactorQuery(QueryBuilders.fieldQuery("yourField", "-1")).boostFactor(NEG_BOOST));
query.should(QueryBuilders.customBoostFactorQuery(QueryBuilders.fieldQuery("yourField", "-2")).boostFactor(HIGH_NEG_BOOST));

query.must(your_normal_query);
</comment><comment author="humbucker" created="2012-09-17T18:22:55Z" id="8625316">Did this one go anywhere @kimchy? We're looking at a similar problem. Any thoughts on performant approaches that would allow us to respect bool and filtered?
</comment><comment author="clintongormley" created="2012-09-17T18:35:09Z" id="8625739">This is easy to do with http://www.elasticsearch.org/guide/reference/query-dsl/custom-filters-score-query.html

```
curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "query" : {
      "custom_filters_score" : {
         "query" : {
            "text" : {
               "content" : "foo bar"
            }
         },
         "filters" : [
            {
               "boost" : "100",
               "filter" : {
                  "term" : {
                     "sponsored" : true
                  }
               }
            }
         ]
      }
   }
}
'
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for highlighting of phrase prefix queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1065</link><project id="" key="" /><description>Prefix queries needs to be rewritten so the highlighter can work. So the pull request.

The highlight of prefix queries doesn't highlight the prefix but the full word. I have written some code on the client side to change the highlighted words. For instance, searching "ski", lucene highlight "&lt;&gt;skiing&lt;/&gt;", "&lt;&gt;skin&lt;/&gt;" or "&lt;&gt;skills&lt;/&gt;". So my code change the highlighted parts to be: "&lt;&gt;ski&lt;/&gt;in",  "&lt;&gt;ski&lt;/&gt;n" or "&lt;&gt;ski&lt;/&gt;lls". This code may be interesting to be on the server part, but I don't know where. If you think it is interesting, let me know.
</description><key id="1120904">1065</key><summary>Support for highlighting of phrase prefix queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-06-27T11:39:33Z</created><updated>2014-06-19T05:09:41Z</updated><resolved>2013-01-29T09:27:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-27T21:29:50Z" id="1450610">pushed, thanks!
</comment><comment author="jhnlsn" created="2012-11-29T20:27:50Z" id="10864871">It appears that support for highlighting with phrase_prefix query no longer works.  Should a new ticket be opened.  Here is the query https://gist.github.com/4171683
</comment><comment author="rastermanden" created="2013-01-28T12:24:33Z" id="12779516">I can't make highlighting work with match_prefix either. Using version:

http://www.elasticsearch.org/blog/2012/12/27/0.20.2-released.html
</comment><comment author="s1monw" created="2013-01-28T14:46:30Z" id="12784689">Guys I pushed a fix for this to master adn 0.20 if somebody could verify its working for you that would be great!
</comment><comment author="rastermanden" created="2013-01-30T08:53:44Z" id="12879885">Just tried it out. It is working! Thanks a lot!
</comment><comment author="s1monw" created="2013-01-30T19:52:52Z" id="12908836">@rastermanden thanks for verifying! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices Segments API: Internal segments info of shard level Lucene indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1064</link><project id="" key="" /><description>Provide low level segments information that a Lucene index (shard leve) is built with. Allows to be used to provide more information on the state of a shard and an index, possibly optimization information, data "wasted" on deletes, and so on.
</description><key id="1115753">1064</key><summary>Indices Segments API: Internal segments info of shard level Lucene indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-25T21:20:18Z</created><updated>2011-06-25T21:21:09Z</updated><resolved>2011-06-25T21:21:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActions.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexSegments.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/Requests.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/segments/IndicesSegmentsRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/node/NodeIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/ClientTransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/admin/indices/segments/ClientTransportIndicesSegmentsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Segment.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/segments/RestIndicesSegmentsAction.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/engine/AbstractSimpleEngineTests.java</file></files><comments><comment>Indices Segments API: Internal segments info of shard level Lucene indices, closes #1064.</comment></comments></commit></commits></item><item><title>"block until refresh" indexing option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1063</link><project id="" key="" /><description>Feature request:  Provide an option to the index operation that will wait until the next scheduled refresh occurs before returning a response.  After the response is returned, all documents indexed in that operation should be visible for search.
</description><key id="1114781">1063</key><summary>"block until refresh" indexing option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">caravone</reporter><labels><label>:CRUD</label><label>feature</label><label>v5.0.0-alpha4</label></labels><created>2011-06-25T14:54:03Z</created><updated>2017-03-24T09:10:56Z</updated><resolved>2016-06-06T15:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2011-07-06T14:36:08Z" id="1512126">Thanks caravone for entering the issue. 

This is discussed here: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/4b882c79671c6e2c/e74e3fc1718b6bf6?lnk=gst&amp;q=visible#e74e3fc1718b6bf6

Btw, we can work around the issue by waiting &gt; 1000ms after an index op before doing a query. But having this at the API level will be great.
</comment><comment author="ghost" created="2011-08-23T13:23:14Z" id="1879647">Has this issue been prioritized? This would be really great and would allow ES to be used in many more use cases. Currently, ES cannot be used to fetch 'screen' data because of the NRT aspects. As you mentionned Shane, a flag that blocks the index operation until the changes have been made visible by the next refresh would do what we need and you indicated that the complexity of implementing this would be manageable.

Thanks again for this great product!
</comment><comment author="NateHark" created="2012-01-19T18:18:06Z" id="3571417">We could definitely benefit from this feature as well. NRT works great for 95% of our use cases, but we still have a couple of use cases that would be simpler if this feature was available. Thanks!
</comment><comment author="ghost" created="2012-01-19T19:01:11Z" id="3572194">We are simulating this behavior doing BulkRequestBuilder.setRefresh(true). That's fine while we are still in development... but this won't scale in production.

BulkRequestBuilder.setBlockUntilRefresh(true) would be a great addition to the request API.

Is there an easy way to have it block until the replicas have also been updated. The next query could be routed to the replica instead of the master an need to behave the same if blocking has been requested.

Thanks again Shay!
</comment><comment author="ghost" created="2012-04-05T13:35:27Z" id="4976498">Hello Shay!

We are getting closer to a beta release and this feature would really make a big difference... and I believe that ES would gain in its ability to become more and more the primary datastore for many applications if we could optionally block on indexing until refresh.

We understand that this behaviour will be used only where needed as this will introduce a mean latency of 500ms for the call. But this would be really helpful in many usecases.

Thanks for the great product. It's been really living up to our expectations.

Remy
</comment><comment author="ghost" created="2012-05-09T20:23:06Z" id="5610599">Me again! ;-)

While we are backed by a relational database for security and for its transactional support, we rely almost entirely on ES for all of our queries. Not just for searching things... all our relations between items, all our acls and security is done through queries to ES.

For some use cases, it would be just great to have index-then-block-until-data-is-visible-for-search... instead of forcing a refresh.

BTW, when doing BulkRequestBuilder.setRefresh(true), does this also forces a refresh on the replicas?

Thanks!
</comment><comment author="missinglink" created="2013-06-27T16:01:08Z" id="20132526">+1
</comment><comment author="richardwalsh" created="2013-10-11T15:33:03Z" id="26146648">+1
</comment><comment author="fresheneesz" created="2013-12-18T21:40:23Z" id="30883574">+1
</comment><comment author="teuneboon" created="2014-01-22T13:44:13Z" id="33022642">+1
</comment><comment author="onnomarsman" created="2014-01-22T13:46:43Z" id="33022881">+1
</comment><comment author="stabenfeldt" created="2014-02-12T14:00:12Z" id="34870790">+1
</comment><comment author="nessup" created="2014-05-14T05:42:58Z" id="43044422">+1
</comment><comment author="pentium10" created="2014-06-03T15:15:51Z" id="44977822">+1
</comment><comment author="mikend" created="2014-06-24T01:22:09Z" id="46922182">+1
</comment><comment author="kevin-montrose" created="2014-07-17T23:05:08Z" id="49377883">+1
</comment><comment author="LiquidMark" created="2014-08-04T12:12:54Z" id="51051980">Hoping this shows up soon. We use the 'refresh' option to all of our index operations. Fine for development and testing, but I'm concerned about performance impact for production. Blocking until the next scheduled refresh has finished would solve our problem!
</comment><comment author="NTCoding" created="2014-08-05T10:49:43Z" id="51181286">This is killing us in production. We need to calculate facets on the indexed documents. but our servers cannot handle many refreshes per second.

I'm looking for some way to get a notification of when the items have been acknowledged so I can wait until then and calculate facets.

So another +1 :)
</comment><comment author="clintongormley" created="2014-08-22T13:21:53Z" id="53059141">From https://github.com/elasticsearch/elasticsearch/issues/7354#issuecomment-52814558

&gt; Yeah ControlledRTReopenThread was designed for exactly this situation; it's basically the same as your 3rd option (Delay the update of the view in the UI...): it delays the rare requests that must see the latest changes while allowing the normal (hopefully vast majority) of requests to just use the last refreshed reader.
</comment><comment author="atott" created="2014-10-02T09:57:02Z" id="57606896">Unfortunately we had faced this problem. So +1 :)
</comment><comment author="satazor" created="2015-01-03T13:17:49Z" id="68594640">+1
</comment><comment author="clintongormley" created="2015-01-26T17:17:25Z" id="71497936">A problem with this solution is that we could have potentially thousands of requests blocking until a refresh happens, all of which suddenly return in the same instant.  All of those requests will use up RAM, and the sudden rush of responses might saturate network bandwidth.

I'm not convinced that the suggested solution here is practical.  Better to take an "eventually consistent" approach, eg:
- user views list of comments (generated by search)
- user posts new comment
- show user original search results and manually insert their comment into the list, and by the time they refresh the page, the index should have refreshed
</comment><comment author="fresheneesz" created="2015-01-26T19:56:44Z" id="71526455">@clintongormley Your concern sounds rather far fetched. All the request has to do is close. You're really saying that thousands of TCP closes would "saturate network bandwidth"? I don't think that would even happen if your server's primary connection to the db is over 56k. And how much ram and resources do you think it takes for something to continuously poll? WAY more. 
</comment><comment author="clintongormley" created="2015-01-26T20:56:07Z" id="71536460">@fresheneesz that's an interesting idea - i was thinking of only sending the responses once the refresh happened, as opposed to just holding back enough to stop the request from completing.  It wouldn't be sufficient to just hold the connection open as it uses keepalive anyway.
</comment><comment author="joar" created="2015-02-19T13:00:52Z" id="75047938">I'm +1 on this, as long as it implies that you may leave the request hanging until the index is flushed (i.e the document becomes available).
</comment><comment author="jannemann" created="2015-02-23T17:33:05Z" id="75590260">+1
</comment><comment author="mrkamel" created="2015-03-23T20:25:23Z" id="85178907">+1
</comment><comment author="bleskes" created="2015-03-27T09:33:03Z" id="86880357">we discussed it and I think the concern about memory is valid but it's one we already have. During master election we accumulate indexing requests as well (up to 1m). There was also the idea of add a timeout to the refresh wait (but not the indexing).
</comment><comment author="javanna" created="2015-03-27T17:30:35Z" id="87023435">I think people might be currently using the refresh option on the index api to achieve the same result (finding the document via _search straight-away), which is scary as it's not feasible to refresh for each single index operation. Having the option to wait for refresh would be much better I think as it wouldn't cause a refresh but just wait for the next one, and it could potentially replace the ability to refresh on indexing completely, or just be its lightweight alternative.
</comment><comment author="spudbean" created="2015-04-08T02:15:28Z" id="90781908">An alternate approach to "block until refresh" is "return a token that I can use to force consistency on my next query". On a PUT, ES could return some opaque consistency token. On subsequent queries, I can pass a "be consistent with" parameter and give it the token from my last PUT (or several such tokens). ES will block until the "refresh" for that token has occurred (it may have already occurred!) and then execute the query.

Note that this blocking can be done per-shard, some shards may already be up to date wrt to the token when the request come in, and only some may need to block. You could even imagine some future optimisation where requests are routed to shard-replicas that are more likely to be up-to-date wrt to a particular token.

So we change from "block a PUTs _return_ until refresh" to "block a QUERYs _start_ until refresh". This allows for better pipelining of activities in the system as a whole.

(EDIT: it is worth noting that if you can implement "block a QUERY's start", then you could easily implement "block a PUTs return" on top of that within ES, or within client code.)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/support/WriteRequestBuilder.java</file><file>core/src/test/java/org/elasticsearch/document/DocumentActionsIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/GatewayIndexStateIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/string/StringFieldMapperPositionIncrementGapTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/index/termvectors/TermVectorsServiceTests.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationIT.java</file><file>core/src/test/java/org/elasticsearch/indices/IndicesServiceTests.java</file><file>core/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateIT.java</file><file>core/src/test/java/org/elasticsearch/mget/SimpleMgetIT.java</file><file>core/src/test/java/org/elasticsearch/search/SearchServiceTests.java</file><file>core/src/test/java/org/elasticsearch/search/SearchTimeoutIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/functionscore/DecayFunctionScoreIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxIT.java</file><file>core/src/test/java/org/elasticsearch/search/geo/GeoShapeQueryTests.java</file><file>core/src/test/java/org/elasticsearch/search/innerhits/InnerHitsIT.java</file><file>core/src/test/java/org/elasticsearch/search/nested/SimpleNestedIT.java</file><file>core/src/test/java/org/elasticsearch/search/query/SearchQueryIT.java</file><file>core/src/test/java/org/elasticsearch/search/scroll/SearchScrollIT.java</file><file>core/src/test/java/org/elasticsearch/search/simple/SimpleSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearch2xIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CustomSuggesterSearchIT.java</file><file>core/src/test/java/org/elasticsearch/similarity/SimilarityIT.java</file><file>core/src/test/java/org/elasticsearch/timestamp/SimpleTimestampIT.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLIT.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateIT.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/SearchFieldsTests.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovyScriptTests.java</file><file>modules/reindex/src/test/java/org/elasticsearch/index/reindex/UpdateByQueryWhileModifyingTests.java</file></files><comments><comment>Remove setRefresh</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/action/DocWriteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/flush/TransportShardFlushAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportShardRefreshAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkShardRequest.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>core/src/main/java/org/elasticsearch/action/delete/DeleteRequest.java</file><file>core/src/main/java/org/elasticsearch/action/delete/DeleteRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>core/src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/ingest/IngestActionFilter.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/action/support/WriteResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicatedWriteRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationOperation.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationRequest.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/ReplicationResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportBroadcastReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/replication/TransportWriteAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/common/settings/IndexScopedSettings.java</file><file>core/src/main/java/org/elasticsearch/index/IndexSettings.java</file><file>core/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/EngineConfig.java</file><file>core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java</file><file>core/src/main/java/org/elasticsearch/index/engine/ShadowEngine.java</file><file>core/src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/shard/RefreshListeners.java</file><file>core/src/main/java/org/elasticsearch/index/shard/ShadowIndexShard.java</file><file>core/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>core/src/main/java/org/elasticsearch/index/translog/TranslogWriter.java</file><file>core/src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/bulk/BulkShardRequestTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/BroadcastReplicationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/ReplicationOperationTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportReplicationActionTests.java</file><file>core/src/test/java/org/elasticsearch/action/support/replication/TransportWriteActionTests.java</file><file>core/src/test/java/org/elasticsearch/aliases/IndexAliasesIT.java</file><file>core/src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteIT.java</file><file>core/src/test/java/org/elasticsearch/document/ShardInfoIT.java</file><file>core/src/test/java/org/elasticsearch/index/WaitUntilRefreshIT.java</file><file>core/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/all/AllFieldMapperPositionIncrementGapTests.java</file><file>core/src/test/java/org/elasticsearch/index/shard/RefreshListenersTests.java</file><file>core/src/test/java/org/elasticsearch/index/translog/TranslogTests.java</file><file>core/src/test/java/org/elasticsearch/routing/AliasRoutingIT.java</file><file>core/src/test/java/org/elasticsearch/routing/SimpleRoutingIT.java</file><file>core/src/test/java/org/elasticsearch/search/child/ChildQuerySearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearch2xIT.java</file><file>modules/lang-groovy/src/test/java/org/elasticsearch/messy/tests/BulkTests.java</file><file>modules/percolator/src/test/java/org/elasticsearch/percolator/PercolatorIT.java</file></files><comments><comment>Add support for waiting until a refresh occurs</comment></comments></commit></commits></item><item><title>Transport Client: Adding more nodes causes more scheduled reconnect tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1062</link><project id="" key="" /><description>Transport Client: Adding more nodes causes more scheduled reconnect tasks, which can cause more load since there are connections that are not really needed.
</description><key id="1111774">1062</key><summary>Transport Client: Adding more nodes causes more scheduled reconnect tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-24T18:32:57Z</created><updated>2011-06-24T18:33:37Z</updated><resolved>2011-06-24T18:33:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file></files><comments><comment>Transport Client: Adding more nodes causes more scheduled reconnect tasks, closes #1062.</comment></comments></commit></commits></item><item><title>Get API: Make type optional</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1061</link><project id="" key="" /><description>Type is now optional when getting a document (which means one can get it just by the id). For the REST API, use the `_all` value for the type in this case.
</description><key id="1109324">1061</key><summary>Get API: Make type optional</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-24T10:43:21Z</created><updated>2011-07-02T20:45:19Z</updated><resolved>2011-06-24T10:43:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-07-01T15:05:20Z" id="1483404">Should this be extended to the delete API as well?
</comment><comment author="kimchy" created="2011-07-02T20:45:19Z" id="1490455">Its problematic because of the semantics of the delete API. This Get API means that the first doc will be returned (because of the semantics of get) even if there are same ids on different types.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/Client.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/get/GetRequestBuilder.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/GetActionTests.java</file></files><comments><comment>Get API: Make type optional, closes #1061.</comment></comments></commit></commits></item><item><title>Realtime GET</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1060</link><project id="" key="" /><description>Realtime GET support allows to get a document once indexed regardless of the "refresh rate" of the index. It is enabled by default.

In order to disable realtime GET, one can either set `realtime` parameter to `false`, or globally default it to by setting the `action.get.realtime` to `false` in the node configuration.
# Realtime GET and stored fields

When getting a document, one can specify `fields` to fetch from it. They will, when possible, be fetched as stored fields (fields mapped as stored in the mapping). When using realtime GET, there is no notion of stored fields (at least for a period of time, basically, until the next flush), so they will be extracted from the source itself (note, even if source is not enabled). It is a good practice to assume that the fields will be loaded from source when using realtime GET, even if the fields are stored.
</description><key id="1108552">1060</key><summary>Realtime GET</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-24T06:38:35Z</created><updated>2011-06-30T08:02:40Z</updated><resolved>2011-06-24T06:39:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2011-06-24T07:33:15Z" id="1430851">Dude. Awesome.
</comment><comment author="stephane-bastian" created="2011-06-24T10:01:59Z" id="1431450">+1. I'll definitely give it a spin today
Thanks Shay. 
</comment><comment author="mahendra" created="2011-06-24T11:23:03Z" id="1431746">Definitely, ES is getting more closer to CouchDB :-)
</comment><comment author="kimchy" created="2011-06-24T12:14:16Z" id="1431963">@stephane-bastian cool, would love an early feedback on this
</comment><comment author="aparo" created="2011-06-25T20:03:11Z" id="1439028">Kimchy this feature is fantastic, now we can use it as a better datastore.
</comment><comment author="talsalmona" created="2011-06-26T03:20:04Z" id="1440033">Great! 
</comment><comment author="oravecz" created="2011-06-27T18:05:24Z" id="1449217">Does this mean a fetch by id will return this "soon to be indexed" document, or will it be part of any query result which includes the document?
</comment><comment author="kimchy" created="2011-06-27T18:06:47Z" id="1449230">When a document is indexed, its indexed, its not "soon to be indexed". When it becomes visible for search is the question (and thats the async refresh). Fetch by Id will work even if it has not been refreshed yet.
</comment><comment author="stephane-bastian" created="2011-06-30T08:02:40Z" id="1473415">Hi Shay, 
Unfortunately I got sidetracked by some higher priority tasks. Things are settling down and I hope to test the RT get feature today or tomorrow 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/get/SimpleGetActionBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/get/GetRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/BytesHolder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/compress/lzf/LZF.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/stream/BytesStreamInput.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentGenerator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentGenerator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/smile/SmileXContentGenerator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/fs/FsTranslogFile.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/get/RestGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/support/RestXContentBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/SearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/mvel/MvelScriptEngineService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/lookup/SourceLookup.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/engine/AbstractSimpleEngineTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/GetActionTests.java</file><file>plugins/lang/groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java</file><file>plugins/lang/python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java</file></files><comments><comment>Realtime GET, closes #1060.</comment></comments></commit></commits></item><item><title>Fix NullPointerException when search request partially fails on one or mo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1059</link><project id="" key="" /><description>I had one shard missing and kept getting NPEs on master instead of a partially failed response because rest status in ShardSearchFailure wasn't initialized. The issue seems to be related to #1035.
</description><key id="1104500">1059</key><summary>Fix NullPointerException when search request partially fails on one or mo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-23T14:56:54Z</created><updated>2014-07-16T21:56:32Z</updated><resolved>2011-06-24T06:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-24T06:42:36Z" id="1430675">Pushed, thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>documentation mistake</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1058</link><project id="" key="" /><description>It seems that a comma is missing in the following example :

'Filtering Executed Queries' section in http://www.elasticsearch.org/guide/reference/api/percolate.html

curl -XPUT localhost:9200/_percolator/test/kuku -d '{
    "color" : "blue"
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    }
}'
</description><key id="1104097">1058</key><summary>documentation mistake</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iorixxx</reporter><labels /><created>2011-06-23T13:31:43Z</created><updated>2011-06-23T21:07:46Z</updated><resolved>2011-06-23T21:07:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-06-23T21:07:46Z" id="1428430">Thanks, fixed.
Note that you can open tickets for documentation in https://github.com/elasticsearch/elasticsearch.github.com/issues
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add initial implementaion of zookeeper-based discovery service.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1057</link><project id="" key="" /><description>This is an initial implementation of a ZooKeeper-based discovery plugin.

Usage:
- Download ZooKeeper 3.3.3 from http://zookeeper.apache.org/releases.html 
- Unzip the ZooKeeper archive into a directory, rename conf/zoo-sample.cnf into zoo.cnf and modify dataDir= line to point to a directory on your machine. Start ZooKeeper by running bin/zkServer.sh start.
- Install ZooKeeper plugin to ES
- Assuming that you are running ZooKeeper on the port  2181 (default),  add the following lines to config/elasticsearch.yml file

```
zookeeper:
    enabled: true
    host: localhost:2181
discovery:
    type: zoo_keeper
```
- Start ES 
</description><key id="1101739">1057</key><summary>Add initial implementaion of zookeeper-based discovery service.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-23T00:50:51Z</created><updated>2014-06-14T15:55:17Z</updated><resolved>2011-07-18T16:47:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="otisg" created="2011-06-29T20:57:41Z" id="1470512">Interesting.  Is the main idea here to make ES use ZK instead of its own Zen?
</comment><comment author="imotov" created="2011-06-29T21:10:37Z" id="1470619">Yes, this is an alternative to Zen discovery. It requires some initial setup but it should prevent split brain condition that sometimes occurs on unreliable networks and overloaded nodes. It still uses small portion of Zen Discovery for state publishing (I am planning to add ZooKeeper-based state publishing in the next iteration). But master election and fault detection is done using ZooKeeper. It's also possible to post ES settings into a ZooKeeper node. 
</comment><comment author="kimchy" created="2011-06-30T00:24:52Z" id="1471736">Heya, even with this zookeeper module, there is still work left to be done when it comes to handling cases where there aren't enough masters in the cluster (or zookeeper decides so). I am working on it now in the zen discovery, which can then be applied to the ZK module, will ping when I push it. Basically, what I am working on now will also make the zookeeper one a bit less relevant since zen will be more resilent to split brains (what we talked about before on minimum master nodes in a cluster).

One interesting aspect in ZK is the option to break down the cluster state into its discrete elements, and only apply and listen to delate changes (where now, the whole state is published in zen), though, it would be interesting to see at what cluster size (indices / shards wise) it really makes a difference, especially with the improvements in master and local gateawy.
</comment><comment author="imotov" created="2011-06-30T02:07:39Z" id="1472184">ZooKeeper uses a fixed list of ZooKeeper nodes, so it’s quite easy for it to decide if quorum is present or not. It doesn’t have to do discovery, only leader election, which simplifies things. In other words split brain is solved on ZooKeeper level and cluster can work even if there is only one potential master available.

I think fixing split brain in Zen would be great. As you remember, I tried to do it by implementing quorum in Zen Discovery. I went through several iterations starting with preventing master election when quorum is not met and ending with an attempt to implement a modified version of Bully algorithm that wouldn’t cause master reelection every time a node with a higher ID shows up. I still have some code from these attempts: (https://github.com/imotov/elasticsearch/commits/election-quorum). This code is pretty useless from implementation perspective, but it has a number of potentially interesting tests that are simulating different network failure scenarios. 

At the end, I came to the conclusion that for an election algorithm to be robust it has to have several phases with nodes committing to a potential master and then a master announcing its election and some retry logic when nodes die in the middle of the process. Otherwise, the same node might be counted in quorum twice by two potential masters or election might never end. And these phases complicate things a lot. On the top of it, the whole idea of specifying quorum in a config file was somewhat clashing with the idea of easy and dynamic cluster resizing by simply adding or shutting down a node and waiting for the green cluster state. 

With ZooKeeper Discovery, I don’t have to specify quorum, I can easily modify recover_after_data_nodes value (or any other global settings) in one place and I don’t have to worry about updating ping.unicast.hosts list. So, for me, it looked like an easy solution for the problems that we had. 
</comment><comment author="imotov" created="2011-06-30T02:30:16Z" id="1472279">By the way, here is a short description of the ZooKeeper Discovery process:

In short, ES ZooKeeper Discovery plug-in is using ZooKeeper as an atomic, reliable, distributed database for node fault detection, master election and sharing common settings. It uses the fact that ZooKeeper can maintain a tree of nodes in which each node can have an associated value as well as child node, these nodes can be created and read atomically and clients can be notified about updates.

 The ZooKeeper discovery process can be divided in 3 logical steps.

### 1. Initialization and Registration

Upon ES node start, the ZooKeeper discovery plugin connects to ZooKeeper and creates the following persistent node if it doesn’t already exist:

/es/_cluster name_/nodes

Then ES node creates an ephemeral ZK node /es/_cluster name_/nodes/_node-id_ with serialized content of local DiscoveryNode.

### 2. Master discovery

After registration ES node first tries to read ZK node /es/_cluster name_/leader and starts watching this node. Existence of this node indicates that cluster already has a master elected. In this case ES node stops the master discovery process and awaits for the cluster state to send to it by the master. 

If ZK node /es/_cluster name_/leader doesn’t exist and ES node is eligible to be elected as master, ES node tries to create ephemeral ZK node /es/_cluster name_/leader. This is an atomic operation and if multiple ES nodes try to create this node at the same time only one of them succeeds. If ZK node creation was successful, ES node that created the ZK node assumes role of the cluster master. If ZK node creation failed, ES node goes back to trying to read ZK node /es/_cluster name_/leader. 

If ZK node /es/_cluster name_/leader doesn’t exist and ES is not eligible to be elected as a master, it sets NO_MASTER_BLOCK (if it’s not initial discovery) and waits for node creation watcher to be triggered. As soon as the ZK node is created, ES node restarts the master discovery process.

The watcher that is set during read operation on the leader ZK node is also triggered when node disappears. Leader node disappearance is causing the restarts the master discovery process. Because the /es/_cluster name_/leader node is ephemeral, it disappears as soon as the ES master node that created it disconnects from the system or stops responding to pings. It ensures that cluster master is always available

### 3. Node List Update

If ES node is elected as a master, it reads the list of children of the /es/_cluster name_/nodes node and starts watching for any changes in the list. Then ES updates the list of nodes in the cluster state to match the list of nodes in ZK. When the watcher on the /es/_cluster name_/nodes node is triggered the Node List Update operation is repeated. In other words ephemeral nodes in es/_cluster name_/nodes perform the role of node fault detections.

### A Few Important ZooKeeper Notes:

A watcher set during any operation is only triggered once. For example, if ES node reads list of nodes and sets a watcher for the node list changes, this watcher will be triggered only once even if two children nodes are added. ES uses trigger only as indicator that change occurred, rereads complete list of children and sets the watcher at the same time.

There are two exceptions that could be caused by loss of connectivity with ZK cluster: ConnectionLossException and SessionExpiredException. The first exception can be thrown during any network hiccups and ZK client simply retries to reconnect if it occurs. When SessionExpiredException is thrown, the ZK session gets closed. In this case, all ephemeral nodes associated with disconnected node are removed it disconnected node has to restart registration and discovery process. 

There is a potential issue that can occur when Master looses connection with ZK but doesn’t loose connection with other nodes. In this case ZK may decide that master is dead and remove its ephemeral node cause reelection, but the disconnected master might not know about it yet and might try to publish state to other nodes. It will not be a problem when we switch to ZK-based state publishing. Meanwhile, I am thinking of adding ZK connection check before publishing state to prevent this problem. 
</comment><comment author="saj" created="2012-12-20T00:36:58Z" id="11555690">G'day @imotov, what became of this feature?
</comment><comment author="imotov" created="2012-12-20T01:11:28Z" id="11556471">@saj this: https://github.com/sonian/elasticsearch-zookeeper
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dynamic Templates path_match of "field.*" is somewhat ambiguous </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1056</link><project id="" key="" /><description>The behavior of a path_match of "tags.*" could either be used to find something in the root of the document or try and match something under tags depending on what the mapping looks like.  This is odd and somewhat inconsistent behavior.

I have a document I'm trying to index that looks like this:

```
curl -XPOST 'http://localhost:9200/test/entry/' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "tags": {
        "Pros": ["Fast", "Stable"],
        "Cons": ["Complex"]
    }
}'
```

I want to make sure that any type of tag that gets added is not analyzed.  So, I create a dynamic template:

```
curl -XPUT 'http://localhost:9200/test/entry/_mapping' -d '
{
    "entry" : {
        "dynamic_templates" : [
            {
                "keep_original" : {
                    "path_match" : "tags.*",
                    "mapping" : {
                        "type" : "string",
                        "index": "not_analyzed",
                        "store": "yes"
                    }
                }
            }
        ]        
    }
}'
```

If this is a fresh mapping, this setup will be incorrect when a document is indexed because "tags.*" will match "tags" and a mapping is setup for "tags" as a string type, even if the data that is coming in is an object.  However, if there is an existing mapping that sets up "tags" as an object, this will have the expected output.  

After MANY hours of trying to figure out why my tags was being set as a string and comparing to an implementation where I had an existing mapping with a dynamic template that was working right, I figured out I needed to do this:  

```
curl -XPUT 'http://localhost:9200/test/entry/_mapping' -d '
{
    "entry" : {
        "properties": {
            "tags": {"dynamic" : "true", "type": "object"}
        },         
        "dynamic_templates" : [
            {
                "keep_original" : {
                    "path_match" : "tags.*",
                    "mapping" : {
                        "type" : "string",
                        "index": "not_analyzed",
                        "store": "yes"
                    }
                }
            }
        ]        
    }
}'
```

I think some cases you want "tags._" to match things in the root directory and sometimes you want "tags._" to only match things underneath an object of "tags".  Maybe there could be something like "tags.+" to handle the case where you want to make sure something is there?
</description><key id="1100973">1056</key><summary>Dynamic Templates path_match of "field.*" is somewhat ambiguous </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mihalik</reporter><labels /><created>2011-06-22T21:38:25Z</created><updated>2013-04-16T07:38:12Z</updated><resolved>2013-04-16T07:38:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="atdixon" created="2013-04-15T16:26:45Z" id="16395644">This issue is fixed, should be closed.
</comment><comment author="s1monw" created="2013-04-16T07:38:12Z" id="16430561">thanks for pinging... closing...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for ?fields=* in the Get API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1055</link><project id="" key="" /><description>When searching, one can provide the `?fields=*` GET parameter (or `{"query":{...},"fields":"*"}` in QueryDSL.

However this feature is not available when GETting a document:

```
curl -XGET 'host:port/index/type/documentid?fields=*'
{
  "_index" : "index",
  "_type" : "type",
  "_id" : "documentid",
  "_version" : 1
}
```

`?fields=*` is simply not matched to a field name as no wildcard matching or special handling is done.

If one has `_source` disabled in the mapping, this is the only way - apart from painfully listing every possible field - to get the doc content.

**Request:**
It would be nice to have `?fields=*` listing each and every field contained in the GETted document.
</description><key id="1092031">1055</key><summary>Support for ?fields=* in the Get API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-06-21T14:15:40Z</created><updated>2014-07-08T12:20:38Z</updated><resolved>2014-07-08T12:20:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2012-11-21T15:03:23Z" id="10600128">A simple workaround is to use an "ids" query:

``` bash
$curl -XGET 'host:port/index/type/_search' -d '{
  "fields": "*",
  "query": {
    "ids": {
      "type": "type",
      "values": [
        "1234"
      ]
    }
  }
}'
```
</comment><comment author="Lensi" created="2013-02-28T14:51:39Z" id="14236620">Multi Get has the same problem. Specifying "fields": "*" in the body like for queries doesn't help either.
</comment><comment author="javanna" created="2013-10-18T18:55:08Z" id="26620699">The problem here is that the `fields` option was meant to work mainly with lucene stored fields, and get and multi_get work in real-time, which means that the documents can be retrieved from the transaction log if not available yet for search, thus not from lucene. Saying `fields=*` is a shortcut to say "give me all the fields that are stored in lucene", which is hard to do when there's no lucene underneath (in case we are retrieving from the translog).

I would suggest you to have a look at a new feature though, that will be available with 1.0, which allows to have finer control over `_source` retrieval for all the relevent apis. The related issue is #3301.
</comment><comment author="clintongormley" created="2014-07-08T12:20:38Z" id="48328630">See @javanna's suggestion above. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: KeywordMarkerFilter and StemmerOverrideFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1054</link><project id="" key="" /><description>Testing script for **KeywordMarkerFilter**:

```
curl -X DELETE 'localhost:9200/test?pretty=1'
echo

curl -X PUT 'localhost:9200/test?pretty=1' -d '{    
    "settings" : {
        "index" : { "number_of_shards" : 1, "number_of_replicas" : 0 },
        "analysis" : {
            "analyzer" : {
                "snowball" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [ "standard", "snowball" ]
                },
                "snowball_with_keywords" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [ "standard", "keyword_marker", "snowball" ]
                }
            },
            "filter" : {
                "keyword_marker" : {
                    "type" : "keyword_marker",
                    "keywords" : [ "manageable", "Managed" ]
                }
            }
        }
    }
}'
echo

curl -XGET 'http://localhost:9200/_cluster/health?wait_for_status=green&amp;pretty=1&amp;timeout=5s'

echo '&gt;&gt;&gt; snowball'; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=snowball'               -d "manager managed manageable manages Managed"
echo '&gt;&gt;&gt; keywords'; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=snowball_with_keywords' -d "manager managed manageable manages Managed"
echo;
```

Testing script for **StemmerOverrideFilter**:

```
curl -X DELETE 'localhost:9200/test?pretty=1'
echo

curl -X PUT 'localhost:9200/test?pretty=1' -d '{    
    "settings" : {
        "index" : { "number_of_shards" : 1, "number_of_replicas" : 0 },
        "analysis" : {
            "analyzer" : {
                "snowball" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [ "standard", "snowball" ]
                },
                "snowball_with_override" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [ "standard", "stemmer_override", "snowball" ]
                }
            },
            "filter" : {
                "stemmer_override" : {
                    "type" : "stemmer_override",
                    "rules" : [ "manageable =&gt; ultramanageable", "Booked =&gt; Books" ]
                }
            }
        }
    }
}'
echo

curl -XGET 'http://localhost:9200/_cluster/health?wait_for_status=green&amp;pretty=1&amp;timeout=5s'

echo '&gt;&gt;&gt; snowball'; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=snowball'               -d "manager managed manageable manages Booked"
echo '&gt;&gt;&gt; keywords'; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=snowball_with_override' -d "manager managed manageable manages Booked"
echo;
```
</description><key id="1092018">1054</key><summary>Analysis: KeywordMarkerFilter and StemmerOverrideFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-06-21T14:13:28Z</created><updated>2014-07-16T21:56:33Z</updated><resolved>2011-06-24T07:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-24T07:52:52Z" id="1430910">Pushed, cool!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: expose Lucene StemmerOverrideFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1053</link><project id="" key="" /><description>Ref: http://lucene.apache.org/java/3_2_0/api/all/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.html
</description><key id="1091652">1053</key><summary>Analysis: expose Lucene StemmerOverrideFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-21T13:00:04Z</created><updated>2013-06-05T13:03:41Z</updated><resolved>2011-06-24T07:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StemmerOverrideTokenFilterFactory.java</file></files><comments><comment>Analysis: expose Lucene StemmerOverrideFilter. Closes #1053</comment></comments></commit></commits></item><item><title>Analysis: expose Lucene KeywordMarkerFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1052</link><project id="" key="" /><description>Ref: http://lucene.apache.org/java/3_2_0/api/all/org/apache/lucene/analysis/KeywordMarkerFilter.html
</description><key id="1091639">1052</key><summary>Analysis: expose Lucene KeywordMarkerFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-21T12:56:09Z</created><updated>2013-06-05T13:01:36Z</updated><resolved>2011-06-24T07:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/KeywordMarkerTokenFilterFactory.java</file></files><comments><comment>Analysis: expose Lucene KeywordMarkerFilter. Closes #1052</comment></comments></commit></commits></item><item><title>Allow to disable automatic date detection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1051</link><project id="" key="" /><description>It will decide some things are dates, then blow up when they aren't, resulting in things not getting indexed. Broken off of #604

The mapping setting should be placed on the root object mapping, and called `date_detection`. Set it to `false` to disable it. For example:

```
{
    "my_type" : {
        "date_detection" : false
    }
}
```
</description><key id="1089494">1051</key><summary>Allow to disable automatic date detection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darkhelmet</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-21T03:24:00Z</created><updated>2011-07-10T20:24:34Z</updated><resolved>2011-07-10T20:24:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file></files><comments><comment>Allow to disable automatic date detection, closes #1051.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file></files><comments><comment>Allow to disable automatic date detection, closes #1051.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Allow to disable automatic date detection, closes #1051.</comment></comments></commit></commits></item><item><title>Remove unnecessary refresh after delete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1050</link><project id="" key="" /><description>These refresh calls are no longer needed since #1047 was fixed.
</description><key id="1088385">1050</key><summary>Remove unnecessary refresh after delete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-20T22:02:11Z</created><updated>2014-07-16T21:56:34Z</updated><resolved>2011-07-06T14:57:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-06T14:57:07Z" id="1512291">thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Precolator; Ability to use filters especially geo filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1049</link><project id="" key="" /><description>It would be awsome; to have precolator queries that can user "filters"; 

this can be very useful for geo distance filtering for instance;

therefore I can get back matching queries that are filtered to certain location that is too cool for school
</description><key id="1084120">1049</key><summary>Precolator; Ability to use filters especially geo filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hookercookerman</reporter><labels /><created>2011-06-20T10:58:28Z</created><updated>2011-06-20T13:20:27Z</updated><resolved>2011-06-20T11:07:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-06-20T11:07:04Z" id="1401900">Just use a filtered query: http://www.elasticsearch.org/guide/reference/query-dsl/filtered-query.html
or if you want just a filter, no query, then use a constant_score query: http://www.elasticsearch.org/guide/reference/query-dsl/constant-score-query.html
</comment><comment author="hookercookerman" created="2011-06-20T11:20:02Z" id="1401950">thanks for reply;

yep have been trying filtered query however the filter does not seem to be applied; Also If you could show me a working example of  a geo_distance filter for a percolate document that would be just amazing;

curl -XPUT localhost:9200/_percolator/photos/not_filtering -d '{
  "filtered" : {
        "query" : {
            "term" : {"tag" : "not_filtering"}
        },
        "filter" : {
            "range" : {
                "age" : { "from" : 10, "to" : 20 }
            }
        }
    }
}'

curl -XGET localhost:9200/photos/photo/_percolate -d '{
  "doc" :{
    "tag" : "not_filtering",
    "age" : 9
  }
}'

{"ok":true,"matches":["not_filtering"]}
</comment><comment author="clintongormley" created="2011-06-20T11:30:54Z" id="1402001">Hiya

Your `filtered` clause needs to be nested inside a query element.

```
PUT localhost:9200/_percolator/photos/not_filtering -d '{
    "query": {
        "filtered" : {
            "query" : {
                "term" : {"tag" : "not_filtering"}
            },
            "filter" : {
                "range" : {
                    "age" : { "from" : 10, "to" : 20 }
                },
            }
        }
    }
}'
```

Here I've combined the range filter with a geo_distance filter:

```
PUT localhost:9200/_percolator/photos/not_filtering -d '{
    "query": {
        "filtered" : {
            "query" : {
                "term" : {"tag" : "not_filtering"}
            },
            "filter" : {
                "and": [
                    "range" : {
                        "age" : { "from" : 10, "to" : 20 }
                    },
                    "geo_distance": {
                        "distance": "100km",
                        "my_point_field": {
                            "lat": 40,
                            "lon": -70
                        }                    
                    }
                ]
            }
        }
    }
}'
```

Note: `my_point_field` needs to be mapped as a `geo_point` field before any of the geo filters will work.

See http://www.elasticsearch.com/docs/elasticsearch/mapping/geo_point/
</comment><comment author="hookercookerman" created="2011-06-20T11:33:55Z" id="1402019">u are legend
</comment><comment author="clintongormley" created="2011-06-20T13:20:27Z" id="1402562">Hiya

This should really go to the mailing list.

OK - I tried it out, and it seems that the mapping needs to exists (and
be correct) before you can percolate against it.

This works:

(note the not_analyzed setting for 'tag')

# [Mon Jun 20 15:17:21 2011] Protocol: http, Server: 192.168.5.103:9200

curl -XPUT 'http://127.0.0.1:9200/photos/?pretty=1'  -d '
{
   "mappings" : {
      "photo" : {
         "properties" : {
            "tag" : {
               "index" : "not_analyzed",
               "type" : "string"
            },
            "age" : {
               "type" : "integer"
            }
         }
      }
   }
}
'

# [Mon Jun 20 15:17:21 2011] Response:

# {

# "ok" : true,

# "acknowledged" : true

# }

# [Mon Jun 20 15:17:26 2011] Protocol: http, Server: 192.168.5.103:9200

curl -XPUT
'http://127.0.0.1:9200/_percolator/photos/not_filtering?pretty=1'  -d '
{
   "query" : {
      "filtered" : {
         "query" : {
            "term" : {
               "tag" : "not_filtering"
            }
         },
         "filter" : {
            "range" : {
               "age" : {
                  "to" : 20,
                  "from" : 10
               }
            }
         }
      }
   }
}
'

# [Mon Jun 20 15:17:26 2011] Response:

# {

# "ok" : true,

# "_index" : "_percolator",

# "_id" : "not_filtering",

# "_type" : "photos",

# "_version" : 11

# }

# [Mon Jun 20 15:17:29 2011] Protocol: http, Server: 192.168.5.103:9200

curl -XGET 'http://127.0.0.1:9200/photos/photo/_percolate?pretty=1'  -d
'
{
   "doc" : {
      "tag" : "not_filtering",
      "age" : 11
   }
}
'

# [Mon Jun 20 15:17:29 2011] Response:

# {

# "ok" : true,

# "matches" : [

# "not_filtering"

# ]

# }

# [Mon Jun 20 15:17:33 2011] Protocol: http, Server: 192.168.5.103:9200

curl -XGET 'http://127.0.0.1:9200/photos/photo/_percolate?pretty=1'  -d
'
{
   "doc" : {
      "tag" : "not_filtering",
      "age" : 9
   }
}
'

# [Mon Jun 20 15:17:33 2011] Response:

# {

# "ok" : true,

# "matches" : []

# }
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update Settings: Allow to change non dynamic settings on a closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1048</link><project id="" key="" /><description>Allow to change non dynamic settings (settings that can be changed on a live open index) on a close index. Once its opened, they will be taken into account.
</description><key id="1080796">1048</key><summary>Update Settings: Allow to change non dynamic settings on a closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-19T14:03:17Z</created><updated>2011-06-19T14:15:17Z</updated><resolved>2011-06-19T14:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/indices/settings/UpdateSettingsTests.java</file></files><comments><comment>Update Settings: Allow to change non dynamic settings on a closed index, closes #1048.</comment></comments></commit></commits></item><item><title>Delete API: Using refresh parameter might not refresh delete operation on replica</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1047</link><project id="" key="" /><description>Delete API: Using refresh parameter might not refresh delete operation on replica
</description><key id="1079652">1047</key><summary>Delete API: Using refresh parameter might not refresh delete operation on replica</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-19T06:14:51Z</created><updated>2011-06-19T06:15:21Z</updated><resolved>2011-06-19T06:15:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file></files><comments><comment>Delete API: Using refresh parameter might not refresh delete operation on replica, closes #1047.</comment></comments></commit></commits></item><item><title>Unexpected results while searching for a query with trailing spaces analyzed using Keyword analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1046</link><project id="" key="" /><description>Steps to reproduce:

&lt;pre&gt;
curl -XDELETE localhost:9200/twitter
curl -XPOST localhost:9200/twitter -d '
{"index": 
  { "number_of_shards": 1,
    "analysis": {
       "filter": {
                  "mynGram" : {"type": "edgeNGram", "min_gram": 1, "max_gram": 10}
                 },
       "analyzer": { "a1" : {
                    "type":"custom",
                    "tokenizer": "keyword",
                    "filter": ["lowercase", "mynGram"]
                    },
                    "a2":{
                        "type":"keyword"
                    }
                  } 
     }
  }
}
}'

curl -XPUT localhost:9200/twitter/tweet/_mapping -d '{
    "seq" : {
        "index_analyzer" : "a1",
        "search_analyzer" : "a2", 
        "date_formats" : ["yyyy-MM-dd", "dd-MM-yyyy"],
        "properties" : {
            "message" : {"type" : "string" }
        }
    }}'

curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '
    {"user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "a new horizon"
}'
curl -XPUT 'http://localhost:9200/twitter/tweet/2' -d '
    {"user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "another new horizon"
}'
 &lt;/pre&gt;

Now searching for 'a ', my expected result would be 
&lt;pre&gt;
{"user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "a new horizon"
}
&lt;/pre&gt;

Instead, when I search I get this:

&lt;pre&gt;
 curl -XGET 'localhost:9200/twitter/_search?q=message:a+&amp;pretty=true&amp;analyzer=a2'   {
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.3125,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 0.3125, "_source" : {"user" : "kimchy","post_date" : "2009-11-15T14:12:12","message" : "a new horizon"}
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : 0.3125, "_source" : {"user" : "kimchy","post_date" : "2009-11-15T14:12:12","message" : "another new horizon"}
    } ]
  }
}
&lt;/pre&gt;


A similar issue occurs when using trailing unicode characters instead of trailing spaces in my query string.
</description><key id="1078209">1046</key><summary>Unexpected results while searching for a query with trailing spaces analyzed using Keyword analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asleepysamurai</reporter><labels /><created>2011-06-18T16:53:16Z</created><updated>2013-04-05T13:22:07Z</updated><resolved>2013-04-05T13:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T13:22:07Z" id="15955153">This is due to the parsing by the query string parser, not the keyword analyzer.  Use the `match` query instead
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bool filter is not cached by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1045</link><project id="" key="" /><description>According to the document(http://www.elasticsearch.org/guide/reference/query-dsl/bool-filter.html) bool filter is cached by default.
However, unlike other filter parsers, BoolFilterParser doesn't turn on cache unless explicitly set since it initializes cache as "boolean cache = false;" in parse method.
</description><key id="1077760">1045</key><summary>Bool filter is not cached by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IoriH</reporter><labels /><created>2011-06-18T13:38:33Z</created><updated>2011-06-19T04:35:54Z</updated><resolved>2011-06-19T04:35:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-18T18:00:39Z" id="1394054">Right, thats why the docs state that its not cached by default, only if you set it to be cached. Whats the problem?
</comment><comment author="IoriH" created="2011-06-19T00:45:11Z" id="1395096">Not being cached itself is not problem. The problem is it's not consistent with the guide. If the guide says "The result of the `bool` filter is automatically cached by default. The `_cache` can be set to `false` to turn it off.", I assume it is the case.
This is issue on document not code, perhaps?
</comment><comment author="kimchy" created="2011-06-19T04:35:53Z" id="1395478">Yea, the doc is wrong. Will fix it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms facet: count and from features</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1044</link><project id="" key="" /><description>Hi all,

I'd like to ask for 2 features for the terms facet that sound not too difficult to implement to me:
- Adding a count / total field to the result of a terms facet, that gives the number of different terms the facet returns. For the moment I ask for maxint results to be sure I have the correct number... 
- Adding something equivalent to the from / size of the query DSL. It would really help me to be able to say that I want results from 1000, with a size of 50, instead of saying I want the 1050 firsts and then have to use only the last 50 client-side. 

Please tell me if this is not clear. 

Thanks for your great work, we really enjoy ElasticSearch here at Mozilla!

Adrian
</description><key id="1076469">1044</key><summary>Terms facet: count and from features</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adngdb</reporter><labels /><created>2011-06-17T23:51:55Z</created><updated>2014-03-15T19:23:37Z</updated><resolved>2013-08-26T20:25:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="deinspanjer" created="2011-07-27T19:50:11Z" id="1666970">Adrian,  we are trying to find a few more ES engineers to work on the metrics team so that hopefully we can begin to tackle some of these feature improvements ourselves and contribute back as more than just users. :)

That said, I'll ping a few people and see what their thoughts are on this issue.
</comment><comment author="karussell" created="2012-07-18T10:26:26Z" id="7062463">+1 :) !
</comment><comment author="clintongormley" created="2012-08-11T09:26:05Z" id="7665312">+1
</comment><comment author="alexperetti" created="2012-08-11T09:29:32Z" id="7665334">+1! This would be crucial for building an analytics engine using ElasticSearch.
</comment><comment author="paulogaspar7" created="2012-10-22T11:07:09Z" id="9659835">+1
</comment><comment author="devilankur18" created="2012-12-27T01:43:47Z" id="11698612">+1

Any suggestion how to solve it until its not implemented ? 
</comment><comment author="jxerome" created="2013-01-17T12:49:55Z" id="12366616">+1
</comment><comment author="mahom" created="2013-04-12T20:08:43Z" id="16314823">+1
</comment><comment author="ferdynice" created="2013-04-13T17:07:24Z" id="16336760">+1
</comment><comment author="d2kagw" created="2013-05-10T23:22:25Z" id="17749470">+1
</comment><comment author="kimchy" created="2013-08-26T20:25:56Z" id="23291922">the plan is to have unique terms supports once the aggregation support is done #3300, using something like HyperLogLog.
</comment><comment author="kzachara" created="2014-01-06T11:12:11Z" id="31641448">Hi,
since #3300 is closed and aggregations are available in 1.0.0Beta2, is there any plan for unique term counts?
</comment><comment author="shulard" created="2014-01-15T17:12:59Z" id="32386531">+1 !!!
I think it will be a great enhancement for statistical analysis... For the moment we are forced to retrieve all facets terms and there is a performance gap when our database grow significantly.
</comment><comment author="aaneja" created="2014-01-15T23:36:17Z" id="32427693">+1. 
Is it possible to get a count of the number of unique fields a bucket-ing would generate without actually making a bucket ?
</comment><comment author="Tombar" created="2014-01-22T06:04:10Z" id="32995816">+1
</comment><comment author="rsilve" created="2014-01-23T06:20:38Z" id="33100058">+1
</comment><comment author="mhuggins7278" created="2014-01-30T03:17:06Z" id="33656975">+1 
</comment><comment author="SeyZ" created="2014-02-05T09:46:38Z" id="34151038">+1 :)
</comment><comment author="yildizib" created="2014-02-12T20:37:12Z" id="34914475">+1
is there any plan for unique term counts?
</comment><comment author="SeyZ" created="2014-02-14T14:41:26Z" id="35088748">@kimchy ?
</comment><comment author="jorgelbg" created="2014-03-02T23:54:28Z" id="36472428">Is any way of using this in the new 1.0 release? counting unique values in a field?
</comment><comment author="uboness" created="2014-03-03T15:25:16Z" id="36520190">@jorgelbg no... not yet. We do plan to add a unique count aggregation in the future though (based on HyperLogLog), but we need to add a new things first before we go about it.
</comment><comment author="jorgelbg" created="2014-03-03T15:31:08Z" id="36520840">Is there any workaround for this feature? Or to get the same value.

On 3/3/14, uboness notifications@github.com wrote:

&gt; @jorgelbg no... not yet. We do plan to add a unique count aggregation in the
&gt; future though (based on HyperLogLog), but we need to add a new things first
&gt; before we go about it.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1044#issuecomment-36520190
</comment><comment author="SeyZ" created="2014-03-03T19:52:59Z" id="36551497">@jorgelbg the only workarround is to retrieve all results of your query and counts the length of the return array. Not efficient of course.

@uboness Glad to hear that. Thanks!
</comment><comment author="jorgelbg" created="2014-03-03T20:28:27Z" id="36555417">Yeap, I was hopping that would be a "better" workaround, in my case
(working with a lot of tweets) that array could be very large.

On 3/3/14, Sandro Munda notifications@github.com wrote:

&gt; @jorgelbg the only workarround is to retrieve all results of your query and
&gt; counts the length of the return array. Not efficient of course.
&gt; 
&gt; @uboness Glad to hear that. Thanks!
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1044#issuecomment-36551497
</comment><comment author="redox" created="2014-03-06T09:00:48Z" id="36836348">Hey guys,

in the meantime you can use a plugin we've just released: [elasticsearch-cardinality-plugin](https://github.com/algolia/elasticsearch-cardinality-plugin).

This plugin extends Elasticsearch providing a fast &amp; memory-efficient way to estimate the cardinality (number of uniq terms) of a field. The field can be either string, numerical or boolean. It registers a new type of aggregation (`cardinality`) and a REST action (`_cardinality`).

Hope you'll like it as we do :)
</comment><comment author="SeyZ" created="2014-03-06T09:13:55Z" id="36837210">Thanks @redox for your comment. I will test your plugin!
</comment><comment author="shulard" created="2014-03-06T09:26:18Z" id="36838037">Thanks @redox, I wait for this tool from a long time now! I will test too while waiting for the HyperLogLog implementation...
</comment><comment author="cybervedaa" created="2014-03-07T01:09:45Z" id="36957885">@redox the plugin is very promising indeed. Exactly what i was looking for. Now, would it be possible to graph the cardinality in Kibana using one of the existing panel types?
</comment><comment author="jpountz" created="2014-03-13T18:34:17Z" id="37570224">&gt; the plan is to have unique terms supports once the aggregation support is done #3300, using something like HyperLogLog.

This was just done in #5426
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>java.lang.OutOfMemoryError: unable to create new native thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1043</link><project id="" key="" /><description>The use case to reproduce the error is quite unusual, but probably it shows some weak in the management of a thread pool.

I have one elastic search server 0.16.1 and a java webapp client. Both of them discover themselves via broadcast. Everything works fine. Then I close my laptop, it goes to sleep. A while after that I reopen it. From that point I have errors on the elasticsearch server. Every 30 sec I have:

```
[2011-06-17 20:27:49,752][WARN ][discovery.zen.ping.multicast] [McKenzie, Namor] failed to connect to requesting node [Magician][d-dJv8_MTCW_reKnhnYd-A][inet[/10.0.0.208:9301]]{client=true, data=false}
org.elasticsearch.transport.ConnectTransportException: [Magician][inet[/10.0.0.208:9301]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:512)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:473)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:126)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver$1.run(MulticastZenPing.java:354)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.net.ConnectException: connection timed out
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processConnectTimeout(NioClientSocketPipelineSink.java:371)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:283)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    ... 3 more
```

After some time, in the log I then see in the error output stream (not from log4j):

```
Exception in thread "elasticsearch[McKenzie, Namor]discovery#multicast#received-pool-16-thread-1" java.lang.OutOfMemoryError: unable to create new native thread
    at java.lang.Thread.start0(Native Method)
    at java.lang.Thread.start(Thread.java:658)
    at java.util.concurrent.ThreadPoolExecutor.addIfUnderMaximumPoolSize(ThreadPoolExecutor.java:727)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:657)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver.run(MulticastZenPing.java:350)
    at java.lang.Thread.run(Thread.java:680)
```

I have done a jstack on the server, see the attachement.

Then continuing every 30s, I got connection timed out exception. I can then see that the number of thread is diminuing, one less every 30s. In the hight load of logs, I also got this statck trace after a while, not sure if it is relevant:

```
[2011-06-17 20:29:05,382][WARN ][transport.netty          ] [McKenzie, Namor] Exception caught on netty layer [[id: 0x482d6445, /10.0.0.208:62813 =&gt; /10.0.0.208:9300]]
java.io.IOException: Operation timed out
    at sun.nio.ch.FileDispatcher.read0(Native Method)
    at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)
    at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:202)
    at sun.nio.ch.IOUtil.read(IOUtil.java:169)
    at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:321)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
```

On my webapp side, if I try to use an search feature, I get:

```
17.06.2011 21:12:43 ERROR [org.mortbay.log:?] /admin/search/user
java.lang.RuntimeException: org.springframework.web.util.NestedServletException: Request processing failed; nested exception is org.elasticsearch.discovery.MasterNotDiscoveredException: 
[...]
Caused by: org.elasticsearch.discovery.MasterNotDiscoveredException: 
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.onTimeout(TransportMasterNodeOperationAction.java:162)
    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:281)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
```
</description><key id="1075342">1043</key><summary>java.lang.OutOfMemoryError: unable to create new native thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-06-17T19:18:04Z</created><updated>2013-04-05T13:24:58Z</updated><resolved>2013-04-05T13:24:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nlalevee" created="2011-06-17T19:29:20Z" id="1390370">I think that to correctly reproduce the issue is to make nodes their IP change (maybe only one changing can make errors too ?). I have noticed that the ip the error logs above is "10.0.0.208" whereas after my laptop sleep, because I'm now home and not at work, my IP is actually 192.168.1.21.

To note to, I restarted only the elasticsearch server, it still fails. Then I only restarted the webapp. Still badly failing. And I could see that the number of thread in the elasticsearch server is then growing, probably it would have ended in another OutOfMemoryError. Restarting both of them make it work nicely and then the IP is the logs was then the 192.168.1.21, not the 10.0.0.208.
</comment><comment author="nlalevee" created="2011-06-17T19:33:11Z" id="1390392">I doesn't seems I can attach anything to this issue manager, so I'll paste here the meaningful (as far as I can tell) part of the stack trace I got just after the OutOfMemoryError:

```
"elasticsearch[cached]-pool-1-thread-346" daemon prio=5 tid=102be2000 nid=0x12a943000 in Object.wait() [12a942000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
    at java.lang.Object.wait(Native Method)
    - waiting on &lt;7d045cc88&gt; (a org.elasticsearch.common.netty.channel.DefaultChannelFuture)
    at java.lang.Object.wait(Object.java:443)
    at org.elasticsearch.common.netty.channel.DefaultChannelFuture.await0(DefaultChannelFuture.java:265)
    - locked &lt;7d045cc88&gt; (a org.elasticsearch.common.netty.channel.DefaultChannelFuture)
    at org.elasticsearch.common.netty.channel.DefaultChannelFuture.awaitUninterruptibly(DefaultChannelFuture.java:237)
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:510)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:473)
    - locked &lt;7d41ac4d0&gt; (a org.elasticsearch.transport.netty.NettyTransport)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:126)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver$1.run(MulticastZenPing.java:354)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)

"elasticsearch[cached]-pool-1-thread-345" daemon prio=5 tid=112696000 nid=0x12a840000 waiting for monitor entry [12a83f000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:466)
    - waiting to lock &lt;7d41ac4d0&gt; (a org.elasticsearch.transport.netty.NettyTransport)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:126)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver$1.run(MulticastZenPing.java:354)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
```
</comment><comment author="clintongormley" created="2013-04-05T13:24:58Z" id="15955272">No more reports of this in 2 years - assuming fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE with BooleanFilter(TermFilter(that matches nothing))</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1042</link><project id="" key="" /><description>I have a written plugin that constructs a tree of BooleanFilters using TermFilters and have encountered a NPE while using an unexisting term at query time.

**Here is the minimal plot:**
Take TermFilterParser for an example. Using this inspiration, create a new filter parser following the following steps.
Create and register a FilterParser.
In parse() do:

```
BooleanFilter filter = new BooleanFilter();
filter.add(new FilterClause(new TermFilter(new Term(fieldName, value)), BooleanClause.Occur.MUST));
```

Compile, distribute the plugin and try it:

```
{"query":{"filtered":{"query":{"match_all":{}},"filter":{"mynewfilter":{"thequeriesfield":"existing value"}}}}}
```

It works as excepted (ie like the corresponding simple "term" filter).
Then try with an unexisting value (unknown from the index).
You should get the following error:

```
[2011-06-17 15:52:37,194][DEBUG][action.search.type       ] [Mimic] [geophy][0], node[YAVwPL5qSM6qaArHM6EqIw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@74a60905]
org.elasticsearch.search.query.QueryPhaseExecutionException: [mynewfilter][0]: query[ConstantScore(BooleanFilter( mynewfilter:z))],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:280)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:224)
        at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:71)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.lucene.search.BooleanFilter.getDISI(BooleanFilter.java:47)
        at org.apache.lucene.search.BooleanFilter.getDocIdSet(BooleanFilter.java:62)
        at org.apache.lucene.search.DeletionAwareConstantScoreQuery$DeletionConstantWeight.scorer(DeletionAwareConstantScoreQuery.java:53)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:517)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:177)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:384)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:291)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:279)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:211)
        ... 9 more
```

The error comes from BooleanFilter.java at line 47.
Looking at Lucene 3.1.0 sources you find:

```
44      private DocIdSetIterator getDISI(ArrayList&lt;Filter&gt; filters, int index, IndexReader reader)
45      throws IOException
46      {
47 =&gt;     return filters.get(index).getDocIdSet(reader).iterator();
48      }
```

The NPE comes from filters.get(index).getDocIdSet(reader) returning null.
If you look at commit becf4baaa200c29903244a87fdd7c08c16ad4047 for file `modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/TermFilter.java` at line 49 (which is what is called by our line), you see that null is clearly returned.

I think this optimization is either a mistake, or buggy.
At the end of the function, something like this might do the job:

```
if (result == null) return new OpenBitSet();
return result;
```
</description><key id="1073963">1042</key><summary>NPE with BooleanFilter(TermFilter(that matches nothing))</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-06-17T14:51:27Z</created><updated>2011-06-17T18:17:46Z</updated><resolved>2011-06-17T18:17:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-17T16:20:32Z" id="1389195">Its not a bug in TermFilter, returning null value is valid. Use XBooleanFilter instead of BooleanFilter.
</comment><comment author="ofavre" created="2011-06-17T18:17:45Z" id="1389919">Ok... not very intuitive without any doc but I agree it's not a bug then.
I guess I can close the issue!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: backport KStem token filter from Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1041</link><project id="" key="" /><description>Lucene received KStem stemmer: https://issues.apache.org/jira/browse/LUCENE-152
KStem is not that aggressive as **snowball** and gives better results then **minimal_english** stemming filter.
The goal is to make KStem available in ES until Lucene 3.3 is released.
</description><key id="1073934">1041</key><summary>Analysis: backport KStem token filter from Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-17T14:45:16Z</created><updated>2011-06-17T14:54:32Z</updated><resolved>2011-06-17T14:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-06-17T14:54:32Z" id="1388634">A simple script that demonstrates application of **kstem** token filter:

```
curl -X DELETE 'localhost:9200/test?pretty=1'
echo

curl -X PUT 'localhost:9200/test?pretty=1' -d '{
    "settings" : {
        "index" : { "number_of_shards" : 1, "number_of_replicas" : 0 },
        "analysis" : {
            "analyzer" : {
                "kstem" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [ "standard", "lowercase", "stop", "kstem" ]
                },
                "snowball" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [ "standard", "lowercase", "stop", "snowball" ]
                },
                "minimal_english" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [ "standard", "lowercase", "stop", "minimal_english" ]
                }
            },
            "filter" : {
                "minimal_english" : {
                    "type" : "stemmer",
                    "name" : "minimal_english"
                }
            }
        }
    }
}'
echo

curl -XGET 'http://localhost:9200/_cluster/health?wait_for_status=green&amp;pretty=1&amp;timeout=5s'

echo; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=snowball'        -d "manager managed manageable manages mark's"
echo; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=minimal_english' -d "manager managed manageable manages mark's"
echo; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=kstem'           -d "manager managed manageable manages mark's"
echo;

text="Lucene was originally written by Doug Cutting. It was initially available for download from its home at the SourceForge web site. It joined the Apache Software Foundation’s Jakarta family of open source Java products in September 2001 and became its own top-level Apache project in February 2005. Until recently, it included a number of sub-projects, such as Lucene Java, Droids, Lucene.Net, Lucy, Mahout, Solr, Nutch, Open Relevance Project, PyLucene and Tika. Solr has been merged into the Lucene project itself and Mahout, Nutch and Tika have been moved to be independent top-level projects."

echo "&gt;&gt;&gt; snowball";        curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=snowball'        -d "$text"; echo;
echo "&gt;&gt;&gt; minimal_english"; curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=minimal_english' -d "$text"; echo;
echo "&gt;&gt;&gt; kstem";           curl 'localhost:9200/test/_analyze?format=text&amp;analyzer=kstem'           -d "$text"
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData1.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData2.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData3.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData4.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData5.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData6.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData7.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemData8.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemFilter.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/en/KStemmer.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/util/OpenStringBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/KStemTokenFilterFactory.java</file></files><comments><comment>Closes #1041</comment></comments></commit></commits></item><item><title>Tests cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1040</link><project id="" key="" /><description>Not sure if I should split these commits into separate pull requests or not.
</description><key id="1071463">1040</key><summary>Tests cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-17T04:12:13Z</created><updated>2014-07-16T21:56:34Z</updated><resolved>2011-06-19T08:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-19T08:14:49Z" id="1395715">pushed, thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parent Child: complex parent child mapping (&gt;2 hierarchy) might fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1039</link><project id="" key="" /><description>Parent Child: complex parent child mapping (&gt;2 hierarchy) might fail
</description><key id="1070203">1039</key><summary>Parent Child: complex parent child mapping (&gt;2 hierarchy) might fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-16T22:02:56Z</created><updated>2011-06-16T22:03:46Z</updated><resolved>2011-06-16T22:03:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file></files><comments><comment>Parent Child: complex parent child mapping (&gt;2 hierarchy) might fail, closes #1039.</comment></comments></commit></commits></item><item><title>River might not start properly after cluster shutdown (timing)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1038</link><project id="" key="" /><description>The retry logic to try and start a river should also handle index missing case if it has not propagated yet.
</description><key id="1066735">1038</key><summary>River might not start properly after cluster shutdown (timing)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-16T11:04:32Z</created><updated>2011-06-16T11:05:05Z</updated><resolved>2011-06-16T11:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/river/RiversService.java</file></files><comments><comment>River might not start properly after cluster shutdown (timing), closes #1038.</comment></comments></commit></commits></item><item><title>Add ec2 specific network hosts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1037</link><project id="" key="" /><description>Add options to set network hosts: `_ec2:privateIpv4_`, `_ec2:privateDns_`, `_ec2:publicIpv4_`, `_ec2:publicDns_`. Less verbost names include: `_ec2_` (private ip). `_ec2:publicIp_`, and `_ec2:privateIp_`. 

It is enabled automatically when installed the cloud aws plugin.
</description><key id="1063325">1037</key><summary>Add ec2 specific network hosts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-15T19:25:58Z</created><updated>2011-06-15T19:26:36Z</updated><resolved>2011-06-15T19:26:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/AwsEc2Service.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/cloud/aws/network/Ec2NameResolver.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java</file></files><comments><comment>Add ec2 specific network hosts, closes #1037.</comment></comments></commit></commits></item><item><title>ArrayIndexOutOfBoundsException during has_child query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1036</link><project id="" key="" /><description>I was unable to come up a with a scenario that will reproduce this reliably.  The problem is difficult to reproduce and timing of the search seems to cause the issue.  I'm hoping the stack trace might make it obvious where the problem is.  During a has_child query we will sometimes get the following stack trace:

[2011-06-15 12:24:59,985][DEBUG][action.search.type       ] [Fallen One] [echo][0], node[9BjO9kriT5i52ryyeST4NQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4c8a7c5]
org.elasticsearch.search.query.QueryPhaseExecutionException: [echo][0]: query[ConstantScore(org.elasticsearch.common.lucene.search.AndFilter@1419a704)],from[0],size[10],sort[&lt;custom:"provider_id": org.elasticsearch.index.field.data.strings.StringFieldDataType$1@5bfdde41&gt;,&lt;custom:"start_date": org.elasticsearch.index.field.data.longs.LongFieldDataType$1@780aac95&gt;]: Query Failed [Failed to execute child query [filtered(ConstantScore(org.elasticsearch.common.lucene.search.AndFilter@3b82c1e8))-&gt;FilterCacheFilterWrapper(_type:granule_minimum_bounding_rectangle)]]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:155)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 3
        at org.elasticsearch.index.cache.id.simple.SimpleIdReaderTypeCache.parentIdByDoc(SimpleIdReaderTypeCache.java:53)
        at org.elasticsearch.index.query.type.child.ChildCollector.collect(ChildCollector.java:72)
        at org.apache.lucene.search.Scorer.score(Scorer.java:90)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:519)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:177)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:332)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:148)
        ... 9 more

This is an example of the query that could cause the issue
{"query":{"filtered":{"query":{"match_all":{}},"filter":{"and":[{"has_child":{"type":"granule_minimum_bounding_rectangle","query":{"constant_score":{"filter":{"and":[{"and":[{"range":{"north":{"from":15.0,"to":90.0}}},{"range":{"south":{"from":-90.0,"to":15.001}}}]},{"or":[{"and":[{"or":[{"range":{"west":{"from":-180.0,"to":15.0001}}},{"range":{"east":{"from":14.9991,"to":180.0}}}]},{"term":{"antimeridian":true}}]},{"and":[{"range":{"west":{"from":-180.0,"to":15.0001}}},{"range":{"east":{"from":14.9991,"to":180.0}}}]}]}]}}}}},{"or":[{"term":{"provider_id":"PROV1"}},{"term":{"provider_id":"PROV2"}}]}]}}},"fields":["dataset_id","granule_ur"],"sort":["provider_id","start_date"],"from":0,"size":10}

I can provide more information if needed.
</description><key id="1062367">1036</key><summary>ArrayIndexOutOfBoundsException during has_child query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasongilman</reporter><labels /><created>2011-06-15T16:51:31Z</created><updated>2011-06-17T17:04:37Z</updated><resolved>2011-06-17T17:04:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-15T16:58:54Z" id="1375137">Which version are you using? Something similar was fixed in 0.16.2.
</comment><comment author="jasongilman" created="2011-06-15T17:04:29Z" id="1375174">We're using 0.16.2
</comment><comment author="kimchy" created="2011-06-16T21:48:45Z" id="1384585">Do you by any chance have a parent child mapping that spans several types? I think I spotted where this might happen in this case...
</comment><comment author="kimchy" created="2011-06-16T22:04:48Z" id="1384656">ok, don't know if this is the reason for the failure or not, but I pushed a fix for what I mentioned: https://github.com/elasticsearch/elasticsearch/issues/1039. Its in 0.16 branch as well.
</comment><comment author="dpilone" created="2011-06-17T03:48:11Z" id="1385898">I'm on the same project as Jason - do you mean multiple children (of different types) referencing a single parent?
</comment><comment author="kimchy" created="2011-06-17T08:23:00Z" id="1386767">For example, yes.
</comment><comment author="dpilone" created="2011-06-17T08:48:10Z" id="1386874">Yes - we have two "top level" types with three to four children each.  We'll give the patch you just pushed a shot.  I should have mentioned it earlier, but you're doing amazing work and we really appreciate how quickly you get patches turned around.  Thanks again. 
</comment><comment author="jasongilman" created="2011-06-17T17:04:04Z" id="1389441">I built the latest code in brake 0.16 and tested it out.  The problem seems to be gone now.  Thanks for the awesome work.  It's incredible how fast you get these issues fixed.
</comment><comment author="kimchy" created="2011-06-17T17:04:37Z" id="1389444">cool!, that was a tricky one :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: When a search request failed completely (all shards fail) return a proper HTTP status code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1035</link><project id="" key="" /><description>Search: When a search request failed completely (all shards fail) return a proper HTTP status code
</description><key id="1061157">1035</key><summary>Search: When a search request failed completely (all shards fail) return a proper HTTP status code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-15T13:26:50Z</created><updated>2011-06-15T13:29:09Z</updated><resolved>2011-06-15T13:29:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/ShardSearchFailure.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/RestStatus.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file></files><comments><comment>Search: When a search request failed completely (all shards fail) return a proper HTTP status code, closes #1035.</comment></comments></commit></commits></item><item><title>Wait for alias operations to be propagated to all nodes before returning </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1034</link><project id="" key="" /><description>Fixed logging level, renamed NodeAliasUpdatedTransportHandler, and move version check to the CountDownListener.  
</description><key id="1061044">1034</key><summary>Wait for alias operations to be propagated to all nodes before returning </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-15T13:04:50Z</created><updated>2014-07-16T21:56:35Z</updated><resolved>2011-06-16T16:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: When all shards fail on second / third phase, make sure to return a response with proper shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1033</link><project id="" key="" /><description /><key id="1060903">1033</key><summary>Search: When all shards fail on second / third phase, make sure to return a response with proper shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.0</label></labels><created>2011-06-15T12:28:00Z</created><updated>2011-06-15T12:28:55Z</updated><resolved>2011-06-15T12:28:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/controller/SearchPhaseController.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/InternalSearchResponse.java</file></files><comments><comment>Search: When all shards fail on second / third phase, make sure to return a response with proper shard failures, closes #1033.</comment></comments></commit></commits></item><item><title>Query DSL: Span Term Query wrongly parses when boost is provided, causing using it in span or to fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1032</link><project id="" key="" /><description>Query DSL: Span Term Query wrongly parses when boost is provided, causing using it in span or to fail
</description><key id="1060293">1032</key><summary>Query DSL: Span Term Query wrongly parses when boost is provided, causing using it in span or to fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-15T10:10:48Z</created><updated>2011-06-15T10:14:37Z</updated><resolved>2011-06-15T10:11:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="melix" created="2011-06-15T10:14:37Z" id="1372573">Great thanks !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Query DSL: Span Term Query wrongly parses when boost is provided, causing using it in span or to fail, closes #1032.</comment></comments></commit></commits></item><item><title>Analyzers for FLT / MLT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1031</link><project id="" key="" /><description>FLT and MLT are text queries. At the moment they use the default search analyzer, but they should actually allow overriding with an `analyzer` parameter. (by default, they will use the mapped analyzer, or default search analyzer)
</description><key id="1060030">1031</key><summary>Analyzers for FLT / MLT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-15T09:00:48Z</created><updated>2011-06-15T10:43:23Z</updated><resolved>2011-06-15T10:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file></files><comments><comment>Analyzers for FLT / MLT, closes #1031.</comment></comments></commit></commits></item><item><title>Wait for alias operations to be propagated to all nodes before returning </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1030</link><project id="" key="" /><description>Added wait for the cluster state propagation after changes to aliases as we discussed. Could you take a look?
</description><key id="1059005">1030</key><summary>Wait for alias operations to be propagated to all nodes before returning </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-15T03:10:09Z</created><updated>2014-07-16T21:56:35Z</updated><resolved>2011-06-15T11:09:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add 'other_terms' option for terms facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1029</link><project id="" key="" /><description>As discussed in this thread, here is a feature request for terms facet : http://elasticsearch-users.115913.n3.nabble.com/Terms-facet-getting-all-the-results-tp3051086p3055360.html

I would like to get within the terms facet result the count of all other terms that have not been returned (due to size limit).

Here is a test case :

``` bash
# Dropping index and Creating articles
curl -X DELETE "http://localhost:9200/articles"
curl -X POST "http://localhost:9200/articles/article" -d '{"tags" : ["one", "two", "three", "four", "five", "six", "seven"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"tags" : ["two", "three", "four", "five", "six", "seven"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"tags" : ["three", "four", "five", "six", "seven"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"tags" : ["four", "five", "six", "seven"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"tags" : ["five", "six", "seven"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"tags" : ["six", "seven"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"tags" : ["seven"]}'
```

``` bash
# Classic search with terms facet
curl -X POST "http://localhost:9200/articles/_search?pretty=true" -d '
  {
    "query" : { "match_all" : { } },
    "facets" : {
      "f_tags" : { "terms" : {"field" : "tags"} }
    }
  }
'
```

``` javascript
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 7,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "gkgAyLN6QWuxSyBLxxknIw",
      "_score" : 1.0, "_source" : {"tags":["two","three","four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "swVlq23SS9aoC8uHNB_LRA",
      "_score" : 1.0, "_source" : {"tags":["one","two","three","four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "oColEdY-SUmMPWvBWBEneQ",
      "_score" : 1.0, "_source" : {"tags":["six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "hk-9osIlRViESqEF7WPj_w",
      "_score" : 1.0, "_source" : {"tags":["three","four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "Ki9KA036TI6n7f6vD2gGWA",
      "_score" : 1.0, "_source" : {"tags":["four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "_bFnruq4TdyJQjHH5bTMrg",
      "_score" : 1.0, "_source" : {"tags":["five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "k1FzqoBwQ82PYNa1jgE-Tg",
      "_score" : 1.0, "_source" : {"tags":["seven"]}
    } ]
  },
  "facets" : {
    "f_tags" : {
      "_type" : "terms",
      "missing" : 0,
      "terms" : [ {
        "term" : "seven",
        "count" : 7
      }, {
        "term" : "six",
        "count" : 6
      }, {
        "term" : "five",
        "count" : 5
      }, {
        "term" : "four",
        "count" : 4
      }, {
        "term" : "three",
        "count" : 3
      } ]
    }
  }
}
```

I would like to add an option to the term facet :

``` bash
# New option "other_terms" for terms facet
curl -X POST "http://localhost:9200/articles/_search?pretty=true" -d '
  {
    "query" : { "match_all" : { } },
    "facets" : {
      "f_tags" : { "terms" : {
        "field" : "tags",
        "other_terms" : true
      } }
    }
  }
'
```

Which should return :

``` javascript
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 7,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "gkgAyLN6QWuxSyBLxxknIw",
      "_score" : 1.0, "_source" : {"tags":["two","three","four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "swVlq23SS9aoC8uHNB_LRA",
      "_score" : 1.0, "_source" : {"tags":["one","two","three","four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "oColEdY-SUmMPWvBWBEneQ",
      "_score" : 1.0, "_source" : {"tags":["six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "hk-9osIlRViESqEF7WPj_w",
      "_score" : 1.0, "_source" : {"tags":["three","four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "Ki9KA036TI6n7f6vD2gGWA",
      "_score" : 1.0, "_source" : {"tags":["four","five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "_bFnruq4TdyJQjHH5bTMrg",
      "_score" : 1.0, "_source" : {"tags":["five","six","seven"]}
    }, {
      "_index" : "articles",
      "_type" : "article",
      "_id" : "k1FzqoBwQ82PYNa1jgE-Tg",
      "_score" : 1.0, "_source" : {"tags":["seven"]}
    } ]
  },
  "facets" : {
    "f_tags" : {
      "_type" : "terms",
      "missing" : 0,
      "terms" : [ {
        "term" : "seven",
        "count" : 7
      }, {
        "term" : "six",
        "count" : 6
      }, {
        "term" : "five",
        "count" : 5
      }, {
        "term" : "four",
        "count" : 4
      }, {
        "term" : "three",
        "count" : 3
      }, {
        "term" : "_other",
        "count" : 3
      } ]
    }
  }
}
```

As you can see, a new _other term field is returned with a count of 3 (2 terms "two" + 1 term "one").

Many thanks
Cheers
David.
</description><key id="1054495">1029</key><summary>Add 'other_terms' option for terms facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-14T11:45:24Z</created><updated>2013-08-26T20:24:25Z</updated><resolved>2011-07-15T04:36:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jsuchal" created="2011-07-13T19:42:55Z" id="1565511">+1 for terms_stats facet
</comment><comment author="kimchy" created="2011-07-15T04:34:01Z" id="1577549">I will push this, it will basically compute it by default, and return `total` and `other` on the same level as `missing`. @jsuchal, can you open a different issue for terms_stats, its a bit different for that one...?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/InternalByteTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/TermsByteFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/TermsByteOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/InternalDoubleTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/InternalFloatTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/TermsFloatFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/TermsFloatOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/index/IndexNameFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/InternalIntTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/TermsIntFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/TermsIntOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/InternalIpTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/TermsIpFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/TermsIpOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/InternalLongTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/InternalShortTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/TermsShortFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/TermsShortOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/FieldsTermsStringFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/InternalStringTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/ScriptTermsStringFieldFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringOrdinalsFacetCollector.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Add 'other_terms' option for terms facet, closes #1029.</comment></comments></commit></commits></item><item><title>Unicast discovery: Improve initial ping connection to nodes to be more lightweight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1028</link><project id="" key="" /><description>When connecting to other nodes, we have several channels open against them in order to improve perf, and control different characteristics of channels (some used for recovery, others used for ping). The unicast discovery does an initial connection to nodes and then disconnects from them, so this type of connection should be more lightweight.
</description><key id="1050819">1028</key><summary>Unicast discovery: Improve initial ping connection to nodes to be more lightweight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-13T19:09:39Z</created><updated>2011-06-13T19:19:58Z</updated><resolved>2011-06-13T19:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/Transport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file></files><comments><comment>Unicast discovery: Improve initial ping connection to nodes to be more lightweight, closes #1028.</comment></comments></commit></commits></item><item><title>Enhancement: Support empty string queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1027</link><project id="" key="" /><description>Add an enhancement to support empty-value mapping (different than null-value) for String fields.
</description><key id="1050261">1027</key><summary>Enhancement: Support empty string queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oravecz</reporter><labels /><created>2011-06-13T17:26:11Z</created><updated>2013-04-05T13:26:00Z</updated><resolved>2013-04-05T13:26:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T13:26:00Z" id="15955317">This is supported by the `missing` / `exists` filters and also in the query-string query
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce a new HTML field type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1026</link><project id="" key="" /><description>Provide out of the box support for fields with HTML content. The goal would be to allow to store both original and cleaned content (for highlighting or document preview). HTML cleaner should be configurable.

More detailed discussion can be found in ML here: http://elasticsearch-users.115913.n3.nabble.com/Strip-HTML-on-indexing-does-not-store-results-td3039614.html
</description><key id="1047874">1026</key><summary>Introduce a new HTML field type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2011-06-13T07:43:28Z</created><updated>2015-08-20T16:39:01Z</updated><resolved>2013-10-31T15:01:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bryangreen" created="2011-12-20T02:16:45Z" id="3213230">+1
</comment><comment author="nickdunn" created="2012-06-20T13:57:51Z" id="6454463">Yes please!
</comment><comment author="slorber" created="2012-07-02T00:51:42Z" id="6698679">+1

I need this because using a river which provides html content and i need highlights
</comment><comment author="lukas-vlcek" created="2013-07-04T12:54:28Z" id="20475443">May be this should be closed? HTML field type might be an interesting idea but can be part of plugin - i.e. does not have to be in the ES core.
</comment><comment author="javanna" created="2013-10-18T17:06:09Z" id="26612454">I agree @lukas-vlcek , interesting but can definitely be implemented as a plugin.
</comment><comment author="lukas-vlcek" created="2013-10-31T15:01:39Z" id="27493090">Right I agree, it makes more sense to implement this as a plugin. Closing this ticket for now. Feel free to reopen it if needed.
</comment><comment author="synhershko" created="2013-10-31T20:33:32Z" id="27525085">Not sure why you'd need this as a type - simply use an HTML filter in your analyzers to get proper highlights, and do cleanup client side if you need the whole thing?
</comment><comment author="fatagun" created="2015-08-20T16:39:01Z" id="133070646">what s up with this issue? was it implemented as a plugin?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aliases: allow to specify routing value for an alias </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1025</link><project id="" key="" /><description>It’s now possible to associate routing values with aliases. This feature can be used together with filtering aliases in order to avoid unnecessary shard operations.

The following command creates a new alias "alias1" that points to index "test". After "alias1" is created, all operations with this alias are automatically modified to use value "1" for routing.

```
$ curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "index" : "test",
                 "alias" : "alias1",
                 "routing" : "1"
            }
        }
    ]
}'
```

It’s also possible to specify different routing values for searching and indexing operations:

```
$ curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "index" : "test",
                 "alias" : "alias2",
                 "search-routing" : "1,2",
                 "index-routing" : "2"
            }
        }
    ]
}'
```

As shown in the example above, search routing may contain several values separated by comma. Index routing can contain only a single value. 

If an operation that uses routing alias also has a routing parameter, an intersection of both alias routing and routing specified in the parameter is used. For example the following command will use "2" as a routing value:

```
$ curl -XGET 'http://localhost:9200/alias2/_search?q=user:kimchy&amp;routing=2,3'
```
</description><key id="1047128">1025</key><summary>Aliases: allow to specify routing value for an alias </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-13T00:49:28Z</created><updated>2011-06-13T01:39:04Z</updated><resolved>2011-06-13T01:39:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to specify routing information for aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1024</link><project id="" key="" /><description>Rebased to resolve merge conflicts. 
</description><key id="1045924">1024</key><summary>Add ability to specify routing information for aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-12T15:52:08Z</created><updated>2014-07-16T21:56:36Z</updated><resolved>2011-06-12T20:49:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-12T20:49:19Z" id="1356093">Great job!, pushed. Can you open an "official" issue for this with maybe some sample curls?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cleaned Groovy classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1023</link><project id="" key="" /><description>- added type information to public methods
- removed semicolons
- replaced GStrings with Strings
</description><key id="1045301">1023</key><summary>cleaned Groovy classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darxriggs</reporter><labels /><created>2011-06-12T10:43:32Z</created><updated>2014-06-22T05:51:13Z</updated><resolved>2011-06-12T11:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-12T11:09:38Z" id="1354565">Applied, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices exists API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1022</link><project id="" key="" /><description>Add an indices exists API, the REST endpoint answered `HEAD` request, and return `200` if (all indices) exists, or `404` if its not found. For example:

```
 curl -XHEAD localhost:9200/test
```
</description><key id="1045157">1022</key><summary>Indices exists API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-12T09:00:57Z</created><updated>2011-06-12T09:01:41Z</updated><resolved>2011-06-12T09:01:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/exists/TransportIndicesExistsAction.java</file></files><comments><comment>Indices exists API, closes #1022.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/TransportActions.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/exists/IndicesExistsRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/exists/IndicesExistsResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/exists/TransportIndicesExistsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/Requests.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/exists/IndicesExistsRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/node/NodeIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/ClientTransportActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/action/admin/indices/exists/ClientTransportIndicesExistsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/exists/RestIndicesExistsAction.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/DocumentActionsTests.java</file></files><comments><comment>Indices exists API, closes #1022.</comment></comments></commit></commits></item><item><title>Numeric Types: By default, omit_norms (boosting) is set to true, automatically set it to false if explicit field mapping boosting is provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1021</link><project id="" key="" /><description>Numeric Types: By default, omit_norms (boosting) is set to true, automatically set it to false if explicit field mapping boosting is provided
</description><key id="1045083">1021</key><summary>Numeric Types: By default, omit_norms (boosting) is set to true, automatically set it to false if explicit field mapping boosting is provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-12T08:12:45Z</created><updated>2011-06-12T08:13:18Z</updated><resolved>2011-06-12T08:13:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file></files><comments><comment>Numeric Types: By default, omit_norms (boosting) is set to true, automatically set it to false if explicit field mapping boosting is provided, closes #1021.</comment></comments></commit></commits></item><item><title>Numeric values always omit norms (boosting), regardless of the configuration set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1020</link><project id="" key="" /><description>Numeric values always omit norms (boosting), regardless of the configuration set
</description><key id="1045070">1020</key><summary>Numeric values always omit norms (boosting), regardless of the configuration set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-12T08:03:24Z</created><updated>2011-06-12T08:03:54Z</updated><resolved>2011-06-12T08:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file></files><comments><comment>Numeric values always omit norms (boosting), regardless of the configuration set, closes #1020.</comment></comments></commit></commits></item><item><title>_boost field should work for *:* too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1019</link><project id="" key="" /><description>When using boost field for every document and doing a query_search with  _:_  it would be logically to simply sort against the boost value. At the moment every doc gets an identical score of 1
</description><key id="1038645">1019</key><summary>_boost field should work for *:* too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-06-10T20:39:03Z</created><updated>2013-04-05T13:28:52Z</updated><resolved>2013-04-05T13:28:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-06-11T08:56:25Z" id="1346937">Could you provide a gist to replicate the problem?
</comment><comment author="kimchy" created="2011-06-12T07:54:22Z" id="1354155">This happens because index time boosting is stored on the field level under norms. In this case, you need to provide a `norms_field` to the match all query to be used when scoring. I have improved the match all query docs to explain that: http://www.elasticsearch.org/guide/reference/query-dsl/match-all-query.html.
</comment><comment author="karussell" created="2011-06-13T07:40:41Z" id="1357465">Thanks! :)
</comment><comment author="karussell" created="2011-06-20T11:22:35Z" id="1401962">Hmmh, even when I include norms_field it does not recognize it (seems to be the same in the Java API): 

"max_score" : 1.0

See https://gist.github.com/1035458

BTW: what should be faster: match_all + sorting or this boosting?
</comment><comment author="kimchy" created="2011-06-24T11:46:17Z" id="1431842">@karussell can you gist a recreation? preferably with curl :)
</comment><comment author="karussell" created="2011-06-24T17:50:18Z" id="1433958">Hi Shay, thanks for reopening :)

Isn't the curl stuff here okay? https://gist.github.com/1035458
</comment><comment author="kimchy" created="2011-06-24T17:56:43Z" id="1434005">The field value does not affect norms, boosting does :)
</comment><comment author="karussell" created="2011-06-24T18:07:26Z" id="1434098">Hmmh, I'm kind of blocked today :)

You mean I need to change the field name test to _boost? 

I tried it (see updated gist) but it still does not work. Where is my stupidity?
</comment><comment author="karussell" created="2011-07-20T13:59:25Z" id="1615584">@kimchy did you get time to look into this? (btw I'm on 0.16.4)
</comment><comment author="kimchy" created="2011-07-20T19:08:16Z" id="1617986">Heay, the `_boost` field is not indexed by default, try to use a norms_field on another field other than that one
</comment><comment author="apatrida" created="2011-12-26T16:34:35Z" id="3275442">I can't seem to make this happen in the unit tests either.  Creating a float field in the mappings, adding field to document and setting the norms_field in match_all query to the name of that field.  All documents come back with 1.0f score.
</comment><comment author="apatrida" created="2011-12-26T16:49:04Z" id="3275517">This won't work as described.  The norms_field must be a field with norms created by the analysis change, rather than a field containing a value of the resulting norms.  Create a float field, it has no norms so does not appear in the norms[] array of the index therefore reader.norms(normsfield) returns null.  Make it a string field and that call returns a valid value, but then it isn't what you are trying to do with the boost value being a field.  To make this work, we would need to subclass the Lucene MatchAllDocsQuery and then allow it to score based on the value of a field or using the _boost field.
</comment><comment author="woahdae" created="2012-08-01T20:51:15Z" id="7439542">&gt; Heay, the _boost field is not indexed by default
&gt; 
&gt; The norms_field must be a field with norms created by the analysis change

These two pieces of information are critical in getting this behavior, thanks! It seems like the only way to do this is to store the boost value twice, so that one of them gets indexed and is available to sort by when users enter empty queries. If anyone solved this in a more elegant way, do speak up.
</comment><comment author="clintongormley" created="2013-04-05T13:28:52Z" id="15955435">The norms_field functionality has been removed from the match_all query
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>river-couchdb's script cannot handle dynamically created arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1018</link><project id="" key="" /><description>I found that I'm getting an invalid array value when I use river-couchdb's script value to create an array. I've configured a dummy _river index with:

```
% curl -XPUT 'http://localhost:9200/_river/foo_bar/_meta' -d '{
    "type": "couchdb",
    "couchdb": {
        "host": "localhost",
        "port": 5984,
        "db": "foo",
        "script": "ctx.doc.ip = [\"1.2.3.4\", \"5.6.7.8\"]"
    }
}
```

When queried, I get a result like this back:  

```
{
  "took":2,
  "timed_out":false,
  "_shards":{
    "total":5,
    "successful":5,
    "failed":0
  },
  "hits":{
    "total":1,
    "max_score":1.0,
    "hits":[
      {
        "_index":"foo",
        "_type":"foo",
        "_id":"bar",
        "_score":1.0,
        "_source":{
          "_rev":"2-13839535feb250d3d8290998b8af17c3",
          "_id":"bar",
          "foo":"bar",
          "ip":"org.mozilla.javascript.NativeArray@3cb5e07a"
        }
      }
    ]
  }
}
```

Notice the `"org.mozilla.javascript.NativeArray@3cb5e07a`. Regular string arrays work fine though.
</description><key id="1038514">1018</key><summary>river-couchdb's script cannot handle dynamically created arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2011-06-10T20:10:31Z</created><updated>2014-07-08T12:19:21Z</updated><resolved>2014-07-08T12:19:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="DawidJanczak" created="2012-11-14T21:34:21Z" id="10386084">Did you manage to find a way out of this? I seem to have the same problem...
</comment><comment author="clintongormley" created="2014-07-08T12:19:21Z" id="48328515">Rivers will be removed in a future version. Rather push data into Elasticsearch with an external process.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Access to child/parent fields of a document within a script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1017</link><project id="" key="" /><description>Similar to #761 where it is asked to join the parent and child within the search result.

It would be nice to have access to the child documents- (and/or the parent document-) 's fields to be able to perform some lookup and script scoring or create a script field.

Examples:
- Calculating some complex aggregation on children documents. (as script field)
- Scoring a parent document depending on some complex aggregation on children documents. (as score script)
- Just about the same, but manipulating the child document based on its parent.
</description><key id="1037449">1017</key><summary>Access to child/parent fields of a document within a script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels><label>discuss</label></labels><created>2011-06-10T16:35:05Z</created><updated>2016-01-14T17:04:58Z</updated><resolved>2014-07-18T08:18:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="oyiptong" created="2012-03-01T15:48:15Z" id="4260453">i would like to see this implemented as well
</comment><comment author="ghost" created="2012-08-08T01:47:56Z" id="7572792">Would love to see this too.
</comment><comment author="ryanrolland" created="2013-05-11T00:15:59Z" id="17750767">Thought this might already be possible. Was a bit confused with the docs. Put this stackoverflow post together: http://stackoverflow.com/questions/16474800/elasticsearch-has-child-filter-i-want-to-reference-child-fields-in-results

Seems like the data is already there to build the existing query up. Why not make it available? Would love this feature since it would enable multi-type query integration into UI controls like facetview.
</comment><comment author="mvallebr" created="2014-02-21T18:29:58Z" id="35758697">+1 I would like that too
</comment><comment author="mattaylor" created="2014-04-01T17:29:11Z" id="39233687">+1 allowing scripts to load other docs by id would be a more usefull general solution here.
</comment><comment author="isabel12" created="2014-05-19T02:37:36Z" id="43462120">+1  this would be awesome.  
</comment><comment author="amerov" created="2014-07-10T09:31:00Z" id="48583432">:+1: 
</comment><comment author="cdebry" created="2014-07-15T16:05:44Z" id="49054474">There is a discussion [here](http://elasticsearch-users.115913.n3.nabble.com/Impossible-to-implement-real-custom-boost-query-when-the-weight-is-in-the-child-document-td4057206.html#a4059644) that explains the issue in further detail.

This should already be possible using a function score query. The parent / child field can be referenced within the function, but ES doesn't return the value as expected. This seems like a bug rather than a feature.
</comment><comment author="clintongormley" created="2014-07-18T08:18:59Z" id="49406576">After discussion, we've decided against this issue as looking up another doc from within a script would be prohibitively expensive.  Potentially what you need will be solvable when aggregations have parent-child support.
</comment><comment author="yehosef" created="2016-01-14T15:35:56Z" id="171675663">We have this use case - do aggregations have parent/child support now?  Are there plans to add it, if not?  I could see looking up children would be expensive because there could potentially be many but perhaps it would be possible to have this just for the parent since there can be one.
</comment><comment author="yehosef" created="2016-01-14T16:55:09Z" id="171701323">Also - even if you gave access to the documents in the inner-hits query for scoring or post-filtering it would be great.  You're already doing to the lookup for the other documents - you just can't do anything with them in a script (unless I'm missing something..)  If you could, that by itself could be very helpful in many use cases.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Merging complex objects / types (geo, objects) can cause failure to lookup field names (without type prefix)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1016</link><project id="" key="" /><description>When specifying a geo_point type in 16.2 geo_distance facet fails. Example works in 16.1 and 16.0
Error: FacetPhaseExecutionException[Facet [geotest]: No mapping found for field [locations]]

Curl recreation at https://gist.github.com/2ba2e3d0d50a5e4c777c
The empty document does throw an error but the bug is reproducible even with a non-empty document.
</description><key id="1036160">1016</key><summary>Merging complex objects / types (geo, objects) can cause failure to lookup field names (without type prefix)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tozz</reporter><labels><label>bug</label><label>v0.16.3</label><label>v0.17.0</label></labels><created>2011-06-10T12:01:49Z</created><updated>2011-06-10T17:01:14Z</updated><resolved>2011-06-10T17:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-10T16:59:54Z" id="1343626">I see where the problem is, I will change the title a bit and push a fix soon.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/merge/Test1MergeMapperTests.java</file></files><comments><comment>Merging complex objects / types (geo, objects) can cause failure to lookup field names (without type prefix), closes #1016.</comment></comments></commit></commits></item><item><title>ElasticSearch uses more memory than it's assigned or it reports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1015</link><project id="" key="" /><description>ElasticSearch 0.16.2.

I've given ES a heap size of 256MB with `ES_MAX_MEM=256m`. After running a while, it shows heap committed about 256MB and non-heap committed about 30MB, as reported by `http://localhost:9200/_cluster/nodes/stats`. So the total is under 290MB.

But what the OS (Linux in this case) thinks is a different story. The `top` command shows 460MB RES size (actual physical memory used by the process). This makes it very hard to limit ES to a certain amount of memory, which is important as there are other processes running on the node, too. I'd like to have more control on the RES memory usage, because the node eventually goes for swapping and gets really slow.
</description><key id="1034832">1015</key><summary>ElasticSearch uses more memory than it's assigned or it reports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">akheron</reporter><labels /><created>2011-06-10T07:36:20Z</created><updated>2011-06-12T07:59:38Z</updated><resolved>2011-06-12T07:59:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-10T09:18:44Z" id="1340699">Thats how Java works, you should expect more overhead on top of the heap itself, and there is no way to control it, though it is bounded.

On Friday, June 10, 2011 at 10:36 AM, akheron wrote:

&gt; ElasticSearch 0.16.2.
&gt; 
&gt; I've given ES a heap size of 256MB with `ES_MAX_MEM=256m`. After running a while, it shows heap committed about 256MB and non-heap committed about 30MB, as reported by `http://localhost:9200/_cluster/nodes/stats`. So the total is under 290MB.
&gt; 
&gt; But what the OS (Linux in this case) thinks is a different story. The `top` command shows 460MB RES size (actual physical memory used by the process). This makes it very hard to limit ES to a certain amount of memory, which is important as there are other processes running on the node, too. I'd like to have more control on the RES memory usage, because the node eventually goes for swapping and gets really slow.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1015
</comment><comment author="kimchy" created="2011-06-12T07:59:38Z" id="1354164">closing this, since there isn't really much that I can do about it...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticSearch fails to report OS info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1014</link><project id="" key="" /><description>ElasticSearch version:

```
"version" : {
    "number" : "0.16.2",
    "date" : "2011-05-31T23:43:18",
    "snapshot_build" : false
},
```

OS info, as reported by http://localhost:9200/_cluster/nodes/stats:

```
"os": {
    "load_average": [], 
    "timestamp": 1307690189014, 
    "uptime": "-1 seconds", 
    "uptime_in_millis": -1000
}, 
```

The system is Ubuntu Lucid 10.04.2 LTS. uname -a output:

```
Linux foo 2.6.32-24-server #39-Ubuntu SMP Wed Jul 28 06:21:40 UTC 2010 x86_64 GNU/Linux
```
</description><key id="1034794">1014</key><summary>ElasticSearch fails to report OS info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">akheron</reporter><labels /><created>2011-06-10T07:19:54Z</created><updated>2011-06-13T07:44:36Z</updated><resolved>2011-06-13T07:44:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-10T09:19:54Z" id="1340704">How did you start it? And please, first, questions on the mailing list.

On Friday, June 10, 2011 at 10:19 AM, akheron wrote:

&gt; ElasticSearch version:
&gt; 
&gt;  "version" : {
&gt;  "number" : "0.16.2",
&gt;  "date" : "2011-05-31T23:43:18",
&gt;  "snapshot_build" : false
&gt;  },
&gt; 
&gt; OS info, as reported by http://localhost:9200/_cluster/nodes/stats:
&gt; 
&gt;  "os": {
&gt;  "load_average": [], 
&gt;  "timestamp": 1307690189014, 
&gt;  "uptime": "-1 seconds", 
&gt;  "uptime_in_millis": -1000
&gt;  }, 
&gt; 
&gt; The system is Ubuntu Lucid 10.04.2 LTS. uname -a output:
&gt; 
&gt;  Linux foo 2.6.32-24-server #39-Ubuntu SMP Wed Jul 28 06:21:40 UTC 2010 x86_64 GNU/Linux
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1014
</comment><comment author="akheron" created="2011-06-10T09:24:51Z" id="1340731">I downloaded the zipball, extracted and run ./bin/elasticsearch. I noticed this because the [elasticsearc-js](https://github.com/lukas-vlcek/elasticsearch-js) stats demo crashed because of missing OS info.

Should we continue on this on mailing list?
</comment><comment author="kimchy" created="2011-06-10T09:36:23Z" id="1340779">Can you set `monitor.sigar` logging level to TRACE? Do you see any additional output in this case?
</comment><comment author="akheron" created="2011-06-10T10:02:24Z" id="1340884">I added to `logging.yaml`:

```
logging:
    [..snip..]
    monitor.sigar: TRACE
```

Is this correct? It resulted in no extra output in `logs/elasticsearch.log`, just the normal.
</comment><comment author="kimchy" created="2011-06-10T15:00:39Z" id="1342874">Yes, the logging looks good.

elasticsearch uses a native library called sigar to try and extract the os information, there was another place where it might have failed to be loaded, but the failure is not logged. I have just pushed a DEBUG logging for that, so if you have the time, can you build master and set `monitor` to `TRACE` and maybe now we can see the failure?
</comment><comment author="akheron" created="2011-06-13T07:22:33Z" id="1357431">Built from current git master `fc17ba165295eacc4ff2edba8048a00a2bbbb108`, invoked `./gradlew`, copied `build/distributions/elasticsearch-0.17.0-SNAPSHOT.zip` to server, unpacked, changed `logs/logging.yml` and run `./bin/elasticsearch`, and gisted the resulted log in https://gist.github.com/1022417. I don't see any sigar related errors, but the stats output is still the same:

```
        "os": {
            "load_average": [],
            "timestamp": 1307949547356,
            "uptime": "-1 seconds",
            "uptime_in_millis": -1000
        },
```
</comment><comment author="akheron" created="2011-06-13T07:44:36Z" id="1357479">Ah sorry. It seems that the previous installation of ElasticSearch was broken, and my attempts of running from another directory somehow used the old libraries.

I now installed fresh Ubuntu 10.04.2 virtual machine and everything works just fine with 0.16.2. So closing as invalid.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>org.elasticsearch.index.IndexShardMissingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1013</link><project id="" key="" /><description>Hi,

Im just starting with elasticsearch and I have this issue when im importing all the users.

Im trying to import 732 users.

``` ruby
class Search &lt; ActiveRecord::Base

  def self.index_users
    require 'rubygems'
    require 'yajl/json_gem'
    require 'tire'

    users = []

    User.all.each do |user|
      users &lt;&lt; {
        :id =&gt; user.id,
        :first_name =&gt; user.first_name,
        :last_name =&gt; user.last_name,
        :employee_t4 =&gt; user.employee_t4,
        :description =&gt; user.description,
        :shop_id =&gt; user.shop_id,
        :email =&gt; user.email,
        :role_id =&gt; user.role_id,
        :active =&gt; user.active
      }
    end

    Tire.index 'users' do

      delete
      create

      import users

      refresh

    end
  end
end
```

And i got this error:

``` sh
[2011-06-10 12:10:35,387][INFO ][cluster.metadata         ] [Veritas] [users] creating index, cause [api], shards [5]/[1], mappings []
[2011-06-10 12:10:35,613][DEBUG][action.admin.indices.status] [Veritas] [users][4], node[lrOEzptuSEOoxTGJT4n-Sg], [P], s[INITIALIZING]: Failed to execute [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@7e64eff0]
org.elasticsearch.index.IndexShardMissingException: [users][4] missing
    at org.elasticsearch.index.service.InternalIndexService.shardSafe(InternalIndexService.java:161)
    at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:150)
    at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:59)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:238)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.access$200(TransportBroadcastOperationAction.java:126)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:195)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
[2011-06-10 12:10:35,808][INFO ][cluster.metadata         ] [Veritas] [users] update_mapping [document] (dynamic)
```

The problem is only 10 Users are imported.

Any idea why ?

Thanks 
</description><key id="1034155">1013</key><summary>org.elasticsearch.index.IndexShardMissingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienbrz</reporter><labels /><created>2011-06-10T02:33:14Z</created><updated>2011-06-10T07:58:05Z</updated><resolved>2011-06-10T02:34:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-10T02:34:57Z" id="1339398">Please don't ask questions on the issues list, as it on the mailing list (more eyeballs) and if its an issue, we can open one.
</comment><comment author="karmi" created="2011-06-10T07:58:05Z" id="1340370">Hi, yeah, as @kimchy has mentioned, please do bring this to the mailing list and/or IRC channel. I'm also there, and can give advice related to _Tire_. I think this is possible an issue in _Tire_, not _ElasticSearch_, so feel free to submit ticket [there](https://github.com/karmi/tire/issues).

Besides, I wonder why do you make this so much complicated for you, why not simply use the _ActiveRecord_ integration of _Tire_?

``` ruby
    class User &lt; ActiveRecord::Base
      include Tire::Model::Search
      include Tire::Model::Callbacks

      def to_indexed_json
        {
          :id          =&gt; id,
          :first_name  =&gt; first_name,
          :last_name   =&gt; last_name,
          :employee_t4 =&gt; employee_t4,
          :description =&gt; description,
          :shop_id     =&gt; shop_id,
          :email       =&gt; email,
          :role_id     =&gt; role_id,
          :active      =&gt; active
        }
      end

    end

    User.elasticsearch_index.import User.all
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow fuzzy ip queries to accept long typed min_similarity values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1012</link><project id="" key="" /><description>Thought this would be useful since I didn't expect to have to pass ip addresses for the min_similarity.
</description><key id="1033364">1012</key><summary>Allow fuzzy ip queries to accept long typed min_similarity values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2011-06-09T23:19:30Z</created><updated>2014-07-16T21:56:37Z</updated><resolved>2011-06-10T00:10:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-10T00:10:31Z" id="1338277">cool, pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to specify routing information for aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1011</link><project id="" key="" /><description>Made API changes that we discussed and added more tests.
</description><key id="1033141">1011</key><summary>Add ability to specify routing information for aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-06-09T22:30:24Z</created><updated>2014-07-16T21:56:37Z</updated><resolved>2011-06-12T14:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-12T11:08:45Z" id="1354561">Heya, is there a chance for another pull request? Some things have changed...
</comment><comment author="imotov" created="2011-06-12T14:22:46Z" id="1354956">Sure, just a moment.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Closed indices should not cause block failures on some APIs what executing against _all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1010</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1' 
curl -XPUT 'http://127.0.0.1:9200/baz/?pretty=1' 
curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "text" : "foo"
}
'
curl -XPOST 'http://127.0.0.1:9200/baz/bar?pretty=1'  -d '
{
   "text" : "foo"
}
'

curl -XPOST 'http://127.0.0.1:9200/foo/_close?pretty=1' 

# [Thu Jun  9 20:29:53 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1' 

# [Thu Jun  9 20:30:04 2011] Response:
# {
#    "status" : 500,
#    "error" : "ClusterBlockException[blocked by: [4/index closed];
# &gt;    ]"
# }
```
</description><key id="1031899">1010</key><summary>Closed indices should not cause block failures on some APIs what executing against _all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-09T18:42:58Z</created><updated>2011-07-10T20:56:11Z</updated><resolved>2011-07-10T20:56:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mobz" created="2011-06-24T13:01:25Z" id="1432167">This pretty much breaks casual browsing on elasticsearch-head when one or more indices are closed.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file></files><comments><comment>Closed indices should not cause block failures on some APIs what executing against _all indices, closes #1010.</comment></comments></commit></commits></item><item><title>Add a global setting to configuration to set default operator for a query </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1009</link><project id="" key="" /><description>Currently the default operator for a query is OR. A global settings (not per query) should be available to specify default operator for all queries.
Discussed in:
http://elasticsearch-users.115913.n3.nabble.com/global-setting-for-default-operator-for-queries-td2631544.html
</description><key id="1029442">1009</key><summary>Add a global setting to configuration to set default operator for a query </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels><label>discuss</label></labels><created>2011-06-09T11:01:51Z</created><updated>2014-07-18T08:16:35Z</updated><resolved>2014-07-18T08:16:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Stanley" created="2011-06-28T11:44:50Z" id="1454888">+1
</comment><comment author="mikeg250" created="2011-10-04T15:32:46Z" id="2287701">This is what I need to convert from Solr.  Elasticsearch looks like a terrific alternative to Solr, but defaulting to a Google-style search of AND would even better.
</comment><comment author="luizgpsantos" created="2012-04-04T14:14:52Z" id="4954205">That would be very nice for me too.
</comment><comment author="clintongormley" created="2014-07-18T08:16:35Z" id="49406404">We've decided against this as we want to avoid setting explosion.  This is easy to set in the application.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scroll search always throws IndexOutOfBoundsException on last iteration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1008</link><project id="" key="" /><description>When using the _scroll search to copy from one index to another I get an exception*\* for the last _scroll search but all objects are successfully fetched + indexed (so not a real issue ;)). Or is my condition if (currentResults == 0) wrong but in the tutorial I read 'The “exit” point from the scrolling process is when no hits are returned back.'?

Java code for the copying is:

```
SearchRequestBuilder srb = client.prepareSearch(fromIndex).
        setVersion(true).
        setQuery(QueryBuilders.matchAllQuery()).setSize(hitsPerPage).
        setSearchType(SearchType.SCAN).
        setScroll(TimeValue.timeValueMinutes(keepTime));
if (additionalFilter != null)
    srb.setFilter(additionalFilter);
SearchResponse rsp = srb.execute().actionGet();

try {
    long total = rsp.hits().totalHits();
    String scrollId = rsp.scrollId();
    int collectedResults = 0;
    while (true) {
        rsp = client.prepareSearchScroll(scrollId).
                setScroll(TimeValue.timeValueMinutes(keepTime)).execute().actionGet();
        long currentResults = rsp.hits().hits().length;
        if (currentResults == 0)
            break;

        // convert rsp into java objects
        Collection&lt;T&gt; objs = createObj.collectObjects(rsp);
        // do bulk indexing of the grabbed objects
        int failed = bulkUpdate(objs, intoIndex, false, false).size();
        // trying to enable flushing to avoid memory issues on the server side?
        flush(intoIndex);
        collectedResults += currentResults;
    }
} catch (Exception ex) {
    logger.error("Failed to copy data from index " + fromIndex + " into " + intoIndex + ".", ex);
}

-----------------------------------------
public Collection&lt;Integer&gt; bulkUpdate(Collection&lt;T&gt; objects, 
   String indexName, boolean refresh, boolean enableVersioning) {

    BulkRequestBuilder brb = client.prepareBulk();
    // this works differently then the direct call to refresh!? maybe refresh is not async?
    // brb.setRefresh(refresh);
    for (T o : objects) {
        if (o.getId() == null) {
            logger.warn("Skipped object without id when bulkUpdate:" + o);
            continue;
        }

        try {
            XContentBuilder source = createDoc(o);
            IndexRequest indexReq = Requests.indexRequest(indexName).type(getIndexType()).id(o.getId()).source(source);
            if (enableVersioning)
                indexReq.version(o.getVersion());

            brb.add(indexReq);
        } catch (IOException ex) {
            logger.warn("Cannot add object:" + o + " to bulkIndexing action." + ex.getMessage());
        }
    }
    if (brb.numberOfActions() &gt; 0) {
        BulkResponse rsp = brb.execute().actionGet();
        if (rsp.hasFailures()) {
            List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(rsp.items().length);
            for (BulkItemResponse br : rsp.items()) {
                list.add(br.itemId());
            }
            return list;
        }
        if (refresh)
            refresh(indexName);
    }

    return Collections.emptyList();
}
```

**

```
org.elasticsearch.transport.RemoteTransportException: [Super Rabbit][inet[/127.0.0.1:9300]][indices/searchScroll]
Caused by: org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] ; shardFailures {SearchContextMissingException[No search context found for id [204864]]}{SearchContextMissingException[No search context found for id [204865]]}
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.finishHim(TransportSearchScrollScanAction.java:209)
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.access$1300(TransportSearchScrollScanAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction$3.onFailure(TransportSearchScrollScanAction.java:199)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteScan(SearchServiceTransportAction.java:377)
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.executePhase(TransportSearchScrollScanAction.java:184)
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.access$700(TransportSearchScrollScanAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction$2.run(TransportSearchScrollScanAction.java:157)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IndexOutOfBoundsException: index (0) must be less than size (0)
        at org.elasticsearch.common.base.Preconditions.checkElementIndex(Preconditions.java:301)
        at org.elasticsearch.common.base.Preconditions.checkElementIndex(Preconditions.java:280)
        at org.elasticsearch.common.collect.Iterables.get(Iterables.java:649)
        at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:259)
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.innerFinishHim(TransportSearchScrollScanAction.java:232)
        at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.finishHim(TransportSearchScrollScanAction.java:207)
        ... 9 more
```
</description><key id="1024723">1008</key><summary>Scroll search always throws IndexOutOfBoundsException on last iteration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-06-08T15:48:49Z</created><updated>2011-06-20T09:08:45Z</updated><resolved>2011-06-20T09:08:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="oravecz" created="2011-06-15T15:34:44Z" id="1374492">I was just writing this up as well. Occurs in 0.16.2.
</comment><comment author="oravecz" created="2011-06-15T15:39:47Z" id="1374531">curl -XPOST 'http://localhost:9200/twitter/tweet/1' -d '{ "user": "kimchy" }'

curl -XGET 'localhost:9200/_search?search_type=scan&amp;scroll=5m&amp;pretty=true' -d '{ "query" : { "term": {"user":"kimchy"} } }'

&gt; get scrollID

curl -GET 'localhost:9200/_search/scroll?scroll=5m&amp;pretty=true' -d '&lt;scrollID&gt;'

&gt; returns 1 hit

curl -GET 'localhost:9200/_search/scroll?scroll=5m&amp;pretty=true' -d '&lt;scrollID&gt;'

&gt; throws exception
</comment><comment author="clintongormley" created="2011-06-16T07:30:27Z" id="1379279">Works for me on 0.16.2: https://gist.github.com/1028836
</comment><comment author="kimchy" created="2011-06-16T16:27:50Z" id="1382347">I think I might have fixed this (in a different issue) in master (the AIOB exception). I will backport to 0.16 branch as well.
</comment><comment author="kimchy" created="2011-06-16T17:03:17Z" id="1382601">ok, backport for 0.16 branch as well.
</comment><comment author="oravecz" created="2011-06-16T20:40:14Z" id="1384111">I think karussel and I were incorrectly using the original scroll id from the initial search request on each subsequent request. This is not the correct procedure. Every search request returns a "new" scroll id and that id needs to be passed along with the subsequent scroll request.
</comment><comment author="kimchy" created="2011-06-16T20:55:56Z" id="1384196">Ahh, yes, I missed it in @karussel code. In any case, there was a problem with the annoying exception, and now its fixed :)
</comment><comment author="karussell" created="2011-06-20T09:08:45Z" id="1401462">Thanks to all for pointing this out + fixing it :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transport: Improve concurrency when connecting to several nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1007</link><project id="" key="" /><description>Transport: Improve concurrency when connecting to several nodes
</description><key id="1024720">1007</key><summary>Transport: Improve concurrency when connecting to several nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-08T15:47:15Z</created><updated>2011-06-08T15:47:57Z</updated><resolved>2011-06-08T15:47:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file></files><comments><comment>Transport: Improve concurrency when connecting to several nodes, closes #1007.</comment></comments></commit></commits></item><item><title>Add an Autowarming option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1006</link><project id="" key="" /><description>According to this discussion:

http://elasticsearch-users.115913.n3.nabble.com/Improve-Query-Speed-td2760835.html

there should be an option "to have warm up queries when a new reader is created"
</description><key id="1023453">1006</key><summary>Add an Autowarming option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2011-06-08T11:51:05Z</created><updated>2012-05-06T21:00:44Z</updated><resolved>2012-05-06T21:00:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2011-06-10T19:42:10Z" id="1344565">+1 on this

When we lose a server our performance degrades by ~10x (average times from 30ms -&gt; 300ms, max times from 2000ms -&gt; 20000ms) and I believe that some auto warming would help mitigate this. 
</comment><comment author="karussell" created="2011-06-10T19:57:05Z" id="1344637">&gt; When we lose a server

do you mean when you shutfown the server the performance drops or while restarting? losing a server + performance decrease is not related to autowarming IMO

E.g. my usecase is that I do a lot indexing while querying so that such a feature is required. a workaround is to trigger your autowarm query on the life index every 10 seconds or so
</comment><comment author="ppearcy" created="2011-06-10T20:19:01Z" id="1344768">I believe it would also be applicable for my case. When a shard gets reallocated, I can only assume that a new reader must be opened for it, and if it auto warmed that would prevent the new shard from being cold when hit with real traffic. Maybe I'm off base... Either way, +1 for the feature :-)
</comment><comment author="karussell" created="2011-06-10T20:40:51Z" id="1344881">&gt; When a shard gets reallocated

ups, sorry. Yes, it could be indeed the case that autowarming could help here! So performance recovers after some seconds for your usecase?
</comment><comment author="ppearcy" created="2011-06-10T20:42:10Z" id="1344890">Yeah, it does recover pretty quickly once various caches get built up and things warm. Thanks!
</comment><comment author="kimchy" created="2011-06-10T22:22:26Z" id="1345394">Agreed, autowarming is important. The trick here is, of course, to build a good solution where auto warming will not be applied whend doing heavy indexing. Something similar to dynamically being able to disable it and then enable it.
</comment><comment author="karussell" created="2011-09-27T21:26:42Z" id="2216624">Here is a solution discussed (applies to Lucene 3.5)

http://blog.mikemccandless.com/2011/09/lucenes-searchermanager-simplifies.html

https://issues.apache.org/jira/browse/LUCENE-3445
</comment><comment author="kimchy" created="2011-09-27T21:31:51Z" id="2216689">Thats not really the problem..., I know how to add it :). The main problem is that you don't want to do that while indexing and not searching..., as it will block indexing (at least with Lucene 3).
</comment><comment author="karussell" created="2011-09-27T22:11:14Z" id="2217216">Ok, I see. Sorry for the confusion
</comment><comment author="kimchy" created="2012-05-06T15:51:14Z" id="5536662">See #1913, should handle this now.
</comment><comment author="karussell" created="2012-05-06T21:00:44Z" id="5540809">Nice!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem with default configuration file for unicast</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1005</link><project id="" key="" /><description>Hello,

I'm starting to work with elasticsearch and pretty excited about the product.
Context : I'm deploying 2 nodes on 2 different hardware computers using each Ubuntu in Virtualbox and with bridged networking.

The multicast didn't work so I decided to go with unicast. The default configuration file is :

&lt;pre&gt;
# Unicast Discovery (disable multicast)
#discovery:
# zen:
# multicast.enabled: false
# unicast.hosts: ["host1", "host2"]
&lt;/pre&gt;


So I tried with :

&lt;pre&gt;
# Unicast Discovery (disable multicast)
discovery:
 zen:
 multicast.enabled: false
 unicast.hosts: ["10......", "10......."]
&lt;/pre&gt;

But it didn't worked ! 
While looking at http://elasticsearch-users.115913.n3.nabble.com/Unicast-help-td2683463.html I saw that I must enclose these statements in a "ping" section and I added unicast.enabled: true.

And it works then ! With :

&lt;pre&gt;
# Unicast Discovery (disable multicast)
discovery:
  zen:
    ping:
      multicast.enabled: false
      unicast.enabled: true
      unicast.hosts: ["10.33.170.156", "10.33.171.234"]
&lt;/pre&gt;


Am I wrong or is there a problem with the default config which is misleading ?

Thanks for your work !
</description><key id="1022762">1005</key><summary>Problem with default configuration file for unicast</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ClementNotin</reporter><labels /><created>2011-06-08T09:05:26Z</created><updated>2011-06-08T13:12:35Z</updated><resolved>2011-06-08T09:09:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-08T09:08:15Z" id="1325461">Yea, looks like a problem, fix coming shortly!
</comment><comment author="ClementNotin" created="2011-06-08T09:35:11Z" id="1325577">Are you sure this is the good fix ?
We are already in a "zen" section and we added a ping section.
Correct me if I'm wrong...
</comment><comment author="kimchy" created="2011-06-08T09:37:09Z" id="1325587">Grr, fix is coming
</comment><comment author="ClementNotin" created="2011-06-08T11:00:11Z" id="1325880">Yep better, last suggestion, it should be better to propose :
ping.unicast.enabled: true
in comments, shouldn't it ? It would have helped me to know there was this setting.
</comment><comment author="kimchy" created="2011-06-08T11:02:19Z" id="1325890">What do you mean by enabling unicast? it is "enabled" by default.
</comment><comment author="ClementNotin" created="2011-06-08T11:14:28Z" id="1325951">In http://elasticsearch-users.115913.n3.nabble.com/Unicast-help-td2683463.html I saw the line :
put("discovery.zen.ping.unicast.enabled", true) 

So I thought I must add it in my config. But maybe it's irrelevant
</comment><comment author="kimchy" created="2011-06-08T13:12:35Z" id="1326538">Ahh, yes, its no longer relevant in current versions.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Problem with default configuration file for unicast, closes #1005.</comment></comments></commit></commits></item><item><title>Issues 974 and 1003</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1004</link><project id="" key="" /><description>This pull request is for the "hello rest" and "hello secure rest" samples plugins, and the required modifications in elasticsearch to support them.

It adds the ability to replace/add to elasticsearch REST API with a custom rest handler (issue 974) and to add "reverse proxies" (filters) on top of REST APIs to perform basic security checks (issue 1003).
</description><key id="1019506">1004</key><summary>Issues 974 and 1003</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">itaifrenkel</reporter><labels /><created>2011-06-07T19:51:09Z</created><updated>2014-07-08T12:16:18Z</updated><resolved>2014-07-08T12:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdzurik" created="2011-08-31T19:42:10Z" id="1961079">I think this may address Issue #664 as well. 
</comment><comment author="clintongormley" created="2014-07-08T12:16:18Z" id="48328234">Not merged after 3 years. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to reject REST requests based on REST header or URI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1003</link><project id="" key="" /><description>This feature allows writing plugins that act as a "reverse proxy". They inspect the request and decide if to return an HTTP error code, or forward the request to the rest action handler.
</description><key id="1019328">1003</key><summary>Add ability to reject REST requests based on REST header or URI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">itaifrenkel</reporter><labels /><created>2011-06-07T19:37:05Z</created><updated>2014-07-08T12:12:26Z</updated><resolved>2014-07-08T12:12:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="itaifrenkel" created="2011-06-07T19:37:45Z" id="1321132">Here is an example for such a reverse proxy, that checks the "X-Forwarded-Proto" header is "HTTPS"

The complete example could be found in plugins/rest/hellosecurerest

```
 @Override public void handleRequest(RestRequest request, RestChannel channel, RestHandler nextHandler) {
    String protocol = request.header("X-Forwarded-Proto");
    if ("HTTPS".equalsIgnoreCase(protocol)) {
        super.handleRequest(request, channel, nextHandler);
    }
    else {
         channel.sendResponse(new StringRestResponse(FORBIDDEN));
    }
}
```
</comment><comment author="itaifrenkel" created="2011-06-07T19:44:39Z" id="1321239">Here is a code sinpet that shows how to install the reverse proxy:

```
 @Override public void processModule(Module module) {
     if (settings.getAsBoolean("hellosecurerest.enabled", true) &amp;&amp;
           module instanceof RestReverseProxyModule) {

            RestReverseProxyModule restModule = (RestReverseProxyModule) module;

            //require https termination for all Rest actions except the main action
            restModule.addDefaultRestReverseProxy(RequireHttpsTerminationProxy.class);
            restModule.addRestReverseProxy(DoNothingProxy.class,RestMainAction.class);
     }
}
```
</comment><comment author="clintongormley" created="2014-07-08T12:12:26Z" id="48327895">No discussion in 3 years. The standard way of handling issues like this is with a proxy like nignx.  Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix plugin.bat classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1002</link><project id="" key="" /><description>``` diff
--- plugin.bat.orig     2011-06-06 21:57:50 +0100
+++ plugin.bat  2011-06-06 21:57:27 +0100
@@ -8,7 +8,7 @@
 for %%I in ("%SCRIPT_DIR%..") do set ES_HOME=%%~dpfI


-set ES_CLASSPATH=$CLASSPATH;"%ES_HOME%/lib/*"
+set ES_CLASSPATH=%CLASSPATH%;%ES_HOME%/lib/*
 set ES_PARAMS=-Delasticsearch -Des.path.home="%ES_HOME%"

 "%JAVA_HOME%\bin\java" %JAVA_OPTS% %ES_JAVA_OPTS% %ES_PARAMS% -cp "%ES_CLASSPATH%" "org.elasticsearch.plugins.PluginManager" %*
```
</description><key id="1011503">1002</key><summary>fix plugin.bat classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rgl</reporter><labels><label>bug</label><label>v0.17.0</label></labels><created>2011-06-06T20:59:47Z</created><updated>2011-06-09T23:35:13Z</updated><resolved>2011-06-09T23:35:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fix plugin.bat classpath, closes #1002.</comment></comments></commit></commits></item><item><title>feature request: output transform</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1001</link><project id="" key="" /><description>hi

it would be nice to have a result post processing feature on the server like the "Result Transformer" feature in "Elasticsearch Head" http://mobz.github.com/elasticsearch-head/ which is done on the client. 

quote from the help of elasticsearch head:

&gt; The Result Transformer can be used to post process the raw json results from a request into a more useful format.
&gt; The transformer should contain the body of a javascript function. The return value from the function becomes the new value   passed to the json printer

is it possible to implement such a feature as a plugin?

thx, bernd
</description><key id="1009513">1001</key><summary>feature request: output transform</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobe</reporter><labels /><created>2011-06-06T14:08:11Z</created><updated>2013-04-05T10:28:55Z</updated><resolved>2013-04-05T10:28:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="michaeldcruz" created="2013-03-15T19:34:43Z" id="14980583">+1
</comment><comment author="joelash" created="2013-03-15T19:34:48Z" id="14980587">+2
</comment><comment author="divideby0" created="2013-03-15T19:38:06Z" id="14980737">We were able to achieve this using a native script factory:

https://github.com/Spantree/elasticsearch-occurrence-sort

We probably could have done this as javascript as well, but I imagine the performance wouldn't be all that great.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add possibility to join between indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1000</link><project id="" key="" /><description>It would be useful if one could "join" from one index to another (e.g. for many to many relations) as in https://issues.apache.org/jira/browse/SOLR-2272. Thus it wouldn't be necessary to denormalize documents and less index updates are necessary.
</description><key id="1009130">1000</key><summary>Add possibility to join between indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magethle</reporter><labels /><created>2011-06-06T12:21:08Z</created><updated>2011-06-06T12:42:38Z</updated><resolved>2011-06-06T12:42:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-06T12:42:38Z" id="1308907">You are pointing to a solr feature that does not really do it, and certainly not in a distributed solution. This will not be implemented because join across shards that can exists on differnt nodes is simply not practical performance wise.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Add `reverse` and `skip` to `path_hierarchy` tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/999</link><project id="" key="" /><description>`reverse` generates the tokens in reverse order (set it to `true`), `skip` controls initial token to skip (defaults `0`).
</description><key id="1005134">999</key><summary>Analysis: Add `reverse` and `skip` to `path_hierarchy` tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-05T12:39:05Z</created><updated>2011-06-05T12:39:31Z</updated><resolved>2011-06-05T12:39:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PathHierarchyTokenizerFactory.java</file></files><comments><comment>Analysis: Add `reverse` and `skip` to `path_hierarchy` tokenizer, closes #999.</comment></comments></commit></commits></item><item><title>New default merge policy - `tiered`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/998</link><project id="" key="" /><description>A new merge policy (provided in Lucene 3.2) called tiered which does a better job at merging segments when using near real time.

Provides the following settings:
- `index.merge.policy.expunge_deletes_allowed`: When expungeDeletes is called, we only merge away a segment if its delete percentage is over this threshold.  Default is `10`.
- `index.merge.policy.floor_segment`: Segments smaller than this are "rounded up" to this size, ie treated as equal (floor) size for merge selection.  This is to prevent frequent flushing of tiny segments from allowing a long tail in the index. Default is `2mb`.
- `index.merge.policy.max_merge_at_once`: Maximum number of segments to be merged at a time during "normal" merging. Default is `10`.
- `index.merge.policy.max_merge_at_once_explicit`: Maximum number of segments to be merged at a time, during optimize or expungeDeletes. Default is `30`.
- `index.merge.policy.max_merged_segment`: Maximum sized segment to produce during normal merging.  This setting is approximate: the estimate of the merged segment size is made by summing sizes of to-be-merged segments (compensating for percent deleted docs).  Default is `5gb`.
- `index.merge.policy.segments_per_tier`:  Sets the allowed number of segments per tier.  Smaller values mean more merging but fewer segments. Default is 10.
- `index.reclaim_deletes_weight`: Controls how aggressively merges that reclaim more deletions are favored.  Higher values favor selecting merges that reclaim deletions.  A value of 0.0 means deletions don't impact merge selection. Defaults to `2.0`.
- `index.compound_format`: Should the index be stored in compound format or not. Defaults to `false`.
</description><key id="1004957">998</key><summary>New default merge policy - `tiered`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-05T11:23:17Z</created><updated>2011-07-17T19:10:44Z</updated><resolved>2011-06-05T11:24:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/MergeFactor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/MergePolicyModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/TieredMergePolicyProvider.java</file></files><comments><comment>New default merge policy - `tiered`, closes #998.</comment></comments></commit></commits></item><item><title>Upgrade to Lucene 3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/997</link><project id="" key="" /><description>Upgrade to Lucene 3.2.
</description><key id="1003792">997</key><summary>Upgrade to Lucene 3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-06-04T22:56:46Z</created><updated>2011-06-04T22:57:22Z</updated><resolved>2011-06-04T22:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/bloom/BloomFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/bloom/ObsBloomFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PathHierarchyTokenizerFactory.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/deps/lucene/SimpleLuceneTests.java</file></files><comments><comment>Upgrade to Lucene 3.2, closes #997.</comment></comments></commit></commits></item><item><title>',' can be used in type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/996</link><project id="" key="" /><description>Providing a type with a ',' in it is possible, but it is not possible to query for a type containing a ','::

curl -XPUT 'http://localhost:9200/twitter
curl -XPUT 'http://localhost:9200/twitter/tweet,pfiff/1' -d '{'a':1}'
curl -XPUT 'http://localhost:9200/twitter/tweet,pfiff/_search'
{"took":0,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}

But the document exists::

curl -XPUT 'http://localhost:9200/twitter/_search'
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"twitter","_type":"tweet,pfiff","_id":"1","_score":1.0, "_source" : {a:1}}]}}
</description><key id="998544">996</key><summary>',' can be used in type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jukart</reporter><labels><label>breaking</label><label>bug</label><label>v0.17.0</label></labels><created>2011-06-03T13:31:14Z</created><updated>2011-06-03T22:26:54Z</updated><resolved>2011-06-03T22:26:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-03T22:25:59Z" id="1299830">Yea, good catch. The `,` is used to denote searching on multiple types, thats why its happening. Will push a chance to disallow it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file></files><comments><comment>',' can be used in type name, closes #996.</comment></comments></commit></commits></item><item><title>PatternTokenizer to capture multiple groups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/995</link><project id="" key="" /><description>Hiya

Would it be possible to add an option to the Pattern Tokenizer to return all captured groups.

For instance, I'd like to be able to parse `FTPServer` and return `ftp`, `server`, `ftpserver`.

The regex could look something like this:

```
(([A-Z]+)([A-Z][a-z]+))
```

At the moment, I can only choose one of those matching groups 
</description><key id="998099">995</key><summary>PatternTokenizer to capture multiple groups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-06-03T11:22:19Z</created><updated>2013-04-05T10:29:09Z</updated><resolved>2013-04-05T10:28:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T10:28:25Z" id="15948715">Working on getting this added to Lucene
</comment><comment author="s1monw" created="2013-04-05T10:29:09Z" id="15948742">oh this reminds me of something....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query Parser Module Refactor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/994</link><project id="" key="" /><description>Refactoring of the internal query parser, removing an over-engineered abstraction. While doing it, moved the builders and parsers one package up, which might cause Java code to need to import the new classes.

This is the first phase of the change.
</description><key id="994066">994</key><summary>Query Parser Module Refactor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.17.0</label></labels><created>2011-06-03T01:31:19Z</created><updated>2011-06-03T01:32:49Z</updated><resolved>2011-06-03T01:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/percolator/EmbeddedPercolatorBenchmarkTest.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/percolator/SinglePercolatorStressBenchmark.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/facet/HistogramFacetSearchBenchmark.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/facet/QueryFilterFacetSearchBenchmark.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/facet/TermsFacetSearchBenchmark.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/stress/NodesStressTest.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/stress/SingleThreadBulkStress.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/stress/SingleThreadIndexingStress.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/ExistsFieldQueryExtension.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/FieldQueryExtension.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MissingFieldQueryExtension.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MultiFieldMapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/count/ShardCountRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/deletebyquery/IndexDeleteByQueryRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/deletebyquery/ShardDeleteByQueryRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/Requests.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/alias/IndicesAliasesRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/count/CountRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/deletebyquery/DeleteByQueryRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/search/SearchRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/aliases/IndexAliasesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/AbstractFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/AllFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/NumberFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/TypeFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/AndFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BaseFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BaseQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoolQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoostingQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/BoostingQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomBoostFactorQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomBoostFactorQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomScoreQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/CustomScoreQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/DisMaxQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/DisMaxQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ExistsFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldMaskingSpanQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilterParserFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilteredQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisFieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyLikeThisQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/FuzzyQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IdsFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IdsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserMissingException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/LimitFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/LimitFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MatchAllFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MatchAllFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MatchAllQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MatchAllQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MissingFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NotFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NotFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NumericRangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/NumericRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/OrFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/OrFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/PrefixQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryParserFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/RangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ScriptFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanFirstQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanFirstQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanOrQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanOrQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanTermQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermsFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermsQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TextQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TextQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TopChildrenQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TypeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/WildcardQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/WildcardQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/XContentIndexQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/XContentQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/TextQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/AbstractFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/DateHistogramFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/filter/FilterFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/filter/FilterFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramScriptFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/query/QueryFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/query/QueryFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeScriptFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/statistical/StatisticalFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/statistical/StatisticalScriptFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/InternalSearchRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/FilterBinaryParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/FilterParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/QueryBinaryParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/QueryParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/QueryParserNameParseElement.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/aliases/IndexAliasesServiceTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/percolator/PercolatorExecutorTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/guice/IndexQueryParserModuleTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/guice/MyJsonFilterParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/guice/MyJsonQueryParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPluginTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/plugin/PluginJsonFilterParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/plugin/PluginJsonQueryParser.java</file></files><comments><comment>Query Parser Module Refactor, closes #994.</comment></comments></commit></commits></item><item><title>Aliases: Validate alias filter before adding it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/993</link><project id="" key="" /><description>Aliases: Validate alias filter before adding it
</description><key id="993323">993</key><summary>Aliases: Validate alias filter before adding it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-02T23:23:04Z</created><updated>2011-06-02T23:23:33Z</updated><resolved>2011-06-02T23:23:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/aliases/IndexAliasesTests.java</file></files><comments><comment>Aliases: Validate alias filter before adding it, closes #993.</comment></comments></commit></commits></item><item><title>Query DSL: Simplify not filter to accept the filter directly, without a wrapping `filter` element</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/992</link><project id="" key="" /><description>For example:

```
{
    "filtered" : {
        "query" : {
            "term" : {
                "name.first" : "shay"
            }
        },
        "filter" : {
            "not" : {
                "term" : {
                    "name.first" : "shay1"
                }
            }
        }
    }
}
```

Note, `filter` wrapping element is still needed if wanting to use the `_cache` or `_name` one.
</description><key id="993100">992</key><summary>Query DSL: Simplify not filter to accept the filter directly, without a wrapping `filter` element</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-02T22:26:33Z</created><updated>2011-06-02T22:43:10Z</updated><resolved>2011-06-02T22:43:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/NotFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryParseContext.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/SimpleIndexQueryParserTests.java</file></files><comments><comment>Query DSL: Simplify not filter to accept the filter directly, without a wrapping `filter` element, closes #992.</comment></comments></commit></commits></item><item><title>Get Mapping: Better error response when asking for specific index type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/991</link><project id="" key="" /><description>Get Mapping: Better error response when asking for specific index type (404)
</description><key id="990353">991</key><summary>Get Mapping: Better error response when asking for specific index type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-02T15:29:52Z</created><updated>2011-06-02T15:30:20Z</updated><resolved>2011-06-02T15:30:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetMappingAction.java</file></files><comments><comment>Get Mapping: Better error response when asking for specific index type, closes #991.</comment></comments></commit></commits></item><item><title>Put Mapping: Fix rare case where the response will timeout (ack=false) even though it has been applied</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/990</link><project id="" key="" /><description>Put Mapping: Fix rare case where the response will timeout (ack=false) even though it has been applied
</description><key id="990313">990</key><summary>Put Mapping: Fix rare case where the response will timeout (ack=false) even though it has been applied</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.17.0</label></labels><created>2011-06-02T15:23:02Z</created><updated>2011-06-02T15:23:49Z</updated><resolved>2011-06-02T15:23:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file></files><comments><comment>Put Mapping: Fix rare case where the response will timeout (ack=false) even though it has been applied, closes #990.</comment></comments></commit></commits></item><item><title>Analysis: Improve custom analyzer construction time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/989</link><project id="" key="" /><description>Improve custom analyzers construction time by reusing filters and the like better.
</description><key id="985390">989</key><summary>Analysis: Improve custom analyzer construction time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-01T16:37:12Z</created><updated>2011-06-01T16:37:43Z</updated><resolved>2011-06-01T16:37:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CustomAnalyzerProvider.java</file></files><comments><comment>Analysis: Improve custom analyzer construction time, closes #989.</comment></comments></commit></commits></item><item><title>why no RegexQueryBuilder?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/988</link><project id="" key="" /><description>I need to do some regex search but can not find a QueryBuilder like 
RegexQueryBuilder. 
how should i do regex search under ES?
</description><key id="984200">988</key><summary>why no RegexQueryBuilder?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">weiweiwang</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2011-06-01T12:37:49Z</created><updated>2012-12-17T00:14:22Z</updated><resolved>2012-12-17T00:14:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mbj" created="2011-06-06T11:24:53Z" id="1308592">I would appreciate a RegexpQuery{Builder,Pharser} too. I looked at the coodbase, it should be possible to do this my self. But I need some directions! Can somebody familiar with the ES source post a list of steps?
</comment><comment author="kimchy" created="2011-06-06T12:41:01Z" id="1308899">regex query is not exposed currently because of the bad perfomance behavior it has in Lucene 3.1 (compared to trunk). In general, you really should not use it.
</comment><comment author="mbj" created="2011-06-06T15:04:51Z" id="1309718">Bad performance is not a universal showstopper in my use case. Super bad performance wold be ;) @kimchy Is there a change to convince you? Bad performance is acceptable when it is briefly documented.
</comment><comment author="kimchy" created="2011-06-06T15:06:06Z" id="1309722">You can add it, no problem. My concern is that people will use it even though it will be stated in bold that its not recommended, and many times, you can solve the need to regex in a different manner (stemming, ngram).
</comment><comment author="mbj" created="2011-06-06T15:34:52Z" id="1309899">To give you some background: I started to write a datamapper adapter for elasticsearch. I would like to get the "shared adapter spec" working. The shared adapter spec is a common test for most dm adapters. The shared adapter spec do some regexp queries. Maybe I'll just disable the regexp queries and I'm done.
</comment><comment author="kimchy" created="2011-06-06T16:14:05Z" id="1310189">Ahh, I see, ok, then I guess for compliance it makes sense, and in the next Lucene version (4.0) its going to be much faster anyhow. Ping me on IRC next week, should be a quick 20 minute feature.
</comment><comment author="mbj" created="2011-06-07T09:40:07Z" id="1314684">Thx, I'll try to ping you.
</comment><comment author="jt6211" created="2011-10-01T12:44:46Z" id="2256908">@mbj @kimchy did anything more come from this?  If not, where would you recommend I start, if I wanted to add this capability?  Having regex search capabilities in ES would be very useful for my application.  Thanks.
</comment><comment author="mbj" created="2011-10-02T16:07:09Z" id="2262683">@jt6211 I pinged kimchy on irc. He explained that regexp matching would not be a very fast operation. This was subjected to change with newer lucene versions. That is what I remember.
</comment><comment author="jt6211" created="2011-10-04T13:05:14Z" id="2285892">thanks @mbj
</comment><comment author="eskatos" created="2012-10-16T08:55:14Z" id="9477391">Lucene 4.0 is out, is there an issue tracking it's use in ElasticSearch?
</comment><comment author="s1monw" created="2012-10-16T09:00:10Z" id="9477526">@eskatos  we will work towards 4.0 in the next couple of weeks and this one is certainly one feature that gets added. stay tuned
</comment><comment author="eskatos" created="2012-10-16T09:01:45Z" id="9477573">Excellent news :+1: 
</comment><comment author="eskatos" created="2012-12-11T10:26:06Z" id="11238460">Hey,
Can't find Regex[Query|Filter] in 0.20.1.
Do you have any plans regarding this?
Best regards.
</comment><comment author="uboness" created="2012-12-11T16:56:12Z" id="11251822">@eskatos We plan to add this support to 0.22 which will be based on Lucene 4.0
</comment><comment author="eskatos" created="2012-12-11T18:56:26Z" id="11256986">Cool, thanks for the heads up @uboness !
</comment><comment author="uboness" created="2012-12-17T00:14:21Z" id="11425415">Commit: 8b74c42099d5b7f76b2fa0d3d85d5f03f4e5e234

# Support for "regexp" query and filter types

Matches documents that have fields matching a regular expression. The following discribes the support regular expression syntax:

```
regexp         ::= unionexp
               |
unionexp       ::= interexp | unionexp                          (union)
               |    interexp    
interexp       ::=  concatexp &amp; interexp                        (intersection)                          [OPTIONAL]
               |    concatexp
concatexp      ::=  repeatexp concatexp                         (concatenation)
               |    repeatexp
repeatexp      ::=  repeatexp ?                                 (zero or one occurrence)
               |    repeatexp *                                 (zero or more occurrences)
               |    repeatexp +                                 (one or more occurrences)
               |    repeatexp {n}                               (n occurrences)
               |    repeatexp {n,}                              (n or more occurrences)
               |    repeatexp {n,m}                             (n to m occurrences, including both)
               |    complexp
complexp       ::=  ~ complexp                                  (complement)                            [OPTIONAL]
               |    charclassexp
charclassexp   ::=  [ charclasses ]                             (character class)
               |    [^ charclasses ]                            (negated character class)
               |    simpleexp
charclasses    ::=  charclass charclasses
               |    charclass
charclass      ::=  charexp - charexp                           (character range, incl. end-points)
               |    charexp
simpleexp      ::=  charexp
               |    .                                           (any single character)
               |    #                                           (the empty language)                    [OPTIONAL]
               |    @                                           (any string)                            [OPTIONAL]
               |    "&lt;unicode string without double-quotes&gt;"    (a string)
               |    ( )                                         (the empty string)
               |    ( unionexp )                                (precedence override)
               |    &lt;n-m&gt;                                       (numerical interval)                    [OPTIONAL]
charexp        ::=  &lt;Unicode character&gt;                         (a single non-reserved character)
               |    \ &lt;Unicode character&gt;                       (a single character)
```

[OPTIONAL] syntax above can be controlled by specifying the following flags as part of the DSL:
- `INTERSECTION` - Support for intersection notation: `&lt;expression&gt; &amp; &lt;expression&gt;`
- `COMPLEMENT` - Support for complement notation: `&lt;expression&gt; &amp; &lt;expression&gt;`
- `EMPTY` - Support for the empty language symbol: `#`
- `ANYSTRING` - Support for the any string symbol: `@`
- `INTERVAL` - Support for numerical interval notation: `&lt;n-m&gt;`
- `NONE` - Disable support for all syntax options
- `ALL` - Enables support for all syntax options

This query/filter is based on Lucene 4.0 RegexpQuery which uses automaton to efficiently iterate over index terms. Although with this in place the regexp queries performance great improved, it should be noted that they'll still be considerably slower than typical "exact" match queries (e.g. term, match, range queries). 

## Examples:

A query to match documents where field "name" matches regular expression "j.*n" :

``` json
{
    "regexp" : { "name" : "j.*n" }
}
```

Same query with specific regular expression syntax flags provided

``` json
{
    "regexp" : { "name" : { "value" : "j.*n", "flags" : "INTERSECTION|COMPLEMENT|EMPTY" } 
}
```

Note: This multi term query allows to control how it gets rewritten using the [rewrite](http://www.elasticsearch.org/guide/reference/query-dsl/multi-term-rewrite.html) parameter.

Same syntax applies to filter DSL. All common filter attributes are supported:

``` json
{
    "regexp" : { 
        "name" : { 
            "value" : "j.*n", 
            "flags" : "INTERSECTION|COMPLEMENT|EMPTY" 
        },
        "_name" : "all_johns",
        "_cache" : true,
        "_cache_key" : "my_cache_key" 
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Empty facet list results in NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/987</link><project id="" key="" /><description>In previous versions of ES, a request containing an empty facet list would succeed. In 16.1 (and possibly earlier versions) the request fails. In ES 12.1, the request would succeed.

Example request:

``` javascript
{
  "query": {
    "bool": {
      "must": [],
      "must_not": [],
      "should": [
        {
          "match_all": {}
        }
      ]
    }
  },
  "from": 0,
  "size": 50,
  "sort": [],
  "facets": []
}
```

Response:

``` javascript
{
  "error": "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {NullPointerException[null]}{NullPointerException[null]}{NullPointerException[null]}]",
  "status": 500
}
```

Is this by design?
</description><key id="982498">987</key><summary>Empty facet list results in NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acreeger</reporter><labels /><created>2011-06-01T03:14:37Z</created><updated>2011-06-01T03:45:55Z</updated><resolved>2011-06-01T03:45:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-06-01T03:45:55Z" id="1271928">Nope this was a bug in 0.16.1 and fixed in 0.16.2, released today.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Thrift: Upgrade to thrift 0.6.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/986</link><project id="" key="" /><description>Upgrade to thrift 0.16.1 and also remove some settings parameters. Remove the `type` and always use the `TThreadPoolServer` (which was the default one). And, add a `frame` parameter (default to `-1` which means no frames), with a higher value, like `15mb`, it will use the framed transport.
</description><key id="982289">986</key><summary>Thrift: Upgrade to thrift 0.6.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.17.0</label></labels><created>2011-06-01T01:45:52Z</created><updated>2011-06-01T01:46:39Z</updated><resolved>2011-06-01T01:46:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/Method.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/Rest.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/RestRequest.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/RestResponse.java</file><file>plugins/transport/thrift/src/main/gen-java/org/elasticsearch/thrift/Status.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftRestRequest.java</file><file>plugins/transport/thrift/src/main/java/org/elasticsearch/thrift/ThriftServer.java</file><file>plugins/transport/thrift/src/test/java/org/elasticsearch/thrift/test/SimpleThriftTests.java</file></files><comments><comment>Thrift: Upgrade to thrift 0.6.1, closes #986.</comment></comments></commit></commits></item><item><title>Update Settings: Properly ignore settings that are not allowed to be updated dynamically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/985</link><project id="" key="" /><description>Update Settings: Properly ignore settings that are not allowed to be updated dynamically. (Currently, everything is passed, which is bad since its confusing).
</description><key id="980666">985</key><summary>Update Settings: Properly ignore settings that are not allowed to be updated dynamically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-05-31T19:27:21Z</created><updated>2011-05-31T23:56:55Z</updated><resolved>2011-05-31T19:27:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2011-05-31T19:41:24Z" id="1269602">This kind of solves a part of #981.
It depends on whether you actually do or don't want to give the ability to use GET parameters for the PUT or POST updating the settings. Anyways, `pretty` (the only legal parameter for update settings I think) won't be taken as a setting anymore.
Further discussion on #981?
</comment><comment author="kimchy" created="2011-05-31T23:14:56Z" id="1271029">Yea, this helps with #981. Lets continue there since I don't understand what GET parameters are... :)
</comment><comment author="ofavre" created="2011-05-31T23:56:55Z" id="1271195">Having a PHP background, parameters can come from 2 source: the URL, the request body (often present only in case of `POST`/`PUT` method).

_Do not hesitate to skip the following paragraph, I'm in verbose mode this night..._
The URL is basically `http://host:port/path/to/resource?firstSoCalledGetParam=value&amp;etc`.
`firstSoCalledGetParam` and `etc` are what PHP calls [GET](http://fr2.php.net/manual/en/reserved.variables.get.php) and [POST](http://fr2.php.net/manual/en/reserved.variables.post.php) parameters.
If the request body has a `Content-Type: application/x-www-form-urlencoded`, then the some other values can be given (under the same format `var=val&amp;var2=val2`, with urlencoding). This is what curl does when using the `-d` flag (although it does not change the form of the value to match the `var=val&amp;var2=val2` format).
The latter parameters are called POST parameters in PHP.
When coding a PHP webpage, one will make sure of the origin of the parameter, preventing easy parameter injection by modifying the URL, so that only user friendly parameters are exposed as GET parameters through the URL, and making sure the other one do come from the request body as POST parameters.

**To conclude:** GET and POST parameters somehow separate the presentation from the data.
This is why I proposed reading only POST parameters for a `POST` to the Update Settings API, leaving GET parameters only configure the presentation of the answer.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/field/data/resident/ResidentFieldDataCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/TranslogService.java</file></files><comments><comment>Update Settings: Properly ignore settings that are not allowed to be updated dynamically, closes #985.</comment></comments></commit></commits></item><item><title>Request for more values identified as boolean for the boolean field type - `no`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/984</link><project id="" key="" /><description>Currently the doc says [1] that `0` and `false` are treated as false.
Actually, `off` is too.

I would like to request `no` to be also interpreted as false. (`yes` being treated as true through the fallback "any other value" case)
Generally speaking, matching what is done in parsing JSON configuration/queries should be nice to have for two reasons (IMHO):
- It makes it more consistent, even though document field values are distinct from ES internal understanding of JSON configuration
- It is good to have multiple _english_ ways to express a false value, as they are very often used internally as values by many programmers (and therefore somehow sent to ES through JSON docs!)

Maybe `F` would be good too, but for another reason: this is the term being used internally (and people may want to query using it...... ^^)

  [1] http://www.elasticsearch.org/guide/reference/mapping/core-types.html
</description><key id="979873">984</key><summary>Request for more values identified as boolean for the boolean field type - `no`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-05-31T17:04:45Z</created><updated>2011-05-31T17:19:00Z</updated><resolved>2011-05-31T17:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-31T17:16:39Z" id="1268575">Make sense, will add `no` as a false value.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/Booleans.java</file></files><comments><comment>Request for more values identified as boolean for the boolean field type - `no`, closes #984.</comment></comments></commit></commits></item><item><title>Documentation not precise enough for "term" queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/983</link><project id="" key="" /><description>According to the documentation, term queries are never analyzed [1].
However it seems that they are, at least concerning boolean fields.

Take a look:
https://gist.github.com/1000701
https://gist.github.com/12e8fcd5528193e6119b

The doc [2] says that boolean fields are indexed under the `T` and `F` terms, so the `term` query against that field using `T` or `F` should work well. But both of them end up being considered as true so the results give back the "true" docs.
Querying with `0`, `off` or `false` works as intended... but those values aren't terms!

The doc should be more precise and say something like:

&gt; For `string` fields, the term is taken as is (**not analyzed**),
&gt; whereas it is analyzed for numeric, date and boolean types
&gt; to match the internally equivalent terms in the index.
&gt; In particular, you should not search for `T` or `F` for boolean queries,
&gt; but you should use the regular values (`0`,`1`,`on`,`off`,`true`,`false`) instead.

  [1] http://www.elasticsearch.org/guide/reference/query-dsl/term-query.html
  [2] http://www.elasticsearch.org/guide/reference/mapping/core-types.html
</description><key id="979786">983</key><summary>Documentation not precise enough for "term" queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-05-31T16:52:10Z</created><updated>2013-04-05T10:28:05Z</updated><resolved>2013-04-05T10:28:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-31T17:20:35Z" id="1268596">Yes, the docs probably need to explain a bit more about it. For boolean values, the fact that its indexed as `T` or `F` is basically hidden from the user.
</comment><comment author="Avatah" created="2011-12-21T17:21:56Z" id="3236607">I try to do term facet on boolean field and it returns me 'T' and 'F' values.
But using this values in TermFilter fails - both return only "true" documents.
Maybe 'F' term should also be considered as false?
How to do faceting on boolean field properly?
</comment><comment author="kimchy" created="2011-12-21T21:46:07Z" id="3240105">@Avatah yea, I will push a change where `F` is also considered false so it will be simpler to use.
</comment><comment author="clintongormley" created="2013-04-05T10:28:04Z" id="15948702">Fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indexation of documents broke index mapping </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/982</link><project id="" key="" /><description>Few day ago I was upgraded from ES 0.15.2 to 0.16.1, dropped indexes, created ones from scratch and reindexed all data.

I have document contact with mapping:
contact": {
                    "_all" : { "enabled" : False },
                    "_source" : { "enabled" : False },
                    "properties" : {
                       "id": {
                            "type": "string",
                            "store": "yes"
                        },
..
                        "fields": {
                            "type": "object",
                                "facebook": {
                                    "type": "object",
                                    "dynamic" : False,
                                    "properties":{
                                        "user_name": {
                                            "type": "string",
                                            "index": "not_analyzed",
                                            "omit_term_freq_and_positions": "true"
                                            },
                                        "profile": {
                                            "type": "string",
                                            "index": "not_analyzed",
                                            "omit_term_freq_and_positions": "true"
                                            },
                                        "user_id": {
                                            "type": "string",
                                            "index": "not_analyzed",
                                            "omit_term_freq_and_positions": "true"
                                            },
                                    }
                                },
...
}

After indexation some of documents I have got the following mapping for "fields.facebook" (i have cut begin of mapping - it is ok):
... "facebook":{"properties"
:{"user_name":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type"
:"string"},"user_id":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"profile":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"}}},"linked in email address ":{"type":"string"},"Please contact for more information. ||| jocelyne@fusicology.com\nhttp://www.facebook.com/profile.php?id":{"type":"string"},"http://twitter.com/account/profile_image/x_type_Nat?hreflang":{"type":"string"},"Twitter ":{"type":"string"},"UNYK ":{"type":"string"},"\n&lt;/script&gt;\n&lt;script type":{"type":"string"},"country":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"http://www.facebook.com/profile.php?id":{"type":"string"},"@markyolton ":{"type":"string"},"http://www.facebook.com/home.php?#!/pages/jobTopia-LLC-exactly-what-you-want/250430156088?ref":{"type":"string"},"adress":{"type":"string"},"email":{"properties":{"profile":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"}}},"Skype ":{"type":"string"},"last_name":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"\n  boxbe_size ":{"type":"string"},"department":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"C":{"type":"string"},"leadType":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"state":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"&lt;E2&gt;&lt;80&gt;&lt;A2&gt; RecruitingBlogs Profile: http://recruitingblogs.ning.com/profile/TReedGary\n&lt;E2&gt;&lt;80&gt;&lt;A2&gt; Facebook Profile: http://www.facebook.com/profile.php?id":{"type":"string"},"title":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"T Co.\n------------------------------------\nMobile:   0084.977.755.384\nSkype:    thanh_vuvan\nYahoo:    vuvanthanh109\nAlibaba:   vn105594707\nGmail:     vuvanthanh1rofile?trk":{"type":"string"},"http://www.youtube.com/watch?v":{"type":"string"},"style":{"type":"string"},"dates":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"name":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"&lt;script type":{"type":"string"},"zip":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"https://www.sdn.sap.com/irj/scn/wiki?path":{"type":"string"},"\n  boxbe_theme ":{"type":"string"},"website":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"linkedin":{"properties":{"user_name":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"user_id":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"profile":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"}}},"fernando7405 // Ref. Alias":{"type":"string"},"suzann@myschroepfer.com\nsuzannsch ":{"type":"string"},"highlight link and double click to obtain health insurance quote with BCBS of NC  https://www.bcbsnc.com/assets/shopper/public/quote/index.htm#?pnumber":{"type":"string"},"Invites: LinkedIn.ehab4sap [at] gmail.com\n 966 (0)5 4461 5090 (KSA) &lt;":{"type":"string"},"ref":{"type":"string"},"assistantPhone":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"},"skype":{"properties":{"profile":{"omit_term_freq_and_positions":true,"index":"not_analyzed","type":"string"}}}}}},"_all":{"enabled":false}}}

Looks like as problem in parsings JSON
</description><key id="979449">982</key><summary>Indexation of documents broke index mapping </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">an2deg</reporter><labels /><created>2011-05-31T15:50:49Z</created><updated>2013-04-05T10:26:50Z</updated><resolved>2013-04-05T10:26:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-31T16:06:45Z" id="1268073">Can you gist a recreation? I don't see the exception happening
</comment><comment author="clintongormley" created="2011-05-31T16:24:13Z" id="1268201">I think it is more likely that the JSON that you indexed originally was incorrect, eg quotes not escaped correctly - I'd check the JSON that you are passing to ES
</comment><comment author="an2deg" created="2011-05-31T16:37:06Z" id="1268285">On Tue, May 31, 2011 at 7:24 PM, clintongormley &lt;
reply@reply.github.com&gt;wrote:

&gt; I think it is more likely that the JSON that you indexed originally was
&gt; incorrect, eg quotes not escaped correctly - I'd check the JSON that you are
&gt; passing to ES

It doesn't happen on 0.15.2 but does on 0.16.1. With the same data and with
the same code which are generated the same JSON. Unfortunately I can't look
deeply in this problem by now, will try to find problem document tomorrow.

## 

Andrew Degtyarev
DA-RIPE
</comment><comment author="clintongormley" created="2013-04-05T10:26:50Z" id="15948664">no more info after 2 years - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update settings HTTP API taking GET argument pretty as input</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/981</link><project id="" key="" /><description>When I update the settings of an index using curl in my console and want a pretty printed output, I sure do get a prettified answer, but the index setting now have an extra setting named "index.pretty".

&lt;pre&gt;
curl -XGET 'http://127.0.0.1:9202/test/_settings?pretty=on' -d '{
  "test" : {
    "settings" : {
      "index.number_of_replicas" : "0",
      "index.number_of_shards" : "1"
    }
  }
}
curl -XPUT 'http://127.0.0.1:9202/test/_settings?pretty=on' -d '{"index":{"foo":"bar"}}'
{
  "ok" : true
}
curl -XGET 'http://127.0.0.1:9202/test/_settings?pretty=on' -d {
  "test" : {
    "settings" : {
      "index.number_of_replicas" : "0",
      "index.number_of_shards" : "1",
      "foo" : "bar",
      "pretty" : "on"
    }
  }
}
&lt;/pre&gt;


I understand that another way of setting `foo` to `bar` would be : `curl -XPUT 'http://127.0.0.1:9202/test/_settings?foo=bar'`, thus setting a `pretty` setting won't be possible.

Settings don't have predefined names so potentially any name should be ok, but a distinction should be made between GET and POST/PUT parameters, to my opinion.
(I actually expect this issue to be closed, marked as invalid in less than 1 hour...)
</description><key id="978817">981</key><summary>Update settings HTTP API taking GET argument pretty as input</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2011-05-31T13:58:06Z</created><updated>2013-04-05T10:26:20Z</updated><resolved>2013-04-05T10:26:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-31T23:16:14Z" id="1271034">Heya, so much of this is solved by #985. I am not sure what you mean by GET parameters? You mean parameters that can be passed as part of the request URI? I think its valid to allow to set settings using that as well, no?
</comment><comment author="clintongormley" created="2013-04-05T10:26:20Z" id="15948642">I don't understand this issue, and no more info provided in 2 years - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect json for a boolean query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/980</link><project id="" key="" /><description>The json generated from a boolean query can produce a map with several same keys. The keys being duplicated are "must", "must_not" and "should". I fixed this by generating arrays, as suggested by the doc: http://www.elasticsearch.org/guide/reference/query-dsl/bool-query.html

This bug had no impact on the use of the Java client per se, but has some consequence when the result of a xbuilder is manipulated in java (the Java map makes duplicated key overrides themselves).
</description><key id="975086">980</key><summary>Incorrect json for a boolean query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-05-30T17:16:01Z</created><updated>2014-07-16T21:56:38Z</updated><resolved>2011-05-30T20:54:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-30T17:22:29Z" id="1262431">Yea, it does not have any affects on the parsing side. If we do this change, maybe it makes more sense to have those 3 lists on the builder level, and add to the correct list when adding a clause.
</comment><comment author="nlalevee" created="2011-05-30T18:36:32Z" id="1262854">As suggested, simplification of the code pushed to my master.
The patch assumes that the list returned by BoolQueryBuilder#clauses() is not modified by the user of the API.
</comment><comment author="kimchy" created="2011-05-30T20:14:27Z" id="1263363">Looks good, one more thing :). We don't really need the internal Clause anymore, we can have lists of `XContentQueryBuilder` since we already its occurrence  from the list we work on, and the field name that we pass.
</comment><comment author="nlalevee" created="2011-05-30T20:35:37Z" id="1263464">As suggested, inner class Clause removed.
Thus I changed the function BoolQueryBuilder#clauses() to just be BoolQueryBuilder#hasClauses()
</comment><comment author="kimchy" created="2011-05-30T20:54:25Z" id="1263552">Great, pushed to master. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node Stats: Remove low level transport stats from response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/979</link><project id="" key="" /><description>Node Stats: Remove low level transport stats from response, does not seem to be used, and just adds overhead.
</description><key id="973526">979</key><summary>Node Stats: Remove low level transport stats from response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.17.0</label></labels><created>2011-05-30T09:52:43Z</created><updated>2011-05-30T09:53:17Z</updated><resolved>2011-05-30T09:53:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/cluster/node/stats/RestNodesStatsAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/TransportStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file></files><comments><comment>Node Stats: Remove low level transport stats from response, closes #979.</comment></comments></commit></commits></item><item><title>Plugins: Allow plugins to serve a _site, automatically download github plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/978</link><project id="" key="" /><description>Plugins can now have sites in them, any plugin that exists under the `plugins` directory with an `_site` directory, its content will be statically served when hitting `/_plugin/[plugin_name]/`.

Installed plugins that do not contain any java related content, will automatically be detected as site plugins, and their content will be moved under `_site`.

Plugins will automatically be downloaded from github if they follow the `user_name/repo_name` structure. 

For example, in order to use the `mobz/elasticsearch-head` plugin, we can execute: `bin/plugin -install mobz/elasticsearch-head`. Then, we can point our browser to `http://localhost:9200/_plugin/head/` and see the site.
</description><key id="968497">978</key><summary>Plugins: Allow plugins to serve a _site, automatically download github plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-05-28T15:42:50Z</created><updated>2011-06-09T22:45:35Z</updated><resolved>2011-05-28T15:43:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-05-30T09:25:00Z" id="1260668">Very nice!
</comment><comment author="mahendra" created="2011-05-30T11:32:02Z" id="1261002">Coolness!! :-)
</comment><comment author="medcl" created="2011-06-02T04:44:48Z" id="1278974">coolala
</comment><comment author="mobz" created="2011-06-02T07:00:40Z" id="1279372">I will definitely be promoting this as _the_ way to use es-head once 0.17 is out. As well as overall awesomeness, there are really useful features of html5 that you can only use when serving the site from http: rather than file:
</comment><comment author="mahendra" created="2011-06-02T07:13:18Z" id="1279401">I feel this blurs the difference between CouchDB and Elasticsearch. CouchDB is a big proponent of hosted HTML/JS apps. With this (maybe) one can host search driven apps right inside ES. :-)
</comment><comment author="janl" created="2011-06-02T07:17:05Z" id="1279409">Cool Stuff :)
</comment><comment author="karmi" created="2011-06-02T13:16:49Z" id="1280597">Awesome!!!
</comment><comment author="lukas-vlcek" created="2011-06-09T22:35:45Z" id="1337850">Will it always refer to master in repo? Or is there any way to refer to particular tags or even commits?
(I was looking into code, it seems there is a way how to specify at least version but it is not yet documented.)
</comment><comment author="kimchy" created="2011-06-09T22:45:35Z" id="1337901">It will try and download first based on the elasticsearch version, first, download a file (from github download, names with ES version), then a tag, and last, master. So, you can simply tag your project and match the elasticsearch version that comes out.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/HttpServer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/BytesRestResponse.java</file></files><comments><comment>Plugins: Allow plugins to serve a _site, automatically download github plugins, closes #978.</comment></comments></commit></commits></item><item><title>Mapping - Date Format: 3 or more custom date formats fail using '||' delimiter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/977</link><project id="" key="" /><description>When using 3 or more custom date formats using the `||` delimiter, it will fail to parse the extra ones. For example: `MM/dd/yyyy||MM-dd-yyyy||yyyy-MM-dd HH:mm:ss` will fail to handle the last format.
</description><key id="968077">977</key><summary>Mapping - Date Format: 3 or more custom date formats fail using '||' delimiter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-28T10:22:01Z</created><updated>2011-05-28T10:22:40Z</updated><resolved>2011-05-28T10:22:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/joda/Joda.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/deps/joda/SimpleJodaTests.java</file></files><comments><comment>Mapping - Date Format: 3 or more custom date formats fail using '||' delimiter, closes #977.</comment></comments></commit></commits></item><item><title>Query DSL: Add limit filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/976</link><project id="" key="" /><description>A limit filter which limits the number of hits (on a shard level). For example:

```
{
    "filtered" : {
        "filter" : {
             "limit" : {"value" : 100}
         },
         "query" : {
            "term" : { "name.first" : "shay" }
        }
    }
}
```

The filter can be also placed (as any other filter) in facets to limit the number of documents the facet executes on.
</description><key id="968036">976</key><summary>Query DSL: Add limit filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-28T09:37:18Z</created><updated>2011-05-30T07:46:04Z</updated><resolved>2011-05-28T09:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aloiscochard" created="2011-05-30T07:46:04Z" id="1260302">Will test it ASAP, thanks a lot shay !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/LimitFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/NoCacheFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractWeightedFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FilterBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/LimitFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/LimitFilterParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/SimpleIndexQueryParserTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Query DSL: Add limit filter, closes #976.</comment></comments></commit></commits></item><item><title>Request to add new Integer Bitwise operator for query DSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/975</link><project id="" key="" /><description>Please add bitwise operator which would allows query similar
to the following SQL command in elasticsearch.

Assuming we have

   currentUserAccessLevelVariable = (int64) 25;

To be able to perform the following psudo sql query:

   SELECT  column1, column2 ..., AccessRightColumn
   FROM someTableName
   WHERE bitwise_AND( AccessRightColumn,
currentUserAccessLevelVariable ) &gt; 0 
</description><key id="964101">975</key><summary>Request to add new Integer Bitwise operator for query DSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gluhur</reporter><labels><label>:Mapping</label><label>:Search</label><label>discuss</label><label>feature</label><label>feedback_needed</label></labels><created>2011-05-27T11:44:45Z</created><updated>2016-07-13T19:00:35Z</updated><resolved>2014-09-29T16:03:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-30T08:22:58Z" id="1260457">Btw, you can do that using a script filter: http://www.elasticsearch.org/guide/reference/query-dsl/script-filter.html.
</comment><comment author="gluhur" created="2011-05-30T12:22:55Z" id="1261148">Thanks for getting back to me so quickly. I am not so sure what MVEL language is, but will give it a go now.
</comment><comment author="kimchy" created="2011-05-30T16:38:51Z" id="1262238">I think we can keep this issue open, as we can have specific implementation for this type of field. In a script (which does incur perf overhead), you can do something like: `doc['field_name'] &amp; 12 == 0` (not tested).
</comment><comment author="gaydenko" created="2013-05-27T11:46:10Z" id="18494701">Is some progress expected? The thing is, at situation with plenty of boolean fields (say, 20 or 30) it would be blazingly faster to use bitwise operation (they are very cheap for CPU) rather to process dozen of boolean fields, when all the Lucene mechanics is in play, and this isn't the end of additional work list :)
</comment><comment author="gaydenko" created="2013-06-01T14:19:16Z" id="18790101">I have found upstream issue: https://issues.apache.org/jira/browse/LUCENE-2460
</comment><comment author="clintongormley" created="2014-07-18T08:14:12Z" id="49406231">Could add a bit field type which would index an integer as individual bits - 1 term per bit.  Then we would need a filter to support bitwise operations.
</comment><comment author="shuhaozhang" created="2014-09-23T13:39:24Z" id="56520504">Hi,kimchy  I have a question. I just write (doc['field_name'] &amp; 12) == 0  ,but it seems that (doc['field_name'] &amp; 12) return null value. And I write (3&amp;3)==3,it is true,which proves that &amp; operator is supported. I am sure the type of doc['field_name'] is a number, could you please tell me why? Thank you so much!!!!
</comment><comment author="clintongormley" created="2014-09-23T13:45:51Z" id="56521496">@shuhaozhang try `doc['field_name'].value &amp; 12`
</comment><comment author="redserpent7" created="2015-06-07T14:12:13Z" id="109759283">Did this issue get addressed or are we still using scripts?
</comment><comment author="tarmath" created="2015-08-20T17:34:10Z" id="133087647">+1 for not using scripts for this? :)
</comment><comment author="Artistan" created="2016-07-11T13:13:07Z" id="231729973">why was this closed? it was low hanging fruit and then just closed???
Please reopen, thanks!!!
</comment><comment author="brino" created="2016-07-11T13:19:35Z" id="231731563">Another +1 for the non-scripted approach!
</comment><comment author="clintongormley" created="2016-07-11T15:23:24Z" id="231768115">What are the use cases for this field?  What are you trying to achieve that you wouldn't be able to achieve with existing functionality?
</comment><comment author="clintongormley" created="2016-07-11T15:28:40Z" id="231769787">@nknize is this something that could be done with dimensional points?
</comment><comment author="Artistan" created="2016-07-11T15:30:21Z" id="231770412">@clintongormley :: we have category flags and status flags that are stored via binary values (int) and we use this to search for specific companies in MySQL. We have migrated most of our search to elasticsearch and would like to finish the migration with efficient searches such as (company.cat &amp; 16).  The scripting makes this inefficient. I believe that the bitwise operations would make this very fast.
</comment><comment author="nknize" created="2016-07-11T19:07:07Z" id="231833129">@Artistan what's the issue using the new Numeric points in 5.0? Unless I'm missing something it seems your example could be solved by creating explicit category and status fields as `integer` types (lucene IntegerPoint) and using boolean combination of `termQuery` (exact), `termsQuery` (set), and/or `rangeQuery`?  With DimensionalPoints these queries are much more efficient than having to worry about bit masking your queries.
</comment><comment author="Artistan" created="2016-07-13T02:29:37Z" id="232237593">@nknize not sure how to do that with elasticsearch. could you elaborate.

@clintongormley -- https://github.com/elastic/elasticsearch/issues/975#issuecomment-49406231
 I would assume that by having a bit field type added, then we could use a filter similar to...
    // having a query + filtered by multiple matches....
    elastic[query][filtered][filter][bool][must][][_bit_][categories][**AND**]=**CategoryBit**
</comment><comment author="clintongormley" created="2016-07-13T19:00:35Z" id="232453738">@nknize i agree that this would be easy to do by breaking things out such as:

```
"cats": ["foo", "bar", "baz" ]
```

Robert suggested that there may be a way to do the equivalent of `field &amp; 16` using dimensional points.  Not sure if this would be better or worse than the above.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to selectivley enable REST commands and add new REST commands</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/974</link><project id="" key="" /><description>Add the ability for plugins to selectivley enable (whitelist) REST commands, and add new REST commands.
</description><key id="963505">974</key><summary>Add ability to selectivley enable REST commands and add new REST commands</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">itaifrenkel</reporter><labels /><created>2011-05-27T09:43:45Z</created><updated>2013-04-05T10:22:45Z</updated><resolved>2013-04-05T10:22:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="itaifrenkel" created="2011-05-27T09:46:01Z" id="1248208">Here is a plugin sample. Pull request would follow:

```
 @Override public void processModule(Module module) {
    if (module instanceof RestModule) {

        RestModule restModule = (RestModule) module;

        //enable only specific REST actions
        restModule.removeDefaultRestActions();
        restModule.addRestAction(RestMainAction.class);
        restModule.addRestAction(RestGetAction.class);
        restModule.addRestAction(RestClusterStateAction.class);
        restModule.addRestAction(RestClusterHealthAction.class);

        //add our own custom hello rest  actions
        restModule.addRestAction(RestPostHelloAction.class);
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Hidden files indexed in mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/973</link><project id="" key="" /><description>Found issue with hidden files attempting to be indexed when mappings being loaded. This allows mappings to skip hidden files.

failed to read / parse mapping [] from location [/opt/elastic/current/config/mappings/n233/.svn], ignoring...
java.io.FileNotFoundException: /opt/elastic/current/config/mappings/n233/.svn (Is a directory)
        at java.io.FileInputStream.open(Native Method)
</description><key id="961177">973</key><summary>Hidden files indexed in mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bterm</reporter><labels /><created>2011-05-26T20:49:38Z</created><updated>2014-07-16T21:56:39Z</updated><resolved>2011-05-26T20:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-26T20:51:12Z" id="1245319">pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>doc['field'] mvel fails to parse properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/972</link><project id="" key="" /><description>See gist https://gist.github.com/993649
for a full example of a lookup of doc['location'] within a search query that fails with the following error:

```
"failed" : 2,
"failures" : [ {
  "reason" : "CompileException[[Error: No field found for [org.elasticsearch.index.mapper.xcontent.geo.GeoPointDocFieldData@16a8423]]\n[Near
```

: {... doc[location].distance(32.7, - ....}]\n ^\n[Line: 1, Column: 1]]; nested: ElasticSearchIllegalArgumentException[No field found
for [org.elasticsearch.index.mapper.xcontent.geo.GeoPointDocFieldData@16a8423]]; "
</description><key id="960757">972</key><summary>doc['field'] mvel fails to parse properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dkoneill</reporter><labels /><created>2011-05-26T19:52:14Z</created><updated>2013-04-05T10:22:27Z</updated><resolved>2013-04-05T10:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-29T18:13:55Z" id="1257712">I think that I know where this failure is coming from. It basically curl removing the `'` character around the `location`, which causes it to break (because we use `-d '....'`. One option is to use, when using curl: `doc[\"location\"].distance(...)`, or, another one is to use `doc.location.distance(...)`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aliases: add an ability to specify filters on aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/971</link><project id="" key="" /><description>An alias can now have a filter associated with it. Aliases with filters provide an easy way to create different "views" of the same index. The filter can be defined using Query DSL and is applied to all Search, Count, Delete By Query and More Like This operations with this alias. 

&lt;pre&gt;
$ curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "index" : "test1",
                 "alias" : "alias2",
                 "filter" : { "term" : { "user" : "kimchy" } }
            }
        }
    ]
}'
&lt;/pre&gt;
</description><key id="959251">971</key><summary>Aliases: add an ability to specify filters on aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>feature</label><label>v0.17.0</label></labels><created>2011-05-26T15:23:58Z</created><updated>2011-05-26T15:53:22Z</updated><resolved>2011-05-26T15:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-26T15:53:22Z" id="1243241">Implemented by @imotov.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add limited faceting functionality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/970</link><project id="" key="" /><description>Add possibility to limit the faceting processing on &lt;X&gt; first hits to prevent performance degradation with lot of documents.

Performance are specially bad when using a facet on a multi-term with a query returning 10M document, would be cool to be able to limit faceting to let say 5000 first hits.
</description><key id="957399">970</key><summary>Add limited faceting functionality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aloiscochard</reporter><labels /><created>2011-05-26T08:25:41Z</created><updated>2011-07-12T06:37:11Z</updated><resolved>2011-07-11T21:13:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-28T09:42:51Z" id="1253568">Added #976. This means you can place that filter as a `facet_filter` within the facets you care about, and limit the number of docs they execute. Note though, that this limit is per shard.
</comment><comment author="kimchy" created="2011-07-11T21:13:12Z" id="1549989">Closing this, I think the facet solves the issue.
</comment><comment author="aloiscochard" created="2011-07-12T06:37:11Z" id="1552307">yep sorry to haven't give you feedback before, but it's OK
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: Ids Filter / Query - allow to execute it with no type defined / several types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/969</link><project id="" key="" /><description>Make the `type` parameter optional, or allow to provide an array of `types` instead of just a single `type`.
</description><key id="957362">969</key><summary>Query DSL: Ids Filter / Query - allow to execute it with no type defined / several types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-26T08:16:27Z</created><updated>2011-05-26T08:17:32Z</updated><resolved>2011-05-26T08:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FilterBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/UidFilter.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Query DSL: Ids Filter / Query - allow to execute it with no type defined / several types, closes #969.</comment></comments></commit></commits></item><item><title>Add support for filtering aliases to DeleteByQuery and MoreLikeThis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/968</link><project id="" key="" /><description>DeleteByQuery serialization in Translog is now using version
</description><key id="955900">968</key><summary>Add support for filtering aliases to DeleteByQuery and MoreLikeThis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-25T23:38:30Z</created><updated>2014-07-16T21:56:39Z</updated><resolved>2011-05-26T09:10:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-26T09:10:22Z" id="1241116">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for filtering aliases to DeleteByQuery and MoreLikeThis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/967</link><project id="" key="" /><description /><key id="953735">967</key><summary>Add support for filtering aliases to DeleteByQuery and MoreLikeThis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-25T16:16:17Z</created><updated>2014-07-16T21:56:40Z</updated><resolved>2011-05-25T23:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for setting the boost on a text query in the Java client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/966</link><project id="" key="" /><description>It is supported on the server side, but not on the java client side.
</description><key id="952594">966</key><summary>Support for setting the boost on a text query in the Java client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-05-25T12:11:22Z</created><updated>2014-07-16T21:56:40Z</updated><resolved>2011-05-25T13:29:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-25T13:29:11Z" id="1235258">pushed to both master and 0.16 branch,thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for filtering aliases to count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/965</link><project id="" key="" /><description /><key id="950705">965</key><summary>Add support for filtering aliases to count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-25T01:54:22Z</created><updated>2014-06-18T07:45:27Z</updated><resolved>2011-05-25T04:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-25T04:11:05Z" id="1233123">pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: `query_string` - Expose QueryParser#setAutoGeneratePhraseQueries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/964</link><project id="" key="" /><description>From Lucene 3.1 the default value of autoGeneratePhraseQueries is change from true to false. It changes the result of search. We need a way to change it.

the `auto_generate_phrase_queries` parameter can be set to `true` in both `query_string` and `field` queries.
</description><key id="946628">964</key><summary>Query DSL: `query_string` - Expose QueryParser#setAutoGeneratePhraseQueries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IoriH</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-24T11:40:48Z</created><updated>2011-05-25T00:59:12Z</updated><resolved>2011-05-25T00:59:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryStringQueryBuilder.java</file></files><comments><comment>Query DSL: `query_string` - Expose QueryParser#setAutoGeneratePhraseQueries, closes #964.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/QueryParserSettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryStringQueryParser.java</file></files><comments><comment>Query DSL: `query_string` - Expose QueryParser#setAutoGeneratePhraseQueries, closes #964.</comment></comments></commit></commits></item><item><title>Add support for filtering aliases to search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/963</link><project id="" key="" /><description>Had to move a couple of things around in order to implement the optimization that we discussed. Also added more tests to cover combinations of aliases and indices.
</description><key id="945448">963</key><summary>Add support for filtering aliases to search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-24T05:48:13Z</created><updated>2014-07-01T03:02:05Z</updated><resolved>2011-05-25T00:07:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-25T00:07:08Z" id="1232348">cool, pushed. Next up count, and then, we need to talk about the rest of the APIs and see where alias filters can be used. Don't think there are any more, maybe delete_by_query (applying it to index/delete/get just does not make sense perf wise).
</comment><comment author="imotov" created="2011-05-25T00:34:28Z" id="1232422">Thanks! Count, delete by query, and more like this - that's all I can think of. In case of more like this, the index/alias is used for follow-up search by default. So, we just need to make sure that the original index/alias is not converted to a concrete index too early.
</comment><comment author="kimchy" created="2011-05-25T00:36:02Z" id="1232426">great. Once those APIs are done, then all we have left to complete the "grand" feature is to allow to associate routing value with an alias as well.
</comment><comment author="imotov" created="2011-05-26T11:08:13Z" id="1241585">Associating routing value with an alias seems to be quite simple and straight-forward. However, I don't quite understand the use case for that, which makes me think that I might be missing something.
</comment><comment author="kimchy" created="2011-05-26T14:08:33Z" id="1242527">The use case is creating a single index that hosts many different users, and a user maps to an alias on that index (and a filter). If the alias also holds the routing value (the username), then all the data for that user will be directed to a single shard, and search will be heavily optimized to execute only on that shard. You can then create a single index with many many shards, without worrying too much about the cost of doing distributed search across them per user.
</comment><comment author="imotov" created="2011-05-26T14:40:42Z" id="1242738">That is similar to the use case that I had in mind. But I just couldn't see an advantage of using aliases for that instead of real indices. As far as I understand, most of the overhead comes from a shard and we would end up with the same number of shards in both cases. 

I guess aliases with routing might be useful if there is a need to search the whole index from time to time. For example, if we have several organizations, we can create one index per organization and use aliases with routing for users. 

Should we allow to specify different routing values for search operations and indexing operations? So, it would be possible to index into a particular shard, but search multiple shards.
</comment><comment author="kimchy" created="2011-05-26T14:42:49Z" id="1242760">It will provide a benefit, since you have many users -&gt; shard mapping compared to single user -&gt; shard use case. So you don't have the shard per user overhead.

Yes, we should provide index_routing and search_routing, and a simplified routing parameter that sets both.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Client nodes disagree with data nodes about started shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/962</link><project id="" key="" /><description>With 0.15.2, we had an instance where a set of client nodes thought that a shard was failed even though the data nodes did not.  Restarting one of the client nodes caused the rest of the client+data nodes to agree.  We are unable to reproduce this issue.
</description><key id="942562">962</key><summary>Client nodes disagree with data nodes about started shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2011-05-23T17:32:42Z</created><updated>2013-04-05T10:22:11Z</updated><resolved>2013-04-05T10:22:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-05T10:22:11Z" id="15948500">No more info after 2 years  - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>adding eclipse support and an Ec2NameResolver</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/961</link><project id="" key="" /><description>- adds a little bit to gradle.build files that allows someone to simple use 'gradle eclipse' to generate eclipse project and classpath files.
- also adds the Ec2NameResolver which is part 1 of https://github.com/elasticsearch/elasticsearch/issues/940

(unfortunately my commit went a little awry and added the Ec2NameResolver to what was supposed to just be eclipse support)
</description><key id="941615">961</key><summary>adding eclipse support and an Ec2NameResolver</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keteracel</reporter><labels /><created>2011-05-23T14:28:59Z</created><updated>2014-07-16T21:56:42Z</updated><resolved>2011-06-15T19:23:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-23T22:17:35Z" id="1224687">Heya,

   the `Ec2NameResolver` should not use the `_` wrapping the values, since they get stripped out before using the name resolver. Also, I think that if resolving the default fails, it should log it and return null.

   I guess that you haven't tested it, since its missing the code that adds the resolver to the NetworkService?
</comment><comment author="keteracel" created="2011-05-24T10:33:52Z" id="1227334">&gt; I guess that you haven't tested it, since its missing the code that adds the resolver to the NetworkService?

I was going to add a comment about that. I put it in Ec2Discovery initially but was going to ask if that was the correct place to put it. You previously said "The cloud plugin can be injected with it [NetworkService]", but the CloudAwsPlugin class doesn't look appropriate.
</comment><comment author="kimchy" created="2011-05-24T10:37:53Z" id="1227348">Yea, Ec2Discovery is the place to put it.
</comment><comment author="keteracel" created="2011-05-24T11:26:30Z" id="1227554">gonna do a round of testing on a 2 node ec2 cluster will report back when done
</comment><comment author="keteracel" created="2011-06-15T15:13:04Z" id="1374300">Sorry, our release got in the way. Back on this now.
</comment><comment author="keteracel" created="2011-06-15T16:32:30Z" id="1374961">hmm, so using zero conf it seems to pick up the Ec2NameResolver. I'm not sure that's completely wrong if you've added the aws cloud plugin. What do you think?
</comment><comment author="keteracel" created="2011-06-15T16:32:53Z" id="1374965">[2011-06-15 16:21:16,535][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2011-06-15 16:21:16,546][INFO ][node                     ] [Grey, Rachel] {elasticsearch/0.17.0-SNAPSHOT/2011-06-15T15:15:15}[1039]: initializing ...
[2011-06-15 16:21:16,550][INFO ][plugins                  ] [Grey, Rachel] loaded [cloud-aws]
[2011-06-15 16:21:19,040][INFO ][node                     ] [Grey, Rachel] {elasticsearch/0.17.0-SNAPSHOT/2011-06-15T15:15:15}[1039]: initialized
[2011-06-15 16:21:19,040][INFO ][node                     ] [Grey, Rachel] {elasticsearch/0.17.0-SNAPSHOT/2011-06-15T15:15:15}[1039]: starting ...
[2011-06-15 16:21:19,077][INFO ][cloud.aws.network        ] obtaining ec2 hostname from ec2 meta-data url http://169.254.169.254/latest/meta-data/local-ipv4
[2011-06-15 16:21:19,130][INFO ][cloud.aws.network        ] obtaining ec2 hostname from ec2 meta-data url http://169.254.169.254/latest/meta-data/local-ipv4
[2011-06-15 16:21:19,132][INFO ][transport                ] [Grey, Rachel] bound_address {inet[/10.235.47.243:9300]}, publish_address {inet[/10.235.47.243:9300]}
[2011-06-15 16:21:38,938][INFO ][cluster.service          ] [Grey, Rachel] new_master [Grey, Rachel][EIdgJSFURyOAmTtPwXUcFw][inet[/10.235.47.243:9300]], reason: zen-disco-join (elected_as_master)
[2011-06-15 16:21:49,150][WARN ][discovery                ] [Grey, Rachel] waited for 30s and no initial state was set by the discovery
[2011-06-15 16:21:49,150][INFO ][discovery                ] [Grey, Rachel] elasticsearch/EIdgJSFURyOAmTtPwXUcFw
[2011-06-15 16:21:49,151][INFO ][cloud.aws.network        ] obtaining ec2 hostname from ec2 meta-data url http://169.254.169.254/latest/meta-data/local-ipv4
[2011-06-15 16:21:49,155][INFO ][cloud.aws.network        ] obtaining ec2 hostname from ec2 meta-data url http://169.254.169.254/latest/meta-data/local-ipv4
[2011-06-15 16:21:49,155][INFO ][http                     ] [Grey, Rachel] bound_address {inet[/10.235.47.243:9200]}, publish_address {inet[/10.235.47.243:9200]}
[2011-06-15 16:21:49,156][INFO ][node                     ] [Grey, Rachel] {elasticsearch/0.17.0-SNAPSHOT/2011-06-15T15:15:15}[1039]: started
[2011-06-15 16:21:51,025][INFO ][gateway                  ] [Grey, Rachel] recovered [0] indices into cluster_state
</comment><comment author="kimchy" created="2011-06-15T17:18:25Z" id="1375279">Looks good. Need to think a bit about it and if it make sense to have it enabled even when not using the ec2 discovery (like using unicast zen disco, but still wanting to use the ec2 name resolved).
</comment><comment author="kimchy" created="2011-06-15T19:22:32Z" id="1376140">So, I pulled the changes, and did some more changes, mainly around 2 things: the first is to register the resolver regardless if the ec2 discovery is used or not, and the second, not to change the default (as it is problematic, I think).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Broken java client for phrase query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/960</link><project id="" key="" /><description>Trivial fix regarding the build of the json query
</description><key id="940939">960</key><summary>Broken java client for phrase query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2011-05-23T11:47:13Z</created><updated>2014-07-16T21:56:42Z</updated><resolved>2011-05-23T21:49:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-23T21:49:13Z" id="1224542">Pushed to master and 0.16 branch, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter Cache: Introduce new `node` level filter cache and make it default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/959</link><project id="" key="" /><description>A new filter cache type called `node` which is also the default. The benefit of this cache is the fact that a configuration can be set to how much memory filter caching is allowed to take on the node level, regardless of the number fo shards allocated and no need to rely on "soft" caching from the VM.

The filter cache type is called `node`, and it has a single setting that _is not an index level setting_ but a node level settings (i.e. configure it in the elasticsearch.(ym;|json) file. The setting, named `indices.cache.filter.size` controls the size the filter cache will take on the node level. It can accept both a percentage parameter (for example, `25%`) and a strict byte size value (`3gb` for example). It defaults to `20%`.
</description><key id="939371">959</key><summary>Filter Cache: Introduce new `node` level filter cache and make it default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-23T01:24:24Z</created><updated>2011-05-23T01:25:15Z</updated><resolved>2011-05-23T01:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/FilterCacheModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/node/NodeFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractWeightedFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/FilterCacheHelper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/FilterCacheValue.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/IndicesModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cache/filter/IndicesNodeFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/document/DocumentActionsTests.java</file></files><comments><comment>Filter Cache: Introduce new `node` level filter cache and make it default, closes #959.</comment></comments></commit></commits></item><item><title>Field Cache: Change default type to `resident` from `soft`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/958</link><project id="" key="" /><description>Most times, field cache should not be handled with soft evictions, but actually be resident, since we rarely want to unload it.
</description><key id="936982">958</key><summary>Field Cache: Change default type to `resident` from `soft`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-22T04:27:07Z</created><updated>2011-05-22T04:27:19Z</updated><resolved>2011-05-22T04:27:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-22T04:27:19Z" id="1216323">Pushed as part of #957.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Field Cache: Allow to set `index.cache.field.expire` when using `resident` type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/957</link><project id="" key="" /><description>Field Cache: Allow to set `index.cache.field.expire` when using `resident` type
</description><key id="936968">957</key><summary>Field Cache: Allow to set `index.cache.field.expire` when using `resident` type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-22T04:13:23Z</created><updated>2011-05-22T04:22:56Z</updated><resolved>2011-05-22T04:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/field/data/FieldDataCacheModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/field/data/resident/ResidentFieldDataCache.java</file></files><comments><comment>Field Cache: Allow to set `index.cache.field.expire` when using `resident` type, closes #957.</comment></comments></commit></commits></item><item><title>Add IndexAliasService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/956</link><project id="" key="" /><description>Updated according to your comments
</description><key id="936766">956</key><summary>Add IndexAliasService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-22T01:19:14Z</created><updated>2014-07-16T21:56:43Z</updated><resolved>2011-05-22T04:12:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-22T04:12:40Z" id="1216302">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add IndexAliasService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/955</link><project id="" key="" /><description>Added IndexAliasService as per our discussion. Not sure how to test logic IndicesClusterStateService level until IndexAliasService is actually used somewhere. The logic of obtaining filter parser in IndexAliasesService.parser can be, probably, improved as well.
</description><key id="936689">955</key><summary>Add IndexAliasService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-22T00:08:02Z</created><updated>2014-06-21T21:01:30Z</updated><resolved>2011-05-22T01:16:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Shared Gateway: Allow to dynamically update the `snapshot_interval` using update settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/954</link><project id="" key="" /><description>Allow to dynamically update the `index.gateway.snapshot_interval` setting using the update settings API.
</description><key id="934363">954</key><summary>Shared Gateway: Allow to dynamically update the `snapshot_interval` using update settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.17.0</label></labels><created>2011-05-20T23:20:34Z</created><updated>2011-05-20T23:31:36Z</updated><resolved>2011-05-20T23:31:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file></files><comments><comment>Shared Gateway: Allow to dynamically update the `snapshot_interval` using update settings, closes #954.</comment></comments></commit></commits></item><item><title>NPE when using "not" filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/953</link><project id="" key="" /><description>I can get a NullPointerException in 0.16.1 using a not filter.  It seems like a similar issue to 892.  

Steps to Reproduce:
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
"user" : "kimchy",
"post_date" : "2009-11-15T14:12:12",
"message" : "trying out Elastic Search"
}'

curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '{"query" : { 
  "filtered" : {
    "query" : {"term" : {"user" : "kimchy"}}, 
    "filter" : {
      "and" : [
        {
            "not":{
              "filter":{
                "query":{"wildcard":{"user":"zzz*"}}}
               }
          }
      ]
      }}}}'

Stack trace:
org.elasticsearch.search.query.QueryPhaseExecutionException: [twitter][2]: query[filtered(user:kimchy)-&gt;org.elasticsearch.common.lucene.search.AndFilter@9496f72d],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:215)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:222)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:134)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.lucene.docset.NotDocIdSet$NotDocIdSetIterator.initialize(NotDocIdSet.java:64)
    at org.elasticsearch.common.lucene.docset.NotDocIdSet$NotDocIdSetIterator.&lt;init&gt;(NotDocIdSet.java:58)
    at org.elasticsearch.common.lucene.docset.NotDocIdSet.iterator(NotDocIdSet.java:49)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:124)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:517)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:174)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:384)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:291)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:279)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:211)
    ... 9 more
</description><key id="932881">953</key><summary>NPE when using "not" filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasongilman</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-20T17:11:16Z</created><updated>2011-05-20T19:16:58Z</updated><resolved>2011-05-20T19:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/AllDocSet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/NotDocIdSet.java</file></files><comments><comment>NPE when using "not" filter, closes #953.</comment></comments></commit></commits></item><item><title>Scripting: Optimize to native script execution when using just `doc.score`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/952</link><project id="" key="" /><description>Scripting: Optimize to native script execution when using `doc.score`
</description><key id="932400">952</key><summary>Scripting: Optimize to native script execution when using just `doc.score`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-20T15:15:13Z</created><updated>2011-05-20T15:16:16Z</updated><resolved>2011-05-20T15:16:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/ScriptService.java</file></files><comments><comment>Scripting: Optimize to native script execution when using just `doc.score`, closes #952.</comment></comments></commit></commits></item><item><title>[Feature Request] Suspend a river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/951</link><project id="" key="" /><description>As writen in the mailing list : http://elasticsearch-users.115913.n3.nabble.com/Suspend-a-CouchDB-river-tp2964592p2964592.html

It’s not possible to suspend a CouchDB river without deleting it.
If I delete it, I will have to restart it from scratch.
By suspending it, I will be able to restart the river from the last sequence number given by _changes API.

Thanks,
David.
</description><key id="932072">951</key><summary>[Feature Request] Suspend a river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-05-20T13:48:59Z</created><updated>2013-04-05T12:23:32Z</updated><resolved>2013-04-05T10:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="maxogden" created="2011-07-31T23:51:02Z" id="1697086">+1
</comment><comment author="dadoonet" created="2011-09-02T14:50:14Z" id="1979663">Hi,

I have seen that curl -XPOST 'localhost:9200/_river/_close' seems to suspend the river.
To restart it, I have only to do curl -XPOST 'localhost:9200/_river/_open'

So, I suggest to close this issue and I will add a pull request on elasticsearch.github.com to document this.

Agree ?

Thanks
</comment><comment author="Andrewsville" created="2011-09-02T15:13:53Z" id="1979892">Doesn't it suspend ALL rivers in the cluster? That might not be exactly what you want.
</comment><comment author="dadoonet" created="2011-09-03T17:57:54Z" id="1987788">@Andrewsville : sure. But I have only one river at this time. So it works.
</comment><comment author="tommymonk" created="2012-08-03T13:15:11Z" id="7483020">+1
</comment><comment author="spinscale" created="2013-04-05T12:23:32Z" id="15952686">Just as a side note. If you want this feature, simply implement it in your river. Most rivers are built similar and use a an indexer and a slurper thread. Before doing anything inside the slurper thread, recheck the configuration by getting it from the _river index and check for a enabled/disabled boolean, which might have been set while the thread was sleeping. Did that for all my rivers in order to disable getting data from flaky sources or when doing administration work on those systems.

No need for getting this natively into ES.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IBM J9 failure to serialize nodes info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/950</link><project id="" key="" /><description>JBM J9 failures to serialize nodes info because non heap info and stats return -1, and we serialize them as positive values. Default it to 0 so we can backport it easily to 0.16 branch as well without breaking changes.
</description><key id="931599">950</key><summary>IBM J9 failure to serialize nodes info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-20T11:40:29Z</created><updated>2011-05-20T11:41:15Z</updated><resolved>2011-05-20T11:41:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/jvm/JvmInfo.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java</file></files><comments><comment>IBM J9 failure to serialize nodes info, closes #950.</comment></comments></commit></commits></item><item><title>ElisionFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/949</link><project id="" key="" /><description>This adds the ability to specify a list of stopword articles with the setting "articles".

&lt;pre&gt;
index:
    analysis:
        analyzer:
            default:
                tokenizer: standard
                filter: [elision]
        filter:
            elision:
                type: elision
                articles: [l, m, t, qu, n, s, j] 
&lt;/pre&gt;


By default the standart stop words are used.
</description><key id="928603">949</key><summary>ElisionFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">belevian</reporter><labels /><created>2011-05-19T19:07:12Z</created><updated>2014-07-16T21:56:44Z</updated><resolved>2011-05-19T21:00:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-19T21:00:38Z" id="1206352">cool!, pushed to 0.16 branch and master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add an ability to define and store filter for aliases.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/948</link><project id="" key="" /><description>Updated according to your comments and squashed into a single commit.

This commit only adds an ability to store filter source. The stored filters are not yet used to filter search results.
</description><key id="927813">948</key><summary>Add an ability to define and store filter for aliases.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-19T16:17:40Z</created><updated>2014-07-16T21:56:44Z</updated><resolved>2011-05-19T17:34:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-19T17:34:36Z" id="1205053">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add an ability to define and store filter for aliases.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/947</link><project id="" key="" /><description>Updated the code according to your comments.
</description><key id="927129">947</key><summary>Add an ability to define and store filter for aliases.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-19T14:01:28Z</created><updated>2014-06-18T13:40:27Z</updated><resolved>2011-05-19T16:10:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-19T15:12:54Z" id="1204064">We need to make sure that whenever we add the filter to the metadata, it is ordered (for example, using `parser.mapOrdered`), which bubbles up to when serializing it and deserializing it, as well as when adding it in the action.

The reason for that is that later on, we are going to check if filters needs to be updated by checking if the CompressedString is equal.

Other than that, looks good.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter Cache: `soft` filter cache can cause bad memory behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/946</link><project id="" key="" /><description>The `soft` filter cache can cause bad JVM memory behavior where its "hard" for the garbage collector to clean memory when needed.
</description><key id="926363">946</key><summary>Filter Cache: `soft` filter cache can cause bad memory behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-19T10:12:14Z</created><updated>2011-05-19T10:13:01Z</updated><resolved>2011-05-19T10:13:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/CacheStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/FilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/none/NoneFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file></files><comments><comment>Filter Cache: `soft` filter cache can cause bad memory behavior, closes #946.</comment></comments></commit></commits></item><item><title>index.mapper.dynamic=false no respected in 0.6.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/945</link><project id="" key="" /><description>After upgrading from 0.5 to 0.6.1, ES started to accept dynamic index updates.

The type we are indexing is "document" and we have a config/mappings/_default/document.json file:

```
{
    "document" : {
        "properties" : {
            "filename" : {"type" : "string"},
            "extension" : {"type" : "string", "index": "not_analyzed"}
           (...)
        }
    }
}
```

Our elasticsearch.yml is:

```
index:
  number_of_shards: 5
  number_of_replicas: 1

  mapper:
    # Using fields not in the schema will cause exceptions
    dynamic: false

  analysis:
    analyzer:
      default:
        tokenizer: lowercase
        # The asciifolding filter converts all non-first-127-ASCII characters to their latin counterparts, if possible
        # This includes e.g. polish characters.
        filter: [ standard, lowercase, stop, asciifolding ]
```

It is possible to index documents with additional fields without an error. The log says:

`[sml] update_mapping [document] (dynamic)`
</description><key id="926064">945</key><summary>index.mapper.dynamic=false no respected in 0.6.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamw</reporter><labels /><created>2011-05-19T08:42:15Z</created><updated>2011-05-20T08:20:50Z</updated><resolved>2011-05-20T08:20:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-19T10:37:06Z" id="1202721">`index.mapper.dynamic` simply disabled the introduction of new mapping types, not fields. Maybe you have dynamic disabled within the document mapping as well? (I can't see it).
</comment><comment author="adamw" created="2011-05-19T14:05:07Z" id="1203579">Ah, so I misunderstood this. And how would I disable dynamic addition of fields?

Adam
</comment><comment author="kimchy" created="2011-05-19T14:51:32Z" id="1203899">Then I am confused how it "worked" for you in 0.15.1 :). Disabling dynamic addition of fields can be done by setting `dynamic` set to `false` in the top level object mapping: http://www.elasticsearch.org/guide/reference/mapping/object-type.html.
</comment><comment author="adamw" created="2011-05-20T08:20:50Z" id="1208691">I must admin now I'm confused how I checked that it "worked" as well ;). We've set dynamic to strict and it works perfect - thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add an ability to define and store filter for aliases.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/944</link><project id="" key="" /><description>As per our discussion, this commit only adds an ability to store filter source. The stored filters are not yet used to filter search results.
</description><key id="925486">944</key><summary>Add an ability to define and store filter for aliases.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-19T05:07:50Z</created><updated>2014-06-21T22:20:24Z</updated><resolved>2011-05-19T12:13:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-19T07:58:23Z" id="1202110">I think that this change should also include the ability to actually store a filter through the APIs we have (both Java API And Rest) using the aliases API. Also, it should not be called source in aliases, it should be called filter, I think.
</comment><comment author="kimchy" created="2011-05-19T11:26:00Z" id="1202909">I see, my commend was wrong. Added specific comments on the pull request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query boost not applied to numeric fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/943</link><project id="" key="" /><description>```
# [Wed May 18 16:39:02 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "mappings" : {
      "bar" : {
         "properties" : {
            "id" : {
               "type" : "integer"
            },
            "tag" : {
               "index" : "not_analyzed",
               "type" : "string"
            }
         }
      }
   }
}
'

# [Wed May 18 16:39:02 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Wed May 18 16:39:05 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "id" : 123,
   "tag" : "aaa"
}
'

# [Wed May 18 16:39:05 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "ezx6K0poT4OrpMZTGi8Ixw",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Wed May 18 16:39:16 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "id" : 124,
   "tag" : "bbb"
}
'

# [Wed May 18 16:39:16 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "y326hSMJSEydthYLdvnGyA",
#    "_type" : "bar",
#    "_version" : 1
# }
```

BOOST DOESN'T WORK FOR INTEGERS

```
curl -XGET 'http://127.0.0.1:9200/foo/_search?pretty=1'  -d '
{
   "fields" : [],
   "query" : {
      "bool" : {
         "should" : [
            {
               "term" : {
                  "id" : {
                     "boost" : 1,
                     "value" : "123"
                  }
               }
            },
            {
               "term" : {
                  "id" : {
                     "boost" : 2,
                     "value" : "124"
                  }
               }
            }
         ]
      }
   }
}
'

# [Wed May 18 16:39:19 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 0.4472136,
#             "_index" : "foo",
#             "_id" : "y326hSMJSEydthYLdvnGyA",
#             "_type" : "bar"
#          },
#          {
#             "_score" : 0.2236068,
#             "_index" : "foo",
#             "_id" : "ezx6K0poT4OrpMZTGi8Ixw",
#             "_type" : "bar"
#          }
#       ],
#       "max_score" : 0.4472136,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```

BOOST WORKS FOR TEXT

```
curl -XGET 'http://127.0.0.1:9200/foo/_search?pretty=1'  -d '
{
   "fields" : [],
   "query" : {
      "bool" : {
         "should" : [
            {
               "term" : {
                  "tag" : {
                     "boost" : 1,
                     "value" : "aaa"
                  }
               }
            },
            {
               "term" : {
                  "tag" : {
                     "boost" : 2,
                     "value" : "bbb"
                  }
               }
            }
         ]
      }
   }
}
'

# [Wed May 18 16:39:23 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 0.4472136,
#             "_index" : "foo",
#             "_id" : "y326hSMJSEydthYLdvnGyA",
#             "_type" : "bar"
#          },
#          {
#             "_score" : 0.2236068,
#             "_index" : "foo",
#             "_id" : "ezx6K0poT4OrpMZTGi8Ixw",
#             "_type" : "bar"
#          }
#       ],
#       "max_score" : 0.4472136,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```
</description><key id="922083">943</key><summary>Query boost not applied to numeric fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-05-18T14:41:24Z</created><updated>2013-04-05T10:21:25Z</updated><resolved>2013-04-05T10:21:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wizidot" created="2011-05-18T15:03:38Z" id="1197516">It ssems that's this problem is the same for "field" query :

{
    "field" : { 
        "name.first" : {
            "query" : "+something -else",
            "boost" : 2.0,
            "enable_position_increments": false
        }
    }
}
</comment><comment author="clintongormley" created="2013-04-05T10:21:25Z" id="15948472">Norms are disabled by default for numeric fields.  Search time boosting should be used instead
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Phonetic filter to support `cologne` encoder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/942</link><project id="" key="" /><description>Support `cologne` encoder (german), and also add `caverphone1` and `caverphone2`.
</description><key id="921198">942</key><summary>Analysis: Phonetic filter to support `cologne` encoder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-18T10:42:49Z</created><updated>2011-05-18T10:43:26Z</updated><resolved>2011-05-18T10:43:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticTokenFilterFactory.java</file></files><comments><comment>Analysis: Phonetic filter to support `cologne` encoder, closes #942.</comment></comments></commit></commits></item><item><title>Upgrade to jackson 1.8.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/941</link><project id="" key="" /><description>Upgrade to jackson 1.8.1 fixing several smile bugs.
</description><key id="920863">941</key><summary>Upgrade to jackson 1.8.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-18T09:10:44Z</created><updated>2011-05-18T09:11:20Z</updated><resolved>2011-05-18T09:11:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Upgrade to jackson 1.8.1, closes #941.</comment></comments></commit></commits></item><item><title>AWS (EC2) Cloud Module to set tcp.host to private ip address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/940</link><project id="" key="" /><description>EC2 Cloud Discovery expects to find other hosts by their private IPv4 address. This is good practice on EC2. So the tcp.host should also be automatically set to this nodes private IPv4 address to reduce configuration required.

The EC2 metadata service http://169.254.169.254/latest/meta-data/ can be used for this purpose.
</description><key id="919411">940</key><summary>AWS (EC2) Cloud Module to set tcp.host to private ip address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keteracel</reporter><labels /><created>2011-05-18T00:53:00Z</created><updated>2013-04-05T10:20:50Z</updated><resolved>2013-04-05T10:20:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-18T09:38:51Z" id="1196002">Check `org.elasticsearch.common.networkNetworkService`, and using `CustomNameResolver`. The cloud plugin can be injected with it, and add a custom name resolver, which will return the private IP as default, and support things like: `_ec2:privateIp_`, and `_ec2:publicIp_`.
</comment><comment author="keteracel" created="2011-05-20T20:40:11Z" id="1212296">finally got everything up and running in eclipse (IDEA and I have issues, we've been to counselling but we just couldn't reconcile :D ). I use apache Http client to get meta-data from EC2. It builds fine in eclipse (using the gradle generated eclipse classpath - i.e. no changes to gradle.build) but the build (gradlew in root of elasticsearch directory) fails:

```
:plugins-cloud-aws:compileJavaC:\Projects\elasticsearch\plugins\cloud\aws\src\main\java\org\elasticsearch\cloud\aws\network\Ec2NameResolver.java:26: package org.apache.commons.httpclient does not exist
import org.apache.commons.httpclient.HttpClient;
                                    ^
```

I'm a gradle newb, sorry!
</comment><comment author="keteracel" created="2011-05-20T21:43:28Z" id="1212553">ahh... compile("commons-httpclient:commons-httpclient:3.0.1") { transitive = false }
</comment><comment author="kimchy" created="2011-05-20T23:43:44Z" id="1213035">I think we can simply use URL and open a stream, slurp it, and then parse it. Will be much simpler and not depend on a specific http client version the aws SDK uses.
</comment><comment author="keteracel" created="2011-05-21T00:50:26Z" id="1213267">sounds reasonable
</comment><comment author="clintongormley" created="2013-04-05T10:20:50Z" id="15948452">This is already supported
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move alias metadata from settings to a separate data structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/939</link><project id="" key="" /><description>Not sure what to do with RestClusterStateAction to make response backward compatible in the future.
</description><key id="917728">939</key><summary>Move alias metadata from settings to a separate data structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-17T18:18:24Z</created><updated>2014-07-16T21:56:46Z</updated><resolved>2011-05-17T21:14:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-17T18:23:34Z" id="1192101">We can break backward compatible for the response for aliases, it should be ok. The important part is to check that we don't break existing alias settings.

Glancing at it, it looks good. Will merge it later today and check. Its a good start!, baby steps :)
</comment><comment author="imotov" created="2011-05-17T18:38:18Z" id="1192181">Thanks! Looking forward to the next "baby step".
</comment><comment author="kimchy" created="2011-05-17T20:40:34Z" id="1192926">Cool, I think the next step is to have the logic of being able to define a filter, and have it stored (as compressed byte array) as part of the metadata and properly serialized and deserialized. Once we have that, we can move to the AliasesService phase where we actually do something with it (similar to mappings), and then plug it all to APIs using it (which can get tricky in some places, need to find a way to do it nicely).
</comment><comment author="kimchy" created="2011-05-17T20:59:31Z" id="1193044">Btw, thinking about the backward comp in cluster state, I think we should stay with just returning the alias names in the response, and later on, add _aliases GET endpoint (similar to index settings) that one can get the full info on it (which filter is used and so on).
</comment><comment author="kimchy" created="2011-05-17T21:14:39Z" id="1193150">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: Empty facets element causes search failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/938</link><project id="" key="" /><description>Search: Empty facets element causes search failures

```
curl localhost:9200/test/_search -d '
{
"query": {
    "bool": {
        "must": [
            {
                "match_all": { }
            }
        ],
        "must_not": [ ],
        "should": [ ]
    }
},
"from": 0,
"size": 50,
"sort": [ ],
"facets": { }
}'
```
</description><key id="916264">938</key><summary>Search: Empty facets element causes search failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-17T13:01:43Z</created><updated>2011-05-17T13:16:12Z</updated><resolved>2011-05-17T13:16:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file></files><comments><comment>Search: Empty facets element causes search failures, closes #938.</comment></comments></commit></commits></item><item><title>Analysis: Regression (0.16.1), Camel cased filters / tokenizers failed to load in custom analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/937</link><project id="" key="" /><description>Analysis: Regression, Camel cased filters / tokenizers failed to load in custom analyzer
</description><key id="915864">937</key><summary>Analysis: Regression (0.16.1), Camel cased filters / tokenizers failed to load in custom analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-17T11:17:36Z</created><updated>2011-05-17T11:20:42Z</updated><resolved>2011-05-17T11:20:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CustomAnalyzerProvider.java</file></files><comments><comment>Analysis: Regression (0.16.1), Camel cased filters / tokenizers failed to load in custom analyzer, closes #937.</comment></comments></commit></commits></item><item><title>Initial implementation of filtering aliases.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/936</link><project id="" key="" /><description>Shay, I took a shot at adding filters to aliases. I know that you were planning to do it at some point of time, but not sure if this is what you had in mind. If it's not, please let me know. Filtering aliases are implemented for most of the methods. A couple of methods where I wasn't sure what would be a proper behavior with regard to filters are Percolator and DeleteIndex. In the first case, it seems that filters shouldn't affect the behavior at all. In the second case, perhaps, it could be appropriate to throw an Exception if a DeleteIndex is called on an alias with a filter to prevent. Could you take a look?
</description><key id="914518">936</key><summary>Initial implementation of filtering aliases.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-17T05:11:48Z</created><updated>2014-07-16T21:56:46Z</updated><resolved>2011-05-17T12:59:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-17T12:26:06Z" id="1190100">Heya,

Yea, thats certainly planned. We should have chatted a bit before you started this endeavor :). The design I had in mind is different (and more optimized), let me explain:

In general, aliases should move to work in a similar manner to how mappings work. In the index metadata we store the alias information (which possibly has a _filter_ associated with it), and, on the concrete index level (similar to MapperService, we will have AliasService),  have the parsed representation already there.

Then, when operations occur, we also pass the aliases that are associated with it, and get the relevant parsed filters using the AliasService. This means we don't need to pass the full filter def on each call, and we don't need to parse it each time.

Also, similar to mappings, we should move the aliases from being defined using settings, to explicit data structures / configuration (while still maintaing backward comp., which is tricky).

 think we should move in baby steps here. The first thing to apply is moving to unique data structures for aliases, the new configuration. Without thinking implementing filters. Once we have that nailed down, we can push this change. Then, the next step would be to add the filter to aliases and add the relevant logic.

Also, one thing that I thing the new aliases support should have is also custom routing value that will automatically be used.

We can chat more about it on IRC.
</comment><comment author="imotov" created="2011-05-17T12:59:33Z" id="1190266">Thanks. That makes sense. This iteration was necessary for me to better understand the code and to get by while the real solution is created. I don't think I would have been able to have a meaningful conversation with you about this area of ES without taking a shot at it. :-) So, now that I have it working, let's do it the right way. 
</comment><comment author="kimchy" created="2011-05-17T13:27:37Z" id="1190379">Great!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix possible false matches with multiple percolators.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/935</link><project id="" key="" /><description>When you register multiple percolators, you can get false matches if at least one percolator matches the query. It's caused by not reseting Lucene.ExistsCollector after the match.
</description><key id="906568">935</key><summary>Fix possible false matches with multiple percolators.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-16T13:58:29Z</created><updated>2014-07-16T21:56:47Z</updated><resolved>2011-05-16T15:10:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-16T15:08:25Z" id="1173891">cool, will push...
</comment><comment author="kimchy" created="2011-05-16T15:10:37Z" id="1173900">pushed to master and 0.16.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>auto_expand_replicas: [0-all] can cause data loss when nodes are removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/934</link><project id="" key="" /><description>Hiya - there is a bug with `auto_expand_replicas: [0-all]` in v 0.16.1 which causes loss of all data in that index.

To replicate:
- start two nodes
- run the script below
- count for index `bar` : 3
- kill the node that holds the primary shard for index `bar`
- count for index `bar`: 0

If you change auto expand to `[1-all]` then data is not lost.

```
curl -XDELETE 'http://127.0.0.1:9200/bar,foo/?pretty=1'

curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "settings" : {
      "number_of_replicas" : 0,
      "number_of_shards" : 1
   }
}
'

curl -XPUT 'http://127.0.0.1:9200/bar/?pretty=1'  -d '
{
   "settings" : {
      "index" : {
         "number_of_replicas" : 0,
         "number_of_shards" : 1
      }
   }
}
'


curl -XGET 'http://127.0.0.1:9200/_cluster/health/bar?pretty=1&amp;wait_for_status=green' 


curl -XPOST 'http://127.0.0.1:9200/_bulk?pretty=1'  -d '
{"index" : {"_index" : "bar", "_type" : "name"}}
{"tokens" : ["stuart", "watt"], "context" : "/2850246/all", "rank" : 1}
{"index" : {"_index" : "bar", "_type" : "name"}}
{"tokens" : ["stuart", "watt"], "context" : "/2850246/jpnw/all", "rank" : 1}
{"index" : {"_index" : "bar", "_type" : "name"}}
{"tokens" : ["stuart", "watt"], "context" : "/2850246/jpnw_pres/all", "rank" : 1}
{"index" : {"_index" : "bar", "_type" : "name"}}
'

curl -XPOST 'http://127.0.0.1:9200/bar/_refresh?pretty=1' 

curl -XPUT 'http://127.0.0.1:9200/bar/_settings?pretty=1'  -d '
{
   "index" : {
      "auto_expand_replicas" : "0-all"
   }
}
'

curl -XGET 'http://127.0.0.1:9200/_cluster/health/bar?pretty=1&amp;wait_for_status=green' 


curl -XGET 'http://127.0.0.1:9200/bar/_count?pretty=1'  -d '
{
   "match_all" : {}
}
'
```
</description><key id="906387">934</key><summary>auto_expand_replicas: [0-all] can cause data loss when nodes are removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-16T13:12:11Z</created><updated>2011-05-17T10:20:36Z</updated><resolved>2011-05-16T22:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-05-16T13:23:58Z" id="1173252">Note: I create two indices because the problem only shows up with both indices present. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/ClusterService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file></files><comments><comment>auto_expand_replicas: [0-auto] can cause data loss when nodes are removed, closes #934.</comment></comments></commit></commits></item><item><title>Assume geo_type from objects with numeric lat/long properties.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/933</link><project id="" key="" /><description>Elasticsearch does a great job at finding types, the one i miss though is an assumption that if an object with numeric lat lon properties its probably a geo_type.
</description><key id="903036">933</key><summary>Assume geo_type from objects with numeric lat/long properties.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2011-05-15T18:24:42Z</created><updated>2013-04-05T10:19:26Z</updated><resolved>2013-04-05T10:19:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gjb83" created="2011-05-18T22:42:01Z" id="1200408">+1
I agree that this seems like a sensible assumption.
</comment><comment author="clintongormley" created="2013-04-05T10:19:26Z" id="15948407">Too much magic can be bad :)  Decision made to specify geo-points manually
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>wrong type returned by date_histogram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/932</link><project id="" key="" /><description>```
"histo1" : {
        "date_histogram" : {
            "field" : "followers.dateOfBirth",
            "interval" : "year"
        }
    },
```

returns with:

```
"histo1" : {
  "_type" : "histogram",
```
</description><key id="902410">932</key><summary>wrong type returned by date_histogram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-15T13:06:43Z</created><updated>2011-05-15T14:13:19Z</updated><resolved>2011-05-15T14:13:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/InternalCountDateHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/InternalFullDateHistogramFacet.java</file></files><comments><comment>wrong type returned by date_histogram, closes #932.</comment></comments></commit></commits></item><item><title>Deleting and recreating a new index with dynamic mapping can cause type failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/931</link><project id="" key="" /><description>Environment:
elasticsearch v0.16.1, Mac OS X 10.6.7, Java 1.6.0_24 (other platforms untested)

Problem:
I am finding that if I add a document to a non-existent index, the index is automatically created, but around 20% of the time, the document is not added correctly.

Steps to reproduce:

```
#!/bin/sh
curl -XDELETE http://localhost:9200/myindex; echo; sleep 1; \
curl -XPOST http://localhost:9200/myindex/mytype/ -d '{ "body" : "Test" }'; echo; sleep 1; \
curl -XPOST http://localhost:9200/myindex/_refresh; echo; sleep 1; \
curl -XGET http://localhost:9200/myindex/_search?pretty=true; echo
```

You may have to repeat this process a few times to reproduce this problem as it doesn't happen every time.

Resulting error:

```
"failures" : [ {
  "reason" : "TypeMissingException[[myindex] type[mytype] missing: failed to find type loaded for doc [VwMCBKs0QpSzSsc-329USg]]"
} ]
```
</description><key id="897427">931</key><summary>Deleting and recreating a new index with dynamic mapping can cause type failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gjb83</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-13T20:55:48Z</created><updated>2011-05-15T09:52:28Z</updated><resolved>2011-05-15T09:52:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-15T09:50:33Z" id="1165516">Recreated, seems like a problem with deleting and then creating an index with dynamic mapping derived fromt eh first doc.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Deleting and recreating a new index with dynamic mapping can cause type failures, closes #931.</comment></comments></commit></commits></item><item><title>Analysis: Add elision token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/930</link><project id="" key="" /><description>Removed `elision` token filter which removes elisions. For example, "l'avion" (the plane) will tokenized as "avion" (plane).
</description><key id="894500">930</key><summary>Analysis: Add elision token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-13T12:10:34Z</created><updated>2011-05-13T12:11:32Z</updated><resolved>2011-05-13T12:11:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ElisionTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file></files><comments><comment>Analysis: Add elision token filter, closes #930.</comment></comments></commit></commits></item><item><title>Mapping: dynamic templates of object type do not initialize some mapping data structures (like analyzers lookup)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/929</link><project id="" key="" /><description>When using dynamic template on `object` type, some mapping data structures are not initialized properly, for example analyzers lookup.
</description><key id="894436">929</key><summary>Mapping: dynamic templates of object type do not initialize some mapping data structures (like analyzers lookup)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-13T11:50:53Z</created><updated>2011-05-13T11:52:20Z</updated><resolved>2011-05-13T11:52:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/ObjectMapper.java</file></files><comments><comment>Mapping: dynamic templates of object type do not initialize some mapping data structures (like analyzers lookup), closes #929.</comment></comments></commit></commits></item><item><title>Analysis: Pattern Tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/928</link><project id="" key="" /><description>Pattern tokenizer allows to define a tokenizer that uses regex to break text into tokens. The `pattern` parameter accepts the regex expression (and flags the common ES level regex flags).

It also accepts `group` (defaults to -1), from teh docs:

group=-1 (the default) is equivalent to "split".  In this case, the tokens will be equivalent to the output from (without empty tokens):String#split(java.lang.String)

Using group &gt;= 0 selects the matching group as the token.  For example, if you have:

```
pattern = \'([^\']+)\'
group = 0

input = aaa 'bbb' 'ccc'
```

the output will be two tokens: 'bbb' and 'ccc' (including the ' marks).  With the same input but using group=1, the output would be: bbb and ccc (no ' marks).
</description><key id="892020">928</key><summary>Analysis: Pattern Tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-05-12T22:19:15Z</created><updated>2011-05-12T22:22:34Z</updated><resolved>2011-05-12T22:22:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PatternTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file></files><comments><comment>Analysis: Pattern Tokenizer, closes #928.</comment></comments></commit></commits></item><item><title>Document that field X in different types of an index should have same data type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/927</link><project id="" key="" /><description>When several types of an index define a field with the same name, it is treated as a single field in the Lucene index. As such, their mapping should be compatible (have the same core type).

This needs to be added to the documentation. Probably under the mapping section.
</description><key id="889276">927</key><summary>Document that field X in different types of an index should have same data type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">plaflamme</reporter><labels /><created>2011-05-12T13:15:35Z</created><updated>2013-04-05T10:18:06Z</updated><resolved>2013-04-05T10:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="oravecz" created="2011-06-16T06:53:13Z" id="1379106">And more importantly, it would be great to log a warning if the field is mapped differently.
</comment><comment author="clintongormley" created="2013-04-05T10:18:06Z" id="15948356">Is being explained in the new documentation
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix double counts when two aliases are pointing to the same index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/926</link><project id="" key="" /><description>If you create two aliases pointing to the same index and then execute _count on both aliases, the count is doubled. Search, however, removes duplicates (at least if a small number of records is returned). See https://gist.github.com/967342 for a demo script. I am not entirely sure if this is a bug of a feature. If this is a bug, here is a small fix. 
</description><key id="886317">926</key><summary>Fix double counts when two aliases are pointing to the same index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-11T21:09:51Z</created><updated>2014-07-16T21:56:47Z</updated><resolved>2011-05-11T21:44:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-11T21:31:15Z" id="1143193">Thats a bug, will push it.
</comment><comment author="kimchy" created="2011-05-11T21:44:07Z" id="1143253">PUshed to master and 0.16 branch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java-API exposes an empty SearchHit with id=_search as first entry on every search.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/925</link><project id="" key="" /><description>Java-API exposes an empty SearchHit with id=_search as first entry on every search.
</description><key id="883875">925</key><summary>Java-API exposes an empty SearchHit with id=_search as first entry on every search.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zimdo</reporter><labels /><created>2011-05-11T15:42:32Z</created><updated>2013-04-05T10:14:37Z</updated><resolved>2013-04-05T10:14:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-11T16:13:49Z" id="1140337">repro please? I have not seen  it.
</comment><comment author="zimdo" created="2011-05-11T22:54:38Z" id="1143571">Hi Shay,

i am using 0.16.0 on a fresh index and just constructing searches like 

search = indexClient.prepareSearch(IDX)
    .setTypes(TYPE)
    .setSearchType(SearchType.QUERY_AND_FETCH)
...

// i am using a booleanquery but have confirmed with term query etc.

searchResponse = search.execute().actionGet();
SearchHit[] hits = searchResponse.hits().hits();

// The first element causes the problems =&gt; hits[0].getId() == _search
for(int i = 1; i &lt; hits.length; i++) {
....
}
</comment><comment author="kimchy" created="2011-05-12T05:59:01Z" id="1144822">You don't index any data? I don't see this happening, index data into a fresh index, and the first hit is properly maintained. Just create a small "main" program that recreates it (with indexing data).
</comment><comment author="zimdo" created="2011-05-18T22:12:58Z" id="1200258">Hi Shay, I have been away for a couple of days. I am indexing data... just not in the code-snippet I pasted here. The indexing is happening in another process and sucks in CSV files. I can reproduce this on my machine with a blank index. I have just got my self a copy of the code and am trying to get to the root... as soon as I know more I'll post it here.
</comment><comment author="clintongormley" created="2013-04-05T10:14:37Z" id="15948234">No new info after 2 years - Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Phonetic Filter : Double Metaphone, partial implementation (not using the secondary code)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/924</link><project id="" key="" /><description>Hello,

Like Wikipedia  Double Metaphone description say (http://en.wikipedia.org/wiki/Double_Metaphone#Double_Metaphone)
"
It is called "Double" because it can return both a primary and a secondary code for a string;
For example, encoding the name "Smith" yields 
- a primary code of SM0 
- and a secondary code of XMT, 
  while the name "Schmidt" yields a primary code of XMT and a secondary code of SMT--both have XMT in common.
  "

In https://github.com/elasticsearch/elasticsearch/blob/master/modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticFilter.java

The PhoneticFilter only use only the primary code but not the secondary code

For using the secondary you have to use doubleMetaphone(String value, boolean alternate)
 with  alternate=false for secondary code
cf http://commons.apache.org/codec/api-release/org/apache/commons/codec/language/DoubleMetaphone.html

The question is how to use this seconday code ?
- May be like a synonym way  ? 

Best regards

Jérome
</description><key id="883337">924</key><summary>Phonetic Filter : Double Metaphone, partial implementation (not using the secondary code)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmorille</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-11T13:40:51Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-05-11T19:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-11T18:27:30Z" id="1142102">Maybe we can implement an expliciit lucene filter for double metaphone that generates both tokens?
</comment><comment author="jmorille" created="2011-05-11T19:41:48Z" id="1142578">I have found the solr implementation 
http://svn.apache.org/viewvc/lucene/dev/branches/branch_3x/solr/src/java/org/apache/solr/analysis/DoubleMetaphoneFilter.java?view=markup

That true add the 2 tokens but not sure of witch PositionIncrement who have to use. 
And the unit test don't help http://svn.apache.org/viewvc/lucene/dev/branches/branch_3x/solr/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterTest.java?view=markup
</comment><comment author="kimchy" created="2011-05-11T19:50:41Z" id="1142634">I see, will add it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/phonetic/DoubleMetaphoneFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticTokenFilterFactory.java</file></files><comments><comment>Phonetic Filter : Double Metaphone, partial implementation (not using the secondary code), closes #924.</comment></comments></commit></commits></item><item><title>Possible failure when using TransportClient (with sniffing)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/923</link><project id="" key="" /><description>Possible failure when using TransportClient (with sniffing) when failing to connect to sniffed nodes (removing from a set that is being iterated on).
</description><key id="879167">923</key><summary>Possible failure when using TransportClient (with sniffing)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-10T22:58:53Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-05-10T22:59:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file></files><comments><comment>Possible failure when using TransportClient (with sniffing), closes #923.</comment></comments></commit></commits></item><item><title>Add snappy compression support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/922</link><project id="" key="" /><description>Now elasticsearch support LZO compression with transportation and indexed source. Please add snappy support too. 
</description><key id="874765">922</key><summary>Add snappy compression support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hz</reporter><labels /><created>2011-05-10T03:41:46Z</created><updated>2013-03-02T22:19:50Z</updated><resolved>2013-03-02T22:19:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrflip" created="2013-03-02T21:43:40Z" id="14336580">dupe of #2081 (and superseded by #2459)
</comment><comment author="kimchy" created="2013-03-02T22:19:50Z" id="14337191">Right, closing....
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>scripts do not work correctly under cygwin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/921</link><project id="" key="" /><description>for example the data directory ends up in c:/cygdrive/..../elasticsearch-0.16.0/data rather than ../data
</description><key id="873523">921</key><summary>scripts do not work correctly under cygwin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keteracel</reporter><labels /><created>2011-05-09T21:11:17Z</created><updated>2013-04-04T19:03:04Z</updated><resolved>2013-04-04T19:03:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-10T19:38:15Z" id="1133002">Don't have access to a windows machine, can you spare some cycles and chase it?
</comment><comment author="keteracel" created="2011-05-10T19:47:41Z" id="1133061">yeah, sure.
</comment><comment author="keteracel" created="2011-05-11T22:06:03Z" id="1143358">couple of small changes here: https://gist.github.com/967493
</comment><comment author="kimchy" created="2011-05-12T06:03:49Z" id="1144833">Which one is the one that fixed the cygwin problem? Is adding just the ES_HOME in the cygwin section enough?
</comment><comment author="keteracel" created="2011-05-12T17:23:31Z" id="1147962">So adding quotes around $JAVA in the exec commands fixes the fact that Java is installed in "Program Files" on windows. People can work around this by changing $JAVA_HOME to be a cygwin path rather than a windows path but then other things that require $JAVA_HOME (like maven) may barf.

getopt is not in cygwin by default, whereas getopts is. But getopt can be added to cygwin so that change is not required unless you want a base install of cygwin to work.

Then yeah, adding ES_HOME to the cygwin section makes the data dir end up in the correct location.
</comment><comment author="kimchy" created="2011-05-13T10:47:26Z" id="1152214">I pushed most of your changes to master, except for using getopts instead of getopt, since I am not sure if it will be available on all platforms... . Would love to hear about it from other people what they think (not an expert)
</comment><comment author="keteracel" created="2011-05-14T20:34:03Z" id="1163634">I think that's sensible. Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Document's Field level boosting </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/920</link><project id="" key="" /><description>[Document's Field level boosting](http://lucene.apache.org/java/3_0_0/api/core/org/apache/lucene/document/Fieldable.html#setBoost%28float%29) is not available in ElasticSearch.

This is a proposal to make it available by extending the existing API.

Suppose we need to have a boost on a specific field named "office" (as an example).  First, we require that an explicit mapping is setup on the "office" field that declares this field of one of the core types (here a string):

```
{
    ...
    "office" : {
        "type": "string"
    }
    ...
}
```

Then, to introduce a document with the field "office" having a boost of 2.0, we must use the following syntax:

```
{
    "office" : {
        "_value":  "Bridgewater",
        "_boost":  2.0
    }
}
```

The mapping is required to disambiguate the meaning of the document.  Otherwise, ElasticSearch would interpret "office" as a value of type "object".

The key `_value` (or `value`)  in the inner document specifies the real string content that should eventually be indexed in Lucene (in the example, this is _Bridgewater_).  The `_boost` (or `boost`) key specifies the per field document boost (here 2.0)
</description><key id="873102">920</key><summary>Mapping: Document's Field level boosting </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fotonauts</reporter><labels><label>feature</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-09T19:58:45Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-05-10T20:27:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/AbstractFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/StringFieldMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/xcontent/compound/CompoundTypesTests.java</file></files><comments><comment>Mapping: Document's Field level boosting, closes #920.</comment></comments></commit></commits></item><item><title>Analysis: All analysis components that accept stopwords to allow to load stopwords from a file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/919</link><project id="" key="" /><description>All analysis components that accept `stopwords` to allow to load stopwords from a config file. The setting is `stopwords_path`, and will automatically resolve to `config/` based location or absolute location.
</description><key id="868450">919</key><summary>Analysis: All analysis components that accept stopwords to allow to load stopwords from a file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-09T11:12:56Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-05-09T11:13:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArabicAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArmenianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BasqueAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BrazilianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BulgarianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CatalanAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CjkAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CzechAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DanishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DutchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/EnglishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FinnishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FrenchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GalicianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GermanAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GreekAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/HindiAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/HungarianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/IndonesianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ItalianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NorwegianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PatternAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PersianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PortugueseAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RomanianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RussianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SpanishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StandardAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StopAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StopTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SwedishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/TurkishAnalyzerProvider.java</file></files><comments><comment>Analysis: All analysis components that accept stopwords to allow to load stopwords from a file, closes #919.</comment></comments></commit></commits></item><item><title>Analysis: Word Delimiter Token Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/918</link><project id="" key="" /><description>Named `word_delimiter`, it Splits words into subwords and performs optional transformations on subword groups. Words are split into subwords with the following rules:
- split on intra-word delimiters (by default, all non alpha-numeric characters).
- "Wi-Fi" -&gt; "Wi", "Fi"
- split on case transitions: "PowerShot" -&gt; "Power", "Shot"
- split on letter-number transitions: "SD500" -&gt; "SD", "500"
- leading and trailing intra-word delimiters on each subword are ignored: "//hello---there, 'dude'" -&gt; "hello", "there", "dude"
- trailing "'s" are removed for each subword: "O'Neil's" -&gt; "O", "Neil"

Parameters include:
- `generate_word_parts`: If `true` causes parts of words to be generated: "PowerShot" =&gt; "Power" "Shot". Defaults to `true`.
- `generate_number_parts`: If `true` causes number subwords to be generated: "500-42" =&gt; "500" "42". Defaults to `true`.
- `catenate_words`: If `true` causes maximum runs of word parts to be catenated: "wi-fi" =&gt; "wifi". Defaults to `false`.
- `catenate_numbers`: If `true` causes maximum runs of number parts to be catenated: "500-42" =&gt; "50042". Defaults to `false`.
- `catenate_all`: If `true` causes all subword parts to be catenated: "wi-fi-4000" =&gt; "wifi4000". Defaults to `false`.
- `split_on_case_change`: If `true` causes "PowerShot" to be two tokens; ("Power-Shot" remains two parts regards). Defaults to `true`.
- `preserve_original`: If `true` includes original words in subwords: "500-42" =&gt; "500" "42" "500-42". Defaults to `false`.
- `split_on_numerics`: If `true` causes "j2se" to be three tokens; "j" "2" "se". Defaults to `true`.
- `stem_english_possessive`: If `true` causes trailing "'s" to be removed for each subword: "O'Neil's" =&gt; "O", "Neil". Defaults to `true`.

Advance settings include:

`protected_words`: A list of protected words from being delimiter. Either an array, or also can set `protected_words_path` which resolved to a file configured with protected words (one on each line). Automatically resolves to `config/` based location if exists.

`type_table`: A custom type mapping table, for example (when configured using `type_table_path`):

```
# Map the $, %, '.', and ',' characters to DIGIT 
# This might be useful for financial data.
$ =&gt; DIGIT
% =&gt; DIGIT
. =&gt; DIGIT
\u002C =&gt; DIGIT

# in some cases you might not want to split on ZWJ
# this also tests the case where we need a bigger byte[]
# see http://en.wikipedia.org/wiki/Zero-width_joiner
\u200D =&gt; ALPHANUM
```
</description><key id="866995">918</key><summary>Analysis: Word Delimiter Token Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-08T23:47:37Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-05-08T23:48:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-05-09T08:54:49Z" id="1121865">Very nice indeed!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterIterator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/AbstractCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>Analysis: Word Delimiter Token Filter, closes #918.</comment></comments></commit></commits></item><item><title>Query DSL: Text Queries (boolean, phrase, and phrase_prefix)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/917</link><project id="" key="" /><description>A new family of `text` queries that accept text, analyzes it, and constructs a query out of it. For example:

```
{
    "text" : {
        "message" : "this is a test"
    }
}
```
## Types of Text Queries
# `boolean`

The default `text` query is of type `boolean`. It means that the text provided is analyzed and the analysis process constructs a boolean query from the provided text. The `operator` flag can be set to `or` or `and` to control the boolean clauses (defaults to `or`).

The `analyzer` can be set to control which analyzer will perform the analysis process on the text. It default to the field explicit mapping definition, or the default search analyzer.

`fuzziness` can be set to a value (depending on the relevant type, for string types it should be a value between `0.0` and `1.0`) to constructs fuzzy queries for each term analyzed. The `prefix_length` and `max_expansions` can be set in this case to control the fuzzy process.
# `text_phrase`

The `text_phrase` query analyzes the text and creates a `phrase` query out of the analyzed text. For example:

```
{
    "text_phrase" : {
        "message" : "this is a test"
    }
}
```

Since `text_phrase` is only a `type` of a `text` query, it can also be used in the following manner:

```
{
    "text" : {
        "message" : "this is a text",
        "type" : "phrase"
    }
}
```

A phrase query maintains order of the terms up to a configurable `slop` (which defaults to 0).

The `analyzer` can be set to control which analyzer will perform the analysis process on the text. It default to the field explicit mapping definition, or the default search analyzer.
# `text_phrase_prefix`

The `text_phrase_prefix` is the same as `text_phrase`, expect it allows for prefix matches on the last term in the text. For example:

```
{
    "text_phrase_prefix" : {
        "message" : "this is a text"
    }
}
```

Or

```
{
    "text" : {
        "message" : "this is a text",
        "type" : "phrase_prefix"
    }
}
```

It accepts the same parameters as the `phrase` type. In addition, it also accepts a `max_expansions` parameter that can control to how many prefixes the last term will be expanded. It is highly recommended to set it to an acceptable value to control the execution time of the query.
## Comparison to query_string / field

The `text` family of queries does not goes through a "query parsing" process. It does not support field name prefixes, wildcard characters, or other "advance" features. For this reason, chances of it failing are very small, and it provides an excellent behavior when it comes to just analyze and run that text as a query behavior (which is usually what a text search box does). Also, the `phrase_prefix` can provide a great "as you type" behavior to automatically load search results. 
</description><key id="865329">917</key><summary>Query DSL: Text Queries (boolean, phrase, and phrase_prefix)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-08T18:40:24Z</created><updated>2011-06-24T09:38:20Z</updated><resolved>2011-05-08T18:42:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2011-05-18T14:26:30Z" id="1197246">This post is no longer up to date. Stick to the docs: http://www.elasticsearch.org/guide/reference/query-dsl/text-query.html
</comment><comment author="lukas-vlcek" created="2011-05-29T00:25:22Z" id="1255642">It is not clear from the doc. Do these new Text Queries allow to specify which field they are executed on or are they executed on _all field only?
</comment><comment author="kimchy" created="2011-05-29T05:56:49Z" id="1256101">In all the samples, `message` is the field and can be replaced with any other field.
</comment><comment author="nachiketkb" created="2011-06-24T08:53:56Z" id="1431143">Does this text_phrase query work for multiple fields?
I mean i have multiple fields on which I have to search on.
But I can not take and of many text_phrase queries.
{
    "query":
    {
        "filtered" : {
            "query" : {
                "text_phrase" : {
                    "message1" : "this is a test"
                }
            }
            ,
                "filter":
                {
                    "and":
                        [
                        {
                            "text_phrase" : {
                                "message2" : "this is a test"
                            }
                        }
                    ]
                }
        }
    }
}

It is giving me error:
org.elasticsearch.index.query.QueryParsingException: [twitter] No filter registered for [text_phrase]

Can you suggest some alternative?
</comment><comment author="kimchy" created="2011-06-24T09:38:20Z" id="1431355">This question really belongs to the mailing list, its not really an issue. The short answer is that you can use the bool query to wrap several text queries.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/MatchNoDocsQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/TextQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/TextQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/TextQueryParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQueryTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/SimpleIndexQueryParserTests.java</file></files><comments><comment>Query DSL: Text Queries (boolean, phrase, and phrase_prefix), closes #917.</comment></comments></commit></commits></item><item><title>Search suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/916</link><project id="" key="" /><description>Elasticsearch needs an API method to return a list of suggestion from a incomplete query based on the indexed content.

More info: http://wiki.apache.org/solr/Suggester

This would be useful for autocompletion of search.
</description><key id="863716">916</key><summary>Search suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-05-08T00:46:09Z</created><updated>2013-04-04T19:03:34Z</updated><resolved>2013-04-04T19:03:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2011-05-13T07:41:35Z" id="1151498">@mofle, have a look at http://www.elasticsearch.org/guide/reference/query-dsl/text-query.html

Suggesting to close the issue.
</comment><comment author="sindresorhus" created="2011-05-13T09:50:28Z" id="1152010">Not really sure how this would work. Can you give an example?
</comment><comment author="karussell" created="2011-05-21T22:28:33Z" id="1215760">Autocompletion can be made via facets and also via indexing into separate field via ngram technic. Then doing a wildcard query. 

Suggesting to close the issue ;)
</comment><comment author="bjfish" created="2011-06-27T17:45:13Z" id="1449079">If suggestions are possible, could this be added to the documentation?
</comment><comment author="karussell" created="2011-06-27T18:34:29Z" id="1449456">Hmmh, someone should do a blog post similar to

http://www.lucidimagination.com/blog/2009/09/08/auto-suggest-from-popular-queries-using-edgengrams/

http://karussell.wordpress.com/2010/12/08/use-cases-of-faceted-search-for-apache-solr/
</comment><comment author="kennknowles" created="2012-09-01T19:07:35Z" id="8215634">I tried: http://codeslashslashcomment.com/2012/09/01/search-query-suggestions-using-elasticsearch-via-shingle-filter-and-facets/

Not perfect, would love tips.
</comment><comment author="clintongormley" created="2013-04-04T19:03:34Z" id="15916994">Also, see the new suggest API that comes with 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Example web front end</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/915</link><project id="" key="" /><description>Elasticsearch should come with an example frontend.

This would lower the barrier to start using the engine, and show how much you can do with it.

Maybe a simplified google like search with search box, results, paging. Using jQuery to get the results from the REST API.
</description><key id="863701">915</key><summary>Example web front end</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-05-08T00:36:38Z</created><updated>2013-04-04T19:03:45Z</updated><resolved>2013-04-04T19:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2011-05-08T14:07:51Z" id="1119165">How would the example dataset be loaded? Which features (eg. facets) would the interface expose?

I think this is an area where client libraries, external projects can be helpful. Eg. an application which loads Wikipedia via the river and exposes interface to search/browse it.
</comment><comment author="sindresorhus" created="2011-05-13T07:58:48Z" id="1151562">I was thinking of just a simple search that using jQuery would send the input using a "query_string" query and return the results, like in a formatting like google does. The example content could be some lorem ipsum kind of sites. Just to easily get people started with a practical example.

The example would just expose the most basic stuff. The dataset could be loaded like the example on the GitHub page, using curl. A wikipedia search would also be a good example project.

It could be external, but most of them would die rather quickly when there is breaking changes in ES. The example project or example projects I'm thinking of should be maintained by ES.

Just a thought though.
</comment><comment author="karmi" created="2011-05-13T08:02:17Z" id="1151573">Yeah, the direction is good. We've been thinking about adding a Wikipedia river to the search server used on ES site, and making a simple interface to allow searches.

But the real target is something like https://github.com/lukas-vlcek/elasticsearch-js/blob/master/demo/stats/stats.html or http://mobz.github.com/elasticsearch-head/.
</comment><comment author="sindresorhus" created="2011-05-13T09:33:50Z" id="1151946">Yeah, that would be awesome.

Not really a frontend thing, but would be really useful as a backend view.
</comment><comment author="karussell" created="2011-05-21T22:30:17Z" id="1215766">&gt; but would be really useful as a backend view.

I think ES-HEAD is good at it and will improve over time. But an example river would be nice
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Admin web UI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/914</link><project id="" key="" /><description>A simple admin web UI to be able to browse the db and do queries.
</description><key id="863697">914</key><summary>Admin web UI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-05-08T00:32:50Z</created><updated>2011-05-08T10:13:15Z</updated><resolved>2011-05-08T10:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-05-08T10:13:15Z" id="1118744">Hi mofle.

See https://github.com/mobz/elasticsearch-head
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JavaScript API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/913</link><project id="" key="" /><description /><key id="863682">913</key><summary>JavaScript API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-05-08T00:23:46Z</created><updated>2011-05-13T09:47:10Z</updated><resolved>2011-05-13T07:52:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2011-05-08T14:09:45Z" id="1119170">Could you be more specific? Are you aware of `lukas-vlcek/elasticsearch-js` and `lukas-vlcek/node.es`?
</comment><comment author="karmi" created="2011-05-13T07:42:01Z" id="1151501">@mofle, check back and close or amend the issue, please.
</comment><comment author="sindresorhus" created="2011-05-13T07:52:17Z" id="1151539">@karmi I have thought about it and come to the conclusion that an JS API would just be bloat, since I can just as easily JSON.stringify an object and use jQuery.ajax.
</comment><comment author="karmi" created="2011-05-13T07:57:01Z" id="1151556">It _would_ be bloat inside ES itself :), but a JS API is useful, of course. Check out the https://github.com/lukas-vlcek/elasticsearch-js repo.
</comment><comment author="sindresorhus" created="2011-05-13T09:47:10Z" id="1152000">Yes, that's what I meant. Looked at elasticsearch.js. It's nice, but too much abstraction for me, REST with jQuery is so easy already.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snowball - more languages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/912</link><project id="" key="" /><description>Can you add more languages to snowball stemmer?

Languages stopword lists can be found here: http://wiki.apache.org/solr/LanguageAnalysis
</description><key id="863679">912</key><summary>Snowball - more languages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-05-08T00:20:38Z</created><updated>2013-04-04T19:04:05Z</updated><resolved>2013-04-04T19:04:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:04:04Z" id="15917014">Done
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"Did you mean" spellchecking</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/911</link><project id="" key="" /><description>Google's "Did you mean" feature is very useful. Would be awesome if ES could implement this.

Lucene has pulled in the &lt;a href="http://lucene.apache.org/java/3_1_0/api/all/org/apache/lucene/search/spell/SpellChecker.html"&gt;SpellChecker contrib&lt;/a&gt;. Maybe ES could expose that?

Ex. if I specify suggestSimilar with some optional parameters in my search object I could get back an array with some suggestions.
</description><key id="863658">911</key><summary>"Did you mean" spellchecking</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-05-08T00:09:03Z</created><updated>2016-11-23T07:30:06Z</updated><resolved>2013-04-04T19:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="keteracel" created="2011-05-09T20:53:00Z" id="1126217">you can implement this yourself by having a search term index, probably using ngram and then sorted by popularity.
</comment><comment author="sindresorhus" created="2011-05-16T04:51:38Z" id="1171469">Can you give an example?
</comment><comment author="keteracel" created="2011-05-16T05:04:43Z" id="1171489">something like this: http://sujitpal.blogspot.com/2007/12/spelling-checker-with-lucene.html
</comment><comment author="keteracel" created="2011-05-16T05:28:38Z" id="1171540">But I also see that Lucene has pulled in the SpellChecker contrib: http://lucene.apache.org/java/3_1_0/api/all/org/apache/lucene/search/spell/SpellChecker.html so I guess ES could expose that.
</comment><comment author="sindresorhus" created="2011-05-17T20:40:00Z" id="1192921">@keteracel Red the article you linked. Looks interesting, but is probably more than I can handle at the moment. I really think something as useful as this should be in ES by default. I've updated the issue with a better description.
</comment><comment author="kimchy" created="2011-05-18T21:24:19Z" id="1199945">The current spell checker requires building an auxilery index in order to support it (and moreover, requires reindexing the data  periodically). In Lucene 4.0, since fuzzy queries are much faster, spell checking can be done on the main index. So, the logic is that it makes little sense to incorperate a feature that is quite heavy weigth currently, and not simply waiting to easily implement it with 4.0 is out.
</comment><comment author="sindresorhus" created="2011-05-18T21:51:06Z" id="1200115">Agreed, that's the best solution. Any idea when 4.0 will be out?
</comment><comment author="kimchy" created="2011-05-18T21:57:42Z" id="1200172">No, no due date yet. It seems like the pace is being picked up towards a release, but it will take a few months I think.
</comment><comment author="sindresorhus" created="2011-05-19T07:26:15Z" id="1201989">Ok, thanks ;) Looking forward to it.
</comment><comment author="richardsyeo" created="2011-07-20T18:21:59Z" id="1617593">We would very much like this feature too.
</comment><comment author="naquad" created="2011-07-27T21:19:14Z" id="1667868">Hi.

Are there any news on this? Tired of running around with ASpell :(
</comment><comment author="j" created="2011-09-05T17:57:32Z" id="2005199">+1
</comment><comment author="voltaire-in" created="2011-09-06T06:19:25Z" id="2009238">We would like to use spellchecker too. Thank you. 
</comment><comment author="conradchu" created="2011-10-17T23:30:36Z" id="2436172">+1
</comment><comment author="mhluongo" created="2011-10-21T14:42:25Z" id="2482038">+1
</comment><comment author="tfreitas" created="2011-10-25T15:32:29Z" id="2518664">+1
</comment><comment author="fbecart" created="2011-11-01T23:19:27Z" id="2597579">+1
</comment><comment author="alexis779" created="2011-11-02T13:40:42Z" id="2603427">+1
</comment><comment author="tfreitas" created="2011-11-04T15:38:21Z" id="2632366">+1
</comment><comment author="dstendardi" created="2011-11-16T09:43:22Z" id="2757638">+1
</comment><comment author="adamw" created="2011-12-14T13:58:25Z" id="3139763">+1
</comment><comment author="juliuss" created="2011-12-14T18:28:54Z" id="3145815">+1
</comment><comment author="bryangreen" created="2011-12-20T02:11:19Z" id="3213170">+1
</comment><comment author="abecciu" created="2011-12-20T02:13:58Z" id="3213204">+1
</comment><comment author="nickdunn" created="2012-01-06T16:18:00Z" id="3386608">Apologies for the +1, but this is way up my wishlist too.
</comment><comment author="ream88" created="2012-01-08T13:59:51Z" id="3402111">Yep, me too! +1
</comment><comment author="sebastianseilund" created="2012-02-05T16:56:03Z" id="3818852">+1
This would be an awesome feature, for an already awesome product! Thank you so much :)
</comment><comment author="ghost" created="2012-03-09T10:04:50Z" id="4413731">+1
</comment><comment author="plentz" created="2012-03-24T19:11:19Z" id="4675789">+1
</comment><comment author="gmatthew" created="2012-03-26T16:24:31Z" id="4697800">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query String Query error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/910</link><project id="" key="" /><description>After doing the Twitter example, I wanted to test doing a JSON query:

``` bash
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
{ 
    "query" : "Elastic" 
}'
```

But I got an error:

``` bash
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[ynu9FCqTTCu7KDxHBAft7w][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : \"Elastic\" \n}]]]; nested: QueryParsingException[[twitter] No query registered for [null]]; }{[ynu9FCqTTCu7KDxHBAft7w][twitter][1]: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : \"Elastic\" \n}]]]; nested: QueryParsingException[[twitter] No query registered for [null]]; }{[ynu9FCqTTCu7KDxHBAft7w][twitter][2]: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : \"Elastic\" \n}]]]; nested: QueryParsingException[[twitter] No query registered for [null]]; }{[ynu9FCqTTCu7KDxHBAft7w][twitter][3]: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : \"Elastic\" \n}]]]; nested: QueryParsingException[[twitter] No query registered for [null]]; }{[ynu9FCqTTCu7KDxHBAft7w][twitter][4]: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\" : \"Elastic\" \n}]]]; nested: QueryParsingException[[twitter] No query registered for [null]]; }]",
  "status" : 500
```
</description><key id="863254">910</key><summary>Query String Query error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sindresorhus</reporter><labels /><created>2011-05-07T20:25:37Z</created><updated>2012-02-28T09:55:26Z</updated><resolved>2011-05-08T05:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-05-07T22:51:10Z" id="1116762">Try  `curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true&amp;q=Elastic'`

or 

```
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d
{ "query": { "field": { "_all": "Elastic"}}}
'
```
</comment><comment author="sindresorhus" created="2011-05-07T23:25:59Z" id="1116836">Sorry, I posted the wrong code.

Here's the one I tried to use:

``` bash
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
{
    "query_string" : {
        "query": "Elastic"
    }
}'
```

Taken from the docs: http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query.html
</comment><comment author="sindresorhus" created="2011-05-07T23:50:19Z" id="1116883">Found out the correct way to do it. But the example in the documentation is misleading, since it's not showing that "query_string" must be within the "query" object:

``` bash
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '
{
    "query": {
        "query_string" : {
            "query": "Elastic"
        }
    }
}'
```

You should update the examples.
</comment><comment author="kimchy" created="2011-05-08T05:54:24Z" id="1118429">The example for query_string have nothing wrong with them. Use the mailing list for questions, not the issue tracker.
</comment><comment author="pulkitsinghal" created="2012-02-28T02:14:19Z" id="4208138">@kimchy Actually if you run the very 1st example on the following page via the elasticsearch head plugin's query feature:
http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query.html

It really does fail with error logs on the server side. And the problem goes away after wrapping it in "query:" {...}
</comment><comment author="clintongormley" created="2012-02-28T09:55:26Z" id="4212647">@pulkitsinghal the query string query can be nested in other query clauses. Whatever query you use should be passed to the `query` param in your search, so by changing the docs to add the outer `query`, you're then showing misleading info on how to nest the query string query.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Stop words config to allow for automatic _lang_ expansion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/909</link><project id="" key="" /><description>The list of stop words allow for `_lang_` notation that automatically expand to the default language specific stop words. The possible values are: _arabic_, _armenian_, _basque_, _brazilian_, _bulgarian_, _catalan_, _danish_, _dutch_, _english_, _finnish_, _french_, _galician_, _german_, _greek_, _hindi_, _hungarian_, _indonesian_, _italian_, _norwegian_, _persian_, _portuguese_, _romanian_, _russian_, _spanish_, _swedish_, _turkish_
</description><key id="855959">909</key><summary>Analysis: Stop words config to allow for automatic _lang_ expansion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-05T22:03:21Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-05-05T22:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/Analysis.java</file></files><comments><comment>Analysis: Stop words config to allow for automatic _lang_ expansion, closes #909.</comment></comments></commit></commits></item><item><title>Analysis: Expose light and minimal language token filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/908</link><project id="" key="" /><description>## Analyzers

All support also `stem_exclusion` on top of `stopwords`.
- `armenian`
- `basque`
- `bulgarian`
- `catalan`
- `danish`
- `english`
- `finnish`
- `galician`
- `hindi`
- `hungarian`
- `indonesian`
- `italian`
- `norwegian`
- `portuguese`
- `romanian`
- `spanish`
- `swedish`
- `turkish`
## Token Filter

`stemmer` (in addition to `snowball` and language specific stems) token filter with possible `name` or `language` values:

armenian, basque, catalan, danish, dutch, english, finnish, french, german, german2, hungarian, italian, kp, lovins, norwegian, porter, porter2, portuguese, romanian, russian, spanish, swedish, turkish, minimal_english, possessive_english, light_finish, light_french, minimal_french, light_german, minimal_german, hindi, light_hungarian, indonesian, light_italian, light_portuguese, minimal_portuguese, portuguese, light_russian, light_spanish, light_swedish
</description><key id="854633">908</key><summary>Analysis: Expose light and minimal language token filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-05T18:03:29Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-05-05T20:46:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArabicAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArmenianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BasqueAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BrazilianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BulgarianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CatalanAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CzechAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DanishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DutchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/EnglishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FinnishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FrenchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GalicianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GermanAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GreekAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/HindiAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/HungarianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/IndonesianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ItalianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NorwegianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PersianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PortugueseAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RomanianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RussianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SnowballTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SpanishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SwedishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/TurkishAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file></files><comments><comment>Analysis: Expose light and minimal language token filters, closes #908.</comment></comments></commit></commits></item><item><title>Query DSL: Fuzzy query support for numeric / date types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/907</link><project id="" key="" /><description>Support the notion of fuzzy query for numeric types and dates. `fuzzy` query on a numeric field will result in a range query "around" the value using the `min_similarity` value. For example:

```
{
    "fuzzy" : {
        "price" : {
            "value" : 12,
            "min_similarity" : 2
        }
    }
}
```

Will result in a range query between `10` and `14`. Same applies to dates, with support for time format for the `min_similarity` field:

```
{
    "fuzzy" : {
        "created" : {
            "value" : "2010-02-05T12:05:07",
            "min_similarity" : "1d"
        }
    }
}
```

In the mapping, numeric and date types now allow to configure a `fuzzy_factor` mapping value (defaults to 1), which will be used to multiply the fuzzy value by it when used in a `query_string` type query. For example, for dates, a fuzzy factor of "1d" will result in multiplying whatever fuzzy value provided in the `min_similarity` by it. Note, this is explicitly supported since `query_string` query only allowed for similarity valued between `0.0` and `1.0`.
</description><key id="853197">907</key><summary>Query DSL: Fuzzy query support for numeric / date types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-05T12:55:51Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-05-05T13:36:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/AbstractFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/BoostFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/ByteFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/DateFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/DoubleFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/FloatFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/IntegerFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/LongFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/NumberFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/ShortFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/SizeFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentTypeParsers.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/ip/IpFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FuzzyQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FuzzyQueryParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/SimpleIndexQueryParserTests.java</file></files><comments><comment>Query DSL: Fuzzy query support for numeric / date types, closes #907.</comment></comments></commit></commits></item><item><title>Simplified Disable Flush operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/906</link><project id="" key="" /><description>Add `index.translog.disable_flush` (defaults to `false`), which can be dynamically updated using the `update_settings` API. Note, this will only disable automatic flushing, explicit, api based flush will still flush.

Based on a conversation on the user list:

&gt; &gt; &gt; Regarding the copying option, yes, you can set teh translog settings (they can be changed dynamically now in 0.16) to high values, and then do the copying over (maybe flush before). This will make sure that no new Lucene commit point is written to disk.
&gt; &gt; 
&gt; &gt; Excellent!  So the 3 translog settings, if they are all changed to some ridiculously large number to ensure none of the conditions triggers a flush, and then a manual flush API call is done, it is safe to replicate the data directory to another location, and then reset the settings.
&gt; 
&gt; Yes. It can be simplified, we can add a disable flush flag that can be set dynamically.

The use case for a disabled flush is to allow a manual flush to occur, and _then_ some other process, like an rsync, or a fast recursive hard-link (and then an rsync perhaps) to facilitate secondary/tertiary/Disaster Recovery site replicant of the index and meta data.
</description><key id="852890">906</key><summary>Simplified Disable Flush operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tallpsmith</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-05T11:00:13Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-05-11T16:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/TranslogService.java</file></files><comments><comment>Simplified Disable Flush operation, closes #906.</comment></comments></commit></commits></item><item><title>Query DSL: Fuzzy Query - add max_expansions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/905</link><project id="" key="" /><description>Add `max_expansions` to the fuzzy query allowing to control the maximum number of terms to match. Default to unbounded (or bounded by the max clause count in boolean query).
</description><key id="852862">905</key><summary>Query DSL: Fuzzy Query - add max_expansions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-05T10:45:54Z</created><updated>2011-08-22T14:21:32Z</updated><resolved>2011-05-05T10:46:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dnavre" created="2011-08-22T14:21:32Z" id="1870504">Hi Shay, I'm very sorry for disturbing you but I was unable to find any documentation actually explaining what max_expansions parameter does. And i do not understand the comment on this commit. I do not have much experience of working with fuzzy algorithms so that can be the reason of me not understanding the issue :)
I've asked this question in more details on stackoverflow here: http://stackoverflow.com/questions/7148615/elasticsearch-fuzzy-matching-max-expansions-min-similarity

Can you please answer it here or on stackoverflow. I will be very thankful :) I have very strict requirements regarding this so the search should produce understandable(at least to me) results.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FuzzyQueryParser.java</file></files><comments><comment>Query DSL: Fuzzy Query - add max_expansions, closes #905.</comment></comments></commit></commits></item><item><title>Week Interval on Date Histogram Facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/904</link><project id="" key="" /><description>I think it would be very useful if we had a "week" interval on the date histogram facet with proper chronology handling. The interval value will be `1w` or `week`.
</description><key id="850971">904</key><summary>Week Interval on Date Histogram Facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rossini</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-04T21:45:43Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-05-04T22:02:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-04T21:47:02Z" id="1102405">There is one, just pass `1w` in the `interval`.
</comment><comment author="rossini" created="2011-05-04T21:51:49Z" id="1102432">Got it, sorry for the issue, I was trying "w" and "week" with no success.
</comment><comment author="kimchy" created="2011-05-04T21:53:15Z" id="1102443">Got you, its not as good as month (time wise), as it simply does bucketing based on how long a week takes in milliseconds. I need to check if the timing library I use can do rounding based on weeks, then it might be simpler and more accurate.
</comment><comment author="kimchy" created="2011-05-04T22:00:35Z" id="1102488">I will reopen, seems like there is good support for that (with proper week handling based on chronology).
</comment><comment author="kimchy" created="2011-05-04T22:02:23Z" id="1102504">ok, pushed support for it (forgot the closes handle in the commit message)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failure when applying failed shards during local gateway allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/903</link><project id="" key="" /><description>&lt;pre&gt;
java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
at java.util.HashMap$KeyIterator.next(HashMap.java:828)
at org.elasticsearch.gateway.local.LocalGatewayNodeAllocation.buildShardStores(LocalGatewayNodeAllocation.java:353)
at org.elasticsearch.gateway.local.LocalGatewayNodeAllocation.allocateUnassigned(LocalGatewayNodeAllocation.java:213)
at org.elasticsearch.cluster.routing.allocation.NodeAllocations.allocateUnassigned(NodeAllocations.java:80)
at org.elasticsearch.cluster.routing.allocation.ShardsAllocation.reroute(ShardsAllocation.java:147)
at org.elasticsearch.cluster.routing.allocation.ShardsAllocation.reroute(ShardsAllocation.java:99)
at org.elasticsearch.cluster.routing.RoutingService$RoutingTableUpdater$1.execute(RoutingService.java:124)
at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:175)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
&lt;/pre&gt;


Patch provided in mailing list:

&lt;pre&gt;
--- a/modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGatewayNodeAllocation.java
+++ b/modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGatewayNodeAllocation.java
@@ -350,9 +350,10 @@ public class LocalGatewayNodeAllocation extends
NodeAllocation {
} else {
nodesIds = Sets.newHashSet();
// clean nodes that have failed
- for (DiscoveryNode node : shardStores.keySet()) {
- if (!nodes.nodeExists(node.id())) {
- shardStores.remove(node);
+ for (Iterator&lt;DiscoveryNode&gt; i =
shardStores.keySet().iterator(); i.hasNext();) {
+ DiscoveryNode node = i.next();
+ if (!nodes.nodeExists(node.id())) {
+ i.remove();
}
}
&lt;/pre&gt;
</description><key id="849835">903</key><summary>Failure when applying failed shards during local gateway allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-04T17:49:18Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-05-04T17:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/blobstore/BlobReuseExistingNodeAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGatewayNodeAllocation.java</file></files><comments><comment>Failure when applying failed shards during local gateway allocation, closes #903.</comment></comments></commit></commits></item><item><title>Possible failure to start a river after cluster restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/902</link><project id="" key="" /><description>Getting this failure:

&lt;pre&gt;
[2011-05-04 16:19:39,401][WARN ][river ]
[Moonstone] failed to get _meta from [testcluster]/[testriver]
org.elasticsearch.action.NoShardAvailableActionException: [_river][0]
No shard available for [testriver#_meta]
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:193)
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.performFirst(TransportShardSingleOperationAction.java:159)
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:116)
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:69)
at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:45)
at org.elasticsearch.action.support.BaseAction.execute(BaseAction.java:61)
at org.elasticsearch.client.node.NodeClient.get(NodeClient.java:152)
at org.elasticsearch.client.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:119)
at org.elasticsearch.client.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:56)
at org.elasticsearch.river.RiversService$ApplyRivers.riverClusterChanged(RiversService.java:217)
at org.elasticsearch.river.cluster.RiverClusterService$1.run(RiverClusterService.java:126)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
&lt;/pre&gt;
</description><key id="849592">902</key><summary>Possible failure to start a river after cluster restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-04T17:01:28Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-05-04T17:27:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/river/RiversService.java</file></files><comments><comment>Possible failure to start a river after cluster restart, closes #902.</comment></comments></commit></commits></item><item><title>Remove unnecessary aliasAndIndexToIndex map</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/901</link><project id="" key="" /><description>The second map doesn't seem to be necessary. 
</description><key id="845701">901</key><summary>Remove unnecessary aliasAndIndexToIndex map</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-05-03T21:12:35Z</created><updated>2014-07-16T21:56:48Z</updated><resolved>2011-05-04T06:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-04T06:54:21Z" id="1098326">Applied, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Synonym Token Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/900</link><project id="" key="" /><description>Add a synonym token filter. The synonym token filter can be configured in the following manner:

```
{
    "index" : {
        "analysis" : {
            "analyzer" : {
                "synonym" : {
                    "tokenizer" : "whitespace",
                    "filter" : ["synonym"]
                }
            },
            "filter" : {
                "synonym" : {
                    "type" : "synonym",
                    "synonym_path" : "analysis/synonym.txt"
                }
            }
        }
    }
}
```

The above configured a `synonym` filter, with a path of `analysis/synonym.txt` (based from the `config` location). The `synonym` analyzer is then configured with the filter. Additional settings are: `ignore_case` (defaults to `false`), and `expand` (defaults to `true`).

Here is a sample format of the file (uses the same solr file format):

```
# blank lines and lines starting with pound are comments.

#Explicit mappings match any token sequence on the LHS of "=&gt;"
#and replace with all alternatives on the RHS.  These types of mappings
#ignore the expand parameter in the schema.
#Examples:
i-pod, i pod =&gt; ipod,
sea biscuit, sea biscit =&gt; seabiscuit

#Equivalent synonyms may be separated with commas and give
#no explicit mapping.  In this case the mapping behavior will
#be taken from the expand parameter in the schema.  This allows
#the same synonym file to be used in different synonym handling strategies.
#Examples:
ipod, i-pod, i pod
foozball , foosball
universe , cosmos

# If expand==true, "ipod, i-pod, i pod" is equivalent to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod, i-pod, i pod
# If expand==false, "ipod, i-pod, i pod" is equivalent to the explicit mapping:
ipod, i-pod, i pod =&gt; ipod

#multiple synonym mapping entries are merged.
foo =&gt; foo bar
foo =&gt; baz
#is equivalent to
foo =&gt; foo bar, baz
```
## Original Request

I'd like to see Synonym analyzer added into ElasticSearch. The Solr equivalent is the [SynonymFilterFactory](http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.SynonymFilterFactory).

My example use case is searching for jobs:

When I search for "ruby developer", I'd like also documents matching "ruby programmer", "ruby engineer", "ruby coder" returned.

When I search for "ZF developer", I'd like also documents matching "Zend Framework programmer", "ZF programmer" returned.

The synonym list could be passed similarly to the [StopAnalyzer](http://www.elasticsearch.org/guide/reference/index-modules/analysis/stop-analyzer.html).

Is something like this possible to do with a [custom analyzer](http://www.elasticsearch.org/guide/reference/index-modules/analysis/custom-analyzer.html) in ES curently?
</description><key id="843812">900</key><summary>Analysis: Synonym Token Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtris</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-03T14:56:01Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-05-10T19:37:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2011-05-03T14:58:43Z" id="1094270">+1 I'd enjoy synonym analyzer as well...
</comment><comment author="rmuir" created="2011-05-06T21:37:24Z" id="1113749">this has been refactored into the lucene analysis module in 4.0... might be a good start, you just have to construct the synonym map somehow from however you want the input file to look:

http://svn.apache.org/repos/asf/lucene/dev/trunk/modules/analysis/common/src/java/org/apache/lucene/analysis/synonym/
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/synonym/SynonymFilter.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/analysis/synonym/SynonymMap.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/Strings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/AbstractCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file></files><comments><comment>Analysis: Synonym Token Filter, closes #900.</comment></comments></commit></commits></item><item><title>Sort missing wrongly sorts negative values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/899</link><project id="" key="" /><description>Sort missing wrongly sorts negative values, see #772.
</description><key id="843339">899</key><summary>Sort missing wrongly sorts negative values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-05-03T13:15:35Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-05-12T18:04:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-12T18:04:05Z" id="1148274">Fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Silent failure when heap larger than available memory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/898</link><project id="" key="" /><description>Simple issue...

If I set the heap min greater than available memory (eg, 8GB on a 7.5GB instance) elasticsearch will quietly fail to start.

```
PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ "/var/assistly/elasticsearch/bin/service/exec/elasticsearch-linux-x86-64" "/var/assistly/elasticsearch/bin/service/elasticsearch.conf" wrapper.syslog.ident="elasticsearch" wrapper.pidfile="/var/assistly/elasticsearch-0.16.0-SNAPSHOT/bin/../../elasticsearch-servicewrapper/service/./elasticsearch.pid" wrapper.name="elasticsearch" wrapper.displayname="ElasticSearch" wrapper.daemonize=TRUE   wrapper.statusfile="/var/assistly/elasticsearch-0.16.0-SNAPSHOT/bin/../../elasticsearch-servicewrapper/service/./elasticsearch.status" wrapper.java.statusfile="/var/assistly/elasticsearch-0.16.0-SNAPSHOT/bin/../../elasticsearch-servicewrapper/service/./elasticsearch.java.status"
PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ echo $?
0
PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ ps auxw | grep elastic | grep -v grep
PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ ls /mnt/elasticsearch/profiling/
PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ tail elasticsearch.log
[2011-05-03 04:18:25,593][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: stopping ...
[2011-05-03 04:18:25,631][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: stopped
[2011-05-03 04:18:25,632][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: closing ...
[2011-05-03 04:18:26,047][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: closed
PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ date
Tue May  3 05:14:56 UTC 2011
```

The command line I used is the same command line generated by the service wrapper.

Notice the exit status is 0, there are no errors logged, etc.  I assume this is to do with the fact that the JVM itself is the one taking care of the allocation?

If that's the case, is it possible to pass additional options to the JVM to tell it to complain loudly so that the service wrapper will pick it up?

Cheers,
David.
</description><key id="841880">898</key><summary>Silent failure when heap larger than available memory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">cloudartisan</reporter><labels /><created>2011-05-03T05:18:34Z</created><updated>2013-08-09T11:22:28Z</updated><resolved>2013-08-09T11:22:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-07T15:03:46Z" id="19112432">Are you running elasticsearch on a 32bit system? Otherwise it should not be a problem to assign more heap than RAM to your JVM.
</comment><comment author="spinscale" created="2013-08-09T11:22:28Z" id="22388515">Closing for now. Happy to reopen and have a further look, if you provide more information.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding support for accessing EC2 and S3 over an http proxy.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/897</link><project id="" key="" /><description>Adding support for accessing EC2 and S3 over an http proxy. This is configured using the `network.proxyHost` and `network.proxyPort` settings.
</description><key id="841408">897</key><summary>Adding support for accessing EC2 and S3 over an http proxy.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acreeger</reporter><labels /><created>2011-05-03T01:14:41Z</created><updated>2014-06-18T08:00:20Z</updated><resolved>2011-05-03T13:45:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-05-03T13:45:27Z" id="1093897">Merged, thanks!. I have changed to settings to be `cloud.aws.proxy_host` (or `cloud.aws.proxyHost`), and `cloud.aws.proxy_port`. This is because it does not relate to the networking layer in elasticsearch itself.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sort: Support "missing" specific handling, include _last, _first, and custom value (for string values)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/896</link><project id="" key="" /><description>This is a follow up to #772 that supports special sorting for 'null' values for numeric fields. The same would be very useful for (not analyzed) string fields as well. If feasible, special handling for empty and blank strings would be useful. The latter is however optional as it is possible to achieve uniform handling by filtering out blank values at indexing time.
</description><key id="839001">896</key><summary>Sort: Support "missing" specific handling, include _last, _first, and custom value (for string values)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jfiedler</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2011-05-02T15:03:01Z</created><updated>2014-06-08T20:54:50Z</updated><resolved>2013-08-27T08:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fabian" created="2011-08-17T12:34:40Z" id="1825883">+1
</comment><comment author="nickhoffman" created="2011-11-16T19:54:29Z" id="2766912">This is a great suggestion, and fairly inevitable, I would imagine.

+1
</comment><comment author="ro-ka" created="2011-11-29T10:27:22Z" id="2927557">+1
</comment><comment author="mleglise" created="2012-03-02T01:37:49Z" id="4278715">+1
</comment><comment author="ghost" created="2012-09-20T14:12:15Z" id="8729653">+1
</comment><comment author="ajhalani" created="2012-12-24T19:03:33Z" id="11664798">+1. Just wondering if anyone is working on this? thanks!
</comment><comment author="kretes" created="2013-01-25T13:45:17Z" id="12701154">When this is handled - it should be for all types, e.g. dates, not just string fields
</comment><comment author="ajhalani" created="2013-01-25T15:00:45Z" id="12703944">It supports dates already, the documentation probably needs to be updated.
</comment><comment author="clintongormley" created="2013-04-04T19:09:34Z" id="15917307">As of 0.90.0.RC2, this is supported for numbers, dates, strings, geo-locations
</comment><comment author="matthuhiggins" created="2013-05-09T19:34:25Z" id="17684796">Does anyone know if there is a way to make the default be `{missing: '_last'}`? 
</comment><comment author="clintongormley" created="2013-05-23T10:29:50Z" id="18335153">Apparently I was incorrect about this being supported for strings already, so reopening this issue
</comment><comment author="bgadoury" created="2013-07-16T03:21:19Z" id="21018944">+1
</comment><comment author="tommymonk" created="2013-07-20T13:19:50Z" id="21293383">+1
</comment><comment author="mobilutz" created="2013-08-05T10:38:55Z" id="22098374">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefFieldComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefOrdValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/NestedWrappableComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/NumberComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NestedFieldComparatorSource.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/AbstractFieldDataImplTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/AbstractNumericFieldDataTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/AbstractStringFieldDataTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/DoubleFieldDataTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/FloatFieldDataTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/LongFieldDataTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/fielddata/NoOrdinalsStringFieldDataTests.java</file><file>src/test/java/org/elasticsearch/test/unit/index/search/nested/NestedSortingTests.java</file></files><comments><comment>Configurable sort order for missing string values.</comment></comments></commit></commits></item><item><title>\W not equivalent to [^\w] in the pattern analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/895</link><project id="" key="" /><description>According to the docs http://download.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html `\W` should be the equivalent of `[^\w]`, but it is not functioning in that way in the pattern analyzer:

```
curl -XPUT 'localhost:9200/test' -d '
{
    "settings":{
        "analysis": {
            "analyzer": {
                "one":{
                    "type": "pattern",
                    "pattern":"\\W+"
                },
                "two":{
                    "type": "pattern",
                    "pattern":"[^\\w]+"
                }
            }
        }
    }
}'

curl 'localhost:9200/test/_analyze?pretty=1&amp;analyzer=one' -d '123 type_1-type_4'
# {
#   "tokens" : [ {
#     "token" : "type",
#     "start_offset" : 4,
#     "end_offset" : 8,
#     "type" : "word",
#     "position" : 1
#   }, {
#     "token" : "type",
#     "start_offset" : 11,
#     "end_offset" : 15,
#     "type" : "word",
#     "position" : 2
#   } ]
# }

curl 'localhost:9200/test/_analyze?pretty=1&amp;analyzer=two' -d '123 type_1-type_4'
# {
#   "tokens" : [ {
#     "token" : "123",
#     "start_offset" : 0,
#     "end_offset" : 3,
#     "type" : "word",
#     "position" : 1
#   }, {
#     "token" : "type_1",
#     "start_offset" : 4,
#     "end_offset" : 10,
#     "type" : "word",
#     "position" : 2
#   }, {
#     "token" : "type_4",
#     "start_offset" : 11,
#     "end_offset" : 17,
#     "type" : "word",
#     "position" : 3
#   } ]
# }
```
</description><key id="833215">895</key><summary>\W not equivalent to [^\w] in the pattern analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-04-30T11:56:52Z</created><updated>2014-07-03T19:12:18Z</updated><resolved>2014-07-03T19:12:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T19:12:18Z" id="47972403">The bug is in Lucene's PatternAnalyzer (deprecated), which maps the `\W+` regex to an isLetter() check only, excluding numbers and underscore.

Will be fixed by switching to an analyzer which uses the PatternTokenFilter instead. Closing in favour of #6717 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CamelCaseAnalyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/894</link><project id="" key="" /><description>Hiya

Moonk said that you were thinking about adding a CamelCaseAnalyzer.  Not sure if there is a more efficient way to write it, but here is a unicode aware one that uses the Pattern Analyzer:

```
curl -XDELETE localhost:9200/test
curl -XPUT 'localhost:9200/test?pretty=1' -d '
{
    "settings":{
        "analysis": {
            "analyzer": {
                "camel":{
                    "type": "pattern",
                    "lowercase": true,
                    "pattern":"(\\W+)|(?&lt;=[^\\p{Lu}])(?=\\p{Lu})|(?&lt;=\\p{Lu})(?=\\p{Lu}[^\\p{Lu}])"
                }
            }
        }
    }
}'

curl 'localhost:9200/test/_analyze?pretty=1&amp;analyzer=camel' -d 'MooseX::FTPClass Acme::CPANAuthors::AlsoANASAAstronaut'
```

Outputs:

```
{
  "tokens" : [ {
    "token" : "moose",
    "start_offset" : 0,
    "end_offset" : 5,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "x",
    "start_offset" : 5,
    "end_offset" : 6,
    "type" : "word",
    "position" : 2
  }, {
    "token" : "ftp",
    "start_offset" : 8,
    "end_offset" : 11,
    "type" : "word",
    "position" : 3
  }, {
    "token" : "class",
    "start_offset" : 11,
    "end_offset" : 16,
    "type" : "word",
    "position" : 4
  }, {
    "token" : "acme",
    "start_offset" : 17,
    "end_offset" : 21,
    "type" : "word",
    "position" : 5
  }, {
    "token" : "cpan",
    "start_offset" : 23,
    "end_offset" : 27,
    "type" : "word",
    "position" : 6
  }, {
    "token" : "authors",
    "start_offset" : 27,
    "end_offset" : 34,
    "type" : "word",
    "position" : 7
  }, {
    "token" : "also",
    "start_offset" : 36,
    "end_offset" : 40,
    "type" : "word",
    "position" : 8
  }, {
    "token" : "anasa",
    "start_offset" : 40,
    "end_offset" : 45,
    "type" : "word",
    "position" : 9
  }, {
    "token" : "astronaut",
    "start_offset" : 45,
    "end_offset" : 54,
    "type" : "word",
    "position" : 10
  } ]
}
```
</description><key id="833168">894</key><summary>CamelCaseAnalyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-04-30T11:09:37Z</created><updated>2011-05-13T11:55:56Z</updated><resolved>2011-05-13T11:55:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-04-30T11:11:08Z" id="1080450">Note. `ANASAAstronaut` becomes `anasa`,`astronaut` because it should really be written `ANasaAstronaut` I think :) 

No way to get around that without a dictionary
</comment><comment author="clintongormley" created="2011-04-30T12:27:52Z" id="1080553">Actually, this is better. Takes numbers into account, and breaks on underscores as well:

```
curl -XDELETE localhost:9200/test
curl -XPUT 'localhost:9200/test?pretty=1' -d '
{
    "settings":{
        "analysis": {
            "analyzer": {
                "camel":{
                    "type": "pattern",
                    "lowercase": true,
                    "pattern":"([^\\p{L}\\d]+)|(?&lt;=\\D)(?=\\d)|(?&lt;=\\d)(?=\\D)|(?&lt;=[\\p{L}&amp;&amp;[^\\p{Lu}]])(?=\\p{Lu})|(?&lt;=\\p{Lu})(?=\\p{Lu}[\\p{L}&amp;&amp;[^\\p{Lu}]])"
                }
            }
        }
    }
}'
curl 'localhost:9200/test/_analyze?pretty=1&amp;analyzer=camel' -d 'MooseX::FTPClass foo_bar123Baz'
```

The regex works like this:

```
  ([^\\p{L}\\d]+)                 # swallow non letters and numbers,
| (?&lt;=\\D)(?=\\d)                 # non-number followed by number,
| (?&lt;=\\d)(?=\\D)                 # number followed by non-number,
| (?&lt;=[ \\p{L} &amp;&amp; [^\\p{Lu}]])    # lower case 
  (?=\\p{Lu})                     #   followed by upper case,
| (?&lt;=\\p{Lu})                    # upper case
  (?=\\p{Lu}                      #   followed by upper 
    [\\p{L}&amp;&amp;[^\\p{Lu}]]          #   then lower
  )
```
</comment><comment author="kimchy" created="2011-05-13T11:55:56Z" id="1152422">Closing this since the word delimiter token filter does that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query statistics: start time, requests, errors, req/sec, time/sec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/893</link><project id="" key="" /><description>It would be useful to know the following per-index (or even per-shard-per-index):
- Start time
- Total requests
- Error count
- Requests/sec rate
- Time/request (average)
</description><key id="830959">893</key><summary>Query statistics: start time, requests, errors, req/sec, time/sec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kcm</reporter><labels /><created>2011-04-29T21:50:10Z</created><updated>2013-04-04T19:10:13Z</updated><resolved>2013-04-04T19:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:10:13Z" id="15917352">These are all supported now
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>And/Or Filter: Possible failure when inner filter does not match any docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/892</link><project id="" key="" /><description>And/Or Filter: Possible failure when inner filter does not match any docs. See Repro:

```
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
"user" : "kimchy",
"post_date" : "2009-11-15T14:12:12",
"message" : "trying out Elastic Search"
}'

curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d '{"query" : { "filtered" : {"query" : {"term" : {"user" : "kimchy"}}, "filter" : {"and" : [{"term" : {"id" : 1}}, {"query": {"query_string" : { "default_field": "message", "query" : "the"}}} ]}}}}'
```
</description><key id="830211">892</key><summary>And/Or Filter: Possible failure when inner filter does not match any docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-04-29T18:44:33Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-04-29T18:45:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/AndDocIdSet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/AndDocSet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/OrDocIdSet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/docset/OrDocSet.java</file></files><comments><comment>And/Or Filter: Possible failure when inner filter does not match any docs, closes #892.</comment></comments></commit></commits></item><item><title>Filtered query standard analyzer stopword issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/891</link><project id="" key="" /><description>When doing a filtered query search for a stopword I get a shard failure error. 

Reproducible below.

curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "user" : "kimchy", 
    "post_date" : "2009-11-15T14:12:12", 
    "message" : "trying out Elastic Search" 
}' 

curl -XGET 'http://localhost:9200/twitter/_search?pretty=true' -d 
' {"query" : { "filtered" : {"query" : {"term" : {"user" : "kimchy"}}, "filter" : {"and" : [{"term" : {"id" : 1}},  {"query": 
{"query_string" : { "default_field": "message", "query" : 
"the"}}}  ]}}}}' 

{ 
  "took" : 3, 
  "timed_out" : false, 
  "_shards" : { 
    "total" : 5, 
    "successful" : 4, 
    "failed" : 1, 
    "failures" : [ { 
      "index" : "twitter", 
      "shard" : 2, 
      "reason" : "QueryPhaseExecutionException[[twitter][2]: 
query[filtered(id:1)- 

&gt; org.elasticsearch.common.lucene.search.AndFilter@bf6de680],from[0],size[10 ]: 

Query Failed [Failed to execute main query]]; nested: " 
    } ] 
  }, 
  "hits" : { 
    "total" : 0, 
    "max_score" : null, 
    "hits" : [ ] 
  } 
} 

If I change my search term to anything other than a stopword all is well. 
</description><key id="830065">891</key><summary>Filtered query standard analyzer stopword issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frazerh</reporter><labels /><created>2011-04-29T18:09:25Z</created><updated>2011-04-29T19:56:31Z</updated><resolved>2011-04-29T19:56:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-29T19:56:30Z" id="1076976">Missed it, and opened #892. Already fixed, so closing this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query String queries does not work properly with accents (french analyzer)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/890</link><project id="" key="" /><description>With a french analyzer, I tried to match the term "élève".

Theese queries work:

```
{"query": {"query_string": {"query": "élève"}}}
{"query": {"query_string": {"query": "éleve"}}}
{"query": {"query_string": {"analyze_wildcard": true, "query": "éle*"}}}
```

Theese one don't work:

```
{"query": {"query_string": {"query": "eleve"}}}
{"query": {"query_string": {"query": "elève"}}}
{"query": {"query_string": {"analyze_wildcard": true, "query": "élè*"}}}
{"query": {"query_string": {"analyze_wildcard": true, "query": "ele*"}}}
{"query": {"query_string": {"analyze_wildcard": true, "query": "elè*"}}}
```
</description><key id="828351">890</key><summary>Query String queries does not work properly with accents (french analyzer)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zebuline</reporter><labels /><created>2011-04-29T09:22:46Z</created><updated>2012-05-11T13:33:19Z</updated><resolved>2012-05-11T13:33:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-29T22:38:07Z" id="1077635">Heya, any chance for a full recreation including index creation and some data indexing? Will be faster for me to test it.
</comment><comment author="zebuline" created="2011-05-02T08:58:47Z" id="1087192">Hi, these are the test data:

Config:

```
index:
        analysis:
                analyzer:
                        default:
                                type: french
```

Index creation and data indexing:

```
curl -XPUT http://localhost:9200/twitter
curl -XPUT http://localhost:9200/twitter/tweet/2 -d '{
    "message": "Try to search the word élève"
}'
curl -XPUT http://localhost:9200/twitter/tweet/3 -d '{
    "message": "Joe Tester"
}'
```

Query String search, successful:

```
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "query": "élève"
        }
    }
}'
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "query": "éleve"
        }
    }
}'
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "analyze_wildcard": true,
            "query": "éle*"
        }
    }
}'
```

Query String search, fail:

```
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "query": "elève"
        }
    }
}'
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "query": "eleve"
        }
    }
}'
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "analyze_wildcard": true,
            "query": "élè*"
        }
    }
}'
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "analyze_wildcard": true,
            "query": "elè*"
        }
    }
}'
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "query_string": {
            "analyze_wildcard": true,
            "query": "ele*"
        }
    }
}'
```

I found an other issue with Term Query, which do not depend on analyzer.

Successful:

```
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "term": {
            "message": "try"
        }
    }
}'
```

Fail:

```
curl -XGET http://localhost:9200/twitter/tweet/_search -d '{
    "query": {
        "term": {
            "message": "joe"
        }
    }
}'
```
</comment><comment author="kimchy" created="2011-05-13T12:02:00Z" id="1152449">It seems like `élève` transforms into `élev` token when using the french analyzer, for example (using your analyzer config):

```
curl -XPOST localhost:9200/twitter/_analyze -d 'élève'
```

What you can do is construct your own custom analyzer, with the french stemmer and asciifolding.
</comment><comment author="zebuline" created="2011-05-17T08:15:15Z" id="1188531">Thanks for your suggestion, it works fine with a french stemmer.

My config is now:

```
index:
        analysis:
                analyzer:
                        default:
                                tokenizer:      standard
                                filter:         [standard, lowercase, asciifolding, french_stemmer]
                filter:
                        french_stemmer:
                                type:           stemmer
                                name:           light_french
```

With stemmer french, the term query `joe` is not found (see test data). It's ok with the stemmer light_french.

Now `élève` is transformed into `elev`. Is it normal that the last character is missing ?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: Optimize (perf) execution of global facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/889</link><project id="" key="" /><description>Optimize the execution of global facets, and, add specific global optimization for specific facets (query, filter).
</description><key id="824485">889</key><summary>Search: Optimize (perf) execution of global facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-04-28T14:19:50Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-04-28T14:20:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetPhase.java</file></files><comments><comment>Search: Optimize (perf) execution of global facets, closes #889.</comment></comments></commit><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/facet/QueryFilterFacetSearchBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/AbstractFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/FacetPhase.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/OptimizeGlobalFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/filter/FilterFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/query/QueryFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Search: Optimize (perf) execution of global facets, closes #889.</comment></comments></commit></commits></item><item><title>Search API: filter element should not be applied to global facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/888</link><project id="" key="" /><description>Search API: filter element should not be applied to global facets
</description><key id="824227">888</key><summary>Search API: filter element should not be applied to global facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-04-28T13:16:17Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-04-28T14:20:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file></files><comments><comment>Search API: filter element should not be applied to global facets, closes #888.</comment></comments></commit></commits></item><item><title>lats_id should be last_id?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/887</link><project id="" key="" /><description>Fix possible typos. If intentional, please ignore.
</description><key id="821198">887</key><summary>lats_id should be last_id?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgeiger</reporter><labels /><created>2011-04-27T19:32:17Z</created><updated>2014-07-16T21:56:49Z</updated><resolved>2011-05-24T23:20:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jgeiger" created="2011-04-27T19:38:12Z" id="1066249">Ugh. I think the "fork and edit file" function may have a bug.

I was only trying to fix the last_id change in the fork, not the main repo.
</comment><comment author="kimchy" created="2011-04-27T19:39:55Z" id="1066258">Yea, strange :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scripting - mvel: Fix thread safety misuse when compiling scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/886</link><project id="" key="" /><description>Scripting: Fix thread safety misuse when compiling scripts
</description><key id="816779">886</key><summary>Scripting - mvel: Fix thread safety misuse when compiling scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.2</label><label>v0.17.0</label></labels><created>2011-04-26T21:28:55Z</created><updated>2011-05-21T18:09:05Z</updated><resolved>2011-05-21T18:09:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-26T21:35:20Z" id="1060930">First pass will be to use ParserConfiguration in teh scripting, mvel still needs to fix a bug. Wll update once its there.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Scripting - mvel: Fix thread safety misuse when compiling scripts, closes #886. Upgrade to mvel 2.1.Beta3</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/mvel/MvelScriptEngineService.java</file></files><comments><comment>Scripting - mvel: Fix thread safety misuse when compiling scripts, first go it so use ParserConfiguration, second is a fix in mvel itself, relates to #886.</comment></comments></commit></commits></item><item><title>Create Index API: Better logic to wait for ack for indices being created on nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/885</link><project id="" key="" /><description>Create Index API: Better logic to wait for ack for indices being created on nodes. This can help with the IndexMissing failure @clintongormley reported at #879.
</description><key id="816750">885</key><summary>Create Index API: Better logic to wait for ack for indices being created on nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-04-26T21:23:54Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-04-26T21:25:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-26T21:25:52Z" id="1060871">Fixed (on a different issue by mistale).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query DSL: Allow to set boost on has_child query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/884</link><project id="" key="" /><description /><key id="815692">884</key><summary>Query DSL: Allow to set boost on has_child query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-04-26T17:39:38Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-04-26T17:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/HasChildQueryParser.java</file></files><comments><comment>Query DSL: Allow to set boost on has_child query, closes #884.</comment></comments></commit></commits></item><item><title>include_in_all not overridable in objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/883</link><project id="" key="" /><description>According to http://www.elasticsearch.org/guide/reference/mapping/object-type.html

"include_in_all can be set on the object type level. When set, it propagates down to all the inner mapping defined within the object that do no explicitly set it."

But this doesn't seem to work.  The top level `include_in_all` is overriding the value that is set lower down:

https://gist.github.com/gists/942353/edit
</description><key id="814836">883</key><summary>include_in_all not overridable in objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-04-26T14:33:01Z</created><updated>2011-05-12T18:09:04Z</updated><resolved>2011-04-28T12:44:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bjon" created="2011-04-28T18:43:14Z" id="1071422">Thanks alot!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/IncludeInAllMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/MultiFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/NumberFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/ObjectMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/StringFieldMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/xcontent/all/SimpleAllMapperTests.java</file></files><comments><comment>include_in_all not overridable in objects, closes #883.</comment></comments></commit></commits></item><item><title>Fix readme typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/882</link><project id="" key="" /><description>Fist -&gt; First
</description><key id="814636">882</key><summary>Fix readme typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgeiger</reporter><labels /><created>2011-04-26T13:49:29Z</created><updated>2014-07-16T21:56:49Z</updated><resolved>2011-04-26T14:16:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-04-26T14:16:22Z" id="1058388">Many thanks :)  Merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merged pull request #882 from jgeiger/master.</comment></comments></commit></commits></item><item><title>Not recognized: index.auto_expand_replicas: true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/881</link><project id="" key="" /><description>In 0.15.x I had set some indices to have 'index.auto_expand_replicas' set to boolean true.  This is not recognized in the new dynamically-configurable 0.16, which accepts either boolean false or a string to set the boundaries (e.g. "0-all"):

&lt;pre&gt;
[2011-04-25 15:47:44,126][WARN ][cluster.metadata         ] [Maggott] failed to set [index.auto_expand_replicas], wrong format [true]
&lt;/pre&gt;


To fix this, I did the following:

&lt;pre&gt;
  curl -XPUT localhost:9200/&lt;index&gt;/_settings -d '{"settings":{"index":{"auto_expand_replicas":"0-all"}}}'
  curl -XPUT localhost:9200/&lt;index&gt;/_settings -d '{"index":{"auto_expand_replicas":"0-all"}}'
&lt;/pre&gt;


The former is for the new dynamic setting, and the latter was for the legacy setting.  Perhaps boolean true could be a silent alias for "0-all"?
</description><key id="812002">881</key><summary>Not recognized: index.auto_expand_replicas: true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kcm</reporter><labels /><created>2011-04-25T22:56:09Z</created><updated>2013-04-04T19:10:41Z</updated><resolved>2013-04-04T19:10:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:10:41Z" id="15917377">I think no longer relevant - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>(Small) optimization to has_child filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/880</link><project id="" key="" /><description>(Small) optimization to has_child filter
</description><key id="809802">880</key><summary>(Small) optimization to has_child filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.1</label><label>v0.17.0</label></labels><created>2011-04-25T14:29:33Z</created><updated>2011-05-12T18:09:03Z</updated><resolved>2011-04-25T14:38:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/child/ChildCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/child/HasChildFilter.java</file></files><comments><comment>(Small) optimization to has_child filter, closes #880.</comment></comments></commit></commits></item><item><title>Intermittent IndexMissingException when auto-creating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/879</link><project id="" key="" /><description>Hiya

Running 0.16.0 on 3 nodes, and indexing a doc to a new index and type, I sometimes get IndexMissingExceptions:

```
# [Sun Apr 24 14:44:22 2011] Protocol: http, Server: 127.0.0.1:9202
curl -XPUT 'http://127.0.0.1:9200/es_test_3/type_3/1?pretty=1'  -d '
{
   "text" : "foo"
}
'

# [Sun Apr 24 14:44:22 2011] Response:
# {
#    "status" : 404,
#    "error" : "IndexMissingException[[es_test_3] missing]"
# }
```
</description><key id="807257">879</key><summary>Intermittent IndexMissingException when auto-creating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-04-24T12:48:19Z</created><updated>2012-08-04T11:31:40Z</updated><resolved>2012-08-04T11:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-24T19:12:35Z" id="1050620">Any way to recreate it?
</comment><comment author="clintongormley" created="2011-04-24T20:01:20Z" id="1050736">The script above is sufficient to recreate it. As I said, it is sporadic.  Happens once every few runs of my test suite, so I presume it is a timing issue.
</comment><comment author="kimchy" created="2011-04-24T21:31:24Z" id="1050947">How do you clean the data? Simply restart the nodes without data dir, or in some other manner?
</comment><comment author="kimchy" created="2011-04-26T21:26:34Z" id="1060881">I closed this by mistake. Pushed #885 which I think will help with this. I can't recreate it locally, but its a timing issue, which I think the mentioned issue solves.
</comment><comment author="clintongormley" created="2012-08-04T11:31:40Z" id="7501137">I haven't experienced this issue for a long time, so it looks like this has been fixed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file></files><comments><comment>Create Index API: Better logic to wait for ack for indices being created on nodes, closes #879.</comment></comments></commit></commits></item><item><title>Unexpected failure to create a shard can lead to data loss if it has no replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/878</link><project id="" key="" /><description>Unexpected failure to create a shard can lead to data loss if it has no replicas (well now its expected)
</description><key id="805070">878</key><summary>Unexpected failure to create a shard can lead to data loss if it has no replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-23T10:28:51Z</created><updated>2011-04-23T10:29:27Z</updated><resolved>2011-04-23T10:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Unexpected failure to create a shard can lead to data loss if it has no replicas, closes #878.</comment></comments></commit></commits></item><item><title>Case insensitive _id field  setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/877</link><project id="" key="" /><description>Add a setting to mapping definition to make _id field case insensitive 

Discussed in

http://elasticsearch-users.115913.n3.nabble.com/Lowercase-id-field-tp2843108p2843108.html
</description><key id="801496">877</key><summary>Case insensitive _id field  setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels /><created>2011-04-22T05:47:21Z</created><updated>2013-04-04T19:11:24Z</updated><resolved>2013-04-04T19:11:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:11:24Z" id="15917407">No plans to support 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ImmutableSettings.getAsClass shouldn't attempt to load camelCased version of the package name.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/876</link><project id="" key="" /><description>I ran into an issue with a discovery module that has a camelCased name. The module's name is org.elasticsearch.discovery.zookeeper.ZooKeeperDiscoveryModule and I am trying to load it using "zoo-keeper" as a discovery type. Unfortunately, ImmutableSettings.getAsClass first tries to load org.elasticsearch.discovery.zooKeeper.ZooKeeperDiscoveryModule and on Mac OS X it fails with NoClassDefFoundError instead of ClassNotFoundException, so the class with correct package name is never loaded. Since using camelCase for package names is somewhat unusual, wouldn't it make sense to not attempt to load a class with such name? 
</description><key id="799252">876</key><summary>ImmutableSettings.getAsClass shouldn't attempt to load camelCased version of the package name.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-04-21T16:05:18Z</created><updated>2014-07-16T21:56:50Z</updated><resolved>2011-04-21T18:58:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-21T17:18:52Z" id="1040244">But then other scenarios won't work. In your case, I would simply name it `zookeeper`, and have the discovery module named ZookeeperDiscoveryModule
</comment><comment author="imotov" created="2011-04-21T17:44:08Z" id="1040372">Sure, I can rename the module or put it in org.elasticsearch.discovery package. Both solutions work. However, I just cannot think of a scenario where the current logic would be useful. This camelCased part appears before "." so it can be either part of the package name or an external class name (if module is an internal class). But it's not really nice package name since it can contain upper case letters and it's not a nice class name since it starts with a lower case letter. So, I really cannot think of a scenario where it would work. Am I missing something?
</comment><comment author="kimchy" created="2011-04-21T18:58:50Z" id="1040749">Yea, I guess you are right, got confused with the diff change in github, but then saw the full change, and it makes sense. Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search request intermittent failures with has_child query/filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/875</link><project id="" key="" /><description>Search request intermittent failures with has_child query/filter.
</description><key id="798941">875</key><summary>Search request intermittent failures with has_child query/filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-21T14:53:25Z</created><updated>2011-04-21T14:54:13Z</updated><resolved>2011-04-21T14:54:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/child/ChildCollector.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/stress/search1/ParentChildStressTest.java</file></files><comments><comment>Search request intermittent failures with has_child query/filter, closes #875.</comment></comments></commit></commits></item><item><title>Update Settings: Allow to dynamically set filter cache settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/874</link><project id="" key="" /><description>Allow to dynamically set `index.cache.filter.max_size` and `index.cache.filter.expire` using the update settings API.
</description><key id="798387">874</key><summary>Update Settings: Allow to dynamically set filter cache settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-21T11:40:09Z</created><updated>2014-06-10T18:05:08Z</updated><resolved>2011-04-21T11:41:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/unit/TimeValueTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/cache/filter/FilterCacheTests.java</file></files><comments><comment>Update Settings: Allow to dynamically set filter cache settings, closes #874.</comment></comments></commit></commits></item><item><title>No highlight results when `number_of_fragments` set to 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/873</link><project id="" key="" /><description>The [Highlight documentation](http://www.elasticsearch.org/guide/reference/api/search/highlighting.html) says:

&gt; If the number_of_fragments value is set to 0 then no fragments are produced, instead the whole content of the field is returned, and of course it is highlighted. 

However, this does not work as stated. When running this gist: https://gist.github.com/931339 against current master, no highlights are returned.
</description><key id="797878">873</key><summary>No highlight results when `number_of_fragments` set to 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-21T08:11:07Z</created><updated>2011-04-21T12:55:54Z</updated><resolved>2011-04-21T12:55:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-04-21T08:38:19Z" id="1038298">The issues seems to be only when the field is not stored, see https://gist.github.com/933978 it works as expected.
So getting the field content from `_source` is probably not working correctly.
</comment><comment author="karmi" created="2011-04-21T08:41:36Z" id="1038310">I don't understand: I think it should work as expected in the original gist, no?
</comment><comment author="lukas-vlcek" created="2011-04-21T09:06:15Z" id="1038407">Based on the docs it should.

Here is the situation: you need to get the content of the Lucene field to have it highlighted. And there are two options, either you store the content (which is what I did in my gits example) and then you can retrieve the content from the Lucene index or you provide the content yourself to the highlighter which is what should happen if the filed is not stored, it should be taken from the `_source` field but this does not seem to work correctly.

So there is probably a bug in highlighting but only for the later situation, ie when the field is expected to be retrieved from the `_source` field.
</comment><comment author="lukas-vlcek" created="2011-04-21T09:09:09Z" id="1038419">btw: here is relevant ticket https://github.com/elasticsearch/elasticsearch/issues/561
</comment><comment author="lukas-vlcek" created="2011-04-21T09:53:06Z" id="1038537">karmi, it works, here is the final solution :-) https://gist.github.com/933978
The problem is that if the field is not stored then you still have to set term_vector (and thus use mapping for this). The documentation needs to be updated to make this crystal clear.
</comment><comment author="karmi" created="2011-04-21T09:56:20Z" id="1038550">Thanks!, but... shouldn't it just work out of the box? Should the issue be really closed in this state?
</comment><comment author="lukas-vlcek" created="2011-04-21T11:12:28Z" id="1038763">You are right, there is a bug.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Fix for highlighting when number_of_fragments is set to 0 and term_vector is not set, closes #873</comment></comments></commit></commits></item><item><title>README: Fixed typo and formatting in the „Building from Source” chapter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/872</link><project id="" key="" /><description>Hi,

I've fixed some typo and URL in Readme,

karmi
</description><key id="797841">872</key><summary>README: Fixed typo and formatting in the „Building from Source” chapter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2011-04-21T07:53:09Z</created><updated>2014-06-17T04:20:14Z</updated><resolved>2011-04-21T12:53:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-21T12:53:02Z" id="1039044">pushed, thanks!.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify Filter Cache Eviction Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/871</link><project id="" key="" /><description>Add `index.cache.filter.max_size` and `index.cache.filter.expire` (on top of using the specific filter cache type based ones), makes it simpler to configure regardless of the type used (`weak`, `resident` or `soft`), though, with how the cache works now, `soft` there isn't a real reason to change the default `soft` type.
</description><key id="796588">871</key><summary>Simplify Filter Cache Eviction Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-20T23:04:41Z</created><updated>2011-04-20T23:05:16Z</updated><resolved>2011-04-20T23:05:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file></files><comments><comment>Simplify Filter Cache Eviction Settings, closes #871.</comment></comments></commit></commits></item><item><title>add a flag to search request to open new new IndexReader from IndexWriter in RobinEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/870</link><project id="" key="" /><description>Discussed in 
http://elasticsearch-users.115913.n3.nabble.com/About-lucene-NRT-feature-tp2839693p2839693.html
</description><key id="792093">870</key><summary>add a flag to search request to open new new IndexReader from IndexWriter in RobinEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels /><created>2011-04-20T05:28:59Z</created><updated>2014-07-18T08:04:48Z</updated><resolved>2014-07-18T08:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2011-07-06T14:37:58Z" id="1512142">The other option, less costly performance wise, is to wait on the index operation when possible.

See issue: https://github.com/elasticsearch/elasticsearch/issues/1063
</comment><comment author="clintongormley" created="2014-07-18T08:04:41Z" id="49405473">Refreshing often is costly.  While you could issue a refresh before searching, doing it too often will be too costly. We don't want to add a flag because then people will misuse it.

Instead we will implement a block-until-refresh flag which will stall search until a refresh has happened. See #1063
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failure to recover a shard might cause loosing translog data (especially with no replicas)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/869</link><project id="" key="" /><description>Failure to recover a shard might cause loosing translog data (especially with no replicas)
</description><key id="791685">869</key><summary>Failure to recover a shard might cause loosing translog data (especially with no replicas)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-20T01:46:27Z</created><updated>2011-04-20T01:52:59Z</updated><resolved>2011-04-20T01:52:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Failure to recover a shard might cause loosing translog data (especially with no replicas), closes #869.</comment></comments></commit></commits></item><item><title>Mapping: By default, don't index _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/868</link><project id="" key="" /><description>In order to save memory usage (per shard), don't index `_id` by default. No functionality is lost unless executing explicit search queries against `_id` field (for example, using term query / filter). These can be replaced with the `ids` query/filter (which work without the need for `_id` to be indexed, and work even faster).

To maintain backward compatibility (assuming the `_id` specific queries can't be translated / no time to do it), the `index.mapping._id.indexed` setting should be set to `true`.

Note, the above flag should not be the one to use if wishing to index the `_id` field on a clean index, instead, the `_id` mapping can have `index` set to `not_analyzed`.
</description><key id="791395">868</key><summary>Mapping: By default, don't index _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.16.0</label></labels><created>2011-04-19T23:48:23Z</created><updated>2011-04-19T23:49:30Z</updated><resolved>2011-04-19T23:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/IdFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentDocumentMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentDocumentMapperParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentMapperBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/xcontent/simple/SimpleMapperTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/child/SimpleChildQuerySearchTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/compress/SearchSourceCompressTests.java</file></files><comments><comment>Mapping: By default, don't index _id, closes #868.</comment></comments></commit></commits></item><item><title>Query DSL: Type filter (can also work when _type is not indexed)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/867</link><project id="" key="" /><description>Special type filter, which can also work when the `_type` field mapping is not indexed. The format is:

```
{
    "type" : {
        "value" : "my_type"
    }
}
```
</description><key id="791204">867</key><summary>Query DSL: Type filter (can also work when _type is not indexed)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-19T22:53:14Z</created><updated>2011-04-19T22:53:36Z</updated><resolved>2011-04-19T22:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-19T22:53:36Z" id="1029884">Pushed in previous issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better support with _type is marked as not indexed, allowing to filter by type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/866</link><project id="" key="" /><description>Better support with _type is marked as not indexed, allowing to filter by type
</description><key id="791110">866</key><summary>Better support with _type is marked as not indexed, allowing to filter by type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-19T22:30:58Z</created><updated>2011-04-19T22:31:52Z</updated><resolved>2011-04-19T22:31:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/AbstractFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/AllFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/NumberFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/TypeFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentDocumentMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FilterBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/TermQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/TermsQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/TypeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/TypeFilterParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/xcontent/lucene/DoubleIndexingDocTest.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Better support with _type is marked as not indexed, allowing to filter by type, closes #866.</comment></comments></commit></commits></item><item><title>Query DSL: Add ids query and filter to fetch docs by ids (do not require _id field to be indexed)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/865</link><project id="" key="" /><description>Query DSL: Add ids query and filter to fetch docs by ids (do not require _id field to be indexed). This allows to not have the `_id` field indexed, but still be able to fetch docs by id. It is also faster compared to terms filter / query.

The query and filter format is:

```
{
    "ids" : {
        "type" : "my_type",
        "values" : ["1", "4", "10"]
    }
}
```
</description><key id="790847">865</key><summary>Query DSL: Add ids query and filter to fetch docs by ids (do not require _id field to be indexed)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-04-19T21:29:34Z</created><updated>2011-08-18T22:11:49Z</updated><resolved>2011-04-19T21:30:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aguereca" created="2011-04-20T22:53:57Z" id="1036613">After trying this new feature and reviewing the code I found that the mentioned format isn't correct, the format is:

```
{
    "ids" : 
        "type" : "my_type",
        "values" : ["1", "4", "10"]
    }
}
```
</comment><comment author="kimchy" created="2011-04-20T22:55:38Z" id="1036624">Opps, right!, fixed the sample to have the correct format.
</comment><comment author="till" created="2011-05-17T13:33:31Z" id="1190399">Is there a way to use `_all` for a type?

I mean, I realize that IDs are not always unique across all types, but what if they are?
</comment><comment author="kimchy" created="2011-05-17T13:51:24Z" id="1190499">No, there isn't an option to do that. Of course, you can always index it (not analyzed) yourself, or, enable indexing of the `_id` field.
</comment><comment author="dadoonet" created="2011-08-18T19:18:15Z" id="1842070">If there any way to make a prefix search on _id ?
I can not find a method for it in IdsQueryBuilder. My need is to find the number of documents which _id start with "xxxx".

Any idea ?

Thanks.
</comment><comment author="kimchy" created="2011-08-18T19:59:55Z" id="1842407">You can do that currently if you explicitly index the `_id` field, but, we can have an `ids_prefix` type filter / query that does not require it to be indexed, open an issue...
</comment><comment author="dadoonet" created="2011-08-18T22:11:49Z" id="1843538">Thanks. Opened #1259 for it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FilterBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/IdsQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/UidFilter.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Query DSL: Add ids query and filter to fetch docs by ids (do not require _id field to be indexed), closes #865.</comment></comments></commit></commits></item><item><title>Weird error message on syntax error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/864</link><project id="" key="" /><description>Hiya

When trying this request (which I know is incorrect) it returns a good error message to my client, but logs a weird error message in the logs:

```
# [Tue Apr 19 11:16:36 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/_aliases?pretty=1'  -d '
{
   "actions" : [
      {
         "add" : [
            "foo",
            "bar"
         ]
      }
   ]
}
'

# [Tue Apr 19 11:16:36 2011] Response:
# {
#    "status" : 500,
#    "error" : "ElasticSearchIllegalArgumentException[Alias action 
# &gt;    [add] requires an [index] to be set]"
# }
```

Weird error in logs:

```
[2011-04-19 11:16:36,498][WARN ][http.netty               ] [Dweller-in-Darkness] Caught exception while handling client http traffic, closing connection
java.lang.IllegalStateException: cannot send more responses than requests
    at org.elasticsearch.common.netty.handler.codec.http.HttpContentEncoder.writeRequested(HttpContentEncoder.java:102)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:266)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:568)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:563)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:611)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:578)
    at org.elasticsearch.common.netty.channel.AbstractChannel.write(AbstractChannel.java:259)
    at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:118)
    at org.elasticsearch.rest.action.admin.indices.alias.RestIndicesAliasesAction$1.onFailure(RestIndicesAliasesAction.java:131)
    at org.elasticsearch.action.support.BaseAction$ThreadedActionListener$2.run(BaseAction.java:95)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
```
</description><key id="787869">864</key><summary>Weird error message on syntax error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-19T09:18:48Z</created><updated>2011-04-19T09:23:40Z</updated><resolved>2011-04-19T09:23:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/ElasticSearchIllegalArgumentException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestIndicesAliasesAction.java</file></files><comments><comment>Weird error message on syntax error, closes #864.</comment></comments></commit></commits></item><item><title>Mapping: Allow to set index to `no` for _id and _type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/863</link><project id="" key="" /><description>Allow to not index both `_id` and `_type`. All operations will still work, except for search based on those fields (as they are no longer indexed).
</description><key id="780951">863</key><summary>Mapping: Allow to set index to `no` for _id and _type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-17T18:06:02Z</created><updated>2011-04-17T18:07:01Z</updated><resolved>2011-04-17T18:07:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/stress/SingleThreadBulkStress.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/IdFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/TypeFieldMapper.java</file></files><comments><comment>Mapping: Allow to set index to `no` for _id and _type, closes #863.</comment></comments></commit></commits></item><item><title>Versioning: Better handling of deletes - time based eviction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/862</link><project id="" key="" /><description>Evict deletes only after a specific time has passed, and not on flush. This creates better sync between a primary and a replica shard. The setting is `index.gc_deletes` and defaults to `60s`.
</description><key id="778303">862</key><summary>Versioning: Better handling of deletes - time based eviction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-16T14:26:01Z</created><updated>2011-04-16T14:26:43Z</updated><resolved>2011-04-16T14:26:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/index/engine/SimpleEngineBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/engine/robin/SimpleRobinEngineTests.java</file></files><comments><comment>Versioning: Better handling of deletes - time based eviction, closes #862.</comment></comments></commit></commits></item><item><title>Highlight doesn't work with _all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/861</link><project id="" key="" /><description>Hi,

Here is something that looks like a bug.

I push 2 documents into ES, than search for it with setting highlight property to _all.

It doesn't highlight anything as written here : http://www.elasticsearch.org/guide/reference/api/search/highlighting.html

You will find a gist curl recreation here : https://gist.github.com/922090

I'm using ES 0.15.2

Thanks for your help
Cheers
David.
</description><key id="775037">861</key><summary>Highlight doesn't work with _all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-04-15T17:43:03Z</created><updated>2011-04-19T10:40:14Z</updated><resolved>2011-04-19T10:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2011-04-15T17:59:26Z" id="1009201">Is your `_all` field set to be stored?

As a side note I think that `_all` field is not an ideal filed for highlighting.
</comment><comment author="dadoonet" created="2011-04-15T18:37:18Z" id="1009648">Hi Lukas,

Thanks for answering.

I don't understand this : Is your `_all` field set to be stored?

How do I do that ?

I thought that _all is a hardcoded field name that means that it works on every document fields.

Cheers.
</comment><comment author="lukas-vlcek" created="2011-04-15T23:56:11Z" id="1011386">Hi,
the `_all` field is not stored by default. For highlighting to work you need either the field to be stored or be present in `_source` data. However, you can change this in your mapping and have ES store it for you. Then you could use it for highlighting. But I would not recommend using it for highlighting, it is meant to allow searching across all your document fields, not for highlighting.
You can also control if individual fields are included in `_all` field by `include_in_all` settings, for more details see http://www.elasticsearch.org/guide/reference/mapping/all-field.html
Regards,
Lukas
</comment><comment author="dadoonet" created="2011-04-16T20:29:35Z" id="1014834">Hi,
I read in the guide that :

&gt; "The _all fields can be completely disabled. Explicit field mapping and object mapping can be excluded / included in the _all field. By default, it is enabled and all fields are included in it for ease of use."

I suppose that _all is enabled by default. I suppose that's the reason that I can make search using _all field.
If I understand what you said before, I can make search using the _all field but it's not store by default. So I can't highlight it without giving all the fields to highlight.

I think I completely misunderstood Apache Lucene Highlighter Javadoc and I was thinking that getBestFragments() try to find all matching fragments in every field.

So, should I close this "issue" ?

Many thanks.
David.
</comment><comment author="lukas-vlcek" created="2011-04-19T07:27:57Z" id="1025446">Hi,

yes, `_all` field is primarily used for searching. It can be set to `store` and then you could use it for highlighting, however, this would be more problematic then useful I think. I am not familiar with internal implementation of `_all` filed but I would guess that it is in fact a single Lucene field that somehow concatenates all other fields together which means that it does not give you any hint about where content of one field ends and content of the other field starts. In other words best matching fragments could then contain mix of several fields. May be this is not a problem in some situations but usually this would not be very practical IMHO.

I think you can go ahead and close this ticket, also I think that it would be better to discuss this topic on ML, I am not sure other can find it here easily.

Regards,
Lukas
</comment><comment author="lukas-vlcek" created="2011-04-19T07:45:33Z" id="1025506">BTW I added note about this into http://www.elasticsearch.org/guide/reference/mapping/all-field.html
</comment><comment author="dadoonet" created="2011-04-19T10:40:14Z" id="1026059">Thanks. It will be helpful for others.
So I close the "issue" ;-)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow plugins to provide additional settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/860</link><project id="" key="" /><description>I would like to be able to load shared settings from a centralized source (ZooKeeper in my case) using a plug-in. It looks like updatedSettings method of PluginsService was meant to allow that, but it wasn't completely implemented for some reason. Would this implementation be OK?

Thank you,

Igor
</description><key id="772175">860</key><summary>Allow plugins to provide additional settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2011-04-15T03:29:51Z</created><updated>2014-06-16T18:27:14Z</updated><resolved>2011-04-15T22:20:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-15T22:17:03Z" id="1011092">Looks good, I will just change additionalSettings to return Settings and not a Map&lt;String, String&gt;.
</comment><comment author="kimchy" created="2011-04-15T22:20:52Z" id="1011101">pushed, thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to delete a closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/859</link><project id="" key="" /><description>Allow to delete a closed index
</description><key id="770094">859</key><summary>Allow to delete a closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-14T17:51:02Z</created><updated>2011-04-14T18:16:07Z</updated><resolved>2011-04-14T18:16:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file></files><comments><comment>Allow to delete a closed index, closes #859.</comment></comments></commit></commits></item><item><title>Shard Allocation: Closed indices are not properly taken into account when rebalancing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/858</link><project id="" key="" /><description>Shard Allocation: Closed indices are not properly taken into account when rebalancing, those shards are counted to compute the average number of shards per node expected.
</description><key id="768644">858</key><summary>Shard Allocation: Closed indices are not properly taken into account when rebalancing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-14T11:52:22Z</created><updated>2011-04-14T12:01:04Z</updated><resolved>2011-04-14T12:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file></files><comments><comment>Shard Allocation: Closed indices are not properly taken into account when rebalancing, closes #858.</comment></comments></commit></commits></item><item><title>Query DSL: Allow to directly wrap a query with a constant_score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/857</link><project id="" key="" /><description>Allow to write a query with a constant_score query, not just a filter.
</description><key id="768510">857</key><summary>Query DSL: Allow to directly wrap a query with a constant_score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-14T11:18:58Z</created><updated>2011-04-14T11:19:36Z</updated><resolved>2011-04-14T11:19:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/ConstantScoreQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/query/QueryFacetCollector.java</file></files><comments><comment>Query DSL: Allow to directly wrap a query with a constant_score query, closes #857.</comment></comments></commit></commits></item><item><title>Query DSL: Geo Distance Range filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/856</link><project id="" key="" /><description>Geo Distance range filter:

```
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_distance_range" : {
                "from" : "200km",
                "to" : "400km"
                "pin.location" : {
                    "lat" : 40,
                    "lon" : -70
                }
            }
        }
    }
}
```

Supports the same point location parameter as the `geo_distance` filter. And also support the common parameters for range (lt, lte, gt, gte, from, to, include_upper and include_lower).
</description><key id="768498">856</key><summary>Query DSL: Geo Distance Range filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-04-14T11:13:07Z</created><updated>2011-04-14T17:20:31Z</updated><resolved>2011-04-14T17:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FilterBuilders.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/GeoDistanceRangeFilterBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/GeoDistanceRangeFilterParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/geo/GeoDistanceTests.java</file></files><comments><comment>Query DSL: Geo Distance Range filter, closes #856.</comment></comments></commit></commits></item><item><title>Unicast Discovery: Concurrently connect to nodes to improve cases where some listed nodes are not up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/855</link><project id="" key="" /><description>Its better to concurrently connect to nodes, so when some nodes are not up, so the connection timeout will not be aggregated based on the number of nodes that are not up.
</description><key id="766701">855</key><summary>Unicast Discovery: Concurrently connect to nodes to improve cases where some listed nodes are not up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-13T23:27:33Z</created><updated>2011-04-13T23:28:10Z</updated><resolved>2011-04-13T23:28:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Unicast Discovery: Concurrently connect to nodes to improve cases where some listed nodes are not up, closes #855.</comment></comments></commit></commits></item><item><title>EC2 Zen Discovery: Automatically use the configured transport port to ping other nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/854</link><project id="" key="" /><description>EC2 Zen Discovery: Automatically use the configured transport port to ping other nodes
</description><key id="766642">854</key><summary>EC2 Zen Discovery: Automatically use the configured transport port to ping other nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-13T23:12:52Z</created><updated>2011-04-13T23:13:39Z</updated><resolved>2011-04-13T23:13:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/AwsEc2UnicastHostsProvider.java</file><file>plugins/cloud/aws/src/main/java/org/elasticsearch/discovery/ec2/Ec2Discovery.java</file></files><comments><comment>EC2 Zen Discovery: Automatically use the configured transport port to ping other nodes, closes #854.</comment></comments></commit></commits></item><item><title>Local Gateway: Listed active shards on each node does apply index deletion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/853</link><project id="" key="" /><description>Local Gateway: Listed active shards on each node does apply index deletion
</description><key id="764035">853</key><summary>Local Gateway: Listed active shards on each node does apply index deletion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-13T13:10:26Z</created><updated>2011-04-13T13:15:39Z</updated><resolved>2011-04-13T13:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file></files><comments><comment>Local Gateway: Listed active shards on each node does apply index deletion, closes #853.</comment></comments></commit></commits></item><item><title>Bug: lexical parse error on special char ^ using QueryStringQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/852</link><project id="" key="" /><description>Hello,

in case the special char "^" is sent to ES within a query, following error occurs; Version is ES 0.15.2; OS X 10.6; JVM 1.6._24; 

Error:

org.elasticsearch.search.SearchParseException: [indexingindex_a][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:416)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:331)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:165)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:132)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:169)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: org.elasticsearch.index.query.QueryParsingException: [indexingindex_a] Failed to parse query [_le^a_ ]
    at org.elasticsearch.index.query.xcontent.QueryStringQueryParser.parse(QueryStringQueryParser.java:198)
    at org.elasticsearch.index.query.xcontent.QueryParseContext.parseInnerQuery(QueryParseContext.java:167)
    at org.elasticsearch.index.query.xcontent.XContentIndexQueryParser.parse(XContentIndexQueryParser.java:234)
    at org.elasticsearch.index.query.xcontent.XContentIndexQueryParser.parse(XContentIndexQueryParser.java:214)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:34)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:403)
    ... 10 more
Caused by: org.apache.lucene.queryParser.ParseException: Cannot parse '_le^a_ ': Lexical error at line 1, column 5.  Encountered: "a" (97), after : ""
    at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:192)
    at org.elasticsearch.index.query.xcontent.QueryStringQueryParser.parse(QueryStringQueryParser.java:192)
    ... 15 more
Caused by: org.apache.lucene.queryParser.TokenMgrError: Lexical error at line 1, column 5.  Encountered: "a" (97), after : ""
    at org.apache.lucene.queryParser.QueryParserTokenManager.getNextToken(QueryParserTokenManager.java:1229)
    at org.apache.lucene.queryParser.QueryParser.jj_consume_token(QueryParser.java:1623)
    at org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1330)
    at org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1250)
    at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:1178)
    at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:1167)
    at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:182)
    ... 16 more
</description><key id="763277">852</key><summary>Bug: lexical parse error on special char ^ using QueryStringQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kbachl</reporter><labels /><created>2011-04-13T08:35:58Z</created><updated>2011-04-13T18:21:08Z</updated><resolved>2011-04-13T10:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-04-13T10:45:07Z" id="994699">kbachi that's because ^ is a reserved character. it is used to "boost" a word/group/phrase in a query string.

See: http://lucene.apache.org/java/3_0_0/queryparsersyntax.html

You need to filter the text you pass to query string before sending it to elasticsearch
</comment><comment author="kbachl" created="2011-04-13T18:21:08Z" id="996947">ok, understand, but shouldn't it at least then fail savely instead of throwing this error out in the ES logs?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Postgres River</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/851</link><project id="" key="" /><description>I'd love to see a river for Postgres. I'd even love to implement it you could point me to some examples or documentation on what to do.
</description><key id="762745">851</key><summary>Postgres River</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benweatherman</reporter><labels /><created>2011-04-13T04:31:53Z</created><updated>2013-03-22T07:21:54Z</updated><resolved>2013-03-22T07:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-13T08:11:30Z" id="994157">A river that does what with postgres? Automatically index data that exists in postgres?

In general, rivers are just Java programs running within elasticsearch. You can write a standalone one that does what you want using the elasticsearch Java API and Node Client, and then, moving it to be a river is simple.
</comment><comment author="dadoonet" created="2011-04-19T10:43:26Z" id="1026074">I'm thinking of writing a river for hibernate. Perhaps, it could be helpful for that need...
I will post something in the mailing list if I start to write it...
</comment><comment author="dadoonet" created="2013-03-22T07:21:54Z" id="15284153">Have a look at: https://github.com/jprante/elasticsearch-river-jdbc
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>River: Failure to reallocate river to another node on rivers node failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/850</link><project id="" key="" /><description>When a node that a river is running on fails, the river is not allocated to a different node.
</description><key id="760758">850</key><summary>River: Failure to reallocate river to another node on rivers node failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-12T18:51:24Z</created><updated>2011-04-12T18:52:15Z</updated><resolved>2011-04-12T18:52:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file></files><comments><comment>River: Failure to reallocate river to another node on rivers node failure, closes #850.</comment></comments></commit></commits></item><item><title>unable to create index w/ mappings and custom analyzers via REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/849</link><project id="" key="" /><description>creating an index and including a mapping along with analyzer config causes the the analyzers to be dropped.
https://gist.github.com/914440

works just fine if you just include the analyzers.
</description><key id="756622">849</key><summary>unable to create index w/ mappings and custom analyzers via REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wireframe</reporter><labels /><created>2011-04-11T21:51:14Z</created><updated>2011-04-11T22:09:22Z</updated><resolved>2011-04-11T22:09:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wireframe" created="2011-04-11T22:09:22Z" id="986429">working as designed.  needed to use :settings + :mappings keys
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better handling when a shard hits OOM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/848</link><project id="" key="" /><description>Better handling when a shard hits OOM, will now cause that shard to fail and be reallocated, and proper handling to Lucene IndexWriter in this case.
</description><key id="755397">848</key><summary>Better handling when a shard hits OOM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-11T18:09:16Z</created><updated>2011-04-11T18:11:13Z</updated><resolved>2011-04-11T18:11:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/EngineClosedException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/IllegalIndexShardStateException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/IndexShardClosedException.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Better handling when a shard hits OOM, closes #848.</comment></comments></commit></commits></item><item><title>Search that works with curl and SearchPhaseExecutionException when playing with Java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/847</link><project id="" key="" /><description>Issue described in the mailing list : http://elasticsearch-users.115913.n3.nabble.com/Search-that-works-with-curl-but-throw-SearchPhaseExecutionException-when-using-Java-tp2762593p2762593.html

Pushing document in a clean ES index :
    curl -XPUT http://localhost:9200/test/dossier/1 -d '{"conformite":"CONFORME", "direction":"DIRECTION", "domaine":{"reglementationsConcernees":["REG"], "supportsDeclaratifsForES":[{"SupportDeclaratif":{"specificite":{"SpecifSDec":{"regime":"1234"}}, "typesVerifications":["DOC"], "controlesPhysiques":[{"ControleMarchandise":{"conclusions":[{"Conclusion":{"reconnu":"1", "typePrecision":"VIDE", "conclusionCtrl":"CONFORME", "declare":"1", "typeAttributMarchandise":"NOMENCLATURE"}}, {"Conclusion":{"reconnu":"586.0", "typePrecision":"VIDE", "conclusionCtrl":"CONFORME", "declare":"586.0", "typeAttributMarchandise":"MASSE_NETTE"}}]}}], "bureauDouane":{"ServiceAdministrationDouane":{"europa":"FR00000", "nom":"Bureau"}}, "conclusion":"null", "infractions":[], "date":"2011-04-01T08:00:00.000+0000", "reference":"11111", "controlesDocumentaires":[{"ControleDocument":{"categorieLibelle":"Libelle", "typePrecision":"null", "categorieCode":"9999", "typeConclusionControle":"CONFORME"}}, {"ControleDocument":{"categorieLibelle":"Facture", "typePrecision":"null", "categorieCode":"8888", "typeConclusionControle":"CONFORME"}}]}}]}, "type":"DOUANE", "sources":[{"type":"CRIT"}], "dateDebut":"2011-04-01T10:00:00.000+0000", "etat":"VALIDE", "agentsRoles":[{"personne":{"Agent":{"nom":"MOI", "personnesIdentifiantsForES":[{"identifiant":"login"}]}}, "typesRoles":["REDACTEUR"]}], "lieuControle":"Bureau", "service":{"ServiceAdministrationDouane":{"europa":"FR00000", "nom":"Bureau"}}, "personnesControlees":[{"personne":{"PersonneMorale":{"nom":"MYCOMP", "personnesIdentifiantsForES":[{"identifiant":"MYEORI"}, {"identifiant":"MYSIRET"}]}}, "role":"DECLARANT"}], "identifiant":"66566767"}'

Works.

Search it with curl :
    curl -XGET "http://localhost:9200/test/dossier/_search" -d '{"query":{"term":{"dossier.domaine.supportsDeclaratifsForES.SupportDeclaratif.reference":"11111"}}}'

Works.

Do it with Java :
    SearchResponse response = client.prepareSearch("test")
          .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
          .setQuery(termQuery("dossier.domaine.supportsDeclaratifsForES.SupportDeclaratif.reference", "11111"))
          .setExplain(true)
          .execute()
          .actionGet();

Fails with a SearchPhaseExecutionException.
    Failed :
    org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], total failure; shardFailures {[1IOMpS32QuSLMWkQkGG4iQ][test][0]: RemoteTransportException[[Lord Dark Wind][inet[/10.115.1.111:9300]][search/phase/dfs]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: RuntimeException[Internal error: this code path should never get executed]; }{[1IOMpS32QuSLMWkQkGG4iQ][test][2]: RemoteTransportException[[Lord Dark Wind][inet[/10.115.1.111:9300]][search/phase/dfs]]; nested: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: RuntimeException[Internal error: this code path should never get executed]; }{[1IOMpS32QuSLMWkQkGG4iQ][test][3]: RemoteTransportException[[Lord Dark Wind][inet[/10.115.1.111:9300]][search/phase/dfs]]; nested: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: RuntimeException[Internal error: this code path should never get executed]; }{[1IOMpS32QuSLMWkQkGG4iQ][test][1]: RemoteTransportException[[Lord Dark Wind][inet[/10.115.1.111:9300]][search/phase/dfs]]; nested: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: RuntimeException[Internal error: this code path should never get executed]; }{[1IOMpS32QuSLMWkQkGG4iQ][test][4]: RemoteTransportException[[Lord Dark Wind][inet[/10.115.1.111:9300]][search/phase/dfs]]; nested: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: RuntimeException[Internal error: this code path should never get executed]; }
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:248)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$400(TransportSearchTypeAction.java:75)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:198)
        at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleException(SearchServiceTransportAction.java:119)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:158)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:149)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:100)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:302)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:317)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:299)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:214)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

I would like to attach a maven project to reproduce the bug but I can't find a way to do it in github issues...
</description><key id="752869">847</key><summary>Search that works with curl and SearchPhaseExecutionException when playing with Java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2011-04-11T07:55:18Z</created><updated>2013-02-25T09:58:27Z</updated><resolved>2011-05-13T19:52:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-11T07:58:12Z" id="982531">There is no need for a sample project, I responded in the mailing list that its a bug in Jackson SMILE format. You can use Json for now. Here is the issue in jackson: http://jira.codehaus.org/browse/JACKSON-552, it has been fixed but no `1.7.6` yet.
</comment><comment author="dadoonet" created="2011-04-11T10:42:10Z" id="982995">Sorry. I didn't understood if you were waiting something from me or not to help the Jackson team to fix the bug.
I need to work on my english a bit more !

Thanks for the Jackson issue and for the hack...

Cheers.
</comment><comment author="kimchy" created="2011-04-11T12:01:45Z" id="983288">No problem :), will keep this issue open till the new jackson version kicks in.
</comment><comment author="mkpandey" created="2013-02-25T09:53:03Z" id="14033041">org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], total failure

This my code 
AndFilterBuilder queryFilters = FilterBuilders.andFilter();
            queryFilters.add(FilterBuilders.prefixFilter("Education", getZeducation()));
            queryFilters.add(FilterBuilders.prefixFilter("ResumeHeadline", getZresumeHeadline()));
            FilterBuilder aggFilter = FilterBuilders.andFilter(queryFilters);*/
            .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                                            .setQuery(QueryBuilders.prefixQuery("Name",getZname()))
                                            .setFilter(aggFilter)

```
                                        .setFrom(10)
                                        .setSize(60)
                                        .setExplain(true)
                                        .execute()
                                        .actionGet();
```

Error: in log file:
 org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], total failure; shardFailures {[W17rw9RHRyeEZGi-aBVlmQ][res13_index][3]: SearchParseException[[res13_index][3]: query[Name:null_],from[10],size[60]: Parse Failure [Failed to parse source [{"from":10,"size":60,"query":{"prefix":{"Name":null}},"filter":{"and":{"filters":[{"and":{"filters":[{"prefix":{"Education":null}},{"prefix":{"ResumeHeadline":null}}]}}]}},"explain":true}]]]; nested: QueryParsingException[[res13_index] No value specified for prefix filter]; }{[W17rw9RHRyeEZGi-aBVlmQ][res13_index][2]: SearchParseException[[res13_index][2]: query[Name:null_],from[10],size[60]: Parse Failure [Failed to parse source [{"from":10,"size":60,"query":{"prefix":{"Name":null}},"filter":{"and":{"filters":[{"and":{"filters":[{"prefix":{"Education":null}},{"prefix":{"ResumeHeadline":null}}]}}]}},"explain":true}]]]; nested: QueryParsingException[[res13_index] No value specified for prefix filter]; }{[W17rw9RHRyeEZGi-aBVlmQ][res13_index][4]: SearchParseException[[res13_index][4]: query[Name:null*],from[10],size[60]: Parse Failure [Failed to parse source [{"from":10,"size":6.............
</comment><comment author="dadoonet" created="2013-02-25T09:58:27Z" id="14033256">Please use the mailing list before submitting issues or adding comments in github.

That said, here is what you can see from your generated query:

``` javascript
{"from":10,"size":60,
"query":
{"prefix":{"Name":null}},"filter":{"and":{"filters":[{"and":{"filters":[{"prefix":{"Education":null}},
{"prefix":{"ResumeHeadline":null}}]}}]}},"explain":true}
```

You can not do a prefixQuery with no value (null). Check your java code: getZeducation() and getZresumeHeadline() return null values.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Optimize API: Don't execute concurrent optimize operations (shard level) on the same node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/846</link><project id="" key="" /><description>Optimize API: Don't execute concurrent optimize operations (shard level) on the same node in order to reduce the IO overload of a single optimize request.
</description><key id="751292">846</key><summary>Optimize API: Don't execute concurrent optimize operations (shard level) on the same node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-10T20:11:42Z</created><updated>2011-04-10T20:12:13Z</updated><resolved>2011-04-10T20:12:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file></files><comments><comment>Optimize API: Don't execute concurrent optimize operations (shard level) on the same node, closes #846.</comment></comments></commit></commits></item><item><title>Better handling of shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/845</link><project id="" key="" /><description>Better handling of failed shards either through a result of failed allocation / reallocation, or because of consistent failed operations.
</description><key id="751031">845</key><summary>Better handling of shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-10T18:57:41Z</created><updated>2011-04-10T19:00:19Z</updated><resolved>2011-04-10T19:00:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/FailedRerouteAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/NodeAllocations.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ShardsAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/blobstore/BlobReuseExistingNodeAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/gateway/local/LocalGatewayNodeAllocation.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file></files><comments><comment>Better handling of shard failures, closes #845.</comment></comments></commit></commits></item><item><title>Field boost in mapping ignored by wildcard query string searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/844</link><project id="" key="" /><description>A query string search like `"config*` should take the field boost (as defined in the mapping) into account.  At the moment it doesn't:
## PREPARE TEST DOCS:

```
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "mappings" : {
      "bar" : {
         "properties" : {
            "body" : {
               "type" : "string"
            },
            "title" : {
               "boost" : 2,
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "body" : "Stuff about how to setup something",
   "title" : "Configure"
}
'

curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "body" : "Stuff about how to configure something",
   "title" : "Setup"
}
'
```

SEARCH _all FOR "configure" - boost taken into account

```
curl -XGET 'http://127.0.0.1:9200/foo/bar/_search?pretty=1'  -d '
{
   "query" : {
      "field" : {
         "_all" : "configure"
      }
   },
   "explain" : 1
}
'

# [Sat Apr  9 12:52:16 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "body" : "Stuff about how to setup something",
#                "title" : "Configure"
#             },
#             "_score" : 0.16273327,
#             "_index" : "foo",
#             "_id" : "KvYfDYbvRkS3ag6Xi5RkWw",
#             "_type" : "bar",
#             "_explanation" : {
#                "value" : 0.16273327,
#                "details" : [
#                   {
#                      "value" : 1.4142135,
#                      "details" : [
#                         {
#                            "value" : 0.70710677,
#                            "description" : "tf(phraseFreq=0.5)"
#                         },
#                         {
#                            "value" : 2,
#                            "description" : "allPayload(...)"
#                         }
#                      ],
#                      "description" : "btq, product of:"
#                   },
#                   {
#                      "value" : 0.30685282,
#                      "description" : "idf(_all:  configure=1)"
#                   },
#                   {
#                      "value" : 0.375,
#                      "description" : "fieldNorm(field=_all, doc=0
# &gt;                     )"
#                   }
#                ],
#                "description" : "fieldWeight(_all:configure in 0),
# &gt;                 product of:"
#             }
#          },
#          {
#             "_source" : {
#                "body" : "Stuff about how to configure something",
#                "title" : "Setup"
#             },
#             "_score" : 0.081366636,
#             "_index" : "foo",
#             "_id" : "CtYiBKZ_T4y0O4DsfXHSzA",
#             "_type" : "bar",
#             "_explanation" : {
#                "value" : 0.081366636,
#                "details" : [
#                   {
#                      "value" : 0.70710677,
#                      "details" : [
#                         {
#                            "value" : 0.70710677,
#                            "description" : "tf(phraseFreq=0.5)"
#                         },
#                         {
#                            "value" : 1,
#                            "description" : "allPayload(...)"
#                         }
#                      ],
#                      "description" : "btq, product of:"
#                   },
#                   {
#                      "value" : 0.30685282,
#                      "description" : "idf(_all:  configure=1)"
#                   },
#                   {
#                      "value" : 0.375,
#                      "description" : "fieldNorm(field=_all, doc=0
# &gt;                     )"
#                   }
#                ],
#                "description" : "fieldWeight(_all:configure in 0),
# &gt;                 product of:"
#             }
#          }
#       ],
#       "max_score" : 0.16273327,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```

SEARCH _all FOR "config*" - boost ignored

```
# [Sat Apr  9 12:52:25 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/foo/bar/_search?pretty=1'  -d '
{
   "query" : {
      "field" : {
         "_all" : "config*"
      }
   },
   "explain" : 1
}
'

# [Sat Apr  9 12:52:25 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "body" : "Stuff about how to setup something",
#                "title" : "Configure"
#             },
#             "_score" : 1,
#             "_index" : "foo",
#             "_id" : "KvYfDYbvRkS3ag6Xi5RkWw",
#             "_type" : "bar",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "boost"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "queryNorm"
#                   }
#                ],
#                "description" : "ConstantScoreQuery(_all:config*),
# &gt;                 product of:"
#             }
#          },
#          {
#             "_source" : {
#                "body" : "Stuff about how to configure something",
#                "title" : "Setup"
#             },
#             "_score" : 1,
#             "_index" : "foo",
#             "_id" : "CtYiBKZ_T4y0O4DsfXHSzA",
#             "_type" : "bar",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "boost"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "queryNorm"
#                   }
#                ],
#                "description" : "ConstantScoreQuery(_all:config*),
# &gt;                 product of:"
#             }
#          }
#       ],
#       "max_score" : 1,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```

`dis_max` search for "config*" - boost ignored

```
# [Sat Apr  9 12:52:32 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/foo/bar/_search?pretty=1'  -d '
{
   "query" : {
      "dis_max" : {
         "queries" : [
            {
               "field" : {
                  "_all" : "config*"
               }
            },
            {
               "field" : {
                  "_title" : "config*"
               }
            }
         ]
      }
   },
   "explain" : 1
}
'

# [Sat Apr  9 12:52:32 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "body" : "Stuff about how to setup something",
#                "title" : "Configure"
#             },
#             "_score" : 1,
#             "_index" : "foo",
#             "_id" : "KvYfDYbvRkS3ag6Xi5RkWw",
#             "_type" : "bar",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "details" : [
#                         {
#                            "value" : 1,
#                            "description" : "boost"
#                         },
#                         {
#                            "value" : 1,
#                            "description" : "queryNorm"
#                         }
#                      ],
#                      "description" : "ConstantScoreQuery(_all:con
# &gt;                     fig*), product of:"
#                   }
#                ],
#                "description" : "max of:"
#             }
#          },
#          {
#             "_source" : {
#                "body" : "Stuff about how to configure something",
#                "title" : "Setup"
#             },
#             "_score" : 1,
#             "_index" : "foo",
#             "_id" : "CtYiBKZ_T4y0O4DsfXHSzA",
#             "_type" : "bar",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "details" : [
#                         {
#                            "value" : 1,
#                            "description" : "boost"
#                         },
#                         {
#                            "value" : 1,
#                            "description" : "queryNorm"
#                         }
#                      ],
#                      "description" : "ConstantScoreQuery(_all:con
# &gt;                     fig*), product of:"
#                   }
#                ],
#                "description" : "max of:"
#             }
#          }
#       ],
#       "max_score" : 1,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```
</description><key id="747472">844</key><summary>Field boost in mapping ignored by wildcard query string searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-04-09T10:57:34Z</created><updated>2012-08-19T06:14:18Z</updated><resolved>2012-08-04T11:33:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gustavobmaia" created="2011-07-29T16:54:53Z" id="1682810">hello,
I'm having the same problem. 
I hope to publish it in the next version. 
This is very important because I am doing a search for similar topics in Quora - http://www.quora.com, so it's important to boost the prevailing topics.
Making a more detailed analysis of the code elasticsearch realized that the class defining 
this WildcardQueryParser query.setRewriteMethod (MultiTermQuery.CONSTANT_SCORE_AUTO_REWRITE_DEFAULT),
 we have to change to query.setRewriteMethod (WildcardQuery.SCORING_BOOLEAN_QUERY_REWRITE). 
this option is native lucene, which kimchy had to do was have a configuration option which defined this new type of rewrite. 
Observation is that switching to SCORING_BOOLEAN_QUERY_REWRITE makes the search slower.
</comment><comment author="gustavobmaia" created="2011-07-29T22:18:06Z" id="1685792">It is resolved.

see #1186
</comment><comment author="dylanahsmith" created="2012-08-18T05:29:19Z" id="7841953">I am currently experiencing this issue in 0.19.8, and reproduced the same behaviour as originally posted using the provided curl requests.

I think this issue should be reopened.
</comment><comment author="clintongormley" created="2012-08-18T10:09:41Z" id="7843370">@dylanahsmith are you using the `rewrite` parameter? 

http://www.elasticsearch.org/guide/reference/query-dsl/multi-term-rewrite.html
</comment><comment author="dylanahsmith" created="2012-08-18T14:58:31Z" id="7845144">Oh I see, so the default is to ignore scores because of performance reasons, so the `rewrite` parameter is needed to override that behaviour.
</comment><comment author="dylanahsmith" created="2012-08-19T06:14:18Z" id="7850854">``` sh
curl -XGET 'http://localhost:9200/foo/bar/_search?pretty=true' -d '{
   "query" : {
      "field" : {
         "_all": {
           "query": "configure*",
           "rewrite": "top_terms_10"
         }
      }
   },
}'
```

[Full gist](https://gist.github.com/3392534)

Modified the query to use `"rewrite": "top_terms_10"`, and it still ignores the field boost.  The only difference is each hit has the _score 0.11506981 instead of 1.0.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JMX: Don't use node id and name as part of the jmx registration, use static org.elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/843</link><project id="" key="" /><description>Just so it will be simpler to monitor it. Was thinking of making each node a JMX aggregator of other nodes, but its not going to happen anytime soon.
</description><key id="745703">843</key><summary>JMX: Don't use node id and name as part of the jmx registration, use static org.elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.16.0</label></labels><created>2011-04-08T15:49:17Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-08T15:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/jmx/JmxService.java</file></files><comments><comment>JMX: Don't use node id and name as part of the jmx registration, use static org.elasticsearch, closes #843.</comment></comments></commit></commits></item><item><title>Start Elastic Node without network link</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/842</link><project id="" key="" /><description>When I try to launch an ES instance from scratch for test purpose without any network link up, it fails.

```
[2011-04-08 08:54:06,866][INFO ][node                     ] [White Fang] {elasticsearch/0.15.2}[1991]: initializing ...
[2011-04-08 08:54:06,877][INFO ][plugins                  ] [White Fang] loaded []
[2011-04-08 08:54:11,336][INFO ][node                     ] [White Fang] {elasticsearch/0.15.2}[1991]: initialized
[2011-04-08 08:54:11,337][INFO ][node                     ] [White Fang] {elasticsearch/0.15.2}[1991]: starting ...
[2011-04-08 08:54:11,559][INFO ][transport                ] [White Fang] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[dpilato-laptop/127.0.1.1:9300]}
[2011-04-08 08:54:11,614][ERROR][bootstrap                ] [White Fang] {elasticsearch/0.15.2}: Startup Failed ...
- DiscoveryException[Failed to setup multicast socket]
    SocketException[bad argument for IP_MULTICAST_IF: address not bound to any interface]
```

I have to connect at least one network card.

It would be nice if ES can start without a network (standalone mode) with the default parameters.

By the way, is there a network configuration to start ES without network signal ?
</description><key id="744131">842</key><summary>Start Elastic Node without network link</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.18.0</label></labels><created>2011-04-08T07:02:07Z</created><updated>2011-08-17T07:33:42Z</updated><resolved>2011-08-16T23:55:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-09T10:50:01Z" id="977086">What do you get when you set monitor.network to TRACE in the logging file?
</comment><comment author="dadoonet" created="2011-04-11T07:17:11Z" id="982433">Sure !

```
[2011-04-11 09:13:50,789][INFO ][node                     ] [Richards, Susan] {elasticsearch/0.15.2}[2009]: initializing ...
[2011-04-11 09:13:50,800][INFO ][plugins                  ] [Richards, Susan] loaded []
[2011-04-11 09:13:55,019][DEBUG][monitor.network          ] [Richards, Susan] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@1a1bc40] with refresh_interval [5s]
[2011-04-11 09:13:55,059][DEBUG][monitor.network          ] [Richards, Susan] net_info
host [dpilato-laptop]
lo  display_name [lo]
        address [/0:0:0:0:0:0:0:1%1] [/127.0.0.1] 
        mtu [16436] multicast [false] ptp [false] loopback [true] up [true] virtual [false]

[2011-04-11 09:13:55,067][TRACE][monitor.network          ] [Richards, Susan] ifconfig

lo  Link encap:Local Loopback
    inet addr:127.0.0.1  Mask:255.0.0.0
    UP LOOPBACK RUNNING  MTU:16436  Metric:1
    RX packets:280 errors:0 dropped:0 overruns:0 frame:0
    TX packets:280 errors:0 dropped:0 overruns:0 carrier:0
    collisions:0
    RX bytes:35713 ( 35K)  TX bytes:35713 ( 35K)
eth0    Link encap:Ethernet HWaddr 00:0A:E4:F7:B0:41
    inet addr:0.0.0.0  Bcast:0.0.0.0  Mask:0.0.0.0
    UP BROADCAST MULTICAST  MTU:1500  Metric:1
    RX packets:0 errors:0 dropped:0 overruns:0 frame:0
    TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
    collisions:0
    RX bytes:0 (  0 )  TX bytes:0 (  0 )
wlan0   Link encap:Ethernet HWaddr 00:13:02:09:10:B1
    inet addr:0.0.0.0  Bcast:0.0.0.0  Mask:0.0.0.0
    UP BROADCAST MULTICAST  MTU:1500  Metric:1
    RX packets:0 errors:0 dropped:0 overruns:0 frame:0
    TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
    collisions:0
    RX bytes:0 (  0 )  TX bytes:0 (  0 )

[2011-04-11 09:13:55,292][INFO ][node                     ] [Richards, Susan] {elasticsearch/0.15.2}[2009]: initialized
[2011-04-11 09:13:55,292][INFO ][node                     ] [Richards, Susan] {elasticsearch/0.15.2}[2009]: starting ...
[2011-04-11 09:13:55,511][INFO ][transport                ] [Richards, Susan] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[dpilato-laptop/127.0.1.1:9300]}
[2011-04-11 09:13:55,575][ERROR][bootstrap                ] [Richards, Susan] {elasticsearch/0.15.2}: [31mStartup Failed ...[0m
- DiscoveryException[Failed to setup multicast socket]
    SocketException[bad argument for IP_MULTICAST_IF: address not bound to any interface]
[2011-04-11 09:13:55,576][INFO ][node                     ] [Richards, Susan] {elasticsearch/0.15.2}[2009]: stopping ...
[2011-04-11 09:13:55,593][INFO ][node                     ] [Richards, Susan] {elasticsearch/0.15.2}[2009]: stopped
[2011-04-11 09:13:55,593][INFO ][node                     ] [Richards, Susan] {elasticsearch/0.15.2}[2009]: closing ...
[2011-04-11 09:13:55,616][INFO ][node                     ] [Richards, Susan] {elasticsearch/0.15.2}[2009]: closed
```

Cheers...
</comment><comment author="dadoonet" created="2011-08-10T05:46:49Z" id="1770046">BTW, it only fails on my Linux (Ubuntu). On Windows, it starts perfectly with or without the Network link.
May be an OS issue ?
As it's a really small issue without importance (you never use ES without a Network, so spending time on it is not useful), I suggest to close the issue as a WontBeFix issue.

What do you think ?
</comment><comment author="kimchy" created="2011-08-10T09:51:12Z" id="1771280">I think that it should be fixed for better OOB usage. I think it fails on the multicast discovery, we can warn then and still continue... .  I need to get the full stack trace for the exception, can you check the log file itself (not the console output) with TRACE logging enabled? You should see the exception there...
</comment><comment author="gsf" created="2011-08-16T16:55:54Z" id="1818170">I ran into the same issue. Log output with monitor.network set to TRACE:

```
[2011-08-16 12:41:48,209][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `
java` with the server vm for best performance by adding `-server` to the command line
[2011-08-16 12:41:48,246][INFO ][node                     ] [Neurotap] {elasticsearch/0.17.5}[2444]: i
nitializing ...
[2011-08-16 12:41:48,269][INFO ][plugins                  ] [Neurotap] loaded [], sites []
[2011-08-16 12:41:52,765][DEBUG][monitor.network          ] [Neurotap] Using probe [org.elasticsearch.
monitor.network.SigarNetworkProbe@1e3cc77] with refresh_interval [5s]
[2011-08-16 12:41:52,783][DEBUG][monitor.network          ] [Neurotap] net_info
host [lizard]
lo      display_name [lo]
                address [/0:0:0:0:0:0:0:1%1] [/127.0.0.1]
                mtu [16436] multicast [false] ptp [false] loopback [true] up [true] virtual [false]

[2011-08-16 12:41:52,791][TRACE][monitor.network          ] [Neurotap] ifconfig

lo      Link encap:Local Loopback
        inet addr:127.0.0.1  Mask:255.0.0.0
        UP LOOPBACK RUNNING  MTU:16436  Metric:1
        RX packets:182 errors:0 dropped:0 overruns:0 frame:0
        TX packets:182 errors:0 dropped:0 overruns:0 carrier:0
        collisions:0
        RX bytes:13984 ( 14K)  TX bytes:13984 ( 14K)
eth0    Link encap:Ethernet HWaddr 00:23:8B:B6:14:B8
        inet addr:0.0.0.0  Bcast:0.0.0.0  Mask:0.0.0.0
        UP BROADCAST MULTICAST  MTU:1500  Metric:1
        RX packets:0 errors:0 dropped:0 overruns:0 frame:0
        TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
        collisions:0
        RX bytes:0 (  0 )  TX bytes:0 (  0 )
eth1    Link encap:Ethernet HWaddr 00:24:2C:E5:57:EB
        inet addr:0.0.0.0  Bcast:0.0.0.0  Mask:0.0.0.0
        BROADCAST MULTICAST  MTU:1500  Metric:1
        RX packets:0 errors:0 dropped:0 overruns:0 frame:0
        TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
        collisions:0
        RX bytes:0 (  0 )  TX bytes:0 (  0 )
vboxnet0        Link encap:Ethernet HWaddr 0A:00:27:00:00:00
        inet addr:0.0.0.0  Bcast:0.0.0.0  Mask:0.0.0.0
        BROADCAST MULTICAST  MTU:1500  Metric:1
        RX packets:0 errors:0 dropped:0 overruns:0 frame:0
        TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
        collisions:0
        RX bytes:0 (  0 )  TX bytes:0 (  0 )

[2011-08-16 12:41:53,490][INFO ][node                     ] [Neurotap] {elasticsearch/0.17.5}[2444]: initialized
[2011-08-16 12:41:53,491][INFO ][node                     ] [Neurotap] {elasticsearch/0.17.5}[2444]: starting ...
[2011-08-16 12:41:53,720][INFO ][transport                ] [Neurotap] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[lizard/127.0.1.1:9300]}
[2011-08-16 12:41:53,772][ERROR][bootstrap                ] [Neurotap] {elasticsearch/0.17.5}: ESC[31mStartup Failed ...ESC[0m
- DiscoveryException[Failed to setup multicast socket]
        SocketException[bad argument for IP_MULTICAST_IF: address not bound to any interface]
[2011-08-16 12:41:53,775][INFO ][node                     ] [Neurotap] {elasticsearch/0.17.5}[2444]: stopping ...
[2011-08-16 12:41:53,812][INFO ][node                     ] [Neurotap] {elasticsearch/0.17.5}[2444]: stopped
[2011-08-16 12:41:53,813][INFO ][node                     ] [Neurotap] {elasticsearch/0.17.5}[2444]: closing ...
[2011-08-16 12:41:53,846][INFO ][node                     ] [Neurotap] {elasticsearch/0.17.5}[2444]: closed
```

This is from the log file, but looks quite a bit like dadoonet's paste above... should something else be set to TRACE as well?
</comment><comment author="kimchy" created="2011-08-16T17:49:17Z" id="1818591">@gsf can you set `bootstrap` to `DEBUG` in the logging and check the log file for the full exception failure and paste it here?
</comment><comment author="gsf" created="2011-08-16T18:33:41Z" id="1818995">Gotcha. The relevant stack trace is as follows:

```
[2011-08-16 14:29:46,858][ERROR][bootstrap                ] [Recorder] {elasticsearch/0.17.5}: [31mStartup Failed ...[0m
- DiscoveryException[Failed to setup multicast socket]
    SocketException[bad argument for IP_MULTICAST_IF: address not bound to any interface]
[2011-08-16 14:29:46,860][DEBUG][bootstrap                ] [Recorder] Exception
org.elasticsearch.discovery.DiscoveryException: Failed to setup multicast socket
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.doStart(MulticastZenPing.java:174)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:80)
    at org.elasticsearch.discovery.zen.ping.ZenPingService.doStart(ZenPingService.java:92)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:80)
    at org.elasticsearch.discovery.zen.ZenDiscovery.doStart(ZenDiscovery.java:146)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:80)
    at org.elasticsearch.discovery.DiscoveryService.doStart(DiscoveryService.java:59)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:80)
    at org.elasticsearch.node.internal.InternalNode.start(InternalNode.java:184)
    at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:121)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:191)
    at org.elasticsearch.bootstrap.ElasticSearch.main(ElasticSearch.java:28)
Caused by: java.net.SocketException: bad argument for IP_MULTICAST_IF: address not bound to any interface
    at java.net.PlainDatagramSocketImpl.socketSetOption(Native Method)
    at java.net.AbstractPlainDatagramSocketImpl.setOption(AbstractPlainDatagramSocketImpl.java:299)
    at java.net.MulticastSocket.setInterface(MulticastSocket.java:448)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.doStart(MulticastZenPing.java:165)
    ... 11 more
[2011-08-16 14:29:46,869][INFO ][node                     ] [Recorder] {elasticsearch/0.17.5}[2361]: stopping ...
```
</comment><comment author="kimchy" created="2011-08-16T23:49:45Z" id="1821762">@gsf: cool, thanks!. Will push an enhancement so it will warn in this case that multicast discovery is disable, but still startup.
</comment><comment author="dadoonet" created="2011-08-17T07:33:42Z" id="1824119">I'm coming too late to give the trace ! Shame on me :-(
Thanks @gsf to have provided it. Thanks @kimchy to close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file></files><comments><comment>Start Elastic Node without network link, closes #842.</comment></comments></commit></commits></item><item><title>Network Settings: Allow to explicitly set ipv4 and ipv4 when using _networkInterface_ notation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/841</link><project id="" key="" /><description>For example: `_en0:ipv4` and `en0:ipv6`.
</description><key id="741874">841</key><summary>Network Settings: Allow to explicitly set ipv4 and ipv4 when using _networkInterface_ notation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-07T17:01:01Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-07T17:02:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-07T17:02:03Z" id="969823">Network Settings: Allow to explicitly set ipv4 and ipv4 when using _networkInterface_ notation, closed by ca01dc7a09cfebc9389a0b1e86917fac80ad692c.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/network/NetworkService.java</file></files><comments><comment>Network Settings: Allow to explicitly set ipv4 and ipv4 when using _networkInterface_ notation, closes #841.</comment></comments></commit></commits></item><item><title>Fix uncaught error when specifying a custom analyzer with no tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/840</link><project id="" key="" /><description>This returns a loooooooooooong error message:

```
curl -XPUT 'http://127.0.0.1:9200/my_listings/?pretty=1'  -d '
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "myPhoneticFilter" : {
               "replace" : 0,
               "encoder" : "metaphone",
               "type" : "phonetic"
            }
         },
         "analyzer" : {
            "myAnalyzer" : {
               "filter" : [
                  "standard",
                  "lowercase",
                  "stop",
                  "myPhoneticFilter"
               ],
               "type" : "custom"
            }
         }
      }
   }
}
'
```
</description><key id="741691">840</key><summary>Fix uncaught error when specifying a custom analyzer with no tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-04-07T15:43:31Z</created><updated>2012-08-04T11:35:06Z</updated><resolved>2012-08-04T11:35:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-08-04T11:35:06Z" id="7501151">This has been fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to search across indices and types even if some types do not exists on some indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/839</link><project id="" key="" /><description>Hi, 

I'm a developer of Escargot (gem to use rails with elasticsearch), i also apport some patch to rubberband (elasticsearch for ruby).

I have a problem using elasticsearch version 0.15.0 or later when i search in all indexes, specifying any types throw a error in this case.

The case is as follow:

echo "Create indexs"
curl -XPUT 'http://localhost:9200/first/tweet/1' -d '
{
    "user" : "kimchy",
    "postDate" : "2009-11-15T14:12:12",
    "message" : "trying out Elastic Search"
}';

curl -XPUT 'http://localhost:9200/first/grillo/4' -d '
{
    "user" : "kimchy",
    "postDate" : "2011-01-15T14:12:12",
    "message" : "Elastic Search"
}';

curl -XPUT 'http://localhost:9200/first/cote/3' -d '
{
    "user" : "kimchy",
    "postDate" : "2011-01-15T14:12:12",
    "message" : "Elastic Search"
}';

curl -XPUT 'http://localhost:9200/second/cote/2' -d '
{
    "user" : "kimchy",
    "postDate" : "2011-01-15T14:12:12",
    "message" : "Elastic Search"
}';

curl -XPUT 'http://localhost:9200/third/mencho/5' -d '
{
    "user" : "kimchy",
    "postDate" : "2011-01-15T14:12:12",
    "message" : "Elastic Search"
}';

curl -XPUT 'http://localhost:9200/third/cote/6' -d '
{
    "user" : "kimchy",
    "postDate" : "2011-01-15T14:12:12",
    "message" : "Elastic Search"
}';

echo "Refresh all"
curl -XPOST 'http://localhost:9200/_refresh'

echo "All indexes, types grillo,cote: (should be 4? or 2?)"
echo "In elasticsearch v-0.14.X return 4 with no errors, in v-0.15.X return 2 with error in log"
curl -XGET 'http://localhost:9200/_all/cote,grillo/_count?q=kimchy'

echo "Delete all indexes"
curl -XDELETE 'http://localhost:9200/first/'
curl -XDELETE 'http://localhost:9200/second/'
curl -XDELETE 'http://localhost:9200/third/'
</description><key id="741214">839</key><summary>Allow to search across indices and types even if some types do not exists on some indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeroig</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-07T12:50:57Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-07T13:00:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-07T13:00:31Z" id="968696">Allow to search across indices and types even if some types do not exists on some indices, closed by 1fdef91fc2ce4c133ed4700eb14e68afd1855ee1.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file></files><comments><comment>Allow to search across indices and types even if some types do not exists on some indices, closes #839.</comment></comments></commit></commits></item><item><title>Get Mapping API: when asking for a mapping of a single index and single type, don't wrap the mapping with the index name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/838</link><project id="" key="" /><description>This can create confusion for people trying to create mappings based on the response.
</description><key id="739058">838</key><summary>Get Mapping API: when asking for a mapping of a single index and single type, don't wrap the mapping with the index name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-06T19:12:04Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-06T19:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-06T19:12:55Z" id="965563">Get Mapping API: when asking for a mapping of a single index and single type, don&amp;#39;t wrap the mapping with the index name, closed by 08f594c3a0119cf1432b58eed60c5c5e06ecd645.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetMappingAction.java</file></files><comments><comment>Get Mapping API: when asking for a mapping of a single index and single type, don't wrap the mapping with the index name, closes #838.</comment></comments></commit></commits></item><item><title>Percolator doesn't work correctly after index recreation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/837</link><project id="" key="" /><description>Steps to reproduce:
1. Start ES with clean data directory
2. Run script https://gist.github.com/a9338b766cc3cef2124b
3. Restart ES
4. Run script again

On my computer (Mac Snow Leopard)
1. Started ES build from master - last commit 35be46df71e5da1e840b678d8650fb012ae7305b with no data directory
2. Response seems ok: https://gist.github.com/50ca6553b2adcd09ff51
3. Restart ES
4. Response is without matches from percolator: https://gist.github.com/fbaf397c8f820c6fe0d0

I tried it without step 3 (restarting ES) too and result was exactly the same. First script run was ok, second was without matches
</description><key id="738007">837</key><summary>Percolator doesn't work correctly after index recreation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vhyza</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-06T15:20:10Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-06T18:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-06T18:01:03Z" id="965241">Percolator doesn&amp;#39;t work correctly after index recreation, closed by 8c50a65699c4f2f8e8fff1a5793c76c7fbbff4c2.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorService.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/percolator/RecoveryPercolatorTests.java</file></files><comments><comment>Percolator doesn't work correctly after index recreation, closes #837.</comment></comments></commit></commits></item><item><title>Sort by a column of type 'short' throws an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/836</link><project id="" key="" /><description>Sort by a column of type 'short' throws an exception
</description><key id="735838">836</key><summary>Sort by a column of type 'short' throws an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aguereca</reporter><labels /><created>2011-04-05T22:43:40Z</created><updated>2011-04-05T22:45:09Z</updated><resolved>2011-04-05T22:45:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aguereca" created="2011-04-05T22:45:08Z" id="961639">Issue duplicated by mistake.
Original: 835
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bug: Sort on a column of type 'short' throws an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/835</link><project id="" key="" /><description>Bug: Sort on a column of type 'short' throws an exception.

Exception is: "IOException[Type not allowed [class java.lang.Short]]"

Column in definied in the mapping file as: 
"status": {
            "type": "short"
}

Query is doing sort using:
"sort": [
        {
            "status": "asc"
        }
    ], 
</description><key id="735709">835</key><summary>Bug: Sort on a column of type 'short' throws an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aguereca</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-05T21:59:26Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-05T23:26:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-05T23:26:09Z" id="961789">Bug: Sort on a column of type &amp;#39;short&amp;#39; throws an exception, closed by 35be46df71e5da1e840b678d8650fb012ae7305b.
</comment><comment author="aguereca" created="2011-04-06T02:38:26Z" id="962319">Thanks, great support !
</comment><comment author="aguereca" created="2011-04-06T03:11:54Z" id="962405">Seems like the Bug still active, now the error is different, what I did was download the v0.15.0-190-g35be46d and build it using the provided gradlew.bat

Then tried again to sort by a field of type short, this time the error is larger:

[wEMjPA12QQmGPiDlPDtomQ][us_features][4]: QueryPhaseExecutionException[[us_features][4]: query[state_alpha:AZ],from[0],size[1],sort[&lt;custom:"relevance": org.elasticsearch.index.field.data.shorts.ShortFieldDataType$1@619195f8&gt;]: Query Failed [Failed to execute main query]]; nested: NumberFormatException[Invalid shift value in prefixCoded string (is encoded value really an INT?)];
</comment><comment author="aguereca" created="2011-04-06T03:26:18Z" id="962428">CORRECTION !

Please accept my apologies and disregard my previous comment, I can confirm that issue has been fixed. 
The problem was me, indexing under a different index_type than the one defined in the mapping.

Again, thanks so much for the great support.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file></files><comments><comment>Bug: Sort on a column of type 'short' throws an exception, closes #835.</comment></comments></commit></commits></item><item><title>Histogram Facet: Add ability to define bounds (from/to) to both improve performance and provide additional bound filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/834</link><project id="" key="" /><description>Histogram Facet: Add ability to define bounds (from/to) to both improve performance and provide additional bound filtering.

Thanks to the fact that we know the bounds, we can create more optimized data structures to aggregate the histogram entries.
</description><key id="733753">834</key><summary>Histogram Facet: Add ability to define bounds (from/to) to both improve performance and provide additional bound filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-04-05T10:52:41Z</created><updated>2013-06-05T18:02:00Z</updated><resolved>2011-04-05T10:53:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-05T10:53:35Z" id="958412">Histogram Facet: Add ability to define bounds (from/to) to both improve performance and provide additional bound filtering, closed by 9b8eceb2296258557a2a6512ecedb58365e2bb55.
</comment><comment author="btiernay" created="2013-06-05T18:02:00Z" id="18995547">Where did this go in 0.90.1?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/facet/HistogramFacetSearchBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/CacheRecycler.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/NumericFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/MultiValueByteFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/SingleValueByteFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/MultiValueDoubleFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/SingleValueDoubleFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/MultiValueFloatFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/SingleValueFloatFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/MultiValueIntFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/SingleValueIntFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/MultiValueLongFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/SingleValueLongFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/MultiValueShortFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/SingleValueShortFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramScriptFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/BoundedCountHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/BoundedValueHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/BoundedValueScriptHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/InternalBoundedCountHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/bounded/InternalBoundedFullHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/CountHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/FullHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/InternalCountHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/InternalFullHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/ScriptHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/ValueHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/unbounded/ValueScriptHistogramFacetCollector.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Histogram Facet: Add ability to define bounds (from/to) to both improve performance and provide additional bound filtering, closes #834.</comment></comments></commit></commits></item><item><title>Geo Distance Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/833</link><project id="" key="" /><description>Geo Distance Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats
</description><key id="732284">833</key><summary>Geo Distance Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-04T21:52:35Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-04T21:53:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T21:53:12Z" id="956305">Geo Distance Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats, closed by cea8c5fefa1429e44963ea51dde4ef18dc8024d9.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/InternalGeoDistanceFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/ScriptGeoDistanceFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/ValueGeoDistanceFacetCollector.java</file></files><comments><comment>Geo Distance Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats, closes #833.</comment></comments></commit></commits></item><item><title>Range Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/832</link><project id="" key="" /><description>Range Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats
</description><key id="732254">832</key><summary>Range Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-04T21:42:17Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-04-04T21:42:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T21:42:59Z" id="956255">Range Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats, closed by 79939222e465543f128fd6b1d46c06eefee7c1ef.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/InternalRangeFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/KeyValueRangeFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/ScriptRangeFacetCollector.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Range Facet: Fix wrong total computation with multi valued fields by introducing total_count, add min/max stats, closes #832.</comment></comments></commit></commits></item><item><title>Date Histogram Facet:  Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count, add min/max stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/831</link><project id="" key="" /><description>Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count, add min/max stats
</description><key id="732164">831</key><summary>Date Histogram Facet:  Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count, add min/max stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-04T21:16:38Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-04-04T21:17:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T21:17:19Z" id="956126">Date Histogram Facet: Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count, add min/max stats, closed by 90a339ad5e4a61e750d0342be6326de46f45bedc.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/DateHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/InternalCountAndTotalDateHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/InternalCountDateHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/InternalDateHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/InternalFullDateHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/ValueDateHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/datehistogram/ValueScriptDateHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalCountHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalFullHistogramFacet.java</file></files><comments><comment>Date Histogram Facet: Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count, add min/max stats, closes #831.</comment></comments></commit></commits></item><item><title>Histogram Facet: Add min/max stats when providing value field / script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/830</link><project id="" key="" /><description>Add min/max stats when providing value field / script to histogram facet (on top of total)
</description><key id="732067">830</key><summary>Histogram Facet: Add min/max stats when providing value field / script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-04T20:48:24Z</created><updated>2014-07-27T15:36:46Z</updated><resolved>2011-04-04T20:49:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T20:49:06Z" id="955970">Histogram Facet: Add min/max stats when providing value field / script, closed by 8d1e9dbd3cd9007356abaaf363a9b04019b9d2a8.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/FullHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalCountHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalFullHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/KeyValueHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/KeyValueScriptHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/ScriptHistogramFacetCollector.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Histogram Facet: Add min/max stats when providing value field / script, closes #830.</comment></comments></commit></commits></item><item><title>Histogram Facet: Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/829</link><project id="" key="" /><description>Fix `count` to reflect the number of docs that matched, and introduce `total_count` which is the count of the values in total field (they might be different in multi valued fields).
</description><key id="731544">829</key><summary>Histogram Facet: Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.16.0</label></labels><created>2011-04-04T18:28:20Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-04-04T18:31:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T18:31:05Z" id="955295">Histogram Facet: Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count, closed by 46088b9f8acc057414ba222809b1b6bc3e1a435e.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/CacheRecycler.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/FullHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/HistogramFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalCountAndTotalHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalCountHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalFullHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/InternalHistogramFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/KeyValueHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/KeyValueScriptHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/histogram/ScriptHistogramFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/InternalByteTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/InternalDoubleTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/InternalFloatTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/InternalIntTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/InternalIpTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/InternalShortTermsFacet.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Histogram Facet: Improve value field case performance, fix wrong total computation with multi valued fields by introducing total_count, closes #829.</comment></comments></commit></commits></item><item><title>Cross-domain (CORS) AJAX woes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/828</link><project id="" key="" /><description>Hi,
I'm trying to `POST` a search request using a cross-domain AJAX call.  In Chrome it doesn't work, it always gives me a JavaScript-level error back saying `Empty response from server: 0`.  In Chrome's JavaScript console I see: `XMLHttpRequest cannot load http://127.0.0.1:9200/droopy/summary/_search. Request header field Content-Type is not allowed by Access-Control-Allow-Headers.`

Here's the output from `tcpdump` on port 9200:

&lt;pre&gt;
OPTIONS /droopy/summary/_search HTTP/1.1
Host: 127.0.0.1:9200
Connection: keep-alive
Referer: http://127.0.0.1:8000/index.html?srv=127.0.0.1:9200
Access-Control-Request-Method: POST
Origin: http://127.0.0.1:8000
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_6) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.3 Safari/534.24
Access-Control-Request-Headers: Content-Type
Accept: */*
Accept-Encoding: gzip,deflate,sdch
Accept-Language: en-US,en;q=0.8
Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3
&lt;/pre&gt;&lt;pre&gt;
HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Access-Control-Max-Age: 1728000
Access-Control-Allow-Methods: PUT, DELETE
Access-Control-Allow-Headers: X-Requested-With
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
&lt;/pre&gt;


I can see two problems:
1. In my AJAX request, I specify `Content-Type: application/json`, and because of that Chromes includes it in the pre-flight request header `Access-Control-Request-Headers` (see http://www.w3.org/TR/cors/ – search for that header name, it's explained).  ElasticSearch should include `Content-Type` in the header `Access-Control-Allow-Headers` in its pre-flight response, in addition to `X-Requested-With`.
2. My request is a `POST` request, but ElasticSearch seems to allow only `PUT` and `DELETE` as shown by its `Access-Control-Allow-Methods` header in the response.

Thankfully, I can easily work around this bug by not setting the `Content-Type` header in my request.  It seems that as of today, the 2nd point above doesn't prevent Chrome from sending the request, but I don't think that's normal, so I'd recommend fixing it too in order to prevent future breakages once Chrome becomes stricter about enforcing CORS headers.

The only other browser I tested was Firefox 3.6.15 and it didn't care about any of that.
</description><key id="731250">828</key><summary>Cross-domain (CORS) AJAX woes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsuna</reporter><labels /><created>2011-04-04T16:56:51Z</created><updated>2012-09-06T15:21:10Z</updated><resolved>2012-09-06T15:21:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tsuna" created="2011-04-05T05:05:53Z" id="957495">For the records, I just wanna add that the behavior with Chrome varies depending on the version you have.  I'm trying to collect more data from people around me, but it seems that older versions of Chrome work while newer versions became more strict about validating the headers in the pre-flight response.
</comment><comment author="kimchy" created="2011-04-06T10:49:06Z" id="963436">Heya,

   Man, this thing can get annoying :). Regarding your points (and I'm being a bit lazy here, since you've done the legwork ;) ):
1. So what should be returned exactly in the `Access-Control-Allow-Headers`? Easily changed to support whats needed.
2. I thought I read somewhere that you only needed it for PUT and DELETE, no problem adding POST if needed.
</comment><comment author="tsuna" created="2011-04-07T22:25:01Z" id="971188">Alright, I'll take a look at this after next week.  I'll provide a patch, but since this isn't a blocker for me right now, it'll have to wait a bit.
</comment><comment author="kimchy" created="2011-04-07T22:29:00Z" id="971199">If you can just tell me what needs to go where, I can easily modify it. Just wanted to save the legwork of me puddling through the spec.
</comment><comment author="grimen" created="2011-09-05T23:56:25Z" id="2007486">Any update on this (aka CORS with ElasticSearch)?
</comment><comment author="kimchy" created="2011-09-06T17:27:35Z" id="2019135">I can easily add POST to the allowed methods (though the last time I read the firefox tutorial, they only required PUT and DELETE). I am a bit fuzzy on the `Access-Control-Allow-Headers`, and what should be set there..., would help if someone have the time to check it.... . Here is the mozilla guide for it: https://developer.mozilla.org/en/HTTP_access_control.
</comment><comment author="grimen" created="2011-09-06T20:07:46Z" id="2021740">OK, me and my partner will discuss this next week - review it more deeply then.
</comment><comment author="tsuna" created="2011-09-14T16:25:27Z" id="2094956">For the records, I do use CORS with ES and it works with Chrome as long as I don't specify a `Content-Type` header on my request.  I believe that simply allowing `Content-Type` in `Access-Control-Allow-Headers` would solve the problem with Chrome.

For me not specifying the `Content-Type` header isn't a show stopper.  I wanted to specify it to send a request that's more semantically correct, but ES always returns `application/json` anyway, so strictly speaking this header isn't necessary.
</comment><comment author="bdargan" created="2012-04-20T02:30:04Z" id="5236779">So based on the spec: "A header is said to be a simple header if the header field name is an ASCII case-insensitive match for Accept, Accept-Language, or Content-Language, or if it is an ASCII case-insensitive **match for Content-Type** **AND** the header field **value media type** (excluding parameters) is an ASCII case-insensitive match for **application/x-www-form-urlencoded**, **multipart/form-data**, or **text/plain**."

as _'application/json' value_ is not considered a media type for a simple method check, chrome will do the preflight request. So adding POST in the Access-Control-Allow-Methods should fix it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file></files><comments><comment>Added proper headers for cross-origin resource sharing (CORS) with Ajax</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java</file></files><comments><comment>Added proper headers for cross-origin resource sharing (CORS) with Ajax</comment></comments></commit></commits></item><item><title>Newly added client node generates NPE in action.admin.cluster.node.info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/827</link><project id="" key="" /><description>```
 [20:02:48,615][INFO ][cluster.service          ] [Kiber the Cruel] added {[Shooting Star][-5hzGrC9SiSlw3tJo4FEaQ][inet[/10.101.59.130:9300]]{client=true, data=false, publish_host=local},}, reason: zen-disco-receive(join from node[[Shooting Star][-5hzGrC9SiSlw3tJo4FEaQ][inet[/10.101.59.130:9300]]{client=true, data=false, publish_host=local}])
 [20:02:49,066][DEBUG][action.admin.cluster.node.info] [Kiber the Cruel] failed to execute on node [-5hzGrC9SiSlw3tJo4FEaQ]
 org.elasticsearch.transport.RemoteTransportException: [Shooting Star][inet[/10.101.59.130:9300]][/cluster/nodes/info/node]
 Caused by: java.lang.NullPointerException
    at org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:64)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:200)
    at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:136)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:72)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:65)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:256)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:249)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:238)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
```
</description><key id="730397">827</key><summary>Newly added client node generates NPE in action.admin.cluster.node.info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2011-04-04T12:25:47Z</created><updated>2013-04-04T19:13:00Z</updated><resolved>2013-04-04T19:13:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="avar" created="2011-09-20T09:11:07Z" id="2143262">I've just encountered this issue as well. It looks like a race
condition in the transport code. I had it on 0.17.7 while upgrading
from 0.17.6:

```
[2011-09-20 10:26:27,823][DEBUG][action.admin.cluster.node.info] [search-02] failed to execute on node [R62UvRZBSGC62ISiKvYc2w]
org.elasticsearch.transport.RemoteTransportException: [search-03][inet[/10.147.174.142:9300]][/cluster/nodes/info/node]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:64)
        at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:215)
        at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:136)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:74)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:66)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:260)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:238)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
```

Basically what happened was:
1. The node that emitted this error had already been upgraded to
   0.17.7 &amp; was running fine
2. Another node (R62UvRZBSGC62ISiKvYc2w) running 0.17.6 was being
   restarted for an upgrade

Reading the code this happens here:

```
@Override public void writeTo(StreamOutput out) throws IOException {
    node.writeTo(out);
}
```

Presumably `node` is `NULL` at this point since it's been checked
earlier for validity, but the `node` has since gone away.
</comment><comment author="clintongormley" created="2013-04-04T19:13:00Z" id="15917499">This appears to have been fixed at some stage - Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade Lucene 3.1: Analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/826</link><project id="" key="" /><description>All analyzers, tokenizers, and filters now support a version setting, defaults to "3.1". It can be set to "3.0" to use lucene 3.0 functionality.
- RussianLetterTokenizer no longer required, standard tokenizer supports it.
- RussianStemFilter can be replaced with Snowball filter with name set to "Russian".
- ChineseAnalyzer no longer needed, standard analyzer supports it.
- BrazilianStemFilter replaced with snowball filter
- `uax_url_email` tokenizer added which is like standard tokenizer, but also handles email and urls
- add `reverse` token filter
- snowball now also support: Armenian, Basque, Catalan, 
- add `path_hierarchy` tokenizer (and doc it)
</description><key id="730303">826</key><summary>Upgrade Lucene 3.1: Analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.16.0</label></labels><created>2011-04-04T11:35:23Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-04-04T11:36:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T11:36:26Z" id="953463">Upgrade Lucene 3.1: Analyzers, closed by 5d6e84f206c85476d25e4b26e7998db2067e3bac.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AbstractIndexAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AbstractTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AbstractTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArabicAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ArabicStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BrazilianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/BrazilianStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ChineseAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CjkAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CustomAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CzechAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CzechStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DutchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/DutchStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FrenchAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FrenchStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GermanAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GermanStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/GreekAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/KeywordAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/KeywordTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/LengthTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/LetterTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NGramTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PathHierarchyTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PatternAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PersianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/PorterStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ReverseTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RussianAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RussianLetterTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/RussianStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SimpleAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/SnowballTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StandardAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StandardTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StandardTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StopAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/StopTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/ThaiAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/UAX29URLEmailTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/WhitespaceAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/WhitespaceTokenizerFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/AbstractCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticTokenFilterFactory.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/analysis/filter1/MyFilterTokenFilterFactory.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuCollationTokenFilterFactory.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuFoldingTokenFilterFactory.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java</file></files><comments><comment>Upgrade Lucene 3.1: Analyzers, closes #826.</comment></comments></commit></commits></item><item><title>Upgrade to Lucene 3.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/825</link><project id="" key="" /><description>Upgrade to new lucene 3.1 version. Some changes in behavior:
- On windows/solaris 64bit systems, automatically use mmapfs storage (matching lucene behavior).
- Analyzers changed behavior, there will be a separate issue with code changes to indicate the changes.
</description><key id="730115">825</key><summary>Upgrade to Lucene 3.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.16.0</label></labels><created>2011-04-04T09:52:34Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-04-04T09:54:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T09:54:33Z" id="953148">Upgrade to Lucene 3.1, closed by 4e4495ff1d27f65d4acdcc239998a463151fe561.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/index/ExtendedIndexSearcher.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/index/memory/CustomMemoryIndex.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MultiFieldMapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/DeletionAwareConstantScoreQuery.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/FieldCache.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/FieldCacheImpl.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/IndexReaderPurgedListener.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferDirectory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/io/stream/StreamInput.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/Directories.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/IndexWriters.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/ReaderSearcherHolder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/all/AllTokenStream.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/analysis/CharSequenceTermAttribute.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/analysis/cz/CzechAnalyzer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/analysis/cz/CzechStemFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/analysis/cz/CzechStemmer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/store/SwitchDirectory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/lucene/uid/UidField.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CustomAnalyzer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CzechAnalyzerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/CzechStemTokenFilterFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/FieldNameAnalyzer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NamedAnalyzer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/NumericAnalyzer.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/analysis/phonetic/PhoneticFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/bloom/simple/SimpleBloomCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/field/data/support/AbstractConcurrentMapFieldDataCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractDoubleConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/strings/StringOrdValFieldDataComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/gateway/blobstore/BlobStoreIndexShardGateway.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/BinaryFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/SourceFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/MergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/child/ChildCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/child/HasChildFilter.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/type/child/TopChildrenQuery.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/recovery/RecoveryTarget.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/IndexStoreModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/MmapFsStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/NioFsStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/fs/SimpleFsStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/support/AbstractStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/analysis/IndicesAnalysisService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryBufferController.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/lucene/IndexWritersTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/lucene/search/MoreLikeThisQueryTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/common/lucene/uid/UidFieldTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/deps/lucene/IndexWriterNoBufferTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/analysis/filter1/MyFilterTokenFilterFactory.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/field/data/ints/IntFieldDataTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/SimpleIndexQueryParserTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/highlight/HighlighterSearchTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/scriptfield/ScriptFieldSearchTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/ICUFoldingFilter.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/ICUNormalizer2Filter.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuFoldingTokenFilterFactory.java</file><file>plugins/analysis/icu/src/main/java/org/elasticsearch/index/analysis/IcuNormalizerTokenFilterFactory.java</file><file>plugins/analysis/icu/src/test/java/org/elasticsearch/index/analysis/Normalizer2Tests.java</file></files><comments><comment>Upgrade to Lucene 3.1, closes #825.</comment></comments></commit></commits></item><item><title>Geo Distance / Range Facets might count documents several times for a range entry if the field is multi valued</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/824</link><project id="" key="" /><description>Curl recreation: https://gist.github.com/6fe64a3dfb963f84cd7b

Issue: The Geo Distance facet reports the number of the matching locations, if a single document has two matching locations the facet count will be 2 instead of 1 (only one document). An option to only count the number of matching documents is desired to fully implement it as a navigation facet.

Note, this also applies to range facet.
</description><key id="727052">824</key><summary>Geo Distance / Range Facets might count documents several times for a range entry if the field is multi valued</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tozz</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-02T17:38:52Z</created><updated>2011-04-10T07:12:09Z</updated><resolved>2011-04-04T14:44:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-04T14:44:51Z" id="954167">Geo Distance / Range Facets might count documents several times for a range entry if the field is multi valued, closed by 105d60ac9c28b1be4ab1e8d04a39c13cf9641c18.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/geo/GeoPointFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/geo/MultiValueGeoPointFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/geo/SingleValueGeoPointFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/ScriptGeoDistanceFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/geodistance/ValueGeoDistanceFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/KeyValueRangeFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/range/RangeFacetCollector.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/geo/GeoDistanceFacetTests.java</file></files><comments><comment>Geo Distance / Range Facets might count documents several times for a range entry if the field is multi valued, closes #824.</comment></comments></commit></commits></item><item><title>Concurrent calls to refresh might result in "dangling" searchers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/823</link><project id="" key="" /><description>Concurrent calls to refresh might result in "dangling" searchers
</description><key id="725239">823</key><summary>Concurrent calls to refresh might result in "dangling" searchers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-04-01T19:43:22Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-04-01T19:44:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-01T19:44:03Z" id="946816">Concurrent calls to refresh might result in &amp;quot;dangling&amp;quot; searchers, closed by db1dcaded33fd7cb716afbd562d425602b6f2c90.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file></files><comments><comment>Concurrent calls to refresh might result in "dangling" searchers, closes #823.</comment></comments></commit></commits></item><item><title>Terms Facet: Performance improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/822</link><project id="" key="" /><description>New internal way to aggregate terms facet increasing perf by up to 10x (or even more).
</description><key id="724208">822</key><summary>Terms Facet: Performance improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-04-01T13:29:53Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-04-01T13:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-04-01T13:31:03Z" id="945458">Terms Facet: Performance improvements, closed by e4cbdfa05bbe87f636d76aa7539e2ab3f0ec39b1.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/search/facet/TermsFacetSearchBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/CacheRecycler.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/FieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/ByteFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/MultiValueByteFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/SingleValueByteFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/DoubleFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/MultiValueDoubleFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/SingleValueDoubleFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/FloatFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/MultiValueFloatFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/SingleValueFloatFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/IntFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/MultiValueIntFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/SingleValueIntFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/LongFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/MultiValueLongFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/SingleValueLongFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/MultiValueShortFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/ShortFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/SingleValueShortFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/strings/MultiValueStringFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/strings/SingleValueStringFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/strings/StringFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/geo/MultiValueGeoPointFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/geo/SingleValueGeoPointFieldData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/InternalByteTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/TermsByteFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/TermsByteOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/InternalDoubleTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/InternalFloatTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/TermsFloatFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/TermsFloatOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/InternalIntTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/TermsIntFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/TermsIntOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/InternalIpTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/TermsIpFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ip/TermsIpOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/InternalLongTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/InternalShortTermsFacet.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/TermsShortFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/TermsShortOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/FieldsTermsStringFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/ScriptTermsStringFieldFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/strings/TermsStringOrdinalsFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/support/EntryPriorityQueue.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>Terms Facet: Performance improvements, closes #822.</comment></comments></commit></commits></item><item><title>Indexing Buffer: Automatically inactivate unindexed into shards and lower their indexing buffer size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/821</link><project id="" key="" /><description>Indexing Buffer: Automatically inactivate unindexed into shards and lower their indexing buffer size.

New settings (default are really ok):
- `indices.memory.inactive_shard_index_buffer_size` defaults to `1mb`.
- `indices.memory.shard_inactive_time`: defaults to `30m`.
</description><key id="721308">821</key><summary>Indexing Buffer: Automatically inactivate unindexed into shards and lower their indexing buffer size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-31T15:02:49Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-03-31T15:03:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-31T15:03:48Z" id="941395">Indexing Buffer: Automatically inactivate unindexed into shards and lower their indexing buffer size, closed by 3ff35d42b5bd42c8a88bb84da09d2601eb22d2eb.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/TranslogService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryBufferController.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalNode.java</file></files><comments><comment>Indexing Buffer: Automatically inactivate unindexed into shards and lower their indexing buffer size, closes #821.</comment></comments></commit></commits></item><item><title>Translog: Reduce the number of translog ops to flush after from 20000 to 5000</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/820</link><project id="" key="" /><description>Reduce `index.translog.flush_threshold_ops` from `20000` to `5000` for lower number of translog entries before flushing.

Also, reduce `index.translog.flush_threshold_size` from `500mb` to `200mb`.

Note, this setting can be set in update settings API, and, can be increased before doing bulk indexing for better indexing TPS.
</description><key id="719463">820</key><summary>Translog: Reduce the number of translog ops to flush after from 20000 to 5000</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-30T23:05:38Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-03-30T23:09:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T23:09:14Z" id="938798">Translog: Reduce the number of translog ops to flush after from 20000 to 5000, closed by 74838fe1aaff86d156def5e4572f0f6b6f91379d.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/TranslogService.java</file></files><comments><comment>Translog: Reduce the number of translog ops to flush after from 20000 to 5000, closes #820.</comment></comments></commit></commits></item><item><title>Shard Allocation: Add node_initial_primaries_recoveries setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/819</link><project id="" key="" /><description>Allow to control specifically the number of initial recoveries of primaries that are allowed per node. Since most times local gateway is used, those should be fast and we can handle more of those per node without creating load.

The `cluster.routing.allocation.node_initial_primaries_recoveries` setting controls that, and defaults to `4`.

Note, for the rest of the recoveries happening in the system (replicas and rebalancing), the current setting `cluster.routing.allocation.node_concurrent_recoveries` applies which defaults to `2`.
</description><key id="719447">819</key><summary>Shard Allocation: Add node_initial_primaries_recoveries setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-30T22:59:01Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-03-30T23:04:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T23:04:23Z" id="938780">Shard Allocation: Add node_initial_primaries_recoveries setting, closed by 14d98a7319555331e5a660f4f31122b9225250af.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ThrottlingNodeAllocation.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java</file></files><comments><comment>Shard Allocation: Add node_initial_primaries_recoveries setting, closes #819.</comment></comments></commit></commits></item><item><title>elasticsearch script: Change CLASSPATH to ES_CLASSPATH</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/818</link><project id="" key="" /><description>Usually, one doen't really change it or update it, so shouldn't affect anybody, but, will also remove it from being used by other apps.
</description><key id="718705">818</key><summary>elasticsearch script: Change CLASSPATH to ES_CLASSPATH</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.16.0</label></labels><created>2011-03-30T19:02:32Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-03-30T19:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T19:03:35Z" id="937738">elasticsearch script: Change CLASSPATH to ES_CLASSPATH, closed by dc0e493ccec367cb312c979f29fb1d88a0d22124.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>elasticsearch script: Change CLASSPATH to ES_CLASSPATH, closes #818.</comment></comments></commit></commits></item><item><title>Shard Allocation: Rename cluster.routing.allocation.concurrent_recoveries to cluster.routing.allocation.node_concurrent_recoveries (old one still works)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/817</link><project id="" key="" /><description>Better setting name, to indicate that its a _node_ level setting.
</description><key id="717722">817</key><summary>Shard Allocation: Rename cluster.routing.allocation.concurrent_recoveries to cluster.routing.allocation.node_concurrent_recoveries (old one still works)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-30T13:43:26Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-03-30T13:44:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T13:44:08Z" id="936161">Shard Allocation: Rename cluster.routing.allocation.concurrent_recoveries to cluster.routing.allocation.node_concurrent_recoveries (old one still works), closed by 2dd5094d37a9e2d778e5fc3a4c9e283d26e678a2.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ThrottlingNodeAllocation.java</file></files><comments><comment>Shard Allocation: Rename cluster.routing.allocation.concurrent_recoveries to cluster.routing.allocation.node_concurrent_recoveries (old one still works), closes #817.</comment></comments></commit></commits></item><item><title>Shard Allocation: Allow to control how many cluster wide concurrent rebalance (relocation) are allowed, default to 3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/816</link><project id="" key="" /><description>Allow to control how many concurrent rebalancing of shards are allowed _cluster wide_, and default it to `3`. The setting is `cluster.routing.allocation.cluster_concurrent_rebalance`. Setting it to `-1` will allow for unbounded number of rebalancing.
</description><key id="717715">816</key><summary>Shard Allocation: Allow to control how many cluster wide concurrent rebalance (relocation) are allowed, default to 3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-30T13:39:47Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-03-30T13:40:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T13:40:23Z" id="936150">Shard Allocation: Allow to control how many cluster wide concurrent rebalance (relocation) are allowed, default to 3, closed by 7ce7fb33e5d37a0991b6b73965b3938b2dbb4565.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceNodeAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/NodeAllocations.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ShardAllocationModule.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java</file></files><comments><comment>Shard Allocation: Allow to control how many cluster wide concurrent rebalance (relocation) are allowed, default to 3, closes #816.</comment></comments></commit></commits></item><item><title>HTTP: Support compression (gzip, deflate) when using Accept-Encoding header</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/815</link><project id="" key="" /><description>HTTP: Support compression (gzip, deflate) when using Accept-Encoding header. By default, its enabled, to disable it set `http.compress` to `false`. Compression level (on responses) can also be configured by setting `http.compress_level` to a value between `1` and `10` (defaults to `6`).
</description><key id="717592">815</key><summary>HTTP: Support compression (gzip, deflate) when using Accept-Encoding header</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-30T12:46:16Z</created><updated>2011-04-10T07:12:08Z</updated><resolved>2011-03-30T12:46:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T12:46:51Z" id="935941">HTTP: Support compression (gzip, deflate) when using Accept-Encoding header, closed by 8a5dd90885347de6b765d9269dc5e99bbbdb3c25.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file></files><comments><comment>HTTP: Support compression (gzip, deflate) when using Accept-Encoding header, closes #815.</comment></comments></commit></commits></item><item><title>Shard Allocation: Add a setting to control when rebalancing will happen based on the cluster wide active shards state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/814</link><project id="" key="" /><description>Currently, rebalancing can start once a shard replication group (a primary and its replicas) are active for those shards. This can become taxing on a cluster with many indices that are slowly being initialized during full cluster restart.

Allow to control when rebalancing will happen based on the total state of all the indices shards in the cluster. The setting is: `cluster.routing.allocation.allow_rebalance` and has the following values:
- `always`: Same behavior as today. Rebalancing will be allowed once a shard replication group is active.
- `indices_primaries_active`: Rebalancing will be allowed only once all primary shards on all indices are active.
- `indices_all_active`: Rebalancing will be allowed only once all shards on all indices are active.

The default value is `indices_all_active`. This does mean that if the cluster is in yellow state (for example, not enough nodes were brought on and it could not find a shard to recover), then no rebalancing will happen until it is resolved.
</description><key id="717369">814</key><summary>Shard Allocation: Add a setting to control when rebalancing will happen based on the cluster wide active shards state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-30T10:44:23Z</created><updated>2011-04-10T07:12:06Z</updated><resolved>2011-03-30T10:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T10:57:08Z" id="935636">Shard Allocation: Add a setting to control when rebalancing will happen based on the cluster wide active shards state, closed by fdbccf28b04b4ba1df2f9f59a63a35354206e26e.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceNodeAllocation.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/NodeAllocations.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/allocation/ShardAllocationModule.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocationTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java</file></files><comments><comment>Shard Allocation: Add a setting to control when rebalancing will happen based on the cluster wide active shards state, closes #814.</comment></comments></commit></commits></item><item><title>Dupplicate element with some id in ES if a child changes parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/813</link><project id="" key="" /><description>If a child changes parent, it can be written two times in ES (shard migration of child).

This is a simple gist: https://gist.github.com/893298

There are different options to correct this problem.

The more safe options, I think,  is to try to fetch the child.

1) If not available, just "index".

2) If available, check _version:
  a) if child _version &lt;= indexed child _version, skip 
  b) just call a delete item with the id of the child, then just "index" (propagate _version if required)

Or warn the users and let them manage the problem.
</description><key id="715645">813</key><summary>Dupplicate element with some id in ES if a child changes parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparo</reporter><labels /><created>2011-03-29T21:22:20Z</created><updated>2013-04-04T19:13:11Z</updated><resolved>2013-04-04T19:13:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-30T18:12:43Z" id="937464">Heya, yea, that will happen... . The problem with any other solution is that the (even warning) is that it will really increase the time of indexing, where, I think, most cases do not really change the parent. Maybe we should add a warning that you need to specifically handle that in the parent section for the index API?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"Cannot parse" Exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/812</link><project id="" key="" /><description>I was stumbling my way through querying and ran in to the following exception. I understand that I have no idea what I'm doing here, and was giving it invalid input, but I guess I wasn't expecting an exception.

The query i was using is below

```
curl -XGET 'http://localhost:9201/default/url/_search?q=latitude:52.0167,asn:20857'
```

Note the comma in the q= string is, what I believe, causes it to throw the exception.

```
[2011-03-29 11:26:59,590][DEBUG][action.search.type       ] [Anti-Cap] [default][4], node[0faY_x8CSeqMIPvuHS-IYw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@553b7e18]
org.elasticsearch.search.SearchParseException: [default][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:416)
at org.elasticsearch.search.SearchService.createContext(SearchService.java:332)
at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:165)
at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:132)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:76)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$000(TransportSearchTypeAction.java:75)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.run(TransportSearchTypeAction.java:151)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
Caused by: org.elasticsearch.index.query.QueryParsingException: [default] Failed to parse query [latitude:52.0167,asn:20857]
at org.elasticsearch.index.query.xcontent.QueryStringQueryParser.parse(QueryStringQueryParser.java:198)
at org.elasticsearch.index.query.xcontent.QueryParseContext.parseInnerQuery(QueryParseContext.java:167)
at org.elasticsearch.index.query.xcontent.XContentIndexQueryParser.parse(XContentIndexQueryParser.java:234)
at org.elasticsearch.index.query.xcontent.XContentIndexQueryParser.parse(XContentIndexQueryParser.java:214)
at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:34)
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:403)
... 10 more
Caused by: org.apache.lucene.queryParser.ParseException: Cannot parse 'latitude:52.0167,asn:20857': Encountered " ":" ": "" at line 1, column 20.
Was expecting one of:
&lt;EOF&gt; 
&lt;AND&gt; ...
&lt;OR&gt; ...
&lt;NOT&gt; ...
"+" ...
"-" ...
"(" ...
"*" ...
"^" ...
&lt;QUOTED&gt; ...
&lt;TERM&gt; ...
&lt;FUZZY_SLOP&gt; ...
&lt;PREFIXTERM&gt; ...
&lt;WILDTERM&gt; ...
"[" ...
"{" ...
&lt;NUMBER&gt; ...

at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:187)
at org.elasticsearch.index.query.xcontent.QueryStringQueryParser.parse(QueryStringQueryParser.java:192)
... 15 more
Caused by: org.apache.lucene.queryParser.ParseException: Encountered " ":" ": "" at line 1, column 20.
Was expecting one of:
&lt;EOF&gt; 
&lt;AND&gt; ...
&lt;OR&gt; ...
&lt;NOT&gt; ...
"+" ...
"-" ...
"(" ...
"*" ...
"^" ...
&lt;QUOTED&gt; ...
&lt;TERM&gt; ...
&lt;FUZZY_SLOP&gt; ...
&lt;PREFIXTERM&gt; ...
&lt;WILDTERM&gt; ...
"[" ...
"{" ...
&lt;NUMBER&gt; ...

at org.apache.lucene.queryParser.QueryParser.generateParseException(QueryParser.java:1759)
at org.apache.lucene.queryParser.QueryParser.jj_consume_token(QueryParser.java:1641)
at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:1168)
at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:182)
... 16 more
[2011-03-29 11:27:15,014][INFO ][node                     ] [Anti-Cap] {elasticsearch/0.15.2}[28499]: stopping ...
[2011-03-29 11:27:15,078][INFO ][node                     ] [Anti-Cap] {elasticsearch/0.15.2}[28499]: stopped
```
</description><key id="715299">812</key><summary>"Cannot parse" Exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">caphrim007</reporter><labels /><created>2011-03-29T19:41:06Z</created><updated>2013-04-04T19:14:25Z</updated><resolved>2013-04-04T19:14:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MalkaFeldman" created="2011-08-28T11:46:13Z" id="1923529">I have this error too. 
</comment><comment author="clintongormley" created="2013-04-04T19:14:25Z" id="15917583">The query syntax parser is strict and is part of lucene - out of our control.  Remove the comma and it parses correctly
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices Status API: Add refresh stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/811</link><project id="" key="" /><description>Add refresh stats in the indices status API to return how long refreshes took per shard and per index.
</description><key id="714594">811</key><summary>Indices Status API: Add refresh stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-29T15:37:22Z</created><updated>2011-03-29T15:54:12Z</updated><resolved>2011-03-29T15:54:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-29T15:54:12Z" id="931575">Indices Status API: Add refresh stats, closed by 3138269573f92dd139789c72a3b70601049fcf73.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/IndexShardStatus.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/IndexStatus.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/ShardStatus.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/MergeStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/refresh/RefreshStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Indices Status API: Add refresh stats, closes #811.</comment></comments></commit></commits></item><item><title>Get Settings API: Allow to retrieve (just) a specific index / indices settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/810</link><project id="" key="" /><description>Sample usages:

```
curl localhost:9200/test/_settings
curl localhost:9200/test1,test2/_settings
```
</description><key id="713916">810</key><summary>Get Settings API: Allow to retrieve (just) a specific index / indices settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-29T11:49:42Z</created><updated>2011-03-29T11:50:41Z</updated><resolved>2011-03-29T11:50:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-29T11:50:41Z" id="930578">Get Settings API: Allow to retrieve (just) a specific index / indices settings, closed by ff8dc2673f74b85ea7ea55a7ea9900b29fc9fb25.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file></files><comments><comment>Get Settings API: Allow to retrieve (just) a specific index / indices settings, closes #810.</comment></comments></commit></commits></item><item><title>Indices Status API: Remove settings/aliases section, and add `recovery`/`snapshot` flags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/809</link><project id="" key="" /><description>- Remote the settings and aliases sections from index status API.
- Recovery section only returned when passing `recovery` and setting it to `true`.
- Snapshot section only returned when passing `snapshot` and setting it to `true`.
</description><key id="713841">809</key><summary>Indices Status API: Remove settings/aliases section, and add `recovery`/`snapshot` flags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-29T11:04:46Z</created><updated>2011-03-30T22:54:31Z</updated><resolved>2011-03-29T11:05:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-29T11:05:52Z" id="930490">Indices Status API: Remove settings/aliases section, and add `recovery`/`snapshot` flags, closed by 508d1d40fba92cd0b2c544cef0213a9c18e9002f.
</comment><comment author="ruflin" created="2011-03-30T22:26:39Z" id="938628">What is the best way to retrieve it know? The info for aliases is in _cluster/state but is there a better way?
</comment><comment author="kimchy" created="2011-03-30T22:28:00Z" id="938639">Settings have a new API. Aliases from cluster state. Aliases are due to majore refactoring, and once its done, then there will also be a get aliases API.
</comment><comment author="ruflin" created="2011-03-30T22:37:23Z" id="938670">Perfect. Then I wait until this is done. I assume this will make it into 0.16.0
</comment><comment author="kimchy" created="2011-03-30T22:49:32Z" id="938718">The alias refactoring? no, it will probably be in for the next release.
</comment><comment author="ruflin" created="2011-03-30T22:54:31Z" id="938732">Ok, even better. Thx.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/IndexStatus.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusResponse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/status/IndicesStatusRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/status/RestIndicesStatusAction.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/gateway/fs/AbstractSimpleIndexGatewayTests.java</file></files><comments><comment>Indices Status API: Remove settings/aliases section, and add `recovery`/`snapshot` flags, closes #809.</comment></comments></commit></commits></item><item><title>Allow field sorts against an index that does not have a mapping for that field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/808</link><project id="" key="" /><description>Say I have two indexes, one with a field named "sortfield" and the other is missing a mapping for this field. 

I'd like to run a query against both indexes and sort on sortfield. The index that does not contain this value would treat it as being missing and then use the logic defined in this feature to determine if they go at top or bottom:
https://github.com/elasticsearch/elasticsearch/issues/772

Let me know if there are any questions. 

Thanks!
</description><key id="712108">808</key><summary>Allow field sorts against an index that does not have a mapping for that field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>adoptme</label></labels><created>2011-03-28T21:13:14Z</created><updated>2014-08-01T16:13:17Z</updated><resolved>2014-08-01T16:13:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nickhoffman" created="2012-01-10T22:18:52Z" id="3438210">I think this is a duplicate of #896.
</comment><comment author="clintongormley" created="2013-04-04T19:17:45Z" id="15917771">No, it's not a duplicate, and it still throws an error.
</comment><comment author="clintongormley" created="2014-07-08T12:51:48Z" id="48331495">Recreation:

```
DELETE /_all

PUT /one/t/1
{"foo":1}

PUT /two/t/1
{}


GET /_search
{
  "sort": {
    "foo": {
      "order": "asc",
      "ignore_unmapped": true
    }
  }
}
```
</comment><comment author="clintongormley" created="2014-08-01T16:13:17Z" id="50903287">Fixed by #7039 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support more stats on histograms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/807</link><project id="" key="" /><description>I'd like to be able to do a date_histogram, but retrieve (eg) the max last_modified time for each day.
</description><key id="711779">807</key><summary>Support more stats on histograms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-28T19:27:17Z</created><updated>2011-07-11T22:15:22Z</updated><resolved>2011-07-11T22:15:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-07-11T22:15:21Z" id="1550458">This has been implemented and min/max are included per bucket.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a `format` parameter for formatting the `time` in histo facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/806</link><project id="" key="" /><description>Hiya

It'd be good to be able to specify how the `time` value should be formatted in histogram facets, instead of just returning epoch seconds.

You already have the time zone, so should be easy to return a local date/time
</description><key id="711616">806</key><summary>Add a `format` parameter for formatting the `time` in histo facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-28T18:39:05Z</created><updated>2014-07-03T18:23:21Z</updated><resolved>2014-07-03T18:23:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="qraynaud" created="2014-06-04T15:41:34Z" id="45107557">This is in `date_histogram` aggregations. You already have a `format` in those. And facets are deprecated. They won't make them better now.

I believe this can be closed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fields containing an array with a single member should remain arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/805</link><project id="" key="" /><description>I don't know if this is possible, but when returning a stored field that was indexed as an array, it should remain an array, even if it only has a single element:

```
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1'  -d '
{
   "mappings" : {
      "bar" : {
         "properties" : {
            "text" : {
               "store" : "yes",
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "text" : [
      "foo",
      "bar"
   ]
}
'

curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "text" : [
      "baz"
   ]
}
'

curl -XGET 'http://127.0.0.1:9200/foo/_search?pretty=1'  -d '
{
   "fields" : [
      "text"
   ],
   "query" : {
      "match_all" : {}
   }
}
'

# [Sat Mar 26 11:06:52 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 1,
#             "fields" : {
#                "text" : "baz"
#             },
#             "_index" : "foo",
#             "_id" : "RSpdyb4zSKSO6lqhvbYtAw",
#             "_type" : "bar"
#          },
#          {
#             "_score" : 1,
#             "fields" : {
#                "text" : [
#                   "foo",
#                   "bar"
#                ]
#             },
#             "_index" : "foo",
#             "_id" : "bxJdAwq2TkijcEKZJZtpUw",
#             "_type" : "bar"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```
</description><key id="706223">805</key><summary>Fields containing an array with a single member should remain arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-26T10:15:12Z</created><updated>2013-04-04T19:18:45Z</updated><resolved>2013-04-04T19:18:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-26T17:04:13Z" id="920639">Might be able to support this, need to think of a way to indicate that this is part of an array, which can become difficult with outer (outer) objects being the actual array.
</comment><comment author="clintongormley" created="2013-04-04T19:18:45Z" id="15917836">Mitigated by returning the fields from the `_source` instead of storing them separately
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Add _size field mapping, indexing the original source size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/804</link><project id="" key="" /><description>Data can be derived from the size of each document indexed. The `_size` field mapping can be used to enable indexing, and possibly storing the size of the original source indexed.

In order to enable it, set in the mappings:

```
{
    "tweet" : {
        "_size" : {"enabled" : true}
    }
}
```

This will index the `_size` field, and allow it to be searchable, and faceted (statistical facets for example). In order to store the field as well:

```
{
    "tweet" : {
        "_size" : {"enabled" : true, "store" : "yes"}
    }
}
```
</description><key id="697746">804</key><summary>Mapping: Add _size field mapping, indexing the original source size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-23T15:00:31Z</created><updated>2011-03-23T15:37:56Z</updated><resolved>2011-03-23T15:37:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-23T15:37:56Z" id="908057">Mapping: Add _size field mapping, indexing the original source size, closed by 20593fb9663029e37d65bc6b83f98d6c3778a088.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/SizeFieldMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentDocumentMapper.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentDocumentMapperParser.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/xcontent/size/SizeMappingTests.java</file></files><comments><comment>Mapping: Add _size field mapping, indexing the original source size, closes #804.</comment></comments></commit></commits></item><item><title>Add option to expire filter's cache based on access time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/803</link><project id="" key="" /><description>LRU, Soft and Weak are the current FilterCache options, but none of them expire elements based on last access time.

Add `expire` flag accepting a time value based setting (`5s`, `10m`) that will cause filters to be expired after that time if not accessed.

With current implementations, in theory, the cache would grow to it's maximum if we keep creating filters used not permanently, but for a long enough time to consider caching them. An example would be time based filters, like {today, yesterday, past week, past month}, that are created anew each day.
</description><key id="692552">803</key><summary>Add option to expire filter's cache based on access time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sebaes</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-22T00:07:00Z</created><updated>2011-03-22T11:14:49Z</updated><resolved>2011-03-22T11:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-22T11:14:49Z" id="902431">Add option to expire filter&amp;#39;s cache based on access time, closed by e759b4c9715e067d73e91b998e7396ec078fc33f.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file></files><comments><comment>Add option to expire filter's cache based on access time, closes #803.</comment></comments></commit></commits></item><item><title>Extend stats API to include filter's cache count and eviction count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/802</link><project id="" key="" /><description>Provide a way to know in runtime the statistics of the filters cache.
Probably useful counters are:
- size
- evictionCount
</description><key id="692542">802</key><summary>Extend stats API to include filter's cache count and eviction count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sebaes</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-22T00:02:24Z</created><updated>2011-03-22T11:30:25Z</updated><resolved>2011-03-22T11:30:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-22T11:30:25Z" id="902472">Extend stats API to include filter&amp;#39;s cache count and eviction count, closed by 8c9000c54cafe7861ac81b236c29b3df332e6d2f.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/CacheStats.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/FilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/none/NoneFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/resident/ResidentFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/soft/SoftFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/support/AbstractDoubleConcurrentMapFilterCache.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/cache/filter/weak/WeakFilterCache.java</file></files><comments><comment>Extend stats API to include filter's cache count and eviction count, closes #802.</comment></comments></commit></commits></item><item><title>Date Field Mapper: Allow to use timestamp value (milliseconds since epoch, UTC)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/801</link><project id="" key="" /><description>The `date` type should also accept timestamp values (milliseconds since the epoch, UTC).
</description><key id="692468">801</key><summary>Date Field Mapper: Allow to use timestamp value (milliseconds since epoch, UTC)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-21T23:29:21Z</created><updated>2011-03-21T23:29:53Z</updated><resolved>2011-03-21T23:29:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-21T23:29:53Z" id="900842">Date Field Mapper: Allow to use timestamp value (milliseconds since epoch, UTC), closed by 0d150e691860a905b1e791b426ca11f644a97af7.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/DateFieldMapper.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/xcontent/date/SimpleDateMappingTests.java</file></files><comments><comment>Date Field Mapper: Allow to use timestamp value (milliseconds since epoch, UTC), closes #801.</comment></comments></commit></commits></item><item><title>Mapping: Failure to update _source mapping - compress and compress_threshold</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/800</link><project id="" key="" /><description>Mapping: Failure to update _source mapping - compress and compress_threshold. For example:

```
curl -XPUT localhost:9200/test

curl -XPUT localhost:9200/test/type1/1 -d '{
    "field" : "value" 
}'

curl localhost:9200/test/type1/_mapping?pretty=1

curl -XPUT localhost:9200/test/type1/_mapping -d '{
    "type1" : {
        "_source" : {
            "compress" : true
        }
    }
}'

curl localhost:9200/test/type1/_mapping?pretty=1
```
</description><key id="691251">800</key><summary>Mapping: Failure to update _source mapping - compress and compress_threshold</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-03-21T15:50:19Z</created><updated>2011-03-21T16:10:29Z</updated><resolved>2011-03-21T16:10:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-21T16:10:28Z" id="898988">Mapping: Failure to update _source mapping - compress and compress_threshold, closed by 61034f5c866cf8786e7c7a96b3c18c64ddc4054c.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentDocumentMapper.java</file></files><comments><comment>Mapping: Failure to update _source mapping - compress and compress_threshold, closes #800.</comment></comments></commit></commits></item><item><title>Update Settings: Allow to control `index.auto_expand_replicas`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/799</link><project id="" key="" /><description>Allow to change the `index.auto_expand_replicas` setting dynamically using update setting. Also, allow it to be set to `false`, basically, disabling it. For example:

```
curl -XPUT localhost:9200/test -d '{
    "settings" : {
        "index" : {
            "number_of_shards" : 1,
            "auto_expand_replicas" : "0-all"
        }
    }
}'

curl -XPUT localhost:9200/test/_settings -d '{
    "index" : {
        "number_of_replicas" : 0,
        "auto_expand_replicas" : false
    }
}'

curl -XPUT localhost:9200/test/_settings -d '{
    "index" : {
        "auto_expand_replicas" : "0-all"
    }
}'
```
</description><key id="690958">799</key><summary>Update Settings: Allow to control `index.auto_expand_replicas`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-21T14:17:41Z</created><updated>2011-03-21T14:18:19Z</updated><resolved>2011-03-21T14:18:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-21T14:18:19Z" id="898462">Update Settings: Allow to control `index.auto_expand_replicas`, closed by 8f8fb89c4e2efac57ea29ac132bd1df40026793b.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file></files><comments><comment>Update Settings: Allow to control `index.auto_expand_replicas`, closes #799.</comment></comments></commit></commits></item><item><title>Can _version be returned by search?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/798</link><project id="" key="" /><description>I note that _version comes back when I call GET but doesn't appear to be returned by SEARCH even when added to fields.
</description><key id="689174">798</key><summary>Can _version be returned by search?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merrellb</reporter><labels /><created>2011-03-20T19:37:27Z</created><updated>2011-03-21T02:08:30Z</updated><resolved>2011-03-20T20:43:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-20T20:43:07Z" id="896229">You can get it, you just need to enable it: http://www.elasticsearch.org/guide/reference/api/search/version.html. Issues are for issues, btw, not questions.
</comment><comment author="merrellb" created="2011-03-21T02:08:30Z" id="896958">Thanks for the clarification.  I was confused by the versioning blog post which stated:
"Same applies when doing a search, we will get a _version per search hit."
With no mention of having to enable it separately I mistakenly thought this was a bug.  I will try to be more vigilant in verifying first before reporting a bug.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard recovery timing bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/797</link><project id="" key="" /><description>Hiya

On 0.15.2 I had one node recovering from master.  I think there is a slight timing bug, because the recovering node threw a `No active shard found` error just before it finished recovering that shard:

```
[13:18:10,295][DEBUG][action.search.type       ] [Abominatrix] [iannounce_object_1298721956][1], node[4T4KAmQeQ6yD6iVePoP5tQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@62d2953d]
org.elasticsearch.transport.RemoteTransportException: No active shard found
Caused by: org.elasticsearch.transport.ResponseHandlerFailureTransportException: No active shard found
Caused by: java.util.NoSuchElementException: No active shard found
    at org.elasticsearch.cluster.routing.PlainShardsIterator.nextActive(PlainShardsIterator.java:104)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:212)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.access$300(TransportSearchTypeAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:194)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:192)
    at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:115)
    at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:108)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:102)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:754)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:302)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:317)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:299)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:216)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:545)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:540)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
[13:18:14,863][DEBUG][index.shard.recovery     ] [Abominatrix] [iannounce_object_1298721956][1] recovery completed from [Jawynn Dueck The Iron christian of Faith][4T4KAmQeQ6yD6iVePoP5tQ][inet[/192.168.10.51:9300]], took[4.1m]
   phase1: recovered_files [244] with total_size of [1.7gb], took [3.8m], throttling_wait [0s]
         : reusing_files   [0] with total_size of [0b]
   phase2: recovered [295] transaction log operations, took [17.5s]
   phase3: recovered [1] transaction log operations, took [80ms]
```
</description><key id="688607">797</key><summary>Shard recovery timing bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-20T13:22:40Z</created><updated>2013-04-04T19:19:06Z</updated><resolved>2013-04-04T19:19:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:19:06Z" id="15917857">Appears to have been fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix bug #795</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/796</link><project id="" key="" /><description>This is a suggested fix for elasticsearch/elasticsearch#795
</description><key id="686687">796</key><summary>Fix bug #795</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsuna</reporter><labels /><created>2011-03-19T09:49:01Z</created><updated>2014-06-27T05:34:58Z</updated><resolved>2011-03-20T01:00:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-19T17:46:34Z" id="893309">Not sure that this is the right way to solve this, we should raise a failure in this case, since its expected that an object is passed, and it does not meet the expected behavior, so, the document indexed will not be as expected).
</comment><comment author="tsuna" created="2011-03-19T22:04:17Z" id="893934">FWIW, this fix allowed me to import my data and I was able to search on the problematic fields without seeing any unexpected behavior during my searches so far.
</comment><comment author="kimchy" created="2011-03-19T22:06:54Z" id="893942">Its by mistake :), the mapping support is not aimed at supporting a property being both a value and an object, might hit problems in many different areas in elasticsearch because of that.
</comment><comment author="tsuna" created="2011-03-20T01:00:04Z" id="894283">Gotcha.  Closing then.  Thanks for the fix in elasticsearch/elasticsearch@fb7fbc8c831a003de15cfd569576a11be1912946
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE when the JSON to index doesn't match the mapping's expectations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/795</link><project id="" key="" /><description>Here's a simple repro (works both on 0.15.2 and on HEAD as of elasticsearch/elasticsearch@70fc8d9af0574c0ad3fa3da8916078034219f716):

&lt;pre&gt;
$ curl -XPOST 'localhost:9200/test/foo' -d '{"field": {"nested": 42}}'
{"ok":true,"_index":"test","_type":"foo","_id":"hHtWqld0RTeRMpQxNeX7bA","_version":1}
$ curl -XPOST 'localhost:9200/test/foo' -d '{"field": "omg"}'         
{"error":"NullPointerException[null]","status":500}
&lt;/pre&gt;

This is on a fresh install of 0.15.2 with no configuration whatsoever.  For reference, the dynamic mapping created by the first `POST` is:

&lt;pre&gt;
$ curl 'localhost:9200/test/foo/_mapping?pretty=true'                 
{
  "test" : {
    "foo" : {
      "properties" : {
        "field" : {
          "dynamic" : "true",
          "properties" : {
            "nested" : {
              "type" : "long"
            }
          }
        }
      }
    }
  }
}
&lt;/pre&gt;

The error that appears on the server side is:

&lt;pre&gt;
[2011-03-19 02:32:49,171][INFO ][node                     ] [Astaroth / Asteroth] {elasticsearch/0.15.2}[25417]: started
[2011-03-19 02:32:53,375][INFO ][cluster.metadata         ] [Astaroth / Asteroth] [test] creating index, cause [auto(index api)], shards [5]/[1], mappings []
[2011-03-19 02:32:54,131][INFO ][cluster.metadata         ] [Astaroth / Asteroth] [test] created and added to cluster_state
[2011-03-19 02:32:54,491][INFO ][cluster.metadata         ] [Astaroth / Asteroth] [test] update_mapping [foo] (dynamic)
[2011-03-19 02:32:56,846][DEBUG][action.index             ] [Astaroth / Asteroth] [test][2], node[wLHdQbsoQS-eSkJNX-CLvw], [P], s[STARTED]: Failed to execute [index {[test][foo][JbZaKPbdTBa2k38UkXf7wA], source[{"field": "omg"}]}]
java.lang.NullPointerException
    at org.elasticsearch.index.mapper.xcontent.ObjectMapper.parse(ObjectMapper.java:308)
    at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:429)
    at org.elasticsearch.index.mapper.xcontent.XContentDocumentMapper.parse(XContentDocumentMapper.java:363)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:250)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:187)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:418)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.access$100(TransportShardReplicationOperationAction.java:233)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:331)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
&lt;/pre&gt;


The bug comes from the fact that the `ObjectMapper.parse` doesn't handle end of file here.  There are two possible behaviors to adopt here:
1. Accept EOF here and Do The Right Thing.
2. Reject EOF with an error because the existing mapping expects another form of data.

In my case I want behavior 1. but I'm fairly new to ES so I don't know if this should be universal or not.  Either way, the NPE is clearly a bug here.
</description><key id="686681">795</key><summary>NPE when the JSON to index doesn't match the mapping's expectations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsuna</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-19T09:45:28Z</created><updated>2011-03-19T18:43:15Z</updated><resolved>2011-03-19T18:43:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tsuna" created="2011-03-19T09:51:50Z" id="892262">The pull request elasticsearch/elasticsearch#796 implements behavior 1 and fixes the problem for me:

&lt;pre&gt;
$ curl -XPOST 'localhost:9200/test/foo' -d '{"field": {"nested": 42}}'
{"ok":true,"_index":"test","_type":"foo","_id":"5DrNKOu0QuizJoyom8A4Tg","_version":1}
$ curl -XPOST 'localhost:9200/test/foo' -d '{"field": "omg"}'         
{"ok":true,"_index":"test","_type":"foo","_id":"XkDiUy10TNeSKRfxF827ZA","_version":1}
&lt;/pre&gt;
</comment><comment author="kimchy" created="2011-03-19T18:42:19Z" id="893415">Behavior 1 is problematic when it comes to elasticsearch, since data you expect to be indexed will not be indexed. Will push implementation for 2.
</comment><comment author="kimchy" created="2011-03-19T18:43:15Z" id="893417">NPE when the JSON to index doesn&amp;#39;t match the mapping&amp;#39;s expectations, closed by fb7fbc8c831a003de15cfd569576a11be1912946.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/ObjectMapper.java</file></files><comments><comment>NPE when the JSON to index doesn't match the mapping's expectations, closes #795.</comment></comments></commit></commits></item><item><title>Small improvements to the startup scripts.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/794</link><project id="" key="" /><description>Just a few minor improvements.
</description><key id="686317">794</key><summary>Small improvements to the startup scripts.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsuna</reporter><labels /><created>2011-03-19T01:30:28Z</created><updated>2014-07-04T22:32:59Z</updated><resolved>2011-04-22T07:39:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-19T18:30:13Z" id="893397">I am concerned if `readlink` is available on different installations / variances of operating systems?
</comment><comment author="tsuna" created="2011-03-20T01:23:54Z" id="894320">Yeah `readlink` isn't the most portable thing, but we're in 2011 and I wasn't too concerned about people running ES on platforms that didn't have a `readlink` utility.  Feel free to ignore this patch if you feel like `readlink` isn't portable enough for ES.
</comment><comment author="tsuna" created="2011-04-22T03:41:15Z" id="1042615">Please take another look.
</comment><comment author="kimchy" created="2011-04-22T07:39:28Z" id="1043101">Pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolator: When deleting an index, make sure to delete all its percolated queries from the _percolator index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/793</link><project id="" key="" /><description>Percolator: When deleting an index, make sure to delete all its percolated queries from the _percolator index
</description><key id="686019">793</key><summary>Percolator: When deleting an index, make sure to delete all its percolated queries from the _percolator index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-18T22:23:53Z</created><updated>2011-03-18T22:24:34Z</updated><resolved>2011-03-18T22:24:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-18T22:24:34Z" id="891362">Percolator: When deleting an index, make sure to delete all its percolated queries from the _percolator index, closed by 70fc8d9af0574c0ad3fa3da8916078034219f716.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/percolator/SimplePercolatorTests.java</file></files><comments><comment>Percolator: When deleting an index, make sure to delete all its percolated queries from the _percolator index, closes #793.</comment></comments></commit></commits></item><item><title>Add a has_parent query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/792</link><project id="" key="" /><description>Would return children objects similar to how the has_child query returns parent objects.
</description><key id="685918">792</key><summary>Add a has_parent query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merrellb</reporter><labels /><created>2011-03-18T21:43:48Z</created><updated>2012-09-13T14:14:23Z</updated><resolved>2012-09-13T14:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dstendardi" created="2011-06-08T09:19:54Z" id="1325515">+1 this feature would be very useful !
</comment><comment author="nhuray" created="2012-03-10T00:22:42Z" id="4427416">@hlian

I've just tested it and It works like a charm !

It's a really nice feature, congratulations.
</comment><comment author="dstendardi" created="2012-03-27T14:11:20Z" id="4717138">Great feature. thank you hlian !
</comment><comment author="fcheung" created="2012-03-29T15:43:40Z" id="4824085">This looks really useful, thanks. Is this ready for prime time or still a work in progress? Have you approached Shay about getting this merged in?
</comment><comment author="hlian" created="2012-04-02T20:10:50Z" id="4887249">Thank you for the kind words, guys. @kevingessner and I are crazy busy right now getting a release out and @flxs is doing a lot of the hard work to get the stuff merge-ready, so I'd like to bless his branch as the place for has_parent development to happen. I'd be happy to answer questions about the implementation, but I'm afraid either of us can't commit to writing more code for this feature or to spending time shepherding it into trunk.
</comment><comment author="jmchambers" created="2012-04-08T03:27:29Z" id="5013193">I just tried @flxs's branch and can confirm that this filter is officially awesome :)

It has allowed me to redesign my un-scalable app (relying on frequent and expensive re-indexing of docs + nested docs) to a fully scalable one using parent &amp; child docs instead.

Thanks to @hilan, @kevingessner and @flxs for getting it working - hope it finds its way into trunk soon.

BTW, if there are any other java-illiterates reading this thread who, like me, can't figure out how to get the download working, you need to make sure all dependencies are assembled in the target/lib directory by running the following (from the root folder): 

$ mvn prepare-package -DskipTests

and then run ES with:

$ ES_CLASSPATH="target/classes:target/lib/*" bin/elasticsearch -f

(from http://bit.ly/HKmckR)

Thanks again.
</comment><comment author="r10r" created="2012-06-03T23:00:31Z" id="6089891">I've added a gist for testing the _has_parent_ search filter from commandline using _curl_ against the json API: https://gist.github.com/2865256.
(Just modified the _has_child_ gist from @ToulBoy slightly https://gist.github.com/1941432)
</comment><comment author="Vineeth-Mohan" created="2012-07-01T15:33:15Z" id="6694322">+1
</comment><comment author="jmchambers" created="2012-07-01T16:14:36Z" id="6694681">**UPDATE** I've narrowed down the problem. If I create the index and add some parent and child docs, the has_parent filter doesn't work until I restart the server. However, this only works once. If I add additional child docs, or if I update the existing ones, the results of the has_parent filter don't reflect the changes. It's as if the filter is working with a stale copy of the data set - very weird...

---

I was wondering if someone on this thread could help me. The has_parent fork was working really nicely, but has now suddenly stopped working - which has left me thoroughly confused!

I haven't changed anything as far as I know. I downloaded and built @flxs's branch 3 months ago and it worked just fine. But when I revisited my test index today it seems to be broken. The has_child filter continues to work as normal, but has_parent seems to ignore newly indexed documents. Sometimes restarting the server will make it pick them up but mostly not.

Can anyone think why this might be? Does ES have runtime dependencies that might have been changed by system updates (I'm on Ubuntu 11.04)? 
</comment><comment author="nickhoffman" created="2012-07-04T14:15:15Z" id="6761207">It looks like this solves the exact problem I'm tackling right now. Any idea when it'll be ready for prime time?
</comment><comment author="nhuray" created="2012-09-01T08:06:59Z" id="8211014">Hi everyone,

I'm very interested by this filter, so I'd like to know if you consider it's ready for production or if it is still unstable.
I can help if you want to review the code and add tests if necessary.

@jmchambers : Did you fix your troubles since 2 month or do you need help to investigate the problem ?

@kimchy : Do you have any plan to integrate the has_parent filter in 0.20 ?

Cheers,
</comment><comment author="jeremydaly" created="2012-09-08T19:55:00Z" id="8397214">I really need this capability.
</comment><comment author="martijnvg" created="2012-09-13T14:02:36Z" id="8529011">This has been pushed in #2243 and will be available 0.19.10
</comment><comment author="jmchambers" created="2012-09-13T14:14:23Z" id="8529384">great news, thanks for getting this in the main release.

@nhuray afraid I didn't get it working, but I'll give it another try with 0.19.10, thanks for asking though.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/CacheRecycler.java</file><file>src/main/java/org/elasticsearch/index/cache/id/IdReaderTypeCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdReaderTypeCache.java</file><file>src/main/java/org/elasticsearch/index/query/FilterBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/HasParentFilter.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesRegistry.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/test/integration/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Added `has_parent` filter (#2243)</comment></comments></commit></commits></item><item><title>Delete IndexAPI: Allow to delete more than one index or _all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/791</link><project id="" key="" /><description>Delete IndexAPI: Allow to delete more than one index or _all indices. For example:

```
# delete test1 and test2
curl -XDELETE localhost:9200/test1,test2
# delete all indices
curl -XDELETE localhost:9200/
```
</description><key id="685345">791</key><summary>Delete IndexAPI: Allow to delete more than one index or _all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-18T17:52:49Z</created><updated>2011-03-18T18:45:13Z</updated><resolved>2011-03-18T18:45:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-18T18:45:13Z" id="890472">Delete IndexAPI: Allow to delete more than one index or _all indices, closed by bd3f490d605e61454e1f98bb825d07dfb0bd2690.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/admin/indices/delete/DeleteIndexRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/admin/indices/delete/RestDeleteIndexAction.java</file></files><comments><comment>Delete IndexAPI: Allow to delete more than one index or _all indices, closes #791.</comment></comments></commit></commits></item><item><title>Percolator failure when deleting and creating an index, also causes problem with percolated queries isolation between different indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/790</link><project id="" key="" /><description>The problem was that the real time updater of percolated queries was not being removed correctly from indices that are deleted.

Also, it was being applied not only on the specific index they relate to.

Details here: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/cf7aba9ef1032a8a#. Will update the issue with more details once its tracked down.
</description><key id="683392">790</key><summary>Percolator failure when deleting and creating an index, also causes problem with percolated queries isolation between different indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-03-18T00:05:03Z</created><updated>2011-03-18T08:49:42Z</updated><resolved>2011-03-18T08:49:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-18T08:49:41Z" id="888544">Percolator failure when deleting and creating an index, also causes problem with percolated queries isolation between different indices, closed by 923fcf239c4b5a9a7501789d657c09bed50b7278.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/percolator/SimplePercolatorTests.java</file></files><comments><comment>Percolator failure when deleting and creating an index, also causes problem with percolated queries isolation between different indices, closes #790.</comment></comments></commit></commits></item><item><title>"exclude" functionality missing for terms facet on numeric fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/789</link><project id="" key="" /><description>In 0.15, the exclude functionally was not implemented for terms facet on numeric fields. Please implement this.

See original discussion here: http://groups.google.com/a/elasticsearch.com/group/users/browse_thread/thread/68084193989b2765
</description><key id="682628">789</key><summary>"exclude" functionality missing for terms facet on numeric fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ogg</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-17T19:21:59Z</created><updated>2011-03-18T09:08:25Z</updated><resolved>2011-03-18T09:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-18T09:08:25Z" id="888585">&amp;quot;exclude&amp;quot; functionality missing for terms facet on numeric fields, closed by 09fbc919b83f3a3beca90cbc9fa488d951a9e677.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/TermsFacetProcessor.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/bytes/TermsByteFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/doubles/TermsDoubleFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/floats/TermsFloatFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/ints/TermsIntFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/longs/TermsLongFacetCollector.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/facet/terms/shorts/TermsShortFacetCollector.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/facet/SimpleFacetsTests.java</file></files><comments><comment>"exclude" functionality missing for terms facet on numeric fields, closes #789.</comment></comments></commit></commits></item><item><title>`auto_create_index` shouldn't apply to percolators or rivers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/788</link><project id="" key="" /><description>Hiya

I think that `action.auto_create_index: 0` shouldn't apply to "special" indices, like `_percolator` which get created automatically with settings specified internally.

Rather should just apply to normal indices

clint
</description><key id="681780">788</key><summary>`auto_create_index` shouldn't apply to percolators or rivers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-17T14:40:18Z</created><updated>2014-07-03T18:23:10Z</updated><resolved>2014-07-03T18:23:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajhalani" created="2012-04-04T23:15:43Z" id="4965327">+1
</comment><comment author="nik9000" created="2014-02-05T15:05:21Z" id="34183391">Just hit this this morning.  Tried to use a river in a puppet configured environment and our puppet sets `action.auto_create_index` to `false`.
</comment><comment author="clintongormley" created="2014-07-03T18:23:10Z" id="47966512">Rethinking this - for security's sake, if you disable automatic index creation, it's because you don't want indices created automatically...

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query: Provide an option to analyze wildcard/prefix in query_string / field queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/787</link><project id="" key="" /><description>Add a flag called `analyze_wildcard` to both `query_string` and `field` queries, once set, a best effort will be made to analyze wildcard and prefix queries as well.

More details:

When we use an analyzer that stems terms into tokens, and then later we want to search against those analyzed terms using a wildcard, by default the search terms are not analyzed, as that analysis could lead into several tokens and the search engine would not be sure which one to use:

http://www.jguru.com/faq/view.jsp?EID=538312

However, in certain circumstances, when the liability of the search can be somehow constrained in favor of better expected results, it would be nice to tell the search engine to analyze the wildcard terms before executing the search, therefore allowing for a more precise search (at least, expected).

Let's put here an example with the Spanish analyzer (which uses the snowball stemmer):
- We index the phrase "I have an iPhone"
- We index the phrase "I love the triad iPad/iPhone/iPod"
- We index the phrase "I found the perfect combination: iPhone/MP3"

If we use the standard current 'query_string', when searching for "*phone*", we will only get the last phrase, due to the way in which the terms have been analyzed:

"I have an iPhone":

{"tokens":[{"token":"i","start_offset":0,"end_offset":1,"type":"&lt;ALPHANUM&gt;","position":1},{"token":"hav","start_offset":2,"end_offset":6,"type":"&lt;ALPHANUM&gt;","position":2},{"token":"an","start_offset":7,"end_offset":9,"type":"&lt;ALPHANUM&gt;","position":3},{"token":"iphon","start_offset":10,"end_offset":16,"type":"&lt;ALPHANUM&gt;","position":4}]}

"I love the triad iPad/iPhone/iPod":

{"tokens":[{"token":"i","start_offset":0,"end_offset":1,"type":"&lt;ALPHANUM&gt;","position":1},{"token":"lov","start_offset":2,"end_offset":6,"type":"&lt;ALPHANUM&gt;","position":2},{"token":"the","start_offset":7,"end_offset":10,"type":"&lt;ALPHANUM&gt;","position":3},{"token":"tri","start_offset":11,"end_offset":16,"type":"&lt;ALPHANUM&gt;","position":4},{"token":"ipad","start_offset":17,"end_offset":21,"type":"&lt;ALPHANUM&gt;","position":5},{"token":"iphon","start_offset":22,"end_offset":28,"type":"&lt;ALPHANUM&gt;","position":6},{"token":"ipod","start_offset":29,"end_offset":33,"type":"&lt;ALPHANUM&gt;","position":7}]}

"I found the perfect combination: iPhone/MP3":

{"tokens":[{"token":"i","start_offset":0,"end_offset":1,"type":"&lt;ALPHANUM&gt;","position":1},{"token":"found","start_offset":2,"end_offset":7,"type":"&lt;ALPHANUM&gt;","position":2},{"token":"the","start_offset":8,"end_offset":11,"type":"&lt;ALPHANUM&gt;","position":3},{"token":"perfect","start_offset":12,"end_offset":19,"type":"&lt;ALPHANUM&gt;","position":4},{"token":"combination","start_offset":20,"end_offset":31,"type":"&lt;ALPHANUM&gt;","position":5},{"token":"iphone/mp3","start_offset":33,"end_offset":43,"type":"&lt;NUM&gt;","position":6}]}

See how the latter stems "iPhone/MP3" as "iphone/mp3"? Hence this is the only one matching a 'query_string' equal to "*phone*" (and similar 'unexpected' results occur when using just one leading or trailing wildcard as well).

This result would be dissapointing for the user, as she'd expect at least something like "iPhone" or even "telephone" to be returned as a result, but due to fact that the Spanish analyzer will always remove the trailing 'e' from most of the words, it won't find them.

So enhancement would be to provide a mechanism, in the form of a parameter, for instance, in the 'query_string', that would tell the ES query parser to analyze those search terms surrounded by wildcards (i.e. either enclosed completely, or just with a leading or trailing wildcard).

Following our previous example, a 'query_string' for "*phone*" would be actually analyzed in the Spanish analyzer as "*phon*" therefore returning absolutely all the phrases previously created, which would be the expected and reasonable behaviour from a user's perspective. Of course, it could have some side effects on other searches, but as a parameter, it would be up to the search designer to either use it or not.
</description><key id="681224">787</key><summary>Query: Provide an option to analyze wildcard/prefix in query_string / field queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">emedina</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-17T09:54:50Z</created><updated>2011-03-17T20:01:35Z</updated><resolved>2011-03-17T20:01:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-17T20:01:35Z" id="886613">Query: Provide an option to analyze wildcard/prefix in query_string / field queries, closed by 25124b084bcbb59bbb123e534386412fbfab1585.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryStringTests.java</file></files><comments><comment>Query: add option for analyze wildcard/prefix also to simple_query_string query</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/queryParser/QueryParserSettings.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FieldQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/FieldQueryParser.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryStringQueryBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/query/xcontent/QueryStringQueryParser.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/query/SimpleQueryTests.java</file></files><comments><comment>Query: Provide an option to analyze wildcard/prefix in query_string / field queries, closes #787.</comment></comments></commit></commits></item><item><title>Add API to add a block to index and block writes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/786</link><project id="" key="" /><description>Discussed in 
http://elasticsearch-users.115913.n3.nabble.com/Intercept-transport-client-http-client-actions-tp2685929p2685929.html
</description><key id="681039">786</key><summary>Add API to add a block to index and block writes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels /><created>2011-03-17T07:50:02Z</created><updated>2013-04-04T19:21:22Z</updated><resolved>2013-04-04T19:21:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:21:22Z" id="15917990">Index setting `index.blocks.read_only` supports this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A mechanism to intercept transport client/http client actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/785</link><project id="" key="" /><description>Discussed in 
http://elasticsearch-users.115913.n3.nabble.com/Intercept-transport-client-http-client-actions-tp2685929p2685929.html
</description><key id="679860">785</key><summary>A mechanism to intercept transport client/http client actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels /><created>2011-03-16T21:13:36Z</created><updated>2013-04-04T19:21:59Z</updated><resolved>2013-04-04T19:21:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shadow000fire" created="2012-05-15T13:09:11Z" id="5715285">Is it possible to do this with a custom Analyzer?
</comment><comment author="clintongormley" created="2013-04-04T19:21:59Z" id="15918025">This would need to be done in a proxy or client
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>WIldcard not working with snowball stemmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/784</link><project id="" key="" /><description>Hiya

A query string search with a wildcard on a field that has passed through the snowball stemmer is not working correctly.

Text indexed: `I have an iPhone`
Search for: `iphone` works, but search for `iphone*` doesn't

```
# [Wed Mar 16 16:02:34 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "doc" : {
         "properties" : {
            "text" : {
               "type" : "string",
               "analyzer" : "spanish"
            }
         }
      }
   },
   "settings" : {
      "analysis" : {
         "analyzer" : {
            "spanish" : {
               "language" : "Spanish",
               "type" : "snowball"
            }
         }
      }
   }
}
'

# [Wed Mar 16 16:02:34 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Wed Mar 16 16:02:39 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XPOST 'http://127.0.0.1:9200/test/doc?pretty=1'  -d '
{
   "text" : "I have an iPhone"
}
'

# [Wed Mar 16 16:02:39 2011] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "imN8_G5rTwGwuESyTEz8pg",
#    "_type" : "doc",
#    "_version" : 1
# }

# [Wed Mar 16 16:02:45 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/test/doc/_search?pretty=1'  -d '
{
   "query" : {
      "field" : {
         "text" : "iphone"
      }
   }
}
'

# [Wed Mar 16 16:02:45 2011] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "text" : "I have an iPhone"
#             },
#             "_score" : 0.15342641,
#             "_index" : "test",
#             "_id" : "imN8_G5rTwGwuESyTEz8pg",
#             "_type" : "doc"
#          }
#       ],
#       "max_score" : 0.15342641,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }

# [Wed Mar 16 16:02:48 2011] Protocol: http, Server: 192.168.5.103:9200
curl -XGET 'http://127.0.0.1:9200/test/doc/_search?pretty=1'  -d '
{
   "query" : {
      "field" : {
         "text" : "iphone*"
      }
   }
}
'

# [Wed Mar 16 16:02:48 2011] Response:
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : null,
#       "total" : 0
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```
</description><key id="678671">784</key><summary>WIldcard not working with snowball stemmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-16T15:06:57Z</created><updated>2013-04-04T19:23:18Z</updated><resolved>2013-04-04T19:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-03-16T15:09:33Z" id="880878">Note, with the spanish stemmer, `iphone` gets stemmed to `iphon`
</comment><comment author="emedina" created="2011-03-16T16:00:50Z" id="881132">Shay, this bug is only happening when you use an analyzer which stems and the query uses wildcards; if no stemmer is used then it works properly.

So the bug pops up when you use wildcards in a search, and a stemmer was used to index the original terms.
</comment><comment author="clintongormley" created="2013-04-04T19:23:18Z" id="15918098">The `analyze_wildcard` parameter was added to attempt to deal with these situations, but wildcard + analyzer is pronte to errors.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Capability: index/shards using memory +FS/S3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/783</link><project id="" key="" /><description>For any deployment, it will help to have capability to store index/shards in memory (typical servers can allow 2-11GB RAM for ES), as well as writing to FS or S3. Having both will help from performance (with lot of RAM) &amp; data integrity (storage to FS/S3).

Additional can be capability to encrypt index on FS/S3 our of  box.

Thanks!

Best Regards,
Aditya
</description><key id="678570">783</key><summary>Capability: index/shards using memory +FS/S3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adityainnovation</reporter><labels /><created>2011-03-16T14:21:46Z</created><updated>2013-04-04T19:23:59Z</updated><resolved>2013-04-04T19:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:23:59Z" id="15918131">It pretty much works this way already
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>faceting _parent field produces mangled ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/782</link><project id="" key="" /><description>When faceting the "_parent" field I don't get the parent docid (eg. "12345") as my terms but instead a mangled form (eg "doc#12345")

If you have problems recreating let me know and I will work on a curl example
</description><key id="677573">782</key><summary>faceting _parent field produces mangled ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merrellb</reporter><labels /><created>2011-03-16T05:23:35Z</created><updated>2013-04-04T19:26:45Z</updated><resolved>2013-04-04T19:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T19:26:44Z" id="15918293">That's how it is stored. Is this a problem?  After two years of inactivity, closing this bug report.  Please reopen if this is a real issue that can't be resolved some other way
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster Health API: Asking for health on an index that does not exists should return RED status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/781</link><project id="" key="" /><description>Cluster Health API: Asking for health on an index that does not exists should return RED status
</description><key id="676394">781</key><summary>Cluster Health API: Asking for health on an index that does not exists should return RED status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-03-15T19:16:13Z</created><updated>2011-03-15T19:23:00Z</updated><resolved>2011-03-16T02:23:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-15T19:23:00Z" id="877608">Cluster Health API: Asking for health on an index that does not exists should return RED status, closed by 96d06d6dc8611d01bddeebef4a338ca4464c7b6d.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/cluster/ClusterHealthTests.java</file></files><comments><comment>Cluster Health API: Asking for health on an index that does not exists should return RED status, closes #781.</comment></comments></commit></commits></item><item><title>Set initial version when creating a new doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/780</link><project id="" key="" /><description>When reindexing from an old index to new, the `version` would currently be reset to `1`. If you try to set a different version number, you get a conflict.

I'm wondering if, when creating a new doc (not indexing) you should be able to set the initial version number?

Actually, while reindexing, the original docs might have been updated more than once, so when we finish reindexing, and we do a final sweep over all docs that have changed since we started, we will get version conflicts.

Perhaps we need a flag which says "override the existing version and set this new version instead ) but possibly only if it is greater.

what do you think?

clint
</description><key id="673139">780</key><summary>Set initial version when creating a new doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-14T20:41:11Z</created><updated>2011-03-15T17:16:47Z</updated><resolved>2011-03-16T00:16:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-03-14T20:43:54Z" id="872082">Sorry - the reason why i suggest a flag instead of just indexing the new doc without a version is so that we can preserve the current version for an application which relies on it
</comment><comment author="kimchy" created="2011-03-15T10:34:38Z" id="875641">With the new `external` version type option in master, you can use that to explicitly define the version of the doc. 
</comment><comment author="clintongormley" created="2011-03-15T12:01:15Z" id="875858">Does internal versioning then take over? ie you can use `external` just for setting the initial version?
</comment><comment author="kimchy" created="2011-03-15T16:05:46Z" id="876775">Yep. Though the more I think about it, I think that you only need to do that when you actually use external versioning to start with
</comment><comment author="clintongormley" created="2011-03-15T17:16:47Z" id="877124">OK, so on the initial create operation on the new index, you would provide `"version_type": "external"` and `"version": 1234`, and from there on, ES would take over with its own internal versioning.

perfect - thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search type 'scan' not scrolling correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/779</link><project id="" key="" /><description>Hiya

On the latest from master, I find that scrolling doesn't work past the first request.  

First, the total reported is incorrect, and second, it throws an error on the second `scroll` request.

See the recreation here: https://gist.github.com/d0934ef71b0d20469fc9

I've tried the same thing with and without `sort` and without `search_type=scan` and none of it works correctly.  At least without  `search_type=scan` I get the correct total, but it still fails on the second scroll request

thanks

clint
</description><key id="672373">779</key><summary>Search type 'scan' not scrolling correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-14T18:04:18Z</created><updated>2011-03-20T14:31:10Z</updated><resolved>2011-03-20T14:31:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2011-03-20T14:31:10Z" id="895256">Incorrect usage of scroll - it works
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk Request: Streamline parameters names to be the same as the single REST request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/778</link><project id="" key="" /><description>This seem to confuse some people, so:

Allow to provide `routing` (on top of `_routing`), `parent` (on top of `_parent`), `version` (on top of `_version`), and `version_type` (on top of `_version_type`).
</description><key id="669028">778</key><summary>Bulk Request: Streamline parameters names to be the same as the single REST request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-13T09:43:07Z</created><updated>2011-03-13T09:43:41Z</updated><resolved>2011-03-13T09:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-13T09:43:41Z" id="865411">Bulk Request: Streamline parameters names to be the same as the single REST request, closed by 96dfdcf97cda93c21d085788aaf927a34a7ac896.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file></files><comments><comment>Bulk Request: Streamline parameters names to be the same as the single REST request, closes #778.</comment></comments></commit></commits></item><item><title>Conflicts in put mapping only reported for properties field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/777</link><project id="" key="" /><description>When putting a mapping, if there is a conflict in `properties`, an exception is thrown.  However, any conflicts in the other parameters ( eg `_all`, `dynamic` etc) are silently ignored.

These should also throw errors.

```
# [Sat Mar 12 19:38:18 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/' 

# [Sat Mar 12 19:38:18 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Mar 12 19:38:21 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/bar/_mapping'  -d '
{
   "bar" : {
      "_all" : {
         "enabled" : 0
      },
      "dynamic" : 0,
      "properties" : {
         "text" : {
            "type" : "string"
         }
      }
   }
}
'

# [Sat Mar 12 19:38:21 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Mar 12 19:38:24 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/bar/_mapping'  -d '
{
   "bar" : {
      "_all" : {
         "enabled" : 1
      },
      "dynamic" : 1,
      "properties" : {
         "text" : {
            "type" : "string"
         }
      }
   }
}
'

# [Sat Mar 12 19:38:24 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }
```
</description><key id="667977">777</key><summary>Conflicts in put mapping only reported for properties field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label></labels><created>2011-03-12T18:40:35Z</created><updated>2014-09-12T09:53:37Z</updated><resolved>2014-09-08T15:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file><file>src/test/java/org/elasticsearch/timestamp/SimpleTimestampTests.java</file></files><comments><comment>_timestamp: enable mapper properties merging</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/AllMapperOnCusterTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnCusterTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file></files><comments><comment>_all: report conflict on merge and throw exception on doc_values</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file></files><comments><comment>_ttl: Report conflict when trying to disable _ttl</comment></comments></commit></commits></item><item><title>Versioning: Add a new version_type parameter, with a new type - `external`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/776</link><project id="" key="" /><description>Add a new `version_type` parameter, supporting a new versioning behavior. The current versioning behavior is called `internal`, the new one named `external`.

The `external` version type allows to provide an external _numeric_ version value (that might be maintained by an external system, like optimistic locking logic in a database). 

When provided, by setting the `version` value and the `version_type`  to `external`, the check will be if the provided version is greater than the current document version. If so, it will be set as the document version value and the document will be indexed. If the current document version is higher than the provided version, a version conflict will be returned / thrown.

This means that async indexing as a result of operations done against the database can use the database versioning scheme and there is no need to maintain strict ordering in the async indexing process.

Or even the simple case of updating the db, and then indexing into elasticsearch is now simplified since if the indexing gets out of order, the external versioning can be used to make sure only the latest version is indexed.
</description><key id="667968">776</key><summary>Versioning: Add a new version_type parameter, with a new type - `external`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-12T18:34:52Z</created><updated>2011-05-21T09:15:03Z</updated><resolved>2011-03-13T02:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-12T18:42:09Z" id="864104">Versioning: Add a new version_type parameter, with a new type - `external`, closed by bdb0fd23e9402da55d6bb98ef75a53930c0226d6.
</comment><comment author="apatrida" created="2011-05-21T00:53:56Z" id="1213275">Does versioning work on deletes by id?  For an example with an external version number, you will only want to delete if the version number is less than that of the delete (i.e. assuming it is a timestamp).  If it isn't, you want the delete to fail.  Optimistic concurrency should work in deletes, not just updates.
</comment><comment author="kimchy" created="2011-05-21T09:13:00Z" id="1213970">It does, but deletes gets expunged after a certain time.
</comment><comment author="apatrida" created="2011-05-21T09:15:02Z" id="1213972">so a delete itself can be blocked by a higher version in the existing record?

and a delete can block an update by having its own version set (external) than an incoming update?

If both are true, then dandy.  Only expunging breaks the protection...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/delete/DeleteRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/delete/DeleteRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/index/IndexRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/VersionType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/SourceToParse.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/engine/AbstractSimpleEngineTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/versioning/SimpleVersioningTests.java</file></files><comments><comment>Versioning: Add a new version_type parameter, with a new type - `external`, closes #776.</comment></comments></commit></commits></item><item><title>Support true|false for the 'store' parameter in mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/775</link><project id="" key="" /><description>Currently `store` only accepts `yes` and `no`, which are the Lucene values, but is inconsistent with other boolean parameters in ES.
</description><key id="667958">775</key><summary>Support true|false for the 'store' parameter in mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-12T18:24:16Z</created><updated>2011-03-12T22:42:09Z</updated><resolved>2011-03-12T22:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-12T22:41:44Z" id="864599">It will also support `0` for false, `1` for true, as all other boolean values.
</comment><comment author="kimchy" created="2011-03-12T22:42:09Z" id="864600">Support true|false for the &amp;#39;store&amp;#39; parameter in mappings, closed by 09006f17c3b517e40a8cd562dc5a43bde35d5839.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/XContentTypeParsers.java</file></files><comments><comment>Support true|false for the 'store' parameter in mappings, closes #775.</comment></comments></commit></commits></item><item><title>Cannot specify fields=&lt;blank&gt; when getting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/774</link><project id="" key="" /><description>```
# [Sat Mar 12 17:45:57 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/' 

# [Sat Mar 12 17:45:57 2011] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Sat Mar 12 17:45:57 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XPUT 'http://127.0.0.1:9200/foo/bar/1?refresh=true'  -d '
{
   "text" : "foo"
}
'

# [Sat Mar 12 17:45:57 2011] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "1",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Sat Mar 12 17:45:57 2011] Protocol: http, Server: 127.0.0.1:9200
curl -XGET 'http://127.0.0.1:9200/foo/bar/1?fields=' 

# [Sat Mar 12 17:45:57 2011] Response:
# {
#    "status" : 500,
#    "error" : "RemoteTransportException[[Beetle][inet[/127.0.0.1:9
# &gt;    302]][indices/get/shard]]; nested: ElasticSearchException[No
# &gt;     mapping for field [] in type [bar]]; "
# }
```
</description><key id="667827">774</key><summary>Cannot specify fields=&lt;blank&gt; when getting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2011-03-12T16:47:16Z</created><updated>2011-03-14T01:35:35Z</updated><resolved>2011-03-14T01:35:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-12T22:38:28Z" id="864590">Really strange, running your curl samples it works well for me. 

Seems like it gets an empty field, which leads me to think that maybe an empty string is passed. I pushed handling of a case where the fields only has whitespaces then it will be treated as "no fields". Can you give it a go?
</comment><comment author="clintongormley" created="2011-03-14T01:35:35Z" id="867464">Fixed 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `max_shard_index_buffer_size` to control the upper limit of a shard indexing buffer size (defaults to `512mb`)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/773</link><project id="" key="" /><description>Add `max_shard_index_buffer_size` to control the upper limit of a shard indexing buffer size (defaults to `512mb`).
</description><key id="667699">773</key><summary>Add `max_shard_index_buffer_size` to control the upper limit of a shard indexing buffer size (defaults to `512mb`)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-12T15:02:38Z</created><updated>2011-03-12T15:03:15Z</updated><resolved>2011-03-12T15:03:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-12T15:03:15Z" id="863698">Add `max_shard_index_buffer_size` to control the upper limit of a shard indexing buffer size (defaults to `512mb`), closed by 73dec5d63f152336fbbb53209f21fa91b0b6bf41.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryBufferController.java</file></files><comments><comment>Add `max_shard_index_buffer_size` to control the upper limit of a shard indexing buffer size (defaults to `512mb`), closes #773.</comment></comments></commit></commits></item><item><title>Sort: Support "missing" specific handling, include _last, _first, and custom value (for numeric values)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/772</link><project id="" key="" /><description>Support specific handling for missing numeric field values, including `_last`, `_first`, and custom values. The following is an example:

```
{
    "sort" : [
        { "price" : {"missing" : "_last"} },
    ],
    "query" : {
        "term" : { "user" : "kimchy" }
    }
}
```

Or, another sample:

```
{
    "sort" : [
        { "price" : {"missing" : 1.2} },
    ],
    "query" : {
        "term" : { "user" : "kimchy" }
    }
}
```
</description><key id="667519">772</key><summary>Sort: Support "missing" specific handling, include _last, _first, and custom value (for numeric values)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfiedler</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-12T12:21:02Z</created><updated>2011-05-03T13:16:07Z</updated><resolved>2011-03-13T00:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-13T00:29:18Z" id="864800">Sort: Support &amp;quot;missing&amp;quot; specific handling, include _last, _first, and custom value (for numeric values), closed by 700a2a9577ac8997684b76613d43f96eeb3f74ce.
</comment><comment author="jfiedler" created="2011-05-02T14:16:05Z" id="1088115">I did some preliminary testing with this. It seems to work for integer attributes but I have seen problems with a reverse double sort with 'missing last' (see assertion output below). I can work on a curl reproduction if you want but maybe the message below is sufficient. I logged the ES response and found the sort value being returned to be  "sort" : [ 4.9E-324 ] for the document carrying the 'null'.

junit.framework.AssertionFailedError: expected:&lt;[9.0, 5.2, 5.1, 1.0, -4.0, null]&gt; but was:&lt;[9.0, 5.2, 5.1, 1.0, null, -4.0]&gt;
</comment><comment author="kimchy" created="2011-05-02T17:27:17Z" id="1089316">A curl recreation would help, had a quick look again and it seems good (the logic).
</comment><comment author="jfiedler" created="2011-05-03T08:17:12Z" id="1092588">I created a curl reproduction: https://gist.github.com/952984
Looking at the ES code I assume the 'null' replacement value Double.MIN_VALUE is wrong. The JavaDoc for it reads: 

&gt; "A constant holding the smallest &lt;b&gt;positive&lt;/b&gt; nonzero value of type ...". 

You probably want to use Double.NEGATIVE_INFINITY and Double.POSITIVE_INFINITY respectively.
</comment><comment author="kimchy" created="2011-05-03T13:16:07Z" id="1093776">Created #899, will push a fix soon. Nice catch on the actual bug!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/DoubleFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/FloatFieldDataType.java</file></files><comments><comment>Sort missing wrongly sorts negative values, closes #772.</comment></comments></commit><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/FieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/ByteFieldDataMissingComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/bytes/ByteFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/DoubleFieldDataMissingComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/doubles/DoubleFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/FloatFieldDataMissingComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/floats/FloatFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/IntFieldDataMissingComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/ints/IntFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/LongFieldDataMissingComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/longs/LongFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/ShortFieldDataMissingComparator.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/shorts/ShortFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/field/data/strings/StringFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/geo/GeoPointFieldDataType.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/FieldSortBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/ScoreSortBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/ScriptSortBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/SortBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/sort/SimpleSortTests.java</file></files><comments><comment>Sort: Support "missing" specific handling, include _last, _first, and custom value (for numeric values), closes #772.</comment></comments></commit></commits></item><item><title>Highlighter returns the whole document and no fragments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/771</link><project id="" key="" /><description>```
eval $(curl https://gist.github.com/raw/866323/40e34f9412494f7b48c4d3803921c5ae1dce990e/gistfile1.txt)
```

The search returns the whole document in the highlighter, which is not what it should do. If I do a search on a different word like `only` it works as expected.

It really depends on the query. It seems like that if there is an underscore in the result, it messes things up.

Version is 0.15.2

Moritz
</description><key id="666134">771</key><summary>Highlighter returns the whole document and no fragments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monken</reporter><labels /><created>2011-03-11T19:07:07Z</created><updated>2013-04-04T19:32:01Z</updated><resolved>2013-04-04T19:32:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="monken" created="2011-03-11T19:17:52Z" id="861619">Doesn't happen when you add `"term_vector" : "with_positions_offsets"` to the property definition
</comment><comment author="kimchy" created="2011-03-12T08:53:03Z" id="863241">Yea, seems like its a problem in the Highlighter implementation in Lucene, while the FastVectorHighlighter does not suffer from it (thats the one used when you add the term_vector to the picture).
</comment><comment author="clintongormley" created="2013-04-04T19:32:01Z" id="15918575">This appears to have been fixed in Lucene
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DeleteByQuery does not delete when multiple indices and types given</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/770</link><project id="" key="" /><description>Issue is created with jdk1.6.04 and elasticsearch-0.15.2

Create an entry with type "type1" in index "index1"

curl -XPUT 'http://localhost:9200/index1/type1/1' -d '{"user":"sezgin"}'

Create an entry with type "type2" in index "index2"

curl -XPUT 'http://localhost:9200/index2/type2/1' -d '{"user":"sezgin"}'

Create an alias for index "index1"

curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "add" : { "index" : "index1", "alias" : "alias1" } }
    ]
}'

The following results are received with delete by query action:

http://localhost:9200/alias1/_query?q=_:_             Deletes all entries with type "type1"

http://localhost:9200/alias1/type1/_query?q=_:_             Deletes all entries with type "type1"

http://localhost:9200/alias1,index2/_query?q=_:_             Deletes all entries with type "type1" and "type2"

http://localhost:9200/alias1,index2/type1,type2/_query?q=_:_             Does not delete any entries
</description><key id="665362">770</key><summary>DeleteByQuery does not delete when multiple indices and types given</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Olric</reporter><labels /><created>2011-03-11T14:27:10Z</created><updated>2013-04-04T19:32:21Z</updated><resolved>2013-04-04T19:32:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-12T08:38:50Z" id="863235">If you check the logs in elasticsearch in this case (or in the result for the delete, which indicate 5 failed shards), you will see the problem. It fails to delete since because of type missing failure. For example, `type1` is missing in `index2`.
</comment><comment author="Olric" created="2011-03-14T09:24:01Z" id="868283">One of my unit tests found this. It was working in 0.14.x releases. I thought there might be a bug
</comment><comment author="kimchy" created="2011-03-14T09:57:42Z" id="868353">I see. Well, its a good question, what should be done in this case. It is now consistent and a type missing will be thrown if you try to access one that is missing.
</comment><comment author="Olric" created="2011-03-14T12:43:12Z" id="868713">Ok, thanks...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search / Get: Allow to specify a preference on which shards (or order) it will be executed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/769</link><project id="" key="" /><description>Allow to set `preference` when performing a search or get operation. By default, the operation is randomized across shards to be executed.

The `preference` can be set to:
- `_primary`: The operation will go and be executed only on the primary shards.
- `_local`: The operation will prefer to be executed on a local allocated shard is possible.
- Custom (string) value: A custom value will be used to guarantee that the same shards will be used for the same custom value. This can help with "jumping values" when hitting different shards in different refresh states. A sample value can be something like the web session id, or the user name.

Discussed in
http://elasticsearch-users.115913.n3.nabble.com/Bulk-indexing-and-search-with-two-different-threads-tp2664361p2664361.html
</description><key id="665336">769</key><summary>Search / Get: Allow to specify a preference on which shards (or order) it will be executed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enguzekli</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-11T14:15:18Z</created><updated>2011-03-14T09:30:20Z</updated><resolved>2011-03-14T09:30:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-14T09:30:20Z" id="868297">Search / Get: Allow to specify a preference on which shards (or order) it will be executed, closed by 1bcd3b67ee9f3462d6cac310b3be9a952f154b48.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/admin/cluster/ping/broadcast/TransportBroadcastPingAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/get/GetRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/single/shard/SingleShardOperationRequest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/get/GetRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/client/action/search/SearchRequestBuilder.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/OperationRouting.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/hash/djb/DjbHashFunction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/get/RestGetAction.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/embedded/ThreeShardsEmbeddedSearchTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/embedded/ThreeShardsUnbalancedShardsEmbeddedSearchTests.java</file><file>modules/test/integration/src/test/java/org/elasticsearch/test/integration/search/preference/SearchPreferenceTests.java</file></files><comments><comment>Search / Get: Allow to specify a preference on which shards (or order) it will be executed, closes #769.</comment></comments></commit></commits></item><item><title>Queries that don't match Lucene query syntax should still be valid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/768</link><project id="" key="" /><description>For instance, search for ":wibble" doesn't work because Lucene places special meaning on ":" and doesn't expect it at the start of a search:

```
curl -XPOST http://localhost:9200/testidx/ -d '              
index :
  store:
    type: memory
'
curl 'http://localhost:9200/testidx/_search?q=%3awibble&amp;pretty=true'
```

results in
    "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[TtLvTQ2MQ6S9JIRz8gSjoA][testidx][0]: SearchParseException[[testidx][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: QueryParsingException[[testidx] Failed to parse query [:wibble]]; nested: ParseException[Cannot parse ':wibble': Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[TtLvTQ2MQ6S9JIRz8gSjoA][testidx][1]: SearchParseException[[testidx][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: QueryParsingException[[testidx] Failed to parse query [:wibble]]; nested: ParseException[Cannot parse ':wibble': Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[TtLvTQ2MQ6S9JIRz8gSjoA][testidx][2]: SearchParseException[[testidx][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: QueryParsingException[[testidx] Failed to parse query [:wibble]]; nested: ParseException[Cannot parse ':wibble': Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[TtLvTQ2MQ6S9JIRz8gSjoA][testidx][3]: SearchParseException[[testidx][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: QueryParsingException[[testidx] Failed to parse query [:wibble]]; nested: ParseException[Cannot parse ':wibble': Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }{[TtLvTQ2MQ6S9JIRz8gSjoA][testidx][4]: SearchParseException[[testidx][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: QueryParsingException[[testidx] Failed to parse query [:wibble]]; nested: ParseException[Cannot parse ':wibble': Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; nested: ParseException[Encountered \" \":\" \": \"\" at line 1, column 0.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"_\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"_\" ...\n    ]; }]",
      "status" : 500
    }

It is impractical to educate every single user of every single system that uses Elastic Search not to do this, and somewhat painful to have to write code in every single system to avoid the exception at client-end (not to mention that you get an enormous unhelpful stacktrace in `elasticsearch.log` which prevents you seeing actual problems clearly).

I believe that anything a user types into a search box should be a valid search. It may not be a _useful_ one, but that's a different matter entirely :-)
</description><key id="665085">768</key><summary>Queries that don't match Lucene query syntax should still be valid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaylett</reporter><labels /><created>2011-03-11T11:39:34Z</created><updated>2013-04-04T19:32:39Z</updated><resolved>2013-04-04T19:32:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2011-05-21T22:37:04Z" id="1215784">I think you'll have to escape them yourself. Other people need the lucene syntax. Although your issue could be put into the query url as additional parameter ... but escaping is not that much:

// copied from solrs' ClientUtils.escapeQueryChars  
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i &lt; str.length(); i++) {
            char c = str.charAt(i);
            if (c == '\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
                    || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
                    || c == '*' || c == '?' || c == '|' || c == '&amp;' || c == ';'
                    || Character.isWhitespace(c)) {
                sb.append('\');
            }
            sb.append(c);
        }
</comment><comment author="jaylett" created="2011-05-28T18:22:57Z" id="1254799">Ick, that's unpleasant :-(

Would it be possible to have an ES search type that everyone could use that didn't have this problem? I get that people may want to use the lucene syntax, and I entirely support having that available. But having to choose between requiring that every user understands lucene syntax, or to escape every special character and hence not support it, isn't a nice tradeoff IMHO.

What I'll do in the short term is to either escape or replace the specials in that list, I guess.
</comment><comment author="kimchy" created="2011-05-28T20:19:55Z" id="1255127">There is already such query in 0.16.1 called `text`, check out the docs here: http://www.elasticsearch.org/guide/reference/query-dsl/text-query.html.
</comment><comment author="jaylett" created="2011-05-29T13:47:26Z" id="1257020">Hadn't noticed that — cool!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Creating an index with mappings and analysis doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/767</link><project id="" key="" /><description>https://gist.github.com/864574

The first example fails with `MapperParsingException[Analyzer [lowercase] not found for field [name]];`. Version is 0.15.1

Cheers,
moritz
</description><key id="662736">767</key><summary>Creating an index with mappings and analysis doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monken</reporter><labels /><created>2011-03-10T18:15:47Z</created><updated>2011-03-11T17:48:57Z</updated><resolved>2011-03-11T17:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-11T12:56:35Z" id="860410">When providing settings and mappings in the create index API, you need to wrap the settings in a "settings" element. Check the samples here: http://www.elasticsearch.org/guide/reference/api/admin-indices-create-index.html.
</comment><comment author="monken" created="2011-03-11T17:48:57Z" id="861327">thanks for clarifying this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make Result-objects Serializable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/766</link><project id="" key="" /><description>Hello,

currently SearchHit, SearchResponse etc. in java-api do not implement Serializable; In order for easy of use in wicket I therefore request to implement this; 
</description><key id="661767">766</key><summary>Make Result-objects Serializable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kbachl</reporter><labels /><created>2011-03-10T11:49:57Z</created><updated>2013-04-04T19:33:06Z</updated><resolved>2013-04-04T19:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-10T12:19:17Z" id="855636">Why do you want them to be serializable? You can transform them to json. They won't be Serializable for the simple reason that its very expensive.
</comment><comment author="kbachl" created="2011-03-10T14:09:56Z" id="855960">well, because in wicket all objects need to be put into models for page serialization - currently, this means much fiddling around as SearchHit, SearchResponse etc. are not implementing serializable interface meaning one has to put another layer in between - and as long as no serilization takes place, I dont know of any impacts regarding CPU

See: https://cwiki.apache.org/WICKET/working-with-wicket-models.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update Settings: Allow to dynamically update `index.translog` settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/765</link><project id="" key="" /><description>Allow to update `index.translog.flush_threshold_ops`, `index.translog.flush_threshold_size`, and `index.translog.flush_threshold_period` dynamically using the update settings API.
</description><key id="658889">765</key><summary>Update Settings: Allow to dynamically update `index.translog` settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-09T12:16:46Z</created><updated>2011-03-09T12:17:46Z</updated><resolved>2011-03-09T12:17:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-09T12:17:45Z" id="851397">Update Settings: Allow to dynamically update `index.translog` settings, closed by 80a797fc4fad31524291ea0475f1a58bedacb83b.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/translog/TranslogService.java</file></files><comments><comment>Update Settings: Allow to dynamically update `index.translog` settings, closes #765.</comment></comments></commit></commits></item><item><title>Mapper: Using `dynamic_template` can result in warning of parsed and original source difference (resulting in excessive mapping parsing)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/764</link><project id="" key="" /><description>Mapper: Using `dynamic_template` can result in warning of parsed and original source difference (resulting in excessive mapping parsing).
</description><key id="658575">764</key><summary>Mapper: Using `dynamic_template` can result in warning of parsed and original source difference (resulting in excessive mapping parsing)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-03-09T09:35:36Z</created><updated>2011-03-09T09:37:05Z</updated><resolved>2011-03-09T09:37:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-09T09:37:05Z" id="850996">Mapper: Using `dynamic_template` can result in warning of parsed and original source difference (resulting in excessive mapping parsing), closed by c2a0e0b767ad4275dabd7abf4625b6ad5f8a8757.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/mapper/xcontent/DynamicTemplate.java</file></files><comments><comment>Mapper: Using `dynamic_template` can result in warning of parsed and original source difference (resulting in excessive mapping parsing), closes #764.</comment></comments></commit></commits></item><item><title>Percolator: Failures when using docs where fields repeat, such as arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/763</link><project id="" key="" /><description>Percolator: Failures when using docs where fields repeat, such as arrays
</description><key id="658517">763</key><summary>Percolator: Failures when using docs where fields repeat, such as arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.16.0</label></labels><created>2011-03-09T08:48:32Z</created><updated>2011-03-09T09:31:21Z</updated><resolved>2011-03-09T09:31:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-09T09:31:21Z" id="850981">Percolator: Failures when using docs where fields repeat, such as arrays, closed by f694b9dfd852af4916864a69cf5db79819e0cb45.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/index/memory/CustomMemoryIndex.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java</file><file>modules/elasticsearch/src/test/java/org/apache/lucene/index/memory/CustomMemoryIndexTests.java</file></files><comments><comment>Percolator: Failures when using docs where fields repeat, such as arrays, closes #763.</comment></comments></commit></commits></item><item><title>Update Settings: Allow to dynamically change `index.term_index_interval` and `index.term_index_divisor`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/762</link><project id="" key="" /><description>Update Settings: Allow to dynamically change `index.term_index_interval` and `index.term_index_divisor`
</description><key id="658239">762</key><summary>Update Settings: Allow to dynamically change `index.term_index_interval` and `index.term_index_divisor`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-09T05:35:08Z</created><updated>2011-03-09T05:36:32Z</updated><resolved>2011-03-09T05:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-09T05:36:32Z" id="850518">Update Settings: Allow to dynamically change `index.term_index_interval` and `index.term_index_divisor, closed by ad0d681b6d5273c0f448fe3490ca01dd028d13a1.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/index/engine/SimpleEngineBenchmark.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/settings/IndexSettingsService.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/engine/robin/SimpleRobinEngineTests.java</file></files><comments><comment>Update Settings: Allow to dynamically change `index.term_index_interval` and `index.term_index_divisor, closes #762.</comment></comments></commit></commits></item><item><title>Feature Request:  The ability to "join" parent and children</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/761</link><project id="" key="" /><description>There are many times I would like both the parent and children of a record.  Currently to find the children of a query (even a has_child query) requires an individual GET for each returned record.

1)  The simplest solution may be to enhance the has_child query, which already specifies parent and children types, allowing the actual children to be returned along with the parents.

2)  Enhance the query DSL to allow the children/parents of any query results to be joined and returned.  Perhaps even allowing additional filtering.

3)  Add a join API call.
</description><key id="657538">761</key><summary>Feature Request:  The ability to "join" parent and children</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">merrellb</reporter><labels /><created>2011-03-08T23:47:25Z</created><updated>2014-11-04T07:51:30Z</updated><resolved>2014-11-04T07:51:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="till" created="2011-05-05T22:34:30Z" id="1108634">_subscribe_
</comment><comment author="bryangreen" created="2011-06-08T22:26:02Z" id="1330447">+1 this would be great
</comment><comment author="mente" created="2011-08-17T11:01:01Z" id="1825325">+1
</comment><comment author="abh" created="2012-01-30T08:48:47Z" id="3716217">I ran into this, too.   Nested documents are a bit too closely tied (specifically that you always get all the nested documents back and not just the matching one(s)) and with parent/child documents I can't get both the matching lower level and the upper level back, either – unless I am missing something.
</comment><comment author="gjb83" created="2012-02-03T19:43:55Z" id="3803001">+1
</comment><comment author="hlian" created="2012-02-09T16:50:17Z" id="3890846">Lucene 3.6 will support a join query: https://issues.apache.org/jira/browse/LUCENE-3602
</comment><comment author="kevingessner" created="2012-02-09T16:51:32Z" id="3890881">Lucene 3.6 added query-time joining: https://issues.apache.org/jira/browse/LUCENE-3602

What's the timeline for ES using Lucene 3.6, @kimchy?
</comment><comment author="kimchy" created="2012-02-12T17:09:08Z" id="3929158">The join query is not really relevant here. Parent child support is similar to the join aspect, its a matter of returning different data set than what is provided now. Note, there will never be a cross shard join in elasticsearch, so any join will happen within a shard, which the parent-child support does now.
</comment><comment author="kevingessner" created="2012-02-13T15:27:37Z" id="3941860">@kimchy Sure, makes sense. I don't actually need full join support -- I really need something more like #792 or #1017, to be able to query the parent's field from a search on the child type.
</comment><comment author="dhardy92" created="2012-03-12T08:50:19Z" id="4447837">[+1]
</comment><comment author="Vineeth-Mohan" created="2012-06-25T19:49:59Z" id="6557729">+1
</comment><comment author="Vineeth-Mohan" created="2012-06-25T19:51:35Z" id="6557784">+1
</comment><comment author="nickhoffman" created="2012-07-04T14:16:07Z" id="6761222">This would be incredibly useful.
</comment><comment author="ghost" created="2012-08-08T01:37:42Z" id="7572660">Any update on this? Would love to have this rather than having to use seperate requests to get the children.
</comment><comment author="gjb83" created="2012-09-28T12:56:01Z" id="8975473">+1
</comment><comment author="keir" created="2013-06-10T08:53:54Z" id="19187272">+1 this would be great to have.
</comment><comment author="mvallebr" created="2014-02-20T02:42:30Z" id="35581206">+1
</comment><comment author="isabel12" created="2014-05-19T02:38:00Z" id="43462137">+1
</comment><comment author="chaitanya24" created="2014-06-17T12:28:00Z" id="46299942">+1
</comment><comment author="vedharish" created="2014-07-02T07:25:57Z" id="47744852">+1
</comment><comment author="clintongormley" created="2014-07-02T10:04:20Z" id="47758158">So what would the response actually look like?  Don't forget that parents and children are separate documents. Presumably you'd want children grouped with parents somehow? A parent may have millions of matching children - how many of those do we return?

The top_hits aggregation #6124 isn't a good solution for this as you would have to aggregate on parent_id, of which there may be millions in the resultset.

By far the most efficient way of doing this is in two queries:
1. retrieve the top 10 parents matching the query
2. use an msearch to find (eg) the top 10 children for each parent id

While this requires two steps, it gives you all the flexibility you need which would otherwise have to be provided by adding new structures to the query dsl and to the response.

Anybody want to flesh out this feature request a bit more?
</comment><comment author="clintongormley" created="2014-08-08T09:43:55Z" id="51582174">No further feedback. Closing
</comment><comment author="jason-mccloskey" created="2014-08-20T17:08:20Z" id="52809236">Oh, no! This is the exact feature that will help complete my elasticsearch implementation. Let me give a hypothetical use case for this feature that is analogous to what I need to do in my implementation. Please forgive me for any misgivings as I am fairly new to elasticsearch and brand new to commenting on issues in GitHub.

Use Case: I want to be able to populate a grid of events at parks in a given city, and allow filtering based upon whether the event is at a "safe" park.

&lt;h2&gt;Mappings&lt;/h2&gt;

We want three types here in a grandparent/parent/child relation.

&lt;h4&gt;City&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/city/_mapping' -d '{ 
    "city" : {
        "_id" : { "path" : "cityName" },
        "properties" : {
            "cityName" : { "type" : "string" },
            "state" : { "type" : "string" }
        }
    }
}'
```

&lt;h4&gt;Park&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/park/_mapping' -d '{ 
    "park" : {
        "_parent":{
            "type" :  "city"
        },
        "_id" : { "path" : "parkName" },
        "properties" : {
            "parkName" : { "type" : "string" },
            "address" : { "type" : "string" }
        }
    }
}'
```

&lt;h4&gt;Park Event&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/park_event/_mapping' -d '{   
    "park_event" : {
        "_parent":{
            "type" :  "park"
        },
        "properties" : {
            "eventName" : { "type" : "string" },
            "eventType" : { "type" : "string" },
            "time" : { "type" : "date" }
        }
    }
}'
```

&lt;h2&gt;Data&lt;/h2&gt;

Let's now consider the data that we'd like to put in this index:

&lt;h4&gt;Cities&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/city/SanDiego?routing=SanDiego' -d '{
        "cityName" : "SanDiego",
        "state" : "California"
}'
curl -XPUT 'http://localhost:9200/parkinfo/city/LosAngeles?routing=LosAngeles' -d '{
        "cityName" : "LosAngeles",
        "state" : "California"
}'
```

&lt;h4&gt;Parks in San Diego&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/park/Balboa?parent=SanDiego&amp;routing=SanDiego' -d '{
        "parkName" : "Balboa",
        "address" : "1549 El Prado"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park/Glen?parent=SanDiego&amp;routing=SanDiego' -d '{
        "parkName" : "Glen",
        "address" : "2149 Orinda Dr"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park/KateSessions?parent=SanDiego&amp;routing=SanDiego' -d '{
        "parkName" : "KateSessions",
        "address" : "5115 Soledad Rd"
}'
```

&lt;h4&gt;Parks in Los Angeles&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/park/48thSt?parent=LosAngeles&amp;routing=LosAngeles' -d '{
        "parkName" : "48thSt",
        "address" : "4800 South Hoover"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park/Alma?parent=LosAngeles&amp;routing=LosAngeles' -d '{
        "parkName" : "Alma",
        "address" : "21st and Meyler"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park/Canal?parent=LosAngeles&amp;routing=LosAngeles' -d '{
        "parkName" : "Canal",
        "address" : "200 Linnie Canal and Venice"
}'
```

&lt;h4&gt;Events in Parks in San Diego&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/park_event/1?parent=Balboa&amp;routing=SanDiego' -d '{
        "eventName" : "Scary Stuff",
        "eventType" : "crime",
        "time" : "2014-08-15T22:58:00"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park_event/2?parent=Balboa&amp;routing=SanDiego' -d '{
        "eventName" : "Bocce Ball Summer 2014",
        "eventType" : "tournament",
        "time" : "2014-08-25T12:00:00"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park_event/3?parent=Glen&amp;routing=SanDiego' -d '{
        "eventName" : "Basketball Summer 2014",
        "eventType" : "tournament",
        "time" : "2014-08-23T12:00:00"
}'
```

&lt;h4&gt;Events in Parks in Los Angeles&lt;/h4&gt;

``` bash
curl -XPUT 'http://localhost:9200/parkinfo/park_event/4?parent=48thSt&amp;routing=LosAngeles' -d '{
        "eventName" : "More Scary Stuff",
        "eventType" : "crime",
        "time" : "2014-08-15T22:58:00"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park_event/5?parent=Alma&amp;routing=LosAngeles' -d '{
        "eventName" : "Really Scary Stuff",
        "eventType" : "crime",
        "time" : "2014-06-25T23:14:00"
}'
curl -XPUT 'http://localhost:9200/parkinfo/park_event/6?parent=Canal&amp;routing=LosAngeles' -d '{
        "eventName" : "Weight Lifting Summer 2014",
        "eventType" : "tournament",
        "time" : "2014-08-23T12:00:00"
}'
```

&lt;h2&gt;Filtering Stories/Requirements&lt;/h2&gt;

As a user I want to be able to display only events that will occur in the next X days in a grid
  The grid shall have the columns: city, state, park name, address, event name, event type, time
As a user I want to be able to filter for events at safe parks
  A park will be determined safe if it has no crime event in the past 4 weeks and there have not been crimes at 2 parks in its the city in the past 3 months

&lt;h2&gt;Filtering Implementation&lt;/h2&gt;

For these 2 requirements, we need a filter to keep only safe parks and then a query to display events in the next X days and join together the data from the 3 generations

&lt;h4&gt;Safe Park Filter&lt;/h4&gt;

This filter must do two things, it must exclude if its parent is considered an unsafe park city and it must exclude if the particular park in question is intrinsically unsafe. The preference would be to be able to do this with a single query. Currently I would expect to have to query cities and save the terms, then use a terms lookup filter.

I see return options as being: none, all, matching, #. None would be the default for all children.

``` json
"filter" : {
    "bool" : {
        "type" : "park",
        "must" : {
            "bool" : {
                "type" : "city",
                "return" : "none",
                "must_not" : {
                    "has_child" : {
                        "type" : "park",
                        "min_children": 2,
                        "return" : "all",
                        "filter" : {
                            "must" : {
                                "has_child" : {
                                    "type" : "park_event",
                                    "filter" : {
                                        "bool" : {
                                            "must" : {
                                                "term" : {
                                                    "eventType" : "crime"
                                                    },
                                                "range" : {
                                                    "time" : {
                                                        "gte" : "2014-05-20",
                                                        "lte" : "now"
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        },
        "must_not" : {
            "has_child" : {
                "type" : "park_event",
                "filter" : {
                    "bool" : {
                        "must" : {
                            "term" : {
                                "eventType" : "crime"
                                },
                            "range" : {
                                "time" : {
                                    "gte" : "2014-07-23",   
                                    "lte" : "now"
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

I don't believe that you can list a "type" in a bool filter, but I felt it made things much more clear by including it. It also may be required for that sort of functionality.

&lt;h4&gt;Returning the data over the next 5 days&lt;/h4&gt;

As this is to be put into a grid, we would want the data to be denormalized, and perhaps sortable for using from and size. If denormalized is false, it would be an array based return, in the case that people aren't trying to display in a grid.

``` json
{
  "denormalized" : "true"
  "filtered": {
        "type" : "park",
        "return" : "matching",
        "query": {
            "has_parent" : {
                "return" : "matching"
            }
            "has_child" : {
                "return" : "matching"
                "range" : {
                    "time" : {
                        "gte" : "now",
                        "lte" : "2014-08-25"
                    }   
                }
            }
        },    
        "filter": "SafeParkFilter"
    }
}
```

&lt;h2&gt;Expected Results&lt;/h2&gt;

&lt;h4&gt;Safe Parks&lt;/h4&gt;

No parks in Los Angeles should be considered safe because there were multiple parks in LA with crime events in the past 3 months. Balboa Park should also be considered unsafe because of the crime event in the past 4 weeks. This leaves the safe parks as:
Glen
KateSessions

&lt;h4&gt;Events at Safe Parks&lt;/h4&gt;

Given that only Glen Park and Kate Sessions Park are safe parks in this scenario, we should only be returning events from those parks which will be held in the next 5 days

| City | State | Park Name | Address | Event Name | Event Type | Time |
| --- | --- | --- | --- | --- | --- | --- |
| SanDiego | California | Glen | 2149 Orinda Dr | Basketball Summer 2014 | tournament | 2014-08-23T12:00:00 |
| SanDiego | California | KateSessions | 5115 Soledad Rd | Bocce Ball Summer 2014 | tournament | 2014-08-25T12:00:00 |

Please let me know if any of this is unclear or doesn't make sense. This is also likely more than the orignal request, but this feature set would be very powerful and is the gap between what I have currently implemented on my project and the toolset I need to finish.
</comment><comment author="clintongormley" created="2014-08-22T13:17:57Z" id="53058758">Hi @ILMN-jmccloskey 

Thanks for the detailed example.  It feels very much like you are trying to use Elasticsearch as a relational DB, which isn't the best way to use it.  I would definitely avoid using grandparent-parent-child relationships as it is very costly, both with joins and the data required to maintain the relationship.

Think about how many times a crime is committed, then how many times your query will run.  A much better approach would be to denormalize your data and to update it when you have new events.  You want your results to be parks, so you should store all the info you need inside the single park document, including the crimes inside that park and the number of crimes for a particular time period in the city where the park is located.

I suggest reading about the various techniques and tradeoffs here: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/modeling-your-data.html
</comment><comment author="jason-mccloskey" created="2014-08-22T16:20:34Z" id="53083224">Hi @clintongormley

Thanks for responding. I fully agree with you that the example, as given, doesn't lend itself to normalization. I was trying to be breif (ha!) in the data for the example. Imagine that the city, park and park_event all have anywhere from 10 to 50 fields, which should be able to be updated independtely from each other and you are creating many events per month. This isn't the acutal index I am trying to create, only an example for illustrative purposes.

I also am not sure how to fill the requirements of a safe park (2 crimes committed at parks in a particular city within the last 3 months) without the parent/child relationship. It seems to me that this is why the parent/child relationship and min_children were created. Assuming that the example was altered to lend itself to normalization using elasticsearch, are there things that need to be added to demonstrate the value of returning data across documents or perhaps clear up implementation details?

Thank you for the link. I will read further to make sure my actual mapping strategy is appropriate for my implementation. Even outside of the number of fields in a type and the ability to update those types independently, it seems to me that I won't be able to look for parks that have some events and not others, or cities that have some events and not others, without a parent/child relationship (**updated: I could if I used use multiple queries). Do you have any suggestions in that regard?
</comment><comment author="asanderson" created="2014-08-22T17:00:44Z" id="53089554">FWIW, we aggregate data into Elasticsearch from many different disparate sources including unstructured, semi-structured (e.g. XML, RESTful services, etc.), and structured (e.g. relational database records), so our basic schema includes master parent documents (e.g. entities, relationships, etc.), and each of them can have many detail child documents each of which can have dozens and dozens of fields. 

We do not want to update the master documents, since most of our data ingest pattern is just adding additional details.  The performance is more than acceptable.

Yes, everyone says not to use Elasticsearch (or Solr) as a relational database replacement, but for data that is primarily write-once/read-many, it is more than an adequate solution as we've proved with Solr and now Elasticsearch.

However, without a simple parent/child join capability baked into Elasticseach, it means that every Elasticsearch client must do it the _hard way_, and pull unnecessary data across the network.

Just my $0.02.
</comment><comment author="kunklejr" created="2014-09-05T13:40:26Z" id="54625797">My situation is similar to @asanderson's. I have a parent document that has one or more child documents containing all the data. They generally don't change but are added all the time. It would be incredibly valuable to search the child documents and get results back in terms of the parent document AND also return the data contained in the children along with the parent.
</comment><comment author="clintongormley" created="2014-11-04T07:51:30Z" id="61604487">Closing in favour of #8153
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem when "fields" have spaces in their names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/760</link><project id="" key="" /><description>I get error similar to the following when I try to specify "fields" in a get/search call:

[{"query": {"match_all": {}}, "size": 100, "from": 0, "fields": ["_MD5HASH", "zMeta_MD5 Hash"]}]]]; nested: CompileException[[Error: unk
nown class or illegal statement: org.elasticsearch.common.mvel2.ParserContext@135064e]
[Near : {... _source.zMeta_MD5 Hash ....}]
</description><key id="657482">760</key><summary>Problem when "fields" have spaces in their names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merrellb</reporter><labels /><created>2011-03-08T23:15:40Z</created><updated>2011-03-15T21:45:24Z</updated><resolved>2011-03-09T16:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-09T08:15:29Z" id="850817">Don't use fields with spaces.
</comment><comment author="merrellb" created="2011-03-09T17:26:07Z" id="852522">Perhaps elastic search could raise an exception if a mapping is created/changed with a field containing whitespace (or any other characters that will cause problems, periods perhaps)?  I've created several databases based on CSV files and simply used the column names present.  Even if field naming guidelines can be highlighted in the documentation I think it would probably be best to simply prevent the creation of these fields if they will later cause trouble.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mvel Script: add more random options, and optimize random</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/759</link><project id="" key="" /><description>Add `randomDouble`, `randomFloat`, `randomInt`, `randomLong`. Also add `randomInt(int)` and `randomLong(long)` to return random between 0 and the provided value.
</description><key id="656992">759</key><summary>Mvel Script: add more random options, and optimize random</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-08T20:23:58Z</created><updated>2011-03-08T20:24:34Z</updated><resolved>2011-03-08T20:24:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-08T20:24:34Z" id="848796">Mvel Script: add more random options, and optimize random, closed by 353d2cb21fc1a46dba727299e8d069af2a4d1157.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/math/UnboxedMathUtils.java</file></files><comments><comment>Mvel Script: add more random options, and optimize random, closes #759.</comment></comments></commit></commits></item><item><title>Update Settings: Allow to dynamically change refresh_interval and merge policy settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/758</link><project id="" key="" /><description>Improve the update settings API to allow to change more index level settings. The first set of settings include the `index.refresh_interval` and `index.merge.policy` settings. This combination of settings can come handy when doing bulk indexing, and then moving to a more real time indexing. For example, before doing bulk indexing, update the settings:

```
curl -XPUT localhost:9200/test/_settings -d '{
    "index" : {
        "refresh_interval" : "-1",
        "merge.policy.merge_factor" : 30
    }
}'
```

In the above, we disable refreshing, and increase the merge factor to 30 to decrease the merge operations we will do. Once done, we can go back to normal:

```
curl -XPUT localhost:9200/test/_settings -d '{
    "index" : {
        "refresh_interval" : "1s",
        "merge.policy.merge_factor" : 10
    }
}'
```

But, since we changed the merge factor, we probably also want to optimize down to 10 (or even 5):

```
curl -XPOST localhost:9200/test/_optimize?max_num_segments=5
```
</description><key id="656811">758</key><summary>Update Settings: Allow to dynamically change refresh_interval and merge policy settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-08T19:34:39Z</created><updated>2011-03-09T10:16:09Z</updated><resolved>2011-03-09T03:53:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-08T19:53:54Z" id="848673">Update Settings: Allow to dynamically change refresh_interval and merge policy settings, closed by 016e5bce047d4e81947e7c511c5d46f9019efc61.
</comment><comment author="medcl" created="2011-03-09T10:16:09Z" id="851113">great~
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/index/engine/SimpleEngineBenchmark.java</file><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/percolator/EmbeddedPercolatorBenchmarkTest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/ScheduledRefreshableEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/BalancedSegmentMergePolicy.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/BalancedSegmentMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogByteSizeMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/LogDocMergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/policy/MergePolicyProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/settings/IndexSettingsModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/settings/IndexSettingsService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/analysis/CompoundAnalysisTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/engine/AbstractSimpleEngineTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/mapper/xcontent/MapperTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/percolator/PercolatorExecutorTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/SimpleIndexQueryParserTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/guice/IndexQueryParserModuleTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/plugin/IndexQueryParserPluginTests.java</file><file>plugins/analysis/icu/src/test/java/org/elasticsearch/index/analysis/SimpleIcuAnalysisTests.java</file></files><comments><comment>Update Settings: Allow to dynamically change refresh_interval and merge policy settings, closes #758.</comment></comments></commit></commits></item><item><title>Oauth support for twitter plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/757</link><project id="" key="" /><description>Added oauth support for twitter river plugin.

The following configuration should be set:

```
curl -XPUT localhost:9200/_river/my_twitter_river/_meta -d '{
    "type" : "twitter",
    "twitter" : {
        "oauth" : {
            "consumer_key" : "...",
            "consumer_secret": "...",
            "access_token" : "...",
            "access_token_secret" : "..."
        }
    },
    "index" : {
        "index" : "my_twitter_river",
        "type" : "status",
        "bulk_size" : 100
    }
}'
```
</description><key id="656020">757</key><summary>Oauth support for twitter plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adoran</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-08T15:19:04Z</created><updated>2014-06-13T15:39:02Z</updated><resolved>2011-03-09T16:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-09T08:12:55Z" id="850812">Pushed, and added some fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>plugin install PLUGIN_NAME uses googlecode.com instead of github.com</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/756</link><project id="" key="" /><description>./bin/plugin install client-groovy fails because it tries to get the file from googlecode.com:

Failed to install client-groovy, reason: Can't get http://elasticsearch.googlecode.com/svn/plugins/client-groovy/elasticsearch-client-groovy-0.15.1.zip

Looks like googlecode.com is not maintained anymore.
</description><key id="655683">756</key><summary>plugin install PLUGIN_NAME uses googlecode.com instead of github.com</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darxriggs</reporter><labels /><created>2011-03-08T12:46:21Z</created><updated>2011-03-08T16:38:48Z</updated><resolved>2011-03-08T16:38:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-08T16:16:39Z" id="847742">It is maintained, the plugin name changed to `lang-groovy`. Where did you find the `client-groovy` one? (it used to be its name).
</comment><comment author="darxriggs" created="2011-03-08T16:38:48Z" id="847838">Ah I see. I guess I've found it somewhere on the net. I could not really find any documentation in the Groovy API section where to get it from. A "plugin" section in the documentation would be a good idea (maybe I just missed it).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Logging: Add merge level logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/755</link><project id="" key="" /><description>Add merge level logging under `index.merge`. If set to `DEBUG` will only log merges longer than `20s`. If set to `TRACE`, will log when a merge starts and when its done.
</description><key id="655023">755</key><summary>Logging: Add merge level logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-08T06:20:22Z</created><updated>2011-03-08T06:21:01Z</updated><resolved>2011-03-08T06:21:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-08T06:21:01Z" id="846111">Logging: Add merge level logging, closed by e709a0bde42137d3bdb70fd96fe95c2967cc61ef.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/apache/lucene/index/TrackingConcurrentMergeScheduler.java</file><file>modules/elasticsearch/src/main/java/org/apache/lucene/index/TrackingSerialMergeScheduler.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/scheduler/ConcurrentMergeSchedulerProvider.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/merge/scheduler/SerialMergeSchedulerProvider.java</file></files><comments><comment>Logging: Add merge level logging, closes #755.</comment></comments></commit></commits></item><item><title>Inconsistent results when searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/754</link><project id="" key="" /><description>I created ticket #753, and think I then managed to press the "comment and close" button.  Since there's no apparent way to reopen the ticket, I'm creating this one to point to it.
</description><key id="653174">754</key><summary>Inconsistent results when searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rboulton</reporter><labels /><created>2011-03-07T16:50:28Z</created><updated>2011-03-08T06:50:05Z</updated><resolved>2011-03-08T06:50:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-08T06:50:05Z" id="846155">I have opened the other issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inconsistent results when re-running queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/753</link><project id="" key="" /><description>I'm getting inconsistent results when running a query multiple times in succession.  The following is a minimal example that reproduces this for me.  The index contains around 80,000 docs, and is using the default sharding configuration (5 shards).  There are 2 nodes in the cluster.  Cluster status is green, and has been running for a couple of days since the initial indexing.  No changes currently happening.

Running the following shell command to run the same query every second, on an index which is not being modified at all:

```
$ while :; do curl -XGET http://hostname.amazonaws.com:9200/core/galleries.Gallery/_search -d "{\"query\": {\"field\": {\"_id\": \"392\"}}, \"from\": 0, \"size\": 10}"; echo; sleep 1; done
```

Output is multiple lines not finding any documents:

```
{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

At this point, I ran the following in a separate console:

```
$ curl -XGET http://hostname.amazonaws.com:9200/core/galleries.Gallery/392;
```

which returned:

```
{"_index":"core_4d6fc008","_type":"galleries.Gallery","_id":"392","_version":1, "_source" : {"known_as": [], "score": 5, "name": ["National Museums Liverpool, Walker Art Gallery"]}}
```

After this point, the output in the first console changed to repeat:

```
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":9.345693,"hits":[{"_index":"core_4d6fc008","_type":"galleries.Gallery","_id":"392","_score":9.345693, "_source" : {"known_as": [], "score": 5, "name": ["National Museums Liverpool, Walker Art Gallery"]}}]}}
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":9.345693,"hits":[{"_index":"core_4d6fc008","_type":"galleries.Gallery","_id":"392","_score":9.345693, "_source" : {"known_as": [], "score": 5, "name": ["National Museums Liverpool, Walker Art Gallery"]}}]}}
```

Leaving the cluster alone for a few minutes, then re-running the original query, it starts returning no results again.

I'm also seeing other more complex searches missing some (but not all) of the results after refreshing; the search described above is just for one of the documents which was only occasionally returned by such a search.
</description><key id="653126">753</key><summary>Inconsistent results when re-running queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rboulton</reporter><labels /><created>2011-03-07T16:33:45Z</created><updated>2014-02-24T12:16:33Z</updated><resolved>2013-04-04T18:41:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rboulton" created="2011-03-07T16:35:01Z" id="843370">(http://hostname.amazonaws.com above is a replacement for a real hostname, obviously.)
</comment><comment author="kimchy" created="2011-03-08T06:51:18Z" id="846159">Strange, GET should not affect the results of search. I know its hard, but is there a chance for a recreation? Something that index sample data (using curl) and then exhibit this?

By the way, it is recommended not to use `.` in type names. I have just pushed a warning when its done.
</comment><comment author="rboulton" created="2011-03-10T17:56:19Z" id="856877">Could the '.' be related in any way to the problem we're seeing here, do you think?

I can't find an easy way to recreate this; it's only happening on the production servers, not on any test system I've found, even with the same data.  Any suggested approaches gratefully received, but it's one of those horrible bugs...
</comment><comment author="rboulton" created="2011-03-11T12:42:03Z" id="860381">It's looking like the problem is a confused node, caused by running out of file handles on the node.  We've restarted one of the two nodes in the cluster to update its file handle limit, but it's now being unable to synchronise to the remaining problematic node.  We're building a new cluster from scratch, and reindexing to it.

We thought we'd set the file handle limit to somewhere in excess of 100000, but starting elasticsearch from upstart had failed to respect the settings in limits.conf (because upstart doesn't hit any of the useful PAM hooks such as common-session).  It would be lovely if elasticsearch could report in the logs the number of file handles available on startup.

So, the bug(s) in elasticsearch seem to be:
- cluster appears healthy even when running out of file handles.
- and unreported out of file handles results in inconsistent cluster.

Not sure if you want to open additional tickets to track these, and close this one?
</comment><comment author="kimchy" created="2011-03-11T12:54:44Z" id="860405">elasticsearch can report the number of open file handles it has, but only if you specify a flag to it (there is no way in Java to tell, so the hackish way I do it is to simply create files until it fails). It is explained here (the flag): http://www.elasticsearch.org/guide/reference/setup/installation.html.

Getting file handles failures is a tricky one, since it can basically happen anywhere in the system (at least in Java its really tricky). Same applies to OutOfMemory failures.
</comment><comment author="karussell" created="2011-05-21T22:42:00Z" id="1215796">Just stumbled over this issue.

A nice explanation is here: http://www.elasticsearch.org/tutorials/2011/04/06/too-many-open-files.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scripts: Allow to register native scripts (Java) for better script execution performance.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/752</link><project id="" key="" /><description>Even though `mvel` is pretty fast, allow to register native Java based scripts for faster execution.

In order to allow for scripts, the `NativeScriptFactory` needs to be implemented that constructs the script that will be executed. There are two main types, one that extends `AbstractExecutableScript` and one that exetends `AbstractSearchScript` (probably the one most users will extend).

Registering them can either be done by settings, for example: `scripts.natives.my.type` set to `sample.MyNativeScriptFactory` will register a script named `my`. Another option is in a plugin, to get `ScriptModule` and call `registerScript` on it.

Executing the script is done by specifying the type as `native`, and the name of the script as the script.
</description><key id="652395">752</key><summary>Scripts: Allow to register native scripts (Java) for better script execution performance.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.16.0</label></labels><created>2011-03-07T10:49:42Z</created><updated>2011-03-07T11:09:21Z</updated><resolved>2011-03-07T11:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-07T11:09:21Z" id="842365">Scripts: Allow to register native scripts (Java) for better script execution performance, closed by 4bdae621f92beb226cf5873a9efe721b38c7e0c7.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/benchmark/micro/src/main/java/org/elasticsearch/benchmark/percolator/EmbeddedPercolatorBenchmarkTest.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/AbstractDoubleSearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/AbstractExecutableScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/AbstractFloatSearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/AbstractLongSearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/NativeScriptFactory.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/ScriptModule.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/SearchScript.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/script/mvel/MvelScriptEngineService.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/percolator/PercolatorExecutorTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/SimpleIndexQueryParserTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/guice/IndexQueryParserModuleTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/index/query/xcontent/plugin/IndexQueryParserPluginTests.java</file><file>modules/elasticsearch/src/test/java/org/elasticsearch/script/NativeScriptTests.java</file><file>plugins/lang/groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java</file><file>plugins/lang/javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java</file><file>plugins/lang/python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java</file></files><comments><comment>Scripts: Allow to register native scripts (Java) for better script execution performance, closes #752.</comment></comments></commit></commits></item><item><title>ES searches and retrieves wrong multi_field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/751</link><project id="" key="" /><description>I have this mapping: https://gist.github.com/856406 and a search simple http://localhost:9200/cpan/release/_search?q=*&amp;pretty=true&amp;fields=distribution.raw gives me https://gist.github.com/856409 .
When I do a search on distribution.raw, it searches on name.raw.
Version is 0.15.1

Cheers,
mo
</description><key id="652200">751</key><summary>ES searches and retrieves wrong multi_field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monken</reporter><labels /><created>2011-03-07T08:56:48Z</created><updated>2013-04-04T18:41:27Z</updated><resolved>2013-04-04T18:41:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-07T09:05:01Z" id="842040">Heya, can you gist a full recreation? The curl that creates teh index + mappings, index a simple doc, and the search that results in the problem?
</comment><comment author="monken" created="2011-03-07T10:15:47Z" id="842265">I know what the problem is. In the "cpan" index there is a type named "distribution". I f I remove that type, everything works fine. It seems like it tries to search on the type "distribution" instead of the property "distribution" on "release".
</comment><comment author="kimchy" created="2011-03-07T10:17:50Z" id="842272">If you specify the full name of the field, then it will try and load that one. Seems strange that you get for `distribution.raw` the `name.raw`. Would love for a recreation so I can check it.
</comment><comment author="monken" created="2011-03-07T10:18:52Z" id="842273">http://localhost:9200/cpan/release/_search?q=*&amp;pretty=true&amp;fields=release.distribution.raw works just fine. I will build a minimal test case and come back to you.
</comment><comment author="monken" created="2011-03-07T10:25:57Z" id="842288">You can run this in one batch. My nick on IRC is moonk for further investigation :)

```
curl -XDELETE http://localhost:9200/cpan/
curl -XPUT http://localhost:9200/cpan
curl -XPUT http://localhost:9200/cpan/release/_mapping -d '{"release":{"properties":{"name":{"type":"multi_field",fields:{"raw":{"store":"yes","type":"string"},"name":{"store":"yes","type":"string"}}},"distribution":{"type":"multi_field",fields:{"raw":{"store":"yes","type":"string"},"distribution":{"store":"yes","type":"string"}}}}}}'
curl -XPUT http://localhost:9200/cpan/distribution/_mapping -d '{"distribution":{"properties":{"name":{"type":"multi_field",fields:{"raw":{"store":"yes","type":"string"},"name":{"store":"yes","type":"string"}}}}}}'
curl -XPUT http://localhost:9200/cpan/release/1 -d ' {"name":"FBP-Perl-0.18","distribution":"FBP-Perl"}'
curl http://localhost:9200/_refresh
curl 'http://localhost:9200/cpan/release/_search?q=*&amp;pretty=true&amp;fields=distribution.raw'
```
</comment><comment author="clintongormley" created="2011-09-10T11:31:25Z" id="2059037">Any news on this bug? It is still there in 0.17.6
</comment><comment author="kimchy" created="2011-09-10T21:57:14Z" id="2061438">This one is tricky, because there is a type called `distribution`, and we support automatic prefix of type when searching against a field (regardless of the type searched against in the "url"), then the `distribution.raw` is identified as searching against type `distribution`, even though you search within the `release` type.

I suggest using a different name either for the `distribution` type, or the `distribution` element within `release.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Design &amp;/OR architecture of ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/750</link><project id="" key="" /><description>Documentation for architecture or design blocks of ES.

From ES user perspective, documentation with 0.15 is getting there, but ES developer related document is still not present.

Best regards,
aditya
</description><key id="649552">750</key><summary>Design &amp;/OR architecture of ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adityainnovation</reporter><labels /><created>2011-03-05T21:00:15Z</created><updated>2013-04-04T18:44:04Z</updated><resolved>2013-04-04T18:44:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T18:44:04Z" id="15915811">I'm afraid this issue has been open for two years and nobody has volunteered to write the documentation. I think it's a case of diving into the code.

sorry
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refresh getting called without really needed to refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/749</link><project id="" key="" /><description>This can cause more load on the server then actually needed (when no dirty operations occurred).
</description><key id="648325">749</key><summary>Refresh getting called without really needed to refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.15.2</label><label>v0.16.0</label></labels><created>2011-03-04T23:59:08Z</created><updated>2011-03-04T23:59:46Z</updated><resolved>2011-03-04T23:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-04T23:59:46Z" id="836607">Fixed here: https://github.com/elasticsearch/elasticsearch/commit/c09773519650d5f1317146b5c996b184876cbeda.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>River CouchDb is not updating ElasticSearch Index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/748</link><project id="" key="" /><description>I have installed the river-couchdb plugin on ElasticSearch 0.15.0 and have used the following set up string:

curl -XPUT 'http://localhost:9200/_river/my_db/_meta' -d '{
    "type" : "couchdb",
    "couchdb" : {
        "host" : "localhost",
        "port" : 5984,
        "db" : "my_db",
        "filter" : null
    },
    "index" : {
        "index" : "my_db",
        "type" : "my_db",
        "bulk_size" : "100",
        "bulk_timeout" : "10ms"
    }}'

However, although River appears to be running and tracking the changes in the correct CouchDB database there are no entries appearing in the my_db index in ElasticSearch. 

CouchDb is being updated regularly and ElasticSearch is definitely tracking it. Is there a step I've missed? even if the docs say it 'just works :)

This is what ES says about the _river:

{
    took: 1
    timed_out: false
     _shards: {
           total: 1
           successful: 1
           failed: 0
      }
     hits: {
           total: 3
           max_score: 1
           hits: [
                 {
                       _index: _river
                       _type: my_db
                       _id: _status
                       _score: 1
                       _source: {
                             ok: true
                             node: {
                                   id: ccHULYqHSNiHjsTJylYfsA
                                   name: Scourge of the Underworld
                                   transport_address: inet[/192.168.0.66:9300]
                              }
                        }
                  }
                 {
                       _index: _river
                       _type: my_db
                       _id: _meta
                       _score: 1
                       _source: {
                             type: couchdb
                             couchdb: {
                                   host: localhost
                                   port: 5984
                                   db: my_db
                                   filter: null
                              }
                             index: {
                                   index: my_db
                                   type: my_db
                                   bulk_size: 100
                                   bulk_timeout: 10ms
                              }
                        }
                  }
                 {
                       _index: _river
                       _type: my_db
                       _id: _seq
                       _score: 1
                       _source: {
                             couchdb: {
                                   last_seq: 465
                              }
                        }
                  }
            ]
      }
}

Running on my local machine while I test.
</description><key id="646654">748</key><summary>River CouchDb is not updating ElasticSearch Index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electricowl</reporter><labels /><created>2011-03-04T13:21:23Z</created><updated>2011-03-07T11:28:38Z</updated><resolved>2011-03-07T18:59:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-04T15:54:14Z" id="834773">Please ask this question on the mailing list, and gist the sample.
</comment><comment author="electricowl" created="2011-03-07T10:59:54Z" id="842345">Thanks for the tip. I've looked it up and Ubuntu 10.04 does not have the latest version of CouchDB and it's a manual install to get to 1.02. Would be great if river-couchdb didn't care about the version of CouchDb.

Thanks!
</comment><comment author="kimchy" created="2011-03-07T11:28:38Z" id="842395">It uses the `_changes` stream, if thats not available, then it can't do much...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Internal: Improve checksum process by bulk writing them into a single file instead of checksum file per index file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/747</link><project id="" key="" /><description>Internal: Improve checksumming by bulk writing them into a single file instead of checksum file per index file.
</description><key id="645676">747</key><summary>Internal: Improve checksum process by bulk writing them into a single file instead of checksum file per index file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.16.0</label></labels><created>2011-03-04T02:21:52Z</created><updated>2011-03-04T02:22:56Z</updated><resolved>2011-03-04T02:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2011-03-04T02:22:56Z" id="832899">Internal: Improve checksum process by bulk writing them into a single file instead of checksum file per index file, closed by 4b92928c779833833814fc5698cf885bd4fd08d6.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>modules/elasticsearch/src/main/java/org/elasticsearch/index/store/support/AbstractStore.java</file><file>modules/elasticsearch/src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file></files><comments><comment>Internal: Improve checksum process by bulk writing them into a single file instead of checksum file per index file, closes #747.</comment></comments></commit></commits></item></channel></rss>