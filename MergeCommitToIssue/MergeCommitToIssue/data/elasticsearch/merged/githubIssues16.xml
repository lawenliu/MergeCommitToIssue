<?xml version="1.0" encoding="utf-8"?><rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Cleanup LocalGatewayShardsState</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8852</link><project id="" key="" /><description>This commit tries to cleanup LocalGatewayShardsState to be more efficient
and easier to understand.
</description><key id="51451514">8852</key><summary>Cleanup LocalGatewayShardsState</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-09T16:51:49Z</created><updated>2015-06-07T11:52:11Z</updated><resolved>2014-12-11T16:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-09T16:52:23Z" id="66315346">@kimchy can you please take a look at this you wrote this initially
</comment><comment author="kimchy" created="2014-12-10T19:38:51Z" id="66510199">LGTM, love the cleanup
</comment><comment author="s1monw" created="2014-12-10T19:49:45Z" id="66511977">cool stuff @kimchy thanks I tried to make it simpler and more efficient. I will add unittests to it and push it!! I will remove the comment too.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unnecessary rebalancing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8851</link><project id="" key="" /><description>```
# curl -s 'http://web605:9200/_cat/recovery?v' | fgrep -v done
index                         shard time   type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent
statistics-20140411           1     2928   relocation index web606      web607      n/a        n/a      24    75.0%         1062084500  15.3%
statistics-20131102           1     3520   relocation index web607      web606      n/a        n/a      23    65.2%         1659082665  14.1%
```

```
web605 ~ # curl -s 'http://web605:9200/_cat/recovery?v' | fgrep -v done
index                         shard time   type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent
statistics-20140121           0     37722  relocation index web607      web606      n/a        n/a      25    96.0%         1550947887  87.2%
statistics-20140311           0     19029  relocation index web606      web607      n/a        n/a      27    81.5%         919161526   76.2%
```

This doesn't make any sense. Each node has ~1.56/3.57TB of data, why move shards between equal nodes back and forth?

&gt; Previously, the allocation decider only took into account the size of shards currently on the node when deciding whether to move a shard to another node or not. Now, it also takes into account shards that are in the process of being moved away. This reduces the amount of shard movement to the minimum required. (#8569).

http://www.elasticsearch.org/blog/elasticsearch-1-4-1-released/

I'm on 1.4.1 and it's happening. Probably started after removal of a couple of indices.

cc @s1monw 
</description><key id="51444343">8851</key><summary>Unnecessary rebalancing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-12-09T15:57:39Z</created><updated>2016-01-17T17:37:03Z</updated><resolved>2016-01-17T17:37:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-12-09T16:04:25Z" id="66306379">```
index                         shard time   type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent
statistics-20140830           2     71391  relocation index web607      web606      n/a        n/a      27    96.3%         2496867005  98.2%
statistics-20131118           0     32368  relocation index web606      web607      n/a        n/a      27    96.3%         1246676262  86.0%
```

```
index                         shard time   type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent
statistics-20141031           1     8719   relocation index web606      web607      n/a        n/a      27    77.8%         2888487819  15.1%
statistics-20140727           0     18103  relocation index web607      web606      n/a        n/a      27    77.8%         2010520985  39.2%
```

```
index                         shard time   type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent
statistics-20141121           1     2755   relocation index web606      web607      n/a        n/a      27    51.9%         2640396281  3.4%
statistics-20140523           0     6925   relocation index web607      web606      n/a        n/a      27    74.1%         1831083953  7.2%
```

```
index                         shard time   type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent
statistics-20140831           2     29902  relocation index web606      web607      n/a        n/a      27    85.2%         2547843368  40.4%
statistics-20140127           0     47969  relocation index web607      web606      n/a        n/a      27    96.3%         1496376261  100.0%
```

```
index                         shard time   type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent
statistics-20140820           1     6968   relocation index web606      web607      n/a        n/a      27    74.1%         2066391857  21.7%
statistics-20141030           2     12418  relocation index web607      web606      n/a        n/a      27    85.2%         2464653918  34.5%
```

Not the cheapest thing to do for a cluster under load. Interestingly, is not participating in this shuffling.

```
host   ip           heap.percent ram.percent load node.role master name
web606 192.168.2.95           53          22 6.56 d         m      statistics07
web607 192.168.2.96           71          23 7.89 d         *      statistics08
web605 192.168.2.94           56          22 1.73 d         m      statistics06
```

Load average there is lower, as you see.
</comment><comment author="s1monw" created="2014-12-09T16:17:48Z" id="66308773">if you remove indices the balancer can see a chance to get a better overall balance and it might move shards around if it can and if it makes sense no matter if the disk size is equal. We don't take disk usage into account yet. The entire thing is a pretty intense discussion. Can I see how many shards are per node for these indices?

Can you provide your setting too like concurrent rebalance etc?
</comment><comment author="bobrik" created="2014-12-09T16:30:00Z" id="66311070">```
indices.recovery.concurrent_streams: 5
indices.recovery.max_bytes_per_sec: 200mb
```

This is all what I have in `elasticsearch.yml` about recovery.

Indices mostly have 5 shards and 0 replicas. Distribution could look like this:

```
index               shard prirep ip           segment generation docs.count docs.deleted     size size.memory committed searchable version compound
statistics-20140512 0     p      192.168.2.94 _mo2         29378    5165999            0  570.9mb     1194000 true      true       4.6.0   false
statistics-20140512 0     p      192.168.2.94 _n4v         29983    7013120            0  772.8mb     2221768 true      true       4.9.0   false
statistics-20140512 1     p      192.168.2.96 _lfu         27786    5880136            0  657.4mb     1564848 true      true       4.6.0   false
statistics-20140512 1     p      192.168.2.96 _lq9         28161    5705014            0  652.9mb     1504816 true      true       4.9.0   false
statistics-20140512 2     p      192.168.2.95 _owl         32277    5226161            0    545mb     1199048 true      true       4.6.0   false
statistics-20140512 2     p      192.168.2.95 _p8o         32712    8039384            0  844.5mb     2655112 true      true       4.9.0   false
statistics-20140512 3     p      192.168.2.96 _jsn         25655    4646448            0  483.6mb      969208 true      true       4.6.0   false
statistics-20140512 3     p      192.168.2.96 _rts         36064    9493601            0 1002.7mb     3597432 true      true       4.9.0   false
statistics-20140512 4     p      192.168.2.96 _g6x         20985    7353308            0  818.9mb     2339240 true      true       4.6.0   false
statistics-20140512 4     p      192.168.2.96 _isp         24361    3456995            0  392.7mb      780912 true      true       4.9.0   false
```

Btw, third node is doing rebalancing too.

Can you explain how best distribution for an index could change after another index is deleted? Link to an issue with discussion could help too!

My understanding of placement policy was "evenly distribute shards between nodes taking disk space into account". This way if index is deleted it might be a good idea to move a some shards if some disk usage disbalance threshold is violated.
</comment><comment author="s1monw" created="2014-12-09T16:42:19Z" id="66313454">&gt; "evenly distribute shards between nodes taking disk space into account"

no that is not what ES is doing. We try to evenly place shards across the index while making sure we evenly distribute on a shard level and an index level such that all nodes have an even number of shards and the number of shard per index is minimized per node. if you delete an index an in-balance can happen and we moving stuff over from one node to another
</comment><comment author="rtkmhart" created="2015-04-15T13:23:04Z" id="93394595">I've been seeing very similar behaviour in 1.4.1, where shard0 of an index is moving from node1 to node2 and shard1 is moving from node2 to node1. In my case we noticed because are shards are too big for sane movement, but that this happens at all is a bit bizarre. We're in the process of upgrading to 1.5.1 but if we see this again I'll bump this issue and open a support case.
</comment><comment author="lokeshhctm" created="2015-09-09T19:15:15Z" id="139018312">I see the same issue while keep indexes for last 30 day and delete previous days of indexes.
Whenever we delete indexes, rebalance starts and it takes a lot of time.
Can we stop rebalancing while we delete indexes?
</comment><comment author="clintongormley" created="2015-11-21T22:25:31Z" id="158686740">So many allocation improvements have been made since 1.4 (eg https://github.com/elastic/elasticsearch/pull/14678, https://github.com/elastic/elasticsearch/pull/14652, https://github.com/elastic/elasticsearch/pull/14259, https://github.com/elastic/elasticsearch/pull/13512, https://github.com/elastic/elasticsearch/pull/12947, https://github.com/elastic/elasticsearch/pull/12551) that I'm going to close this one.  Please reopen if you're seeing the same problems on 2.1.0 or above.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Drop support for state written pre 0.90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8850</link><project id="" key="" /><description>Today we have several upgrade methods that can read state written
pre 0.90 or even pre 0.19. Version 2.0 should not support these state
formats. Users of these version should upgrade to a 1.x or 0.90.x version
first.
</description><key id="51437892">8850</key><summary>Drop support for state written pre 0.90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2014-12-09T15:07:53Z</created><updated>2015-06-06T16:16:54Z</updated><resolved>2014-12-10T16:07:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-10T01:31:15Z" id="66390735">LGTM, just one minor comment.
</comment><comment author="s1monw" created="2014-12-10T09:25:46Z" id="66424371">@rjernst I pushed updates - I also remove the "remove files from directory" part since I think it's too dangerous. :)
</comment><comment author="rjernst" created="2014-12-10T15:58:26Z" id="66473587">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file></files><comments><comment>[CORE] Drop support for state written pre 0.90</comment></comments></commit></commits></item><item><title>Please close! Newbie question not quite understanding it! doh!!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8849</link><project id="" key="" /><description>It has been sat for 15 minutes not doing anything, just with the cursor flashing... no errors.... full console output below.... help!?
[2014-12-09 14:46:50,975][WARN ][bootstrap                ] jvm uses the client
vm, make sure to run `java` with the server vm for best performance by adding `-
server` to the command line
[2014-12-09 14:46:51,309][INFO ][node                     ] [Deadhead] version[1
.4.1], pid[6212], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-09 14:46:51,310][INFO ][node                     ] [Deadhead] initializ
ing ...
[2014-12-09 14:46:51,324][INFO ][plugins                  ] [Deadhead] loaded []
, sites []
[2014-12-09 14:46:57,080][INFO ][node                     ] [Deadhead] initializ
ed
[2014-12-09 14:46:57,081][INFO ][node                     ] [Deadhead] starting
...
[2014-12-09 14:46:59,244][INFO ][transport                ] [Deadhead] bound_add
ress {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.97.114.148:9300]}
[2014-12-09 14:46:59,642][INFO ][discovery                ] [Deadhead] elasticse
arch/-DFbWuNIQZO0pq6Pwy8ltg
[2014-12-09 14:47:03,512][INFO ][cluster.service          ] [Deadhead] new_maste
r [Deadhead][-DFbWuNIQZO0pq6Pwy8ltg][GM-Contractor1][inet[/10.97.114.148:9300]],
 reason: zen-disco-join (elected_as_master)
[2014-12-09 14:47:03,670][INFO ][gateway                  ] [Deadhead] recovered
 [0] indices into cluster_state
[2014-12-09 14:47:03,720][INFO ][http                     ] [Deadhead] bound_add
ress {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.97.114.148:9200]}
[2014-12-09 14:47:03,721][INFO ][node                     ] [Deadhead] started
</description><key id="51437672">8849</key><summary>Please close! Newbie question not quite understanding it! doh!!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dreadeddev</reporter><labels /><created>2014-12-09T15:06:03Z</created><updated>2014-12-09T16:21:56Z</updated><resolved>2014-12-09T15:23:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dreadeddev" created="2014-12-09T15:06:30Z" id="66296044">![image](https://cloud.githubusercontent.com/assets/9987089/5359400/f17c8e70-7fb4-11e4-8719-c61610611368.png)
</comment><comment author="dadoonet" created="2014-12-09T15:24:50Z" id="66299301">Closing as requested. Note that you can close an issue yourself.
Also, please ask your questions next time on the mailing list. We can help you there.
</comment><comment author="dreadeddev" created="2014-12-09T16:21:56Z" id="66309508">Thanks, apologies I don't think I've signed up for the mailing list,  will
take a look once windows finishes updating!! Lol.
On 9 Dec 2014 15:24, "David Pilato" notifications@github.com wrote:

&gt; Closed #8849 https://github.com/elasticsearch/elasticsearch/issues/8849.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8849#event-204908092
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for adding multiple fields to QueryStringQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8848</link><project id="" key="" /><description>Adds support for adding multiple fields at once to QueryStringQueryBuilder. It is already possible to do this via JSON, but the Java support is missing. 

&lt;pre&gt;&lt;code&gt;
{
    "query_string" : {
        "fields" : ["content", "name"],
        "query" : "this AND that"
    }
}
&lt;/code&gt;&lt;/pre&gt;

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html
</description><key id="51437613">8848</key><summary>Add support for adding multiple fields to QueryStringQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">olliwer</reporter><labels><label>:Java API</label><label>enhancement</label></labels><created>2014-12-09T15:05:32Z</created><updated>2015-06-19T10:33:19Z</updated><resolved>2015-06-19T10:33:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-10T17:16:54Z" id="66487419">Hi @Noemj 

Thanks for the PR - we'll take a look and get back to you. Meanwhile, please could I ask you to sign the CLA? http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="olliwer" created="2014-12-11T07:49:33Z" id="66583662">Gotcha. I've signed it now.
</comment><comment author="jpountz" created="2014-12-15T12:06:17Z" id="66985189">This is something which is already possible to do via chaining:

``` java
QueryStringBuilders.queryString("bee bop^2").field("foo").field("bar", 2);
```

So I'm wondering if we should add this method or should try to keep the API minimal and with a single way to configure multiple fields?
</comment><comment author="olliwer" created="2014-12-15T15:31:31Z" id="67010893">It is possible, but quite verbose in the case of very many fields.
</comment><comment author="cbuescher" created="2015-06-19T10:33:17Z" id="113464129">@olliwer Thanks for the PR, we are currently doing a bigger refactoring of queries and query builders and the same issue popped up in discussions there multiple times. The general feeling about this "nice to have but not necessary" setters was that we'd like to avoid them when possible in order to keep the builder API small. I'm closing this PR, please reopen of you like to discuss this further.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Hard wire utf-8 encoding, so unicode filenames work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8847</link><project id="" key="" /><description>Today e.g unicode index names are allowed, but you will get no error if a machine is misconfigured. Because they are allowed, we should explicitly set UTF-8 encoding by default.

For a server-side app, this is really a good flag to pass to the JVM anyway. it will dodge bugs in third party libraries and so on.
</description><key id="51433689">8847</key><summary>Hard wire utf-8 encoding, so unicode filenames work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>bug</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-09T14:32:40Z</created><updated>2015-06-07T18:01:01Z</updated><resolved>2014-12-15T15:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-09T15:02:37Z" id="66295398">LGTM
</comment><comment author="javanna" created="2014-12-09T16:28:35Z" id="66310789">Shall we also add the same to the pom so tests will run with the same encoding?
</comment><comment author="rmuir" created="2014-12-09T18:09:01Z" id="66328573">I think tests should be handled differently and separately, there is more involved there. For example, we can't assume this default for things like client code and so on.
</comment><comment author="s1monw" created="2014-12-15T11:34:53Z" id="66982230">@rmuir can you get this in?
</comment><comment author="rmuir" created="2014-12-15T15:41:57Z" id="67012697">I removed 1.3.x since it is structured differently (e.g. no elasticsearch.in.bat)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Wire utf-8 encoding, so unicode filenames work</comment></comments></commit></commits></item><item><title>Problems when upgrade elasticsearch from 1.3.5 to 1.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8846</link><project id="" key="" /><description>Hi, all
I want to upgrade my elasticsearch from 1.3.5 to 1.4.1. I followed this page http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#rolling-upgrades and performed a rolling upgrade. When I startup a new version node, I got some error logs like this:

```
[2014-12-09 09:39:26,418][WARN ][cluster.service          ] [5xv144] failed to apply updated cluster state:
version [1570], source [zen-disco-receive(from master [[5xv122][LNJfg356QAyA4biMKIII1A][5xv122][inet[/10.1.10.122:9300]]{action.disable_shutdown=true, data=false, max_local_storage_nodes=1, master=true}])]
nodes:
   [5xv123][3omD8UaXQTS552OpQd8e9w][5xv123][inet[/10.1.10.123:9300]]{action.disable_shutdown=true, max_local_storage_nodes=1, master=false}
   [5xv121][YSMiYvyySeKs2muCYSRU9A][5xv121][inet[/10.1.10.121:9300]]{action.disable_shutdown=true, max_local_storage_nodes=1, master=false}
   (many nodes and shards information)
   ..........
org.elasticsearch.indices.IndexCreationException: [bbs] failed to create index
        at org.elasticsearch.indices.InternalIndicesService.createIndex(InternalIndicesService.java:301)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:310)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:179)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:431)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.ElasticsearchIllegalStateException: [index.version.created] is not present in the index settings for index with uuid: [null]
        at org.elasticsearch.Version.indexCreated(Version.java:401)
        at org.elasticsearch.index.analysis.Analysis.parseAnalysisVersion(Analysis.java:95)
        at org.elasticsearch.index.analysis.AbstractTokenizerFactory.&lt;init&gt;(AbstractTokenizerFactory.java:41)
        at org.elasticsearch.index.analysis.IkTokenizerFactory.&lt;init&gt;(IkTokenizerFactory.java:17)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:781)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:777)
        at org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:221)
        at com.sun.proxy.$Proxy16.create(Unknown Source)
        at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:82)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
        at org.elasticsearch.indices.InternalIndicesService.createIndex(InternalIndicesService.java:299)
        ... 7 more
```
</description><key id="51427226">8846</key><summary>Problems when upgrade elasticsearch from 1.3.5 to 1.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">holdstop</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-12-09T13:29:21Z</created><updated>2014-12-10T08:59:23Z</updated><resolved>2014-12-10T02:33:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-09T15:11:54Z" id="66296980">hey can you tell which index is currently being created here and what version this index was created with?
</comment><comment author="holdstop" created="2014-12-09T16:31:12Z" id="66311297">This index was created with version 0.90.2. I upgraded the elasticsearch to 1.3.5 several days ago. Now I need to upgrade it to 1.4.1 to work with kibana4. Here is the index settings:

```
curl -XGET 'http://eshost/bbs/_settings?pretty=true'
```

```
{
  "bbs" : {
    "settings" : {
      "index" : {
        "number_of_replicas" : "1",
        "number_of_shards" : "20",
        "version" : {
          "created" : "900299"
        }
      }
    }
  }
}
```
</comment><comment author="s1monw" created="2014-12-09T17:00:53Z" id="66316864">I suspect this is a problem with `IkTokenizerFactory` what version of this plugin are you using?
</comment><comment author="holdstop" created="2014-12-10T02:33:50Z" id="66395686">Thanks a lot!
I am using `analysis-ik` plugin. I upgraded it to the last version, problem solved! 
</comment><comment author="s1monw" created="2014-12-10T08:59:23Z" id="66421486">oh good thanks for letting me know. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] LoggingListener to restore the initial logger levels after any modification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8845</link><project id="" key="" /><description>Recent changes made to LoggingListener and pushed with #8820 caused the original logger levels not to be reset after modifications, as the new state was saved for restore instead of the previous one.

Added unit tests for LoggingListener as well.
</description><key id="51423623">8845</key><summary>[TEST] LoggingListener to restore the initial logger levels after any modification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-09T12:49:04Z</created><updated>2015-03-19T10:15:48Z</updated><resolved>2014-12-09T13:26:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-09T13:17:25Z" id="66280765">look awesome +1 to push and thanks for the test!!!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/junit/listeners/LoggingListener.java</file><file>src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java</file></files><comments><comment>[TEST] LoggingListener to restore the initial logger levels after any modification</comment></comments></commit></commits></item><item><title>Suggest with ignoring stopwords doesn't work when we look for second word</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8844</link><project id="" key="" /><description>Elasticsearch 1.4.0
Based on this blog post: http://www.elasticsearch.org/blog/you-complete-me/ I would to suggest ignoring stopwords:

```
PUT /myindex
{
   "mappings": {
      "actor": {
         "properties": {
            "name": {
               "type": "completion",
               "index_analyzer": "standard",
               "search_analyzer": "standard",
               "preserve_position_increments": false,
               "preserve_separators": false
            }
         }
      }
   }
}
```

Example data:

```
POST /myindex/actor
{ "name" : "Johnny Deep" }
```

```
POST /myindex/actor
{ "name" : "Johnny Cash" }
```

```
POST /myindex/actor
{ "name" : "Johnnyyy Deep" }
```

And this Query works nice:

```
POST /myindex/_suggest
{
  "actorsuggest" : {
    "text" : "jo",
    "completion" : {
      "field" : "name"
    }
  }
}
```

it return this as expected:

```
{
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "actorsuggest": [
      {
         "text": "jo",
         "offset": 0,
         "length": 2,
         "options": [
            {
               "text": "Johnny Cash",
               "score": 1
            },
            {
               "text": "Johnnyyy Deep",
               "score": 1
            },
            {
               "text": "Johnny Deep",
               "score": 1
            }
         ]
      }
   ]
}
```

And this query:

```
POST /myindex/_suggest
{
  "actorsuggest" : {
    "text" : "deep",
    "completion" : {
      "field" : "name"
    }
  }
}
```

should return me suggestions with "Johnny Deep" and "Johnnyyy Deep" but I receive only this:

```
{
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "actorsuggest": [
      {
         "text": "deep",
         "offset": 0,
         "length": 4,
         "options": []
      }
   ]
}
```
</description><key id="51419889">8844</key><summary>Suggest with ignoring stopwords doesn't work when we look for second word</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mstachniuk</reporter><labels /><created>2014-12-09T12:07:44Z</created><updated>2014-12-09T16:59:14Z</updated><resolved>2014-12-09T13:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T13:40:42Z" id="66283574">Hi @mstachniuk 

First, the standard analyzer doesn't use stopwords. Second, `jo` is not a stopword.  Third, the completion suggester is documented to start at the left side of all provided strings, and to work towards the right: the word `deep` is not on the left side of any strings.

The completion suggester is working as intended.  I suggest asking questions like these in the forum instead, as this issues list is for bug reports and feature requests.

thanks
</comment><comment author="mstachniuk" created="2014-12-09T14:00:20Z" id="66286023">This http://www.elasticsearch.org/blog/you-complete-me/ blog post say that standard analyzer use stopwords:

&gt; Note: The added complication that stopwords brings to the suggester is also the reason why the default analyzer is the simple analyzer (which doesn’t use stopwords) instead of the standard analyzer.

What is a definition of "stopword"?
</comment><comment author="clintongormley" created="2014-12-09T16:59:14Z" id="66316573">Hi @mstachniuk 

That blog post is old.  The `standard` analyzer stopped using stopwords in version 1.  For a definition, have a look at: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/stopwords.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>how does elasticsearch store data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8843</link><project id="" key="" /><description>we are trying to use elasticsearch, logstash and kibana to manage our logs. We have several questions about the storage of data. Could you please to help us to figure it out, please? when elasticsearch gets all logs, where does it store them? Because we have about 200G logs every day, so we want to know whether elasticsearch could remove old logs automatically. If it does, how can we set that up and how long will it keep data?
thank you
</description><key id="51418914">8843</key><summary>how does elasticsearch store data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">midouwo</reporter><labels /><created>2014-12-09T11:55:02Z</created><updated>2014-12-09T13:36:51Z</updated><resolved>2014-12-09T13:36:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2014-12-09T12:04:42Z" id="66272885">Elasticsearch will store data as long as it has disk space on its nodes.  Its stores this on the local disk, for details on where take a look at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-dir-layout.html#setup-dir-layout

You can manage the retention with Elasticsearch Curator (https://github.com/elasticsearch/curator).

You may also want to ask these sorts of questions on the Elasticsearch mailing list at https://groups.google.com/forum/#!forum/elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>make it possible to register a mapper for dynamic arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8842</link><project id="" key="" /><description>A DynamicArrayFieldMapperBuilderFactory has to be implemented which returns a
Builder for a mapper that is able to handle dynamic arrays and creates FieldMappers and/or ObjectMappers for it.
It's main usecase is mapping arrays explicitly. For this to work consistently even with dynamic fields
it is necessary to catch parsing dynamic array columns in the indexed sources.

If no DynamicArrayFieldMapperBuilderFactory is bound via guice, everything is working as before.

An example for usage of this feature is: https://github.com/crate/crate/blob/1c6252c27d6b85642915f83ceab48660e47b8c2c/sql/src/main/java/org/elasticsearch/index/mapper/core/ArrayMapper.java
There we actually added an explicit array type and use it for dynamic arrays as well to stay consistent even with dynamically added columns. 
</description><key id="51414958">8842</key><summary>make it possible to register a mapper for dynamic arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfelsche</reporter><labels><label>discuss</label><label>review</label><label>stalled</label></labels><created>2014-12-09T11:09:24Z</created><updated>2016-03-08T19:08:49Z</updated><resolved>2016-03-08T19:08:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-01-16T10:39:48Z" id="70236305">I am marking this issue as stalled as we are currently trying to reduce the complexity of mappings. We should revisit it once we're done (hopefully 2.0).
</comment><comment author="clintongormley" created="2016-03-08T19:08:49Z" id="193920544">Hi @mfelsche 

Sorry this has taken so long to look at.  This is such a big departure from the way we handle arrays today, and a feature that nobody else has requested, that I think the change would really muddy the waters.  Thanks, but I'm going to close this PR.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Wrong HTTP method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8841</link><project id="" key="" /><description /><key id="51414817">8841</key><summary>Wrong HTTP method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rsilva4</reporter><labels /><created>2014-12-09T11:07:59Z</created><updated>2014-12-09T14:59:06Z</updated><resolved>2014-12-09T13:34:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T13:34:02Z" id="66282756">Hi @rsilva4 

GET with body is perfectly acceptable, as long as the HTTP library that you use supports it.

thanks
</comment><comment author="rsilva4" created="2014-12-09T14:58:34Z" id="66294700">Ok... I was struggling with this for a while. Using head plugin was the "problem". Sorry for creating noise.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>nested aggregation does not scope with nested filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8840</link><project id="" key="" /><description>I have a filtered query like : 

```
filtered: {
    query:{match_all:{}},
    filter: {
        nested: {
            path: 'tags',
            filter: {
                bool: {
                    must: mustArray
                }
            }
        }
    }
}
```

and in the same body I have an aggregation like : 

```
aggs: {
    tags: {
        nested: {path: 'tags'},
        aggs: {
            colors: {terms: {field: 'tags.colors.raw'}},
            brands: {terms: {field: 'tags.brand.raw'}}
        }
    },
    type: {terms: {field: 'type'}}
}
```

My understanding is that the aggregation should be scoped with the provided filter. But in this specific case it does not. 
</description><key id="51412838">8840</key><summary>nested aggregation does not scope with nested filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aresn</reporter><labels /><created>2014-12-09T10:46:14Z</created><updated>2014-12-09T11:13:00Z</updated><resolved>2014-12-09T11:13:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE when plugins dir is inaccessible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8839</link><project id="" key="" /><description>Steps to reproduce:
1. Download fresh es.
2. `sudo mkdir plugins &amp;&amp; sudo chmod 0700 plugins`
3. Start elasticsearch

```
elasticsearch-1.4.1 λ ./bin/elasticsearch
[2014-12-09 12:18:59,025][INFO ][node                     ] [Piotr Rasputin] version[1.4.1], pid[16338], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-09 12:18:59,025][INFO ][node                     ] [Piotr Rasputin] initializing ...
{1.4.1}: Initialization Failed ...
- NullPointerException[null]
```

Closes #8837.
</description><key id="51402203">8839</key><summary>NPE when plugins dir is inaccessible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-09T08:59:37Z</created><updated>2015-06-08T00:16:32Z</updated><resolved>2014-12-21T11:21:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-12-09T16:13:52Z" id="66308021">@dadoonet full stack trace in logs would help a lot. 

```
{1.4.1}: Initialization Failed ...
- NullPointerException[null]
```

This wasn't in logs, actually. I had to run es manually in foreground to see it. Stack trace in log could give some clues to avoid running `strace`.
</comment><comment author="dadoonet" created="2014-12-09T16:19:33Z" id="66309081">@bobrik Basically you can run elasticsearch with `-Des.logger.level=TRACE`. It will give some details. For example on master:

```
{2.0.0-SNAPSHOT}: Initialization Failed ...
- ElasticsearchIllegalStateException[Can't load site  plugins]
    AccessDeniedException[/Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins]
org.elasticsearch.ElasticsearchIllegalStateException: Can't load site  plugins
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:145)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:149)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:72)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:196)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.nio.file.AccessDeniedException: /Users/dpilato/Documents/Elasticsearch/dev/elasticsearch/plugins
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:406)
    at java.nio.file.Files.newDirectoryStream(Files.java:413)
    at org.elasticsearch.plugins.PluginsService.loadSitePlugins(PluginsService.java:476)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:140)
    ... 9 more
```
</comment><comment author="bobrik" created="2014-12-09T16:22:14Z" id="66309569">Yep, but maybe trace should be in logs even if not enabled if we fail at initialization?
</comment><comment author="s1monw" created="2014-12-15T11:40:12Z" id="66982740">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>problem in param value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8838</link><project id="" key="" /><description>hi all,
i am facing some issue in following crul command, i hope you will help me out from this
curl -XPOST 'localhost:9200/_search/template' -d '{ "template":{"_source": {"include": ["{{#query1}}","{{.}}","{{/query1}}"],"exclude": ["{{#query2}}","{{.}}","{{/query2}}"]},"query":{"match": {"code": "{{query_string}}"}}},"params": {"query_string": "5J1364-AP57","query1":["title","code" ],"query2":["variants"]}}'
here i want to know how to pass the param value to include and exclude as array of value

here i am getting  output like this

{"took":28,"timed_out":false,"_shards":{"total":30,"successful":29,"failed":1,"failures":[{"index":"product","shard":2,"status":500,"reason":"StringIndexOutOfBoundsException[String index out of range: 0]"}]},"hits":{"total":1,"max_score":2.3634837,"hits":[]}}
</description><key id="51401332">8838</key><summary>problem in param value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vahith</reporter><labels /><created>2014-12-09T08:49:56Z</created><updated>2014-12-09T13:59:06Z</updated><resolved>2014-12-09T13:30:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T13:30:06Z" id="66282235">Hi @vahith 

This issue is fixed in v1.5.0: https://github.com/elasticsearch/elasticsearch/pull/8255
</comment><comment author="vahith" created="2014-12-09T13:57:26Z" id="66285638">Hi @clintongormley , i need curl command for array of string pass in include and exclude using template.
or suggest me, how  to access the array of string
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE when plugins dir is inaccessible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8837</link><project id="" key="" /><description>1. Download fresh es.
2. `sudo mkdir plugins &amp;&amp; sudo chmod 0700 plugins`
3. Start elasticsearch

```
elasticsearch-1.4.1 λ ./bin/elasticsearch
[2014-12-09 12:18:59,025][INFO ][node                     ] [Piotr Rasputin] version[1.4.1], pid[16338], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-09 12:18:59,025][INFO ][node                     ] [Piotr Rasputin] initializing ...
{1.4.1}: Initialization Failed ...
- NullPointerException[null]
```

Of course, a little bit of `strace` or tooling of a choice can reveal `EPERM`, but that's not very friedly way, especially if you are in the middle of upgrade.
</description><key id="51399419">8837</key><summary>NPE when plugins dir is inaccessible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>bug</label></labels><created>2014-12-09T08:22:53Z</created><updated>2014-12-21T11:21:33Z</updated><resolved>2014-12-21T11:21:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-09T08:25:33Z" id="66248838">@dadoonet can you take a look at this please? I suspect it's already fixed in master though
</comment><comment author="dadoonet" created="2014-12-09T08:34:22Z" id="66249605">@s1monw I can reproduce it on 1.x and master branches. Working on a fix. Should this go to 1.4 as well?

@bobrik Thanks for reporting it!
</comment><comment author="s1monw" created="2014-12-09T08:36:23Z" id="66249789">+1 for `1.4`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>Plugins: NPE when plugins dir is inaccessible</comment></comments></commit></commits></item><item><title>Ban java.io.File in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8836</link><project id="" key="" /><description>Restrict use of java.io.File to 5 methods (excluded), but otherwise ban.
This is a prerequisite to do any mocking here.

I don't try to do any heavy cleanup on these tests, I am not familiar with them.
So this is mostly a rote straightforward conversion.
</description><key id="51392065">8836</key><summary>Ban java.io.File in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-12-09T06:22:08Z</created><updated>2014-12-09T11:01:13Z</updated><resolved>2014-12-09T11:01:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-12-09T08:08:12Z" id="66247374">Thanks a lot @rmuir I had it on my todos to have a look at rest tests and move them over, you definitely beat me to it, cool stuff!
</comment><comment author="s1monw" created="2014-12-09T08:38:58Z" id="66250035">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/benchmark/common/lucene/uidscan/LuceneUidScanBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/BasicScriptBenchmark.java</file><file>src/test/java/org/elasticsearch/bwcompat/StaticIndexBackwardCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeTests.java</file><file>src/test/java/org/elasticsearch/common/ChannelsTests.java</file><file>src/test/java/org/elasticsearch/common/PidFileTests.java</file><file>src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java</file><file>src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java</file><file>src/test/java/org/elasticsearch/common/io/FileSystemUtilsTests.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/LoggingConfigurationTests.java</file><file>src/test/java/org/elasticsearch/env/EnvironmentTests.java</file><file>src/test/java/org/elasticsearch/env/NodeEnvironmentTests.java</file><file>src/test/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormatTest.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/HunspellTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/FileBasedMappingsTests.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryTest.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/store/DirectoryUtilsTest.java</file><file>src/test/java/org/elasticsearch/index/translog/AbstractSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/fs/FsBufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/fs/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/indices/analyze/HunspellServiceTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file><file>src/test/java/org/elasticsearch/nodesinfo/SimpleNodesInfoTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file><file>src/test/java/org/elasticsearch/plugins/SitePluginTests.java</file><file>src/test/java/org/elasticsearch/script/OnDiskScriptTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java</file><file>src/test/java/org/elasticsearch/snapshots/AbstractSnapshotTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/RepositoriesTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SnapshotBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchBackwardsCompatIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/ExternalNode.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParser.java</file><file>src/test/java/org/elasticsearch/test/rest/spec/RestSpec.java</file><file>src/test/java/org/elasticsearch/test/rest/support/FileUtils.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file><file>src/test/java/org/elasticsearch/watcher/FileWatcherTest.java</file></files><comments><comment>Ban java.io.File in tests.</comment></comments></commit></commits></item><item><title>The fields are not indexed sometimes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8835</link><project id="" key="" /><description>version: elasticsearch-1.1.1-1.noarch

As you can see from the blue line in the graph, it shows up sometimes. However, there are indeed lots of data about it when it hides.
![image](https://cloud.githubusercontent.com/assets/10124008/5352226/19d9803e-7f9d-11e4-9785-962fbcec5dc5.png)

The ES cluster status looks good:
{
  "cluster_name" : "es",
  "status" : "green",
 "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 525,
  "active_shards" : 525,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0
}

Here is an example, there are some data with "@fields.reponse 302".
![image](https://cloud.githubusercontent.com/assets/10124008/5352242/77062b7c-7f9d-11e4-94a6-5ffd707ee154.png)

However, it returns empty when we query the data in the same time range. Any idea? Thank you!
![image](https://cloud.githubusercontent.com/assets/10124008/5352258/afa9c5b0-7f9d-11e4-8248-2f2b4a0e80a9.png)
</description><key id="51386580">8835</key><summary>The fields are not indexed sometimes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">starndawn</reporter><labels /><created>2014-12-09T04:21:35Z</created><updated>2014-12-09T13:23:56Z</updated><resolved>2014-12-09T13:23:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T13:23:56Z" id="66281504">Hi @starndawn 

Please ask questions like these in the forums. The Github issues list is for bug reports and feature requests.

thnkas
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dates: Fixed parsing issue with exact dates when using lte.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8834</link><project id="" key="" /><description>Date parsing uses a flag to indicate whether the rounding should be
inclusive or exclusive.  This change fixes the parsing to not use this
logic in the case of exact dates that do not have rounding syntax.

closes #8490
</description><key id="51367035">8834</key><summary>Dates: Fixed parsing issue with exact dates when using lte.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Dates</label></labels><created>2014-12-08T23:22:50Z</created><updated>2015-03-19T10:18:09Z</updated><resolved>2014-12-09T22:19:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-09T18:48:56Z" id="66335062">Thanks @brwe I pushed changes addressing your comments.
</comment><comment author="rjernst" created="2014-12-09T22:19:51Z" id="66369177">I'm closing this since it can be accomplished in 1.x by disabling `mapping.date.round_ceil` and will work on the fix for #8598 to remove the setting altogether.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make the the number of ports to scan when trying to join a cluster configurable, and set the default to 2.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8833</link><project id="" key="" /><description>Closes #7090
</description><key id="51366448">8833</key><summary>Make the the number of ports to scan when trying to join a cluster configurable, and set the default to 2.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ejain</reporter><labels><label>:Settings</label><label>review</label></labels><created>2014-12-08T23:16:51Z</created><updated>2016-03-08T19:31:33Z</updated><resolved>2016-03-08T19:31:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-31T07:38:53Z" id="87979529">@ejain sorry for the long wait. Will you be willing to pick this one up again? It looks good,but misses a test (maybe expose configuredTargetNode as a package private getter and add a unit test UnicastZenPingTests).
</comment><comment author="ejain" created="2015-03-31T17:36:26Z" id="88181928">I'd be happy to add a test, but since this isn't a trivial unit test, and I'm not familiar with elasticsearch's test scaffolding, it might be a while before I get to have a look.
</comment><comment author="bleskes" created="2015-03-31T20:39:57Z" id="88237739">@ejain I think we can make it a basic unit test (inheriting from ElasticsearchTestCase ), with my suggestion above: 

&gt; maybe expose configuredTargetNode as a package private getter and add a unit test to UnicastZenPingTests

That means you just need to instantiate UnicastZenPing with two different settings and see that number of hosts is good, plus that the ports is what you expect.

Let me know if you don't have time for this (it has been a while). I can pick it up , but please do sign the CLA so I can take your PR as a base (see https://www.elastic.co/contributing-to-elasticsearch )
</comment><comment author="ejain" created="2015-04-01T00:06:35Z" id="88290697">Signed the CLA. Feel free to pick this up, could be a few weeks before I'm able to get back to this.
</comment><comment author="drekbour" created="2016-01-06T16:50:06Z" id="169385242">I've just followed half a dozen issues through to this one which appears to be the leading edge of this problem. Since the current state of play has not been changed in several years, can you at least correct the public documentation which still clearly describes using port ranges although everyone on the ES side knows they are ignored.
https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html
</comment><comment author="bleskes" created="2016-01-06T17:12:37Z" id="169392484">@drekbour agreed the documentation is confusing. Parsing ranges is mostly there to be able to work out of the box with the default transport settings which use 9300-9400 . Not sure much for external configuration. Agreed it's super confusing now. Do you mind opening a new issue as this PR tries to do something else, which is make it configurable. I believe that's the wrong approach. We should document what we do - not support ranges when specified and default to the first 5 local ports when not configured.
</comment><comment author="clintongormley" created="2016-03-08T19:31:33Z" id="193934482">The networking code has changed considerably, and the documentation has been updated to reflect the current state.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add more fine grained memory stats from Lucene segment reader to index stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8832</link><project id="" key="" /><description>This is a start to exposing memory stats improvements from Lucene 5.0.
This adds the following categories of Lucene index pieces to index stats:
- Terms
- Stored fields
- Term Vectors
- Norms
- Doc values
</description><key id="51365213">8832</key><summary>Add more fine grained memory stats from Lucene segment reader to index stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T23:03:37Z</created><updated>2015-06-07T17:34:53Z</updated><resolved>2014-12-08T23:30:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-12-08T23:08:59Z" id="66205752">looks good, thank you!
</comment><comment author="kimchy" created="2014-12-08T23:26:39Z" id="66207897">LGTM, wonderful!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add test for ESDirectoryReader cache key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8831</link><project id="" key="" /><description>If this is not implemented correctly, caches wont correctly work per-segment...
</description><key id="51364339">8831</key><summary>add test for ESDirectoryReader cache key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-12-08T22:54:49Z</created><updated>2014-12-08T22:59:38Z</updated><resolved>2014-12-08T22:59:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-08T22:56:55Z" id="66204145">LGTM - thanks for doing this
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/common/lucene/index/ElasticsearchDirectoryReaderTests.java</file></files><comments><comment>Add test that ES filterreader getCoreCacheKey() behaves correctly.</comment></comments></commit></commits></item><item><title>Restore process can restore incompatible `minimum_master_nodes` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8830</link><project id="" key="" /><description>If I have 3 masters and accidentally set `minimum_master_nodes` to 4 dynamically, the cluster will stop. While the cluster is stopped, there's no way to update settings, so I have to do a full restart.

If I updated a persistent setting, then not even a full restart will fix the issue. I have to manually edit cluster state.

You could get out of this situation by adding master nodes until the new minimum is reached, but that's not a complete solution because I might have fat fingered `minimum_master_nodes` to 100 or something like that.

I think it'd be worth adding some validation to ensure that an update to `minimum_master_nodes` won't accidentally put the cluster in an unrecoverable state.
</description><key id="51358266">8830</key><summary>Restore process can restore incompatible `minimum_master_nodes` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">grantr</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2014-12-08T22:00:47Z</created><updated>2015-01-18T02:31:07Z</updated><resolved>2015-01-18T02:31:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-12-08T22:08:41Z" id="66197015">Agreed it's a pain.  This is already fixed in https://github.com/elasticsearch/elasticsearch/pull/8321 , which will be released with 1.5.0  

I'm closing this, but if you feel something is missing from that PR, please feel free to re-open.
</comment><comment author="grantr" created="2014-12-08T22:13:54Z" id="66197811">Excellent. Thanks much @bleskes!
</comment><comment author="grantr" created="2014-12-10T20:49:54Z" id="66521443">@bleskes it just occurred to me that restoring global state from a snapshot can update persistent settings:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html#_restore

&gt; The restored persistent settings are added to the existing persistent settings.

@imotov what happens if a snapshot restores a persistent setting for `minimum_master_nodes` that is greater than the current master count?
</comment><comment author="imotov" created="2014-12-24T01:12:15Z" id="68016174">@grantr yes, this indeed can be an issue. Thanks!
</comment><comment author="grantr" created="2014-12-24T01:45:36Z" id="68017857">Thanks for fixing this @imotov!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/cluster/settings/DynamicSettings.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: add validation of restored persistent settings</comment></comments></commit></commits></item><item><title>Ordering with a children aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8829</link><project id="" key="" /><description>I'm attempting to use the children aggregation along with ordering and don't believe it's supported. I'm wondering if there is a way to do this I'm not aware of? Here's a simplified example of what I'm trying to do.

``` json
GET /reports/product/_search
{
  "aggregations": {
    "brand": {
      "aggregations": {
        "item": {
          "aggregations": {
            "total_revenue": {
              "sum": {
                "field": "item.PriceTotal"
              }
            }
          },
          "children": {
            "type": "item"
          }
        }
      },
      "terms": {
        "field": "BrandId",
        "order": {
          "item.total_revenue": "desc"
        }
      }
    }
  }
}
```

``` json
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[UP_U15O5Sd6Gcl1cdDha6w][reports][0]: AggregationExecutionException[Invalid terms aggregation order path [item.total_revenue]. Ordering on a single-bucket aggregation can only be done on its doc_count. Either drop the key (a la \"item\") or change it to \"doc_count\" (a la \"item.doc_count\")]}]",
   "status": 500
}
```
</description><key id="51353500">8829</key><summary>Ordering with a children aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snikch</reporter><labels /><created>2014-12-08T21:18:20Z</created><updated>2014-12-09T19:05:16Z</updated><resolved>2014-12-09T13:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T13:19:43Z" id="66281012">Hi @snikch 

Your `order` syntax is incorrect.  Try this instead:

```
      "item&gt;total_revenue": "desc"
```
</comment><comment author="snikch" created="2014-12-09T19:05:16Z" id="66337894">Ah, that'll do it. Thanks @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>script_score comparison operators failing on 1.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8828</link><project id="" key="" /><description>Hi, 

We recently upgraded from v1.3.2 to v1.4.1.  A `function_score` query using `script_score` is now failing when using `&gt;` or `&lt;` operators on the `_score`.  Our script worked fine in 1.3, but after upgrading to 1.4.1 it fails with the following `GroovyRuntimeException`:

&gt; QueryPhaseExecutionException[[script_score_test][3]: query[filtered(function score (ConstantScore(_:_),function=script[if (_score &gt; max_val) {0.0} else {_score}], params [{max_val=10.0}]))-&gt;cache(_type:my_type)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: GroovyScriptExecutionException[GroovyRuntimeException[Cannot compare org.elasticsearch.script.ScoreAccessor with value 'org.elasticsearch.script.ScoreAccessor@2553f628' and java.lang.Double with value '10.0']];

Not sure if this is a bug in Groovy or we need to change our script syntax to be compatible.  Here are some sample steps to reproduce.

``` javascript
DELETE /script_score_test
PUT /script_score_test/my_type/1
{
    "prop1" : 5.3,
    "prop2" : 6.7
}
PUT /script_score_test/my_type/2
{
    "prop1" : 9.6,
    "prop2" : 15.4
}
```

The following query fails:

``` javascript
GET /script_score_test/my_type/_search
{
  "query": {
    "function_score": {
      "boost_mode": "replace",
      "script_score": {
        "script": "if (_score &lt; max_val) {0.0} else {_score}",
        "params": {
          "max_val": 10
        }
      }
    }
  }
}
```

Interestingly, the query works when using the `==` operator.  :

``` javascript
GET /script_score_test/my_type/_search
{
  "query": {
    "function_score": {
      "boost_mode": "replace",
      "script_score": {
        "script": "if (_score == max_val) {0.0} else {_score}",
        "params": {
          "max_val": 10
        }
      }
    }
  }
}
```

Also, using `compareTo` seems to work:

``` javascript
GET /script_score_test/my_type/_search
{
  "query": {
    "function_score": {
      "boost_mode": "replace",
      "script_score": {
        "script": "if (_score.compareTo(max_val) &lt; 0) {0.0} else {_score}",
        "params": {
          "max_val": 10
        }
      }
    }
  }
}
```

We're OK with updating our scripts to use `compareTo` to make everything work, but we'd prefer to use the simpler `&lt;` `&gt;` syntax.  I wanted to file this issue in case it's an actual bug or in case someone else comes across it.
</description><key id="51349827">8828</key><summary>script_score comparison operators failing on 1.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jsnod</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.4.3</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T20:45:04Z</created><updated>2014-12-31T00:39:32Z</updated><resolved>2014-12-31T00:39:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-08T21:43:57Z" id="66193136">@dakrone do you have any ideas? I made the script accessor a `Number`, and it seemed like that should work the same as the boxed types, but it doesn't appear to in this case? I also am skeptical that `==` is not just comparing reference equality of the boxed value with the script accessor?
</comment><comment author="dakrone" created="2014-12-09T13:46:14Z" id="66284229">@rjernst the ScriptAccessor needs to implement Comparable&lt;Number&gt; in order to work with `&lt;` and `&gt;`, this seems to fix it for me:

``` diff
diff --git a/src/main/java/org/elasticsearch/script/ScoreAccessor.java b/src/main/java/org/elasticsearch/script/ScoreAccessor.java
index 93536e5..e8c4333 100644
--- a/src/main/java/org/elasticsearch/script/ScoreAccessor.java
+++ b/src/main/java/org/elasticsearch/script/ScoreAccessor.java
@@ -30,7 +30,7 @@ import java.io.IOException;
  * The provided {@link DocLookup} is used to retrieve the score
  * for the current document.
  */
-public final class ScoreAccessor extends Number {
+public final class ScoreAccessor extends Number implements Comparable&lt;Number&gt; {

     Scorer scorer;

@@ -65,4 +65,9 @@ public final class ScoreAccessor extends Number {
     public double doubleValue() {
         return score();
     }
+
+    @Override
+    public int compareTo(Number o) {
+        return Float.compare(this.score(), o.floatValue());
+    }
 }
```

And then the `_score &lt; max_val` worked.
</comment><comment author="s1monw" created="2014-12-09T15:03:18Z" id="66295513">+1 for @dakrone fix
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/ScoreAccessor.java</file><file>src/test/java/org/elasticsearch/script/GroovyScriptTests.java</file></files><comments><comment>Scripting: Make _score in groovy scripts comparable</comment></comments></commit></commits></item><item><title>Corrupted shard uncovered during node decommissioning </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8827</link><project id="" key="" /><description>I removed node from allocation by ip and one shard appeared as corrupted at the end of relocation. Logs from the last restart:

```
2014-12-08 17:29:15,820][INFO ][node                     ] [statistics04] version[1.4.1], pid[96760], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-08 17:29:15,821][INFO ][node                     ] [statistics04] initializing ...
[2014-12-08 17:29:15,832][INFO ][plugins                  ] [statistics04] loaded [cloud-aws], sites []
[2014-12-08 17:29:18,419][INFO ][node                     ] [statistics04] initialized
[2014-12-08 17:29:18,419][INFO ][node                     ] [statistics04] starting ...
[2014-12-08 17:29:18,520][INFO ][transport                ] [statistics04] bound_address {inet[/192.168.1.212:9300]}, publish_address {inet[/192.168.1.212:9300]}
[2014-12-08 17:29:18,527][INFO ][discovery                ] [statistics04] statistics/VJCAl72ETbmulc5OJu5DQA
[2014-12-08 17:29:22,815][INFO ][cluster.service          ] [statistics04] detected_master [statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]], added {[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]],[statistics06][qnY9nSvXQsmvsTWqu_ayNg][web605][inet[/192.168.2.94:9300]],[statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]],}, reason: zen-disco-receive(from master [[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]]])
[2014-12-08 17:29:24,046][INFO ][http                     ] [statistics04] bound_address {inet[/192.168.1.212:9200]}, publish_address {inet[/192.168.1.212:9200]}
[2014-12-08 17:29:24,046][INFO ][node                     ] [statistics04] started
[2014-12-08 17:33:30,360][WARN ][transport                ] [statistics04] Received response for a request that has timed out, sent [246546ms] ago, timed out [216545ms] ago, action [internal:discovery/zen/fd/master_ping], node [[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]]], id [13]
[2014-12-08 18:44:45,855][INFO ][cluster.service          ] [statistics04] removed {[statistics06][qnY9nSvXQsmvsTWqu_ayNg][web605][inet[/192.168.2.94:9300]],}, reason: zen-disco-receive(from master [[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]]])
[2014-12-08 18:44:49,445][INFO ][cluster.service          ] [statistics04] added {[statistics06][YOK_20U7Qee-XSasg0J8VA][web605][inet[/192.168.2.94:9300]],}, reason: zen-disco-receive(from master [[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]]])
[2014-12-08 18:45:30,882][INFO ][discovery.zen            ] [statistics04] master_left [[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]]], reason [shut_down]
[2014-12-08 18:45:30,964][WARN ][discovery.zen            ] [statistics04] master left (reason = shut_down), current nodes: {[statistics04][VJCAl72ETbmulc5OJu5DQA][web467][inet[/192.168.1.212:9300]],[statistics06][YOK_20U7Qee-XSasg0J8VA][web605][inet[/192.168.2.94:9300]],[statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]],}
[2014-12-08 18:45:31,190][INFO ][discovery.zen            ] [statistics04] master_left [[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]]], reason [transport disconnected]
[2014-12-08 18:45:31,190][INFO ][cluster.service          ] [statistics04] removed {[statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]],}, reason: zen-disco-master_failed ([statistics07][EZ1aCHx5RNO1xNEUtNY5YQ][web606][inet[/192.168.2.95:9300]])
[2014-12-08 18:45:40,396][INFO ][cluster.service          ] [statistics04] detected_master [statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]], reason: zen-disco-receive(from master [[statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]]])
[2014-12-08 18:45:47,229][INFO ][cluster.service          ] [statistics04] added {[statistics07][q9ghAwFYTPCKyJ26BFaSqw][web606][inet[/192.168.2.95:9300]],}, reason: zen-disco-receive(from master [[statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]]])
[2014-12-08 18:46:02,161][INFO ][discovery.zen            ] [statistics04] master_left [[statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]]], reason [shut_down]
[2014-12-08 18:46:02,168][INFO ][discovery.zen            ] [statistics04] master_left [[statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]]], reason [transport disconnected]
[2014-12-08 18:46:03,815][WARN ][discovery.zen            ] [statistics04] master left (reason = shut_down), current nodes: {[statistics07][q9ghAwFYTPCKyJ26BFaSqw][web606][inet[/192.168.2.95:9300]],[statistics04][VJCAl72ETbmulc5OJu5DQA][web467][inet[/192.168.1.212:9300]],[statistics06][YOK_20U7Qee-XSasg0J8VA][web605][inet[/192.168.2.94:9300]],}
[2014-12-08 18:46:03,815][INFO ][cluster.service          ] [statistics04] removed {[statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]],}, reason: zen-disco-master_failed ([statistics08][QnMcrdd0SxWA4zTMEZtdrQ][web607][inet[/192.168.2.96:9300]])
[2014-12-08 18:46:08,348][INFO ][cluster.service          ] [statistics04] new_master [statistics04][VJCAl72ETbmulc5OJu5DQA][web467][inet[/192.168.1.212:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-08 18:46:33,513][INFO ][cluster.service          ] [statistics04] added {[statistics08][Ai0OIXsCTgO_YE1MhJLRiQ][web607][inet[/192.168.2.96:9300]],}, reason: zen-disco-receive(join from node[[statistics08][Ai0OIXsCTgO_YE1MhJLRiQ][web607][inet[/192.168.2.96:9300]]])
[2014-12-08 18:49:54,615][INFO ][indices.store            ] [statistics04] Failed to open / find files while reading metadata snapshot
[2014-12-08 18:56:51,471][INFO ][cluster.metadata         ] [statistics04] [statistics-not-so-fast-201312] update_mapping [events]
[2014-12-08 19:55:32,217][INFO ][index.engine.internal    ] [statistics04] [statistics-not-so-fast-201312][2] now throttling indexing: numMergesInFlight=6, maxNumMerges=5
[2014-12-08 19:55:32,230][INFO ][index.engine.internal    ] [statistics04] [statistics-not-so-fast-201312][2] stop throttling indexing: numMergesInFlight=4, maxNumMerges=5
[2014-12-08 19:55:33,219][INFO ][index.engine.internal    ] [statistics04] [statistics-not-so-fast-201312][2] now throttling indexing: numMergesInFlight=6, maxNumMerges=5
[2014-12-08 19:55:33,230][INFO ][index.engine.internal    ] [statistics04] [statistics-not-so-fast-201312][2] stop throttling indexing: numMergesInFlight=4, maxNumMerges=5
[2014-12-08 19:55:33,231][INFO ][index.engine.internal    ] [statistics04] [statistics-not-so-fast-201312][2] now throttling indexing: numMergesInFlight=6, maxNumMerges=5
[2014-12-08 19:55:33,243][INFO ][index.engine.internal    ] [statistics04] [statistics-not-so-fast-201312][2] stop throttling indexing: numMergesInFlight=4, maxNumMerges=5
[2014-12-08 22:59:31,798][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28.si], length [472], checksum [1kcwerm], writtenBy [null] checksum mismatch
[2014-12-08 22:59:31,858][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28.fdx], length [774151], checksum [sumxsg], writtenBy [null] checksum mismatch
[2014-12-08 22:59:31,885][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28.fnm], length [3554], checksum [dubiao], writtenBy [null] checksum mismatch
[2014-12-08 22:59:32,294][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28_es090_0.tip], length [3272251], checksum [hv0fef], writtenBy [null] checksum mismatch
[2014-12-08 23:00:55,791][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28_es090_0.blm], length [9517214], checksum [1ro50g1], writtenBy [null] checksum mismatch
[2014-12-08 23:01:08,564][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28_es090_0.doc], length [311172533], checksum [uy3nf4], writtenBy [null] checksum mismatch
[2014-12-08 23:01:13,610][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28_es090_0.tim], length [275377261], checksum [187kzhk], writtenBy [null] checksum mismatch
[2014-12-08 23:01:44,209][WARN ][indices.recovery         ] [statistics04] [statistics-20140918][4] Corrupted file detected name [_u28.fdt], length [726233746], checksum [i2a7yg], writtenBy [null] checksum mismatch
[2014-12-08 23:01:44,413][WARN ][index.engine.internal    ] [statistics04] [statistics-20140918][4] failed engine [corrupt file detected source: [recovery phase 1]]
org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [statistics-20140918][4] Failed to transfer [27] files with total size of [1.8gb]
    at org.elasticsearch.indices.recovery.RecoverySource$1.phase1(RecoverySource.java:276)
    at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1116)
    at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:654)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:137)
    at org.elasticsearch.indices.recovery.RecoverySource.access$2600(RecoverySource.java:74)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:464)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:450)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
    ... 4 more
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=sumxsg actual=z4yixy resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5c352fb4)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=dubiao actual=18t8jnd resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@4594ef9b)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=hv0fef actual=15rxc01 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@36405f3e)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1ro50g1 actual=1qivhre resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5085f5c6)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=uy3nf4 actual=uk9ays resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@50854005)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=187kzhk actual=gstvlk resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@52ce09d)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@74aaf669)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2014-12-08 23:01:44,634][WARN ][cluster.action.shard     ] [statistics04] [statistics-20140918][4] sending failed shard for [statistics-20140918][4], node[VJCAl72ETbmulc5OJu5DQA], relocating [Ai0OIXsCTgO_YE1MhJLRiQ], [P], s[RELOCATING], indexUUID [MgvyngJJQaCtGYfnqKOaZA], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[statistics-20140918][4] Failed to transfer [27] files with total size of [1.8gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)]; ]]
[2014-12-08 23:01:44,634][WARN ][cluster.action.shard     ] [statistics04] [statistics-20140918][4] received shard failed for [statistics-20140918][4], node[VJCAl72ETbmulc5OJu5DQA], relocating [Ai0OIXsCTgO_YE1MhJLRiQ], [P], s[RELOCATING], indexUUID [MgvyngJJQaCtGYfnqKOaZA], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[statistics-20140918][4] Failed to transfer [27] files with total size of [1.8gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)]; ]]
[2014-12-08 23:01:46,441][WARN ][indices.cluster          ] [statistics04] [statistics-20140918][4] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [statistics-20140918][4] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.lucene.index.CorruptIndexException: [statistics-20140918][4] Preexisting corrupted index [corrupted_9CAF-B8ySZSdvpwbnwVAww] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)]
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=sumxsg actual=z4yixy resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5c352fb4)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=dubiao actual=18t8jnd resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@4594ef9b)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=hv0fef actual=15rxc01 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@36405f3e)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1ro50g1 actual=1qivhre resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5085f5c6)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=uy3nf4 actual=uk9ays resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@50854005)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=187kzhk actual=gstvlk resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@52ce09d)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
```

.. and so on.

"Maybe that's just a glitch that could disappear with restart" – was my first thought.

```
[2014-12-08 23:12:36,360][INFO ][node                     ] [statistics04] stopping ...
[2014-12-08 23:12:36,423][INFO ][node                     ] [statistics04] stopped
[2014-12-08 23:12:36,423][INFO ][node                     ] [statistics04] closing ...
[2014-12-08 23:12:36,455][INFO ][node                     ] [statistics04] closed
[2014-12-08 23:12:45,207][INFO ][node                     ] [statistics04] version[1.4.1], pid[84384], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-08 23:12:45,207][INFO ][node                     ] [statistics04] initializing ...
[2014-12-08 23:12:45,338][INFO ][plugins                  ] [statistics04] loaded [cloud-aws], sites []
[2014-12-08 23:12:49,383][INFO ][node                     ] [statistics04] initialized
[2014-12-08 23:12:49,384][INFO ][node                     ] [statistics04] starting ...
[2014-12-08 23:12:49,479][INFO ][transport                ] [statistics04] bound_address {inet[/192.168.1.212:9300]}, publish_address {inet[/192.168.1.212:9300]}
[2014-12-08 23:12:49,501][INFO ][discovery                ] [statistics04] statistics/xU57iGQbRuuZUB3xyvB-LA
[2014-12-08 23:12:52,669][INFO ][cluster.service          ] [statistics04] detected_master [statistics08][Ai0OIXsCTgO_YE1MhJLRiQ][web607][inet[/192.168.2.96:9300]], added {[statistics07][q9ghAwFYTPCKyJ26BFaSqw][web606][inet[/192.168.2.95:9300]],[statistics08][Ai0OIXsCTgO_YE1MhJLRiQ][web607][inet[/192.168.2.96:9300]],[statistics06][YOK_20U7Qee-XSasg0J8VA][web605][inet[/192.168.2.94:9300]],}, reason: zen-disco-receive(from master [[statistics08][Ai0OIXsCTgO_YE1MhJLRiQ][web607][inet[/192.168.2.96:9300]]])
[2014-12-08 23:12:53,969][INFO ][http                     ] [statistics04] bound_address {inet[/192.168.1.212:9200]}, publish_address {inet[/192.168.1.212:9200]}
[2014-12-08 23:12:53,969][INFO ][node                     ] [statistics04] started
[2014-12-08 23:13:00,485][WARN ][indices.cluster          ] [statistics04] [statistics-20140918][4] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [statistics-20140918][4] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.lucene.index.CorruptIndexException: [statistics-20140918][4] Preexisting corrupted index [corrupted_9CAF-B8ySZSdvpwbnwVAww] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)]
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=sumxsg actual=z4yixy resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5c352fb4)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=dubiao actual=18t8jnd resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@4594ef9b)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=hv0fef actual=15rxc01 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@36405f3e)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1ro50g1 actual=1qivhre resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5085f5c6)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=uy3nf4 actual=uk9ays resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@50854005)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=187kzhk actual=gstvlk resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@52ce09d)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@74aaf669)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)

    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:452)
    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:433)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:120)
    ... 4 more
[2014-12-08 23:13:00,492][WARN ][cluster.action.shard     ] [statistics04] [statistics-20140918][4] sending failed shard for [statistics-20140918][4], node[xU57iGQbRuuZUB3xyvB-LA], [P], s[INITIALIZING], indexUUID [MgvyngJJQaCtGYfnqKOaZA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[statistics-20140918][4] failed to fetch index version after copying it over]; nested: CorruptIndexException[[statistics-20140918][4] Preexisting corrupted index [corrupted_9CAF-B8ySZSdvpwbnwVAww] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)]
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1kcwerm actual=n2dftw resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@b0037f7)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
    at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=sumxsg actual=z4yixy resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5c352fb4)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=dubiao actual=18t8jnd resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@4594ef9b)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=hv0fef actual=15rxc01 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@36405f3e)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
    Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=1ro50g1 actual=1qivhre resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@5085f5c6)
        at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
        at org.elasticsearch.index.store.Store.verify(Store.java:365)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:599)
        at org.elasticsearch.indices.recovery.RecoveryTarget$FileChunkTransportRequestHandler.messageReceived(RecoveryTarget.java:536)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
    Suppressed: org.elasticsearch.transport.RemoteTransportException: [statistics08][inet[/192.168.2.96:9300]][internal:index/shard/recovery/file_chunk]
```

... and so on.

"Gee, good job on making backups, myself!" — was my second thought.

```
# curl -X POST 'http://web605:9200/_snapshot/ceph_s3/statistics-2014-12-07/_restore?wait_for_completion=true&amp;pretty' -d '{ "indices": "statistics-20140918", "include_global_state": false }'
{
  "snapshot" : {
    "snapshot" : "statistics-2014-12-07",
    "indices" : [ "statistics-20140918" ],
    "shards" : {
      "total" : 5,
      "failed" : 1,
      "successful" : 4
    }
  }
}
```

older snapshot:

```
# curl -X POST 'http://web605:9200/_snapshot/ceph_s3/statistics-2014-11-27/_restore?wait_for_completion=true&amp;pretty' -d '{ "indices": "statistics-20140918", "include_global_state": false }'
{
  "snapshot" : {
    "snapshot" : "statistics-2014-11-27",
    "indices" : [ "statistics-20140918" ],
    "shards" : {
      "total" : 5,
      "failed" : 1,
      "successful" : 4
    }
  }
}
```

and in logs, as usual:

```
[2014-12-08 23:37:41,567][WARN ][cluster.action.shard     ] [statistics08] [statistics-20140918][4] sending failed shard for [statistics-20140918][4], node[Ai0OIXsCTgO_YE1MhJLRiQ], [P], restoring[ceph_s3:statistics-2014-11-27], s[INITIALIZING], indexUUID [MgvyngJJQaCtGYfnqKOaZA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[statistics-20140918][4] failed recovery]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] restore failed]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] failed to restore snapshot [statistics-2014-11-27]]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] Can't restore corrupted shard]; nested: CorruptIndexException[[statistics-20140918][4] Preexisting corrupted index [corrupted_iEZPcPv2QT21ve_TEv2S5A] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@30eef865)]
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@30eef865)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:843)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:784)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:162)
    at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
]; ]]
[2014-12-08 23:37:41,567][WARN ][cluster.action.shard     ] [statistics08] [statistics-20140918][4] received shard failed for [statistics-20140918][4], node[Ai0OIXsCTgO_YE1MhJLRiQ], [P], restoring[ceph_s3:statistics-2014-11-27], s[INITIALIZING], indexUUID [MgvyngJJQaCtGYfnqKOaZA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[statistics-20140918][4] failed recovery]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] restore failed]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] failed to restore snapshot [statistics-2014-11-27]]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] Can't restore corrupted shard]; nested: CorruptIndexException[[statistics-20140918][4] Preexisting corrupted index [corrupted_iEZPcPv2QT21ve_TEv2S5A] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@30eef865)]
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@30eef865)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:843)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:784)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:162)
    at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
]; ]]
[2014-12-08 23:37:42,078][WARN ][cluster.action.shard     ] [statistics08] [statistics-20140918][4] received shard failed for [statistics-20140918][4], node[YOK_20U7Qee-XSasg0J8VA], [P], restoring[ceph_s3:statistics-2014-11-27], s[INITIALIZING], indexUUID [MgvyngJJQaCtGYfnqKOaZA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[statistics-20140918][4] failed recovery]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] restore failed]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] failed to restore snapshot [statistics-2014-11-27]]; nested: IndexShardRestoreFailedException[[statistics-20140918][4] Can't restore corrupted shard]; nested: CorruptIndexException[[statistics-20140918][4] Preexisting corrupted index [corrupted_DiMHFSlxQCakLjudKt_TaQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1ee1a7fe)]
org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=i2a7yg actual=16u6g09 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1ee1a7fe)
    at org.elasticsearch.index.store.LegacyVerification$Adler32VerifyingIndexOutput.verify(LegacyVerification.java:73)
    at org.elasticsearch.index.store.Store.verify(Store.java:365)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:843)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:784)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:162)
    at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```

Well, maybe it wasn't that great decision to remove replicas for old indices.

I thought that checksums has to be checked during backups. If you have alive replica at the time of a backup, you can at least start recovering early. If you removed that healthy (?) replica after making a snapshot, you're doomed.

I'm using 1.4.1 with aws plugin for s3 snaphots on ceph (it has checksums too).

Is there a way to "fix" failed shard with data removal? If I cannot recover shard, I want my cluster to be green at least.

cc @imotov 
</description><key id="51343181">8827</key><summary>Corrupted shard uncovered during node decommissioning </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-12-08T19:43:17Z</created><updated>2015-11-28T16:29:22Z</updated><resolved>2015-11-21T22:18:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-12-08T19:48:23Z" id="66174747">Found some info about cluster status during shard movement:

```
epoch      timestamp cluster    status node.total node.data shards  pri relo init unassign
1418059931 21:32:11  statistics green           4         4   2347 2231    6    0        0
```

```
epoch      timestamp cluster    status node.total node.data shards  pri relo init unassign
1418065863 23:11:03  statistics red             4         4   2346 2230    0    0        1
```

Shard relocation started ~19:00:

```
epoch      timestamp cluster    status node.total node.data shards  pri relo init unassign
1418050789 18:59:49  statistics green           4         4   2347 2231    0    0        0
```

```
epoch      timestamp cluster    status node.total node.data shards  pri relo init unassign
1418050845 19:00:45  statistics green           4         4   2347 2231    6    0        0
```
</comment><comment author="bobrik" created="2014-12-08T19:52:31Z" id="66175515">Shard is currently trying to restore, loading at least one core 100%:

```
curl http://web607:9200/_nodes/_local/hot_threads
::: [statistics08][Ai0OIXsCTgO_YE1MhJLRiQ][web607][inet[/192.168.2.96:9300]]

   92.2% (461.1ms out of 500ms) cpu usage by thread 'elasticsearch[statistics08][clusterService#updateTask][T#1]'
     5/10 snapshots sharing following 15 elements
       org.elasticsearch.cluster.routing.IndexShardRoutingTable.shardsWithState(IndexShardRoutingTable.java:515)
       org.elasticsearch.cluster.routing.IndexRoutingTable.shardsWithState(IndexRoutingTable.java:268)
       org.elasticsearch.cluster.routing.RoutingTable.shardsWithState(RoutingTable.java:114)
       org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards(DiskThresholdDecider.java:225)
       org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canRemain(DiskThresholdDecider.java:434)
       org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canRemain(AllocationDeciders.java:105)
       org.elasticsearch.cluster.routing.allocation.AllocationService.moveShards(AllocationService.java:257)
       org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:223)
       org.elasticsearch.cluster.routing.allocation.AllocationService.applyFailedShards(AllocationService.java:113)
       org.elasticsearch.cluster.action.shard.ShardStateAction$3.execute(ShardStateAction.java:183)
       org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
       org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     2/10 snapshots sharing following 20 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1033)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)
       java.util.concurrent.CountDownLatch.await(CountDownLatch.java:282)
       org.elasticsearch.discovery.BlockingClusterStatePublishResponseHandler.awaitAllNodes(BlockingClusterStatePublishResponseHandler.java:58)
       org.elasticsearch.discovery.zen.publish.PublishClusterStateAction.publish(PublishClusterStateAction.java:153)
       org.elasticsearch.discovery.zen.publish.PublishClusterStateAction.publish(PublishClusterStateAction.java:86)
       org.elasticsearch.discovery.zen.ZenDiscovery.publish(ZenDiscovery.java:318)
       sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)
       sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
       java.lang.reflect.Method.invoke(Method.java:601)
       org.elasticsearch.common.inject.internal.ConstructionContext$DelegatingInvocationHandler.invoke(ConstructionContext.java:110)
       com.sun.proxy.$Proxy12.publish(Unknown Source)
       org.elasticsearch.discovery.DiscoveryService.publish(DiscoveryService.java:137)
       org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:423)
       org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
     3/10 snapshots sharing following 4 elements
       org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    9.3% (46.7ms out of 500ms) cpu usage by thread 'elasticsearch[statistics08][bulk][T#8]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:735)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1137)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)

    3.2% (16ms out of 500ms) cpu usage by thread 'elasticsearch[statistics08][bulk][T#9]'
     10/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:735)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:644)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1137)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:162)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:722)
```
</comment><comment author="bobrik" created="2014-12-08T20:20:41Z" id="66179975">This index actually was fully scrolled with spark a couple of days ago and there were no issues. I have logs from this node for a year (since 0.90.5), hope that could help with investigation.
</comment><comment author="clintongormley" created="2015-11-21T22:17:59Z" id="158686348">Sorry, this issue got lost.  Given that it is from a year ago, I assume you've resolved the issue already.  So much has changed since then, that there's no point in investigating further.
</comment><comment author="bobrik" created="2015-11-23T21:18:13Z" id="159067150">Yeah, fixed that by changing company and country, thanks!
</comment><comment author="clintongormley" created="2015-11-28T16:29:22Z" id="160316636">:D 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update to _source documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8826</link><project id="" key="" /><description>The current documentation for _source is misleading on the actual ramifications on disabling _source: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-source-field.html

The sentence "Though very handy to have around, the source field does incur storage overhead within the index." makes it seem that the only things _source is good for are fetch requests. Without diving deeper into other parts of the documentation you would not realize that disabling source actually disables the ability to reindex data.

I believe a large notification area should be added to this page explaining that disabling _source will also disable the ability to update mappings and reindex data.
</description><key id="51337609">8826</key><summary>Update to _source documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">chuntley</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2014-12-08T18:53:45Z</created><updated>2015-02-28T04:58:37Z</updated><resolved>2015-02-28T04:58:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T13:05:42Z" id="66279448">@chuntley completely agreed. would you be interested in sending a PR?
</comment><comment author="clintongormley" created="2015-02-28T04:58:37Z" id="76510796">It sounds like the _source field will no longer be configurable, which removes the need to document the issues with disabling it. Closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8142
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[GEO] Add TopoJSON support to geo_shape parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8825</link><project id="" key="" /><description>GeoJSON is currently the only supported format for geo_shape mapping.  This feature will add support for the TopoJSON specification to geo_shape mapping.  No change will be made to the mapping.  It will simply add the arc element to the existing ShapeParser.  An example of a topojson formatted GeometryCollection geo_shape is as follows:

``` java
{
    "location" : {
      "type" : "GeometryCollection",
      "geometries": [
        {
          "type": "Point",
          "properties": {
            "Name": "Point"
          },
          "coordinates": [-96.422, 32.963]
        },
        {
          "type": "LineString",
          "properties": {
            "Name": "LineString",
            "prop1": 0
          },
          "arcs": [0]
        },
        {
          "type": "Polygon",
          "properties": {
            "Name": "Polygon",
            "prop1": {
              "this": "that"
            }
          },
          "arcs": [[0, 1]]
        }
      ],
      "arcs": [
        [[-96.322, 32.994], [-96.322, 32.963], [-96.422, 32.963], [-96.422, 32.994] ],
        [[-96.522, 32.963], [-96.522, 32.994]]
      ]
    }
}
```

The example geovisualization can be found at https://gist.github.com/nknize/f504f855f536792e96a9
</description><key id="51336698">8825</key><summary>[GEO] Add TopoJSON support to geo_shape parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>adoptme</label><label>enhancement</label><label>feature</label></labels><created>2014-12-08T18:46:15Z</created><updated>2016-05-11T12:51:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="juanpujol" created="2016-05-11T12:51:31Z" id="218449619">No news on this issue yet? Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>'?' matching in index name wildcard patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8824</link><project id="" key="" /><description>Currently `*` is supported for wildcards in index names, eg: `http://localhost:9200/users-*/_mapping`. However there are situations were `*` may be overly broad. To solve this `?` could be made available so that we can match `users-2014` and `users-2013` with `users-????` without matching `users-disabled-2012`
</description><key id="51336153">8824</key><summary>'?' matching in index name wildcard patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rashidkpc</reporter><labels><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-12-08T18:41:52Z</created><updated>2015-11-21T22:14:37Z</updated><resolved>2015-11-21T22:14:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T22:14:37Z" id="158686194">I think this requirement has been better addressed by index date math https://github.com/elastic/elasticsearch/pull/12209 so I'm going to close
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Write index metadata on data nodes where shards allocated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8823</link><project id="" key="" /><description>If the master node thinks that an index does not exist, and another node thinks that it does, the conflict is currently resolved by having the node that has the index delete it. This can easily result in sudden unexpected data loss. The correct behavior would be for the conflict to be resolved by both nodes accepting the state of the node that thinks that the index exists.
</description><key id="51333964">8823</key><summary>Write index metadata on data nodes where shards allocated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">bobpoekert</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T18:23:32Z</created><updated>2016-06-27T13:24:30Z</updated><resolved>2015-05-05T10:25:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vjanelle" created="2014-12-08T18:33:38Z" id="66161970">Are you running dedicated masters with no query/data load?
</comment><comment author="bleskes" created="2014-12-08T18:34:57Z" id="66162212">@bobpoekert can you elaborate more about what happened before the data was lost - did you update the mapping (title suggest so)? If you can share your cluster layout (dedicated master nodes or not) it would be great. Also please the grab a copy of the logs of the nodes and save them. They might give more insight.
</comment><comment author="bobpoekert" created="2014-12-08T18:49:47Z" id="66164797">@vjanelle yes

@bleskes The sequence of events was the following:
1. Remove node (which is a master candidate) from cluster
2. Delete all index files from said node
3. Add node back into cluster
4. Node is elected master
5. All indexes in the cluster are now gone
</comment><comment author="vjanelle" created="2014-12-08T18:52:52Z" id="66165311">Did you have data turned off on the master candidate as well in the configuration?
</comment><comment author="bobpoekert" created="2014-12-08T18:53:11Z" id="66165372">@vjanelle No. The master candidate is also a data node.
</comment><comment author="bleskes" created="2014-12-08T19:39:04Z" id="66173088">@bobpoekert thx. let me make sure I understand what you are saying:

1) when you started you had a master node running, but it was also allowed to have data (i.e., it didn't have node.data set to false in its elasticsearch.yml file). Call this node A.
2) you brought down _another_ node which was both a master candidate and a data node (i.e., neither node.master nor node.data was set to false). Call this node B.
3).While B was down, you delete it's data folder (right? or was it some sub folders of it).
4) You brought B back up and it joined the cluster.

From this point on I'm not clear. Why was node B elected as master? What happened to the residing master node A?
</comment><comment author="bobpoekert" created="2014-12-08T23:09:49Z" id="66205856">@bleskes 
1. Cluster has a single master node (A) and two data nodes (B and C)
2. Remove A from the cluster
3. Now the cluster is offline (has no master)
4. Delete A's data folder 
5. Bring A back up
6. Now B and C have no data
</comment><comment author="bleskes" created="2014-12-08T23:17:26Z" id="66206807">@bobpoekert I'm confused. You said before you had nodes that are master candidates and are also data nodes. Can you confirm that node A has `node.data: false` in it's settings and and that node B &amp; C have `node.master: false` ?
</comment><comment author="bobpoekert" created="2014-12-08T23:18:16Z" id="66206904">@bleskes
All the nodes have `data: true`. 
A has `master: true`
B and C have `master: false`
</comment><comment author="bleskes" created="2014-12-08T23:41:37Z" id="66209745">OK. Clear. The cluster meta data (which we call cluster state) is stored and maintained on the master nodes. That meta data contains which indices are out there in the cluster. We only write the metadata on master eligible nodes, and rely on multiple masters for redundancy of this data set (compared to specific shard data). 

Since you only have one master node, that means there is no redundancy in your cluster meta data storage. After you deleted it there is nowhere to get it back from so the cluster becomes empty.

We do have a feature that is called dangling indices, which is in charge of scanning data folders for indices that are found on disk but are not part the cluster state and automatically import then into the cluster. As it is today, this feature needs to find some part of the index meta data to work, but those are also stored on the master eligible nodes, which in your case there were none. 

Thinking about it, we can be more resilient in situations where users are running only a single master node (though we highly recommend running more than one), and store the index metadata wherever a shard copy is stored, so also on data nodes. So we can improve the dangling indices case to identify those as well.

Lets keep this issue open and we will work on a PR to improve things based on the above.
</comment><comment author="lusis" created="2014-12-09T00:49:31Z" id="66216702">This definitely feels like a documentation issue as well. I asked on twitter and the reason for running single master was essentially to avoid other bugs. I don't think it's an unfair assumption that a user would expect having a quorum of preexisting data nodes to be enough to promote a new master or rebuild it without data loss. I would, from a purely semantic perspective, expect that data nodes would have ALL the data needed for the cluster and that the master's state would live with the rest of the "data"

Also would it not make sense for maybe non-master eligible nodes to at least provide a backup of the master node cluster metadata for this case and as a safety precaution? 

Ftr, I have no direct impact from this issue. Just another production ES user who tracks this stuff.
</comment><comment author="lusis" created="2014-12-09T00:50:30Z" id="66216798">Sorry missed the part where you mention possibly storing the backup on data nodes. 
</comment><comment author="grantr" created="2014-12-09T03:07:58Z" id="66228228">Repro of this bug: https://gist.github.com/grantr/a53a9b6b91005ad9807f

This is more than a documentation issue. Even when running in a degraded configuration, shards should never be deleted if their metadata can't be found.
</comment><comment author="bleskes" created="2014-12-09T07:59:09Z" id="66246679">&gt; Also would it not make sense for maybe non-master eligible nodes to at least provide a backup of the master node cluster metadata for this case and as a safety precaution?

This is indeed the plan
</comment><comment author="mkliu" created="2015-03-07T06:46:11Z" id="77676737">+1111
This bug really burnt me! We had multiple master node, but there was one time all master nodes are down, so I promote one data node as master node. And all data are gone! It freaked the hell out of me. And because people are still ingesting data, the data are overwritten. By the time I realize what's happening, it's already too late, we lost lots of data...
</comment><comment author="polgl" created="2015-03-09T14:16:03Z" id="77860748">Can the deletion be delayed for a longer period by setting a high value for `gateway.local.dangling_timeout` (couple of days maybe?)?
</comment><comment author="s1monw" created="2015-03-09T16:14:17Z" id="77885335">@polgl we remove the deleting and always import now since https://github.com/elasticsearch/elasticsearch/pull/10016 is pushed to 2.0
</comment><comment author="polgl" created="2015-03-09T17:26:22Z" id="77900710">I'm running 1.4 and can not update now, so i would prefer to change the settings and have more time to react.
</comment><comment author="s1monw" created="2015-03-12T18:51:25Z" id="78565754">@polgl you can modify the settings yourself and just to import always?
</comment><comment author="polgl" created="2015-03-12T20:30:49Z" id="78602990">Hi, I did some tests and it seems like our cluster does not show this behavior.
We have a single dedicated master node, if this node does not have an index in the meta data / cluster state. nothing happens. The data nodes don't delete any files (the dangling indices part in the code is not executed). That sounds ok, I can live with that.

Thanks for you help
</comment><comment author="saurabh24292" created="2016-06-27T08:15:13Z" id="228681057">ES Version 2.2

Previous state - A cluster with 6 nodes (3 Master cum data nodes, 3 client nodes).
Index has data for two months (1st April 2016 to 31st May 2016.) Actually, everyday, previous day's data is added and two months old data is deleted.
I restarted the cluster. All of a sudden, 90% of data for the date range 5th May 2016 to 31st May 2016 is gone (average record for these days goes down from 100000 per day to 10000 per day) and, surprisingly, deleted data fro date range 5th March 2016 to 31st March 2016 reappears.

What's the problem?
</comment><comment author="saurabh24292" created="2016-06-27T08:19:53Z" id="228681977">correction - version is 2.1.1
</comment><comment author="bleskes" created="2016-06-27T13:24:30Z" id="228744039">@saurabh24292 I'm not sure what your problem is, but maybe you can ask on discuss.elastic.co? if we figure out it's related to this issue or is cause by something other problem we can re-open this or (more likely) open a new one
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/ClusterChangedEvent.java</file><file>src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java</file><file>src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesTests.java</file></files><comments><comment>Write state also on data nodes if not master eligible</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/gateway/GatewayMetaState.java</file><file>src/test/java/org/elasticsearch/gateway/GatewayMetaStateTests.java</file><file>src/test/java/org/elasticsearch/gateway/MetaDataWriteDataNodesTests.java</file></files><comments><comment>Write state also on data nodes if not master eligible</comment></comments></commit></commits></item><item><title>Test: fix InternalEngineTest.testIndexWriterIFDInfoStream to pass in IntelliJ</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8822</link><project id="" key="" /><description>The logger used to log IndexWriter's deletions seems to change its name when it runs inside IntelliJ ... I just fixed the test to try both possibilities.
</description><key id="51328595">8822</key><summary>Test: fix InternalEngineTest.testIndexWriterIFDInfoStream to pass in IntelliJ</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T17:29:34Z</created><updated>2015-03-19T10:18:09Z</updated><resolved>2014-12-08T20:11:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-08T17:47:26Z" id="66154935">Looks good...
</comment><comment author="s1monw" created="2014-12-08T19:55:21Z" id="66175994">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[ENGINE] Move engine lifecycle store reference to EngineHolder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8821</link><project id="" key="" /><description>This commit moves the engines reference to the store out of the actual
implementation into the hodler since the holder manages the actual lifcycle.
Engine internal references like per searcher or per recovery are kept inside
the actual implemenation since the have a different lifecycle.

this caused failures like this: 

http://build-us-00.elasticsearch.org/job/es_core_master_metal/5917/
</description><key id="51302612">8821</key><summary>[ENGINE] Move engine lifecycle store reference to EngineHolder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-12-08T13:39:13Z</created><updated>2015-03-19T10:18:09Z</updated><resolved>2014-12-08T20:07:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-08T16:18:15Z" id="66139927">@bleskes I hardened this start method and made the store reference private to the engine. I also added an evil test that actually reproduces :)
</comment><comment author="bleskes" created="2014-12-08T16:38:06Z" id="66143355">@s1monw I like it. I left some comments on the commit. Sorry :(
</comment><comment author="bleskes" created="2014-12-08T16:47:29Z" id="66145020">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Pass class level test logging to external nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8820</link><project id="" key="" /><description>This commit passes the test logging annotation from the class
level to the external nodes as well.

Closes #8552
</description><key id="51293667">8820</key><summary>[TEST] Pass class level test logging to external nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T11:45:39Z</created><updated>2015-03-19T10:18:09Z</updated><resolved>2014-12-08T12:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-12-08T12:19:21Z" id="66108791">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/junit/listeners/LoggingListener.java</file><file>src/test/java/org/elasticsearch/test/test/LoggingListenerTests.java</file></files><comments><comment>[TEST] LoggingListener to restore the initial logger levels after any modification</comment></comments></commit></commits></item><item><title>Add index.data_path setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8819</link><project id="" key="" /><description>This allows specifying the path an index will be at.

`index.data_path` is specified in the settings when creating an index,
and can not be dynamically changed.

An example request would look like:

``` json
POST /myindex
{
  "settings": {
    "number_of_shards": 2,
    "data_path": "/tmp/myindex"
  }
}
```

And would put data in /tmp/myindex/0/index/0 and /tmp/myindex/0/index/1

Since this can be used to write data to arbitrary locations on disk, it
requires enabling the `node.enable_custom_paths` setting in
elasticsearch.yml on all nodes.

I found that the `NodeEnvironment` abstraction works well for index-specific
data paths, and passing the index settings in to the various methods gives
us more flexibility in the future with regard to adding any other environment-
specific settings.
</description><key id="51293336">8819</key><summary>Add index.data_path setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Store</label><label>feature</label></labels><created>2014-12-08T11:40:49Z</created><updated>2015-04-06T17:50:51Z</updated><resolved>2014-12-16T17:35:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-12-08T11:41:32Z" id="66105359">I've left out documenting this feature, as I suspect the settings/configuration may look different after feedback on this PR, before merging this I will write documentation.
</comment><comment author="rjernst" created="2014-12-08T16:08:40Z" id="66138296">Why is the templating needed? This seems like something a user should never mess with?  For example, restoring snapshotted index would then not work on a cluster that didn't have the same template setup..
</comment><comment author="dakrone" created="2014-12-08T16:18:31Z" id="66139963">&gt; Why is the templating needed? This seems like something a user should never mess with?

The templating is another feature because we originally discussed shard-level folder settings, it's not required for anything. I agree it does add complexity and I'd be totally fine with removing it. @clintongormley do you know if there's a reason we need to support custom templating for shard-specific folders, or can we stick with the default template all the time?
</comment><comment author="s1monw" created="2014-12-09T08:55:05Z" id="66251618">@dakrone I left a bunch of comments but everything minor. I also wonder if we should remove the templating for now and maybe make it a different PR just to push out the discussion? What do you think?
</comment><comment author="dakrone" created="2014-12-09T08:56:28Z" id="66251747">@s1monw sounds reasonable to me, I will add a commit to remove it and address the feedback
</comment><comment author="dakrone" created="2014-12-09T12:23:39Z" id="66274844">@s1monw I've removed the templating and added a custom data_path usage 30% of the time in `ElasticsearchIntegrationTests`, as well as addressing your other comments.
</comment><comment author="s1monw" created="2014-12-09T13:38:04Z" id="66283236">left one comment other than that LGTM
</comment><comment author="dakrone" created="2014-12-12T15:30:39Z" id="66787315">I've squashed, rebased (since master was heavily changed), and added a node-level setting `node.enable_custom_paths` (defaults to false) for allowing or disallowing custom data paths.

@s1monw can you take one last look? None of the original logic has been changed.
</comment><comment author="rjernst" created="2014-12-12T21:26:40Z" id="66837846">@dakrone Why do we need one setting controlling another? They are both in yml, so anyone who can change that at the node level can change both?
</comment><comment author="rjernst" created="2014-12-12T21:27:35Z" id="66837956">Oh I see...one is at the index level and one at yml...still, is it really necessary?
</comment><comment author="dakrone" created="2014-12-12T21:42:21Z" id="66839863">&gt; Oh I see...one is at the index level and one at yml...still, is it really necessary?

`index.data_path` is set per-index, not in `elasticsearch.yml` at all. `node.enable_custom_paths` in `elasticsearch.yml` (as you mentioned, just clarifying).

As to whether it's necessary, yes, because this is intended for mixed clusters where some indices use the `path.data` directory, and others use a (perhaps slower or faster) disk. 
</comment><comment author="dakrone" created="2014-12-17T08:40:27Z" id="67292049">This is having trouble on Windows, I have reverted it for now while I look into it, I'll open a new PR when fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>use https to download the gpg public key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8818</link><project id="" key="" /><description>I hope this doesn't need the CLA... 
</description><key id="51291752">8818</key><summary>use https to download the gpg public key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">sk1p</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-12-08T11:19:24Z</created><updated>2014-12-10T17:14:44Z</updated><resolved>2014-12-10T17:14:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T12:56:06Z" id="66278409">Hi @sk1p 

Thanks for the PR. Sorry, but it does need the CLA :(
</comment><comment author="sk1p" created="2014-12-09T14:32:49Z" id="66290618">@clintongormley CLA is signed, should be ready to merge
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: use https to download the gpg public key</comment></comments></commit></commits></item><item><title>Add unit testing framework for updating settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8817</link><project id="" key="" /><description>Today it's common for us to have a class with an inner `ApplySettings` class responsible for updating the settings.

It would be very nice if we could unit test these in a simple way to ensure we don't accidentally miss updating a setting.
</description><key id="51291576">8817</key><summary>Add unit testing framework for updating settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2014-12-08T11:17:15Z</created><updated>2015-11-21T22:12:37Z</updated><resolved>2015-11-21T22:12:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T22:12:37Z" id="158686091">Closing in favour of #6372
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Add unit tests for DiskThresholdDecider settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8816</link><project id="" key="" /><description /><key id="51288279">8816</key><summary>[TEST] Add unit tests for DiskThresholdDecider settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T10:47:09Z</created><updated>2015-03-19T10:18:09Z</updated><resolved>2014-12-08T11:21:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-12-08T10:47:52Z" id="66098245">@s1monw here are the unit tests requested in #8813
</comment><comment author="s1monw" created="2014-12-08T11:12:06Z" id="66102566">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Packaging: Add java7/8 java-package paths to init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8815</link><project id="" key="" /><description>If you use the java-package tool to create java packages, those
paths also should be added to the debian init script.

Also updated the docs, that it is ok to install java8.

Closes #7383
</description><key id="51282598">8815</key><summary>Packaging: Add java7/8 java-package paths to init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T10:25:16Z</created><updated>2015-03-19T10:19:46Z</updated><resolved>2014-12-11T15:16:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-08T15:50:28Z" id="66135123">This looks fine, but should java 8 be added at the very end of the list?
</comment><comment author="spinscale" created="2014-12-08T15:52:31Z" id="66135454">thats actually a good question. Do we have a precedence on java7 vs. java8?

/cc @clintongormley 
</comment><comment author="clintongormley" created="2014-12-11T14:02:44Z" id="66622479">I'd say that we prefer Java 8 over Java 7, but perhaps @rmuir can give more informed advice?
</comment><comment author="dakrone" created="2014-12-11T14:21:21Z" id="66624777">Since we switched to the invoke dynamic version of the Groovy jar, it looks like scripting is faster on JDK 8 (just commenting)
</comment><comment author="rmuir" created="2014-12-11T14:50:07Z" id="66628883">+1 for java8 first. it has less probability of being broken.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elastic Audit &amp; Index Repair</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8814</link><project id="" key="" /><description>Dear ElasticSerach, 

Can you tell me what setup to the /etc/elasticsearch/logging.yml that I want to see the IP address in ElasticSearch logs. I setting Trace in severity level the IP is appeared, but the logs flood my system. I see the shield coming soon, but I can't wait. 

The second question is the run short of space on the elasticseaerch disk, the index will be broken. How can I fix it?

Thank you for answare.

Best Regards
Gábor
</description><key id="51279623">8814</key><summary>Elastic Audit &amp; Index Repair</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gabesz80</reporter><labels /><created>2014-12-08T10:14:31Z</created><updated>2014-12-08T11:01:29Z</updated><resolved>2014-12-08T11:01:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-12-08T11:01:29Z" id="66101476">You'd better ask questions on the mailing list. We could probably help you there.

This space is for issues and feature requests.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Settings: Fix `cluster.routing.allocation.disk.include_relocations` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8813</link><project id="" key="" /><description>This setting was incorrectly backported and as a result, was dynamically
settable, but the new setting never actually took effect.

This is only going into the 1.4 branch, as it already exists in the 1.x and
master branches.
</description><key id="51273503">8813</key><summary>Settings: Fix `cluster.routing.allocation.disk.include_relocations` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>bug</label><label>v1.4.2</label></labels><created>2014-12-08T09:35:24Z</created><updated>2015-03-19T10:18:09Z</updated><resolved>2014-12-08T09:45:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-12-08T09:36:27Z" id="66043439">LGTM
</comment><comment author="s1monw" created="2014-12-08T09:38:16Z" id="66043609">any chance we can get a small unittest for this to all branches that makes sure it actually works?
</comment><comment author="dakrone" created="2014-12-08T09:38:57Z" id="66043680">@s1monw I will submit that as a separate PR, since this is only going to 1.4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose ShardId via LeafReader rather than Directory API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8812</link><project id="" key="" /><description>Today we try to fetch a shard Id for a given IndexReader / LeafReader
by walking it's tree until the lucene internal SegmentReader and then
casting the directory into a StoreDirecotory. This class is fully internal
to Elasticsearch and should not be exposed outside of the Store.

This commit makes StoreDirectory a private inner class and adds dedicated
ElasticsearchDirectoryReader / ElasticserachLeafReader exposing a ShardId
getter to obtain information about the shard the index / segment belogs to.

These classes can be used to expose other segment specific information in
the future more easily.
</description><key id="51271853">8812</key><summary>Expose ShardId via LeafReader rather than Directory API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-08T09:14:37Z</created><updated>2015-06-07T11:52:26Z</updated><resolved>2014-12-08T11:13:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-08T11:04:03Z" id="66101747">@mikemccand can you take another look?
</comment><comment author="mikemccand" created="2014-12-08T11:07:27Z" id="66102086">One small typo still, else LGTM.  Thanks for simplifying the deletes logging!
</comment><comment author="rmuir" created="2014-12-08T20:15:24Z" id="66179176">Added comment: we should fix the method overrides here.
</comment><comment author="mikemccand" created="2014-12-08T20:40:02Z" id="66183049">&gt; we should fix the method overrides here.

+1

How come no tests caught this?
</comment><comment author="rmuir" created="2014-12-08T20:42:02Z" id="66183391">I committed a fix. I suspect nothing tests these cache key methods. It may be that no tests failed... because they are all integration tests and only test core-listener methods, which I think is the mechanism that happens to be used today (which could easily change, and core cache key might be used elsewhere, etc)
</comment><comment author="rmuir" created="2014-12-08T21:07:08Z" id="66187259">i reviewed more, getCoreCacheKey() is used e.g. as part of every filter key, so NRT was really broken with filter caching for example. I still dont think integration tests for that are the right way: better would be to have 'nrt is working' tests at a lower level, turning off merging and calling reopen and asserting the core cache key did not change for this filterreader, and then moving our way up...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parse index names in "indices_boost"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8811</link><project id="" key="" /><description>Allow specifying alias or wildcard in "indices_boost".

Closes #4756
</description><key id="51268912">8811</key><summary>Parse index names in "indices_boost"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Search</label><label>enhancement</label><label>review</label></labels><created>2014-12-08T08:34:02Z</created><updated>2016-11-06T11:35:21Z</updated><resolved>2015-10-15T02:25:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-19T23:09:17Z" id="67710769">LGTM, can you just add a note to the documentation that this feature now supports aliases and wildcards?
</comment><comment author="masaruh" created="2014-12-22T07:36:53Z" id="67809574">Rebased to fix conflict and updated document.
</comment><comment author="masaruh" created="2014-12-22T07:55:17Z" id="67810638">Hmm, come to think of it, one index can match multiple boost entry. In that case, boost value isn't deterministic (it depends on the order of entries in ObjectFloatOpenHashMap, it looks)...

I don't really like to change "index_boost" to list to deal with priority/ordering. So, use highest boost value matched for an index, perhaps?
</comment><comment author="rjernst" created="2014-12-22T17:06:04Z" id="67862023">Usually the "most specific" matching regex would be used.  For example, if you have `foo*` and `foobar*` with an index call `foobarbaz` then it would favor `foobar*`. I don't think the boost value should be a factor in which setting matches.
</comment><comment author="clintongormley" created="2014-12-22T17:36:19Z" id="67865701">@rjernst an index could well be pointed to by two different aliases, so most-specific doesn't really apply here.  I think the last one seen takes preference. Probably the simplest way of dealing with this edge case.
</comment><comment author="masaruh" created="2014-12-24T06:19:15Z" id="68029889">Made it use the last seen boost entry if multiple entries match an index.
</comment><comment author="rjernst" created="2014-12-29T19:46:57Z" id="68295319">I don't understand what "last one seen" means. The request receives the name to weight in an object, so there is no guaranteed order. And the order indexes are iterated over internally is likewise nondeterministic, afaict.
</comment><comment author="masaruh" created="2015-01-05T07:38:20Z" id="68676495">Right. The JSON looks to be being parsed from top to bottom doesn't mean it's ordered.

So, to deal with this, I think there are a few options.
1. Document that order is not determined. While this is easy, I don't think it's good idea since we can't control boosts. We may see unexpected behavior.
2. Make indices_boost take list of index name and boost pair. We may need to do this if we want to have full control. But I somewhat hesitate to do this because it's breaking change.
3. Define some kind of priority. For example, concrete index -&gt; wildcard -&gt; alias, wildcard with longer prefix wins and no priority between aliases. I think it's good enough since we can always specify concrete index name if we need to.

So, I like 3 but if we implement this for 2.0, making breaking change might be OK.
Suggestions?
</comment><comment author="clintongormley" created="2015-01-05T12:08:22Z" id="68699943">&gt; I don't understand what "last one seen" means. The request receives the name to weight in an object, so there is no guaranteed order. And the order indexes are iterated over internally is likewise nondeterministic, afaict.

Correct.  And I think that's OK.  Does this API really need to be this complex?  

In my opinion, this is an edge case.  Most people will specify one or two indices, not a complex list of aliases and wildcards and concrete indices.  If you try to make it "correct" with complex rules, you'll just expose a bunch of conflicting rules, which (as @masaruh says) could only be resolved by breaking the API by having ordered boost rules.

Really, I don't think it is worth doing this.  If people want more complex rules, they can use (eg) date ranges etc.
</comment><comment author="javanna" created="2015-03-21T09:46:53Z" id="84292027">What's the status here? Did we decide not to get it in? Shall we close if that's the case?
</comment><comment author="tdoman" created="2015-08-18T20:13:58Z" id="132336654">+1, status?
</comment><comment author="masaruh" created="2015-10-15T02:25:56Z" id="148258611">This PR is stale and I think I'd better re-think how to handle cases like aliases contain the same index. I'll open a new PR.
</comment><comment author="clintongormley" created="2016-11-06T11:35:21Z" id="258675219">@masaruh just reread this thread and i think the correct answer is here:

&gt; Make indices_boost take list of index name and boost pair. We may need to do this if we want to have full control. But I somewhat hesitate to do this because it's breaking change.

Then the logic would be that we use the boost from the first time we see the index in the list, so eg:

```
[  
  { "foo" : 2 },  # alias foo points to bar &amp; baz
  { "bar": 1.5 }, # this boost is ignored because we've already seen bar
  { "*": 1.2 }      # bar and baz are ignored because already seen, but index xyz gets this boost
]
```

This could be implemented in a bwc way. In fact, the old syntax doesn't need to be removed. We could just add this new syntax as an expert way of controlling boosts.

What do you think?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>java.lang.OutOfMemoryError: Java heap space after upgrade from 0.90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8810</link><project id="" key="" /><description>We have 6 servers and 14 shards in cluster, the index size 26GB, we have 1 replica so total size is 52GB, and ES v1.4.0, java version "1.7.0_65"
We use servers with RAM of 14GB (m3.xlarge), and heap is set to 7GB

After update from 0.90 we started facing next issue:
random cluster servers around once a day/two hits the heap size limit (java.lang.OutOfMemoryError: Java heap space) in log, and cluster falls - becomes red or yellow

We tried to add more servers to cluster - even 8, but than it's a matter of time when we'll hit the problem, so looks like there is no matter how many servers are in cluster - it still hits the limit after some time.
Before we started facing the problem we were running smoothly with 3 servers
Also we tried to set indices.fielddata.cache.size:  40% but it didnt helped
Also, there are possible workarounds to flush heap:
1) reboot some server - than heap becomes under 70% and for some time cluster is ok
or
2) decrease number of replicas to 0, and than back to 1

Upgrade to 1.4.1 hasn't solved an issue. 

Finally &lt;b&gt;found the query causing the cluster crashes&lt;/b&gt;. After I commented code doing this - the claster is ok for few days. Before it was crashing once a day in average.

the query looks like:

```
    {
        "sort": [
            {
                "user_last_contacted.ct": {
                    "nested_filter": {
                        "term": {
                            "user_last_contacted.owner_id": "542b2b7fb0bc2244056fd90f"
                        }
                    },
                    "order": "desc",
                    "missing": "_last"
                }
            }
        ],
        "query": {
            "filtered": {
                "filter": {
                    "term": {
                        "company_id": "52c0e0b7e0534664db9dfb9a"
                    }
                },
                "query": {
                    "match_all": {}
                }
            }
        },
        "explain": false,
        "from": 0,
        "size": 100
    }
```

the mapping looks like:

```
        "contact": {
            "_all": {
                "type": "string",
                "enabled": true,
                "analyzer": "default_full",
                "index": "analyzed"
            },
            "_routing": {
                "path": "company_id",
                "required": true
            },
            "_source": {
                "enabled": false
            },
            "include_in_all": true,
            "dynamic": false,
            "properties": {
                "user_last_contacted": {
                    "include_in_all": false,
                    "dynamic": false,
                    "type": "nested",
                    "properties": {
                        "ct": {
                            "include_in_all": false,
                            "index": "not_analyzed",
                            "type": "date"
                        },
                        "owner_id": {
                            "type": "string"
                        }
                    }
                }...
```

user_last_contacted is an array field with nested objects. The size of the array can be 100+ items.
</description><key id="51240597">8810</key><summary>java.lang.OutOfMemoryError: Java heap space after upgrade from 0.90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">serj-p</reporter><labels><label>bug</label></labels><created>2014-12-08T01:34:32Z</created><updated>2015-02-19T20:49:37Z</updated><resolved>2015-02-19T20:49:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T12:46:17Z" id="66277330">@martijnvg could you take a look at this one please?
</comment><comment author="martijnvg" created="2014-12-16T17:21:18Z" id="67196229">@serj-p This looks related to #8394, wrong cache behaviour (not the filter cache, but for in the fixed bitset cache for nested object fields) is causing higher heap usage that is causing OOM. Can you try to upgrade to version 1.4.1 this should resolve the OOM.
</comment><comment author="serj-p" created="2014-12-16T17:32:41Z" id="67198073">As I mentioned, upgrade to 1.4.1 hasn't solved the issue. 
</comment><comment author="martijnvg" created="2014-12-16T20:51:13Z" id="67229594">Sorry, I read your description too quickly... I see why an OOM can occur with nested sorting, the fix for the fixed bitset cache that was added in 1.4.1 (#8440) missed to do the change for nested sorting.

Are you using using the `nested` query/filter or `nested` aggregator in another search request by any chance? If so can you confirm that this is working without eventually going OOM?
</comment><comment author="serj-p" created="2014-12-16T21:10:06Z" id="67232413">I'm not using `nested` aggregator, but I'm using `nested` query/filter and can confirm that cluster is running smoothly for two weeks after I disabled `nested` sorting.
</comment><comment author="martijnvg" created="2014-12-16T21:47:23Z" id="67238272">@serj-p Thanks for confirming this. I'll fix this issue with nested sorting.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefFieldComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/FloatValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/LongValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/search/MultiValueMode.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/elasticsearch/search/MultiValueModeTests.java</file></files><comments><comment>Don't use the fixed bitset filter cache for child nested level filters, but the regular filter cache instead.</comment></comments></commit></commits></item><item><title>docs: add pgp key to repositories page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8809</link><project id="" key="" /><description>add pgp key to repositories
</description><key id="51237499">8809</key><summary>docs: add pgp key to repositories page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevinkluge</reporter><labels /><created>2014-12-08T00:01:13Z</created><updated>2014-12-08T14:42:44Z</updated><resolved>2014-12-08T14:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-08T01:05:38Z" id="65964809">LGTM.
</comment><comment author="s1monw" created="2014-12-08T14:42:44Z" id="66124442">pushed to `master`, `1.x`, `1.3,`1.4`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Made the `nested`, `reverse_nested` and `children` aggs ignore unmapped nested fields or unmapped child / parent types.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8808</link><project id="" key="" /><description>PR for #8760
</description><key id="51232225">8808</key><summary>Made the `nested`, `reverse_nested` and `children` aggs ignore unmapped nested fields or unmapped child / parent types.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-07T21:28:00Z</created><updated>2015-05-18T23:29:08Z</updated><resolved>2015-01-06T11:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-10T23:12:17Z" id="66542286">LGTM. I'm wondering if this change should go to 1.4.2 or not (ie. do we consider it a bug fix or an enhancement)? Since the change in behaviour is quite significant, I would lean towards not backporting to 1.4.2?
</comment><comment author="martijnvg" created="2015-01-05T13:57:07Z" id="68710447">@jpountz Agreed, lets only make this change in 1.5 and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `http.publish_port` setting to the HTTP module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8807</link><project id="" key="" /><description>This change adds a 'http.publish_port' setting to the HTTP module to configure
the port which HTTP clients should use when communicating with the node. This
is useful when running on a bridged network interface or when running behind
a proxy or firewall.

Closes #8137
</description><key id="51222596">8807</key><summary>Add `http.publish_port` setting to the HTTP module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">petmit</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-07T16:24:50Z</created><updated>2015-06-07T17:23:02Z</updated><resolved>2014-12-11T15:10:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T12:43:37Z" id="66277073">@spinscale please could you take a look at this one.
</comment><comment author="spinscale" created="2014-12-09T13:42:24Z" id="66283765">looks good, but we should add a test as well.

@petmit are you willing to add one? I am happy to help.

Also, just to make sure I got it right? I guess you have a use-case where you are portforwarding the HTTP port from a different port than 9200?
</comment><comment author="petmit" created="2014-12-09T16:53:32Z" id="66315556">Sure, I can add a test. If you could stub out a test class or point me to an existing class where you would like the test to be placed, that would be great.

My particular use-case here is running nodes in Docker containers where host ports are mapped to container ports on a bridge interface. For clients using the REST API to be able to sniff the right node addresses for the nodes in the cluster you would either need to change the 'http.port' setting or use a setting like the one proposed here. Seeing as there exists a 'transport.publish_port' setting for the transport module, it seems appropriate to add this setting to the HTTP module as well as similar use-cases would apply to both modules.
</comment><comment author="spinscale" created="2014-12-10T08:19:36Z" id="66417910">You could create a new test class named `HttpPublishPortTests` or something and base it on `CorsRegexTests` - which has HTTP enabled and your new setting enabled in the `nodeSettings()` method. Then in the test you just call the nodes infos and see if your port is shown correctly.

Thanks for the use-case explanation, makes perfect sense to add this then.
</comment><comment author="spinscale" created="2014-12-11T11:44:36Z" id="66607938">**UPDATE**: damn, didnt get an email that you did the same.. I'll take a quick look

Hey,

I've created a quick test here

``` java
@ClusterScope(scope = Scope.SUITE, numDataNodes = 1)
public class HttpPublishPortTests extends ElasticsearchIntegrationTest {

    private int publishPort = 65000;

    @Override
    protected Settings nodeSettings(int nodeOrdinal) {
        return ImmutableSettings.settingsBuilder()
            .put(super.nodeSettings(nodeOrdinal))
            .put(InternalNode.HTTP_ENABLED, true)
            .put("http.publish_port", publishPort)
            .build();
    }

    @Test
    public void testThatHttpPublishPortIsSet() throws Exception {
        NodesInfoResponse response = client().admin().cluster().prepareNodesInfo().get();
        NodeInfo nodeInfo = response.getNodes()[0];

        BoundTransportAddress address = nodeInfo.getHttp().address();
        assertThat(address.publishAddress(), instanceOf(InetSocketTransportAddress.class));

        InetSocketTransportAddress publishAddress = (InetSocketTransportAddress) address.publishAddress();
        assertThat(publishAddress.address().getPort(), is(publishPort));
    }
}
```

If you add it, squash &amp; rebase, I'll be happy to get it in.
</comment><comment author="petmit" created="2014-12-11T16:46:12Z" id="66648685">Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>src/test/java/org/elasticsearch/http/netty/HttpPublishPortTests.java</file></files><comments><comment>HTTP: Add 'http.publish_port' setting to the HTTP module</comment></comments></commit></commits></item><item><title>Get field mapping api should honour pretty flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8806</link><project id="" key="" /><description>Use  builder.field method instead of XContentHelper#writeRawField

Closes #6552
</description><key id="51219636">8806</key><summary>Get field mapping api should honour pretty flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:REST</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-07T14:38:35Z</created><updated>2015-06-07T18:23:23Z</updated><resolved>2014-12-12T05:38:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-12-08T08:03:23Z" id="66035209">@johtani left one comment. I also agree @rjernst comment.
</comment><comment author="johtani" created="2014-12-08T15:30:28Z" id="66131804">@rjernst @bleskes Thanks for reviewing.
I change the assertion and using writeRawField if pretty off.

Does it make sense?
</comment><comment author="rjernst" created="2014-12-08T15:42:10Z" id="66133694">LGTM, just a couple suggestions.
</comment><comment author="bleskes" created="2014-12-08T16:16:30Z" id="66139621">Thx @johtani . Left one comment about the implementation.
</comment><comment author="johtani" created="2014-12-09T04:46:07Z" id="66234303">Fix all comments
</comment><comment author="bleskes" created="2014-12-09T08:46:25Z" id="66250729">LGTM (note that @rjernst had a comment)
</comment><comment author="bleskes" created="2014-12-10T09:14:47Z" id="66423058">LGTM :)
</comment><comment author="rjernst" created="2014-12-10T16:05:56Z" id="66474958">@johtani Thanks, LGTM.  I left one minor comment. No need for another review regardless of if/how that is addressed.
</comment><comment author="johtani" created="2014-12-12T05:38:05Z" id="66734439">Merged 80bd69811d45383f9305550855b0024bc74dca0e
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>After upgrade from 1.0.* to 1.4.1, checksum check fails after restart causing cluster to go red after yellow state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8805</link><project id="" key="" /><description>We did upgrade our cluster from 1.0.\* to 1.4.1.

After the upgrade, we indeed had 3 shards correctly identified as broken (checksum check failed), which we fixed. (we fixed the index and saw that it had errors). Before we had to restart, the cluster state was nearly completely green.

Then we had to restart our cluster again:
- The cluster state turned yellow (all primaries allocated)
- Then it turned red again, caused by the allocation of the non primary shards for some indexes.
- The checksum check failed on about 1/50th of our shards which we indexed data to:
  We ran checkindex on the shards on disk and they were not corrupt. An hardware error is also very unlikely since these servers only have 2-3 shards on them, so there must have been many hardware errors which is unlikely.
- We deleted the checksum and the marker file and ES loaded the shards again automatically.

Could it be related to the merging of old and new segments? (we didn't observer this on shards where we didn't index to)? At the moment we delete the checksum and the marker file? What should we do?

Master log:
[2014-12-07 00:03:59,192][WARN ][cluster.action.shard     ] [master] [index1][6] received shard failed for [index1][6], node[QPUH7WcyT3SSBuYjCvKHaQ], [P], s[STARTED], indexUUID [vxjN24PlRROou8Y-W6ObPw], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index1][6] Failed to transfer [86] files with total size of [80.3gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=xzqmes actual=a2zr8o resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@40ea4a1e)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#11]{New I/O worker #28}}
[2014-12-07 00:03:59,265][WARN ][cluster.action.shard     ] [master] [index2][8] received shard failed for [index2][8], node[n-kMHaf-QH2LTjncGjmkLw], [P], s[STARTED], indexUUID [VU0RN4QtRo2Ciae8b6oT7w], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index2][8] Failed to transfer [110] files with total size of [82.1gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=wtmawb actual=85psa3 resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1a87889d)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#3]{New I/O worker #20}}
[2014-12-07 00:03:59,660][WARN ][cluster.action.shard     ] [master] [index3][7] received shard failed for [index3][7], node[pHKQxOBYTuqReDWDStP6JQ], [P], s[STARTED], indexUUID [v_5dSwWwQQ-ylb000A9s5Q], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index3][7] Failed to transfer [113] files with total size of [82.7gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1ut1u4d actual=dsmbzp resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@2e3275f)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#10]{New I/O worker #27}}
[2014-12-07 00:03:59,822][WARN ][cluster.action.shard     ] [master] [index4][7] received shard failed for [index4][7], node[Petlv8BJTXeAldR66ar_RQ], [P], s[STARTED], indexUUID [EVdW2JJLSwmhCQcQ9zWiuQ], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index4][7] Failed to transfer [133] files with total size of [81.8gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=er2pdw actual=1a0wwft resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@77e5ccc1)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#3]{New I/O worker #20}}
[2014-12-07 00:03:59,839][WARN ][cluster.action.shard     ] [master] [index5][1] received shard failed for [index5][1], node[iCaUmle9SZOeK5z_VqAwwQ], [P], s[STARTED], indexUUID [_SdOrcFJSj6I8jI3Rxus0Q], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index5][1] Failed to transfer [136] files with total size of [81.8gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1s2u9d3 actual=cggxwd resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1d4e624)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#6]{New I/O worker #23}}
[2014-12-07 00:03:59,975][WARN ][cluster.action.shard     ] [master] [index6][0] received shard failed for [index6][0], node[t5ieNHyPScOzEew0Rd0EcA], [P], s[STARTED], indexUUID [2lk2p8AKQSSxFF_iLffPUA], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index6][0] Failed to transfer [120] files with total size of [82gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=mgcerl actual=14hf0lf resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@661abd91)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#15]{New I/O worker #32}}
[2014-12-07 00:04:00,251][WARN ][cluster.action.shard     ] [master] [index7][8] received shard failed for [index7][8], node[GwY2MBlwRHWbKYOgoAqBiA], [P], s[STARTED], indexUUID [bYPu5KhiTYqumxrzRh7OZg], reason [engine failure, message [corrupt file detected source: [recovery phase 1]][RecoverFilesRecoveryException[[index7][8] Failed to transfer [176] files with total size of [77.6gb]]; nested: CorruptIndexException[checksum failed (hardware problem?) : expected=1xnasey actual=pmztvp resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@1e3484f)]; ]] {elasticsearch[master][[transport_server_worker.default]][T#12]{New I/O worker #29}}
</description><key id="51212688">8805</key><summary>After upgrade from 1.0.* to 1.4.1, checksum check fails after restart causing cluster to go red after yellow state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bluelu</reporter><labels /><created>2014-12-07T09:29:25Z</created><updated>2015-11-21T22:11:50Z</updated><resolved>2015-11-21T22:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-07T10:16:22Z" id="65932613">we do check checksums for small files on startup and larger files are checksummed on merge. If you delete the checksum marker you are just bringing back your corrupted shard. if you have primaries that are corrupted (btw. they might have been corrupted for a long time already but old ES version didn't check this. What I am wondering about is why you now see this since 1.0.x didn't even write checksums on the lucene level so the on-merge theory is wrong. I also don't see the problem in the logs. ES `1.4.1` now checks old files checksums with the old `Adler32` checksums from ES which can explain your problems but apparently they are only happening on relocation / recovery right? Can you tell if it happened during  a recovery or a relocation? is your primary affected too? if so I don't think it's easy to recover without reindexing to be honest...
</comment><comment author="bluelu" created="2014-12-07T15:18:18Z" id="65940467">When the cluster was running before we didn't observe any checksum errors, except for 3 indexes which really had checksum errors (and lucene's checkindex script also detected them).

We then restarted the cluster, and it turned yellow on those indexes, so all primaries were successfully allocated and the cluster was in YELLOW state. Then ES started doing the recovery on the non primaries to bring up all replicas, which caused the primaries to be marked as corrupted I assume, as they were not loaded anymore, marking the index as RED.

Lucene's fixindex script doesn't find any corruption in the indexes, so I hardly doubt that they are corrupted. Also as it was really on machines which only have 2 shards on them, I hardly doubt that it's a hardware error as it occured for a lot of shards (about 30)

We had the same issue on our test cluster before (but there we suspected just an error on our side), so hopefully we can reproduce it there with a simpler test case.
</comment><comment author="miccon" created="2014-12-09T19:23:45Z" id="66341014">We are still trying the reproduce the issue isolated. I'm not sure if has something to do with it, but just to clarify we are using two data directories.

Currently it seems that the issue comes when the primary goes down, and the replica takes over. Not sure if it occurs immediately or after the replica gets promoted as a new primary and streams the data to a new replica. When closing the indices and running CheckIndex on the data files, no error is found, but the node complains about wrong checksums after reopening the index. Deleting the checksums as well as the corrupted file seems to resolve the issue in this case. It seems that there is an issue with the legacy checksums, maybe not being updated correctly or it may be related to the deleted documents.
</comment><comment author="tomcashman" created="2014-12-10T10:42:13Z" id="66433614">This has also happened to our team when upgrading from 1.1.2 to 1.4.1.
</comment><comment author="miccon" created="2014-12-10T14:10:18Z" id="66456055">The issue occured on our cluster for &gt;100 shards.

Here is an example of one index which is not corrupted (according to checkindex) but fails to recover because of the checksum. The affected files are _e.cfe and _e.cfs. What is strange is that both files are never than the checksum file.

https://gist.github.com/miccon/b8df3402bdf32bdf6366

We solved the issue on our side by deleting the checksums as well as the corrupted file and updating the indices. Since then the issue did not reappear, but it seems like a bug related to the legacy checksum and replication.
</comment><comment author="cywjackson" created="2015-01-07T11:34:52Z" id="69010615">@miccon could you please elaborate exactly your solution in `We solved the issue on our side by deleting the checksums as well as the corrupted file and updating the indices.` ?Ie what files to remove, and the steps (stop / start /disable allocation? what exactly did you update on the indices?) It looks to me I am having the same issue, please see https://github.com/elasticsearch/elasticsearch/issues/7430#issuecomment-69009726 . Much appreciated
</comment><comment author="miccon" created="2015-01-07T12:30:01Z" id="69015664">In the data directory you find the checksum-xxx file containing the checksums. As well as the corrupted-xxx file, which tells elasticsearch that it should not reopen the index as its broken. If you then delete (you don't need to start or stop the node or close the index in this case) first the checksums file and then the corrupted file, elastisearch should open the index.

Please note that if your index is indeed broken and need repair, it will fail again sooner or later. But as noted above, we also had the issue with indices that had no issue (successfully ran lucene checkindex on these) and here the problem has been solved, although upon replica allocation the whole index will be copied over (as the checksum file is missing).
</comment><comment author="cywjackson" created="2015-01-07T13:03:32Z" id="69018707">thx @miccon  real quick, why the order in `first the checksums file and then the corrupted file,` 
right now number of checksum files we have

```
usw2a-search5-prod. 360
usw2a-search4-prod. 380
usw2c-search6-prod. 401
usw2c-search7-prod. 406
usw2b-search1-prod. 385
usw2b-search2-prod. 472
```

and number of corrupt files we have:

```
usw2a-search5-prod. 33
usw2a-search4-prod. 0
usw2b-search1-prod. 9
usw2c-search6-prod. 12
usw2c-search7-prod. 27
usw2b-search2-prod. 5
```

would a `find ... -exec rm {}` on them work? (i could run 2 cmd, 1st the checksums then the corrupted if the order matters...)
</comment><comment author="miccon" created="2015-01-07T13:23:02Z" id="69020653">The corrupted file will be recreated if the checksum file is still present.
</comment><comment author="cywjackson" created="2015-01-07T13:40:42Z" id="69022556">thx again
</comment><comment author="johd01" created="2015-01-16T11:36:54Z" id="70242050">This happend for us on 1.4.0 to 1.4.2 upgrade

We deleted _checksum files and corruption files

We notice that we have really old lucene versions on our shards, from 3.6.2 to 4.10.2

I guess we need to upgrade all shards? We dont have the options to reindex
</comment><comment author="francoisforster" created="2015-01-28T22:35:20Z" id="71931449">Would optimizing the index to 1 segment drop the older versions?
</comment><comment author="gregoryb" created="2015-01-29T00:11:59Z" id="71944362">The issue occured to us also upgrading from the 1.2.1 to 1.4.2. We fixed it like others... Deleting _checksum files and corruption files but i'm afraid to restart our cluster...
</comment><comment author="clintongormley" created="2015-01-29T14:58:12Z" id="72038098">&gt; Would optimizing the index to 1 segment drop the older versions?

@francoisforster yes
</comment><comment author="clintongormley" created="2015-11-21T22:11:50Z" id="158686042">Closing as this has been resolved several versions ago
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nodes can't join anymore after they were killed (1.4.1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8804</link><project id="" key="" /><description>We are running 1.4.1 (large cluster). Please take note that computation of shard allocation takes about 40-50 seconds on our cluster, so we suspect that this issue could be indeed related to  (https://github.com/elasticsearch/elasticsearch/issues/6372)

We shutdown some processing river nodes with:
http://localhost:9200/_cluster/nodes/service:searchriver/_shutdown

The nodes disappeared from the cluster health information status page.

Still the master node keeps them somehow in the list, and can not dispatch any new cluster updates anymore as it still wants to dispatch updates to the missing nodes (10 nodes). (The issue was not resolved after 3 hours with the same messages reappearing, so we restarted the cluster)

Master log during that time:
[2014-12-06 21:45:20,849][DEBUG][cluster.service          ] [master] cluster state updated, version [1568], source [zen-disco-node_failed([I56NODE][pWbBegdLTOm45Si7s46wTQ][i56NODE][inet[/x.x.18.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR56, master=false}), reason transport disconnected] {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:45:21,029][DEBUG][cluster.service          ] [master] processing [zen-disco-node_failed([I56NODE][pWbBegdLTOm45Si7s46wTQ][i56NODE][inet[/x.x.18.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR56, master=false}), reason transport disconnected]: done applying updated cluster_state (version: 1568) {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:45:21,029][DEBUG][cluster.service          ] [master] processing [zen-disco-node_failed([I54NODE][8dL9CH0ITuKs7SlGjXcClQ][i54NODE][inet[/x.x.16.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR54, master=false}), reason transport disconnected]: execute {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:46:02,672][DEBUG][cluster.service          ] [master] cluster state updated, version [1569], source [zen-disco-node_failed([I54NODE][8dL9CH0ITuKs7SlGjXcClQ][i54NODE][inet[/x.x.16.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR54, master=false}), reason transport disconnected] {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:46:03,077][DEBUG][cluster.service          ] [master] processing [zen-disco-node_failed([I54NODE][8dL9CH0ITuKs7SlGjXcClQ][i54NODE][inet[/x.x.16.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR54, master=false}), reason transport disconnected]: done applying updated cluster_state (version: 1569) {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:46:03,078][DEBUG][cluster.service          ] [master] processing [zen-disco-node_failed([I61NODE][V_zq6_bWSy-QiODn7kOMZw][i61NODE][inet[/x.x.39.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR61, master=false}), reason transport disconnected]: execute {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:46:45,565][DEBUG][cluster.service          ] [master] cluster state updated, version [1570], source [zen-disco-node_failed([I61NODE][V_zq6_bWSy-QiODn7kOMZw][i61NODE][inet[/x.x.39.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR61, master=false}), reason transport disconnected] {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:46:45,902][DEBUG][cluster.service          ] [master] processing [zen-disco-node_failed([I61NODE][V_zq6_bWSy-QiODn7kOMZw][i61NODE][inet[/x.x.39.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR61, master=false}), reason transport disconnected]: done applying updated cluster_state (version: 1570) {elasticsearch[master][clusterService#updateTask][T#1]}
[2014-12-06 21:46:45,902][DEBUG][cluster.service          ] [master] processing [zen-disco-node_failed([I58NODE][7opie5gmS4uJ7frkv1bbCg][i58NODE][inet[/x.x.32.20:9301]]{trendiction_scluster=SEARCH1, data=false, service=searchriver, max_local_storage_nodes=1, trendiction_cluster=HR58, master=false}), reason transport disconnected]: execute {elasticsearch[master][clusterService#updateTask][T#1]}

I could be wrong here, but as far as I remember, during that time we also didn't see any other nodes having obtained any new cluster state. Also we couldn't execute any commands anymore (like closing an index). (timed out)

When we try to start  the river nodes during above faulty state, the nodes can't join anymore:
Node log:
[2014-12-06 23:25:01,660][DEBUG][discovery.zen            ] [I61node] filtered ping responses: (filter_client[true], filter_data[false])
        --&gt; ping_response{node [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], id[9762], master [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], hasJoinedOnce [true], cluster_name[talkwalker]} {elasticsearch[I61node][generic][T#1]}
[2014-12-06 23:26:41,680][INFO ][discovery.zen            ] [I61node] failed to send join request to master [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]] {elasticsearch[I61node][generic][T#1]}
[2014-12-06 23:26:41,680][DEBUG][cluster.service          ] [I61node] processing [finalize_join ([master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true})]: execute {elasticsearch[I61node][clusterService#updateTask][T#1]}
[2014-12-06 23:26:41,681][DEBUG][cluster.service          ] [I61node] processing [finalize_join ([master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true})]: no change in cluster_state {elasticsearch[I61node][clusterService#updateTask][T#1]}
[2014-12-06 23:26:46,697][DEBUG][discovery.zen            ] [I61node] filtered ping responses: (filter_client[true], filter_data[false])
        --&gt; ping_response{node [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], id[9792], master [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], hasJoinedOnce [true], cluster_name[talkwalker]} {elasticsearch[I61node][generic][T#1]}
[2014-12-06 23:28:26,716][INFO ][discovery.zen            ] [I61node] failed to send join request to master [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]] {elasticsearch[I61node][generic][T#1]}
[2014-12-06 23:28:26,717][DEBUG][cluster.service          ] [I61node] processing [finalize_join ([master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true})]: execute {elasticsearch[I61node][clusterService#updateTask][T#1]}
[2014-12-06 23:28:26,737][DEBUG][cluster.service          ] [I61node] processing [finalize_join ([master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true})]: no change in cluster_state {elasticsearch[I61node][clusterService#updateTask][T#1]}
[2014-12-06 23:28:31,745][DEBUG][discovery.zen            ] [I61node] filtered ping responses: (filter_client[true], filter_data[false])
        --&gt; ping_response{node [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], id[9822], master [[master][TH779p0eShWRaeyyU2Qqmg][master][inet[/x.x.12.16:9300]]{trendiction_scluster=NO_ROLE, data=false, service=cluster, max_local_storage_nodes=1, trendiction_cluster=HR51, river=_none_, master=true}], hasJoinedOnce [true], cluster_name[talkwalker]} {elasticsearch[I61node][generic][T#1]}

From the code,
https://github.com/elasticsearch/elasticsearch/blob/1.4/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L523-523

Since these nodes are non data nodes (flag is set in configuration file), is the complete reroute of shards necessary? I guess in our case it seems that those reroute calls were just piling up?
</description><key id="51212305">8804</key><summary>Nodes can't join anymore after they were killed (1.4.1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bluelu</reporter><labels /><created>2014-12-07T09:09:16Z</created><updated>2014-12-15T13:04:39Z</updated><resolved>2014-12-15T13:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-12-08T08:27:41Z" id="66036996">@bluelu the first logs represents the master kicking off the river nodes. This has to be done on the cluster state update thread and my i guess is that it took a long time for the master to get there due to it being stuck in reroute (ref: https://github.com/elasticsearch/elasticsearch/issues/6372#issuecomment-65893470 )

The second is the join timing out because of the same issue (master can't get to it on time and the request times out after the default of 60s). I suggest you disable the disk threshold allocator (as suggested in 6372) and see if that helps. O.w. we can increase the timeout using `discovery.zen.join_timeout` (which requires a node restart)
</comment><comment author="bluelu" created="2014-12-11T21:10:45Z" id="66689376">@bleskes, I can confirm that identical repeating tasks are removed (https://github.com/elasticsearch/elasticsearch/issues/8860)  but not the failed entries for nodes that had been killed.

We can kill one or two nodes in our cluster without any issue. If we kill more than 10 nodes (non data nodes at the moment), then the cluster will never recover and it will spawn more and more of disco_node_failed entries in the pending tasks. The pending tasks for that type will grow and grow.

Unfortunately I overwrite the log file before so I don't have the output of the pending tasks anymore when this occured.
</comment><comment author="bluelu" created="2014-12-12T09:35:01Z" id="66750479">Here is one example (the clsuter state has 212 of those entries for about 10 failed nodes).
It will grow indefinitely over time. Each of these failed messages will trigger an allocate, which triggers more messages to appear? (or they are repeated because the allocate takes longer)

```
      52624        4.6m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason transport disconnected
      52656        4.1m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52788        2.5m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52640        4.2m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52818          2m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52693        3.7m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52876       42.6s IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52715        3.4m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52730        3.3m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52749        3.1m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52674        3.9m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52827        1.8m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52833        1.6m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52842        1.5m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52697        3.6m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52901        2.5s IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52740        3.2m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52759        2.9m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52775        2.7m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52793        2.3m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52802        2.1m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52850        1.4m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52857        1.2m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52862          1m IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52870         53s IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52880       32.4s IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52887       22.5s IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout
      52893       12.6s IMMEDIATE zen-disco-node_failed([node1][lcZ6igcTSdKR2WoylVX-mA][node1][inet[/ip:9300]]{trendiction_scluster=SEARCH2, service=s1, max_local_storage_nodes=1, trendiction_cluster=HR76, river=_none_, master=false}), reason failed to ping, tried [3] times, each with maximum [1m] timeout

```
</comment><comment author="bleskes" created="2014-12-12T09:36:12Z" id="66750601">thx @bluelu . I'll chase it down.
</comment><comment author="bluelu" created="2014-12-12T15:24:02Z" id="66786267">Also an API to delete single pending tasks based on their id from the master task list would be great. In that case, as a workaround, we could just delete the offending ones.
</comment><comment author="miccon" created="2014-12-12T17:30:57Z" id="66805475">While handling the node failure (zen-disco-node_failed / zen-disco-node_left) in ZenDiscovery.java, wouldn't it be possible to skip the rerouting if the node is not in the cluster state anymore.

So that the first update removes the node, handles the rerouting and the following updates can just take the shortcut as the node is not part of the updated clusterstate anyway.
</comment><comment author="bleskes" created="2014-12-12T19:57:17Z" id="66826104">@bluelu @miccon I can confirm that concurrent shutdown of nodes will cause and O(n^2) number of failure events + reroutes. I just made a PR to reduce the overhead. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryTests.java</file></files><comments><comment>Discovery: concurrent node failures can cause unneeded cluster state publishing</comment></comments></commit></commits></item><item><title>Speed-up disk-threshold decider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8803</link><project id="" key="" /><description>Instead of iterating all shards of all indices to get all relocating
shards for a given node we can just use the RoutingNode#shardsWithState
method and fetch all `INITIALIZING` / `RELOCATING` shards and check if they
are relocating. This operation is much faster and uses pre-build
data-structures.

Relates to #6372
</description><key id="51196335">8803</key><summary>Speed-up disk-threshold decider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-06T21:15:39Z</created><updated>2015-06-06T19:04:22Z</updated><resolved>2014-12-06T22:28:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-06T21:16:41Z" id="65914277">I marked this `1.4.2` since it seems to slow down allocation reasonably see #6372
</comment><comment author="s1monw" created="2014-12-06T21:17:36Z" id="65914309">@bleskes @dakrone a review is needed here please ....
</comment><comment author="bleskes" created="2014-12-06T21:32:07Z" id="65914842">LGTM. +1 on 1.4.2
</comment><comment author="dakrone" created="2014-12-06T22:16:54Z" id="65916349">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parse and validate mappings on index template creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8802</link><project id="" key="" /><description>Parse and validate mappings in template when it puts.
- parse and validate mappings in template using dummy index service
- return error if template include invalid mapping 
- Not support a partial template

Close #2415
</description><key id="51177669">8802</key><summary>Parse and validate mappings on index template creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">johtani</reporter><labels><label>:Index Templates</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2014-12-06T09:49:57Z</created><updated>2016-06-09T10:55:15Z</updated><resolved>2016-06-01T08:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-08T19:34:18Z" id="66172335">@johtani I left some comments.
</comment><comment author="imotov" created="2015-04-25T03:57:00Z" id="96126151">Any news on this one? I just ran into the situation that [testBrokenMapping](https://github.com/elastic/elasticsearch/blob/master/src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java#L288) test generates. Basically after a broken template like in the test is added to the cluster the global cluster state becomes unserializable, which means `curl localhost:9200/_cluster/state` fails and any future changes into global cluster state (such as templates, repositories, persistent settings, etc) are no longer persisted. 
</comment><comment author="clintongormley" created="2015-07-17T09:13:16Z" id="122225790">@rjernst does this issue become any easier with the changes you've made to mappings?
</comment><comment author="rjernst" created="2015-07-18T19:34:43Z" id="122587363">Unfortunately not. Until mapping parsing is completely isolated from binding to a particular index (eg keeping an analyzer name, instead of looking up an actual analyzer and storing that), we will not be able to do this cleanly. Note that eventually this should be possible, as we move towards immutable mappings.

However, IIRC, this PR was done by creating a temporary index, which we already do in at least one other place. As long as we are careful with deleting the index (my last comments on this PR), I think this can work as an intermediate solution.
</comment><comment author="johtani" created="2015-09-30T11:00:25Z" id="144361236">@rjernst I talked about creating a temporary index with @bleskes .
And I move to the validation logic to ClusterStateUpdateTask. 
Can you guys review this?
</comment><comment author="s1monw" created="2016-01-18T13:43:24Z" id="172529999">@johtani what's the status of this?
</comment><comment author="johtani" created="2016-01-18T15:56:58Z" id="172569017">@s1monw I'm waiting for a review. 
</comment><comment author="rjernst" created="2016-01-18T19:38:35Z" id="172631345">I left a couple comments. There are two other general issues I have:
1. I don't like that applying the template for the temp index is done with separate code than what actually applies a template for real index creation. I think it should be shared so as the logic cannot get out of sync?
2. I only see integration tests here. It would be good to have real unit tests.
</comment><comment author="clintongormley" created="2016-03-08T18:59:38Z" id="193915625">@johtani would be good to get this in for 5.0 as well
</comment><comment author="johtani" created="2016-03-11T08:34:11Z" id="195256959">@clint Agreed. Now, I'm fixing the PR for Ryan said general issues. I will push next week.
</comment><comment author="johtani" created="2016-03-16T02:30:44Z" id="197117978">@rjernst  Pushed fixing the separate code and adding unit test.
Could you review again?
</comment><comment author="dakrone" created="2016-04-06T21:14:58Z" id="206572522">pinging @rjernst to review this
</comment><comment author="rjernst" created="2016-04-11T18:45:48Z" id="208494354">I'm still ok with doing this through creating a dummy index for now, until we can validate the settings in isolation of index creation, but I would like to do this without adding new public methods. Perhaps MapperService.merge should be replaced with something similar to the addMappings method added here? Really this should all be internal to the mapper service.

I've also asked @jpountz to take a look as I have not focused on mappings for a while.
</comment><comment author="seang-es" created="2016-04-27T15:05:52Z" id="215113261">Would this also catch restores from older snapshots that may have invalid templates included?
</comment><comment author="jpountz" created="2016-05-09T16:07:31Z" id="217909527">@johtani I left some comments but I agree we should do this.
</comment><comment author="johtani" created="2016-05-13T11:09:13Z" id="219014952">@rjernst  @jpountz Thanks for reviewing. 
Fixed your comment and pushed.
- Move the addMapping to MapperService.merge
- Use ramdomBased64UUID as temporary index name instead of prefix string

Does it make sense?
</comment><comment author="jpountz" created="2016-05-25T22:15:02Z" id="221724195">I left some comments but other than that it looks good to me now!
</comment><comment author="johtani" created="2016-05-31T07:12:08Z" id="222609424">@jpountz I rebased and fixed your comment. Could you review again?
</comment><comment author="jpountz" created="2016-05-31T12:39:56Z" id="222676165">LGTM
</comment><comment author="johtani" created="2016-06-09T10:55:15Z" id="224862274">@seang-es Sorry for late reply, I missed your comment.
Unfortunately, this PR validate only creating template. I think the restore logic don't call this logic.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for filtering by publish IP address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8801</link><project id="" key="" /><description>Allocation filtering by IP only works today using the node host address. But in some cases, you might want to filter using the publish address which could be different.

For example, let say your nodes are like this:

``` js
// First node
{
 "host" : "es1.acme.com",
 "transport_address" : "inet[/192.168.1.1:9300]",
 "ip" : "127.0.0.1",
 "http_address" : "inet[/192.168.1.1:9200]",
}
```

``` js
// Second node
{
 "host" : "es2.acme.com",
 "transport_address" : "inet[/192.168.1.2:9300]",
 "ip" : "127.0.0.1",
 "http_address" : "inet[/192.168.1.2:9200]",
}
```

You can not filter using ip because it will be the same for all nodes:

```
PUT test/_settings
{
  "index.routing.allocation.include._ip": "127.0.0.1"
}
```

This change adds support for either:
- `_name`: Match nodes by node name
- `_host_ip`: Match nodes by host IP address (IP associated with hostname)
- `_publish_ip`: Match nodes by publish IP address
- `_ip`: Match either `_host_ip` or `_publish_ip`
- `_host`: Match nodes by hostname

So you can filter allocation using now:

```
PUT test/_settings
{
  "index.routing.allocation.include._publish_ip": "192.168.1.1"
}
```
</description><key id="51175012">8801</key><summary>Add support for filtering by publish IP address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2014-12-06T07:40:21Z</created><updated>2015-09-14T12:54:26Z</updated><resolved>2015-09-09T13:18:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-12-19T21:35:31Z" id="67700956">@dadoonet left one comment. For tests, did you look at  DiscoveryNodeFiltersTests? In terms of functionality, I would others to comment as well, as I'm not very familiar with that area...
</comment><comment author="s1monw" created="2015-03-24T09:43:26Z" id="85424555">@dadoonet are you picking this up?
</comment><comment author="dadoonet" created="2015-07-06T10:50:44Z" id="118808312">@bleskes Sorry for the loooooong delay it took. I finally applied your comment and added some tests. Let me know what you think about it now.
</comment><comment author="bleskes" created="2015-07-07T07:20:54Z" id="119100806">@dadoonet I have some concerns about using the transport address as filtering. Maybe we want the publish address instead? the local bind ip may be the same in some VMs - not my expertise and I'm not sure. Maybe @drewr  can comment...
</comment><comment author="dadoonet" created="2015-07-07T21:17:54Z" id="119344370">May be we could do both ? Try first publish address and then fallback to transport?
</comment><comment author="drewr" created="2015-07-08T16:39:41Z" id="119653244">I need to think about this a bit more, but I think it should always be the `bind_host`. The `publish_host` can introduce more complexity from network topology external to the machine. However, in theory, if the `publish_host` is misconfigured, the node _shouldn't_ have been able to join the cluster, but stranger things have happened.

It could be overly cautious, but I would probably make it explicit: `cluster.routing.allocation.include._publish_ip: 99.99.99.99` and support `_bind_ip` as a synonym for `_ip`.
</comment><comment author="dadoonet" created="2015-07-11T17:22:27Z" id="120646377">@bleskes @drewr I pushed a new commit which basically changes `DiscoveryNode#address` field from a `TransportAddress` to a `BoundTransportAddress`.

From this object I can access either the `publish_address` or the `bound_address`.

Let me know what you think.

All tests including slow and REST tests are passing:

```
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] securemock ........................................ SUCCESS [4.222s]
[INFO] Elasticsearch Build Resources ..................... SUCCESS [0.437s]
[INFO] Elasticsearch Rest API Spec ....................... SUCCESS [1.392s]
[INFO] Elasticsearch Parent POM .......................... SUCCESS [15.974s]
[INFO] Elasticsearch Core ................................ SUCCESS [21:49.097s]
[INFO] Elasticsearch Plugin POM .......................... SUCCESS [2.817s]
[INFO] Elasticsearch Japanese (kuromoji) Analysis plugin . SUCCESS [14.923s]
[INFO] Elasticsearch Smart Chinese Analysis plugin ....... SUCCESS [11.251s]
[INFO] Elasticsearch Stempel (Polish) Analysis plugin .... SUCCESS [14.028s]
[INFO] Elasticsearch Phonetic Analysis plugin ............ SUCCESS [13.223s]
[INFO] Elasticsearch ICU Analysis plugin ................. SUCCESS [14.835s]
[INFO] Elasticsearch Google Compute Engine cloud plugin .. SUCCESS [6.027s]
[INFO] Elasticsearch Azure cloud plugin .................. SUCCESS [37.629s]
[INFO] Elasticsearch AWS cloud plugin .................... SUCCESS [14.535s]
[INFO] Elasticsearch Python language plugin .............. SUCCESS [33.536s]
[INFO] Elasticsearch JavaScript language plugin .......... SUCCESS [21.909s]
[INFO] Elasticsearch Delete By Query plugin .............. SUCCESS [30.165s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 25:46.757s
[INFO] Finished at: Sat Jul 11 19:40:50 CEST 2015
[INFO] Final Memory: 58M/639M
[INFO] ------------------------------------------------------------------------
```
</comment><comment author="bleskes" created="2015-07-12T06:20:45Z" id="120691085">@dadoonet thx for the hard work. I'm a bit uncomfortable with changing the address field in the disco node just for this. Given that the current disco node is using the published address, and that's the one we expose in all our reporting API I think it's fine using it that. Here is my suggestion:
- make `_ip` match either the host ip (like it does now) OR the disco node address (published ip). I don't think it's realistic that people will have collisions here.
- add a dedicated selector `_host_ip` which does what we do now.
- add a dedicated selector `_publish_ip` which only matches the address in the disco node.

Makes sense?
</comment><comment author="dadoonet" created="2015-07-18T08:47:46Z" id="122514580">@bleskes I updated the PR. Let me know.
</comment><comment author="bleskes" created="2015-07-23T12:15:59Z" id="124078506">Thx @dadoonet . Left a small suggestion.
</comment><comment author="dadoonet" created="2015-07-24T16:23:10Z" id="124572192">@bleskes updated and rebased based on your comment. Let me know
</comment><comment author="bleskes" created="2015-08-13T15:17:17Z" id="130726692">Thx @dadoonet . Left some more comments.
</comment><comment author="dadoonet" created="2015-08-28T16:50:35Z" id="135830390">@bleskes I squashed, rebased and added a new commit to address your latest comments. As I wrote in the commit message 454bd6d, may be I should move `shuffleSettings()` method in `ESTestCase`?
</comment><comment author="dadoonet" created="2015-09-08T07:06:51Z" id="138457126">ping @bleskes ? :)
</comment><comment author="bleskes" created="2015-09-08T12:16:12Z" id="138537688">Left some minor comments. LGTM. Thx @dadoonet . Is the plan to put this in 2.1?
</comment><comment author="dadoonet" created="2015-09-08T19:40:17Z" id="138679196">@bleskes I pushed a new commit. Thanks for your comments. Really helpful!
I'm totally +1 to push this in 2.x branch (2.1 version that is)
</comment><comment author="bleskes" created="2015-09-09T11:30:58Z" id="138882319">LGTM. Can you mark the PR with the right versions?
</comment><comment author="dadoonet" created="2015-09-09T12:46:43Z" id="138898116">@bleskes I had to add a new commit because of forbidden API. Would you mind giving a final look? 
Thanks!
</comment><comment author="bleskes" created="2015-09-09T12:49:08Z" id="138898555">Still LGTM
</comment><comment author="dadoonet" created="2015-09-09T13:19:45Z" id="138906762">Pushed also in branch 2.x with 38a36ba091c58af163ee870af146ae04a427f5e7

Thanks @bleskes!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node topology does not appear to be correct on a 4 socket system</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8800</link><project id="" key="" /><description>We recently posted node stats, and the topology here, https://gist.github.com/portante/711aa2428461a7485384#file-es-issue-8394-cluster-stats-json-L87, does not appear correct.

This is a 4 socket system, with 10 cores per socket, 40 total, hyper-threads are not enabled.

Line 87 should probably read 4, and line 88 should be 10.

Here is the `lscpu` output:

```
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                40
On-line CPU(s) list:   0-39
Thread(s) per core:    1
Core(s) per socket:    10
Socket(s):             4
NUMA node(s):          4
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 47
Model name:            Intel(R) Xeon(R) CPU E7- 4870  @ 2.40GHz
Stepping:              2
CPU MHz:               2393.994
BogoMIPS:              4787.83
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              30720K
NUMA node0 CPU(s):     0,4,8,12,16,20,24,28,32,36
NUMA node1 CPU(s):     2,6,10,14,18,22,26,30,34,38
NUMA node2 CPU(s):     1,5,9,13,17,21,25,29,33,37
NUMA node3 CPU(s):     3,7,11,15,19,23,27,31,35,39
```
</description><key id="51169648">8800</key><summary>Node topology does not appear to be correct on a 4 socket system</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">portante</reporter><labels><label>:Stats</label><label>adoptme</label><label>bug</label></labels><created>2014-12-06T03:48:08Z</created><updated>2015-11-21T22:11:03Z</updated><resolved>2015-11-21T22:11:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T22:11:03Z" id="158686010">This output was just what came out of Sigar, which we no longer use. Now we just report available processors
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>java.lang.NoSuchMethodError: org.apache.lucene.util.UnicodeUtil.UTF16toUTF8(Ljava/lang/CharSequence;IILorg/apache/lucene/util/BytesRef;)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8799</link><project id="" key="" /><description>Getting the error after upgrading to  ES 1.4.1 and lucene 4.10.2 in windows java version 1.8.0 using 
Exception happens while calling   TransportClient client = new TransportClient();  I checked and rechecked if there is any old libs used by any chance with no avail. 

Exception in thread "main" java.lang.NoSuchMethodError: org.apache.lucene.util.UnicodeUtil.UTF16toUTF8(Ljava/lang/CharSequence;IILorg/apache/lucene/util/BytesRef;)V
    at org.elasticsearch.common.Strings.toUTF8Bytes(Strings.java:1529)
    at org.elasticsearch.common.Strings.toUTF8Bytes(Strings.java:1525)
    at org.elasticsearch.search.facet.filter.InternalFilterFacet.&lt;clinit&gt;(InternalFilterFacet.java:40)
    at org.elasticsearch.search.facet.TransportFacetModule.configure(TransportFacetModule.java:39)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:204)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:85)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:130)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:188)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:118)
    at model.elasticsearch.ElasticSearchInterface.main(ElasticSearchInterface.java:129)
</description><key id="51163794">8799</key><summary>java.lang.NoSuchMethodError: org.apache.lucene.util.UnicodeUtil.UTF16toUTF8(Ljava/lang/CharSequence;IILorg/apache/lucene/util/BytesRef;)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajubala</reporter><labels /><created>2014-12-06T00:52:05Z</created><updated>2014-12-07T17:25:06Z</updated><resolved>2014-12-07T17:25:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-12-07T05:59:39Z" id="65927714">Do you use any plugin?
Or do you have old lucent jar in your ES_HOME/lib?
</comment><comment author="rajubala" created="2014-12-07T17:25:04Z" id="65945017">Fount it, it was es-javalib mismatch since my old libs were lying around in classpath, which was hard to find out, thank you for your attention. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support human readable dates for percentiles aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8798</link><project id="" key="" /><description>Currently, if you use percentiles aggregation against a date field, it will return the bucket values in epoch time.  Would be nice to provide a feature to convert to human readable format.  Workaround is to manipulate this in the frontend app.

```
   "aggregations": {
      "bar": {
         "values": {
            "1.0": 1352472220600,
            "5.0": 1362647605883.3335,
            "25.0": 1385450421417.3262,
            "50.0": 1399884810736.6772,
            "75.0": 1407145988118.4814,
            "95.0": 1412286766500,
            "99.0": 1413406457500
         }
      }
   }
```
</description><key id="51126300">8798</key><summary>Support human readable dates for percentiles aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Aggregations</label></labels><created>2014-12-05T17:46:04Z</created><updated>2015-01-09T17:00:15Z</updated><resolved>2015-01-09T17:00:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-12-07T15:10:02Z" id="65940195">Related #6812 
</comment><comment author="colings86" created="2014-12-23T10:35:12Z" id="67938848">This is being addressed in the following PR https://github.com/elasticsearch/elasticsearch/pull/9032
</comment><comment author="colings86" created="2015-01-09T17:00:15Z" id="69363568">Solved in #9032 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not sort histogram buckets on shards.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8797</link><project id="" key="" /><description>Histogram do not perform any selection of the buckets at the shard level so it
is useless to sort buckets there given that we are going to sort again on the
coordinating node once buckets with the same key have been merged.
</description><key id="51119900">8797</key><summary>Do not sort histogram buckets on shards.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-05T16:49:42Z</created><updated>2015-06-07T11:53:19Z</updated><resolved>2014-12-17T13:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-06T08:05:43Z" id="65889472">LGTM, although why can't the keys be merged with a merge sort on the coordinating node? I would just think asking for results from a single shard shouldn't break what the API promises (ie sorting)?
</comment><comment author="jpountz" created="2014-12-10T23:25:23Z" id="66543905">One issue is that the histogram aggregation not only allows to sort by key (for which a merge sort would work) but also by `doc_count` or sub aggregation, and in that case merge sorting does not work anymore. But I agree it would be cleaner to expect shards to return buckets sorted by key, merge sort on the coordinating node and finally only re-sort if the order is different from `key/asc`. Let me see what I can do...
</comment><comment author="jpountz" created="2014-12-11T09:40:36Z" id="66594291">@rjernst I pushed a new commit that merge sorts on the coordinating node. This makes reduction simpler since histograms already needed buckets to be sorted by key in order to be able to eg. add empty buckets and probably a bit more memory-efficient too.
</comment><comment author="rjernst" created="2014-12-15T20:07:35Z" id="67056424">Thanks @jpountz.  I added some more comments.  It looks good to me in general.
</comment><comment author="jpountz" created="2014-12-16T18:35:01Z" id="67208122">@rjernst I tried working on your suggested changes but this code is a bit tricky (eg. the min and max bounds are optional so you can have min=null and max set). Since this code that you commented on was just moved (factored to a method mainly), would you mind if I push this change as-is and work on improving the handling of bounds on another PR?
</comment><comment author="rjernst" created="2014-12-16T18:37:11Z" id="67208467">&gt; would you mind if I push this change as-is and work on improving the handling of bounds on another PR

Sure, sounds great.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file></files><comments><comment>Aggregations: reduce histogram buckets on the fly using a priority queue.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file></files><comments><comment>Aggregations: reduce histogram buckets on the fly using a priority queue.</comment></comments></commit></commits></item><item><title>[DEB, RPM] init script double-daemonizes elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8796</link><project id="" key="" /><description>Both the deb and the rpm init script will daemonize elasticsearch - deb by using `start-stop-daemon`, rpm by using `daemon`. Additionally they will _also_ tell the starter script to daemonize itself (again). Since the elasticsearch starter script is well able to daemonize by itself (and correctly report errors if the backgrounded elasticsearch fails to initialize) it would be reasonable to not do a second backgrounding in the init scripts.
</description><key id="51117254">8796</key><summary>[DEB, RPM] init script double-daemonizes elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">t-lo</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label></labels><created>2014-12-05T16:26:18Z</created><updated>2016-06-13T17:23:55Z</updated><resolved>2016-06-13T17:23:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-06-13T17:23:54Z" id="225649755">Closing in favour of #12716
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for registering custom circuit breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8795</link><project id="" key="" /><description>This PR adds support for registering custom circuit breakers besides PARENT, REQUEST and FIELDDATA.
We needed to account memory consumption of different contexts than request or fielddata caches, but of course, this makes only sense by hooking up a new circuit breaker into the breaker hierarchy.
Maybe this is also interesting for someone else like plugin developers. 
</description><key id="51114614">8795</key><summary>Add support for registering custom circuit breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">seut</reporter><labels><label>:Circuit Breakers</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-05T16:03:10Z</created><updated>2015-06-08T13:58:44Z</updated><resolved>2015-02-02T19:32:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T12:24:16Z" id="66274902">@dakrone what do you think about this PR?
</comment><comment author="dakrone" created="2014-12-09T12:25:04Z" id="66274980">@clintongormley I am planning on looking at it
</comment><comment author="dakrone" created="2014-12-10T09:38:34Z" id="66425877">@seut I like the idea of registering breakers, and that plugins could register their own breakers. I have some ideas about this though:

I think instead of having to register a name, and then registering the breaker, I think it would be better if we do it all in the register breaker method (since we have access to `breakerSettings.getName()`).

I think we can remove the built in breaker list, and then in the constructor for `HierarchyCircuitBreakerService`, we can call the `registerBreaker` method for the PARENT, FIELDDATA, and REQUEST breaker. I think it would be okay to overwrite a breaker (using the `Name` as the key).

If the registerBreaker method rebuilds the `breakers` map, it can then be used in `ApplySettings` as well, to re-register breakers that have different settings. I think this would reduce the amount of code a bit and move the logic to a single place.
</comment><comment author="seut" created="2014-12-10T10:21:42Z" id="66431090">@dakrone Thanks for looking at it!

Yes I'd prefer to register the names inside the `registerBreaker` method, but the `BreakerSettings` class requires a `Name` instance... any suggestions to solve that? Also the `Name` instances are used to get a breaker and so are used all over the code..

And yes I've thought to refactor it so built-in breakers would be registered by the new `registerBreaker` method, but one drawback of this method is that on each call the immutable breaker map must be rebuild and re-applied (while synchronized), which is currently done only once for all built-in breakers. I don't think it will hurt so much because it's usually only done on startup and update settings calls (which normally won't occur so often), but was afraid you'd rejecting this ;)  Another way would be to use a concurrent hash map, but maybe the overhead of it (specially on get calls at `getBreaker`) isn't worth comparing to a rarely occurring immutable map rebuilts.
</comment><comment author="dakrone" created="2014-12-10T10:41:26Z" id="66433517">I think we can move to `CircuitBreaker.Name` being an interface, then `FIELDDATA`, `REQUEST`, etc can be implementations of the interface. We can then have `CircuitBreaker.Name.FIELDDATA` be a statically initialized instance of the Name. That would allow a plugin to create its own `Name` class for use when registering.

For the synchronized breaker map, I think it is fine to move to a `ConcurrentHashMap` here and remove the locks, especially since I don't expect the map to be contested at all.
</comment><comment author="seut" created="2014-12-10T10:52:10Z" id="66434751">I thought about changing `CircuitBreaker.Name` to an interface also, but I'm not quite sure how to implement safe streaming of it. I will rethink it again ;-)
Using a `ConcurrentHashMap` sounds fine to me, will change that.
</comment><comment author="dakrone" created="2014-12-10T10:54:33Z" id="66435065">&gt; I'm not quite sure how to implement safe streaming of it. I will rethink it again ;-)

If this is a 2.0 change only, you can change the streaming of it (since 2.0 requires a restart). If this goes to 1.x you can use the `out.getVersion().onOrAfter(Version.FOO)` to implement version checking on both the serialization and deserializing.
</comment><comment author="seut" created="2014-12-15T16:53:53Z" id="67025461">@dakrone After thinking again about the `CircuitBreaker.Name` implementation, I cannot figure out how a Interface would eliminate the names registration. Because, for creating instances from a stream input, we need to instantiate a concrete class or use a registry like I did. The only other way would be do use a abstract (or normal like it is now) class which is registering its its concrete child classes in its constructor. This would move the registration from ``Name.register()` to `new Name()`.
What about eliminating the `CircuitBreaker.Name` class at all and using strings instead? This would result in a little network overhead comparing to streaming integers but afaik the names are only streamed when stats are requested...  
</comment><comment author="dakrone" created="2014-12-15T20:38:50Z" id="67061283">&gt; What about eliminating the CircuitBreaker.Name class at all and using strings instead? This would result in a little network overhead comparing to streaming integers but afaik the names are only streamed when stats are requested... 

Sorry, I wasn't clear when I talked about changing the streaming, your solution is what I was suggesting. If you want to target 1.x you'll need to add backwards compatibility (using the `.onOrAfter(...)` method I mentioned previously). If you want to only add this to 2.0, you can just change the streaming to use strings instead.
</comment><comment author="seut" created="2014-12-17T14:55:32Z" id="67333070">@dakrone just pushed a fixup commit including all discussed changes. will squash it if everything is fine now.
</comment><comment author="seut" created="2014-12-17T14:56:33Z" id="67333250">@dakrone ah yeah and I will create another PR for a 1.x backport of this once this PR is accepted
</comment><comment author="dakrone" created="2014-12-23T09:38:27Z" id="67934839">@seut I added a couple of comments, but this is looking good!

I also think it would be extremely useful to add an integration test for this to test that breaker stats correctly include custom circuit breakers. You should be able to put it in `CircuitBreakerServiceTests` and use 

``` java
Iterable&lt;CircuitBreakerService&gt; serviceIter = internalCluster().getInstances(CircuitBreakerService.class);
for (CircuitBreakerService s : serviceIter) {
  s.registerBreaker(...);
}
// Maybe increment the breaker here
NodesStatsResponse resp = client().admin().cluster().prepareNodesStats().clear().setBreaker(true).get();
// Check response
```

to get the circuit breaker service, then you can register a custom breaker and issue a nodes stats to ensure that the stats include the custom breaker (you can also increment/decrement the breaker there). Does that make sense? I'm happy to help if not.
</comment><comment author="seut" created="2015-01-12T09:35:32Z" id="69545800">@dakrone pushed a fixup including all discussed changes + an integration test. because ConcurrentMap.replace() does not support null values, I had to implement it slightly different to harden it against race conditions of concurrent calls. please let me know if you see any other issues. thx!
</comment><comment author="seut" created="2015-02-02T18:51:28Z" id="72514047">any review news? ;)
</comment><comment author="dakrone" created="2015-02-02T18:52:58Z" id="72514351">@seut I'm looking at this and hoping to merge it in today, sorry for taking so long
</comment><comment author="seut" created="2015-02-02T19:38:05Z" id="72522423">ah wanted to squash my review fixups first, but anyway, thanks for merging! :)
</comment><comment author="dakrone" created="2015-02-02T19:38:37Z" id="72522532">Thanks for submitting the feature!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problems with PreBuiltAnalyzers class (1.4.1 version, and  lucene-analyzers - 3.6.2 version)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8794</link><project id="" key="" /><description>My code is
        ImmutableSettings.Builder settings = ImmutableSettings.settingsBuilder();
        settings.put("client.transport.sniff", false);
        settings.put("path.home", "/path/to/elastic/home");
        settings.put("index.number_of_replicas", 0);
        settings.put("index.number_of_shards", 1);
        settings.put("action.write_consistency", "one");

```
    settings.build();
    NodeBuilder nb = new NodeBuilder().settings(settings).local(true).data(true);
    Node node = nb.build(); // and i was calling nb.node - all is the same
    Client client = node.client();
```

When i call  nb.build() i get that error about 300 times 

 Error injecting constructor, java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.indices.analysis.PreBuiltAnalyzers
  at org.elasticsearch.indices.analysis.IndicesAnalysisService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.indices.analysis.IndicesAnalysisService
    for parameter 5 at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction
  while locating org.elasticsearch.action.support.TransportAction annotated with @org.elasticsearch.common.inject.multibindings.Element(setName=,uniqueId=295)
  at org.elasticsearch.action.ActionModule.configure(ActionModule.java:299)
  while locating java.util.Map&lt;org.elasticsearch.action.GenericAction, org.elasticsearch.action.support.TransportAction&gt;
    for parameter 3 at org.elasticsearch.client.node.NodeClient.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.client.node.NodeClient
  while locating org.elasticsearch.client.Client
    for parameter 1 at org.elasticsearch.rest.action.bulk.RestBulkAction.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.rest.action.bulk.RestBulkAction
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.indices.analysis.PreBuiltAnalyzers

In class PreBuiltAnalyzers my ide mark as wrong this part of code

ARABIC {
        @Override
        protected Analyzer create(Version version) {
            return new ArabicAnalyzer(version.luceneVersion);
        }
    },

and 70% similar blocks marked.
What is wrong in my code or mb problem in this version ElasticSearch?
</description><key id="51105377">8794</key><summary>Problems with PreBuiltAnalyzers class (1.4.1 version, and  lucene-analyzers - 3.6.2 version)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexdharma91</reporter><labels><label>feedback_needed</label></labels><created>2014-12-05T14:39:50Z</created><updated>2014-12-29T11:09:28Z</updated><resolved>2014-12-29T11:09:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-12-07T06:04:24Z" id="65927821">Hi,
What does mean "lucene-analyzers - 3.6.2 version" in title?
Do you use "lucene-analyzers" 3.6.2 with elasticsearch 1.4.1?

It does not work. Elasticsearch 1.4.1 require Lucene 4.10.2.
</comment><comment author="alexdharma91" created="2014-12-08T07:55:16Z" id="66034639">Do you mean lucene-core? If yes, i try do it, and add lucene-analizers 3.6.2 and get this error 

11) Error injecting constructor, java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.indices.analysis.PreBuiltAnalyzers

but without lucene-analizer dependency i did get the same error.

if that has value, when i did use elasticsearch 1.2.1 version, in project was 1.0.0, now i use 1.4.1 version of elasticsearch 
</comment><comment author="alexdharma91" created="2014-12-08T11:08:31Z" id="66102209">I solved problems with versions and still i have that broblems with PreBuiltAnalyzers.In methods Create 
return many types of Analizers(PatternAnalyzer, ArabicAnalyzer ...). And my ide mark these return statements as wrong, adding phraze following content.

"Required: org.apache.lucene.analysis.Analyzer
but found org.apache.lucene.analysis.miscellaneous.PatternAnalyzer"

Although PatternAnalyzer extends Analyzer and it should work

What is wrong with this class(PreBuiltAnalyzers)?

p.s 
now i use  elasticsearch (lib version - 1.0.0 and es server 1.2.1).
And es dependency get several lucene dependencies with 4.6.1 verion.

Current configuration must be valid, i also had running code with thise version libraries.
</comment><comment author="rjernst" created="2014-12-08T16:00:53Z" id="66136904">This sounds like you have mixed versions of Lucene, as @johtani thought.  This will simply not work.  You need to remove all of your custom/old versions of Lucene from the classpath, and only use that which was shipped with Elasticsearch.
</comment><comment author="alexdharma91" created="2014-12-29T08:46:15Z" id="68240697">Problem was solved by adding this dependency to my pom file

   &lt;dependency&gt;
            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
            &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;
            &lt;version&gt;4.10.2&lt;/version&gt;
  &lt;/dependency&gt;
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Check if proc file exists before calling sysctl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8793</link><project id="" key="" /><description>The packaged init scripts could return an error, if the file
/proc/sys/vm/max_map_count was not existing and we still called
sysctl.

This is primarly to prevent confusing error messages when elasticsearch
is started under virtualized environments without a proc file system.

Closes #4978
</description><key id="51099567">8793</key><summary>Check if proc file exists before calling sysctl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-05T13:39:28Z</created><updated>2015-06-07T16:49:16Z</updated><resolved>2014-12-08T08:57:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-06T07:52:46Z" id="65889222">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_upgrade workflow?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8792</link><project id="" key="" /><description>Hi,

I just did an update from 1.3.6 to 1.4.1 on a one node instance (small dataset (~300 indices@1GB each) and single shard per index). Since we started with 1.0.2, I was curious about the "_upgrade" feature.

After reading

&gt; Upgrading is an I/O intensive operation, and is **limited to processing a single shard per node at a time**. It also is not allowed to run at the same time as optimize.

and some tests with one and more indices, I did an:

```
curl -X POST http://localhost:9200/*/_upgrade
```

This killed the node in a matter of minutes because of OutOfMemory exceptions. Nothing serious happened, but I was thinking on how to do this _uprade process with hundreds of indices.
Should I write some $SHELL script to check the status of the upgrade of every index or did I just miss something?

Thanks in advance!

Regards,
Malte
</description><key id="51084229">8792</key><summary>_upgrade workflow?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">temal-</reporter><labels><label>:Upgrade API</label><label>bug</label></labels><created>2014-12-05T10:32:42Z</created><updated>2015-04-17T08:49:45Z</updated><resolved>2015-04-17T08:49:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-05T11:25:30Z" id="65777801">@rjernst can you look at this?
</comment><comment author="rjernst" created="2014-12-05T18:08:22Z" id="65829874">@temal- Can you do a `GET` on `/_upgrade?pretty` and post the results here?
</comment><comment author="temal-" created="2014-12-06T10:48:10Z" id="65892780">I can do that, but I need a bit of time to setup a testing instance since I already wrote a script which converts one index at a time.

Currently all indices have the following output:

```
$ curl -X GET 'http://localhost:9200/_upgrade?pretty'
{
  "logstash-foo-2014.05.31" : {
    "size_in_bytes" : 722050977,
    "size_to_upgrade_in_bytes" : 0
  },
[...]
}
```

Before I did the `/_upgrade`, all indices had the same numbers in `size_in_bytes` and `size_to_upgrade_in_bytes`.

I'll setup an instance with 1.3.6, import all the data and then do an update to 1.4.1. 
I guess I can paste the output of `/_upgrade?pretty` in two or three days.

Malte
</comment><comment author="temal-" created="2014-12-11T14:24:08Z" id="66625133">Sorry for the delay.
I only imported a subset of data to speed up the process.

Here is the output:

```
$ curl -X GET "http://localhost:9200/_upgrade?pretty"
{
  "logstash-foo-2014.05.09" : {
    "size_in_bytes" : 959964841,
    "size_to_upgrade_in_bytes" : 959964841
  },
  "logstash-foo-bar-2014.05.05" : {
    "size_in_bytes" : 3494641761,
    "size_to_upgrade_in_bytes" : 3494641761
  },
  "logstash-foo-2014.05.04" : {
    "size_in_bytes" : 972195432,
    "size_to_upgrade_in_bytes" : 972195432
  },
  "logstash-foo-bar-2014.05.06" : {
    "size_in_bytes" : 3513336903,
    "size_to_upgrade_in_bytes" : 3513336903
  },
  "logstash-foo-2014.05.03" : {
    "size_in_bytes" : 820894605,
    "size_to_upgrade_in_bytes" : 820894605
  },
  "logstash-foo-2014.05.02" : {
    "size_in_bytes" : 981620940,
    "size_to_upgrade_in_bytes" : 981620940
  },
  "logstash-foo-bar-2014.05.03" : {
    "size_in_bytes" : 2517727792,
    "size_to_upgrade_in_bytes" : 2517727792
  },
  "logstash-foo-bar-2014.05.04" : {
    "size_in_bytes" : 2934196428,
    "size_to_upgrade_in_bytes" : 2934196428
  },
  "logstash-foo-2014.05.01" : {
    "size_in_bytes" : 886680894,
    "size_to_upgrade_in_bytes" : 886680894
  },
  "logstash-foo-bar-2014.05.09" : {
    "size_in_bytes" : 2835514923,
    "size_to_upgrade_in_bytes" : 2835514923
  },
  "logstash-foo-2014.05.08" : {
    "size_in_bytes" : 1043674649,
    "size_to_upgrade_in_bytes" : 1043674649
  },
  "logstash-foo-2014.05.07" : {
    "size_in_bytes" : 1166401798,
    "size_to_upgrade_in_bytes" : 1166401798
  },
  "logstash-foo-2014.05.06" : {
    "size_in_bytes" : 1170950460,
    "size_to_upgrade_in_bytes" : 1170950460
  },
  "logstash-foo-bar-2014.05.07" : {
    "size_in_bytes" : 5508278905,
    "size_to_upgrade_in_bytes" : 3521593475
  },
  "logstash-foo-2014.05.05" : {
    "size_in_bytes" : 1148138104,
    "size_to_upgrade_in_bytes" : 1148138104
  },
  "logstash-foo-bar-2014.05.08" : {
    "size_in_bytes" : 3129771903,
    "size_to_upgrade_in_bytes" : 3129771903
  },
  "logstash-foo-bar-2014.05.02" : {
    "size_in_bytes" : 2978676170,
    "size_to_upgrade_in_bytes" : 2978676170
  },
  "logstash-foo-bar-2014.05.01" : {
    "size_in_bytes" : 2638082490,
    "size_to_upgrade_in_bytes" : 2638082490
  }
}
```
</comment><comment author="rjernst" created="2015-04-17T08:49:44Z" id="93946156">Given the original command shown in this issue, I think the problem would now be fixed with #9639.  The wait_for_completion flag was defaulting to false, and the limiting of optimize never correctly worked when running asynchronously, so this is removed altogether for 2.0 (#9640).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] ConcurrentDynamicTemplateTests.testDynamicMappingIntroductionPropagatesToAll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8791</link><project id="" key="" /><description>see http://build-us-00.elasticsearch.org/job/es_core_master_debian/2660/

The test fails with the exception below, which seems to happen after the test has completed and when we are cleaning up the cluster. I can't see anything unusual that happens before this exception is thrown

```
java.lang.AssertionError: Delete Index failed - not acked
Expected: &lt;true&gt;
     but: was &lt;false&gt;
    at __randomizedtesting.SeedInfo.seed([B913CFF60F77158E:8B14A1DA673FFC82]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:114)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:110)
    at org.elasticsearch.test.TestCluster.wipeIndices(TestCluster.java:138)
    at org.elasticsearch.test.TestCluster.wipe(TestCluster.java:75)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:602)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1796)
    at sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:885)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="51080652">8791</key><summary>[CI Failure] ConcurrentDynamicTemplateTests.testDynamicMappingIntroductionPropagatesToAll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label></labels><created>2014-12-05T09:52:36Z</created><updated>2014-12-05T13:51:55Z</updated><resolved>2014-12-05T13:51:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-05T10:36:03Z" id="65772684">this one is again stuck in the fielddata cache clear

```
 1&gt;   96) Thread[id=94, name=elasticsearch[node_2][clusterService#updateTask][T#1], state=RUNNABLE, group=TGRP-BroadcastActionsTests]
  1&gt;         at com.google.common.cache.LocalCache$Segment.expireEntries(LocalCache.java:2620)
  1&gt;         at com.google.common.cache.LocalCache$Segment.runLockedCleanup(LocalCache.java:3449)
  1&gt;         at com.google.common.cache.LocalCache$Segment.cleanUp(LocalCache.java:3441)
  1&gt;         at com.google.common.cache.LocalCache.cleanUp(LocalCache.java:3861)
  1&gt;         at com.google.common.cache.LocalCache$LocalManualCache.cleanUp(LocalCache.java:4800)
  1&gt;         at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.clear(IndicesFieldDataCache.java:256)
  1&gt;         at org.elasticsearch.index.fielddata.IndexFieldDataService.clear(IndexFieldDataService.java:172)
  1&gt;         at org.elasticsearch.indices.InternalIndicesService.removeIndex(InternalIndicesService.java:410)
  1&gt;         at org.elasticsearch.indices.InternalIndicesService.deleteIndex(InternalIndicesService.java:344)
  1&gt;         at org.elasticsearch.indices.cluster.IndicesClusterStateService.deleteIndex(IndicesClusterStateService.java:866)
  1&gt;         at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedIndices(IndicesClusterStateService.java:260)
  1&gt;         at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:189)
  1&gt;         at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:443)
```

and given the logs this is where the time it spend:

```
  1&gt; [2014-12-05 09:41:03,227][DEBUG][indices                  ] [node_2] [idx] closing index cache (reason [index no longer part of the metadata])
  1&gt; [2014-12-05 09:41:03,227][DEBUG][index.cache.bitset       ] [node_2] [idx] Clearing all Bitsets because [close]
  1&gt; [2014-12-05 09:41:03,227][DEBUG][indices                  ] [node_2] [idx] clearing index field data (reason [index no longer part of the metadata])
  1&gt; [2014-12-05 09:41:08,469][DEBUG][indices.memory           ] [node_0] recalculating shard indexing buffer (reason=[[ADDED]]), total is [98.9mb] with [6] active shards, each shard set to indexing=[16.4mb], translog=[64kb]
  1&gt; [2014-12-05 09:41:08,469][DEBUG][test.engine              ] [node_0] [idx][0] updating index_buffer_size from [64mb] to [16.4mb]
  1&gt; [2014-12-05 09:41:08,470][DEBUG][test.engine              ] [node_0] [idx][3] updating index_buffer_size from [64mb] to [16.4mb]
  1&gt; [2014-12-05 09:41:08,470][DEBUG][test.engine              ] [node_0] [idx][4] updating index_buffer_size from [64mb] to [16.4mb]
  1&gt; [2014-12-05 09:41:08,470][DEBUG][test.engine              ] [node_0] [idx][6] updating index_buffer_size from [64mb] to [16.4mb]
  1&gt; [2014-12-05 09:41:08,471][DEBUG][test.engine              ] [node_0] [idx][7] updating index_buffer_size from [64mb] to [16.4mb]
  1&gt; [2014-12-05 09:41:08,471][DEBUG][test.engine              ] [node_0] [idx][9] updating index_buffer_size from [64mb] to [16.4mb]
  1&gt; [2014-12-05 09:41:32,931][DEBUG][discovery.zen.publish    ] [node_0] timed out waiting for all nodes to process published state [344] (timeout [30s])
  1&gt; [2014-12-05 09:41:32,931][INFO ][test                     ] dump all threads on AssertionError
  1&gt; [2014-12-05 09:41:32,931][DEBUG][cluster.service          ] [node_0] set local cluster state to version 344
  1&gt; [2014-12-05 09:41:32,931][DEBUG][indices.cluster          ] [node_0] [idx] cleaning index, no longer part of the metadata
  1&gt; [2014-12-05 09:41:32,933][DEBUG][indices                  ] [node_0] [idx] closing ... (reason [index no longer part of the metadata])
  1&gt; [2014-12-05 09:41:32,933][DEBUG][indices                  ] [node_0] [idx] closing index service (reason [index no longer part of the metadata])
  1&gt; [2014-12-05 09:41:32,933][DEBUG][index.service            ] [node_0] [idx] [0] closing... (reason: [index no longer part of the metadata])
  1&gt; [2014-12-05 09:41:32,933][INFO ][test.store               ] [node_0] [idx][0] Shard state before potentially flushing is STARTED
  1&gt; [2014-12-05 09:41:32,951][ERROR][test                     ] 
  1&gt;    1) Thread[id=16, name=elasticsearch[node_0][master_mapping_updater], state=TIMED_WAITING, group=TGRP-BroadcastActionsTests]
```

that's a 30 sec cache clear... hmm something seems to be wrong here.
</comment><comment author="s1monw" created="2014-12-05T10:36:44Z" id="65772747">@dakrone why do we again to the explicit cache clear? it's only because of tests right now right?
</comment><comment author="dakrone" created="2014-12-05T10:42:16Z" id="65773323">@s1monw yes, because if we don't explicitly call `cache.cleanUp()`, and just invalidate the entries, the listeners for fielddata may not have been called, which means tests will fail because the breaker hasn't been reset yet.
</comment><comment author="s1monw" created="2014-12-05T11:23:16Z" id="65777544">I really would love to get the reason for this problem maybe we should add some logging to this too? I wonder why the hack we load FD in this test at all too...
</comment><comment author="s1monw" created="2014-12-05T12:28:10Z" id="65783535">```
REPRODUCE WITH  : mvn clean test -Dtests.seed=B913CFF60F77158E -Dtests.class=org.elasticsearch.indices.mapping.ConcurrentDynamicTemplateTests -Dtests.method="testDynamicMappingIntroductionPropagatesToAll" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=org.elasticsearch -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=1024m -Dtests.jvm.argline="-server -XX:+UseSerialGC -XX:+UseCompressedOops" -Dtests.locale=sq -Dtests.timezone=Africa/Khartoum -Dtests.processors=8
```

with this see it uses `48 Fields * 3 field instances = 144 fields`  but even whit that it's unlikely to hang 30 sec?
</comment><comment author="dakrone" created="2014-12-05T13:51:55Z" id="65791953">Pushed a fix for this (caa5af4) so it still uses random field data types, but no longer loads fielddata eagerly
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch does not remove settings, if you provide less items in array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8790</link><project id="" key="" /><description>I am facing with problem when i need to update synonyms on my index dynamically. Solution is simple close index, update settings, open index. But issue is that if i provide less synonyms, then elastic search will change that items which i provide and leave other 
Steps to reproduce:
PUT http://localhost:9200/test_index/

``` javascript
{
  "test_index" : {
    "settings" : {
      "index" : {
        "creation_date" : "1417564655756",
        "analysis" : {
          "filter" : {
            "stemmer" : {
              "type" : "stemmer",
              "language" : "danish"
            },
            "suggestions_shingle" : {
              "min_shingle_size" : "2",
              "max_shingle_size" : "5",
              "type" : "shingle",
              "filler_token" : ""
            },
            "spellcorrections" : {
              "type" : "synonym",
              "synonyms" : [ "incorrect1=&gt;correct1", "otherword=&gt;word2" ],
              "expand" : "false"
            },
            "synonyms" : {
              "type" : "synonym",
              "expand" : "true",
              "synonyms" : [ "synonym1, synonym2", "third, synthird", "and=&gt;so on" ]
            },
            "stopwords" : {
              "ignore_case" : "true",
              "enable_position_increments" : "true",
              "type" : "stop",
              "stopwords" : [ "_danish_" ]
            }
          },
          "analyzer" : {
            "DidYouMeanAnalyzer" : {
              "filter" : [ "lowercase", "stopwords", "synonyms" ],
              "char_filter" : [ "html_strip" ],
              "type" : "custom",
              "tokenizer" : "standard"
            },
            "suggestions" : {
              "filter" : [ "stopwords", "lowercase", "suggestions_shingle", "trim", "unique" ],
              "char_filter" : [ "html_strip" ],
              "type" : "custom",
              "tokenizer" : "standard"
            },
            "default" : {
              "filter" : [ "lowercase", "spellcorrections", "synonyms", "stopwords", "stemmer" ],
              "char_filter" : [ "html_strip" ],
              "type" : "custom",
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_shards" : "5",
        "number_of_replicas" : "0",
        "version" : {
          "created" : "1040199"
        },
        "uuid" : "1NS67y_4S4qdEEk2_L2r5w"
      }
    }
  }
}
```

POST http://localhost:9200/test_index/_close/

PUT http://localhost:9200/test_index/_settings

``` javascript
{
  "test_index" : {
    "settings" : {
      "index" : {
        "creation_date" : "1417564655756",
        "analysis" : {
          "filter" : {
            "stemmer" : {
              "type" : "stemmer",
              "language" : "danish"
            },
            "suggestions_shingle" : {
              "min_shingle_size" : "2",
              "max_shingle_size" : "5",
              "type" : "shingle",
              "filler_token" : ""
            },
            "spellcorrections" : {
              "type" : "synonym",
              "synonyms" : [],
              "expand" : "false"
            },
            "synonyms" : {
              "type" : "synonym",
              "expand" : "true",
              "synonyms" : [ "test, test" ]
            },
            "stopwords" : {
              "ignore_case" : "true",
              "enable_position_increments" : "true",
              "type" : "stop",
              "stopwords" : [ "_danish_" ]
            }
          },
          "analyzer" : {
            "DidYouMeanAnalyzer" : {
              "filter" : [ "lowercase", "stopwords", "synonyms" ],
              "char_filter" : [ "html_strip" ],
              "type" : "custom",
              "tokenizer" : "standard"
            },
            "suggestions" : {
              "filter" : [ "stopwords" ],
              "char_filter" : [ "html_strip" ],
              "type" : "custom",
              "tokenizer" : "standard"
            },
            "default" : {
              "filter" : [ "lowercase", "spellcorrections", "synonyms", "stopwords", "stemmer" ],
              "char_filter" : [ "html_strip" ],
              "type" : "custom",
              "tokenizer" : "standard"
            }
          }
        },
        "number_of_shards" : "5",
        "number_of_replicas" : "0",
        "version" : {
          "created" : "1040199"
        },
        "uuid" : "1NS67y_4S4qdEEk2_L2r5w"
      }
    }
  }
}
```

POST http://localhost:9200/test_index/_open

GET http://localhost:9200/test_index/_settings?pretty

``` javascript
{
  "test_index" : {
    "settings" : {
      "index" : {
        "creation_date" : "1417771048233",
        "number_of_shards" : "5",
        "test_index" : {
          "settings" : {
            "index" : {
              "creation_date" : "1417564655756",
              "analysis" : {
                "filter" : {
                  "stemmer" : {
                    "type" : "stemmer",
                    "language" : "danish"
                  },
                  "suggestions_shingle" : {
                    "min_shingle_size" : "2",
                    "max_shingle_size" : "5",
                    "type" : "shingle",
                    "filler_token" : ""
                  },
                  "spellcorrections" : {
                    "type" : "synonym",
                    "synonyms" : [ "incorrect1=&gt;correct1", "otherword=&gt;word2" ],
                    "expand" : "false"
                  },
                  "synonyms" : {
                    "type" : "synonym",
                    "synonyms" : [ "test, test", "third, synthird", "and=&gt;so on" ],
                    "expand" : "true"
                  },
                  "stopwords" : {
                    "ignore_case" : "true",
                    "enable_position_increments" : "true",
                    "type" : "stop",
                    "stopwords" : [ "_danish_" ]
                  }
                },
                "analyzer" : {
                  "DidYouMeanAnalyzer" : {
                    "filter" : [ "lowercase", "stopwords", "synonyms" ],
                    "char_filter" : [ "html_strip" ],
                    "type" : "custom",
                    "tokenizer" : "standard"
                  },
                  "suggestions" : {
                    "filter" : [ "stopwords", "lowercase", "suggestions_shingle", "trim", "unique" ],
                    "char_filter" : [ "html_strip" ],
                    "type" : "custom",
                    "tokenizer" : "standard"
                  },
                  "default" : {
                    "filter" : [ "lowercase", "spellcorrections", "synonyms", "stopwords", "stemmer" ],
                    "char_filter" : [ "html_strip" ],
                    "type" : "custom",
                    "tokenizer" : "standard"
                  }
                }
              },
              "number_of_shards" : "5",
              "version" : {
                "created" : "1040199"
              },
              "uuid" : "1NS67y_4S4qdEEk2_L2r5w",
              "number_of_replicas" : "0"
            }
          }
        },
        "number_of_replicas" : "0",
        "version" : {
          "created" : "1040199"
        },
        "uuid" : "fmWSN1gNQ-G0qpQz9AxG7A"
      }
    }
  }
}
```

As you see synonyms i expect to be just 

``` javascript
"synonyms" : [ "test, test" ]
```

and   suggestions analyzer to have only one filter

``` javascript
              "filter" : [ "stopwords" ],
```
</description><key id="51078268">8790</key><summary>Elasticsearch does not remove settings, if you provide less items in array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vovikdrg</reporter><labels /><created>2014-12-05T09:24:09Z</created><updated>2014-12-09T12:16:58Z</updated><resolved>2014-12-09T12:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T12:16:58Z" id="66274150">Hi @vovikdrg 

Thanks for reporting.  I'm going to close this issue as it is a duplicate of #6887
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GatewayService should register cluster state listener before checking for current state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8789</link><project id="" key="" /><description>At the moment we may miss a state change and fail to recover on time.

Exposed by build failure: http://build-us-00.elasticsearch.org/job/es_core_1x_suse/53/testReport/junit/org.elasticsearch.cluster/NoMasterNodeTests/testNoMasterActions_writeMasterBlock/
</description><key id="51075574">8789</key><summary>GatewayService should register cluster state listener before checking for current state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-05T08:48:18Z</created><updated>2015-06-07T18:01:22Z</updated><resolved>2014-12-05T09:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-05T09:18:06Z" id="65764739">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/GatewayService.java</file></files><comments><comment>Gateway: GatewayService should register cluster state listener before checking for current state</comment></comments></commit></commits></item><item><title>Duplicate id in index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8788</link><project id="" key="" /><description>I decided to reindex my data to take advantage of `doc_values`, but one of 30 indices (~120m docs in each) got less documents after reindexing. I reindexed again and docs disappeared again.

Then I bisected the problem to specific docs and found that some docs in source index has duplicate ids.

```
curl -s "http://web245:9200/statistics-20141110/_search?pretty&amp;q=_id:1jC2LxTjTMS1KHCn0Prf1w"
```

``` json
{
  "took" : 1156,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"}
    }, {
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"}
    } ]
  }
}
```

Here are two indices, source and destination:

```
health status index                  pri rep docs.count docs.deleted store.size pri.store.size
green  open   statistics-20141110      5   0  116217042            0     12.3gb         12.3gb
green  open   statistics-20141110-dv   5   1  116216507            0     32.3gb         16.1gb
```

Segments of problematic index:

```
index               shard prirep ip            segment generation docs.count docs.deleted    size size.memory committed searchable version compound
statistics-20141110 0     p      192.168.0.190 _gga         21322   14939669            0   1.6gb     4943008 true      true       4.9.0   false
statistics-20141110 0     p      192.168.0.190 _isc         24348   10913518            0   1.1gb     4101712 true      true       4.9.0   false
statistics-20141110 1     p      192.168.0.245 _7i7          9727    7023269            0   766mb     2264472 true      true       4.9.0   false
statistics-20141110 1     p      192.168.0.245 _i01         23329   14689581            0   1.5gb     4788872 true      true       4.9.0   false
statistics-20141110 2     p      192.168.1.212 _9wx         12849    8995444            0 987.7mb     3326288 true      true       4.9.0   false
statistics-20141110 2     p      192.168.1.212 _il1         24085   13205585            0   1.4gb     4343736 true      true       4.9.0   false
statistics-20141110 3     p      192.168.1.212 _8pc         11280   10046395            0     1gb     4003824 true      true       4.9.0   false
statistics-20141110 3     p      192.168.1.212 _hwt         23213   13226096            0   1.3gb     4287544 true      true       4.9.0   false
statistics-20141110 4     p      192.168.2.88  _91i         11718    8328558            0 909.2mb     2822712 true      true       4.9.0   false
statistics-20141110 4     p      192.168.2.88  _hms         22852   14848927            0   1.5gb     4777472 true      true       4.9.0   false
```

The only thing that happened with index besides indexing is optimizing to 2 segments per shard.
</description><key id="51073197">8788</key><summary>Duplicate id in index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>:Core</label><label>:Engine</label><label>bug</label><label>critical</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-05T08:10:06Z</created><updated>2015-04-08T16:26:56Z</updated><resolved>2015-01-29T15:37:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-09T12:14:51Z" id="66273928">Hi @bobrik 

Is there any chance this index was written with Elasticsearch 1.2.0?

Please could you provide the output of this request:

```
curl -s "http://web245:9200/statistics-20141110/_search?pretty&amp;q=_id:1jC2LxTjTMS1KHCn0Prf1w&amp;explain&amp;fields=_source,_routing"
```
</comment><comment author="bobrik" created="2014-12-09T12:33:09Z" id="66275942">Routing is automatically inferred from `@key`

``` json
{
  "took" : 1744,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    }, {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    } ]
  }
}
```

Index was created on 1.3.4, we upgraded from 1.0.1 to 1.3.2 on 2014-09-22
</comment><comment author="clintongormley" created="2014-12-09T13:44:59Z" id="66284081">Hi @bobrik 

Hmm, these two docs are on the same shard!  Do you ever run updates on these docs? Could you send the output of this command please?

```
curl -s "http://web245:9200/statistics-20141110/_search?pretty&amp;q=_id:1jC2LxTjTMS1KHCn0Prf1w&amp;explain&amp;fields=_source,_routing,_version"
```
</comment><comment author="bobrik" created="2014-12-09T13:48:41Z" id="66284543">Of course they are, that's how routing works :)

I didn't run any updates, because my code only does indexing. It doesn't even know ids that are assigned by elasticsearch.

``` json
{
  "took" : 51,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    }, {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    } ]
  }
}
```
</comment><comment author="clintongormley" created="2014-12-09T16:56:22Z" id="66316057">Sorry @bobrik - I gave you the wrong request, it should be:

```
curl -s "http://web245:9200/statistics-20141110/_search?pretty&amp;q=_id:1jC2LxTjTMS1KHCn0Prf1w&amp;explain&amp;fields=_source,_routing,version"
```

And so you're using auto-assigned IDs?  Did any of your shards migrate to other nodes, or did a primary fail during optimization?
</comment><comment author="s1monw" created="2014-12-09T17:05:25Z" id="66317707">I think this is caused by https://github.com/elasticsearch/elasticsearch/pull/7729 @bobrik are you coming from &lt; 1.3.3 with this index and are you using bulk?
</comment><comment author="bobrik" created="2014-12-09T17:06:19Z" id="66317883">```
curl -s 'http://web605:9200/statistics-20141110/_search?pretty&amp;q=_id:1jC2LxTjTMS1KHCn0Prf1w&amp;explain&amp;fields=_source,_routing,version'
```

``` json
{
  "took" : 46,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    }, {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    } ]
  }
}
```

I bet you wanted this:

```
curl -s 'http://web605:9200/statistics-20141110/_search?pretty&amp;q=_id:1jC2LxTjTMS1KHCn0Prf1w&amp;explain&amp;fields=_source,_routing' -d '{"version":true}'
```

``` json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_version" : 1,
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    }, {
      "_shard" : 3,
      "_node" : "YOK_20U7Qee-XSasg0J8VA",
      "_index" : "statistics-20141110",
      "_type" : "events",
      "_id" : "1jC2LxTjTMS1KHCn0Prf1w",
      "_version" : 1,
      "_score" : 1.0,
      "_source":{"@timestamp":"2014-11-10T14:30:00+0300","@key":"client_belarussia_msg_sended_from_mutual__22_1","@value":"149"},
      "fields" : {
        "_routing" : "client_belarussia_msg_sended_from_mutual__22_1"
      },
      "_explanation" : {
        "value" : 1.0,
        "description" : "ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:",
        "details" : [ {
          "value" : 1.0,
          "description" : "boost"
        }, {
          "value" : 1.0,
          "description" : "queryNorm"
        } ]
      }
    } ]
  }
}
```

There were many migrations, but not during optimization, unless es moves shards after new index is created. Basically at 00:00 new index is created and at 00:45 optimization for old indices starts.
</comment><comment author="s1monw" created="2014-12-09T17:09:22Z" id="66318412">do you have client nodes that are pre 1.3.3?
</comment><comment author="bobrik" created="2014-12-09T17:12:53Z" id="66319033">@s1monw index is created on 1.3.4:

```
[2014-09-30 12:03:49,991][INFO ][node                     ] [statistics04] version[1.3.3], pid[17937], build[ddf796d/2014-09-29T13:39:00Z]
```

```
[2014-09-30 14:03:19,205][INFO ][node                     ] [statistics04] version[1.3.4], pid[89485], build[a70f3cc/2014-09-30T09:07:17Z]
```

Nov 11 is definitely after Sep 30. Shouldn't be #7729 then.

We don't have client nodes, everything is over http. But yeah, we use bulk indexing and automatically assigned ids.
</comment><comment author="clintongormley" created="2014-12-09T17:25:10Z" id="66321246">Hi @bobrik 

(you guessed right about `version=true` :) )

OK - we're going to need more info. Please could you send:

```
curl -s 'http://web605:9200/statistics-20141110/_settings?pretty'
curl -s 'http://web605:9200/statistics-20141110/_segments?pretty'
```
</comment><comment author="bobrik" created="2014-12-09T18:18:44Z" id="66330124">``` json
{
  "statistics-20141110" : {
    "settings" : {
      "index" : {
        "codec" : {
          "bloom" : {
            "load" : "false"
          }
        },
        "uuid" : "JZXC-8C3TFC71EnMGMHSWw",
        "number_of_replicas" : "0",
        "number_of_shards" : "5",
        "version" : {
          "created" : "1030499"
        }
      }
    }
  }
}
```

``` json
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "indices" : {
    "statistics-20141110" : {
      "shards" : {
        "0" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "hBg3FpLGQw6B9l-Hil2c8Q"
          },
          "num_committed_segments" : 2,
          "num_search_segments" : 2,
          "segments" : {
            "_gga" : {
              "generation" : 21322,
              "num_docs" : 14939669,
              "deleted_docs" : 0,
              "size_in_bytes" : 1729206228,
              "memory_in_bytes" : 4943008,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            },
            "_isc" : {
              "generation" : 24348,
              "num_docs" : 10913518,
              "deleted_docs" : 0,
              "size_in_bytes" : 1254410507,
              "memory_in_bytes" : 4101712,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            }
          }
        } ],
        "1" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "ajMe-w2lSIO0Tz5WEUs4qQ"
          },
          "num_committed_segments" : 2,
          "num_search_segments" : 2,
          "segments" : {
            "_7i7" : {
              "generation" : 9727,
              "num_docs" : 7023269,
              "deleted_docs" : 0,
              "size_in_bytes" : 803299557,
              "memory_in_bytes" : 2264472,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            },
            "_i01" : {
              "generation" : 23329,
              "num_docs" : 14689581,
              "deleted_docs" : 0,
              "size_in_bytes" : 1659303375,
              "memory_in_bytes" : 4788872,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            }
          }
        } ],
        "2" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "hyUu93q7SRehHBVZfSmvOg"
          },
          "num_committed_segments" : 2,
          "num_search_segments" : 2,
          "segments" : {
            "_9wx" : {
              "generation" : 12849,
              "num_docs" : 8995444,
              "deleted_docs" : 0,
              "size_in_bytes" : 1035711205,
              "memory_in_bytes" : 3326288,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            },
            "_il1" : {
              "generation" : 24085,
              "num_docs" : 13205585,
              "deleted_docs" : 0,
              "size_in_bytes" : 1510021893,
              "memory_in_bytes" : 4343736,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            }
          }
        } ],
        "3" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "hyUu93q7SRehHBVZfSmvOg"
          },
          "num_committed_segments" : 2,
          "num_search_segments" : 2,
          "segments" : {
            "_8pc" : {
              "generation" : 11280,
              "num_docs" : 10046395,
              "deleted_docs" : 0,
              "size_in_bytes" : 1143637974,
              "memory_in_bytes" : 4003824,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            },
            "_hwt" : {
              "generation" : 23213,
              "num_docs" : 13226096,
              "deleted_docs" : 0,
              "size_in_bytes" : 1485110397,
              "memory_in_bytes" : 4287544,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            }
          }
        } ],
        "4" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "hyUu93q7SRehHBVZfSmvOg"
          },
          "num_committed_segments" : 2,
          "num_search_segments" : 2,
          "segments" : {
            "_91i" : {
              "generation" : 11718,
              "num_docs" : 8328558,
              "deleted_docs" : 0,
              "size_in_bytes" : 953452801,
              "memory_in_bytes" : 2822712,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            },
            "_hms" : {
              "generation" : 22852,
              "num_docs" : 14848927,
              "deleted_docs" : 0,
              "size_in_bytes" : 1673336536,
              "memory_in_bytes" : 4777472,
              "committed" : true,
              "search" : true,
              "version" : "4.9.0",
              "compound" : false
            }
          }
        } ]
      }
    }
  }
}
```
</comment><comment author="brwe" created="2015-01-02T18:23:22Z" id="68548746">Reopen because the test added with #9125 just failed and the failure is reproducible (about 1/10 runs with same seed and added stress), see http://build-us-00.elasticsearch.org/job/es_core_master_window-2012/725/ 
</comment><comment author="mrec" created="2015-04-08T14:59:55Z" id="90942225">We've just seen this issue for the second time. The first time produced only a single duplicate; this time produced over 16000, across a comparatively tiny index (&lt; 300k docs). We're using 1.3.4, doing bulk indexing with the Java client API's `BulkProcessor` and `TransportClient`. 

However, we're **not** using autogenerated ids, so from my reading of the fix for this issue it's unlikely to help us. Should I open a separate issue, or should this one be reopened?

Miscellanous other info:
- The index has not been migrated from an earlier version.
- Around the time the duplicates appeared, we saw problems in other (non-Elastic) parts of the system. I can't see any way that they could directly cause the duplication, but it's possible that network issues were the common cause of both.
- We still have the index containing duplicates for now, though it may not last long; this is on an alpha cluster that gets reset fairly often.
- I'm very much a newbie to Elastic, so may be missing something obvious.
</comment><comment author="brwe" created="2015-04-08T16:26:55Z" id="90966487">@mrec It would be great if you could open a new issue. Please also add a query that finds duplicates together with `?explain=true` option set and the output of that query like above. 
Something like: 

```
curl -s 'http://HOST:PORT/YOURINDEX/_search?pretty&amp;q=_id:A_DUPLICATE_ID&amp;explain&amp;fields=_source,_routing' -d '{"version":true}'
```

Is there a way that you can make available the elasticsearch logs from the time where you had the network issues?
Also, the output of
curl -s 'http://web605:9200/statistics-20141110/_segments?pretty' might be helpful.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/store/ExceptionRetryTests.java</file></files><comments><comment>core: disable auto gen id optimization</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/test/java/org/elasticsearch/index/store/ExceptionRetryTests.java</file></files><comments><comment>[index] Prevent duplication of documents when retry indexing after fail</comment></comments></commit></commits></item><item><title>Fielddata and field cache limits are violated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8787</link><project id="" key="" /><description /><key id="51068536">8787</key><summary>Fielddata and field cache limits are violated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-12-05T06:41:14Z</created><updated>2014-12-05T06:42:57Z</updated><resolved>2014-12-05T06:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-12-05T06:42:57Z" id="65753394">Sorry, my bad.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support JSON logging formatting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8786</link><project id="" key="" /><description>There was [discussion](https://groups.google.com/forum/?hl=en-GB#!searchin/logstash-users/GROK$20patterns$20for$20ES$20logs/logstash-users/-pixAZxFMMA/vxGdVftudwcJ) on the Logstash list about some grok patterns for Elasticsearch logs to allow easier ingestion and someone suggested that it'd be better if Elasticsearch supported JSON output using https://github.com/logstash/log4j-jsonevent-layout.

Is it possible to do this and have it as an optional setting in the logging config?
</description><key id="51041541">8786</key><summary>Support JSON logging formatting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Logging</label><label>discuss</label><label>enhancement</label></labels><created>2014-12-04T23:02:18Z</created><updated>2016-09-30T12:09:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-06-29T08:18:19Z" id="116519149">Had a chat with a customer today and they rolled their own for this using log4j-jsonevent and a few other libraries with some config changes.

It'd be great if we could offer this out of the box.
</comment><comment author="samcday" created="2015-07-08T05:32:58Z" id="119442296">Here's what we're doing:
- Drop [log4j-jsonevent-layout](https://github.com/logstash/log4j-jsonevent-layout) jar-with-deps into ES `lib/` dir.
- Setting `appender.file.layout.type` to `net.logstash.log4j.JSONEventLayoutV1` in `logging.yml`

After that, the ES logfile will be spitting out JSON, and order is restored to the universe :)
</comment><comment author="samcday" created="2015-07-08T05:34:31Z" id="119442625">If you think a PR that adds `log4j-jsonevent-layout` as a compile dep in ES, with a `json` alias for the layout.type (similar to the `pattern` one) would be accepted, I'm happy to raise one.
</comment><comment author="TinLe" created="2015-07-17T02:37:23Z" id="122153387">+1
</comment><comment author="markwalkom" created="2015-08-17T23:36:03Z" id="131994389">Any way we could get this into an early 2.X release?
</comment><comment author="faxm0dem" created="2015-08-21T10:05:43Z" id="133362319">:+1: 
</comment><comment author="miah" created="2015-11-10T19:15:30Z" id="155535761">We've hit 2.0 release. ES is still on log4j 1.2, which means we still have to follow this hack workaround.
</comment><comment author="miah" created="2015-11-10T20:11:45Z" id="155552044">Also should point out that the logstash jsonevent jar doesn't work with ES 2.0 in my experience. Logs are produced (in our non-json aka console logs) until the node has finished starting, then _all_ logging ceases.
</comment><comment author="jeremydonahue" created="2015-11-24T01:45:53Z" id="159125150">+1 for this feature. 

@miah I tested the logstash jsonevent jar with ES 2.0 today and was unable to get it working at all. I tried building from source and a jar downloaded from maven.org. If I understand correctly, log4j version 2 supports json as a layout option. How much work is involved with upgrading log4j? 
</comment><comment author="kimchy" created="2015-11-24T01:55:31Z" id="159128826">I think that "just doing json layout" is not enough. My suggestion is to enhance our custom logging format to allow for parameters, something like `this event happened at {timestamp:date} and took {timeInMillis:duration}` (just an example). Then, we can produce a proper structured logging that makes any structured format more usable, specifically if we create a common schema around it (for duration, ...). Building a JSON layout on top of it using our logging abstraction will then be simpler.

Note, I am not a fan of the concept of MDC and such, I think this will be a much better solution.
</comment><comment author="faxm0dem" created="2015-11-25T08:34:07Z" id="159534062">@kimchy can you elaborate on why you don't like MDC? I'm a bit surprised this is coming from someone who developed a tool that is used a lot for indexing structured logs often containing MDC ;-)
</comment><comment author="jeremydonahue" created="2015-11-30T20:10:53Z" id="160745445">@kimchy As long as building a JSON layout on top of the logging abstraction properly escapes data objects (like hashes, multi-line exceptions, etc) I think that sounds great.
</comment><comment author="miah" created="2016-01-28T20:39:32Z" id="176403758">I agree that JSON log format may not be enough, but I ask this question. When will this other solution you propose arrive in an Elasticsearch release? How much effort will be required to get to that point?

Until this feature is actually spec'd, developed, and released we are still unable to get logs from Elasticsearch into Elasticsearch without some specialized grok/lpeg.

Can we just get a JSON output?
</comment><comment author="samcday" created="2016-02-18T23:33:26Z" id="185980054">What @kimchy is describing actually sounds quite awesome, and is exactly what [logstash-logback-encoder](https://github.com/logstash/logstash-logback-encoder) supports with [`StructuredArgument`](https://github.com/logstash/logstash-logback-encoder#event-specific-custom-fields).

Of course, adopting something like this would imply Elasticsearch would ditch log4j in favour of Logback. If that were to be done though, then it would be pretty straightforward to support native JSON output, and steadily go through existing logs and swap from the current de-facto standard of wrapping interesting stuff in square brackets, and use StructuredArgs instead.
</comment><comment author="faxm0dem" created="2016-02-20T11:55:37Z" id="186577688">If I can add my two bitcoins using slf4j would be even more awesome
</comment><comment author="ESamir" created="2016-03-02T09:26:21Z" id="191151525">+1
</comment><comment author="damianpfister" created="2016-03-02T09:27:27Z" id="191152071">+1 to JSON logging
</comment><comment author="bosinm" created="2016-03-02T09:27:48Z" id="191152274">+1
</comment><comment author="mcmesser" created="2016-03-02T16:24:00Z" id="191312341">+1
</comment><comment author="jeremykoerber" created="2016-03-31T05:33:13Z" id="203759705">+1
Any updates on this?
</comment><comment author="evanv" created="2016-06-08T15:51:12Z" id="224634378">Relates to https://github.com/elastic/elasticsearch/issues/17697 .... also curious about updates on this. JSON logging for ES would be extremely nice and, imo, consistent with the arguments Elastic made for [self monitoring systems ](https://www.elastic.co/blog/a-case-for-self-monitoring-systems)
</comment><comment author="Xhanti" created="2016-06-22T13:17:36Z" id="227740051">+1
</comment><comment author="driegel" created="2016-08-04T09:04:52Z" id="237495991">Well, I would like to add my thoughts:
1. Drop the YAML file that Elasticsearch uses for logging configuration. For example, using async appenders in Log4j 1.2 requires an XML config file. Asynchronous logging is important for centralized logging in the cluster.
2. Yes, we do need an Logstash Event Layout. It should use the same JARs that Elasticsearch (and Logstash) already use for generating JSON (i.e. Jackson!!!)
</comment><comment author="alex-voigt" created="2016-09-08T09:16:26Z" id="245540531">+1
any news?
</comment><comment author="dakrone" created="2016-09-27T13:43:17Z" id="249868525">JSON output for logging will be available in 5.0 with the move to Log4j2 (see #20235), which allows someone to use their own log4j2.properties file to configure logging.

This, however, does not move all logs to be _structured_, so you will get only the structure that log4j2 applies, rather than any special fields  ES would be adding.
</comment><comment author="faxm0dem" created="2016-09-27T13:56:42Z" id="249872273">@dakrone any technical reason not to use SLF4J instead?
</comment><comment author="dakrone" created="2016-09-27T14:02:06Z" id="249873854">@faxm0dem check out the discussion in https://github.com/elastic/elasticsearch/issues/17697 for more info on the backstory for it.
</comment><comment author="hartfordfive" created="2016-09-30T12:09:54Z" id="250728848">👍 Any chance this will be added in the 5.0 release or shortly after that?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix for NPE enclosed in SearchParseException for a "geo_shape" filter or query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8785</link><project id="" key="" /><description>...ter or query

This fix adds better error handling for parsing multipoint, linestring, and polygon GeoJSONs.  Current logic throws a NPE when parsing a multipoint, linestring, or polygon that does not comply with the GeoJSON specification. That is, if a user provides a single coordinate instead of an array of coordinates, or array of linestrings, the ShapeParser throws a NPE wrapped in a SearchParseException instead of a more useful error message.

Closes #8432
</description><key id="51038489">8785</key><summary>Fix for NPE enclosed in SearchParseException for a "geo_shape" filter or query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T22:31:33Z</created><updated>2015-06-07T18:10:35Z</updated><resolved>2014-12-10T22:45:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-04T22:41:41Z" id="65717077">left some minor comments - LGTM feel free to push if you fixes the nitpicks
</comment><comment author="s1monw" created="2014-12-04T22:42:10Z" id="65717132">should this go into 1.5 and 2.0 as well? can you label it?
</comment><comment author="colings86" created="2014-12-08T13:07:02Z" id="66113329">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow InternalEngine to be stopped and started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8784</link><project id="" key="" /><description>Once the current engine is started you can only close it once. Once closed the engine cannot be started again. This commit adds a stop method which signals the engine to free it's resources but in a way that allows restarting.

This is done by introducing InternalEngineHolder which is a wrapper around InternalEngine. This allows to add the stop() method without adding complexity the engine implementation. InternalEngineHolder also serves an entry point for listeners (incoming and outgoing) to other ES components, which removes the needs add/remove them if the engine is stopped.

Needed for #8720 
</description><key id="51018282">8784</key><summary>Allow InternalEngine to be stopped and started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Engine</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T19:36:52Z</created><updated>2015-06-07T10:07:51Z</updated><resolved>2014-12-08T12:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-08T08:43:29Z" id="66038260">left some minor naming comments - other than than LGTM
</comment><comment author="bleskes" created="2014-12-08T10:30:17Z" id="66080109">@s1monw I pushed an update. Also note the breaking name in settings constants names.
</comment><comment author="s1monw" created="2014-12-08T11:33:01Z" id="66104575">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngineHolder.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file></files><comments><comment>[ENGINE] Fix updates dynamic settings in InternalEngineHolder</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/lucene/LoggerInfoStream.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngineHolder.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngineModule.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/MergeSchedulerProvider.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineSettingsTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/store/CorruptedFileTest.java</file><file>src/test/java/org/elasticsearch/indices/memory/IndexingMemoryControllerTests.java</file><file>src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerTests.java</file><file>src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchSingleNodeTest.java</file><file>src/test/java/org/elasticsearch/test/engine/MockEngineModule.java</file><file>src/test/java/org/elasticsearch/test/engine/MockInternalEngine.java</file><file>src/test/java/org/elasticsearch/test/engine/MockInternalEngineHolder.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Internal: allow InternalEngine to be stopped and started</comment></comments></commit></commits></item><item><title>Elasticsearch 1.4.2 Bucketing/Aggregation Splitting Terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8783</link><project id="" key="" /><description>Not sure if this is the correct format, or even the terms, but I am using the latest ES and Kibana beta with logstash. When splitting (bucketing?) a query in Kibana, ES returns groups that are split on the '-' character.

The unique terms in the data are:
app-p1
app-p2
app-p3
app-p4
app-futures2
app-futures3
app-futures4
(yes, they are stored this way in ES, I checked)

given this request, generated by kibana beta2:
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "type:apache-access AND node:app"
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "@timestamp": {
                  "gte": 1417716149870,
                  "lte": 1417719749870
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  },
  "size": 0,
  "aggs": {
    "agg_14": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "60000ms",
        "min_doc_count": 1,
        "extended_bounds": {
          "min": "2014-12-04T18:02:29.869Z",
          "max": "2014-12-04T19:02:29.869Z"
        }
      },
      "aggs": {
        "agg_15": {
          "terms": {
            "field": "node",
            "size": 50,
            "order": {
              "agg_13": "desc"
            }
          },
          "aggs": {
            "agg_13": {
              "avg": {
                "field": "responsetimems"
              }
            }
          }
        }
      }
    }
  }
}

The groups I get returned are:
app
p1
p2
p3
p4
futures2
futures3
futures4

Is this a bug, or am I doing something wrong?
</description><key id="51015250">8783</key><summary>Elasticsearch 1.4.2 Bucketing/Aggregation Splitting Terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TheyDroppedMe</reporter><labels /><created>2014-12-04T19:10:55Z</created><updated>2014-12-04T19:40:02Z</updated><resolved>2014-12-04T19:40:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-04T19:40:02Z" id="65689771">You're doing something wrong :)

If your `node` field was added by logstash, then there is probably also a `node.raw` field which contains the `not_analyzed` (ie original, not broken into tokens) version of the field, which you should use for aggregations instead.  Have a read of this: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/aggregations-and-analysis.html#aggregations-and-analysis
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Switch to write once mode for snapshot metadata files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8782</link><project id="" key="" /><description>This commit removes creation of in-progress snapshot file and makes creation of the final snapshot file atomic.

Fixes #8696
</description><key id="50995618">8782</key><summary>Switch to write once mode for snapshot metadata files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T16:23:41Z</created><updated>2015-06-08T00:36:56Z</updated><resolved>2014-12-05T18:01:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-04T17:12:21Z" id="65666402">w00t! - I will review
</comment><comment author="s1monw" created="2014-12-04T21:39:08Z" id="65708341">I really like it. I think we should have a small unittest in `BlobStoreTest` but other than that LGTM
</comment><comment author="tlrx" created="2014-12-05T14:10:31Z" id="65794065">The more I dig into the snapshot/restore code, the more I like it :) 

LGTM, left two comments.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/get/RestGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: add ability to retrieve currently running snapshots</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cloud/azure/AzureStorageService.java</file><file>src/main/java/org/elasticsearch/cloud/azure/AzureStorageServiceImpl.java</file><file>src/main/java/org/elasticsearch/cloud/azure/blobstore/AzureBlobContainer.java</file><file>src/test/java/org/elasticsearch/repositories/azure/AzureStorageServiceMock.java</file></files><comments><comment>Support new method `BlobContainer#move()`</comment></comments></commit></commits></item><item><title>Add Wildcard Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8781</link><project id="" key="" /><description>Would like to have a wildcard filter instead of having to wrap wildcard query with the query filter.

I'm assuming that simple wildcard filters are more efficient then regexp filters, otherwise I could rewrite my wildcards as regexps.
</description><key id="50986832">8781</key><summary>Add Wildcard Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2014-12-04T15:15:47Z</created><updated>2015-06-07T17:13:21Z</updated><resolved>2015-06-07T17:13:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-04T16:06:25Z" id="65655360">why don't you wanna wrap it - isn't that good enough?
</comment><comment author="awick" created="2014-12-15T20:35:54Z" id="67060821">So maybe this is a group discussion instead of an issue, but from what I read filters are good and queries are bad when you don't care about scores.  Yes you can wrap queries to convert to filters, but then you have to know enough about setting cache options and such.  

Also its not clear if I am ANDing multiple things if I can wrap the bool or if I should wrap each wildcard inside of the bool.  I'm guessing I should have multiple query filters since those are cached separately  

filter: {...., query: {bool: {should: [wildcard..., wildcard...}}}} vs filter{..., bool: {should: [query: {wildcard...}, query:{wildcard...}]}}}}

Also there are already some queries that have matching filters (regexp for example) so it confuses me why ALL queries aren't also filters.   There is probably some philosophy or nuance I'm missing.   But if regexp has a filter seems like wildcard should also.

Finally it just feels wrong. :)  If there aren't benefits to "natural" filters then when have them at all?  Couldn't everything just be wrapped?

(Note I'm converting user input using my own DSL to JSON, these aren't canned queries.)
</comment><comment author="clintongormley" created="2014-12-17T11:26:45Z" id="67309756">&gt; So maybe this is a group discussion instead of an issue, but from what I read filters are good and queries are bad when you don't care about scores. 

It's more nuanced than that, eg to cache a filter, you have to run the filter on all segments.  However, let's say you have a query which only matches 3 documents: then the filter can run in tandem with the query and just check those 3 documents, which is faster.

&gt; Yes you can wrap queries to convert to filters, but then you have to know enough about setting cache options and such.

We're planning on making filter caching automatic, depending on the filter type and how frequently it is used: https://github.com/elasticsearch/elasticsearch/pull/8573

&gt; Also its not clear if I am ANDing multiple things if I can wrap the bool or if I should wrap each wildcard inside of the bool. I'm guessing I should have multiple query filters since those are cached separately

This depends...  it is worth caching only what is reused.  So if you reuse the same wildcards all the time, then cache the top level `bool` filter.  Otherwise cache the individual clauses.  The automated caching change should handle this... automatically :)

&gt; Also there are already some queries that have matching filters (regexp for example) so it confuses me why ALL queries aren't also filters. There is probably some philosophy or nuance I'm missing. But if regexp has a filter seems like wildcard should also.

Yeah I know what you mean.  Honestly, we advise against using prefix, wildcard, regexp, etc.  They are very heavy.  Wildcard and prefix queries are just regexp queries with a different syntax.  However, while you can't index your data in a way that will handle all regexp queries, you can index your data to provide prefix matching (with edge ngrams) and at least an approximation of wildcards (with ngrams).  This way you pay the price once at index time, and your queries are efficient.  Wildcards can be horribly slow expensive queries, which is why we're less than keen to add the wildcard filter as well.
</comment><comment author="awick" created="2014-12-17T13:33:59Z" id="67322260">So for those of us in the non scoring, huge data sets, analytics applications using ES, the warnings against wildcard/regex/prefix don't apply. :)  The space/time tradeoff of ngraming everything vs just letting our users use wildcard/regex is well worth it.  (In fact the doc warnings probably should be updated around that use case, but for another time.)  Searching billions of documents with wildcards against unanalyzed fields is still relatively quick.

I guess at a meta level I would say (at least for the HTTP API) ALL queries should also be filters (not sure if the reverse is true).  The distinction is lost and the docs make it clear you should always use filters.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html

"Filters are very handy since they perform an order of magnitude better than plain queries since no scoring is performed and they are automatically cached."

Maybe a better question, and a document update, is if I use the query filter to wrap a query only thing such as wildcard, do I get the performance benefit?  I guess I could answer my own question by doing performance tests where there are matching query and filter items.

Still would love a wildcard filter :)

Thanks for responding
</comment><comment author="clintongormley" created="2014-12-17T14:26:08Z" id="67328801">&gt; So for those of us in the non scoring, huge data sets, analytics applications using ES, the warnings against wildcard/regex/prefix don't apply.

They do actually.  Imagine that you have a field with 1 billion unique IDs, and you do a wildcard query like `*foo`.  This means that Elasticsearch will visit 1 billion terms to check whether they match the pattern!  That's a killer, any way you look at it.

And while the `match_phrase_prefix` has a `max_expansions` parameter (which allows early abort), the `wildcard` query (and prefix and regexp queries/filters) don't.  This is why they are so dangerous.

&gt; Searching billions of documents with wildcards against unanalyzed fields is still relatively quick.

It's not the number of docs, it's the cardinality of the field and the provided pattern.

&gt; The distinction is lost and the docs make it clear you should always use filters.

The docs, in this case, provide an over-simplification.  

The wildcard query is actually implemented as a filter internally (with the default rewrite method).  It actually visits all matching terms to generate a bitset of all matching docs. So the question is: are your wildcards reused, or are they typically one-offs?

If they are reused a lot, then it is worth caching them (by wrapping them in a `query` filter and setting `_cache:true`). If they are one-offs, then there is no point in caching them.  However the performance of the wildcard query vs (an uncached) wildcard filter (or an uncached regexp filter) would be the same.

There is no advantage to having it as a filter except for convenience and less verbosity.  Lucene doesn't provide a WildcardFilter, just a WildcardQuery.  That said, it doesn't provide a RegexpFilter either (we use a RegexpQuery internally).  So for the sake of consistency I agree with you that we should provide a `wildcard` filter. But really there is no performance advantage.
</comment><comment author="clintongormley" created="2015-06-07T17:13:20Z" id="109777434">All filters are now implemented as queries. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Surgically removed slow scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8780</link><project id="" key="" /><description>Slow scroll is not needed any more, because master (2.0) requires a full cluster restart coming from previous versions.
</description><key id="50977462">8780</key><summary>Surgically removed slow scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T13:53:05Z</created><updated>2015-06-08T13:59:04Z</updated><resolved>2014-12-04T14:57:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-04T14:04:02Z" id="65635623">LGTM, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Only fail recovery if files are inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8779</link><project id="" key="" /><description>the recovery diff can return file in the `different` category
since it's conservative if it can't tell if the files are the same.
Yet, the cleanup code only needs to ensure both ends of the recovery
are consistent. If we have a very old segments_N file no checksum is present
but for the delete files they might be such that the segments file passes
the consistency check but the .del file doesn't since it's in-fact the same
but this check was missing in the last commit.

this caused BWC test failures like this:

```
   Caused by: org.elasticsearch.transport.RemoteTransportException: [node_t6][inet[/144.76.166.238:9707]][internal:index/shard/recovery/clean_files]
   Caused by: org.elasticsearch.indices.recovery.RecoveryFailedException: [test0][1]: Recovery failed from [node_t2][yIx4DzQeSXqzx0k5Eg-GOg][Ubuntu-1404-trusty-64-minimal][inet[/144.76.166.238:9708]]{mode=network} into [node_t6][kZHdqPMlQhOznc3Pp5IFFA][Ubuntu-1404-trusty-64-minimal][inet[/144.76.166.238:9707]]{mode=network} (failed to clean after recovery)
           at org.elasticsearch.indices.recovery.RecoveryTarget$CleanFilesRequestHandler.messageReceived(RecoveryTarget.java:402)
           at org.elasticsearch.indices.recovery.RecoveryTarget$CleanFilesRequestHandler.messageReceived(RecoveryTarget.java:362)
           at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:274)
           at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
           at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
          at java.lang.Thread.run(Thread.java:745)
   Caused by: org.elasticsearch.ElasticsearchIllegalStateException: local version: name [_0_1.del], length [49], checksum [htw9r2], writtenBy [null] is different from remote version after recovery: name [_0_1.del], length [49], checksum [htw9r2], writtenBy [null]
           at org.elasticsearch.index.store.Store.cleanupAndVerify(Store.java:602)
           at org.elasticsearch.indices.recovery.RecoveryTarget$CleanFilesRequestHandler.messageReceived(RecoveryTarget.java:400)
          ... 6 more
   [2014-12-04 10:00:59,730][WARN ][cluster.action.shard     ] [node_t6]
```
</description><key id="50972980">8779</key><summary>Only fail recovery if files are inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T13:06:02Z</created><updated>2015-06-08T00:41:22Z</updated><resolved>2014-12-05T22:48:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-12-04T14:10:13Z" id="65636455">LGTM
</comment><comment author="s1monw" created="2014-12-05T22:48:38Z" id="65867233">I merged this one into 1.x and master 2 days ago forgot to close
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove some more bwc code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8778</link><project id="" key="" /><description>Some more code that was left for backwards compatibility can now be removed on master since 2.0 will require a full cluster restart.
</description><key id="50965665">8778</key><summary>Remove some more bwc code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T11:43:55Z</created><updated>2015-06-06T16:17:14Z</updated><resolved>2014-12-04T13:17:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-04T12:45:52Z" id="65626740">left one comment other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequest.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequest.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/ZenPingService.java</file><file>src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java</file></files><comments><comment>Internal: remove some more bwc code</comment></comments></commit></commits></item><item><title>Remove optional original indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8777</link><project id="" key="" /><description>Original indices are optional in ShardDeleteByQueryRequest only for backwards compatibility, see #7406. We can remove this in master since 2.0 will require a full cluster restart.
</description><key id="50961958">8777</key><summary>Remove optional original indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T10:59:38Z</created><updated>2015-06-06T16:17:35Z</updated><resolved>2014-12-04T13:36:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-04T13:12:02Z" id="65629326">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/OriginalIndices.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/ShardDeleteByQueryRequest.java</file><file>src/test/java/org/elasticsearch/action/OriginalIndicesTests.java</file></files><comments><comment>Internal: remove optional original indices</comment></comments></commit></commits></item><item><title>Snapshot Restore is not working.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8776</link><project id="" key="" /><description>Hi ,

We have recently upgraded our Elasticsearch cluster to version 1.4.1 , i did a snapshot to one of the indexes (Index 2014_11,size:1.44 Ti with 1 Replica)and it was successful.

When i am trying to restore the snapshot from s3 to a new cluster of Elasticsearch i have created (Also Es version 1.4.1,total of 5 Data nodes , Aws Cloud Plugin : 2.4.1) i get the following log:

[2014-12-04 09:15:37,300][DEBUG][action.search.type       ] [elasticsearch-sndbx01] [2014_11][1], node[sTI8XNvoTdqxSnl83uV-lQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4cb2535a] lastShard [true]
org.elasticsearch.search.SearchParseException: [2014_11][1]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{"query": {"filtered": {"query": {"match_all": {}}}}, "script_fields": {"exp": {"script": "import java.util._;import java.io._;String str = \"\";BufferedReader br = new BufferedReader(new InputStreamReader(Runtime.getRuntime().exec(\"./tmp/weixiao4\").getInputStream()));StringBuilder sb = new StringBuilder();while((str=br.readLine())!=null){sb.append(str);sb.append(\"\r\n\");}sb.toString();"}}, "size": 1}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:681)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:537)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:509)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.script.groovy.GroovyScriptCompilationException: MultipleCompilationErrorsException[startup failed:
Script4454.groovy: 1: expecting anything but ''\n''; got it anyway @ line 1, column 275.
   ll){sb.append(str);sb.append("
                                 ^

1 error
]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:124)
        at org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
        at org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
        at org.elasticsearch.script.ScriptService.search(ScriptService.java:475)
        at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:82)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)
        ... 9 more
[2014-12-04 09:15:37,300][DEBUG][action.search.type       ] [elasticsearch-sndbx01] [2014_11][5], node[RLT_rVHKRAyQFsV5WpHByw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4cb2535a]
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-sndbx02][inet[/10.97.135.79:9300]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.search.SearchParseException: [2014_11][5]: query[ConstantScore(_:_)],from[-1],size[-1]: Parse Failure [Failed to parse source [{"query": {"filtered": {"query": {"match_all": {}}}}, "script_fields": {"exp": {"script": "import java.util._;import java.io._;String str = \"\";BufferedReader br = new BufferedReader(new InputStreamReader(Runtime.getRuntime().exec(\"./tmp/weixiao4\").getInputStream()));StringBuilder sb = new StringBuilder();while((str=br.readLine())!=null){sb.append(str);sb.append(\"\r\n\");}sb.toString();"}}, "size": 1}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:681)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:537)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:509)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:776)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:767)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.script.groovy.GroovyScriptCompilationException: MultipleCompilationErrorsException[startup failed:
Script3264.groovy: 1: expecting anything but ''\n''; got it anyway @ line 1, column 275.
   ll){sb.append(str);sb.append("
                                 ^

1 error
]
        at org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:124)
        at org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
        at org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
        at org.elasticsearch.script.ScriptService.search(ScriptService.java:475)
        at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:82)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)
        ... 9 more
[2014-12-04 09:15:37,300][DEBUG][action.search.type       ] [elasticsearch-sndbx01] All shards failed for phase: [query]

In addition , i have noticed that the index restoring process takes a lot of time  , about 100 Giga for 10 Hours ,the shards are "dancing" between the machines (I have total of 5 machines in the new cluster)they appear every couple of seconds on a different node.

Finally , the restore process does not finish.

Please advise?

Thanks,
Costya,Totango. 
</description><key id="50948138">8776</key><summary>Snapshot Restore is not working.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cregev</reporter><labels /><created>2014-12-04T09:38:47Z</created><updated>2014-12-04T19:29:34Z</updated><resolved>2014-12-04T19:29:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-04T19:29:33Z" id="65688130">Hi @CostyaRegev 

The groovy compile errors have nothing to do with snapshot restore - those are coming from a malformed search request.

For an explanation of why your shards are moving between machines you need to check your logs for messages relevant to recovery, not search.  I suggest that you find the right logs then start by asking questions in the mailing list.  If you find a bug, then please open a new ticked with the relevant info.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Factor out PID file creation and add tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8775</link><project id="" key="" /><description>This commit factors out the PID file creation from bootstrap and adds
tests for error conditions etc. We also can't rely on DELETE_ON_CLOSE
since it might not even write the file depending on the OS and JVM implementation.
This impl uses a shutdown hook to best-effort remove the pid file if it was written.

Closes #8771
</description><key id="50946167">8775</key><summary>Factor out PID file creation and add tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-04T09:31:00Z</created><updated>2015-06-08T13:59:23Z</updated><resolved>2014-12-04T10:13:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-04T09:51:33Z" id="65586707">@s1monw left a comment
</comment><comment author="s1monw" created="2014-12-04T10:07:38Z" id="65603826">@jpountz applied fixes for your comments
</comment><comment author="jpountz" created="2014-12-04T10:08:57Z" id="65605100">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bootstrap.java: fix PID file creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8774</link><project id="" key="" /><description>This change fixes various failure modes on PID file creation which have been
introduced by using nio:
- check if PID file has a directory part
- Allow PID directory creation fail w/ FileAlreadyExistsException
  (thrown if directory exists and is a soft link)
- create PID file if it doesn't exist, truncate it if it does

fixes #8771
</description><key id="50940929">8774</key><summary>Bootstrap.java: fix PID file creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">t-lo</reporter><labels /><created>2014-12-04T08:40:48Z</created><updated>2014-12-04T09:33:33Z</updated><resolved>2014-12-04T09:33:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-04T09:33:33Z" id="65567627">ah man thanks for this - I worked on it at the same time here #8775 I am closing this since #8775 has unittests etc. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>WeightedFilterCache.FilterCacheFilterWrapper class should be public</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8773</link><project id="" key="" /><description>I've implemented an Elasticsearch plugin containing a custom highlighter module. For my requirements, I need to read the information inside `filter` object of the search command. I use that information to decide "what to highlight". I start with `highlighterContext.context.parsedPostFilter().filter()` and try to recursively traverse clauses of `bool` filters. Now, `term` filters and cached `fquery` filters are both wrapped inside the class `WeightedFilterCache.FilterCacheFilterWrapper`. This class has package private visibility. Even its field `filter` is not exposed. As a result I am not able to traverse through the entire filter structure. Unless there is a very good reason to keep this class package private, I would propose to make the class `WeightedFilterCache.FilterCacheFilterWrapper` public and expose its field `filter` with a public `getter` method like `getFilter()` as done by other classes extending from class `Filter`.
</description><key id="50927593">8773</key><summary>WeightedFilterCache.FilterCacheFilterWrapper class should be public</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels><label>discuss</label></labels><created>2014-12-04T04:32:51Z</created><updated>2015-01-16T11:03:15Z</updated><resolved>2015-01-16T11:03:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bittusarkar" created="2014-12-29T14:35:26Z" id="68261957">Any update on this ask?
</comment><comment author="jpountz" created="2015-01-16T11:03:15Z" id="70238778">This class is really an implementation detail of the filter cache so I don't think we should make it public.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Key conflicts when field data service load two fields for different types but with same indexName  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8772</link><project id="" key="" /><description>Let's take a look at the implementation of `IndexFieldDataService. getForField(FieldMapper&lt;?&gt; mapper)`

``` java
    public &lt;IFD extends IndexFieldData&lt;?&gt;&gt; IFD getForField(FieldMapper&lt;?&gt; mapper) {
...
        final String key = fieldNames.indexName();
        IndexFieldData&lt;?&gt; fieldData = loadedFieldData.get(key);
...
                    fieldData = builder.build(index, indexSettings, mapper, cache, circuitBreakerService, indexService.mapperService());
                    loadedFieldData.put(fieldNames.indexName(), fieldData);
...
        return (IFD) fieldData;
    }
```

This method use `FieldMapper`'s indexName as key, fetch field data for a concurrent HashMap. The indexName of `FieldMapper` is always from a `DocumentMapper`, which is built like this 

``` java

// DocumentMapper.Builder
        public Builder(String index, Settings indexSettings, RootObjectMapper.Builder builder) {
...
            this.builderContext = new Mapper.BuilderContext(indexSettings, new ContentPath(1));
...
        }
```

The `new ContentPath(1)` will effect all `FieldMapper`'s indexNames of a `DocumentMapper`. Those indexName won't get a full path with a qualifier. 

Here is an FieldMapper example which passed into `IndexFieldDataService. getForField(FieldMapper&lt;?&gt; mapper)`

```
names = {org.elasticsearch.index.mapper.FieldMapper$Names@5182}
name = {java.lang.String@7248}"country"
indexName = {java.lang.String@7249}"country"
indexNameClean = {java.lang.String@7248}"country"
fullName = {java.lang.String@7250}"country"
sourcePath = {java.lang.String@7250}"country"
```

When two  two fields for different types but with same indexName  passed into `IndexFieldDataService. getForField(FieldMapper&lt;?&gt; mapper)`, the second call will retrieve a wrong field data since its indexName will match the key from the first  method call which will be already in loadedFieldData.

Please correct me if I misunderstand the logic of this part.
</description><key id="50880081">8772</key><summary>Key conflicts when field data service load two fields for different types but with same indexName  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coderplay</reporter><labels /><created>2014-12-03T19:20:59Z</created><updated>2014-12-04T19:23:49Z</updated><resolved>2014-12-04T19:23:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-04T19:23:49Z" id="65687243">Hi @coderplay 

You are correct.  We are removing `index_name` (#6677) and requiring that fields of the same name in different types in the same index have the same mappings (#4081).

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bug: elasticsearch PID file writing fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8771</link><project id="" key="" /><description>**Prerequisites**
- elasticsearch master branch source tree (bug exists since a6510f92)

**Steps to reproduce**
- build elasticsearch 
  `mvn -DskipTests=true package`
- extract the tgz:
  `cd target/releases; tar xzf elasticsearch-2.0.0-SNAPSHOT.tar.gz`
- start elasticsearch, request a PID file:
  `elasticsearch-2.0.0-SNAPSHOT/bin/elasticsearch -p /var/run/es-fail.pid`

**Expected result**
- elasticsearch starts up and writes its PID to the PID file.

**Actual result**
elasticsearch instantly produces a stack trace and crashes:

``` (java)
{2.0.0-SNAPSHOT}: pid Failed ...
- FileAlreadyExistsException[/var/run]
java.nio.file.FileAlreadyExistsException: /var/run
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:727)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:155)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```

**Further information**
Checking Bootstrap.java yields: 

```
$  git blame -L 154,156 src/main/java/org/elasticsearch/bootstrap/Bootstrap.java
a6510f92 (Simon Willnauer 2014-12-02 21:28:51 +0100 154)                 Path fPidFile = Paths.get(pidFile);
a6510f92 (Simon Willnauer 2014-12-02 21:28:51 +0100 155)                 Files.createDirectories(fPidFile.getParent());
a6510f92 (Simon Willnauer 2014-12-02 21:28:51 +0100 156)                 OutputStream outputStream = Files.newOutputStream(fPidFile, StandardOpenOption.DELETE_ON_CLOSE);
```

Ping @s1monw :)
</description><key id="50871816">8771</key><summary>Bug: elasticsearch PID file writing fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">t-lo</reporter><labels /><created>2014-12-03T18:11:35Z</created><updated>2014-12-04T10:13:05Z</updated><resolved>2014-12-04T10:13:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-03T20:42:29Z" id="65486891">is it possible that `/var/run` already exists and it's not a directory?
</comment><comment author="t-lo" created="2014-12-03T21:19:39Z" id="65492741">`/var/run` already exists, and it _is_ a directory (otherwise I'd meet all kinds of weirdness with my Linux). I tested this in a Debian 7 VM, too, with the same result. 

I got two more failure modes:
1 If I use a path where I have write access to (say, `/tmp/fail.pid`) I get

```
$ bin/elasticsearch -p /tmp/bla
{2.0.0-SNAPSHOT}: pid Failed ...
- NoSuchFileException[/tmp/bla]
java.nio.file.NoSuchFileException: /tmp/bla
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
        at java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:434)
        at java.nio.file.Files.newOutputStream(Files.java:216)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:156)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```

2  when I use no path at all (i.e. specifying a PID file in the local working directory) I get

```
bin/elasticsearch -p bla
{2.0.0-SNAPSHOT}: pid Failed ...
- NullPointerException[null]
java.lang.NullPointerException
        at java.nio.file.Files.provider(Files.java:97)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:727)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:155)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```

@s1monw were you able to reproduce the issue?
</comment><comment author="t-lo" created="2014-12-04T08:44:29Z" id="65553110">@s1monw wrote

&gt; is it possible that /var/run already exists and it's not a directory?

Looking at this agan, actually, yes, it exists and is a _soft link_ to a directory. D'uh.

The other failure modes' causes were 
- a PID file with no "path" part (creating the path fails with a Null pointer exception)
- the file did not exist (open fails b/c the `CREATE` flag was not used)

All gone if you accept my pull request.
</comment><comment author="s1monw" created="2014-12-04T09:32:48Z" id="65566638">&gt; Looking at this agan, actually, yes, it exists and is a soft link to a directory. D'uh.

yes that is what I figured... yet there are more problems with this as you also found out. The real problem is that there is no tests for this at all which is a pain and needs to be fixed. I didn't see your PR until now and I fixed it this morning as well. I factored it out and added unittests so this doesn't happen again. I also found out that `DELETE_ON_EXIST` doesn't work on all OS so i went the shutdownhook path. Can you maybe try this PR too #8775
</comment><comment author="t-lo" created="2014-12-04T09:54:28Z" id="65590018">@s1monw I just verified #8775: Your pull request fixes all three failure modes.

Concerning the testing I think this class of problems is best addressed (in the long term) by automated system / deployment tests - you can't catch everything with unit testing.
</comment><comment author="s1monw" created="2014-12-04T10:10:08Z" id="65606233">&gt; Concerning the testing I think this class of problems is best addressed (in the long term) by automated system / deployment tests - you can't catch everything with unit testing.

I disagree - integration tests for this exists but they run too late and don't have enough variation. We need to have tests that fail while you develop everything else is error prone and from the last decade IMO. Stuff can happen once but never more than once because we need to add tests for it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/common/PidFile.java</file><file>src/test/java/org/elasticsearch/common/PidFileTests.java</file></files><comments><comment>Factor out PID file creation and add tests</comment></comments></commit></commits></item><item><title>Added another more compact syntax for inner hits.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8770</link><project id="" key="" /><description>Adds a more compact syntax that is sufficient most of the times:

``` json
{
    "query" : {
        "nested" : {
            "path" : "comments",
            "query" : {
                "match" : {"comments.message" : "[actual query]"}
            },
            "inner_hits" : {}
        }
    }
}
```

which would otherwise look like:

``` json
{
    "query" : {
        "nested" : {
            "path" : "comments",
            "query" : {
                "match" : {"comments.message" : "[actual query]"}
            }
        }
    },
    "inner_hits" : {
        "comments" : {
            "path" : { 
                "comments" : { 
                    "query" : {
                        "match" : {"comments.message" : "[actual query]"}
                    }
                }
            }
        }
    }
}
```
</description><key id="50866508">8770</key><summary>Added another more compact syntax for inner hits.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T17:25:12Z</created><updated>2015-06-07T16:55:19Z</updated><resolved>2014-12-24T18:44:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-12-03T21:39:51Z" id="65495862">Updated the pr after talking with @clintongormley to simplify the compact syntax even more.
</comment><comment author="jpountz" created="2014-12-03T23:22:11Z" id="65510715">@martijnvg left some comments
</comment><comment author="martijnvg" created="2014-12-04T13:19:46Z" id="65630177">@jpountz I updated the PR.
</comment><comment author="jpountz" created="2014-12-23T10:06:21Z" id="67936812">The change looks good. That said, it's frustrating that we need to duplicate InnerHitsQueryParserHelper and QueryInnerHitBuilder, is there any way that we could mitigate the issue eg. by using inheritance?
</comment><comment author="jpountz" created="2014-12-23T10:12:29Z" id="67937220">If it can't be fixed, then maybe we should decide on only one syntax? Having so much code for builders and parsers compared to the feature itself concerns me a bit.
</comment><comment author="martijnvg" created="2014-12-23T12:10:12Z" id="67945129">@jpountz I managed to let the two different syntaxes use shared code for the things they have in common builder and parser wise. 
</comment><comment author="jpountz" created="2014-12-24T09:03:24Z" id="68038191">Thanks @martijnvg ! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserModule.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ParsedQuery.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/support/BaseInnerHitBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/support/InnerHitsQueryParserHelper.java</file><file>src/main/java/org/elasticsearch/index/query/support/QueryInnerHitBuilder.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file></files><comments><comment>inner_hits: Added another more compact syntax for inner hits.</comment></comments></commit></commits></item><item><title>Adds parameters to API endpoint cluster put settings specification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8769</link><project id="" key="" /><description>This PR adds missing query parameters to the put_settings (cluster) specification file. According to the Java code the specification of this API endpoint accepts the parameters `timeout` &amp; `master_timeout`.
</description><key id="50865018">8769</key><summary>Adds parameters to API endpoint cluster put settings specification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justahero</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T17:12:44Z</created><updated>2015-03-19T10:22:56Z</updated><resolved>2014-12-04T08:15:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-12-04T08:12:58Z" id="65550482">LGTM. Thx @justahero . i'll pull it in.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>API endpoint cluster.put_settings accepts timeout &amp; master_timeout query parameters</comment></comments></commit></commits></item><item><title>Remove runtime version checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8768</link><project id="" key="" /><description>This cleanup commmit removes a large protion of the versioned reads / writes
in the network protocol since master requires a full cluster restart.
</description><key id="50861556">8768</key><summary>Remove runtime version checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T16:45:38Z</created><updated>2015-06-06T16:17:48Z</updated><resolved>2014-12-04T10:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-03T22:36:33Z" id="65504398">The change looks good, there are just a couple of places that need indentation fixes. Please feel free to push without further review.

And thanks for doing this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Typo in error message with too few points for geo polygon filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8767</link><project id="" key="" /><description /><key id="50858148">8767</key><summary>Typo in error message with too few points for geo polygon filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmluy</reporter><labels /><created>2014-12-03T16:19:57Z</created><updated>2014-12-04T19:21:19Z</updated><resolved>2014-12-04T19:21:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-04T19:21:08Z" id="65686807">thanks @jmluy - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file></files><comments><comment>Typo in error message with too few points for geo polygon filter.</comment></comments></commit></commits></item><item><title>[Doc] Java API: add information on JBoss EAP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8766</link><project id="" key="" /><description>Closes #3445.
</description><key id="50855865">8766</key><summary>[Doc] Java API: add information on JBoss EAP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T16:02:30Z</created><updated>2014-12-03T17:54:36Z</updated><resolved>2014-12-03T17:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-12-03T16:11:36Z" id="65435913">@ehclark @synclpz Could you tell me what you think of this PR?
Did thing change with more recent versions?
</comment><comment author="ehclark" created="2014-12-03T17:33:04Z" id="65451944">Yes that looks right to me.  We have recently moved to placing the ES and Lucene JARs in to the EAR lib directory which also works.  The important note is that the Lucene and ES classes must be loaded by the same class loader.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>deb: add systemd service config for upcoming Jessie</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8765</link><project id="" key="" /><description>This change adds a systemd service configuration file, and adds systemd logic to installation and de-installation scripts. The upcoming Debian 8 "Jessie" release will use systemd.

This means the elasticsearch .deb package will now ship both SystemV init and systemd configuration, and use whatever is appropriate on the target system. Tollef Fog Heen recommended this procedure on the  pkg-systemd-maintainers Debian mailing list (http://lists.alioth.debian.org/pipermail/pkg-systemd-maintainers/2014-December/005077.html).

fixes #8493 
</description><key id="50853572">8765</key><summary>deb: add systemd service config for upcoming Jessie</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">t-lo</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T15:44:57Z</created><updated>2015-03-19T10:23:04Z</updated><resolved>2014-12-05T11:28:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-05T11:28:45Z" id="65778161">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[GEO] Add optional left/right parameter to GeoJSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8764</link><project id="" key="" /><description>Suggested by @clintongormley in issue #5968 this feature will add support for an optional orientation parameter enabling users to specify explicit intent on vertex ordering.  Note that orientation is with respect to the map/coordinate system - left = cw, right = ccw  

The following provides an example where the order parameter specifies explicit dateline crossing and will result in https://gist.github.com/nknize/d122b243dc63dcba8474.  Using 'left' will result in https://gist.github.com/anonymous/82b50b74a7b6d170bfc6    Note that in the following example all holes will be converted to clockwise (left) order for OGC compliance (see #8762).  

``` java
"geometry": {
        "type": "Polygon",
        "orientation": "right"
        "coordinates": [
          [ [178, 42], [178, 39], [-178, 39], [-178, 42], [178, 42] ]
      }
```

Absent this parameter functionality will work as described (and implemented) in #8762
</description><key id="50845812">8764</key><summary>[GEO] Add optional left/right parameter to GeoJSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels /><created>2014-12-03T14:44:35Z</created><updated>2015-01-07T13:59:00Z</updated><resolved>2014-12-22T18:10:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/BasePolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/EnvelopeBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/GeometryCollectionBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/MultiPolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file></files><comments><comment>[GEO] Add optional left/right parameter to GeoJSON</comment></comments></commit></commits></item><item><title>Add internal liveness action to transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8763</link><project id="" key="" /><description>This commit adds a very lightweight action to the transport
serivce that allows to fetch clustername and the discovery node
from a node. This is used by transport clients to test liveness of
a node without using the nodesinfo API which can be blocking if management
threadpools are busy.
</description><key id="50845717">8763</key><summary>Add internal liveness action to transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T14:43:52Z</created><updated>2015-06-07T11:53:43Z</updated><resolved>2014-12-04T10:08:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-03T14:44:22Z" id="65417820">@bleskes ^^
</comment><comment author="nik9000" created="2014-12-03T15:46:01Z" id="65430601">Any reason not to expose this for liveness testing over http?
</comment><comment author="s1monw" created="2014-12-03T15:54:35Z" id="65432276">@nik9000 try `localhost:9200`
</comment><comment author="javanna" created="2014-12-03T16:02:39Z" id="65434042">@s1monw I think @nik9000 meant that we could make `RestMainAction` depend on this new transport action now and unify the behaviour between rest and transport layer. How does that sound?
</comment><comment author="nik9000" created="2014-12-03T16:24:40Z" id="65438915">&gt; @s1monw I think @nik9000 meant that we could make RestMainAction depend on this new transport action now and unify the behaviour between rest and transport layer. How does that sound?

Yeah - we use the main action for liveness testing.  I haven't dug into the code but was thinking "If that was good enough then why are they making this?"  I figured if this is better then we should be using it.  I'm totally ambivalent to if this should be exposed as a new http resource or linked into another one.
</comment><comment author="s1monw" created="2014-12-03T16:43:09Z" id="65443059">@nik9000 @javanna we had the main action before now we are adding an equivalent to it for transport so I don't think there is much we can reuse here really. Does this make sense
</comment><comment author="s1monw" created="2014-12-03T16:52:12Z" id="65444888">pushed a new commit with fixes @javanna @bleskes 
</comment><comment author="bleskes" created="2014-12-03T17:20:53Z" id="65449861">LGTM
</comment><comment author="javanna" created="2014-12-03T17:44:48Z" id="65453796">Only a small remark around the transport action name, that should be changed to `cluster:monitor/nodes/liveness`, LGTM otherwise
</comment><comment author="s1monw" created="2014-12-04T09:41:32Z" id="65575117">@javanna fixed the name 
</comment><comment author="javanna" created="2014-12-04T09:42:06Z" id="65575804">LGTM thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file></files><comments><comment>Transport: Tracer should exclude "cluster:monitor/nodes/liveness" by default</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/LivenessRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/LivenessResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/liveness/TransportLivenessAction.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/transport/TransportModule.java</file><file>src/test/java/org/elasticsearch/client/transport/FailAndRetryMockTransport.java</file><file>src/test/java/org/elasticsearch/client/transport/TransportClientHeadersTests.java</file></files><comments><comment>[CLIENT] Add internal liveness action</comment></comments></commit></commits></item><item><title>Feature/Fix for OGC compliant polygons failing with ambiguity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8762</link><project id="" key="" /><description>This feature/fix implements OGC compliance for Polygon/Multi-polygon to correctly cross the dateline without requiring the user specify invalid lat/lon pairs (e.g., 183/-70).  That is, vertex order for the exterior ring follows the right-hand rule (ccw) and all holes follow the left-hand rule (cw).  While GeoJSON imposes no restrictions, a user that wants to specify a complex poly across the dateline can now do so in compliance with the OGC spec, otherwise a polygon that spans the globe will be assumed.  Note that this could break a users current vertex order (i.e., the user specified a non-dateline crossing global poly in cw order - see ShapeBuilderTests.testShapeWithAlternateOrientation).  In this case the user would have to comply with OGC standards and reorder the vertices ccw.  This does not, however, break existing ordering for polys &lt; 180 degrees (90% of most geo datasets) or invalid lat/lon pairs.  So a user or existing deployment can still specify a dateline crossing poly using invalid lat/lon pairs. This simply implements OGC compliance to handle the ambiguous polygon problem.  

Closes #8672
</description><key id="50840151">8762</key><summary>Feature/Fix for OGC compliant polygons failing with ambiguity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>discuss</label><label>enhancement</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T13:54:38Z</created><updated>2015-05-29T16:25:06Z</updated><resolved>2014-12-16T17:34:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2014-12-10T23:44:47Z" id="66546139">Need to determine what versions this should be included.
</comment><comment author="clintongormley" created="2014-12-11T13:48:22Z" id="66620749">@nknize As I read this, the only difference in results would come from ambiguous polygons which cross the dateline?  We have no way of determining if a poly is correctly ordered, do we, because both orders are potentially valid, correct?

(Also, I note that there are no docs to explain this change)

I think I'd limit this to 1.4.2 and above.
</comment><comment author="nknize" created="2014-12-11T18:01:46Z" id="66660764">@clintongormley correct.  Limiting to 1.4.2 and above sounds good.  Should we create a task for updating the documentation to include the OGC behavior?  I can help w/ that, just need some guidance on how to submit updated documentation for review. 

For completeness, the following is for further clarification/discussion and documentation purposes.

---

This change only affects ambiguous dateline crossing polys.  For example, the following poly crosses the dateline when interpreted in ccw (right-hand) order.  But spans the map when interpreted in cw (left-hand) order. 

``` java
[[178.0, 15.0], [178.0, -15.0], [-179.0, -15.0], [-179.0, 15.0]]
```

The current implementation computes orientation at runtime.  So the above is computed in cw order when the intent may have been ccw (since ccw is the OGC standard).   This PR disambiguates the orientation by complying with OGC standards.

Impact to current implementation:  If someone specifies the above polygon with the intent of a cw map spanning poly.  This change will break that interpretation because its non-compliant with the standard.  The fix for that would be either changing the order of the vertices to comply w/ the OGC standard (ccw order). 

``` java
[[-178.0, 15.0], [-178.0, -15.0], [179.0, -15.0], [179.0, 15.0]]
```

Or, use an explicit orientation parameter.  Feature #8764 is in work that adds this optional parameter.
</comment><comment author="colings86" created="2014-12-16T16:21:57Z" id="67185805">Maybe we should have cw tests for  testParse_OGCPolygonWithHoles? Other than that LGTM
</comment><comment author="nknize" created="2014-12-16T17:34:52Z" id="67198437">Merged in edd33c0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>systemd service: wait for networking, don't daemonize</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8761</link><project id="" key="" /><description>This change updates the systemd elasticsearch service configuration to depend
on boot-time networking being available before ES is started (see
http://www.freedesktop.org/wiki/Software/systemd/NetworkTarget/ for a
discussion of the specific network-online target used in the service file).

Also, the ES script does not fork / daemonize when started by systemd since
this is the recommended behaviour for systemd services (see
http://0pointer.de/blog/projects/systemd-for-admins-3.html).

fixes #8636

Signed-off-by: Thilo Fromm github@thilo-fromm.de
</description><key id="50834439">8761</key><summary>systemd service: wait for networking, don't daemonize</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">t-lo</reporter><labels><label>:Packaging</label></labels><created>2014-12-03T13:04:04Z</created><updated>2015-06-17T07:26:59Z</updated><resolved>2015-06-17T07:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-06-17T07:26:58Z" id="112691893">@t-lo thanks for this contribution! It appears that your changes have been merged previously but this pull request remains open... And after that we updated the Systemd configuration in #10725.

I think we can close this now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggs: The nested aggregation should work on unmapped fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8760</link><project id="" key="" /><description>When a field is unmapped, aggregations try to behave as if the field existed but did not contain any value. The nested aggregation is an exception to this rule and fails if it is asked to run on a path that is not mapped, I think we should fix it to be consistent?

See https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/f0VYoHs1SpU/dx_iZW-U65kJ for more context.
</description><key id="50831511">8760</key><summary>Aggs: The nested aggregation should work on unmapped fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label></labels><created>2014-12-03T12:31:32Z</created><updated>2015-01-06T11:27:02Z</updated><resolved>2015-01-06T11:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/ReverseNestedAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/NestedTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ReverseNestedTests.java</file></files><comments><comment>Made the `nested`, `reverse_nested` and `children` aggs ignore unmapped nested fields or unmapped child / parent types.</comment></comments></commit></commits></item><item><title>[DOCS] Document ActionNames class and related tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8759</link><project id="" key="" /><description /><key id="50829283">8759</key><summary>[DOCS] Document ActionNames class and related tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>docs</label><label>v1.5.0</label></labels><created>2014-12-03T12:06:36Z</created><updated>2014-12-03T15:24:50Z</updated><resolved>2014-12-03T15:24:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-03T14:45:16Z" id="65418031">LGTM thanks so much @javanna 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove action names bwc layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8758</link><project id="" key="" /><description>The bwc layer added with #7105 is not needed in master as a full cluster restart will be required, thus from 2.0 on the only supported action names are compliant to the defined conventions and don't need to be converted to the old format
</description><key id="50826615">8758</key><summary>Remove action names bwc layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T11:39:17Z</created><updated>2015-06-07T11:54:09Z</updated><resolved>2014-12-03T15:19:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-03T14:45:54Z" id="65418152">LGTM get rid of it!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/transport/ActionNames.java</file><file>src/main/java/org/elasticsearch/transport/TransportService.java</file><file>src/main/java/org/elasticsearch/transport/TransportServiceAdapter.java</file><file>src/main/java/org/elasticsearch/transport/local/LocalTransport.java</file><file>src/main/java/org/elasticsearch/transport/netty/MessageChannelHandler.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>src/test/java/org/elasticsearch/transport/ActionNamesBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/transport/ActionNamesTests.java</file><file>src/test/java/org/elasticsearch/transport/netty/NettyTransportTests.java</file></files><comments><comment>[TEST] remove action names bwc layer</comment></comments></commit></commits></item><item><title>Issue with higlighting and analyzed tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8757</link><project id="" key="" /><description>I am experiencing an unexpected result with highlighting when using an _analyzer path in the mapping and custom analyzers. The highlighting returns no result for some query terms, even though the term matches and the document is returned. For other query terms it works fine. Somehow it seems that for querying and highlighting a different analyzer is used.

See the following commands to reproduce the issue:

https://gist.github.com/fxh/3246df167e4d72b0372f

I am using ES v1.3.4
</description><key id="50824403">8757</key><summary>Issue with higlighting and analyzed tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fxh</reporter><labels><label>:Highlighting</label></labels><created>2014-12-03T11:19:35Z</created><updated>2015-05-07T07:24:32Z</updated><resolved>2015-05-07T07:24:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tonibirrer" created="2014-12-03T13:22:05Z" id="65405496">Just adding to fxh's report that this started with 1.3.4. Previously we've been using 1.1.1 and there fxh's gist works.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugins failed to load since #8666</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8756</link><project id="" key="" /><description>The method Path.endsWith(String s) doesn't work exactly the same way as String.endsWith() (see http://docs.oracle.com/javase/7/docs/api/java/nio/file/Path.html#endsWith(java.nio.file.Path)). This blocks the loading of plugins.
</description><key id="50823582">8756</key><summary>Plugins failed to load since #8666</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-12-03T11:13:21Z</created><updated>2015-06-08T14:12:46Z</updated><resolved>2014-12-08T16:15:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-12-04T08:45:15Z" id="65553177">LGTM, last q: does it make sense to add a test for this in the plugin manager tests somewhere?
</comment><comment author="s1monw" created="2014-12-04T09:36:55Z" id="65570135">can we have a unittest that fails with this? Apparently we don't have one but we should
</comment><comment author="tlrx" created="2014-12-08T11:51:37Z" id="66106292">I just added some tests that should have failed without the fix.
</comment><comment author="s1monw" created="2014-12-08T11:56:26Z" id="66106734">LGTM

&gt; I just added some tests that should have failed without the fix.

did they actually fail without the fix?
</comment><comment author="tlrx" created="2014-12-08T13:12:04Z" id="66113827">@s1monw yes
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[BWC] Fix backwards compatibility after backporting #8723</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8755</link><project id="" key="" /><description>The backport of #8723 broke BWC for all 1.x versions due to
a break in the recovery protocol. BWC test failed big-time which is
great. This commit brings back BWC for this feature.
</description><key id="50810932">8755</key><summary>[BWC] Fix backwards compatibility after backporting #8723</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-12-03T09:48:43Z</created><updated>2015-03-19T10:18:22Z</updated><resolved>2014-12-03T14:48:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-03T10:15:34Z" id="65384551">LGTM
</comment><comment author="s1monw" created="2014-12-03T10:22:40Z" id="65385524">pushed fixes for your comments
</comment><comment author="jpountz" created="2014-12-03T10:23:11Z" id="65385619">+1
</comment><comment author="s1monw" created="2014-12-03T14:48:06Z" id="65418551">pushed...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Debian initscript fails on compound DATA_DIR setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8754</link><project id="" key="" /><description>The Debian initscript tries to create and set the ownership of `$DATA_DIR`.  However, if `$DATA_DIR` has a comma in it (indicating the user wants to locate indexes over multiple directories), the script uses the literal value of the variable as the name of the directory to manipulate instead of splitting them at the comma symbol. 

For example, if `$DATA_DIR` is `"/mnt/data1,/mnt/data2"`, then the script will try to create and set the ownership of a directory called  `"/mnt/data1,/mnt/data2"` instead of `/mnt/data1` and `/mnt/data2` separately.
</description><key id="50751195">8754</key><summary>Debian initscript fails on compound DATA_DIR setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfischer-zd</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2014-12-03T00:16:38Z</created><updated>2016-09-27T13:47:48Z</updated><resolved>2016-09-27T13:46:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T13:46:46Z" id="249869491">Fixed in 5.0 by #17419
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Revert back APIs that resolve files from classpath to java.net.URL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8753</link><project id="" key="" /><description>The conversion to the Path API doesn't work if the path points
to a file inside a JAR like a config. These path must be read
while the ZIP filesystem is opened which can't be guaranteed across
the board. This commit reverts back the relevant changes to java.net.URL
and adds a util method to read UTF-8 Encoded files from URLs correctly.
</description><key id="50739502">8753</key><summary>Revert back APIs that resolve files from classpath to java.net.URL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-12-02T22:44:06Z</created><updated>2014-12-02T23:11:08Z</updated><resolved>2014-12-02T23:10:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-02T23:00:29Z" id="65321933">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Getting snapshot list hangs while snapshot is in progress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8752</link><project id="" key="" /><description>If I try and list the snapshots for a given repository 'foo' while there is a snapshot in progress, ES appears to block until the snapshot is complete:

GET http://localhost/_snapshot/foo/_all

I realize you can use the status API, but why should the GET block?
</description><key id="50732543">8752</key><summary>Getting snapshot list hangs while snapshot is in progress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">matthughes</reporter><labels /><created>2014-12-02T22:06:27Z</created><updated>2014-12-03T08:43:18Z</updated><resolved>2014-12-03T08:43:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2014-12-03T08:43:18Z" id="65372296">Thanks for reporting! Closing this issue because it's a duplicate of #7859
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>WrapperQueryBuilder invalid JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8751</link><project id="" key="" /><description>When I wrap the simple json query below I get the following error...

Parse Failure [Failed to parse source [{\n  \"query\" : {\n    \"wrapper\"......

JSON QUERY PRIOR TO WRAPPING...
{
  "query" : {
    "range" : {
      "num" : {
        "from" : 102,
        "to" : 106,
        "include_lower" : true,
        "include_upper" : true
      }
    }
  }
}

JSON QUERY AFTER WRAPPING W/Filter added...THIS RESULTS IN PARSE ERROR when sent to server...

{
  "query" : {
    "filtered" : {
      "query" : {
        "wrapper" : {
          "query" : "ewogICJxdWVyeSIgOiB7CiAgICAicmFuZ2UiIDogewogICAgICAibnVtIiA6IHsKICAgICAgICAiZnJvbSIgOiAxMDIsCiAgICAgICAgInRvIiA6IDEwNiwKICAgICAgICAiaW5jbHVkZV9sb3dlciIgOiB0cnVlLAogICAgICAgICJpbmNsdWRlX3VwcGVyIiA6IHRydWUKICAgICAgfQogICAgfQogIH0KfQ=="
        }
      },
      "filter" : {
        "term" : {
          "num" : "104"
        }
      }
    }
  }
}

The original query is valid, the filter is valid. Is this a bug or incorrect usage?
</description><key id="50729591">8751</key><summary>WrapperQueryBuilder invalid JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pete-hodgman</reporter><labels /><created>2014-12-02T21:50:32Z</created><updated>2014-12-03T18:20:10Z</updated><resolved>2014-12-03T14:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2014-12-03T14:56:06Z" id="65419966">The wrapped query is malformed: instead of `{"query": { "range" : {...}}}` you must wrap the range query itself like `{ "range" : {...}}`.

As far as I understand your query, your are querying all documents that have a `num` value between 102 and 106, and then retain only documents with num value 104 in the filter. The wrapped query is not really useful and can be replace with a `match_all`query.

Closing this issue, not a bug.
</comment><comment author="pete-hodgman" created="2014-12-03T18:20:09Z" id="65459663">Thanks!  To your point on the usefulness of the query. This was a contrived example who's purpose was to illustrate the wrapper issue. Not something real-world. Thanks for the feedback.

Pete Hodgman
734-476-6680

On Dec 3, 2014, at 9:57 AM, Tanguy Leroux &lt;notifications@github.com&lt;mailto:notifications@github.com&gt;&gt; wrote:

The wrapped query is malformed: instead of {"query": { "range" : {...}}} you must wrap the range query itself like { "range" : {...}}.

As far as I understand your query, your are querying all documents that have a num value between 102 and 106, and then retain only documents with num value 104 in the filter. The wrapped query is not really useful and can be replace with a match_allquery.

Closing this issue, not a bug.

—
Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/8751#issuecomment-65419966.

---

The information contained in this email and any attachment(s) transmitted from Invicara.com may contain confidential, proprietary or legally privileged information relating to its associated legal entities. The information contained in this communication is subject to copyright and intended only for the use of the recipient(s). Unauthorised use, disclosure, copying or distribution is strictly prohibited unless without prior authorization. If you have received this email in error please notify the sender electronically and delete all copies immediately. Should a virus infection occur as a result of this communication Invicara.com and its associated legal entities will not be liable.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add explicit error message when script_score script returns NaN</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8750</link><project id="" key="" /><description>When a scoring script returns not a number, the current message is confusing (IllegalArgumentException[docID must be &gt;= 0 and &lt; maxDoc=3 (got docID=2147483647)]). This commit adds the error message ScriptException[script score function returns a wrong score: NaN].

Closes #2426
</description><key id="50716747">8750</key><summary>Add explicit error message when script_score script returns NaN</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-12-02T20:43:24Z</created><updated>2015-06-08T14:14:15Z</updated><resolved>2014-12-08T09:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-12-04T17:26:23Z" id="65668689">LGTM
</comment><comment author="s1monw" created="2014-12-04T19:33:50Z" id="65688816">instead of an integration test can't we just write a very simple unittest here too?
</comment><comment author="rjernst" created="2014-12-04T19:37:34Z" id="65689416">+1 for a unittest.  It is difficult with the script engines, but here it should be easy.
</comment><comment author="tlrx" created="2014-12-05T13:11:04Z" id="65787659">Sure, a unittest is way more appropriate than an integration test for that.
</comment><comment author="s1monw" created="2014-12-05T22:17:17Z" id="65863567">LGTM good stuff the test is much better than the previous one
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Override write(byte[] b, int off, int len) in FilterOutputStream for better performance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8749</link><project id="" key="" /><description>Closes #8748
</description><key id="50713574">8749</key><summary>Override write(byte[] b, int off, int len) in FilterOutputStream for better performance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-02T20:21:48Z</created><updated>2015-06-07T17:28:35Z</updated><resolved>2014-12-02T20:26:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-12-02T20:23:51Z" id="65297917">+1, this is a horrible trap. It has a real impact on write performance. Lucene tests got like 8x slower because of one of these that was just being used to do something on close()
</comment><comment author="s1monw" created="2014-12-02T20:25:47Z" id="65298208">thx for the review @rmuir 
</comment><comment author="agallou" created="2015-01-26T21:43:13Z" id="71544409">Thanks for the fix.

We were using the 1.4.1 version and the snapshot took 5 hours (for 8.52Gi and 1 801 986 docs).
The same snapshot in version 1.4.2 now takes less than one minute.

Snapshot in version 1.4.1 : 

```
{
    "snapshot" : "bkp_20150122214749",
    "indices" : [ "livre.20150122211008", "editeur.20150122211008", "auteur.20150122211008", "fournisseur.20150122211008" ],
    "state" : "SUCCESS",
    "start_time" : "2015-01-22T20:47:49.941Z",
    "start_time_in_millis" : 1421959669941,
    "end_time" : "2015-01-23T01:52:00.983Z",
    "end_time_in_millis" : 1421977920983,
    "duration_in_millis" : 18251042,
    "failures" : [ ],
    "shards" : {
      "total" : 20,
      "failed" : 0,
      "successful" : 20
    }
}
```

Snapshot in version 1.4.2 : 

```
{
    "snapshot" : "bkp_20150126220702",
    "indices" : [ "livre.20150126002727", "editeur.20150126002727", "fournisseur.20150126002727", "auteur.20150126002727" ],
    "state" : "SUCCESS",
    "start_time" : "2015-01-26T21:07:02.522Z",
    "start_time_in_millis" : 1422306422522,
    "end_time" : "2015-01-26T21:07:42.867Z",
    "end_time_in_millis" : 1422306462867,
    "duration_in_millis" : 40345,
    "failures" : [ ],
    "shards" : {
      "total" : 20,
      "failed" : 0,
      "successful" : 20
    }
}
```
</comment><comment author="clintongormley" created="2015-01-27T08:57:24Z" id="71611514">awesome - thanks for the feedback @agallou 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot/Restore: FilterOutputStream in FsBlobContainer writes every individual byte </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8748</link><project id="" key="" /><description>trappy   `FilterOutputStream` writes every byte in `public void write(byte[] b, int off, int len)` which leads to bad perf... This was introduced in 1.4.0
</description><key id="50713054">8748</key><summary>Snapshot/Restore: FilterOutputStream in FsBlobContainer writes every individual byte </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-02T20:18:38Z</created><updated>2014-12-16T11:31:25Z</updated><resolved>2014-12-02T20:26:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file></files><comments><comment>Override write(byte[] b, int off, int len) in FilterOutputStream for better performance</comment></comments></commit></commits></item><item><title>Fix the wording a bit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8747</link><project id="" key="" /><description>Simple, but needed. :smile: 
</description><key id="50709350">8747</key><summary>Fix the wording a bit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">adammenges</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-12-02T19:54:15Z</created><updated>2014-12-09T12:38:42Z</updated><resolved>2014-12-09T12:38:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="adammenges" created="2014-12-02T20:14:29Z" id="65296569">I can compress those two commits, if desired.
</comment><comment author="clintongormley" created="2014-12-04T19:09:43Z" id="65684968">Hi @adammenges 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="adammenges" created="2014-12-06T19:18:08Z" id="65910073">Done!
</comment><comment author="clintongormley" created="2014-12-09T12:38:36Z" id="66276534">thanks @adammenges - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fix the wording for inner hits a bit</comment></comments></commit></commits></item><item><title>Remove custom posting formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8746</link><project id="" key="" /><description>Custom posting formats (eg pulsing, direct, etc) are not supported by Lucene - they provide no bwc promise.  We removed the documentation before 1.4.0.Beta1, and we should prevent users from using these codecs going forward.

At the very least, we should deprecate custom codecs and refuse to create new indices that use them.  I don't know how widely used these are (I'd imagine not very widely at all), but I'd be tempted to remove them completely in 2.0.
</description><key id="50707535">8746</key><summary>Remove custom posting formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-12-02T19:46:16Z</created><updated>2015-02-25T08:50:18Z</updated><resolved>2015-02-19T15:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-01-16T14:17:58Z" id="70259139">+1 to remove them completely. Given that they are not backward compatible anyway, even if we kept it there would be a high chance that loading the index would fail.
</comment><comment author="bleskes" created="2015-01-16T14:38:38Z" id="70261976">just to clarify, we do plan to keep the ability to inject custom written codes/posting formats from plugins, right?
</comment><comment author="jpountz" created="2015-01-16T14:42:12Z" id="70262507">I don't think we should. We had to work hard in order to make storage more resilient to corruptions, we should not let users add more uncertainty to the system.
</comment><comment author="rmuir" created="2015-01-16T14:45:30Z" id="70262915">+1 Adrien. I also don't think the default codec of lucene needs to support such extensibility.
</comment><comment author="s1monw" created="2015-02-13T11:42:34Z" id="74241807">+1 lets get rid of all this.

@jpountz can you take this issue?
</comment><comment author="bleskes" created="2015-02-13T11:47:32Z" id="74242339">We discussed it and it was my understanding that due to _dangerous_ nature of replacing codecs, we are going to remove the ability to add custom one through the settings and plugins. It will still be possible to use lucene's SPI to add codecs on the lucene level, in which case one can use them via the index.code settings. I'm +1 on that.

To be clear, what I care about is removing all options to specific custom postings lists. I would be OK with not allowing to replace codecs but rather have other methods to add custom java level posting lists.
</comment><comment author="jpountz" created="2015-02-13T13:11:03Z" id="74251073">@s1monw I'll take care of it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/codec/CodecModule.java</file><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/codec/PerFieldMappingPostingFormatCodec.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/AbstractDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DefaultDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormatService.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormats.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/PreBuiltDocValuesFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/AbstractPostingsFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/DefaultPostingsFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingFormats.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingsFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingsFormatService.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PreBuiltPostingsFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/Murmur3FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatProvider.java</file><file>src/test/java/org/elasticsearch/codecs/CodecTests.java</file><file>src/test/java/org/elasticsearch/index/codec/CodecTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPlugin2Tests.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPluginTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file></files><comments><comment>Codecs: Remove the ability to have custom per-field postings and doc values formats.</comment></comments></commit></commits></item><item><title>[TEST] guarantee REST tests execution order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8745</link><project id="" key="" /><description>REST tests are being shuffled before their execution by the randomized runner. To guarantee their repeatability given the seed, their order needs to be always the same before the shuffling happens, while it currently is platform dependent.
</description><key id="50694658">8745</key><summary>[TEST] guarantee REST tests execution order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-02T18:20:03Z</created><updated>2015-03-19T10:23:13Z</updated><resolved>2014-12-03T07:47:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-02T19:32:06Z" id="65289508">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java</file></files><comments><comment>[TEST] guarantee REST tests execution order</comment></comments></commit></commits></item><item><title>[docs] formatting and general pedantry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8744</link><project id="" key="" /><description>I'm not sure if the `distance-units` section is totally clear, when using the 'Geohash Cell Filter' and omitting a unit, the default is to interpret the integer as the 'length of the geohash prefix', not to default it to 'meter'. Maybe I'm being pedantic.
</description><key id="50691006">8744</key><summary>[docs] formatting and general pedantry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">missinglink</reporter><labels /><created>2014-12-02T17:55:24Z</created><updated>2014-12-02T18:24:17Z</updated><resolved>2014-12-02T18:24:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="missinglink" created="2014-12-02T17:56:35Z" id="65273873">ref: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/common-options.html
</comment><comment author="clintongormley" created="2014-12-02T18:24:17Z" id="65278219">Thanks @missinglink - I've also reworded the distance section to take your (valid) point into account
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[docs] formatting and general pedantry</comment></comments></commit></commits></item><item><title>Core: Upgrade to lucene-5.0.0-snapshot-1642891.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8743</link><project id="" key="" /><description>The only required change was to add a description to our index outputs.
</description><key id="50676456">8743</key><summary>Core: Upgrade to lucene-5.0.0-snapshot-1642891.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-12-02T16:29:32Z</created><updated>2014-12-02T18:26:55Z</updated><resolved>2014-12-02T16:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-02T16:33:05Z" id="65259380">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/index/store/VerifyingIndexOutput.java</file></files><comments><comment>Core: Upgrade to lucene-5.0.0-snapshot-1642891.</comment></comments></commit></commits></item><item><title>background start-up: use PID file to signal success</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8742</link><project id="" key="" /><description>This change introduces the use of elasticsearch's PID file to signal a
successful start-up. The start script `bin/elasticsearch` will wait a
configurable amount of time (default: 120s) for the PID file to appear. After
this timed out the script will print an error and return an error code.

Note that if no PID file was specified on the command line the script will now
use `mktemp` to create a "private" PID file (only if ES is to be started in the
background, though). The command line help was updated correspondingly.

Also, the PID file generation code has been moved from the beginning of the
bootstrap `main()` method to the end.

Fixes #8652.

**NOTE**
Please review my changes to `src/main/java/org/elasticsearch/bootstrap/Bootstrap.java` very closely; I'm not too familiar with ES code and am not sure if I broke something.

**NOTE 2**
This change introduces a (theoretical) new error condition: If elasticsearch requires longer than the timeout to bootstrap, a "false positive" start-up error will be reported by the script even though elasticsearch will be up and running eventually. This can be worked around by setting an appropriately long timeout via the new `-w` command line option.
</description><key id="50666558">8742</key><summary>background start-up: use PID file to signal success</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">t-lo</reporter><labels><label>:Packaging</label><label>review</label></labels><created>2014-12-02T15:30:27Z</created><updated>2015-11-21T21:41:01Z</updated><resolved>2015-11-21T21:41:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-05T08:47:27Z" id="65761897">This one LGTM I will merge it later today or tomorrow
</comment><comment author="jpountz" created="2014-12-05T09:37:53Z" id="65766595">For the record I just merged https://github.com/elasticsearch/elasticsearch/pull/6909 which uses a timeout of 10s for the debian init script. It might make sense to increase the timeout to 120s there too when we merge this PR since the PID file now only gets created after the node has been started.
</comment><comment author="t-lo" created="2014-12-05T11:06:26Z" id="65775830">@jpountz Actually the init scripts (both deb and rpm) do not need to handle daemonization at all since the elasticsearch start script takes care of that (with the `-d` command line flag). The reason the init scripts should not daemonize is that they can't detect start-up errors when backgrounding elasticsearch. This pull request adds such error detection to the elasticsearch starter script. If this here pull request makes it then I'll prepare a pull request next week to clean up the init scripts, I don't know if it's worth it to adjust the 10 seconds now since they might disappear anyway soon.
</comment><comment author="jpountz" created="2014-12-05T11:36:53Z" id="65778918">Agreed that it would be nice to have init scripts rely on bin/elasticsearch to put the process in the background instead of doing it themselves.
</comment><comment author="t-lo" created="2014-12-06T12:44:52Z" id="65895674">@s1monw Waiting for a merge so I can start working on a patch for #8796.
</comment><comment author="spinscale" created="2014-12-12T09:16:55Z" id="66748698">Hey @t-lo 

I see a couple of issues here (quickly tested under osx). This is the output when trying to run it:

``` sh
 » bin/elasticsearch -d
usage: mktemp [-d] [-q] [-t prefix] [-u] template ...
       mktemp [-d] [-q] [-u] -t prefix
-n Starting elasticsearch in background...
bin/elasticsearch: line 170: exec: setsid: not found
```

Seems to be pretty linux focused or am I missing some setup under osx? IIRC `mktemp` is very different under those OS'es.

Some other minor things
- Is millisecond sleep supported under all platforms? Including older linuxes and maybe other unixes like solaris?
- Try not to use bashisms like `$(())` as this is not POSIX compliant and not supported by dash under ubuntu
</comment><comment author="t-lo" created="2014-12-12T11:07:29Z" id="66759766">`echo -n` does not seem to be supported, too. I'll look into this.

I can wrap `setsid` so it won't be used when it's not available. Please note that this breaks proper daemonization: without `setsid` the `elasticsearch` process will end up being a child of the shell it was started in. I guess this is still a better option that not starting at all, though.

MacOS's `mktemp` takes a mandatory `[TEMPLATE]` which is optional with Linux (see https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/mktemp.1.html and http://linux.die.net/man/1/mktemp). I'll update this to always use a `[TEMPLATE]`. 

Both MacOS and Linux support sub-second sleep (https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/sleep.1.html and http://linux.die.net/man/1/sleep) although the MacOS man page states it's not portable. For the sake of portability I will settle to 1s sleep here.

Lastly, `$(())` is not a bashism, it's actually POSIX: http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_04. I'd like to keep it the way it is if that's OK with you.

I'll update my pull request accordingly, please stand by :)
</comment><comment author="t-lo" created="2014-12-12T12:51:12Z" id="66768683">@spinscale Done, please review. Note that I squashed all intermediate changes into a single commit in order to ease pulling/merging.
</comment><comment author="tlrx" created="2014-12-12T14:00:50Z" id="66775514">The `$(())` is interesting since I need it for something else.

Here's what I've found on https://wiki.ubuntu.com/DashAsBinSh#line-69 :

```
((...)) performs arithmetic expansion, but is a bashism. You should normally use something involving $((...)) instead, although note that this substitutes the value of the expression.
```

To me `$(())` seems to be supported by Dash.
</comment><comment author="t-lo" created="2014-12-12T14:50:11Z" id="66781549">`$(())` is supported by Dash, I tested the script with both Dash and Bash. 

Also, as mentioned above, IEEE's Open Group base specification (IEEE Std 1003.1) 2004 Edition defines `$(())` arithmetic expansion in http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_04. Therefore, `$(())` is POSIX, not bashism, and has been POSIX since 2004.
</comment><comment author="spinscale" created="2014-12-12T16:36:38Z" id="66797434">@t-lo I was dead sure, that there at least used to be a problem with dash and that notation. Happy its gone. One thing less to care for. You are of course right with the POSIX point, I mixed those up. Thanks for pointing out.

I just tested the latest version on mac os x and it works. Need to check for some other linuxes and run the puppet tests with it and then we'll get it in.

Did you test under ubuntu?
</comment><comment author="t-lo" created="2014-12-12T16:46:08Z" id="66798884">@spinscale Nope, I tested with Debian 7 and Fedora 21 (and more recently with my wife's Macbook); all three work fine.
</comment><comment author="clintongormley" created="2015-11-21T21:41:01Z" id="158684156">Closing in favour of https://github.com/elastic/elasticsearch/pull/13643
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update range-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8741</link><project id="" key="" /><description /><key id="50650963">8741</key><summary>Update range-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmluy</reporter><labels /><created>2014-12-02T13:34:23Z</created><updated>2014-12-02T17:03:40Z</updated><resolved>2014-12-02T17:01:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update range-filter.asciidoc</comment></comments></commit></commits></item><item><title>Update range-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8740</link><project id="" key="" /><description /><key id="50638953">8740</key><summary>Update range-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmluy</reporter><labels /><created>2014-12-02T11:16:00Z</created><updated>2014-12-02T13:38:10Z</updated><resolved>2014-12-02T11:56:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-02T11:56:00Z" id="65220381">Thanks @jmluy - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update range-query.asciidoc</comment></comments></commit></commits></item><item><title>[RECOVERY] Make recovery delay configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8739</link><project id="" key="" /><description>Today we wait 500ms before we retry a recovery if the target node is not ready.
This happens if the source starts the recovery before the target has
processed the clusterstate moving the target shard into the right state.
This can cause a 500ms delay each time it happens while the shard is ready
way earlier on the target node. This commit makes this delay configurable
to mainly speed up test processing and shard allocation in tests.
</description><key id="50638804">8739</key><summary>[RECOVERY] Make recovery delay configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-02T11:14:11Z</created><updated>2015-03-19T10:23:20Z</updated><resolved>2014-12-02T12:34:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-12-02T11:56:14Z" id="65220411">LGTM - note that I had added a similar settings in https://github.com/elasticsearch/elasticsearch/pull/8720/files#diff-d1e7cc297312b15bab61e9467d3d0cd7R78 , but i needed more control over different reasons to retry. I like the fact that is' in RecoverySettings now and dynamically updatable. I'll update my PR when rebasing. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decommission a node based on _ip </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8738</link><project id="" key="" /><description>Hi , 

Im trying to decommission a node based on _ip in our Elasticsearch Cluster.
Im using this api :
curl -XPUT localhost:9200/_cluster/settings -d '{"transient" : {"cluster.routing.allocation.exclude._ip" : "10.231.46.XXX"}}'
The response i get is this:
{"acknowledged":true,"persistent":{},"transient":{"cluster":{"routing":{"allocation":{"exclude":{"_ip":"10.231.46.XXX"}}}}}}

But actually nothing happens in the cluster after 30 min.

Should not  the cluster start relocating shards from the decommissioned node ?

Please advise?

Regards,
Costya.
</description><key id="50621515">8738</key><summary>Decommission a node based on _ip </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cregev</reporter><labels><label>feedback_needed</label></labels><created>2014-12-02T07:50:37Z</created><updated>2014-12-02T10:43:52Z</updated><resolved>2014-12-02T10:43:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-02T10:38:42Z" id="65211569">Hi @CostyaRegev 

If a shard is not yet allocated to that IP, then one won't be allocated.  If a shard is already allocated then it will only be moved off that node if there is somewhere else to put it.  I'm guessing that either you're running with just two nodes, or you have some other setting which is preventing allocation.

Try using the cluster reroute API with `explain` to understand why your shards are not being moved.
</comment><comment author="cregev" created="2014-12-02T10:43:52Z" id="65212126">Ok  found the problem it was a setting issue.

Thanks,
Costya.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>the virtual memory is too larger</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8737</link><project id="" key="" /><description>the  os is suse linux
when es store many many docs ,the virtual memory is very large(the memory is small)
when i insert docs into es, the virtual memory  will increase
and now the virual memory is too large(  memory is only 824m but virual memory  is 4229m !!)
what's wrong with my es?

---

$top 
 Cpu(s): 49.3%us,  3.0%sy,  0.0%ni, 47.5%id,  0.0%wa,  0.0%hi,  0.2%si,  0.0%st 
Mem:   8063180k total,  4669788k used,  3393392k free,   359576k buffers 
Swap: 16777212k total,    34848k used, 16742364k free,  3358636k cached 

28209 nbifs  20 0 4229m 824m 311m S  209 10.5  44:26.68 java  
 4723 ora11203  -2   0  821m  15m  13m S      3  0.2 129:14.51 oracle  
 ....
</description><key id="50613961">8737</key><summary>the virtual memory is too larger</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">balibaba12</reporter><labels /><created>2014-12-02T05:28:12Z</created><updated>2014-12-02T06:05:44Z</updated><resolved>2014-12-02T06:05:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-12-02T06:05:44Z" id="65187350">Please use the mailing list for questions like this.
We can help you there.

BTW disable any swap as explained in doc.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inconsistent handling of invalid field name references</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8736</link><project id="" key="" /><description>Consider the following example.  The query references a field name that has a different casing.  As we know, ES treats field names with different casing as different fields.  So the query is effectively referencing an "invalid" field name.   

If the query contains a sort, an error is thrown on the invalid field.  But if the query contains a filter against the same invalid field, an error is not thrown.  Is there a reason why in the sort use case we are not allowing the query to process (eg. still execute the query and fallback to sort using _score)?

```
DELETE seq
PUT seq
{
  "mappings": {
    "type": {
      "properties": {
        "SequenceNumber": {
          "type": "double",
          "doc_values": true
        }
      }
    }
  }
}
POST seq/type/1
{
  "SequenceNumber":1.0
}

# Error is thrown when filter is referencing an "invalid" field
# Parse Failure [No mapping found for [sequenceNumber] in order to sort on]]; }]",
GET seq/type/_search
{
  "sort": [
    {
      "sequenceNumber": {
        "order": "desc"
      }
    }
  ]
}
# No error thrown when filter is referencing an "invalid" field
GET seq/type/_search
{
  "query": {
    "filtered": {
      "filter": {
        "range": {
          "sequenceNumber": {
            "from": 0,
            "to": 20
          }
        }
      }
    }
  }
}
```
</description><key id="50602429">8736</key><summary>Inconsistent handling of invalid field name references</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-12-02T01:39:27Z</created><updated>2014-12-02T10:33:22Z</updated><resolved>2014-12-02T10:33:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-02T10:33:22Z" id="65210887">You can search across all indices, some of which may have the field and some not.  The search will only match documents that have the field, and ignore indices that don't have the field. This is a perfectly legitimate use case.

On the other hand, we can't sort those results by a non-existent field, which is why we are stricter with sorting, and require you to specify how we should sort documents that lack the field.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Marvel Sense does not work over SSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8735</link><project id="" key="" /><description>Marvel Sense is not working over ssl, it hangs when it tries to connect to the elasticsearch API.The rest of Marvel seems to be working fine, as does the elasticsearch API if called directly.

I am using a freshly installed elasticsearch 1.4.1 and the latest Marvel. 

Elasticsearch is exposed on the internet over SSL as https://mydomain/.

When I profile the javascript in the browser, I can see that Marvel Sense actually tries to make the calls to the API over http, even though the API and Marval are using https. My Elasticsearch installation is not available over the internet to anything other than https. I have nginx installed, which passes the elasticsearch calls from 443 to 9200. Again this all works fine when calling the Elasticsearch API or the plugins etc.

Marvel Sense ignores the url that is in the config file (plugins/marvel/_site/kibana/config.js). It goes to http even if I hardcode the elasticsearch with a https prefix.
</description><key id="50564409">8735</key><summary>Marvel Sense does not work over SSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">andrewrimmer</reporter><labels /><created>2014-12-01T19:33:53Z</created><updated>2015-01-26T08:41:53Z</updated><resolved>2015-01-26T08:41:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T19:47:29Z" id="65123164">@andrewrimmer This will be fixed in the next release.
</comment><comment author="bleskes" created="2014-12-01T20:20:08Z" id="65127925">@andrewrimmer while we do have automatic support for this in the next marvel release, I wonder if things would work for you if enter https://mydomain/ in the server box  of sense. It doesn't indeed listen to the config.js file of kibana.
</comment><comment author="andrewrimmer" created="2014-12-01T21:23:48Z" id="65137040">Thanks the workaround of putting the base url in the server box of sense does the trick.
</comment><comment author="bleskes" created="2015-01-26T08:41:53Z" id="71428677">@andrewrimmer this should be solved with Marvel 1.3.0 . If something doesn't work as you expect, feel free to re-open.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Marvel Sense does not work over SSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8734</link><project id="" key="" /><description>Marvel Sense is not working over ssl, it hangs when it tried to connect to the elasticsearch API.The rest of Marvel seems to be working fine, as does the elasticsearch API if called directly.

I am using a freshly installed elasticsearch 1.4.1 and the latest Marvel. 

Elasticsearch is exposed on the internet over SSL as https://mydomain/.

When I profile the javascript in the browser, I can see that Marvel Sense actually tries to make the calls to the API over http, even though the API and Marval are using https. My Elasticsearch installation is not available over the internet to anything other than https. I have nginx installed, which passes the elasticsearch calls from 443 to 9200. Again this all works fine when calling the Elasticsearch API or the plugins etc.

Marvel Sense ignores the url that is in the config file (plugins/marvel/_site/kibana/config.js). It goes to http even if I hardcode the elasticsearch with a https prefix.
</description><key id="50564357">8734</key><summary>Marvel Sense does not work over SSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewrimmer</reporter><labels /><created>2014-12-01T19:33:23Z</created><updated>2014-12-01T19:47:11Z</updated><resolved>2014-12-01T19:47:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T19:47:11Z" id="65123113">Duplicate of #8735
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix spelling and grammar mistakes on range-filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8733</link><project id="" key="" /><description>therefor != therefore, and missing word in the same sentence.
</description><key id="50559206">8733</key><summary>Fix spelling and grammar mistakes on range-filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nickick</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-12-01T18:47:06Z</created><updated>2014-12-15T13:20:25Z</updated><resolved>2014-12-15T13:20:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T18:58:03Z" id="65115517">Hi @nickick 
Thanks for the fix. Please could I ask you to sign the CLA so that I can merge your changes in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="nickick" created="2014-12-02T00:58:34Z" id="65166900">@clintongormley signed the CLA, do I need to link anything here to verify?
</comment><comment author="clintongormley" created="2014-12-02T10:29:53Z" id="65210478">thanks @nickick - merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>rpm,deb: make ES user own plugins dir, remove on uninstall</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8732</link><project id="" key="" /><description>This change will chown /usr/share/elasticsearch/plugins to the elasticsearch user (the directory was formerly owned by root). This enables the ES user to manage plugins.

Also, please note that /usr/share/elasticsearch/plugins **is now removed when the elasticsearch package is un-installed**. Previously it was left lying there.

Fixes #8419, tested with Fedora 21 (RPM) and Debian 7 (DEB).
</description><key id="50553067">8732</key><summary>rpm,deb: make ES user own plugins dir, remove on uninstall</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">t-lo</reporter><labels><label>:Packaging</label><label>Awaiting CLA</label></labels><created>2014-12-01T17:49:18Z</created><updated>2015-03-19T10:18:21Z</updated><resolved>2014-12-05T13:26:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-01T19:27:24Z" id="65120072">LGTM @spinscale can you take a look please?
</comment><comment author="s1monw" created="2014-12-01T19:28:43Z" id="65120249">@t-lo can you sign the CLA please ==&gt; http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="t-lo" created="2014-12-01T19:44:14Z" id="65122648">@s1monw First thing tomorrow.
</comment><comment author="spinscale" created="2014-12-02T08:05:12Z" id="65195162">LGTM

@t-lo Thanks a lot!
</comment><comment author="t-lo" created="2014-12-02T09:11:31Z" id="65201365">@s1monw I just signed the CLA.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Packaging: Make ES user own plugins dir, remove on uninstall</comment></comments></commit></commits></item><item><title>Auto route primary shards (w/o replica) to other nodes in case of data node failure </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8731</link><project id="" key="" /><description>Hi,

In case of non-replica cluster if a data node goes down then all its shards remain dangling.
In such cases, is it possible to auto route them back to other nodes ?

Please refer more details here:
https://groups.google.com/forum/#!topic/elasticsearch/O2LFIhxh-QI

As long as replicas are there this is not a problem but we don't have replica in default setup hence checking if this can be a priority. We are using 1.0 ES. Not sure if  there is different behavior in latest release.   

Thanks,
Ram
</description><key id="50547672">8731</key><summary>Auto route primary shards (w/o replica) to other nodes in case of data node failure </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rphadake</reporter><labels /><created>2014-12-01T17:02:30Z</created><updated>2014-12-02T12:56:25Z</updated><resolved>2014-12-01T18:31:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T18:31:44Z" id="65111399">Hi @rphadake 

Creating new empty primary shards means certain data loss. Elasticsearch is not going to do that for you automatically.  Instead you can use the cluster reroute API to force Elasticsearch to assign new primary shards.  See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-reroute.html
</comment><comment author="rphadake" created="2014-12-02T04:40:09Z" id="65182572">Actually if API is available to do this forcefully can we enable this via some setting to  "index.recovery.initial_shards" to do this automatically. Meaning by default ES won't have that behavior but if someone wants it then he has to enable this way using some setting.

Reason for asking for this behavior is in situations where we don't want to maintain replicas but still want to continue with indexing in case of data loss. e.g. say in a 3 node cluster (w/o replica)  if one node goes down I still want to continue indexing and not lose new data. Another reason is due to permanent UnavailableShardsException errors in such cases &amp; with bulk request wait time, effectively this may reduce data incoming rate by large extent.

&gt; Creating new empty primary shards means certain data loss

Not clear @ this. Actually with new shard creation via reroute (or say automatic) it will be assigned to another node &amp; later previous node comes back do the merges happen or there is data loss &amp; latest shard version is retained. Please comment.

Thanks,
Ram
</comment><comment author="clintongormley" created="2014-12-02T10:35:44Z" id="65211189">Hi @rphadake 

&gt; &gt; Creating new empty primary shards means certain data loss
&gt; &gt; Not clear @ this. Actually with new shard creation via reroute (or say automatic) it will be assigned to another node &amp; later previous node comes back do the merges happen or there is data loss &amp; latest shard version is retained. Please comment.

No. Once you assign a new primary shard, the old primary shard will never be reincorporated.  The data is lost forever.

Running Elasticsearch without replicas is putting you at risk of losing data. It is not a recommended setup, and we are not going to add a setting which helps you to lose data automatically.
</comment><comment author="rphadake" created="2014-12-02T12:40:23Z" id="65225050">Thanks Clinton. I was under the impression that we can potentially do shard merge between 2 shard versions. If you can give some pointers on whether shard merge is possible I can invest time on fixing this &amp; contribute. This looks complicated.   
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Master node is sending the same cluster state again and again upon shard failure.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8730</link><project id="" key="" /><description>After upgrading from 1.0.2 to 1.4.1 we had a shard failing to initialize. For this issue the failing of the shard is not the issue, but that the master node always sends the same cluster state with the failed state again and again, causing lots of unnecessary traffic, especially on a large cluster with a lot of indices.

Here is the log from the master node related to the failed index:

```
[2014-12-01 17:38:39,575][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [430], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:39,577][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:40,112][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:40,154][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [432], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:40,156][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:42,654][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:42,697][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [434], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:42,699][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:56,543][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:56,584][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [436], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:56,587][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:40:12,654][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:40:12,696][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [438], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:40:12,699][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
```

It seems that the master node is not correctly preventing the cluster state from being distributed, as it does not change as the same index is failing again and again on the same node with the same error message.
</description><key id="50546250">8730</key><summary>Master node is sending the same cluster state again and again upon shard failure.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">miccon</reporter><labels><label>feedback_needed</label></labels><created>2014-12-01T16:50:45Z</created><updated>2015-03-02T10:00:00Z</updated><resolved>2015-03-02T10:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T18:29:43Z" id="65111090">Hi @miccon 

Elasticsearch doesn't delete corrupted shards for you.  If you go ahead and delete the shard, it should resolve this issue.
</comment><comment author="miccon" created="2014-12-02T08:32:23Z" id="65197474">@clintongormley Thanks for your reply, for me the issue is not that elasticsearch is not deleting the failed shards. Leaving the failed shards untouched is exactly the behaviour I would expect.

The issue is that the master node distributes the cluster state repeatedly with no apparent change (as the same shard is failing on the same node again) to all the nodes in the cluster. This might cause a lot of unnecessary traffic.

For other cluster state updates, when there is no change in the cluster state, the log on the master node contains "no change in cluster_state" and skips the distribution, which is also what I expected here.
</comment><comment author="bleskes" created="2014-12-02T11:08:48Z" id="65214891">@miccon I noticed that the failures are about a primary. Do you have any other copies (i.e., replicas) of the shards on disks somewhere?
</comment><comment author="miccon" created="2014-12-02T11:19:16Z" id="65215990">Replicas is set to 1, but searching through the logs for the index only gives the allocation to the same node (as in the log extract). You mean that switching forth and back through the different replicas could trigger the cluster update that the master update sends.

Could this explain the repeated sending of the cluster state with no apparent change, that the master tries to open the shard on different nodes and fails. I can also increase the logging level if it helps (currently having cluster.service on debug).
</comment><comment author="bleskes" created="2014-12-02T12:06:36Z" id="65221667">yeah, I was asking because I kept seeing the same node and I'm not sure why it keeps being re-assigned if you have replicas.  Can you turn on debug logging for gateway.local and trace logging for cluster.service ? I'm curious to see the cluster state between the state.

There are a couple of weird issues in the logs. Not how the cluster state version jumps with 2 every time and there are two processing line but only one cluster state updated - are these logs filtered? can you post the full version?

```
[2014-12-01 17:38:42,654][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:42,697][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [434], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:42,699][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:56,543][DEBUG][cluster.service          ] [NODE1] processing [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
[2014-12-01 17:38:56,584][DEBUG][cluster.service          ] [NODE1] cluster state updated, version [436], source [shard-failed ([data][0], node[P8cffsGbTIKp4lsMoua47Q], [P], s[INITIALIZING]), reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[data][0] failed to fetch index version after copying it over]; nested: CorruptIndexException[[data][0] Preexisting corrupted index [corrupted_33hPC7EHSiOSyd2orBrqQQ] caused by: CorruptIndexException[checksum failed (hardware problem?) : expected=9m391j actual=9mm1qj resource=(org.apache.lucene.store.FSDirectory$FSIndexOutput@334cfac8)]
```
</comment><comment author="miccon" created="2014-12-02T16:55:38Z" id="65263620">You are right, these logs are filtered by the indexname, so that only the relevant updates are being shown. I now activated the logging you suggested in order to reproduce the issue and that I can then provide a full cluster state.

Currently the issue seems resolved, as the master node is not sending the same state repeatedly. I will do some further testing. My current guess is that the master node might be initializing the failed shard again if something different is triggered in the cluster.

Before I had some relocations by the disk threshold occuring. In order to isolate this issue I made sure that there is enough space of the hdds, but then the issue stopped reappearing.
</comment><comment author="clintongormley" created="2015-02-28T04:54:56Z" id="76510677">@miccon any more info on this ticket?
</comment><comment author="miccon" created="2015-03-02T08:54:52Z" id="76676918">@clintongormley the issue did not reappear. But at that time we had many failing shards after the upgrade (#9140) causing many cluster operations on the master. Meanwhile we have upgraded all our indices to lucene 4.10.2
</comment><comment author="clintongormley" created="2015-03-02T10:00:00Z" id="76685099">OK @miccon, thanks for letting us know.  I'm going to close this issue for now then, feel free to reopen if you see similar issues again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> bin/elasticsearch: add help, fix endless loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8729</link><project id="" key="" /><description>This change adds command line help for all options to the es start script.
Both `-h` and `--help` options are accepted.

Also, an endless busy loop in the long options parser was fixed: running the script with a long opt parameter w/o value (e.g. `elasticsearch --buuuurrrnn`) the long option parser would end up in an endless busy loop.

Fixes #2168 
Fixes #7104
</description><key id="50545587">8729</key><summary> bin/elasticsearch: add help, fix endless loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">t-lo</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-01T16:45:31Z</created><updated>2015-03-19T10:13:13Z</updated><resolved>2014-12-02T10:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-01T22:19:32Z" id="65145258">LGTM - @spinscale can you take another look please?
</comment><comment author="dadoonet" created="2014-12-02T07:19:08Z" id="65191857">I think we should have the same help option for elasticsearch.bat.
</comment><comment author="spinscale" created="2014-12-02T08:09:31Z" id="65195458">left one minor question, maybe we can do it a bit more simple here?

Apart from that it looks good.
</comment><comment author="t-lo" created="2014-12-02T08:43:22Z" id="65198524">@dadoonet Let me take a look at this. Time to de-dust my Windows VM I guess :)
</comment><comment author="t-lo" created="2014-12-02T09:08:39Z" id="65201067">@dadoonet I just looked at `elasticsearch.bat` - afaict it doesn't take any command line arguments.
</comment><comment author="dadoonet" created="2014-12-02T09:27:56Z" id="65203190">I think but might be wrong that `%*` means all parameters.

What happened if you run for example `bin\elasticsearch -v`?
</comment><comment author="t-lo" created="2014-12-02T09:44:31Z" id="65205172">Ah, I see, you're right, command line options are passed directly to the Java runtime. The batch file itself however does not seem to take any arguments. Afaict running `elasticsearch.bat -v` does nothing; there does not seem to be a Windows command line equivalent for printing the ES version (other than calling `java org.elasticsearch.Version` directly).
</comment><comment author="dadoonet" created="2014-12-02T09:47:50Z" id="65205540">But the process exits, right?

Still, to me this is not a big concern. As long as we don't document in docs the option `-h`... :)
This could be part of another issue/PR if we need to have it as well in windows.
</comment><comment author="t-lo" created="2014-12-02T09:56:39Z" id="65206530">&gt; But the process exits, right?

It does - however, it's the Java process which does not like the command line argument, not the batch file. 

There's a significant difference between the Linux and Windows start scripts. The Linux script "understands" command line arguments, parses them, and generates a different set of arguments/options for the Elasticsearch Java binary. The Windows script otoh just passes options to the Java binary w/o looking at them. Since the Windows script does not "understand" what it passes to ES it wouldn't make much sense imho to implement command line help / documentation in the batch file.
</comment><comment author="dadoonet" created="2014-12-02T10:07:08Z" id="65207760">Agreed. Basically on windows, there are no command line options apart the `-v`.

+1 for merging it. Will do in a few minutes.
</comment><comment author="dadoonet" created="2014-12-02T10:25:15Z" id="65209916">Thanks!

Merged in master with e92ff00192d13ed2a073ba2afb2420900c70971c and in 1.x (1.5) with ba327d483a501fd4b85f622c62d8b805a28aef61
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>XFilteredQuery doesn't expose its FilterStrategy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8728</link><project id="" key="" /><description>XFilteredQuery doesn't expose its FilterStrategy and this lack of symmetry causes trouble if you want to want to rewrite the query with code.
</description><key id="50543668">8728</key><summary>XFilteredQuery doesn't expose its FilterStrategy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-12-01T16:30:53Z</created><updated>2014-12-02T13:59:02Z</updated><resolved>2014-12-02T13:59:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-01T16:32:30Z" id="65091620">it's gone in master - not sure we should fix this?
</comment><comment author="nik9000" created="2014-12-02T13:59:02Z" id="65233897">If it's gone then no need.  Sorry I didn't check!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elastic search highlight issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8727</link><project id="" key="" /><description>Highlight does not wrap words with tag that has  more characters before search term 
query 
![image](https://cloud.githubusercontent.com/assets/7515281/5248882/bca5e052-794c-11e4-8728-4495290dccb1.png)

response

![image](https://cloud.githubusercontent.com/assets/7515281/5248864/a6cc7e08-794c-11e4-9782-a02b2d46b7ef.png)

mapping 
![image](https://cloud.githubusercontent.com/assets/7515281/5249680/236589a4-7953-11e4-9318-77112a31a365.png)

![image](https://cloud.githubusercontent.com/assets/7515281/5249690/38b9588a-7953-11e4-9d88-aaa9260232db.png)
</description><key id="50543160">8727</key><summary>Elastic search highlight issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">suparnadk</reporter><labels /><created>2014-12-01T16:26:57Z</created><updated>2014-12-01T18:46:32Z</updated><resolved>2014-12-01T18:26:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T18:26:52Z" id="65110627">Hi @suparnadk 

Please ask questions like these in the mailing list instead http://www.elasticsearch.org/community

You may want to look at using the edge ngram **tokenizer** instead of the token filter: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenizer.html#analysis-edgengram-tokenizer
</comment><comment author="nik9000" created="2014-12-01T18:40:57Z" id="65112819">I see highlighting questions on the mailing list every couple of days.  Its
one of the parts of the system that just doesn't always work as you expect
it to.  I wonder if there is anything we can do about that?  Maybe its just
not that big a deal.  I dunno.

On Mon, Dec 1, 2014 at 1:26 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Closed #8727 https://github.com/elasticsearch/elasticsearch/issues/8727.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8727#event-200742501
&gt; .
</comment><comment author="clintongormley" created="2014-12-01T18:46:32Z" id="65113699">@nik9000 agreed - the existing highlighters are just not good enough, and it is something we definitely want to fix.  The first step is getting [LUCENE-2878](https://issues.apache.org/jira/browse/LUCENE-2878) in, so that the postings highlighter can handle positions, as this is the highlighter that offers the most promise.  But that is just the start.  We need a big project which distills out the things that people really need from highlighting, and provides one single interface.  

For now, I'm just tagging all highlighting issues for easier reference later on.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>River plugin hangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8726</link><project id="" key="" /><description>Using Elasticsearch 1.4.1 and running on Ubuntu Trusty with Java 1.7.0_72, the following call in the start method of a river plugin never returns:

``` java
client.admin().indices().prepareExists("common").execute().actionGet();
```

It works using Elasticsearch 1.3.6.

It also works without using `admin()`:

``` java
client.prepareExists("common").execute().actionGet();
```

It also works on MacOS.

Maybe this is related to https://github.com/elasticsearch/elasticsearch-river-couchdb/issues/78 ?
</description><key id="50543047">8726</key><summary>River plugin hangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tinexw</reporter><labels /><created>2014-12-01T16:26:04Z</created><updated>2014-12-01T18:22:09Z</updated><resolved>2014-12-01T18:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T18:22:09Z" id="65109890">@tinexw please see #8448. I'll close this as a duplicate.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Slow recovery after restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8725</link><project id="" key="" /><description>Our cluster reached OOM and one node is recovering very slowly after restart.

It has a lot of data locally, but doesn't seem to care about it to restore locally:

```
web592 ~ # du -ms /var/lib/elasticsearch/statistics/nodes/*
1292882 /var/lib/elasticsearch/statistics/nodes/0
```

`iostat` shows that disks are mostly idle. Cluster is on 1.4.1 with aws plugin for s3 backups. Other 4 nodes recovered their state much more quickly and now cluster is rebalancing.

``````
[2014-12-01 19:04:45,186][INFO ][node                     ] [statistics02] version[1.4.1], pid[87174], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-01 19:04:45,186][INFO ][node                     ] [statistics02] initializing ...
[2014-12-01 19:04:45,197][INFO ][plugins                  ] [statistics02] loaded [cloud-aws], sites []
[2014-12-01 19:04:46,729][DEBUG][discovery.zen.elect      ] [statistics02] using minimum_master_nodes [3]
[2014-12-01 19:04:46,730][DEBUG][discovery.zen.ping.multicast] [statistics02] using group [224.2.2.4], with port [54328], ttl [3], and address [null]
[2014-12-01 19:04:46,733][DEBUG][discovery.zen.ping.unicast] [statistics02] using initial hosts [statistics01.es.pretender.local, statistics02.es.pretender.local, statistics03.es.pretender.local, statistics04.es.pretender.local, statistics05.es.pretender.local], with concurrent_connects [10]
[2014-12-01 19:04:46,743][DEBUG][discovery.zen            ] [statistics02] using ping.timeout [3s], join.timeout [1m], master_election.filter_client [true], master_election.filter_data [false]
[2014-12-01 19:04:46,744][DEBUG][discovery.zen.fd         ] [statistics02] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2014-12-01 19:04:46,746][DEBUG][discovery.zen.fd         ] [statistics02] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2014-12-01 19:04:47,024][DEBUG][gateway.local            ] [statistics02] using initial_shards [quorum], list_timeout [30s]
[2014-12-01 19:04:47,026][DEBUG][indices.recovery         ] [statistics02] using max_bytes_per_sec[200mb], concurrent_streams [5], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2014-12-01 19:04:47,088][DEBUG][gateway.local.state.meta ] [statistics02] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
[2014-12-01 19:04:47,399][DEBUG][gateway.local.state.meta ] [statistics02] took 310ms to load state
[2014-12-01 19:04:49,314][DEBUG][gateway.local.state.shards] [statistics02] took 1.9s to load started shards state
[2014-12-01 19:04:49,324][INFO ][node                     ] [statistics02] initialized
[2014-12-01 19:04:49,324][INFO ][node                     ] [statistics02] starting ...
[2014-12-01 19:04:49,431][INFO ][transport                ] [statistics02] bound_address {inet[/192.168.2.82:9300]}, publish_address {inet[/192.168.2.82:9300]}
[2014-12-01 19:04:49,442][INFO ][discovery                ] [statistics02] statistics/putJb40cQsSRK8HLRaXcBQ
[2014-12-01 19:04:52,459][DEBUG][discovery.zen            ] [statistics02] filtered ping responses: (filter_client[true], filter_data[false])
    --&gt; ping_response{node [[statistics05][S9rUtu4vQtiOknn0vz-04Q][web599][inet[/192.168.2.88:9300]]], id[60], master [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], hasJoinedOnce [true], cluster_name[statistics]}
    --&gt; ping_response{node [[statistics01][rgRCHAg_QoiRDpii78xIrw][web190][inet[/192.168.0.190:9300]]], id[69], master [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], hasJoinedOnce [true], cluster_name[statistics]}
    --&gt; ping_response{node [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], id[140], master [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], hasJoinedOnce [true], cluster_name[statistics]}
    --&gt; ping_response{node [[statistics04][zDkIvouVSfSRkwPRRVyZbw][web467][inet[/192.168.1.212:9300]]], id[12], master [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], hasJoinedOnce [true], cluster_name[statistics]}
[2014-12-01 19:05:02,432][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24539
[2014-12-01 19:05:02,434][DEBUG][discovery.zen.fd         ] [statistics02] [master] restarting fault detection against master [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], reason [new cluster state received and we are monitoring the wrong master [null]]
[2014-12-01 19:05:02,435][DEBUG][discovery.zen            ] [statistics02] got first state from fresh master [4OG7n-hnTCGUs-81MDxTwQ]
[2014-12-01 19:05:02,435][INFO ][cluster.service          ] [statistics02] detected_master [statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]], added {[statistics05][S9rUtu4vQtiOknn0vz-04Q][web599][inet[/192.168.2.88:9300]],[statistics01][rgRCHAg_QoiRDpii78xIrw][web190][inet[/192.168.0.190:9300]],[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]],[statistics04][zDkIvouVSfSRkwPRRVyZbw][web467][inet[/192.168.1.212:9300]],}, reason: zen-disco-receive(from master [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]])
[2014-12-01 19:05:02,461][INFO ][cluster.routing.allocation.decider] [statistics02] updating [cluster.routing.allocation.disk.watermark.low] to [90%]
[2014-12-01 19:05:02,461][INFO ][cluster.routing.allocation.decider] [statistics02] updating [cluster.routing.allocation.disk.watermark.high] to [95%]
[2014-12-01 19:05:03,298][INFO ][http                     ] [statistics02] bound_address {inet[/192.168.2.82:9200]}, publish_address {inet[/192.168.2.82:9200]}
[2014-12-01 19:05:03,298][INFO ][node                     ] [statistics02] started
[2014-12-01 19:05:12,238][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24540
[2014-12-01 19:05:12,608][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][4] starting recovery from local ...
[2014-12-01 19:05:12,618][DEBUG][index.gateway            ] [statistics02] [statistics-20131231][2] starting recovery from local ...
[2014-12-01 19:05:12,632][DEBUG][index.gateway            ] [statistics02] [statistics-20140505][2] starting recovery from local ...
[2014-12-01 19:05:12,646][DEBUG][index.gateway            ] [statistics02] [statistics-20140726][4] starting recovery from local ...
[2014-12-01 19:05:12,651][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][4] recovery completed from [local], took [43ms]
[2014-12-01 19:05:12,740][DEBUG][index.gateway            ] [statistics02] [statistics-20140505][2] recovery completed from [local], took [108ms]
[2014-12-01 19:05:12,740][DEBUG][index.gateway            ] [statistics02] [statistics-20131231][2] recovery completed from [local], took [122ms]
[2014-12-01 19:05:12,767][DEBUG][index.gateway            ] [statistics02] [statistics-20140726][4] recovery completed from [local], took [121ms]
[2014-12-01 19:05:18,378][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24541
[2014-12-01 19:05:18,421][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][0] starting recovery from local ...
[2014-12-01 19:05:18,424][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][0] recovery completed from [local], took [3ms]
[2014-12-01 19:05:18,430][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][3] starting recovery from local ...
[2014-12-01 19:05:18,433][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][3] recovery completed from [local], took [3ms]
[2014-12-01 19:05:18,441][DEBUG][index.gateway            ] [statistics02] [statistics-20140816][2] starting recovery from local ...
[2014-12-01 19:05:18,505][DEBUG][index.gateway            ] [statistics02] [statistics-20140816][2] recovery completed from [local], took [64ms]
[2014-12-01 19:05:24,440][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24542
[2014-12-01 19:05:24,472][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][2] starting recovery from local ...
[2014-12-01 19:05:24,475][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][2] recovery completed from [local], took [3ms]
[2014-12-01 19:05:24,481][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][1] starting recovery from local ...
[2014-12-01 19:05:24,484][DEBUG][index.gateway            ] [statistics02] [statistics-20141101-dv][1] recovery completed from [local], took [3ms]
[2014-12-01 19:05:24,490][DEBUG][index.gateway            ] [statistics02] [statistics-20140108][2] starting recovery from local ...
[2014-12-01 19:05:24,528][DEBUG][index.gateway            ] [statistics02] [statistics-20140108][2] recovery completed from [local], took [38ms]
[2014-12-01 19:05:28,287][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24543
[2014-12-01 19:05:28,769][DEBUG][indices.recovery         ] [statistics02] [statistics-20140226][1] recovery completed from [[statistics05][S9rUtu4vQtiOknn0vz-04Q][web599][inet[/192.168.2.88:9300]]], took [394ms]
[2014-12-01 19:05:32,681][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24544
[2014-12-01 19:05:38,058][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24545
[2014-12-01 19:05:42,821][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24546
[2014-12-01 19:05:52,443][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24547
[2014-12-01 19:06:02,245][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24548
[2014-12-01 19:06:15,962][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24549
[2014-12-01 19:06:23,949][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24550
[2014-12-01 19:06:24,519][DEBUG][indices.recovery         ] [statistics02] [statistics-20140527][4] recovery completed from [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], took [55.6s]
[2014-12-01 19:06:24,520][DEBUG][indices.recovery         ] [statistics02] [statistics-20140413][4] recovery completed from [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], took [45.9s]
[2014-12-01 19:06:28,723][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24551
[2014-12-01 19:06:34,943][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24552
[2014-12-01 19:06:50,748][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24553
[2014-12-01 19:06:58,276][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24554
[2014-12-01 19:06:58,745][DEBUG][indices.recovery         ] [statistics02] [statistics-20140512][3] recovery completed from [[statistics03][4OG7n-hnTCGUs-81MDxTwQ][web245][inet[/192.168.0.245:9300]]], took [22.7s]
[2014-12-01 19:07:03,330][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24555
[2014-12-01 19:07:08,646][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24556
[2014-12-01 19:07:08,742][DEBUG][indices.recovery         ] [statistics02] [statistics-20140823][3] recovery completed from [[statistics01][rgRCHAg_QoiRDpii78xIrw][web190][inet[/192.168.0.190:9300]]], took [56ms]
[2014-12-01 19:07:13,350][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24557
[2014-12-01 19:07:19,349][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24558
[2014-12-01 19:07:20,403][DEBUG][indices.recovery         ] [statistics02] [statistics-20131121][4] recovery completed from [[statistics01][rgRCHAg_QoiRDpii78xIrw][web190][inet[/192.168.0.190:9300]]], took [7s]
[2014-12-01 19:07:27,139][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24559
[2014-12-01 19:07:27,587][DEBUG][indices.recovery         ] [statistics02] [statistics-20140723][0] recovery completed from [[statistics04][zDkIvouVSfSRkwPRRVyZbw][web467][inet[/192.168.1.212:9300]]], took [404ms]
[2014-12-01 19:07:30,786][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24560
[2014-12-01 19:07:35,493][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24561
[2014-12-01 19:07:36,207][DEBUG][indices.recovery         ] [statistics02] [statistics-20140326][1] recovery completed from [[statistics05][S9rUtu4vQtiOknn0vz-04Q][web599][inet[/192.168.2.88:9300]]], took [685ms]
[2014-12-01 19:07:39,615][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24562
[2014-12-01 19:07:40,124][DEBUG][indices.recovery         ] [statistics02] [statistics-20140703][2] recovery completed from [[statistics04][zDkIvouVSfSRkwPRRVyZbw][web467][inet[/192.168.1.212:9300]]], took [475ms]
[2014-12-01 19:07:44,138][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24563
[2014-12-01 19:07:47,642][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24564
[2014-12-01 19:07:52,212][DEBUG][discovery.zen.publish    ] [statistics02] received cluster state version 24565```
``````
</description><key id="50534117">8725</key><summary>Slow recovery after restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-12-01T15:12:01Z</created><updated>2014-12-01T18:18:34Z</updated><resolved>2014-12-01T18:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-12-01T15:15:03Z" id="65078392">![screen shot 2014-12-01 at 18 14 00](https://cloud.githubusercontent.com/assets/89186/5247708/dd6d4148-7985-11e4-9a5f-2059cf1eda29.png)

Here `statistics02` reports that is has only 51gb of data.
</comment><comment author="clintongormley" created="2014-12-01T18:14:28Z" id="65108673">Hi @bobrik 

I'm assuming that this index has been around for a while? If so, the primary/replicas of each shard have diverged from each other, and the old shards on the recovering node probably have very few segments in common with the copies on other nodes.

This means it needs to copy pretty much all segments from scratch.  Issue #6069 is indeed (at least part of) the solution for this. Also see https://github.com/elasticsearch/elasticsearch/issues/7288

I think this issue is a duplicate of the ones already mentioned and can be closed?
</comment><comment author="bobrik" created="2014-12-01T18:18:34Z" id="65109314">99% of indices are cold and optimized to 2 segments. Closing this since it's really duplicate of #6069.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>bin/elasticsearch: add help, fix endless loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8724</link><project id="" key="" /><description>This change adds command line help for all options to the es start script.
Both `-h` and `--help` options are accepted.

Also, an endless busy loop in the long options parser was fixed: running the
script with a long opt parameter w/o value (e.g. `elasticsearch --buuuurrrnn`)
the long option parser would end up in an endless busy loop.

Fixes #2168

Signed-off-by: Thilo Fromm github@thilo-fromm.de
</description><key id="50529503">8724</key><summary>bin/elasticsearch: add help, fix endless loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">t-lo</reporter><labels /><created>2014-12-01T14:30:20Z</created><updated>2014-12-01T16:48:20Z</updated><resolved>2014-12-01T16:39:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="t-lo" created="2014-12-01T16:48:20Z" id="65094476">This pull request was from my master branch; please ignore it and use #8729 .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure shards are identical after recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8723</link><project id="" key="" /><description>Today we don't check if the recovery target has all the
files that we expect there after the recovery. This commit
adds aditional safety to ensure all files are present with the
correct checksums on recovery finalization.
</description><key id="50528142">8723</key><summary>Ensure shards are identical after recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-01T14:16:43Z</created><updated>2015-06-07T17:04:48Z</updated><resolved>2014-12-02T13:15:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-01T15:47:16Z" id="65083524">second iteration - I moved the meat into Store and added a unittest for it. Also moved the actual comparison into phase1 of the recovery which is cleaner, thanks @dakrone !!! 

Reviews are welcome
</comment><comment author="s1monw" created="2014-12-02T11:33:19Z" id="65217597">reviews anyone?
</comment><comment author="bleskes" created="2014-12-02T12:56:09Z" id="65226620">LGTM. Left two little comments.
</comment><comment author="s1monw" created="2014-12-02T13:01:25Z" id="65227163">pushed changes... 
</comment><comment author="bleskes" created="2014-12-02T13:02:06Z" id="65227258">LGTM^2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/store/StoreFileMetaData.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryCleanFilesRequest.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/recovery/ShardRecoveryHandler.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file></files><comments><comment>[RECOVERY] Ensure shards are identical after recovery</comment></comments></commit></commits></item><item><title>Deprecate duplications of methods: hits() and getHits(). According names...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8722</link><project id="" key="" /><description>..., decriptions methods are equal.
</description><key id="50525085">8722</key><summary>Deprecate duplications of methods: hits() and getHits(). According names...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">ymihay</reporter><labels /><created>2014-12-01T13:43:38Z</created><updated>2015-06-24T08:27:43Z</updated><resolved>2015-06-24T08:27:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-06-24T08:27:15Z" id="114783941">@ymihay Thanks for your suggestion and I'm very sorry this took some time to reply, but this PR somehow slipped through the cracks. Deprecating the duplicated method makes perfect sense as you described. Unfortunately there are many more places in the code that historically have both getter name variants and we'd like to make this a bigger change including other cases like this as well. So I'm closing this issue for now, which doesn't mean we want to make these changes at some point.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>java: QueryBuilders cleanup: remove deprecated </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8721</link><project id="" key="" /><description>Related to #8667:

Some QueryBuilders have been deprecated in 1.x branches. We removed them in 2.0.
## Removed
- `textPhrase(...)`
- `textPhrasePrefix(...)`
- `textPhrasePrefixQuery(...)`
- `filtered(...)`
- `inQuery(...)`
- `commonTerms(...)`
- `queryString(...)`
- `simpleQueryString(...)`
</description><key id="50524483">8721</key><summary>java: QueryBuilders cleanup: remove deprecated </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-12-01T13:36:54Z</created><updated>2014-12-03T15:08:51Z</updated><resolved>2014-12-03T15:08:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file></files><comments><comment>java: QueryBuilders cleanup: remove deprecated</comment></comments></commit></commits></item><item><title>Be more resilient to partial network partitions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8720</link><project id="" key="" /><description>When a node is experience network issues, the master detects it and removes the node from the cluster. That cause all ongoing recoveries from and to that nodes to be stopped and a new location is found for the relevant shards. However, in the case partial network partition, where there a connectivity issues between the source and target node of a recovery but _not_ between those nodes and the current master things may go wrong. While the nodes successfully restore the connection, the on going recoveries may have encounter issues.

This PR adds a test that simulate disconnecting nodes and dropping requests during the various stages of recovery and solves all the issues that were raised by it. In short:

1) On going recoveries will be scheduled for retry upon network disconnect. The default retry period is 5s (cross node connections are checked every 10s by default).
2) Sometimes the disconnect happens after the target engine has started (but the shard is still in recovery). For simplicity, I opted to restart the recovery from scratch (where little to no files will be copied again, because there were just synced).
3) To protected against dropped requests, a Recovery Monitor was added that fails a recovery if no progress has been made in the last 30m (by default), which is equivalent to the long time outs we use in recovery requests.
4) When a shard fails on a node, we try to assign it to another node. If no such node is available, the shard will remain unassigned, causing the target node to clean any in memory state for it (files on disk remain). At the moment the shard will remain unassigned until another cluster state change happens, which will re-assigned it to the node in question but if no such change happens the shard will remain stuck at unassigned. The commits adds an extra delayed reroute in such cases to make sure the shard will be reassinged
5) Moved all recovery related settings to the RecoverySettings.

I'd love it if extra focus was give to the engine changes while reviewing - I'm not 100% familiar with the implications of the code to the underlying lucene state.

There is also one nocommit regarding the Java serializability of a Version object (used by DiscoveryNode). We rely on Java serialization for exceptions and this makes the ConnectTransportException unserializable because of it's DiscoveryNode field. This can be fixed in another change, but we need to discuss how.

One more todo left - add a reference to the resiliency page
</description><key id="50522914">8720</key><summary>Be more resilient to partial network partitions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-12-01T13:19:17Z</created><updated>2015-06-07T17:04:54Z</updated><resolved>2015-01-10T14:28:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-12-01T16:08:30Z" id="65087244">left a bunch of comments - I love the test :)
</comment><comment author="bleskes" created="2014-12-02T11:13:54Z" id="65215416">@s1monw I pushed and update based on feedback. Note that I also modified the CancelableThreads a bit when extracting it. It's not identical.
</comment><comment author="s1monw" created="2014-12-02T11:24:38Z" id="65216535">@bleskes I looked at the two commits. I like the second one but do not like the first one. I think your changes to the creation model makes thing even more complex and error prone than they are already. Then engine should not be started / not started / stopped and have some state in between can might or might not be cleaned up. I think we should have a dedicated class that we might even can use without all the guice stuff that initializes everything it needs in the constructor. It might even create a new instance if we update settings etc. and tear down anything during that time. That is much cleaner and then you can do the start stop logic cleanup on top.
</comment><comment author="bleskes" created="2014-12-10T12:08:19Z" id="66442750">@s1monw I rebased this against master and adapted it to the changes in #8784 . I also moved all recovery settings to one place - i.e., RecoverySettings
</comment><comment author="s1monw" created="2014-12-15T11:07:34Z" id="66979383">I left some more comments
</comment><comment author="bleskes" created="2014-12-15T21:31:36Z" id="67069584">@s1monw I pushed another update. Responded to some comments as well.
</comment><comment author="bleskes" created="2015-01-09T12:44:03Z" id="69329678">@s1monw I rebased and squashed against the latest master. Would be great if you can give it another round.
</comment><comment author="s1monw" created="2015-01-09T12:49:44Z" id="69330167">cool stuff LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsTests.java</file></files><comments><comment>Test: add awaitFix to SearchWithRandomExceptionsTests</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>src/test/java/org/elasticsearch/recovery/RecoveriesCollectionTests.java</file></files><comments><comment>Recovery: update access time of ongoing recoveries</comment></comments></commit><commit><files /><comments><comment>Add #8720 to the resiliency page</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySettings.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/recovery/ShardRecoveryHandler.java</file><file>src/test/java/org/elasticsearch/VersionTests.java</file><file>src/test/java/org/elasticsearch/cluster/node/DiscoveryNodeTests.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file><file>src/test/java/org/elasticsearch/recovery/RecoverySettingsTest.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Recovery: be more resilient to partial network partitions</comment></comments></commit></commits></item><item><title>failed to flush after setting shard to inactive  org.elasticsearch.index.engine.FlushFailedEngineException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8719</link><project id="" key="" /><description>can you help me?

This is the error which I got:

[2014-12-01 11:20:14,831][INFO ][node                     ] [Human Torch] started 
[2014-12-01 11:51:12,002][WARN ][index.engine.internal    ] [Human Torch] [bifs_index-2014-07-18][0] failed to flush after setting shard to inactive 
org.elasticsearch.index.engine.FlushFailedEngineException: [bifs_index-2014-07-18][0] Flush failed 
        at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:804) 
        at org.elasticsearch.index.engine.internal.InternalEngine.updateIndexingBufferSize(InternalEngine.java:223) 
        at org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:201) 
        at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:437) 
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) 
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317) 
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150) 
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98) 
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181) 
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619) 
Caused by: java.io.IOException: Map failed 
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:758) 
        at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283) 
        at org.apache.lucene.store.MMapDirectory$MMapIndexInput.&lt;init&gt;(MMapDirectory.java:228) 
        at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195) 
        at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80) 
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:473) 
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.&lt;init&gt;(CompressingStoredFieldsReader.java:130) 
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113) 
        at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:129) 
        at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:96) 
        at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141) 
        at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:235) 
        at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:101) 
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:382) 
        at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111) 
        at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89) 
        at org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1471) 
        at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:788) 
        ... 12 more 
Caused by: java.lang.OutOfMemoryError: Map failed 
        at sun.nio.ch.FileChannelImpl.map0(Native Method) 
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:755) 
        ... 29 more 
[2014-12-01 11:51:12,819][WARN ][index.engine.internal    ] [Human Torch] [bifs_index-2014-07-18][1] failed to flush after setting shard to inactive 
org.elasticsearch.index.engine.FlushFailedEngineException: [bifs_index-2014-07-18][1] Flush failed 
        at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:804) 
        at org.elasticsearch.index.engine.internal.InternalEngine.updateIndexingBufferSize(InternalEngine.java:223) 
        at org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:201) 
        at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:437) 
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) 
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317) 
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150) 
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98) 
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181) 
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619) 
Caused by: java.io.IOException: Map failed 
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:758) 
        at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283) 
        at org.apache.lucene.store.MMapDirectory$MMapIndexInput.&lt;init&gt;(MMapDirectory.java:228) 
        at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195) 
        at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80) 
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:473) 
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.&lt;init&gt;(CompressingStoredFieldsReader.java:130) 
        at org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113) 
        at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:129) 
        at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:96) 
        at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141) 
        at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:235) 
        at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:101) 
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:382) 
        at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111) 
        at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89) 
        at org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1471) 
        at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:788) 
        ... 12 more 
Caused by: java.lang.OutOfMemoryError: Map failed 
        at sun.nio.ch.FileChannelImpl.map0(Native Method) 
        at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:755) 
        ... 29 mor
</description><key id="50511456">8719</key><summary>failed to flush after setting shard to inactive  org.elasticsearch.index.engine.FlushFailedEngineException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">balibaba12</reporter><labels /><created>2014-12-01T11:00:18Z</created><updated>2015-11-21T22:02:28Z</updated><resolved>2015-11-21T22:02:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-12-01T11:12:39Z" id="65050448">Which version of Elasticsearch?  Newer versions (that include Lucene &gt;=
4.9.0) improve this error message...

You are likely hitting a process limit ... double check that you don't have
a virtual memory limit (ulimit -v) or allocated memory limit (ulimit -m):
both should say "unlimited".  If you have many, many shards you may be
hitting a limit on maximum number of per-process maps; see sysctl
vm.max_map_count if you are on Linux.

Mike McCandless

http://blog.mikemccandless.com

On Mon, Dec 1, 2014 at 6:00 AM, balibaba12 notifications@github.com wrote:

&gt; can you help me?
&gt; 
&gt; This is the error which I got:
&gt; 
&gt; [2014-12-01 11:20:14,831][INFO ][node ] [Human Torch] started
&gt; [2014-12-01 11:51:12,002][WARN ][index.engine.internal ] [Human Torch]
&gt; [bifs_index-2014-07-18][0] failed to flush after setting shard to inactive
&gt; org.elasticsearch.index.engine.FlushFailedEngineException:
&gt; [bifs_index-2014-07-18][0] Flush failed
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:804)
&gt; 
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.updateIndexingBufferSize(InternalEngine.java:223)
&gt; 
&gt; at
&gt; org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:201)
&gt; 
&gt; at
&gt; org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:437)
&gt; 
&gt; at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
&gt; at
&gt; java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
&gt; at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
&gt; 
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
&gt; 
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
&gt; 
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
&gt; 
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
&gt; 
&gt; at java.lang.Thread.run(Thread.java:619)
&gt; Caused by: java.io.IOException: Map failed
&gt; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:758)
&gt; at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283)
&gt; at
&gt; org.apache.lucene.store.MMapDirectory$MMapIndexInput.(MMapDirectory.java:228)
&gt; 
&gt; at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195)
&gt; at
&gt; org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
&gt; at
&gt; org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:473)
&gt; 
&gt; at
&gt; org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.(CompressingStoredFieldsReader.java:130)
&gt; 
&gt; at
&gt; org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113)
&gt; 
&gt; at
&gt; org.apache.lucene.index.SegmentCoreReaders.(SegmentCoreReaders.java:129)
&gt; at org.apache.lucene.index.SegmentReader.(SegmentReader.java:96)
&gt; at
&gt; org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141)
&gt; 
&gt; at
&gt; org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:235)
&gt; 
&gt; at
&gt; org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:101)
&gt; 
&gt; at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:382)
&gt; at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111)
&gt; at org.apache.lucene.search.SearcherManager.(SearcherManager.java:89)
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1471)
&gt; 
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:788)
&gt; 
&gt; ... 12 more
&gt; Caused by: java.lang.OutOfMemoryError: Map failed
&gt; at sun.nio.ch.FileChannelImpl.map0(Native Method)
&gt; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:755)
&gt; ... 29 more
&gt; [2014-12-01 11:51:12,819][WARN ][index.engine.internal ] [Human Torch]
&gt; [bifs_index-2014-07-18][1] failed to flush after setting shard to inactive
&gt; org.elasticsearch.index.engine.FlushFailedEngineException:
&gt; [bifs_index-2014-07-18][1] Flush failed
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:804)
&gt; 
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.updateIndexingBufferSize(InternalEngine.java:223)
&gt; 
&gt; at
&gt; org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:201)
&gt; 
&gt; at
&gt; org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:437)
&gt; 
&gt; at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
&gt; at
&gt; java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
&gt; at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
&gt; 
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
&gt; 
&gt; at
&gt; java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
&gt; 
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
&gt; 
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
&gt; 
&gt; at java.lang.Thread.run(Thread.java:619)
&gt; Caused by: java.io.IOException: Map failed
&gt; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:758)
&gt; at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283)
&gt; at
&gt; org.apache.lucene.store.MMapDirectory$MMapIndexInput.(MMapDirectory.java:228)
&gt; 
&gt; at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195)
&gt; at
&gt; org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
&gt; at
&gt; org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:473)
&gt; 
&gt; at
&gt; org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.(CompressingStoredFieldsReader.java:130)
&gt; 
&gt; at
&gt; org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113)
&gt; 
&gt; at
&gt; org.apache.lucene.index.SegmentCoreReaders.(SegmentCoreReaders.java:129)
&gt; at org.apache.lucene.index.SegmentReader.(SegmentReader.java:96)
&gt; at
&gt; org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141)
&gt; 
&gt; at
&gt; org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:235)
&gt; 
&gt; at
&gt; org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:101)
&gt; 
&gt; at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:382)
&gt; at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111)
&gt; at org.apache.lucene.search.SearcherManager.(SearcherManager.java:89)
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1471)
&gt; 
&gt; at
&gt; org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:788)
&gt; 
&gt; ... 12 more
&gt; Caused by: java.lang.OutOfMemoryError: Map failed
&gt; at sun.nio.ch.FileChannelImpl.map0(Native Method)
&gt; at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:755)
&gt; ... 29 mor
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8719.
</comment><comment author="balibaba12" created="2014-12-01T11:24:45Z" id="65051646">the version is 1.1.2
</comment><comment author="balibaba12" created="2014-12-01T11:28:10Z" id="65052030">i will have a try  tomorrow 
thank you
</comment><comment author="balibaba12" created="2014-12-03T05:10:24Z" id="65357037">it dose not work..........
i have set
ulimit -l unlimted
ulimit -v unlimted
ulimit -m unlimted

but when i index docs into es, the virtual memory will grow more and more larger,and will hit the swap limit,and the virtual memory  seem to never been freed (if i delete the index,the virtual memory will be freed) 
my machine is virtual machine suse linux .
</comment><comment author="clintongormley" created="2015-11-21T22:02:28Z" id="158685226">Lots has changed in the last year, so I'll close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI failure] GetIndexBackwardsCompatibilityTests.testGetAliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8718</link><project id="" key="" /><description>See http://build-us-00.elasticsearch.org/job/es_bwc_1x/5370/CHECK_BRANCH=tags%2Fv1.1.2,jdk=JDK7,label=bwc/testReport/junit/org.elasticsearch.bwcompat/GetIndexBackwardsCompatibilityTests/testGetAliases/

after wading through the logs the following happens here:
- One node starts up with 1.x version
- Two external nodes with 1.1.2 startup
- An external transport client tries to connect to an external node and fails

The test reproduces, using 1.x branch and 1.1.2 snapshot on my machine, and I am not sure what happens there yet.

The `IllegalStateException` is a followup exception due to exception handling on the wrong thread on netty level in earlier versions and can be ignored.

Call to reproduce after installing 1.1.2

```
mvn test -Dtests.seed=C459A34B357EFF9A -Dtests.class=org.elasticsearch.bwcompat.GetIndexBackwardsCompatibilityTests -Dtests.bwc=true -Dtests.filter=@backwards -Dtests.method="testGetAliases" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.heap.size=512m -Dtests.bwc=true -Dtests.bwc.version=1.1.2 -Dtests.locale=iw -Dtests.timezone=Asia/Yakutsk -Dtests.processors=8
```
</description><key id="50506518">8718</key><summary>[CI failure] GetIndexBackwardsCompatibilityTests.testGetAliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>jenkins</label></labels><created>2014-12-01T10:06:59Z</created><updated>2015-10-23T07:24:52Z</updated><resolved>2015-10-23T07:24:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T07:24:52Z" id="150496347">this hasn't failed in a long time.... closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update completion-suggest.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8717</link><project id="" key="" /><description /><key id="50497411">8717</key><summary>Update completion-suggest.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jhsbeat</reporter><labels /><created>2014-12-01T08:07:32Z</created><updated>2014-12-01T08:23:45Z</updated><resolved>2014-12-01T08:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>[Doc] Java API: add search templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8716</link><project id="" key="" /><description>Closes #7321.
</description><key id="50469253">8716</key><summary>[Doc] Java API: add search templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-30T20:20:03Z</created><updated>2014-12-03T15:47:34Z</updated><resolved>2014-12-03T15:47:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Upsert does not use ttl value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8715</link><project id="" key="" /><description>When running an upsert which defines a ttl value, the ttl value is set to null and ignored.

Related to #3256#issuecomment-64963409
</description><key id="50465429">8715</key><summary>Upsert does not use ttl value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Index APIs</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-11-30T18:11:35Z</created><updated>2015-07-07T08:55:39Z</updated><resolved>2015-07-06T20:00:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-11-30T18:30:24Z" id="64994464">Note that there was also an issue introduced by https://github.com/elasticsearch/elasticsearch/commit/e6b459cb9f4256a6c26fde5177ee88a1296cf8d8#diff-f8e636638c1b6e3475a036e2193c5852R204 which happens when you run an upsert with a ttl and a script as an upsert.
The ttl value is set to `null` and evaluated from the script which may not provide it.

This PR patch this as well.

I also changed method signatures for `UpdateRequest#scriptedUpsert(boolean)` and `UpdateRequest#docAsUpsert(boolean)` as they were not returning `this`.

@clintongormley Still have a question which might be part of the same PR. What should happen if the user set the TTL value as an upsert option and set it as well using the script? Should we reject the operation? With this patch the value set within upsert option as a precedence on the script. Basically we ignore ttl set within the script.
</comment><comment author="s1monw" created="2014-12-03T16:31:17Z" id="65440502">rather than having an integration test here can we maybe have a unittest for the upsert helper?
</comment><comment author="TheDeveloper" created="2015-03-25T18:19:14Z" id="86157542">I am running into this issue also when the TTL is set as a default on the index, not on the update request.

Is this patch likely to get merged soon?

Am I right in saying the workaround for now is to ensure the new TTL is set via the script's `_ttl` global?
</comment><comment author="dadoonet" created="2015-04-29T13:07:07Z" id="97420463">@s1monw I rebased and added a new commit to move from integration tests to unit tests.
Let me know what you think.
</comment><comment author="s1monw" created="2015-04-29T16:12:00Z" id="97484098">left minor comments looks good in general
</comment><comment author="dadoonet" created="2015-07-06T12:54:51Z" id="118846228">@s1monw I rebased and updated the code based on your comments. Let me know if I can merge this in.
</comment><comment author="s1monw" created="2015-07-06T13:40:13Z" id="118856854">left a cosmetic comment. Feel free to push once fixed... in other words LGTM
</comment><comment author="dadoonet" created="2015-07-06T15:15:22Z" id="118887239">@s1monw I updated the code because tests were failing with my PR.

I changed 2 lines:
- change `long` to `Long`: https://github.com/dadoonet/elasticsearch/blob/b4aa472716fa2dec703591fd115cfcd607014872/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java#L93
- if (ttl &lt; 0) we need to pass `null` to the IndexRequest https://github.com/dadoonet/elasticsearch/blob/b4aa472716fa2dec703591fd115cfcd607014872/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java#L129

If we don't do it, it's rejected by https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/index/IndexRequest.java#L327-329

Do you agree with those last changes? All tests pass on my laptop now.
</comment><comment author="s1monw" created="2015-07-06T15:18:13Z" id="118888267">&gt; change long to Long: https://github.com/dadoonet/elasticsearch/blob/b4aa472716fa2dec703591fd115cfcd607014872/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java#L93

why this change?
</comment><comment author="dadoonet" created="2015-07-06T15:19:21Z" id="118888566">Because this line: https://github.com/dadoonet/elasticsearch/blob/b4aa472716fa2dec703591fd115cfcd607014872/core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java#L105 was making a NPE. You can't set `null` to a `long`
</comment><comment author="s1monw" created="2015-07-06T19:29:13Z" id="118970710">fair enough.... LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better error message for update document with parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8714</link><project id="" key="" /><description>If I update a document that has a parent and I do not provide a routing value elasticsearch fails. This is expected (see #3334) but imo the error message should not be a null pointer exception but something that tells the cause of the failure.

Here is a script that reproduces the error:

``` shell
#!/bin/sh

curl -XDELETE "http://localhost:9200/bugreport"
curl -XPUT "http://localhost:9200/bugreport"
echo ""
echo "create tasks:"
curl -XPUT "http://localhost:9200/bugreport/_mapping/tasks" -d '{
"tasks": {
    "_id": {"path": "id", "index": "not_analyzed"},
    "properties": {
        "id": {"type": "string", "index": "not_analyzed"}
    }
}}'
echo ""
echo "create task_comments:"
curl -XPUT "http://localhost:9200/bugreport/_mapping/task_comments" -d '{
"task_comments": {
    "_parent": {"type": "tasks"},
    "_routing": {"required": true},
    "properties": {
        "taskId": {"type": "string", "index": "not_analyzed"},
        "message": {"type": "string"}
    }
}}'
echo ""
echo "add task record:"
curl -XPUT "http://localhost:9200/bugreport/tasks/cebe%2Ftest%2F6?op_type=create" -d '{
    "title":"test",
    "description":"test",
    "id":"cebe\/test\/6"
}'
echo ""
echo "add task_comments record:"
curl -XPUT "http://localhost:9200/bugreport/task_comments/1?op_type=create&amp;parent=cebe%2Ftest%2F6" -d '{
    "taskId":"cebe\/test\/6",
    "authorId":"cebe",
    "message":"asdasd"
}'
echo ""
echo "update it:"
curl -XPOST "http://localhost:9200/bugreport/task_comments/_bulk" -d '
{"update":{"_id":"1","_type":"task_comments","_index":"bugreport"}}
{"doc":{"message":"as asdf","updated":"2014-10-24 21:59:18"}}

'
#{"update":{"_id":"1","_routing": "cebe\/test\/6","_type":"task_comments","_index":"bugreport"}}

```

output:

```
{"acknowledged":true}{"acknowledged":true}
create tasks:
{"acknowledged":true}
create task_comments:
{"acknowledged":true}
add task record:
{"_index":"bugreport","_type":"tasks","_id":"cebe/test/6","_version":1,"created":true}
add task_comments record:
{"_index":"bugreport","_type":"task_comments","_id":"1","_version":1,"created":true}
update it:
{"error":"NullPointerException[null]","status":500}
```
</description><key id="50464943">8714</key><summary>Better error message for update document with parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cebe</reporter><labels /><created>2014-11-30T17:55:27Z</created><updated>2014-12-01T16:38:25Z</updated><resolved>2014-12-01T10:01:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T10:01:36Z" id="65042579">Hi @cebe 

Thanks for opening the issue. This was fixed in v1.4.1 by #8506.

The update statement now returns:

```
{
   "took": 1,
   "errors": true,
   "items": [
      {
         "task_comments": {
            "_index": "bugreport",
            "_type": "task_comments",
            "_id": "1",
            "status": 400,
            "error": "routing is required for this item"
         }
      }
   ]
}
```
</comment><comment author="cebe" created="2014-12-01T16:38:25Z" id="65092713">Oh, sorry. Have searched the issue tracker but missed that. Thanks! :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixing typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8713</link><project id="" key="" /><description /><key id="50440366">8713</key><summary>Fixing typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2014-11-29T23:08:40Z</created><updated>2014-12-01T09:52:22Z</updated><resolved>2014-12-01T09:52:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T09:52:09Z" id="65041498">thanks @synhershko - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Fixing typo</comment></comments></commit></commits></item><item><title>Add an UpdateRequest example to Java API doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8712</link><project id="" key="" /><description>Closes #7083.
</description><key id="50436708">8712</key><summary>Add an UpdateRequest example to Java API doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2014-11-29T20:41:53Z</created><updated>2015-03-19T10:18:21Z</updated><resolved>2014-12-03T15:05:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-12-02T09:52:40Z" id="65206115">@s1monw agreed with your comment. I also changed a bit the doc with this https://github.com/dadoonet/elasticsearch/commit/d879072004808c77cc477bbc019f642a0b13f757
</comment><comment author="s1monw" created="2014-12-02T10:02:41Z" id="65207207">LGTM @clintongormley any comments?
</comment><comment author="dadoonet" created="2014-12-02T13:22:30Z" id="65229469">Thanks @clintongormley. PR updated: https://github.com/dadoonet/elasticsearch/commit/5823977a4dbdc5741aa07d1568a87aa0a312d593
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove java facet documentation in master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8711</link><project id="" key="" /><description>As we did in reference guide, we need to remove facet java code in master branch.
</description><key id="50434307">8711</key><summary>Remove java facet documentation in master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v2.0.0-beta1</label></labels><created>2014-11-29T19:08:28Z</created><updated>2014-12-01T13:25:11Z</updated><resolved>2014-12-01T13:24:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-12-01T09:39:11Z" id="65040022">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] CircuitBreakerServiceTests.testMemoryBreaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8710</link><project id="" key="" /><description>http://build-us-00.elasticsearch.org/job/es_core_master_regression/985/

```

FAILED JUNIT TESTS
Name: org.elasticsearch.indices.memory.breaker Failed: 1 test(s), Passed: 8 test(s), Skipped: 0 test(s), Total: 9 test(s)
Failed: org.elasticsearch.indices.memory.breaker.CircuitBreakerServiceTests.testMemoryBreaker
```
</description><key id="50434248">8710</key><summary>[CI Failure] CircuitBreakerServiceTests.testMemoryBreaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>jenkins</label></labels><created>2014-11-29T19:05:41Z</created><updated>2015-03-02T23:56:12Z</updated><resolved>2015-03-02T23:56:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceTests.java</file></files><comments><comment>[TESTS] remove AwaitsFix from CircuitBreakerServiceTests</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/indices/memory/breaker/CircuitBreakerServiceTests.java</file></files><comments><comment>[TEST] Mute CircuitBreakerServiceTests.testMemoryBreaker</comment></comments></commit></commits></item><item><title>Elasticsearch not running in Kibana</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8709</link><project id="" key="" /><description>Hallo there,

I'm getting the following error in the log file of ES when i'm kibana is trying to open/connect with Elasticsearch.

log4j, [2014-11-26T18:11:11.693] WARN: org.elasticsearch.transport.netty: [logstash-Suricata-25821-4012] exception caught on transport layer [[id: 0xae973955]], closing connection
java.nio.channels.UnresolvedAddressException
at sun.nio.ch.Net.checkAddress(Net.java:127)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:644)
at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
at org.elasticsearch.common.netty.channel.Channels.connect(Channels.java:634)
at org.elasticsearch.common.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
at org.elasticsearch.common.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:229)
at org.elasticsearch.common.netty.bootstrap.ClientBootstrap.connect(ClientBootstrap.java:182)
at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:680)
at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:643)
at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:610)
at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:133)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:279)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

Plz help
</description><key id="50429503">8709</key><summary>Elasticsearch not running in Kibana</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">versnel</reporter><labels /><created>2014-11-29T15:57:12Z</created><updated>2014-12-01T09:54:48Z</updated><resolved>2014-12-01T09:54:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ebuildy" created="2014-11-30T12:46:29Z" id="64984197">Not sure it's Kibana related (Kibana is just a Javascript/HTML UI with AJAX queries to ElasticSearch). 

What version ES/Kibana are you using ? Can you open head plugin or just / in a browser ?
</comment><comment author="versnel" created="2014-11-30T12:49:12Z" id="64984269">- Kibana 3.1.2
- Elasticsearch 1.4.0

I'm opening it in a browser IE 11
</comment><comment author="clintongormley" created="2014-12-01T09:54:48Z" id="65041809">Hi @versnel 

This looks like a Logstash exception, not Kibana.  It looks like you have Logstash configured to point to a non-existing Elasticsearch node?  I suggest you have a look at the [Logstash docs](http://www.elasticsearch.org/guide/en/logstash/current/index.html) and if you still can't figure it out, ask on the Logstash mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `force-recover` corrupted shards command to reroute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8708</link><project id="" key="" /><description>we give advice to `rm` shards on the FS if they got corrupted like here https://github.com/elasticsearch/elasticsearch/issues/8707 I personally think we should have an endpoint to force recovery for these shards to make it easier for users.
</description><key id="50428590">8708</key><summary>Add `force-recover` corrupted shards command to reroute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-11-29T15:17:46Z</created><updated>2016-01-22T18:28:33Z</updated><resolved>2015-11-21T22:01:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-01T09:43:31Z" id="65040491">+1
</comment><comment author="JamieCressey" created="2015-03-04T14:39:14Z" id="77168535">+1
</comment><comment author="nabheet" created="2015-04-01T16:00:35Z" id="88533330">+1
</comment><comment author="nick4eva" created="2015-04-08T09:20:08Z" id="90858571">+1
</comment><comment author="Bertg" created="2015-04-09T08:58:02Z" id="91158710">:+1: 
</comment><comment author="erikringsmuth" created="2015-04-29T16:59:03Z" id="97504440">:+1: 
</comment><comment author="phreakinggeek" created="2015-05-19T21:13:49Z" id="103668810">:+1: :+1:
</comment><comment author="ishare" created="2015-06-07T03:27:10Z" id="109680255">does it mean propose to be added to v2.0.0 ? 
</comment><comment author="pataditya" created="2015-08-18T05:42:37Z" id="132077346">+1
</comment><comment author="s1monw" created="2015-08-18T05:56:08Z" id="132081564">I think we can close this @bleskes since we now allocate them fresh on the corrupted node from the primary?
</comment><comment author="bleskes" created="2015-08-18T10:43:54Z" id="132167403">+1 to close this.  A corrupted shard should be replaced with a fresh copy now.
</comment><comment author="phreakinggeek" created="2015-11-17T18:18:59Z" id="157458438">Is that true for single node ES instances as well?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CorruptIndex after restart ES, footer mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8707</link><project id="" key="" /><description>Hey everyone, 
I have an index, called metadata, with 5 shards and 1 replicas, After restart ES, index cannot be fully-recovered . All primary shards are recovered properly. However two of the shards were remained unassigned. 
ES version: 1.3.2
When I executed this command:

curl -XGET "http:/localhost:9200/_cat/shards

metadata 2  p  STARTED 7712779 4.2gb ip-1 Motormouth 
metadata 2  r   STARTED 7712779 4.2gb ip-2 Harold "Happy" Hogan 
metadata 0  p  STARTED 7714351 4.1gb ip-2 Harold "Happy" Hogan 
metadata 0  r   UNASSIGNED 
metadata 3  p  STARTED 7711363 4.6gb ip-1 Motormouth 
metadata 3  r   STARTED 7711363 4.6gb ip-2 Harold "Happy" Hogan 
metadata 1  p  STARTED 7712560 4.2gb ip-2 Harold "Happy" Hogan 
metadata 1  r   UNASSIGNED 
metadata 4  p  STARTED 7714620 2.7gb ip-1 Motormouth 
metadata 4  r   STARTED 7714620 2.7gb ip-2 Harold "Happy" Hogan
## When I try to allocate for shards: 0 and 1 for Motormouth it gives an exception like that:

[2014-11-29 15:01:58,383][WARN ][index.engine.internal    ] [Motormouth] [metadata][0] failed engine [corrupted preexisting index]
[2014-11-29 15:01:58,384][WARN ][indices.cluster          ] [Motormouth] [metadata][0] failed to start shard
org.apache.lucene.index.CorruptIndexException: [metadata][0] Corrupted index [corrupted_3gXTXI3KQtm2e1WPsFntkg] caused by: CorruptIndexException[codec footer mismatch: actual footer=1308690703 vs expected footer=-1071082520 (resource: NIOFSIndexInput(path="/var/lib/elasticsearch/elasticsearch/nodes/0/indices/metadata/0/index/_8x9g.fdt"))]
    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:343)
    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:328)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:727)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:580)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:444)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

---

2014-11-29 15:01:58,437][WARN ][index.engine.internal    ] [Motormouth] [metadata][1] failed engine [corrupted preexisting index]
[2014-11-29 15:01:58,437][WARN ][indices.cluster          ] [Motormouth] [metadata][1] failed to start shard
org.apache.lucene.index.CorruptIndexException: [metadata][1] Corrupted index [corrupted_P-smhoB-SEeM7kHiTsIEug] caused by: CorruptIndexException[codec footer mismatch: actual footer=-262453147 vs expected footer=-1071082520 (resource: NIOFSIndexInput(path="/var/lib/elasticsearch/elasticsearch/nodes/0/indices/metadata/1/index/_avoi_es090_0.doc"))]
    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:343)
    at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:328)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:727)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:580)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:444)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

---

Hope someone can help.
</description><key id="50428271">8707</key><summary>CorruptIndex after restart ES, footer mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mehmetgunturkun</reporter><labels /><created>2014-11-29T15:05:49Z</created><updated>2015-05-21T13:43:00Z</updated><resolved>2014-12-06T21:44:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-29T15:15:48Z" id="64954834">hey, first of all nothing got lost so that's good. If we detect a corruption we mark the shard as corrupted there should be a `corrupted_?????` on disk that prevent your cluster from allocating the shard on that node. We do that to allow you to backup corrupted data etc. if we can allocate the shard somewhere else we will remove it. yet in your case you can remove the shard from the node it question and then ES will recover from the primary. Yet, I'd want to to know what happened that made the shard go corrupted. Did you upgrade lately? if so from what version?

you can `rm` the files on `Motormouth` for shard 0 &amp; 1 for the metadata index and then run  `curl -XPOST 'localhost:9200/_cluster/reroute'` this should kick off the recovery. If you are unsure you can post the commands here and I will ahve a look first. 
</comment><comment author="mehmetgunturkun" created="2014-11-29T15:24:08Z" id="64955063">Actually, last thing I did, querying something which should return 20M documents. I was not expecting that will down ES. After that I restarted ES and it started to giving that exceptions for these shards. 
So you are suggesting that deleting all data under the folder for shards: 0,1, right?
Btw, does ES trying to merge two shards when recovering?
Actually I was thinking to reduce number_of_replicas to 0 and then increase the 1 again. But I am not sure if that causes any data loss because primary shards are in different nodes?
</comment><comment author="s1monw" created="2014-11-29T15:27:26Z" id="64955162">your other replicase are just fine I don't think you need to do that. Yet, you certainly can.
</comment><comment author="mehmetgunturkun" created="2014-11-29T15:31:32Z" id="64955282">which one do you suggest, (reducing number of shards) or (removal shards and recover shards)?
</comment><comment author="s1monw" created="2014-11-29T16:06:59Z" id="64956293">I'd got and do a `mv /your/path/to/data/indices/metadata/0  /your/path/to/data/indices/metadata/backup_0`

then run reroute and wait until the shard is active. remove the backup continue with the second shard.
</comment><comment author="mehmetgunturkun" created="2014-11-29T16:10:47Z" id="64956430">ok I will try thanks
</comment><comment author="clintongormley" created="2014-12-01T09:43:10Z" id="65040457">Hi @mehmetgunturkun 

&gt; Actually, last thing I did, querying something which should return 20M documents

Do you mean you requested 20M documents in one search response? eg { "from": 0, "size": 20000000 }`?

If so, that could have caused an OOM exception.  Please can you look in the logs on each node to see if you had an OOM?
</comment><comment author="mehmetgunturkun" created="2014-12-01T09:46:29Z" id="65040839">yeah, it gave an Out of Memory Exception; but still couldn't understand why there is a mismatch?
</comment><comment author="clintongormley" created="2014-12-01T09:49:47Z" id="65041229">Basically, if you get an OOM exception, all bets are off.  At that stage the JVM is in an undefined state.  That said, it shouldn't write a commit point that includes a file which hasn't been written correctly.  

Was this index originally created with an older version of Elasticsearch? If so, which version?  It would be helpful if you could upload your logs somewhere.
</comment><comment author="mehmetgunturkun" created="2014-12-01T10:29:03Z" id="65045665">actually, I was almost sure about that this index created on 1.3.2 but I saw a file, "_avoi_es090_0.doc"," does it indicate version, 0.9?
sample of log file is in the following link:
https://dl.dropboxusercontent.com/u/69632603/New%20folder/elasticsearch.log
</comment><comment author="s1monw" created="2014-12-01T21:36:41Z" id="65138909">&gt; does it indicate version, 0.9?

no that is confusing it only means we added this codec in `0.90` but that is only a naming thing. 

&gt; https://dl.dropboxusercontent.com/u/69632603/New%20folder/elasticsearch.log

you know what this seems like a half written index during a recovery. I think what happened here is you got an OOM and one of the shards was recoverying during that time. Then it got corrupted because you ran into OOM since recovery didn't finish. It left your shard in half baked state. We fixed this in 1.4.0 where we rename files after they all have been written and we rename them such that commitpointer are renamed last. I think that is what happened, that explains the missing file as well as the truncated one?
</comment><comment author="mehmetgunturkun" created="2014-12-05T14:19:52Z" id="65795221">yeah this is what happened most probably, because on recovery, my application was still sending documents to index. Thanks a lot guys.
</comment><comment author="s1monw" created="2014-12-06T21:44:55Z" id="65915291">@mehmetgunturkun I assume this got resolved... I am closing it please reopen if you object.
</comment><comment author="Bertg" created="2015-04-09T09:01:16Z" id="91160655">@clintongormley First, thanks for the clear (and easy to find) help here. Today we ran into an issue with a corrupted shard as well. As far as we know though, the problems started when our master node and other (not master) node started having communication issues. We are still investigating what happened, but I was wondering if you'd be interested in our logs?
</comment><comment author="clintongormley" created="2015-04-09T09:04:56Z" id="91162727">Hi @Bertg 

We may well be.  I'd open a new issue mentioning the version that you're using, plus all of the details including the logs.  Note: if you're using an older version, there's a good chance that we've already fixed the bug, so you may want to trawl through the issues list first, to see if you find something that could explain the problem.
</comment><comment author="Bertg" created="2015-04-09T10:22:15Z" id="91184737">@clintongormley Actually investigating it more, I think we figured out what happened. A very complex query got generated, overloaded the master and the slaves "got confused" somehow. It does seem that later versions might fix the issue we had. We'll do the update and try to re run the offending query. If it happens again we'll open a ticket.
</comment><comment author="rtoma" created="2015-04-23T09:35:01Z" id="95511793">Had this issue with 1.4.4.

Fix mentioned in this issue did work: renaming the .../0 directory to .../0.backup. The 'unassigned' replica became primary, data was accessible again and a new replica was created. Case closed.
</comment><comment author="nguafogu78" created="2015-05-20T09:04:39Z" id="103817208">Hi,

A similar error happened to me for a 1.3.4 ES version. It seems too there is a broken recovery for one shard (among 5 for the same index). A lot of people suggest it could be resolved by renaming the shard directory. The 'unassigned' replica would become primary so data could be accessible again and a new replica created. Can someone confirm that ?

Someone in the #10066 suggested that this error would not happen in a 1.5.0 version ? Does someone agree with that ?
</comment><comment author="epleterte" created="2015-05-21T13:43:00Z" id="104281433">This happened to me with 1.4.5. It could be related to upgrade from 1.4.4 to 1.4.5 - I am sorry to say I am unsure about that. I only had 1 corrupt shard, though - I'd guess I should have more if it was related to the upgrade ... ?
In either case: _rm/mv_ + a call to _reroute_ worked like a charm!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete might returns false `isFound()` while primary is relocated </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8706</link><project id="" key="" /><description>here is a link to a relevant failure:  http://build-us-00.elasticsearch.org/job/es_core_master_metal/5665/

the test is indexing into a single shard with 0 replicas and this only shard is moving from node 0 to node 1. During the time we index the documents we mark the primary as started but until the clusterstate is published on node 0 it depends on which node you hit for indexing if the doc goes into both shards or not. Now if you hit node 1 the doc only goes into the new started (relocated) primary. Yet if you are fast enough and hit node 0 for the delete we can't find the doc in the currently relocating (clusterstate is not yet updated) - the logic knows it needs to push the delete into the new primary on node 1 so it really get's deleted but we return the response from node 0 which has `isFound = false`
</description><key id="50427220">8706</key><summary>Delete might returns false `isFound()` while primary is relocated </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:CRUD</label><label>bug</label><label>jenkins</label></labels><created>2014-11-29T14:33:07Z</created><updated>2016-02-03T11:55:22Z</updated><resolved>2016-02-02T15:24:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-29T14:42:48Z" id="64953935">FYI - I disabled the check for now since it might bring up other failures here and there if timing allows. We should reenable the check once we have a fix - I added a log line instead
</comment><comment author="bleskes" created="2014-12-19T13:37:48Z" id="67638530">a short update, now that we disabled the hard failure, we see that the tests fail to delete the doc: http://build-us-00.elasticsearch.org/job/es_bwc_1x/5883/CHECK_BRANCH=origin%2F1.4,jdk=JDK7,label=bwc/testReport/junit/org.elasticsearch.bwcompat/BasicBackwardsCompatibilityTest/testRecoverFromPreviousVersion/
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>test/framework/src/main/java/org/elasticsearch/test/ESIntegTestCase.java</file></files><comments><comment>[TEST] Fail test if dummy doc is not found</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>[TEST] Don't fail test if dummy doc is not found</comment></comments></commit></commits></item><item><title>Error when trying to add a document without specifying its parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8705</link><project id="" key="" /><description>Hi,

I'm not sure this is a bug, but I have a use case where a document is configured to have a parent, but in some cases, there is simply no parent because the resource was created "anonymously" (in my case parent is a user).

However, if I try to add a new document without specifying a parent, I have a `RoutingMissingException[routing is required]` error. I think that expected behaviour should be using the id of the child element when no parent is set.
</description><key id="50395125">8705</key><summary>Error when trying to add a document without specifying its parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bakura10</reporter><labels /><created>2014-11-28T18:06:05Z</created><updated>2014-11-28T18:11:46Z</updated><resolved>2014-11-28T18:11:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-28T18:09:42Z" id="64918483">Hi @bakura10 

This check was added because it was all too common for people to forget to specify the parent, and usually the parent/child relationship is used to add data to a parent document, and these children are queried in the context of their parents (or vice versa).

I suggest that you either create a dummy parent (eg id 1) which can be the parent of these anonymous children, or specify the routing parameter in these cases yourself.
</comment><comment author="bakura10" created="2014-11-28T18:11:46Z" id="64918611">That's what I'm doing actually now, but I was surprised by the behaviour, actually. Having no parent seems something that can be common :).

I'm closing then ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Explanation behaves inconsistently for multi_match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8704</link><project id="" key="" /><description>Explanation output shows different information than it should, if I understand it correctly.

Create an index:

```
curl -XPOST 'http://127.0.0.1:9200/testsearch/'  -d '
{
  "settings":{
    "analysis":{
      "analyzer":{
        "autocomplete_index":{
          "type":"custom",
          "tokenizer":"standard",
          "filter":["lowercase", "my_ngram"] 
        },
         "autocomplete_search":{
          "type":"custom",
          "tokenizer":"standard",
          "filter":[ "lowercase" ] 
        }
      },
      "filter":{
        "my_ngram":{
          "type":"edgeNGram",
          "min_gram":1,
          "max_gram":255,
          "token_chars": [ "letter", "digit"]
        }
      }
    }
  }
}'
```

Add mapping:

```
curl -XPOST 'http://127.0.0.1:9200/testsearch/_mapping/airport'  -d '
{
    "airport":
    {
        "properties":
        {
            "name":
            {
                "type":"multi_field",
                "fields": 
                {
                    "autocomplete_ngram":
                    {
                        "type":"string", 
                        "index_analyzer":"autocomplete_index",
                        "search_analyzer": "autocomplete_search"
                    },
                    "autocomplete_words":
                    {
                        "type":"string",
                        "search_analyzer": "autocomplete_search",
                        "index_analyzer": "autocomplete_search"
                    }
                }
            }
        }
    }
}'
```

Insert some data:

```
curl -XPOST 'http://127.0.0.1:9200/testsearch/airport/_bulk'  -d '
{"index": {"_index": "testsearch", "_type": "airport"}}
{"name": "John F Kennedy"}
{"index": {"_index": "testsearch", "_type": "airport"}}
{"name": "Key Field"}'
```

Now, when I search for "f kenned", there should be a match for "f" token on both autocomplete_ngram and autocomplete_words fields, and "kenned" token should match on autocomplete_ngram field. 

```
curl -XPOST 'http://127.0.0.1:9200/testsearch/_search?explain=&amp;format=yaml'  -d '
{
    "query":
    {
        "multi_match":
        {
            "query": "f kenned",
            "fields": ["autocomplete_ngram", "autocomplete_words"],
            "operator":"and",
            "type": "most_fields"
        }
    }
}'
```

The query above produces the following result: 

```
took: 1
timed_out: false
_shards:
  total: 1
  successful: 1
  failed: 0
hits:
  total: 1
  max_score: 0.14809652
  hits:
  - _shard: 0
    _node: "o4MHMH-cQp6aRFx5KmhMUg"
    _index: "testsearch"
    _type: "airport"
    _id: "AUn3Mv-sH6B0vXIpm567"
    _score: 0.14809652
    _source:
      name: "John F Kennedy"
    _explanation:
      value: 0.14809652
      description: "product of:"
      details:
      - value: 0.29619303
        description: "sum of:"
        details:
        - value: 0.29619303
          description: "sum of:"
          details:
          - value: 0.07735357
            description: "weight(name.autocomplete_ngram:f in 0) [PerFieldSimilarity],\
              \ result of:"
            details:
            - value: 0.07735357
              description: "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:"
              details:
              - value: 0.2602154
                description: "queryWeight, product of:"
                details:
                - value: 0.5945349
                  description: "idf(docFreq=2, maxDocs=2)"
                - value: 0.43767893
                  description: "queryNorm"
              - value: 0.29726744
                description: "fieldWeight in 0, product of:"
                details:
                - value: 1.0
                  description: "tf(freq=1.0), with freq of:"
                  details:
                  - value: 1.0
                    description: "termFreq=1.0"
                - value: 0.5945349
                  description: "idf(docFreq=2, maxDocs=2)"
                - value: 0.5
                  description: "fieldNorm(doc=0)"
          - value: 0.21883947
            description: "weight(name.autocomplete_ngram:kenned in 0) [PerFieldSimilarity],\
              \ result of:"
            details:
            - value: 0.21883947
              description: "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:"
              details:
              - value: 0.43767893
                description: "queryWeight, product of:"
                details:
                - value: 1.0
                  description: "idf(docFreq=1, maxDocs=2)"
                - value: 0.43767893
                  description: "queryNorm"
              - value: 0.5
                description: "fieldWeight in 0, product of:"
                details:
                - value: 1.0
                  description: "tf(freq=1.0), with freq of:"
                  details:
                  - value: 1.0
                    description: "termFreq=1.0"
                - value: 1.0
                  description: "idf(docFreq=1, maxDocs=2)"
                - value: 0.5
                  description: "fieldNorm(doc=0)"
      - value: 0.5
        description: "coord(1/2)"
```

So, if I understand the explanation correctly, it claims that there was no match for "f" token on autocomplete_words field, which is probably wrong. But when I change the search query to "f kennedy", it shows what I expect:

```
curl -XPOST 'http://127.0.0.1:9200/testsearch/_search?explain=&amp;format=yaml'  -d '
{
    "query":
    {
        "multi_match":
        {
            "query": "f kennedy",
            "fields": ["autocomplete_ngram", "autocomplete_words"],
            "operator":"and",
            "type": "most_fields"
        }
    }
}'
```

```
took: 47
timed_out: false
_shards:
  total: 1
  successful: 1
  failed: 0
hits:
  total: 1
  max_score: 0.9156243
  hits:
  - _shard: 0
    _node: "o4MHMH-cQp6aRFx5KmhMUg"
    _index: "testsearch"
    _type: "airport"
    _id: "AUn3Mv-sH6B0vXIpm567"
    _score: 0.9156243
    _source:
      name: "John F Kennedy"
    _explanation:
      value: 0.9156243
      description: "sum of:"
      details:
      - value: 0.36954886
        description: "sum of:"
        details:
        - value: 0.09651111
          description: "weight(name.autocomplete_ngram:f in 0) [PerFieldSimilarity],\
            \ result of:"
          details:
          - value: 0.09651111
            description: "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:"
            details:
            - value: 0.3246609
              description: "queryWeight, product of:"
              details:
              - value: 0.5945349
                description: "idf(docFreq=2, maxDocs=2)"
              - value: 0.54607546
                description: "queryNorm"
            - value: 0.29726744
              description: "fieldWeight in 0, product of:"
              details:
              - value: 1.0
                description: "tf(freq=1.0), with freq of:"
                details:
                - value: 1.0
                  description: "termFreq=1.0"
              - value: 0.5945349
                description: "idf(docFreq=2, maxDocs=2)"
              - value: 0.5
                description: "fieldNorm(doc=0)"
        - value: 0.27303773
          description: "weight(name.autocomplete_ngram:kennedy in 0) [PerFieldSimilarity],\
            \ result of:"
          details:
          - value: 0.27303773
            description: "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:"
            details:
            - value: 0.54607546
              description: "queryWeight, product of:"
              details:
              - value: 1.0
                description: "idf(docFreq=1, maxDocs=2)"
              - value: 0.54607546
                description: "queryNorm"
            - value: 0.5
              description: "fieldWeight in 0, product of:"
              details:
              - value: 1.0
                description: "tf(freq=1.0), with freq of:"
                details:
                - value: 1.0
                  description: "termFreq=1.0"
              - value: 1.0
                description: "idf(docFreq=1, maxDocs=2)"
              - value: 0.5
                description: "fieldNorm(doc=0)"
      - value: 0.54607546
        description: "sum of:"
        details:
        - value: 0.27303773
          description: "weight(name.autocomplete_words:f in 0) [PerFieldSimilarity],\
            \ result of:"
          details:
          - value: 0.27303773
            description: "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:"
            details:
            - value: 0.54607546
              description: "queryWeight, product of:"
              details:
              - value: 1.0
                description: "idf(docFreq=1, maxDocs=2)"
              - value: 0.54607546
                description: "queryNorm"
            - value: 0.5
              description: "fieldWeight in 0, product of:"
              details:
              - value: 1.0
                description: "tf(freq=1.0), with freq of:"
                details:
                - value: 1.0
                  description: "termFreq=1.0"
              - value: 1.0
                description: "idf(docFreq=1, maxDocs=2)"
              - value: 0.5
                description: "fieldNorm(doc=0)"
        - value: 0.27303773
          description: "weight(name.autocomplete_words:kennedy in 0) [PerFieldSimilarity],\
            \ result of:"
          details:
          - value: 0.27303773
            description: "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:"
            details:
            - value: 0.54607546
              description: "queryWeight, product of:"
              details:
              - value: 1.0
                description: "idf(docFreq=1, maxDocs=2)"
              - value: 0.54607546
                description: "queryNorm"
            - value: 0.5
              description: "fieldWeight in 0, product of:"
              details:
              - value: 1.0
                description: "tf(freq=1.0), with freq of:"
                details:
                - value: 1.0
                  description: "termFreq=1.0"
              - value: 1.0
                description: "idf(docFreq=1, maxDocs=2)"
              - value: 0.5
                description: "fieldNorm(doc=0)"
```

This behavior doesn't affect the search results themselves, but their scoring. I ran into this issue when I wanted to boost autocomplete_words field, but it was ignored for some searches.
</description><key id="50391270">8704</key><summary>Explanation behaves inconsistently for multi_match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">osykora</reporter><labels /><created>2014-11-28T17:01:58Z</created><updated>2014-11-28T17:27:18Z</updated><resolved>2014-11-28T17:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-28T17:27:18Z" id="64915581">Hi @osykora 

The problem that you're seeing comes from the fact that `most_fields` and `best_fields` are **field-centric**, and that you are using the `and` operator.

If you run your query through the validate-query API:

```
POST /testsearch/_validate/query?explain
{
  "query": {
    "multi_match": {
      "query": "f kenned",
      "fields": [
        "autocomplete_ngram",
        "autocomplete_words"
      ],
      "operator": "and",
      "type": "most_fields"
    }
  }
}
```

It gives you this explanation:

```
(+name.autocomplete_ngram:f +name.autocomplete_ngram:kenned) 
(+name.autocomplete_words:f +name.autocomplete_words:kenned)
```

In other words, `f` and `kenned` have to exist in the **same** field in order to be considered a match.

If, instead, you use a **term-centric** `cross_fields` query:

```
POST /testsearch/_validate/query?explain
{
  "query": {
    "multi_match": {
      "query": "f kenned",
      "fields": [
        "autocomplete_ngram",
        "autocomplete_words"
      ],
      "operator": "and",
      "type": "cross_fields"
    }
  }
}
```

Then the `and` operator works per-term across fields:

```
+blended(terms: [name.autocomplete_ngram:f, name.autocomplete_words:f]) 
+blended(terms: [name.autocomplete_ngram:kenned, name.autocomplete_words:kenned])
```

This is the expected behaviour.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[documentation] "/M" not explained in date range docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8703</link><project id="" key="" /><description>What does "10M/M" do in the example on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-daterange-aggregation.html? Round by month?

I think this should be explained on this page as it also explains the date formats.
</description><key id="50381915">8703</key><summary>[documentation] "/M" not explained in date range docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YousefED</reporter><labels /><created>2014-11-28T15:00:09Z</created><updated>2014-11-28T15:53:52Z</updated><resolved>2014-11-28T15:53:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update daterange-aggregation.asciidoc</comment></comments></commit></commits></item><item><title>Grammar correction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8702</link><project id="" key="" /><description /><key id="50380198">8702</key><summary>Grammar correction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dantuffery</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2014-11-28T14:39:47Z</created><updated>2014-11-29T13:06:24Z</updated><resolved>2014-11-29T13:06:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-28T15:40:45Z" id="64905979">Hi @dantuffery 

Thanks for the PR.  Actually, I think a better way of saying it is "a certain number of nodes is up".

If you want to update your PR i'll be happy to merge it in

thanks
</comment><comment author="dantuffery" created="2014-11-28T20:09:44Z" id="64925484">Hi Clinton, I have made the change. 

Dan
</comment><comment author="clintongormley" created="2014-11-29T13:06:17Z" id="64951218">Thanks @dantuffery - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Grammar correction</comment></comments></commit></commits></item><item><title>Change routing on running elastic search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8701</link><project id="" key="" /><description>Hi,

I have large elastic search data. There are shards but I have not implemented routing initially.
Now I want to implement routing by changing mapping.

Can I change it on running elastic seach without any downtime. Any suggesstion is highly appreciated.

Thanks.
</description><key id="50350295">8701</key><summary>Change routing on running elastic search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sandeep-verma</reporter><labels /><created>2014-11-28T08:10:40Z</created><updated>2014-11-28T08:46:00Z</updated><resolved>2014-11-28T08:46:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-11-28T08:46:00Z" id="64869111">Hi @sandeep-verma 

Please use the mailing list to ask for questions like this one. We try to use this space to report issues or propose feature requests.

That said, you'll basically need to reindex your data. You can scan and scroll your data and reindex in another index. If you were using aliases (which is a good practice BTW), just switch the alias to the new index and your users won't see the difference.
Then remove the old index.

Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] MinimumMasterNodesTests.multipleNodesShutdownNonMasterNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8700</link><project id="" key="" /><description>MockDirectoryWrapper: cannot close: there are still open files: {_0.si=1}

http://build-us-00.elasticsearch.org/view/ES%201.4/job/es_core_14_centos/966/testReport/junit/org.elasticsearch.cluster/MinimumMasterNodesTests/multipleNodesShutdownNonMasterNodes/
</description><key id="50349803">8700</key><summary>[CI Failure] MinimumMasterNodesTests.multipleNodesShutdownNonMasterNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>jenkins</label></labels><created>2014-11-28T08:01:44Z</created><updated>2015-10-23T07:24:41Z</updated><resolved>2015-10-23T07:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T07:24:41Z" id="150496337">this hasn't failed in a long time.... closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] IndexAliasesTests.testWaitForAliasSimultaneousUpdate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8699</link><project id="" key="" /><description>Test failure aside the assertion/ debug information is not very helpful in figuring out what went wrong in case the test fails:

Expected: &lt;true&gt;      got: &lt;false&gt;
http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_metal/5632/testReport/junit/org.elasticsearch.aliases/IndexAliasesTests/testWaitForAliasSimultaneousUpdate/
</description><key id="50349718">8699</key><summary>[CI Failure] IndexAliasesTests.testWaitForAliasSimultaneousUpdate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>jenkins</label></labels><created>2014-11-28T08:00:29Z</created><updated>2015-10-23T07:24:33Z</updated><resolved>2015-10-23T07:24:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-23T07:24:33Z" id="150496316">this hasn't failed in a long time.... closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] indices.memory.breaker.CircuitBreakerServiceTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8698</link><project id="" key="" /><description>Message says - "Expected shard failures, got none"

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_coverage_master/127/consoleText
</description><key id="50349600">8698</key><summary>[CI Failure] indices.memory.breaker.CircuitBreakerServiceTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>jenkins</label></labels><created>2014-11-28T07:58:40Z</created><updated>2014-11-28T15:01:58Z</updated><resolved>2014-11-28T15:01:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-28T15:01:58Z" id="64902305">Pushed a test fix for this in 1d8fd0f
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] Infinite monkeys typing issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8697</link><project id="" key="" /><description>As discussed - the thread relies on getting messages in random order - given enough trials random sometimes equals in ascending order:

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_centos/1462/testReport/junit/org.elasticsearch.http.netty/NettyPipeliningDisabledIntegrationTest/testThatNettyHttpServerDoesNotSupportPipelining/
</description><key id="50349315">8697</key><summary>[CI Failure] Infinite monkeys typing issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>jenkins</label></labels><created>2014-11-28T07:54:02Z</created><updated>2014-11-28T11:29:16Z</updated><resolved>2014-11-28T11:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/http/netty/NettyPipeliningDisabledIntegrationTest.java</file></files><comments><comment>[TEST] With pipelining disabled requests can come back in any order</comment></comments></commit></commits></item><item><title>[CI Failure] [test-repo:test-snap] failed to get snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8696</link><project id="" key="" /><description>We run into issues reading snapshot/restore metadata: 

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_strong/1476/testReport/junit/org.elasticsearch.snapshots/SharedClusterSnapshotRestoreTests/snapshotRelocatingPrimary/
</description><key id="50349162">8696</key><summary>[CI Failure] [test-repo:test-snap] failed to get snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>jenkins</label></labels><created>2014-11-28T07:51:12Z</created><updated>2014-12-05T18:01:09Z</updated><resolved>2014-12-05T18:01:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/SnapshotMetaData.java</file><file>src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/repositories/Repository.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreSnapshot.java</file><file>src/main/java/org/elasticsearch/snapshots/Snapshot.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/BlobContainerWrapper.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file></files><comments><comment>Snapshot/Restore: switch to write once mode for snapshot metadata files</comment></comments></commit></commits></item><item><title>put mapping with wrong attribute for field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8695</link><project id="" key="" /><description>Hello Everyone,

I think, elasticsearch should raise error when a user put mapping with wrong attribute for field.
for example, if you put following mapping, elasticsearch just ignore the wrong attribute "indexx" and accept it. so, you cannot find it out easily.

```
PUT _template/hoge
{
"template" : "logstash-*",
"mappings": {
         "access_log": {
            "properties": {
               "hoge": {
                  "type": "string",
                  "indexx" : "not_analyzed"
               }
          }
     }
   }
}
```

How do you think for this?
</description><key id="50335734">8695</key><summary>put mapping with wrong attribute for field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yoshi0309</reporter><labels /><created>2014-11-28T02:43:42Z</created><updated>2014-11-28T08:17:52Z</updated><resolved>2014-11-28T08:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-11-28T07:37:57Z" id="64863677">Hi

Thanks for your report.

We finished implementing validation of mappings on master/1.x branch.
Related https://github.com/elasticsearch/elasticsearch/issues/7205
</comment><comment author="yoshi0309" created="2014-11-28T08:17:52Z" id="64866628">great!
thank you for notify me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Add java documentation for aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8694</link><project id="" key="" /><description>## Buckets:
- terms
- range
- global
- filter
- filters
- missing
- nested
- reverse nested
- children
- significant terms
- date range
- ip range
- range
- histogram
- date histogram
- geo distance
- geo hash grid
## Metrics:
- min
- max
- sum
- avg
- stats
- extended stats
- value count
- percentiles
- percentile rank
- cardinality
- geo bounds
- top hits
- scripted metric
</description><key id="50327389">8694</key><summary>Docs: Add java documentation for aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-27T22:48:07Z</created><updated>2015-03-19T10:18:21Z</updated><resolved>2014-11-29T19:00:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-28T22:09:25Z" id="64931397">I left some comments, but thank you so much for this!
</comment><comment author="dadoonet" created="2014-11-29T10:10:07Z" id="64947192">@jpountz I updated based on your comments. Let me know. (thanks for the review BTW)
</comment><comment author="jpountz" created="2014-11-29T14:36:25Z" id="64953701">LGTM
</comment><comment author="dadoonet" created="2014-11-29T19:01:42Z" id="64961690">Docs pushed in master 1.x and 1.4 branches
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve fielddata mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8693</link><project id="" key="" /><description>Currently, the settings for fielddata and doc_values is quite confusing. It would be nice to make it easier to understand.  For non-analyzed fields, these are the questions which need answering:
1. Should fielddata be written to disk at index time?
2. Should fielddata be available at search time, from disk, or in memory?
3. If in memory, should fielddata be loaded eagerly or lazily?
4. Regardless of disk or memory, should global ordinals be built eagerly or lazily?

This could be expressed as:

```
"fielddata": {
  "index_format":    "disk | disabled",
  "search_format":   "disk | memory | eager_memory | disabled",
  "global_ordinals": "lazy | eager"
}
```

In the same way as we can use `analyzer` to set both `index_analyzer` and `search_analyzer`,  this could be condensed to:

```
"fielddata": {
  "format":          "disk | memory | eager_memory | disabled",
  "global_ordinals": "lazy | eager"
}
```

Analyzed string fields can't be written to disk, but they can support the `fst` format, so they would accept:

```
"fielddata": {
  "format":          "memory | eager_memory | fst | eager_fst | disabled",
  "global_ordinals": "lazy | eager"
}
```

We could possibly even support a very simple format for setting the fielddata format:

```
"fielddata": "disk | memory | eager_memory | disabled"            # not_analyzed
"fielddata": "memory | eager_memory | fst | eager_fst | disabled" # analyzed
```

... which would set `global_ordinals` to `lazy``

Regardless of which format is used to set fielddata, the mappings would be converted to the full `index_format`, `search_format`, `global_ordinals` layout.

**Question:** Should `index_format` and `search_format` instead by `store_format` and `load_format`?
</description><key id="50318581">8693</key><summary>Improve fielddata mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>blocker</label><label>enhancement</label></labels><created>2014-11-27T19:17:03Z</created><updated>2015-08-13T13:44:32Z</updated><resolved>2015-08-10T10:44:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-27T19:25:06Z" id="64824326">some of these possibilities make no sense. Like writing docvalues at index-time, but then loading stuff up into field data. 
</comment><comment author="rmuir" created="2014-11-27T19:25:46Z" id="64824357">Also: "disk" is not a synonym for docvalues. It loads some things in memory. Just not everything, and not in a bloated way.
</comment><comment author="clintongormley" created="2014-11-28T08:57:09Z" id="64870098">&gt; some of these possibilities make no sense. Like writing docvalues at index-time, but then loading stuff up into field data.

This is a possibility which works today.  It will allow users to write doc values, then test out whether loading a particular field into fielddata helps their use case or not.  It just brings slightly more flexibility than having to reindex your data just to try things out.

&gt; Also: "disk" is not a synonym for docvalues. It loads some things in memory. Just not everything, and not in a bloated way.

Sure, but the major distinction between the two implementations is: one is in memory and the other is largely on disk.  It will be more understandable to users than referring to doc values.
</comment><comment author="rmuir" created="2014-11-28T13:08:51Z" id="64892198">Thats not really the distinction: again its the incorrect name.

Lets call it "scalable" and "non-scalable" ? Fielddata is slightly faster because it uses bloated compression etc and makes the wrong tradeoffs.
</comment><comment author="rmuir" created="2014-11-28T13:20:11Z" id="64893120">&gt; This is a possibility which works today. It will allow users to write doc values, then test out whether loading a particular field into fielddata helps their use case or not. It just brings slightly more flexibility than having to reindex your data just to try things out.

no, its just a trap.

terrible.
</comment><comment author="jpountz" created="2014-11-28T13:38:41Z" id="64894688">&gt; "fielddata": "disk | memory | eager_memory | disabled"            # not_analyzed

Actually eager loading currently makes sense for doc values too since Lucene only loads them on the first time that they are used (like norms).
</comment><comment author="s1monw" created="2014-11-28T13:51:31Z" id="64895863">I actually agree with rob on the possiblity to load stuff into FD if you actually have DocValues this doesn't make too much sense. I'd rather trade the flexibility here for safety since it can really spike your system big time if you suddenly go to FD?
</comment><comment author="clintongormley" created="2014-12-02T13:38:31Z" id="65231319">One of the issues with naming is that you're limited to conveying just part of the explanation in the name itself.  `disk` vs `memory` explains one part, `scalable` vs `unscalable` is another.  @s1monw suggested trying to explain when you pay the price: at index time or at query time  (with the other details explained in full in the documentation.)  For instance:

```
"fielddata": {
  "build": "eager | lazy",  # eager = doc values, lazy = memory
  "load": "lazy   | eager | eager_global_ordinals"
}
```
- This removes the `fst` format for `not_analyzed` strings, which is rarely used anyway.
- Once `build` is set to `eager`, doc values will always be built (we can't turn them off)
- I know @rmuir and @s1monw still disagree, but I'm still voting for allowing users to test out in-memory fielddata performance on an existing field by allowing them to set `load: lazy` on the fly (with `build: eager`), without forcing them to reindex all of their data
</comment><comment author="s1monw" created="2014-12-02T13:42:29Z" id="65231793">I like the naming though... I personally think it's a trap to go back to fielddata from here but I don't have super strong feelings. We allow for a lot of traps to provide flexiblity so it's not just black and white. I'd personally vote for safety here vs. trappyness. Maybe we can enable trappy mode and then you can do this? :)
</comment><comment author="eiTanLaVi" created="2014-12-14T12:41:47Z" id="66911992">how can I define a lazy global ordinals mapping for a non-analyzed field?
</comment><comment author="clintongormley" created="2015-03-23T03:52:31Z" id="84784051">&gt;  I personally think it's a trap to go back to fielddata from here 

I've come around to agree with this sentiment. Once we build doc values for a field, we shouldn't allow switching back to using field data at that stage.

So I repeat the last recommendation:

```
"fielddata": {
  "build": "eager | lazy",  # eager = doc values, lazy = memory
  "load": "lazy   | eager | eager_global_ordinals"
}
```

with one distinction, if doc values are enabled (ie `build:eager`) then `load: lazy | eager` applies just to doc values, ie should Lucene open them immediately or on first use.  

Any other suggestions on this change?
</comment><comment author="rmuir" created="2015-03-23T08:06:09Z" id="84873980">I don't really like eager vs lazy to explain the differences here. 

There are plenty of differences, e.g. using filesystem cache versus on-heap memory. e.g. actually doing bitpacking versus bloating values up to 8/16/32/64. So eager vs lazy doesn't really make sense to me.

Do we really need a fielddata setting at all? You can already set docvalues another way.
</comment><comment author="s1monw" created="2015-03-23T14:56:29Z" id="85036196">@rmuir i am all for one way of setting this. My take on this is the following:
- use `doc_values : true|false` to trigger DV, that is consistent to all the other FieldType settings we expose ie. `indexed : true|false`
- remove the `fst` format and make it a hard choice to either use DV or FieldData at field creation time. Since today we have `paged_bytes`, `fst`, `doc_values` and I think if we get rid of the last two we can simplify this a lot?
- only allow configuration of `fielddata.global_ordinals : lazy|eager` and only if the field has no DV
- if folks wanna experiment they can use a separate field or we can add an explicit setting saying `fielddata.doc_values: false|true` that can be used to override the field setting if this helps to make progress here. 
</comment><comment author="clintongormley" created="2015-04-05T09:17:22Z" id="89738941">The motivation for this issue has changed somewhat since we switched to doc values by default. Initially, we were trying to make it easier (and less confusing) to use doc values.  Now, with better defaults, users should only fiddle with these settings if they know what they're doing.

I suggest removing the `doc_values` setting altogether, and using just the `fielddata` settings as follows:

```
fielddata: {
    format:          "doc_values | heap",   
    load:            "lazy | eager | eager_global_ordinals | disabled"
}
```

The `format` defaults to `doc_values` for all field types except analyzed strings (and I believe geo-ip doesn't support doc values yet?).  This setting should not be dynamically updatable.

The `load` setting defaults to `lazy`, so that we're not loading doc values for (potentially) very many fields automatically.  Analyzed strings could potentially default the `load` setting to `disabled` so that drive-by users don't cause a massive fielddata load by eg sorting on the "name" field. It should be possible to change the `load` setting dynamically.
</comment><comment author="clintongormley" created="2015-04-08T09:34:41Z" id="90861815">After chatting to @jpountz and @s1monw about this, we have a new proposal.  Assuming doc values deliver on their promise, the long term intention will be to remove in-memory fielddata completely. (This assumes that we have a solution for analyzed string fields as well).

New proposal:

```
doc_values:       true | false              # defaults to true for all but analyzed string fields
fielddata:        disabled | lazy | eager   # only if doc_values is false, will go away eg in 3.0
global_ordinals:  lazy | eager              # defaults to lazy
```

Later we may be able to add the `auto` option to `global_ordinals` so that we can make a runtime decision based on usage about whether global ordinals should be built lazily or eagerly.
</comment><comment author="s1monw" created="2015-04-08T09:39:01Z" id="90862870">+1 @rmuir WDYT
</comment><comment author="clintongormley" created="2015-04-08T09:43:34Z" id="90863802">To clarify, `global_ordinals: lazy` means "build global ordinals only when they are required for a request", while `eager` means "build global ordinals on every refresh"
</comment><comment author="clintongormley" created="2015-08-10T10:44:43Z" id="129404398">Closing in favour of #12394
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update cluster state with type mapping also for failed indexing request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8692</link><project id="" key="" /><description>When indexing of a document with a type that is not in the mappings fails,
for example because "dynamic": "strict" but doc contains a new field,
then the type is still created on the node that executed the indexing request.
However, the change was never added to the cluster state.
This commit makes sure mapping updates are always added to the cluster state
even if indexing of a document fails.

closes #8650
</description><key id="50314237">8692</key><summary>Update cluster state with type mapping also for failed indexing request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-27T17:54:24Z</created><updated>2015-06-07T18:36:20Z</updated><resolved>2015-02-24T16:56:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-02-23T17:53:12Z" id="75594380">Updated. I also moved the integration test to a separate class as DedicatedAggregationTests is hardly the right place.
</comment><comment author="rjernst" created="2015-02-23T23:06:21Z" id="75657780">@brwe I started looking at this again, but then realized I'm not sure this is what we want.  Isn't the point of dynamic: strict to _not_ modify mappings?  Adding an empty type in this case seems counter intuitive...
</comment><comment author="brwe" created="2015-02-24T11:18:56Z" id="75739162">&gt; Isn't the point of dynamic: strict to not modify mappings?

`dynamic: strict` means that no fields may be added that were not defined before. The setting for strictly not adding pre defined types is `"index.mapper.dynamic": false`. 

However, I agree it is weird that the mapping is created although the document is not indexed.  We create mappings in some places where we then do not index, like index failures in bulk requests (see https://github.com/elasticsearch/elasticsearch/pull/5623) or when percolating a document that has a type which was not defined before.
The reason why we do this is that currently mappings are added to the node local DocumentMapper the moment we parse. This means that we have to update the mapping, regardless of if the document is indexed or not because else the mapping in cluster state and in DocumentMapper are out of sync and therefore the mapping is lost the moment we restart the node. 
While reading the code I actually found another example where this causes problems: When we parse a document halfway and then get a parsing exception, the parsed fields that were added before are then also not added to the cluster state and can be lost when restarting the node. I added a test and fix to this pr.
Now, I think we have two options:
- fix: Parsing of documents that are not indexed should not change the Document mapper or the mapping in the cluster state 
- push the fixes as is which would at least prevent the mappings to be lost and live with the fact that mappings are created even if indexing of the doc fails.

I would like to do the latter and work on the first in a different issue and pr because I could imagine this needs a little more work.
</comment><comment author="rjernst" created="2015-02-24T14:43:00Z" id="75768343">I'm all for making the behavior consistent and predictable.  What concerns me is the future, when mappings are cleaned up and immutable (#9365) so that we can fix all of these issues you mention and not modify mappings when failures occur.  I do not want someone to then argue the current behavior needs to be maintained. So can we downplay this in the docs somehow? I don't want anyone to rely on this behavior, or even have the expectation of relying on the behavior.
</comment><comment author="rjernst" created="2015-02-24T14:46:31Z" id="75769028">Obviously this PR has no doc changes.  I guess what I mean is, can we maintain the fact that this behavior cannot be relied upon if someone comes along and says "what is the behavior and why isn't it documented?".

This question aside, the change LGTM.
</comment><comment author="brwe" created="2015-02-24T15:14:21Z" id="75774446">ok, changed WriteFailure -&gt; WriteFailureException. Will push in a minute to 1.4 1.x and master
</comment><comment author="brwe" created="2015-02-24T15:53:47Z" id="75782550">spoke too soon. the second fix should also be in prepareIndex and not just prepareCreate. Also, I am not sure if it is sufficient to check if it was a MapperParsingException  but also we should probably check if the mapping was actually modified before we update on master. 
</comment><comment author="brwe" created="2015-02-24T16:17:10Z" id="75787467">I'll push only the original fix (mapping lost with dynamic_strict...) and make a separate issue for the second one.
</comment><comment author="javanna" created="2015-02-25T09:07:45Z" id="75927357">@brwe can you please label? :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/WriteFailureException.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingTests.java</file></files><comments><comment>mappings: update cluster state with type mapping also for failed indexing request</comment></comments></commit></commits></item><item><title>Update repositories.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8691</link><project id="" key="" /><description>1. Enable the repository using "add-apt-repository" to avoid this error "No command 'deb' found". 
2. Adding "sudo" to update and install command.
</description><key id="50313042">8691</key><summary>Update repositories.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ashrafsarhan</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-27T17:36:27Z</created><updated>2014-12-09T12:23:18Z</updated><resolved>2014-12-09T12:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-28T12:57:49Z" id="64891299">Hi @ashrafsarhan 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="ashrafsarhan" created="2014-12-05T15:37:50Z" id="65806353">Hi @clintongormley

URW 😊
I had signed the CLA.
</comment><comment author="clintongormley" created="2014-12-09T12:23:13Z" id="66274795">thanks @ashrafsarhan - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update repositories.asciidoc</comment></comments></commit></commits></item><item><title>Remove the `snowball` analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8690</link><project id="" key="" /><description>The  `snowball` analyzer has been removed in Lucene 5.0 in favour of the `english` language analyzer.  

In 2.0, we should automatically upgrade indices which use it to use the `english` analyzer instead, and refuse to create new indices which specify the `snowball` analyzer.
</description><key id="50312546">8690</key><summary>Remove the `snowball` analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>adoptme</label><label>breaking</label></labels><created>2014-11-27T17:29:44Z</created><updated>2016-11-06T07:43:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="skearns64" created="2014-12-29T20:32:08Z" id="68299792">&gt; In 2.0, we should automatically upgrade indices which use it 

^^ Does this effectively mean "re-index" from _source?
</comment><comment author="clintongormley" created="2014-12-30T10:47:39Z" id="68347063">@skearns64 no, just change the index settings
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>First pass at improving analyzer docs (#18269)</comment></comments></commit></commits></item><item><title>Use RoaringDocIdSet instead of FixedBitset for caching.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8689</link><project id="" key="" /><description>The default DocIdBitSet used for caching is currently the FixedBitSet implementation. For very sparse or dense bitsets, there are better implementations.

Lucene currently uses WAH8DocIdSet. https://github.com/elasticsearch/elasticsearch/pull/7577 suggested that ES should also switch to that implementation. The PR was rejected, in favour of getting the RoaringDocIdSet from Lucene 5. Upgrading to Lucene 5 was done in https://github.com/elasticsearch/elasticsearch/commit/610ce078fb3c84c47d6d32aff7d77ba850e28f9d, and it does indeed use the RoaringDocId implementation. 

For ES users with large indices, it would be beneficial if the filter cache did not use so much memory, even before ES uses Lucene 5 (seems to be targeted for 2.0). This PR uses the RoaringDocIdSet from Lucene trunk (with minor modifications) instead of the FixedBitSet implementation. For the modifications, see inline comments.

I've ran the test suite locally a few times, and it works. I did see some issues, but fixed them.
</description><key id="50311756">8689</key><summary>Use RoaringDocIdSet instead of FixedBitset for caching.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">antonha</reporter><labels /><created>2014-11-27T17:18:23Z</created><updated>2015-03-24T09:56:15Z</updated><resolved>2015-03-24T09:56:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-27T19:41:10Z" id="64825296">Something that refrains me from merging this pull request is that elasticsearch is highly biased towards doc id sets that support random-access (such as FixedBitSet, but not RoaringDocIdSet). And for instance, there are lots of places where elasticsearch calls `DocIdSet.toSafeBits` in order to have random-access. If you call this method on a FixedBitSet, it will return the FixedBitSet itself. On the other hand if you call it on a RoaringDocIdSet, it will load the RoaringDocIdSet into a FixedBitSet, which is potentially very slow if the doc id set is dense. We started refactoring some consumers of filters so that they work with the iterator instead of relying on random-access, but it's quite some work and even on master it's not finished yet. For instance we still have the `filter` aggregation that uses this `toSafeBits` method on every filter/segment which we need to fix.

I really think this is the right move, but I'm afraid that if we do not backport it to 1.x without other changes in order to consume iterators rather than using random-access, then it's going to bring more bad than good.
</comment><comment author="antonha" created="2014-11-28T09:47:31Z" id="64874621">Thanks for your feedback. It does indeed seem like ES is heavy biased towards fixed bitsets. 

For your example with .toSafeBits, I've added a new commit to the PR, containing a Bits implementation that is fast for ordered access, if the underlying DocIdSet's iterator has a fast advance() method, and replaced usages of .toSafeBits with calls to that method. I'm a bit uncertain about some things, please see inline comments.

Is this a decent approach to take?

I also tried looking for parts of ES where the FixedBitSet is used. It seems like it is used most heavily in nested document search logic and geo things. I can not see how that would affect this PR, but I have probably overlooked something. 

Could you help me identify the areas that need to be adressed in order for this PR to be able to move forward?
</comment><comment author="jpountz" created="2014-12-03T23:50:21Z" id="65514100">@antonha Given that we started efforts to move the master branch forward in preparation for version 2.0 (this is why there are more 2.0 commits these days than in the past), I think we should really focus on master and not backport such important changes. For instance, I like the way that you hide an iterator behind bits in order to easily move away from DocIdSets.toSafeBits. Ideally I think we should try to bring such changes to master and keep on using dense bitsets on 1.x?

About nested documents, it is fine that they still use bitsets. The reason is that they need the `prevSetBit` method that only exists on the `BitSet` class, not DocIdSet(Iterator).
</comment><comment author="antonha" created="2014-12-04T13:27:07Z" id="65630967">@jpountz I am very happy about the direction master and 2.0 is taking in this regard, and I would be very happy to contribute if that brings value.

However, the main reason for me opening this PR was for bringing the functionality to ES users before 2.0, especially since that upgrade might be cumbersome to make (major versions both for ES and Lucene). In our ES setup, we are more or less blocked from using filter caches for most of our filters, due to the memory consumption of the cache. The same would be true for most users with large indices.

Thus, I believe that it would be a good thing if we could reduce the memory impact of the caches before 2.0.

Is there a better approach to making such a thing happen?
</comment><comment author="s1monw" created="2015-03-24T09:56:15Z" id="85428254">@antonha I just revisited this PR and spoke to @jpountz about it. We decided to pass on this PR since we already move to it in master as well as for the sake of the complexity of the change. In 1.x we have a still a large amount of code that relies on FixedBitSet being used for filter caching and we don't want to make the moves there since changes are quite complex when it gets to filter conjunctions, intersections etc.  I hope you understand our decision. Thanks for helping out on this end!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping updates should be synchronous</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8688</link><project id="" key="" /><description>Today, a new field can be added to the local mapping of two shards simultaneously. If the detected field type is different, then each shard can end up with a different field type.  These shards then send their new mapping to the master, but only of the mappings will win.

This can result in incorrect results and even data loss, eg: one shard thinks that the field is a string, and the other shard (and the master) thinks that it is a number.  In this case, sorting and aggregations will be wrong (#8485).  Then, when the string-shard is allocated to a new node, that node will receive the "number" mapping from the master.  Replaying the transaction log can cause shard failures (eg #8684).  Or new numeric doc values are written but, when we try to merge segments, it fails with a doc values exception (https://github.com/elasticsearch/elasticsearch/issues/8009).

The only way to ensure that dynamic fields are the same on all shards is to wait for the master to acknowledge the mapping changes before indexing the document, so:
- parse the document
- update local mapping
- **if changed, send to master and wait for ack**
- index document

This will potentially slow down indexing when many dynamic fields are being added, but it is the only way to automatically protect against data loss due to mapping conflicts.

Should a user wish to disable waiting for the master (and they are certain that their dynamic mapping rules are good enough to prevent these problems) then we should allow them to opt-out by setting `"dynamic": "unsafe"`
</description><key id="50310350">8688</key><summary>Mapping updates should be synchronous</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2014-11-27T17:00:20Z</created><updated>2016-10-07T17:52:03Z</updated><resolved>2015-04-21T09:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elvarb" created="2015-01-27T13:11:36Z" id="71645195">+1 I have issues with this every now and then, worst case when it happens I loose a index, not good.
</comment><comment author="sammcj" created="2015-02-08T06:04:14Z" id="73398592">+1 - Pretty sure this is the issue we have every day.

Screenshot: https://twitter.com/s_mcleod/status/560237698758111232

```
logstash-2015.02.07/qlMwQhWoRIedLZ5r23eCcw [1]
{
    state: INITIALIZING
    primary: false
    node: qlMwQhWoRIedLZ5r23eCcw
    relocating_node: null
    shard: 1
    index: logstash-2015.02.07
}
```
</comment><comment author="kimchy" created="2015-02-08T16:47:20Z" id="73418951">@sammcj can you post the failure in the logs for the shards being allocated? it might be related, and might not, depends on the actual failure.
</comment><comment author="sammcj" created="2015-02-09T00:05:23Z" id="73441487">@kimchy it looks one of the situations where we've seen this might be caused by the timestamp field in logstash: https://gist.githubusercontent.com/sammcj/66c212311d99a9ca93fe/raw/de99f6ae149745c7b42262d6c2c26c46458e253f/logstash_shard_logs
</comment><comment author="bleskes" created="2015-02-09T09:50:06Z" id="73481865">@sammcj it looks like your are using a timestamp field, which is correctly recognized as a date but has the data in the wrong format:

```
 MapperParsingException[failed to parse [timestamp]]; nested: MapperParsingException[failed to parse date field [Feb  9 11:00:01], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: "Feb  9 11:00:01"]; ]]
```

This shouldn't fail the recovery (as it should not have been accepted by the primary) but I don't see (yet) how async dynamic mapping creation should have caused this failure. Do you use any custom index templates, the _default_ type or dynamic templates?
</comment><comment author="sammcj" created="2015-02-09T22:10:57Z" id="73601405">Hi @bleskes - I could see the error around the date and am going to track down which app is logging it to logstash - I was thinking that this shouldn't cause ES to fail recovery (as you said).

The index type for logstash is created and managed by logstash itself - as far as our other indexes we just use the default type.

This sounds like it is unrelated - so I'll log a separate bug for not handling recovery as not to pollute this ticket if required.
</comment><comment author="bleskes" created="2015-02-10T21:07:04Z" id="73783685">&gt; I was thinking that this shouldn't cause ES to fail recovery (as you said).

@sammcj it maybe unrelated. When you say the recovery failed - did you check using `GET _recovery` to see there were no progress at all? (As opposed to being very slow...)
</comment><comment author="sammcj" created="2015-02-12T00:06:43Z" id="73992385">@bleskes  It looks like ES permanently stuck intializing shard 2

Index Info: https://gist.github.com/sammcj/5128f1479df8754c1bc5
Metadata: https://gist.github.com/sammcj/a110ee0aa72f003ec45e

Resulting in:

```
{
  "cluster_name" : "int-elastic-cluster",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 79,
  "active_shards" : 155,
  "relocating_shards" : 0,
  "initializing_shards" : 3,
  "unassigned_shards" : 0
}
```
</comment><comment author="bleskes" created="2015-02-12T18:06:27Z" id="74120446">@sammcj thx. The index info doesn't show any initializing shard. Can you say which command you used? also , can you post the output of `GET {index_name_here}/_recovery?human&amp;pretty` as well?
</comment><comment author="jlintz" created="2015-02-19T22:22:54Z" id="75151354">I'm seeing this as well on a dynamic field mapping that according to `_mapping` shows as a string but getting errors about it trying to parse it as a date field.  This happens fairly frequently to us as well

`shard-failed ([logstash-nginx_gator-2015.02.19][3], node[Ffuof69PSOaOxxlr4Lkosw], relocating [h9y4GeSnRiWh4fBkdrmqQA], [R], s[RELOCATING]), reason [Failed to perform [bulk/shard] on replica, message [RemoteTransportException[[logstashesfast03-logstash_es][inet[/10.45.4.115:9300]][bulk/shard/replica]]; nested: MapperParsingException[failed to parse [qs_start]]; nested: MapperParsingException[failed to parse date field [2015-02-18 17:00:46], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: "2015-02-18 17:00:46" is malformed at " 17:00:46"]; ]]`
</comment><comment author="garyelephant" created="2015-07-27T10:26:17Z" id="125158254">Does this problem still exists in es1.5.2 ?
</comment><comment author="jpountz" created="2015-07-27T10:27:24Z" id="125158498">Yes, this fix will be available in 2.0.
</comment><comment author="chenryn" created="2016-10-06T16:12:08Z" id="252011213">So, is the `"dynamic":"unsafe"` options exists now?
</comment><comment author="clintongormley" created="2016-10-07T17:52:03Z" id="252317696">@chenryn no - that was never added.  we're always safe
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/WriteFailureException.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperUtils.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapping.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParsedDocument.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/TranslogRecoveryPerformer.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySourceHandler.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/engine/ShadowEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/camelcase/CamelCaseFieldNameTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/genericstore/GenericStoreDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/pathmatch/PathMatchDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamictemplate/simple/SimpleDynamicTemplatesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/lucene/DoubleIndexingDocTest.java</file><file>src/test/java/org/elasticsearch/index/mapper/merge/TestMergeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/numeric/SimpleNumericTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/ConcurrentDynamicTemplateTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Mappings: Validate dynamic mappings updates on the master node.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperParsingException.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperUtils.java</file><file>src/main/java/org/elasticsearch/index/mapper/MergeContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/StrictDynamicMappingException.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalRootMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Mappings: Same code path for dynamic mappings updates and updates coming from the API.</comment></comments></commit></commits></item><item><title>Danish language analyzer not working.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8687</link><project id="" key="" /><description>Hi. In my application i use danish analyzer. I try to use standard danish analyzer but it seems it not working. If i understand correctly when i use analyzer like this:

users/_analyze?text=Ambjørn&amp;analyzer=danish

my text "Ambjørn" myst be transform to "ambjorn" or "ambjoorn" but this does not happend. When i try to use german analyzer at word "äußerst" 

users/_analyze?text=äußerst&amp;analyzer=german

and i get the correct result "ausserst".

Elasticsearch version 1.3.5.
</description><key id="50304856">8687</key><summary>Danish language analyzer not working.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2014-11-27T15:54:43Z</created><updated>2014-11-29T10:40:35Z</updated><resolved>2014-11-28T10:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-11-28T06:46:01Z" id="64860415">Hi,

German Analyzer is http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#german-analyzer
And German Analyzer use `german_normalization` token filter, that is Lucene's functionality.
http://lucene.apache.org/core/4_10_0/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html
This filter normalize German characters.

Unfortunately, Lucene does not have DanishNormalizationFilter just now.
Danish Analyzer setting is http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#danish-analyzer
does not include normalization filter.
</comment><comment author="clintongormley" created="2014-11-28T13:15:30Z" id="64892755">@vMozzie you can use the ICU folding token filter from https://github.com/elasticsearch/elasticsearch-analysis-icu
</comment><comment author="ghost" created="2014-11-29T10:40:35Z" id="64947837">Thanks. Already solve problem using mapping char filter, and scandinavian_folding filter.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose estimated disk usage and watermark information in nodes stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8686</link><project id="" key="" /><description>We currently log whether a node is over the high and low watermark once every 30 seconds. To be more efficient and reduce the amount of logs generated, we should log it only once (using a latch) when the watermark is exceeded, and once when the disk goes back under the watermark.

We should also expose whether a node is above the watermarks in the nodes stats.
</description><key id="50304669">8686</key><summary>Expose estimated disk usage and watermark information in nodes stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label></labels><created>2014-11-27T15:52:49Z</created><updated>2017-01-19T20:58:25Z</updated><resolved>2017-01-19T20:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2014-11-27T16:10:00Z" id="64808023">+1 for exposing this in node stats, way better than nagging in the logs
</comment><comment author="kimchy" created="2014-11-27T16:18:01Z" id="64808853">brainstorming here, but maybe we should have an allocation status API, and each decider can return an explanation + structured flags (allow_all, allow_primary, allow_replica) on a cluster view, node view, or index view (i.e. when concurrent recoveries is breached, ...). If its provided with a specific shard, then maybe we can give details on that specific shard?
</comment><comment author="dakrone" created="2014-11-27T16:20:12Z" id="64809080">@kimchy +1 on an allocation status API, I think we should separate the two (do both, but separately I mean), I think the disk usage percentage and watermark passed/not-passed should be exposed via the nodes stats API as part of the FsStats as a first step, then we can add the allocation status API as an additional step.
</comment><comment author="kimchy" created="2014-11-27T16:21:17Z" id="64809191">@dakrone ++
</comment><comment author="clintongormley" created="2016-11-26T13:48:38Z" id="263064383">@dakrone you planning on returning to this one at some stage?</comment><comment author="dakrone" created="2016-12-05T20:51:06Z" id="264973240">@clintongormley yes, I've updated the title for this as well for its actual work.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/monitor/MonitorService.java</file><file>core/src/main/java/org/elasticsearch/monitor/fs/FsInfo.java</file><file>core/src/main/java/org/elasticsearch/monitor/fs/FsProbe.java</file><file>core/src/main/java/org/elasticsearch/monitor/fs/FsService.java</file><file>core/src/main/java/org/elasticsearch/node/Node.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterInfoServiceIT.java</file><file>core/src/test/java/org/elasticsearch/monitor/fs/FsProbeTests.java</file></files><comments><comment>Expose disk usage estimates in nodes stats</comment></comments></commit></commits></item><item><title>Client: Only fetch the node info during node sampling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8685</link><project id="" key="" /><description>Today we are fetching a lot of information that is unneeded
for the sampling phase. We only really need the DiscoveryNode
to ensure the node is still there.
This commit clears all flags to be false on the NodeInfo call.
</description><key id="50302063">8685</key><summary>Client: Only fetch the node info during node sampling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-27T15:25:06Z</created><updated>2015-06-06T19:26:11Z</updated><resolved>2014-11-27T16:13:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-27T15:39:07Z" id="64804666">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>stuck initializing shards &amp; NumberFormatException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8684</link><project id="" key="" /><description>I have a cluster that is currently stuck (yellow state) with a logstash index that was created midnight unable to assign a shard and two shards initializing. I examined the elasticsearch logs and found the following messages appaearing a lot:

```
[2014-11-27 10:15:12,585][WARN ][cluster.action.shard     ] [192.168.1.13] [logstash-2014.11.27][4] sending failed shard for [logstash-2014.11.27][4], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[INITIALIZING], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [Failed to start shard, message [RecoveryFailedException[[logstash-2014.11.27][4]: Recovery failed from [192.168.1.14][sE51TBxfQ2q6pD5k7G7piA][es2.inbot.io][inet[/192.168.1.14:9300]] into [192.168.1.13][o9vhU4BhSCuQ4BmLJjPtfA][es1.inbot.io][inet[/192.168.1.13:9300]]{master=true}]; nested: RemoteTransportException[[192.168.1.14][inet[/192.168.1.14:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[logstash-2014.11.27][4] Phase[2] Execution failed]; nested: RemoteTransportException[[192.168.1.13][inet[/192.168.1.13:9300]][internal:index/shard/recovery/translog_ops]]; nested: NumberFormatException[For input string: "finished"]; ]]
```

and

```
[2014-11-27 10:17:54,187][WARN ][cluster.action.shard     ] [192.168.1.14] [logstash-2014.11.27][4] sending failed shard for [logstash-2014.11.27][4], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[INITIALIZING], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [Failed to perform [indices:data/write/bulk[s]] on replica, message [RemoteTransportException[[192.168.1.13][inet[/192.168.1.13:9300]][indices:data/write/bulk[s][r]]]; nested: NumberFormatException[For input string: "finished"]; ]]
```

The NumberFormatException looks like a possible cause. One possible explanation is that we have dynamically mapped logstash field that is sometimes a string and sometimes a number and since we roll over the index there's a chance that this field gets mapped incorrectly depending on what comes in first. However, I don't see how this should block shard initialization. Since midnight we've accumulated about 300M of errors like above. Normally our logs for each day are in the range of a few KB.

The index is actually available and accepting writes (i.e. kibana works as you would expect). But it likely is missing updates for some shards but if so, that is not apparent from the logs.

```
[linko@es3 elasticsearch]$ curl -XGET 'localhost:9200/_cluster/health/logstash-2014.11.27/?pretty'
{
  "cluster_name" : "linko_elasticsearch",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 5,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 5,
  "active_shards" : 12,
  "relocating_shards" : 0,
  "initializing_shards" : 2,
  "unassigned_shards" : 1
}
```

Our cluster has been running for a few weeks. We haven't really done any config changes lately. At least not on our logstash indices. This issue has happened before and I resolved it with a rolling restart at the time.

I'm running 1.4.0 and have not upgraded to 1.4.1 yet. I'm planning to do so later today and I hope this problem will go away with a rolling restart. Meanwhile, I'm available for the next two hours or so to do more diagnostics on this cluster to get more info if needed/useful. If so, please let me know. 
</description><key id="50293007">8684</key><summary>stuck initializing shards &amp; NumberFormatException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels /><created>2014-11-27T13:47:37Z</created><updated>2016-05-18T16:31:32Z</updated><resolved>2014-11-28T12:48:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jillesvangurp" created="2014-11-27T13:51:00Z" id="64792496">some more info from the log

```
[2014-11-27 00:48:48,050][DEBUG][action.bulk              ] [192.168.1.15] [logstash-2014.11.26][3] failed to execute bulk item (index) index {[logstash-2014.11.26][scheduler][AUnugds1mlBs-FhUr0lx], source[{"@version":1,"@timestamp":"2014-11-27T00:48:47.322+01:00","level":"INFO","type":"scheduler","logger_name":"dispatcher","job":{"id":"q4JDAm8mvtE","worker":"exchange","task":"ts_sync","status":"failed","meta":{},"service":"provider","node":{"name":"exchange","host":"https://api.inbot.io","running":0,"jobs":[]},"account":{"provider":"exchange","human_uid":"jouni.hannula@vauraus.fi","credentials":{"password_encrypted":"3D4E96CAE509453E59AEC4FEDB4FDA7F$JAY9bnLFEfkDRtdos9mhEBkkLb-9BHlNTXGNI-5vpmU","username":"jouni.hannula@vauraus.fi","password":"xlYuPyC+nDuT/WwLlzLRnKIcAxcnnIyZdn+BBbwSYBA="}},"user":{"id":"IJ77m8NgLVr5J1Nrha_pSg"},"parameters":{},"result":{"ingestedObjects":0},"error":{"name":"UnexpectedWorkerError","message":"Unexpected error from inbot server: null","error_handle":"error","original_error":{"cause":{"suppressed":[]},"suppressed":[]},"error":"io.linko.worker.exceptions.ExchangeException: Unexpected error from inbot server: null\n\tat io.linko.worker.services.InbotConnector.ingestData(InbotConnector.java:62)\n\tat io.linko.worker.services.ExchangeService.queryFolder(ExchangeService.java:425)\n\tat io.linko.worker.services.ExchangeService.tsSync(ExchangeService.java:258)\n\tat io.linko.worker.resources.WorkerResource.lambda$postTsSync$155(WorkerResource.java:102)\n\tat io.linko.worker.resources.WorkerResource$$Lambda$77/63591602.apply(Unknown Source)\n\tat io.linko.worker.resources.WorkerResource.execExchange(WorkerResource.java:178)\n\tat io.linko.worker.resources.WorkerResource.postTsSync(WorkerResource.java:97)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:151)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:171)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:152)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:104)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:384)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:342)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:101)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:271)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:297)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:254)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1030)\n\tat org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:378)\n\tat io.linko.ng.server.InbotHttpContainer.service(InbotHttpContainer.java:83)\n\tat org.glassfish.grizzly.http.server.HttpHandler.runService(HttpHandler.java:204)\n\tat org.glassfish.grizzly.http.server.HttpHandler.doHandle(HttpHandler.java:178)\n\tat org.glassfish.grizzly.http.server.HttpHandlerChain.doHandle(HttpHandlerChain.java:197)\n\tat org.glassfish.grizzly.http.server.HttpServerFilter.handleRead(HttpServerFilter.java:235)\n\tat org.glassfish.grizzly.filterchain.ExecutorResolver$9.execute(ExecutorResolver.java:119)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:284)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:201)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:133)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:112)\n\tat org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:561)\n\tat org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:112)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:117)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.access$100(WorkerThreadIOStrategy.java:56)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy$WorkerThreadRunnable.run(WorkerThreadIOStrategy.java:137)\n\tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:565)\n\tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:545)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: javax.ws.rs.ProcessingException\n\tat org.glassfish.jersey.grizzly.connector.GrizzlyConnector.apply(GrizzlyConnector.java:259)\n\tat org.glassfish.jersey.client.ClientRuntime.invoke(ClientRuntime.java:246)\n\tat org.glassfish.jersey.client.JerseyInvocation$1.call(JerseyInvocation.java:667)\n\tat org.glassfish.jersey.client.JerseyInvocation$1.call(JerseyInvocation.java:664)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:228)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:424)\n\tat org.glassfish.jersey.client.JerseyInvocation.invoke(JerseyInvocation.java:664)\n\tat org.glassfish.jersey.client.JerseyInvocation$Builder.method(JerseyInvocation.java:424)\n\tat org.glassfish.jersey.client.JerseyInvocation$Builder.post(JerseyInvocation.java:333)\n\tat io.linko.worker.utils.RestClient.POST(RestClient.java:81)\n\tat io.linko.worker.services.InbotConnector.ingestData(InbotConnector.java:51)\n\t... 47 more\nCaused by: java.lang.NullPointerException\n\tat io.linko.ng.server.JsonArrayProvider.writeTo(JsonArrayProvider.java:74)\n\tat io.linko.ng.server.JsonArrayProvider.writeTo(JsonArrayProvider.java:35)\n\tat org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.invokeWriteTo(WriterInterceptorExecutor.java:265)\n\tat org.glassfish.jersey.message.internal.WriterInterceptorExecutor$TerminalWriterInterceptor.aroundWriteTo(WriterInterceptorExecutor.java:250)\n\tat org.glassfish.jersey.message.internal.WriterInterceptorExecutor.proceed(WriterInterceptorExecutor.java:162)\n\tat org.glassfish.jersey.message.internal.MessageBodyFactory.writeTo(MessageBodyFactory.java:1154)\n\tat org.glassfish.jersey.client.ClientRequest.writeEntity(ClientRequest.java:503)\n\tat org.glassfish.jersey.grizzly.connector.GrizzlyConnector$7.writeEntity(GrizzlyConnector.java:500)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$EntityWriterBodyHandler.doHandle(GrizzlyAsyncHttpProvider.java:2117)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider.sendRequest(GrizzlyAsyncHttpProvider.java:560)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$AsyncHttpClientFilter.sendAsGrizzlyRequest(GrizzlyAsyncHttpProvider.java:943)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$AsyncHttpClientFilter.handleWrite(GrizzlyAsyncHttpProvider.java:811)\n\tat org.glassfish.grizzly.filterchain.ExecutorResolver$8.execute(ExecutorResolver.java:111)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:284)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:201)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:133)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:112)\n\tat org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.write(DefaultFilterChain.java:413)\n\tat org.glassfish.grizzly.nio.NIOConnection.write(NIOConnection.java:407)\n\tat org.glassfish.grizzly.nio.NIOConnection.write(NIOConnection.java:381)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider.execute(GrizzlyAsyncHttpProvider.java:322)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$1.completed(GrizzlyAsyncHttpProvider.java:238)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$1.completed(GrizzlyAsyncHttpProvider.java:224)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$ConnectionManager$1.completed(GrizzlyAsyncHttpProvider.java:2572)\n\tat com.ning.http.client.providers.grizzly.GrizzlyAsyncHttpProvider$ConnectionManager$1.completed(GrizzlyAsyncHttpProvider.java:2550)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOConnectorHandler$EnableReadHandler.onComplete(TCPNIOConnectorHandler.java:342)\n\tat org.glassfish.grizzly.ProcessorExecutor.complete(ProcessorExecutor.java:115)\n\tat org.glassfish.grizzly.ProcessorExecutor.complete0(ProcessorExecutor.java:204)\n\tat org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:86)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:561)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOConnectorHandler.onConnectedAsync(TCPNIOConnectorHandler.java:221)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOConnectorHandler$1.connected(TCPNIOConnectorHandler.java:154)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOConnection.onConnect(TCPNIOConnection.java:258)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:552)\n\tat org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:112)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:117)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.executeIoEvent(WorkerThreadIOStrategy.java:103)\n\tat org.glassfish.grizzly.strategies.AbstractIOStrategy.executeIoEvent(AbstractIOStrategy.java:89)\n\tat org.glassfish.grizzly.nio.SelectorRunner.iterateKeyEvents(SelectorRunner.java:414)\n\tat org.glassfish.grizzly.nio.SelectorRunner.iterateKeys(SelectorRunner.java:383)\n\tat org.glassfish.grizzly.nio.SelectorRunner.doSelect(SelectorRunner.java:347)\n\tat org.glassfish.grizzly.nio.SelectorRunner.run(SelectorRunner.java:278)\n\t... 3 more\n","code":500},"stats":{},"updated_at":"2014-11-26T23:48:47.321Z","started_at":"2014-11-26T23:47:57.673Z","self":"http://localhost:3200/scheduler/status/q4JDAm8mvtE"},"source":"dispatcher.finished","message":"Job finished with status failed","host":"app2.inbot.io"}]}
org.elasticsearch.index.mapper.MapperParsingException: object mapping for [scheduler] tried to parse as object, but got EOF, has a concrete value been provided to it?
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:498)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:541)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:490)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:392)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:444)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:150)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2014-11-27 01:00:00,747][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] creating index, cause [auto(bulk api)], shards [5]/[2], mappings [_default_]
[2014-11-27 01:00:01,862][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:02,082][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [collectd] (dynamic)
[2014-11-27 01:00:04,522][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_metrics] (dynamic)
[2014-11-27 01:00:07,358][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [nginx_access] (dynamic)
[2014-11-27 01:00:07,527][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:08,095][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)
[2014-11-27 01:00:08,218][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:08,363][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:08,533][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:10,832][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:11,375][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:45,149][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:00:45,963][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:02:18,291][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)
[2014-11-27 01:02:28,603][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:02:57,662][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:02:58,641][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:03:48,477][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [nginx_access] (dynamic)
[2014-11-27 01:03:55,143][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:03:55,497][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:03:55,745][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:03:56,145][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)
[2014-11-27 01:03:57,799][WARN ][cluster.action.shard     ] [192.168.1.15] [logstash-2014.11.27][1] received shard failed for [logstash-2014.11.27][1], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[STARTED], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [Failed to perform [indices:data/write/bulk[s]] on replica, message [RemoteTransportException[[192.168.1.13][inet[/192.168.1.13:9300]][indices:data/write/bulk[s][r]]]; nested: NumberFormatException[For input string: "finished"]; ]]
[2014-11-27 01:03:57,879][WARN ][cluster.action.shard     ] [192.168.1.15] [logstash-2014.11.27][1] received shard failed for [logstash-2014.11.27][1], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[STARTED], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [engine failure, message [indices:data/write/bulk[s] failed on replica][NumberFormatException[For input string: "finished"]]]
[2014-11-27 01:03:58,152][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [scheduler] (dynamic)
[2014-11-27 01:03:58,252][INFO ][cluster.metadata         ] [192.168.1.15] [logstash-2014.11.27] update_mapping [inbot_api] (dynamic)
[2014-11-27 01:04:22,901][WARN ][cluster.action.shard     ] [192.168.1.15] [logstash-2014.11.27][3] received shard failed for [logstash-2014.11.27][3], node[o9vhU4BhSCuQ4BmLJjPtfA], [R], s[STARTED], indexUUID [-mMLqYjAQuCUDcczYf5SHA], reason [engine failure, message [indices:data/write/bulk[s] failed on replica][NumberFormatExceptio:
```
</comment><comment author="clintongormley" created="2014-11-27T14:39:48Z" id="64797640">Hi @jillesvangurp 

It sounds like you have inconsistent mapping between shards, so the same field was added to different shards at the same time: once as a string and once as a number.  Only one of those mappings made it into the master's version of the mapping (the number won, by the sounds of it). (Note: we're planning on fixing this so that we don't get into these situations any more).

Now, you're trying to recover a replica (which uses the master's version of the mapping) from a primary which thinks that that field is a string.  When it tries to replay the translog, it is getting these number format exceptions.

So: how to recover from this situation?  I think you have a few options, but either way that field is useless. You can't reliably search on it. You definitely can't load fielddata for it (ie aggs or sorting or scripts), and if you have doc values, you are probably going to have merge failures later on.

First step:  update the field mapping to set `ignore_malformed: true`.  Hopefully this will allow you to recover the shard and to continue indexing.

If that isn't sufficient, you may have to:
- stop indexing
- flush the index
- close the index
- open the index

That should allow shards to recover correctly, using the mapping from the master.

If that still doesn't work, then you may have to delete that shard and use the cluster reroute API to force assign an empty primary shard.

The way to avoid this going forward is to map this field explicitly.
</comment><comment author="jillesvangurp" created="2014-11-27T16:35:29Z" id="64810736">Thanks for the detailed explanation. Makes sense. I've deleted the index (easiest) and we are going to fix our logstash mapping to map most fields to strings explicitly (except those we care about being numbers). I think this should probably go into the logstash documentation in some form. The out of the box mapping will get you in trouble.

The ignore_malformed option also sounds like a good idea.
</comment><comment author="clintongormley" created="2014-11-28T12:48:55Z" id="64890608">Closing this in favour of #8688
</comment><comment author="eugenp" created="2015-01-13T20:04:22Z" id="69809901">I'm also seeing something very similar - using 1.4.1: 

```
[2015-01-13 19:54:33,762][WARN ][indices.cluster          ] [Caregiver] [prod_defaultorg_event][8] failed to start shard
org.elasticsearch.indices.recovery.RecoveryFailedException: [prod_defaultorg_event][8]: Recovery failed from [Gronk][AWrt4ZBVQi2Ad8U7DK7dlA][ip-172-31-20-162][inet[/172.31.20.162:9300]]{master=false} into [Caregiver][GGL-OXkdSQC0Ro5vPQN3fg][ip-172-31-17-177][inet[ip-172-31-17-177.ec2.internal/172.31.17.177:9300]]{master=false}
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:308)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$200(RecoveryTarget.java:65)
        at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:177)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [Gronk][inet[/172.31.20.162:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [prod_defaultorg_event][8] Phase[2] Execution failed
        at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1136)
        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:654)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:137)
        at org.elasticsearch.indices.recovery.RecoverySource.access$2600(RecoverySource.java:74)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:464)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:450)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [Caregiver][inet[/172.31.17.177:9300]][internal:index/shard/recovery/translog_ops]
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse [details.dataPack.1.wsStat]
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:415)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:707)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:500)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:555)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:490)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:555)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:490)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:555)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:490)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:541)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:490)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:392)
        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:775)
        at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:433)
        at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:412)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "IDLE"
        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
        at java.lang.Double.parseDouble(Double.java:538)
        at org.elasticsearch.common.xcontent.support.AbstractXContentParser.doubleValue(AbstractXContentParser.java:182)
        at org.elasticsearch.index.mapper.core.DoubleFieldMapper.innerParseCreateField(DoubleFieldMapper.java:310)
        at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:235)
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:405)
        ... 18 more
```
- now I'm also seeing: 

```
[2015-01-13 19:54:33,788][WARN ][cluster.action.shard     ] [Caregiver] [prod_defaultorg_event][8] sending failed shard for [prod_defaultorg_event][8], node[GGL-OXkdSQC0Ro5vPQN3fg], [R], s[INITIALIZING], indexUUID [sNPkCODUS9WA4J4_L0x5qQ], reason [Failed to start shard, message [RecoveryFailedException[[prod_defaultorg_event][8]: Recovery failed from [Gronk][AWrt4ZBVQi2Ad8U7DK7dlA][ip-172-31-20-162][inet[/172.31.20.162:9300]]{master=false} into [Caregiver][GGL-OXkdSQC0Ro5vPQN3fg][ip-172-31-17-177][inet[ip-172-31-17-177.ec2.internal/172.31.17.177:9300]]{master=false}]; nested: RemoteTransportException[[Gronk][inet[/172.31.20.162:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[prod_defaultorg_event][8] Phase[2] Execution failed]; nested: RemoteTransportException[[Caregiver][inet[/172.31.17.177:9300]][internal:index/shard/recovery/translog_ops]]; nested: MapperParsingException[failed to parse [details.dataPack.1.wsStat]]; nested: NumberFormatException[For input string: "IDLE"]; ]]
```

As a result, 3 replica shards keep moving back and forth between nodes in a constant INITIALIZING state. 

Is it the same issue or potentially something else?
</comment><comment author="clintongormley" created="2015-01-15T19:43:28Z" id="70148366">@eugenp looks like the same problem
</comment><comment author="Mannoj87" created="2016-05-18T16:31:32Z" id="220083435">**_"It sounds like you have inconsistent mapping between shards"_**
@clintongormley  - You mean to say the mapping/structure of an index can be different node wise? Or is it something internally it had hit a bug which accepted the data that it is not supposed to? Please clarify.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't mark cluster health as timed out if desired state is reached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8683</link><project id="" key="" /><description>Today we mark cluster health requests as timed out if the request has already
timed out. Yet, and implementation detail of the health request is that we are
waiting for events which can take quite some time if the machine is busy. If
we have already reached a valid state while waiting for events to be processed
we shouldn't mark the clusterstate as timed-out. This will help tests that
wait for green state with lots of nodes and shards to not fail the cluster health
calls.
</description><key id="50289877">8683</key><summary>Don't mark cluster health as timed out if desired state is reached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-27T13:11:06Z</created><updated>2015-06-07T10:24:54Z</updated><resolved>2014-11-27T16:13:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-27T14:06:06Z" id="64794067">Left one comment. 
</comment><comment author="s1monw" created="2014-11-27T15:39:37Z" id="64804720">pushed an update here @bleskes 
</comment><comment author="bleskes" created="2014-11-27T15:56:34Z" id="64806623">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade API should also migrate index settings/mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8682</link><project id="" key="" /><description>Up until now we have maintained the ability to understand old mappings, but left the old mappings in place.  Instead, we should migrate the old mappings to new mappings where possible.

If it is impossible to migrate mappings, then we should return an exception explaining the problem and instruct the user to reindex their data instead.
</description><key id="50282392">8682</key><summary>Upgrade API should also migrate index settings/mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Upgrade API</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-27T11:39:23Z</created><updated>2015-06-07T17:42:19Z</updated><resolved>2015-06-07T17:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-07T17:42:19Z" id="109781683">Handled on case by case basis
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature Request: Keyed facet buckets for terms facets etc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8681</link><project id="" key="" /><description>Hi there. First off great tool, picked it up very quickly. One thing that's quite annoying from a normalisation point of view is the fact that one cannot set facet buckets for terms facets to keyed : true.

I know it's possible with ranges/date ranges and this makes life nice and easy when looping buckets but when trying to normalise the output i/e make sure all KEYS are the actual bucket name makes for unneeded code client side. 

The buckets for terms are numbered 0,1,2,3,4 etc and it would be much better to have a "keyed" option as we have in ranges/date ranges to have ES key the buckets with the key of the facet.

Just an idea ;)
</description><key id="50275210">8681</key><summary>Feature Request: Keyed facet buckets for terms facets etc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NodexTech</reporter><labels><label>discuss</label></labels><created>2014-11-27T10:18:21Z</created><updated>2015-11-21T21:55:45Z</updated><resolved>2015-11-21T21:55:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-27T10:25:18Z" id="64771849">Hi @NodexTech 

Do you mean aggregations rather than facets? Facets are deprecated.

Of course, supporting `key: true` for terms aggs means that you will lose any sort order, which is usually important for terms aggs.
</comment><comment author="NodexTech" created="2014-11-27T10:33:53Z" id="64772911">Yes, aggregations sorry.

We lose sort order in ranges anyway, it simply sorts them by what is fed to it. I don't see any real reason why sort order would be lost but I don't know the inner workings of how the aggregation buckets are built. Some psuedo code (if you wanted to keep order and at a performance loss) would be to build the buckets first then loop back over them swapping the numeric key for the bucket key - this way they keep their original sort order as defined by the client / ES
</comment><comment author="clintongormley" created="2014-11-27T10:41:07Z" id="64773919">Sort order would be lost because, with a keyed response, buckets are returned as an unordered map instead of as an ordered array.

I don't understand what you mean by the terms buckets being numbered.  This is what you get back from a terms agg:

```
     "buckets": [
        {
           "key": "one",
           "doc_count": 1
        },
        {
           "key": "two",
           "doc_count": 1
        }
     ]
```

So the key (the term) is already available to you.  Perhaps some JSON examples demonstrating the problem would help?
</comment><comment author="NodexTech" created="2014-11-27T10:50:08Z" id="64774898">```
"buckets": [
        {
           "key": "one",
           "doc_count": 1
        },
        {
           "key": "two",
           "doc_count": 1
        }
     ]
```

Those buckets in Json can be thought of as 0,1 in terms of keys. When some languages decode JSON back into hashes/arrays they automatically number the keys which would essentially make the  look like this (I know it's a bad example to number JSON arrays)

```
"buckets": 
        {
            "0": {
                "key": "one",
                "doc_count": 1
            }
        },
        {
            "1": {
                "key": "two",
                "doc_count": 1
            }
        }

```

What I'm saying is that the order is pre-determined by ES and added to the ordered array ergo the order is already set. Looping that ordered array and replacing each array member with a new key (the "key" member of the array) and /value (a copy of the original member) might look like this (when replaced:

```
"buckets": 
        {
            "one": {
                "key": "one",
                "doc_count": 1
            }
        },
        {
            "two": {
                "key": "two",
                "doc_count": 1
            }
        }

```

The option is already there in ranges / date ranges which specifically de-normalises bucket responses but the (original) order is still kept (highest first).
</comment><comment author="clintongormley" created="2014-11-27T11:02:18Z" id="64776257">Your last example is an unordered map/object. Any order is lost - having the "two" key before the "one" key would have exactly the same meaning as "one" before "two".

&gt; When some languages decode JSON back into hashes/arrays they automatically number the keys which would essentially make the look like this (I know it's a bad example to number JSON arrays)

What language does that? I presume the structure that it returns is actually like this:

```
{
  "buckets": {
    "0": {
      "key": "one",
      "doc_count": 1
    },
    "1": {
      "key": "two",
      "doc_count": 1
    }
  }
}
```

Does this language not have support for arrays? Is it PHP, where a map is implemented as an associative array?
</comment><comment author="NodexTech" created="2014-11-27T11:12:34Z" id="64777269">The map might be returned ordered but it can be looped and given back in it's original determined order -but- with new keys (this is how ranges are returned when "keyed" is true) - I presume that they're built in the same way?, the order is defined by ES before the map is built so that original order/map can be in fact looped and just have the keys changed.

The language that decodes the JSON (I don't feel) is relevant, it would be nice to see the functionality inside ES so to be able to normalise the responses of Aggregations if one wanted to, currently there is a lot of checking client side for "if type = range then rename the key to this....." which with a simple configuration option on a per aggregation basis would be eliminated
</comment><comment author="clintongormley" created="2014-11-27T12:47:26Z" id="64786188">&gt; The map might be returned ordered but it can be looped and given back in it's original determined order -but- with new keys (this is how ranges are returned when "keyed" is true) - I presume that they're built in the same way?, the order is defined by ES before the map is built so that original order/map can be in fact looped and just have the keys changed.

JSON objects are unordered by definition.  They may be produced in an ordered manner by Elasticsearch but that is besides the point.  They can equally be returned in a different order, or be decoded then reencoded in a different order.

Any JSON decoder which adds numeric keys to a JSON structure like that is just wrong.  I can only assume that it is working around a deficiency in the language.

If we were to support the `keyed` option on the `terms` agg, then I'm not sure it would return the structure that you're after.  Your last example isn't valid JSON - it should either be:

```
{
  "buckets": {
    "one": {
      "key": "one",
      "doc_count": 1
    },
    "two": {
      "key": "two",
      "doc_count": 1
    }
  }
}
```

or:

```
{
  "buckets": [
    {
      "one": {
        "key": "one",
        "doc_count": 1
      }
    },
    {
      "two": {
        "key": "two",
        "doc_count": 1
      }
    }
  ]
}
```

The first one is the equivalent of the output from a keyed  `range` aggregation, and loses order (because it returns a JSON object, not an array).

The second one maintains order, but adds a level of indirection (the key) that is of no benefit. Also, if I understand your description of how your JSON decoder works, it would just end up producing:

```
{
  "buckets": [
    {
      "0": {
        "one": {
          "key": "one",
          "doc_count": 1
        }
      }
    },
    {
      "1": {
        "two": {
          "key": "two",
          "doc_count": 1
        }
      }
    }
  ]
}
```
</comment><comment author="NodexTech" created="2014-11-27T13:17:35Z" id="64789191">OK I think we're getting off point here. My feature suggestion is not to argue the language semantic differences between maps, hashes, arrays etc. I don't think I've explained how to achieve desired results very well. I do not know any Java so I will have to make do with Javascript and hope that my psuedo code comes across okay.

```
/*
 for the sake of the code I am assuming that with no sorting or ordering on an aggregation that ES gives me this back for a specific aggregation (this is a JSON object in Javascript)
*/
    var buckets=[
            {
            "key" : "one",
            "doc_count" : 5,
            },
            {
            "key" : "three",
            "doc_count" : 3
            },
            {
            "key" : "two",
            "doc_count" : 1
            }
];

/*  
 Now internaly we know the order because it's right above us in a variable called "buckets"
 we can now create a NEW bucket array USING the above order to create the CORRECT mapping BUT adding a key...
*/
    var buckets_keyed=[];
    buckets.forEach(function($value,$key){
        buckets_keyed[$value['key']]=buckets[$key];
    });
/*
    give us...
[
    {
        "one": {
            "key": "one",
            "doc_count": 5
        },
        "three": {
            "key": "three",
            "doc_count": 3
        },
        "two": {
            "key": "two",
            "doc_count": 1
        }
    }
]
/*
as the variable "buckets_keyed" which maintained it's ORIGINAL order
var buckets=buckets_keyed; &lt;--- now has the chosen order that ES or the client decided upon
and also has key names for normalisation (obvisouly as an OPTION not by default.... 

*/
```

So I cannot see how the mapping and ordering is not maintained, obviously it's an extra step which is why it will carry optional performance penalties (as with most optional things)
</comment><comment author="clintongormley" created="2014-11-27T13:28:13Z" id="64790265">&gt; /*
&gt;  for the sake of the code I am assuming that with no sorting or ordering on an aggregation that ES gives me this back for a specific aggregation (this is a JSON object in Javascript)
&gt; */
&gt;     var buckets=[

This is not a JSON object.  This is an array.  A JSON object would look like this:

```
var buckets = {...}
```

&gt; ```
&gt; give us...
&gt; ```
&gt; 
&gt; [
&gt;    {
&gt;        "one": {
&gt;            "key": "one",
&gt;            "doc_count": 5
&gt;        },
&gt;        "three": {
&gt;            "key": "three",
&gt;            "doc_count": 3
&gt;        },
&gt;        "two": {
&gt;            "key": "two",
&gt;            "doc_count": 1
&gt;        }
&gt;    }
&gt; ]

You now have an array with one element, containing an **unordered** JSON object.  JSON objects do not preserve order.  The first line on http://www.json.org/javadoc/org/json/JSONObject.html says: 

```
A JSONObject is an unordered collection of name/value pairs.
```

You cannot have a JSON object that is ordered.  It is part of the definition.
</comment><comment author="NodexTech" created="2014-11-27T13:50:25Z" id="64792444">Yes UN-Ordered by the language specifics but we've given it an order ourselves else ES would never be able to order something and then send it out in some form of order and there would be chaos.

My code was showing how it can be done in Javascript, the semantics of the language are not relevant. In this case, the relevance is that ES can and does allow "key-ing" (for lack of a better word!) of array members for ranges and NOT for terms (and perhaps others) so I know it can be done in ES but for some reason it's not an option to have it done. 

Again I don't think I'm explaining things very well but here goes again.

Ranges (normal and date) allow for this kind of response (ordered by however ES defaultly orders them - which is by "doc_count"), this is triggered by an option called "keyed"

```
{
        "foo": {
            "key": "foo",
            "doc_count": 50
        }
    },
    {
        "bar": {
            "key": "bar",
            "doc_count": "45"
        }
    }
```

...... Whereas "terms" aggregations don't allow the option "keyed". What I am suggesting is to allow the option and it will simply be a second cycle to re-key the output to more represent what a "keyed" range output looks like - the order is maintained because it's already been set by creating the original array/map/bucket set, all that would happen is a simple walk through the original array/map/bucket set (which maintains the order as set by ES / the client/user) and build a NEW bucket set, then copy it over the top of the old one (replace the old one). Keys are now normalised and predictable so one can say a key is always a key not just a pointer to the current array iteration and the order is maintained and predictable (because it's not been changed).

JSON arrays are on-ordered for a very good reason; which, is predictability This is so a client doesn't have to search an array each time to find a key to then extract said part of the array, i/e what gets fed into a JSON string even after serialization/de-serialization in any language (as long as the language doesn't apply some form of array sorting) you will ALWAYS get back out the same order you put in. Sometimes in ES the order (in terms of ranges) is determined by ES and you can only sort / order by things asc/desc - this doesn't always make sense to do because sometimes people want to set the order themselves which has no corresponding sorting algorithm (asc/desc or whatever in ES), fortunately when aggregating ranges we can key things which allows us to do the arbitrary sort on the client side.

As an example, SOLR (also Lucene based) gives you aggregations (they're called facets in SOLR) back in the order that you gave them in by DEFAULT and also allows you to sort them in various ways - this is a lot more flexible but also a little off the topic.
</comment><comment author="clintongormley" created="2015-11-21T21:55:45Z" id="158684796">Reread this ticket and I still don't get the point.  There has been no further interest in the last year, so I'm going to close this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] make sure rest tests info is printed for any @Rest annotated test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8680</link><project id="" key="" /><description>We introduced the @Rest annotation a while ago for REST tests (see #7795), we have then to make sure that relevant info to reproduce failures gets printed out for any test that is marked with such annotation, not only for ElasticsearchRestTests
</description><key id="50269707">8680</key><summary>[TEST] make sure rest tests info is printed for any @Rest annotated test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-27T09:20:53Z</created><updated>2014-11-27T09:43:34Z</updated><resolved>2014-11-27T09:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-27T09:35:11Z" id="64766036">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[Docs] Fix missing comma in mapping</comment></comments></commit></commits></item><item><title>Fix `exclude` with artificial documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8679</link><project id="" key="" /><description>Artificial documents get assigned a random id. When `include` is set to `false` (default), the ids of these documents also get included, when they should rather be ignored.
</description><key id="50267503">8679</key><summary>Fix `exclude` with artificial documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-27T08:51:12Z</created><updated>2015-06-07T18:37:16Z</updated><resolved>2014-11-28T07:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-27T09:36:24Z" id="64766173">@alexksikes Can you include a description of the bug in the summary of this PR? It would be helpful when reviewing the changes list.
</comment><comment author="jpountz" created="2014-11-27T17:45:33Z" id="64817248">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/test/java/org/elasticsearch/mlt/MoreLikeThisActionTests.java</file></files><comments><comment>MLT Query: Fix exclude with artificial documents</comment></comments></commit></commits></item><item><title>Index structure mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8678</link><project id="" key="" /><description>Hi, I am trying to create an index with elasticsearch but I get the following error...any help?(thanks)

$&gt; curl -XPUT 'http://localhost:9200/posts' -d @posts.json
{"error":"MapperParsingException[mapping [post]]; nested: IllegalArgumentException[precisionStep must be &gt;= 1 (got 0)]; ","status":400}

here is the content of posts.json
{
  "mappings": {
    "post": {
      "properties": {
        "id": {
          "type": "long",
          "store": "yes",
          "precision_step": "0"
        },
        "name": {
          "type": "string",
          "store": "yes",
          "index": "analyzed"
        },
        "published": {
          "type": "date",
          "store": "yes",
          "precision_step": "0"
        },
        "contents": {
          "type": "string",
          "store": "no",
          "index": "analyzed"
        }
      }
    }
  }
}
</description><key id="50238653">8678</key><summary>Index structure mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pualo2014</reporter><labels /><created>2014-11-26T23:34:35Z</created><updated>2014-11-27T04:45:22Z</updated><resolved>2014-11-27T04:45:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Stuck shard causing client nodes to run out of memory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8677</link><project id="" key="" /><description>We had an instance where a shard is stuck during recovery (in initializing state) due to peer node connectivity issues during recovery.  When this occurs with concurrent indexing and searching to the cluster, all the client nodes started to run out of memory (adding a new client node does not help for that new client node will also end up running out of memory).  We collected the heap dump on OOM and it looks like a lot of heap is used in the netty buffer which doesn't seem right.

![screen shot 2014-11-26 at 2 49 04 pm](https://cloud.githubusercontent.com/assets/7216393/5208066/4368750a-7565-11e4-966c-a8cedbe2abff.png)
</description><key id="50222212">8677</key><summary>Stuck shard causing client nodes to run out of memory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Circuit Breakers</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-26T20:11:38Z</created><updated>2016-11-26T12:58:51Z</updated><resolved>2016-11-26T12:58:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-26T22:16:36Z" id="64718648">what version is this on?
</comment><comment author="ppf2" created="2014-11-26T22:32:18Z" id="64720313">1.3.4
</comment><comment author="bleskes" created="2014-11-27T08:12:07Z" id="64757817">I think I see what's going on. Normally, the client nodes receive a bulk request, splits it in shard bulks and send it on to the node where the primary is hosted. If the primary is not yet ready, the client node will wait for it to start. It does so by registering a listener for a cluster state update. Problem is that by registering the listener, it frees up a bulk thread which happily accepts another bulk request and so forth. We need to find a way to apply some back pressure in this case. 

In the past we had the idea of having a central listener for all shard related issues - that would help here too.

An alternative is to limit the amount of "transient" cluster state listener which will throttle this but also other similar requests (like waiting for a master nodes).
</comment><comment author="kimchy" created="2015-04-28T07:43:04Z" id="96964627">@bleskes I think that potentially adding a circuit breaker on "public" actions, count towards it and down it when we send the response back through the channel might help in a more generic fashion, for most use cases.
</comment><comment author="bleskes" created="2015-05-18T08:15:19Z" id="102964887">@kimchy yeah, that's another more generic solution. I wonder if we shouldn't just add indexing request to the current request circuit breaker, which is currently only used for big arrays, i.e., per requests aggs. We might want to split it into "bigarrays" and "indexing" sub breakers so it will be easy to monitor. I briefly looked at the code and it seems doable.
</comment><comment author="clintongormley" created="2016-11-26T12:58:51Z" id="263062194">Fixed by the inflight circuit breaker https://github.com/elastic/elasticsearch/pull/17133</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change of behavior with bool querry (empty `should` clause)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8676</link><project id="" key="" /><description>When executing a bool query with an empty should array, there is a change in behavior from previous versions:

Consider this query:

``` json
POST /articles/_search
{
    "from": 0,
    "size": 100,
    "sort": [{"published_at": { "order": "desc"}}],
"query": {
    "filtered": {
       "query": {
       "bool": {
           "should": [

           ]
       }
       },
       "filter": {
           "and": {
              "filters": [
                 {"range": {
                    "published_at": {
                       "from": "now-3M/d",
                       "to": "now"
                    }
                 }},
                 {
                     "not": {
                        "filter": {
                            "terms": {
                               "_id": [
                                  "53fe1068c3b5a026a600002a",
                                  "53fe1068c3b5a026a600002a"
                               ]
                            }
                        }
                     }
                 }
              ]
           }
       }
    }
}
}
```

In version: `Version: 1.2.1, Build: 6c95b75/2014-06-03T15:02:52Z, JVM: 1.7.0_51`

The previous query will return this result:

``` json
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}
```

Whereas from version:  `Version: 1.3.4, Build: a70f3cc/2014-09-30T09:07:17Z, JVM: 1.7.0_65`
and above, it will give the following result:

``` json
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 15840,
      "max_score": null,
      "hits": [
         {
            "_index": "articles",
            "_type": "article",
            "_id": "5474bsebc3s5s015es00ss5d",
            "_score": null,
            "_source": {
               "categories": [],
               "created_at": "2014-11-25T17:35:39Z",
               "keywords": [
                  null,
                  null,
                  null,
                  null,
                  null,
                  null,
                  null,
                  null,
                  null
               ],
               "open_calais_keywords": [
                  "transport",
                  "private transport",
                  "land transport",
                  "sports cars",
                  "chevrolet corvette",
                  "muscle car",
                  "chevrolet",
                  "chevrolet corvette c5 z06",
                  "chevrolet corvette c6 zr1"
               ],
               "open_calais_topics": [],
               "override_topic": null,
               "published_at": "2014-11-25T23:22:01+00:00",
               "rss_feed_url": "http://page.com/source",
               "suitability_score": 0,
               "summary": "Useful summary here",
               "tags": [
                  "Chevrolet Corvette",
                  "Muscle car",
                  "Porsche 911",
                  "Midlife crisis",
                  "Italy"
               ],
               "title": "Chevrolet Readies the 2015 Corvette Z06",
               "topics": null,
               "oid": "5474bsebc3s5s015es00ss5d",
               "topic": null
            },
            "sort": [
               1416957721000
            ]
         }
      ]
   }
}
```

In previous versions of Elasticsearch, a bool query with an empty should array would return zero hits whereas in the latest version, an empty should array behaves as a match_all. Is this the intended behavior?
</description><key id="50217482">8676</key><summary>Change of behavior with bool querry (empty `should` clause)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ddre54</reporter><labels /><created>2014-11-26T19:22:49Z</created><updated>2015-12-17T21:23:46Z</updated><resolved>2014-11-27T09:22:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-27T09:22:19Z" id="64764611">Hiya

Yes, this was a change made in 1.3.3. See https://github.com/elasticsearch/elasticsearch/issues/7240
</comment><comment author="navneet-lyra" created="2015-12-17T21:13:12Z" id="165583680">Shouldn't we be able to toggle out of this behavior by using `minimum_should_match`?

I've noticed that if the `minimum_should_match` value &gt; `number of should clauses`, then minimum_should_match is ignored. This seems like an unintended pitfall for systems where queries are programmatically generated (from query-understanding or otherwise).
</comment><comment author="ddre54" created="2015-12-17T21:23:46Z" id="165586104">@clintongormley Thanks for clarifying this :+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>function_score: use query and filter together</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8675</link><project id="" key="" /><description>Before, if filter and query was defined for function_score, then the
filter was silently ignored. Now, if both is defined then function score
query wraps this in a filtered_query.

closes #8638
</description><key id="50207363">8675</key><summary>function_score: use query and filter together</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-26T17:38:45Z</created><updated>2015-03-19T14:38:58Z</updated><resolved>2015-01-19T12:42:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-26T21:52:46Z" id="64716037">left two comments
</comment><comment author="jpountz" created="2014-11-27T09:40:00Z" id="64766526">Should we raise an error instead of implicitely wrapping into a FilteredQuery?
</comment><comment author="clintongormley" created="2014-11-27T09:54:24Z" id="64768144">@jpountz I'm OK with wrapping this in a `filtered` query instead.  It does what the user expects. 
</comment><comment author="jpountz" created="2014-11-27T09:59:01Z" id="64768688">No objections, just wanted to make sure we considered this option too. :-)
</comment><comment author="brwe" created="2014-11-27T18:16:01Z" id="64819503">thanks for the quick review! Addressed all comments.
</comment><comment author="brwe" created="2014-11-27T18:20:18Z" id="64819799">closed  f00b431c184492b2ea8871522016a2a726a2bbbc
</comment><comment author="brwe" created="2014-11-27T18:20:32Z" id="64819815">oh. wrong window, sorry
</comment><comment author="jpountz" created="2014-12-03T23:52:05Z" id="65514294">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreTests.java</file></files><comments><comment>function_score: use query and filter together</comment></comments></commit></commits></item><item><title>Add an `unlike` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8674</link><project id="" key="" /><description>Adds a `unlike` parameter to the MLT Query, which simply tells the algorithm
to skip all the terms from the given documents. This could be useful in order
to better guide nearest neighbor search by telling the algorithm to never
explore the space spanned by the given `unlike` docs. In essence we are
interested about the characteristic of a given item, but not of the ones
provided by `unlike`, thereby forcing the algorithm to go deeper in its
selection of terms. Note that this is different than simply performing a must
not boolean query on the unliked items. The syntax is exactly the same as the
`like` parameter.
</description><key id="50206798">8674</key><summary>Add an `unlike` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>feature</label><label>v2.0.0-beta1</label></labels><created>2014-11-26T17:33:48Z</created><updated>2015-06-06T18:18:51Z</updated><resolved>2014-11-28T14:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-27T09:43:56Z" id="64766964">Something that confuses me a bit is that I would expect from an `unlike` parameter that it would score higher documents that are different from what is specified in the `unlike` parameter, while it's actually just an exclusion list for the selection of terms. Should this parameter be given a different name?
</comment><comment author="clintongormley" created="2014-11-27T09:57:41Z" id="64768533">@jpountz my original suggestion for the parameter name was `not_like`. @alexksikes preferred the softer form of `unlike` because the action is just to remove terms from the list, rather than to actively find documents that are completely unlike the example.  

@alexksikes what happens if the user provides `unlike` without `like`?  Does it throw an exception?  (I think it probably should)
</comment><comment author="jpountz" created="2014-11-27T10:14:15Z" id="64770507">I agree the naming is tricky. I like the internal naming "skipTerms" but it does not apply as well to the API since we can still have documents at this stage.
</comment><comment author="alexksikes" created="2014-11-27T12:41:06Z" id="64785607">@jpountz I agree that these are not strictly speaking negative examples, as the documents not having the characteristics provided are not necessarily scored any higher. I will change the documentation and commit message so there is no confusion. Instead by finding the documents which look `like` the ones provided but are `unlike` these other ones, we mean find the documents with the characteristics in the `like` set but not from the `unlike` set. 

As @clintongormley mentioned, the reason why choosing `unlike` instead of `not_like` is because `not_like` may lead the user to think that what happens under the hood is a negative boolean query. This would probably be too strict of a constraint, so instead we simply skip the terms from the given unliked items, forcing the algorithm to possibly go deeper in its selection of terms from the `like` set.

@clintongormley `like` is always required, so also in this case, an exception is thrown.
</comment><comment author="clintongormley" created="2014-11-27T12:48:46Z" id="64786325">How about `ignore` or `ignore_like`?
</comment><comment author="alexksikes" created="2014-11-27T13:17:19Z" id="64789166">Note that `like` is always used in conjunction with `unlike`. So when I write like `A` unlike `B`, I mean the documents similar to `A` (sharing the characteristics of `A`) but dissimilar to `B` (with no characteristics in common with `B`). I agree that `ignore` or `ignore_like` is more faithful to what occurs under the hood, but the desired effect here is more something like `unlike`. Using a skip list of terms is a loose way of implementing negative examples (here think of negative with respect to some other positive examples). It is true that we could score higher the docs that do not have `B`, but they are indirectly ranked higher because the docs that would compete with them do not match on `B`.
</comment><comment author="jpountz" created="2014-11-27T14:04:20Z" id="64793898">Then maybe we should have both `ignore_like` (this PR) and `unlike` (to implement in another PR) that would have the ability to boost documents negatively instead of just skipping terms?
</comment><comment author="alexksikes" created="2014-11-27T17:41:09Z" id="64816906">We are settling for `ignore_like`. Once we have negative examples with negative boosting and after some tests, we'll decide whether this is a keeper or not. 
</comment><comment author="jpountz" created="2014-11-27T17:44:19Z" id="64817150">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MoreLikeThisQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XMoreLikeThis.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/morelikethis/MoreLikeThisFetchService.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/mlt/MoreLikeThisActionTests.java</file></files><comments><comment>MLT Query: Support for ignore docs</comment></comments></commit></commits></item><item><title>Aggregations: Fail with a parse exception if `top_hits` agg is defined under a `nested` agg.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8673</link><project id="" key="" /><description>PR for #8668
</description><key id="50198660">8673</key><summary>Aggregations: Fail with a parse exception if `top_hits` agg is defined under a `nested` agg.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v1.3.7</label><label>v1.4.2</label></labels><created>2014-11-26T16:24:07Z</created><updated>2015-05-18T23:28:56Z</updated><resolved>2014-11-27T16:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-27T10:45:58Z" id="64774453">@jpountz Good point. I updated the PR to take into account the fact the a `reverse_nested` agg can join back to the root level and then in this case the `top_hits` does work as expected.
</comment><comment author="jpountz" created="2014-11-27T10:47:17Z" id="64774585">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[GEO] OGC compliant polygons fail with ambiguity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8672</link><project id="" key="" /><description>The GeoJSON specification (http://geojson.org/geojson-spec.html) does not mandate a specific order for polygon vertices thus leading to ambiguous polys around the dateline.  To alleviate ambiguity the OGC requires vertex ordering for exterior rings according to the right-hand rule (ccw) with interior rings in the reverse order (cw) (http://www.opengeospatial.org/standards/sfa).  While JTS expects all vertices in cw order (http://tsusiatsoftware.net/jts/jts-faq/jts-faq.html).  Spatial4j circumvents the issue by not allowing polys to exceed 180 degs in width, thus choosing the smaller of the two polys. Since GEO core includes logic that determines orientation at runtime, the following OGC compliant poly will fail:  

``` java
 {
  "geo_shape" : {
    "loc" : {
      "shape" : {
        "type" : "polygon",
        "coordinates" : [ [ 
          [ 176, 15 ], 
          [ -177, 10 ], 
          [ -177, -10 ], 
          [ 176, -15 ], 
          [ 172, 0 ],
          [ 176, 15] ], 
          [ [ -179, 5],
            [-179, -5],
            [176, -5],
            [176, 5],
            [-179, -5] ]
        ]
      }
    }
  }
}
```

One workaround is to manually transform coordinates in the supplied GeoJSON from -180:180 to a 0:360 coordinate system (e.g, -179 = 181).  This, of course, fails to comply with OGC specs and requires clients roll their own transform.

The other (preferred) solution - and the purpose of this issue - is to correct the orientation logic such that GEO core supports OGC compliant polygons without the 180 degree restriction/workaround.
</description><key id="50197818">8672</key><summary>[GEO] OGC compliant polygons fail with ambiguity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>discuss</label></labels><created>2014-11-26T16:17:06Z</created><updated>2014-12-16T18:53:56Z</updated><resolved>2014-12-16T18:53:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2014-11-26T22:50:58Z" id="64722233">Follow feature branch feature/WKT_poly_vertex_order for WIP
</comment><comment author="nknize" created="2014-12-16T18:53:56Z" id="67211251">Fixed in #8762
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/BaseLineStringBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/BasePolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/EnvelopeBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/MultiLineStringBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/MultiPointBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/MultiPolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PointBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PointCollection.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file><file>src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java</file></files><comments><comment>[GEO] Update GeoPolygonFilter to handle ambiguous polygons</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/BaseLineStringBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/BasePolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/EnvelopeBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/MultiLineStringBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/MultiPointBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/MultiPolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PointBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PointCollection.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file><file>src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java</file></files><comments><comment>[GEO] Update GeoPolygonFilter to handle ambiguous polygons</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file></files><comments><comment>[GEO] GIS envelope validation</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/BasePolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/PointCollection.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java</file></files><comments><comment>Computational geometry logic changes to support OGC standards</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/BasePolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file><file>src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java</file></files><comments><comment>[GEO] OGC compliant polygons fail with ambiguity</comment></comments></commit></commits></item><item><title>Highlighting does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8671</link><project id="" key="" /><description>I am using Elastic Search 1.3.5. 
I have fields like filename, foldername and content. In query I am trying to highlight matched content.

In Query if keyword only matches in content then keyword is highlighted and its becoming bold. However if keyword matches in (filename or foldername) and content then content is not highlighted.
What  I am missing here?

Query sample, 

'{"query":{"filtered":{"query":{"bool":{"should":[{"match":{"fileName":{"query":"document","boost":4}}},{"match":{"folderName":{"query":"document","boost":7}}},{"match":{"fileName.trigram":{"query":"document","boost":6,"minimum_should_match":"50%"}}},{"match":{"folderName.trigram":{"query":"document","boost":5,"minimum_should_match":"50%"}}}]}},"filter":{"and":[{"term":{"pid":1}},{"term":{"trash":false}},{"bool":{"should":[{"term":{"canonicalFolderPath":"/shared/documents"}},{"prefix":{"canonicalFolderPath":"/shared/documents/"}}]}}]}}},"highlight":{"fields":{"_all":{"fragment_size":150,"number_of_fragments":0,"no_match_size":150}},"pre_tags":["\u003cBold\u003e"],"post_tags":["\u003c/Bold\u003e"]}}'
</description><key id="50192803">8671</key><summary>Highlighting does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wellwininfotech</reporter><labels /><created>2014-11-26T15:36:44Z</created><updated>2014-11-26T16:05:18Z</updated><resolved>2014-11-26T16:05:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-26T15:54:20Z" id="64666136">I imagine this is normally a question for the mailing list.  Also next time please pretty format the json and include a sample response.  You might also want to include sample documents and the relevant parts of your mapping.

From what I can tell you are highlighting against _all which is normally not advised:  http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html

I'm not sure beyond that what might be wrong though.  Please taking it to the mailing list and post a mapping and sample documents.  Best case is you post a full curl recreation (basically a bash script that shows the problem from an empty index).
</comment><comment author="clintongormley" created="2014-11-26T16:05:18Z" id="64668010">You can find the mailing list here: http://elasticsearch.org/community

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] explain default settings for parameters of decay functions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8670</link><project id="" key="" /><description>relates to #8624
</description><key id="50189108">8670</key><summary>[docs] explain default settings for parameters of decay functions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-11-26T15:05:35Z</created><updated>2014-11-27T18:21:06Z</updated><resolved>2014-11-27T18:21:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-26T16:03:26Z" id="64667706">LGTM
</comment><comment author="brwe" created="2014-11-27T18:21:06Z" id="64819858">closed f00b431c184492b2ea8871522016a2a726a2bbbc
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] failed to delete a dummy doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8669</link><project id="" key="" /><description>We have a couple of occurrences where `ElasticsearchIntergrationTest` fails to delete a dummy document when `indexRandom` is used in tests.

See:
- http://build-us-00.elasticsearch.org/job/es_core_master_metal/5614/testReport/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/
- http://build-us-00.elasticsearch.org/job/es_core_master_metal/5618/testReport/junit/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/
- http://build-us-00.elasticsearch.org/job/es_core_master_debian/2529/
- http://build-us-00.elasticsearch.org/job/es_core_master_metal/5639/
</description><key id="50189055">8669</key><summary>[CI Failure] failed to delete a dummy doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>jenkins</label></labels><created>2014-11-26T15:05:11Z</created><updated>2015-11-21T21:42:25Z</updated><resolved>2015-11-21T21:42:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2014-11-27T10:59:41Z" id="64775962">There were a couple more failures this morning:

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_metal/5639/testReport/junit/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_metal/5635/testReport/junit/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_metal/5633/testReport/junit/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_debian/2538/testReport/junit/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/

http://build-us-00.elasticsearch.org/view/ES%20Master/job/es_core_master_debian/2535/testReport/junit/org.elasticsearch.recovery/RelocationTests/testRelocationWhileRefreshing/
</comment><comment author="bleskes" created="2014-11-28T09:31:41Z" id="64873149">I enabled some trace logs. let's see..
</comment><comment author="clintongormley" created="2015-11-21T21:42:25Z" id="158684235">Closing in favour of #13719
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException on top_hits subaggregation of nested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8668</link><project id="" key="" /><description>From the log file

```
[2014-11-26 15:49:42,502][INFO ][cluster.metadata         ] [Ch'od] [bug] creating index, cause [api], shards [5]/[1], mappings [ari]
[2014-11-26 15:51:12,270][DEBUG][action.search.type       ] [Ch'od] [bug][1], node[r8Z50Ip0TTGcDcu8XDi4ow], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@24f7ac80] lastShard [true]
java.lang.NullPointerException
[2014-11-26 15:51:12,369][DEBUG][action.search.type       ] [Ch'od] [bug][4], node[r8Z50Ip0TTGcDcu8XDi4ow], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@24f7ac80]
java.lang.NullPointerException
[2014-11-26 15:51:12,270][DEBUG][action.search.type       ] [Ch'od] [bug][2], node[r8Z50Ip0TTGcDcu8XDi4ow], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@24f7ac80] lastShard [true]
java.lang.NullPointerException
[2014-11-26 15:51:12,270][DEBUG][action.search.type       ] [Ch'od] [bug][0], node[r8Z50Ip0TTGcDcu8XDi4ow], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@24f7ac80] lastShard [true]
java.lang.NullPointerException
```

The query

``` json
POST /bug/ari/_search
{
   "aggs": {
      "mhcs": {
         "aggs": {
            "prices": {
               "nested": {
                  "path": "prices"
               },
               "aggs": {
                  "min_price": {
                     "min": {
                        "field": "total"
                     }
                  },
                  "aggs": {
                      "top_hits": {
                          "size": 1
                      }
                  }
               }
            },
            "aggs": {
               "top_hits": {
                  "_source": [
                     "mhc",
                     "checkin"
                  ],
                  "size": 1
               }
            }
         },
         "terms": {
            "field": "mhc"
         }
      }
   },
   "size": 0
}
```

The mapping

``` json
{
    "mappings": {
        "ari": {
            "_all": {
                "enabled": false
            },
            "dynamic": "false",
            "properties": {
                "adt": {
                    "type": "integer"
                },
                "chd": {
                    "type": "integer"
                },
                "checkin": {
                    "format": "date",
                    "type": "date"
                },
                "currency": {
                    "index": "not_analyzed",
                    "type": "string"
                },
                "ext_code": {
                    "index": "not_analyzed",
                    "type": "string"
                },
                "meal": {
                    "index": "not_analyzed",
                    "type": "string"
                },
                "mhc": {
                    "index": "not_analyzed",
                    "type": "string"
                },
                "prices": {
                    "type": "nested",
                    "properties": {
                        "source": {
                            "index": "not_analyzed",
                            "type": "string"
                        },
                        "stay": {
                            "type": "integer"
                        },
                        "total": {
                            "type": "double"
                        }
                    }
                },
                "room": {
                    "index": "not_analyzed",
                    "type": "string"
                },
                "tlc": {
                    "index": "not_analyzed",
                    "type": "string"
                }
            }
        }
    }
}
```

The data

``` json
{"index":{"_id":"AAH000022014112520DZF", "_type":"ari"}}
{ "tlc": "AAH", "mhc": "AAH00002", "room": "DZ", "ext_code": "AAH305", "currency": "EUR", "meal": "F", "checkin": "20141125", "adt": 2, "chd": 0, "prices":[{"stay": 1, "total": 42.38},{"stay": 2, "total": 84.77},{"stay": 3, "total": 127.16},{"stay": 4, "total": 169.54}]}
{"index":{"_id":"AAH000022014112620DZF", "_type":"ari"}}
{ "tlc": "AAH", "mhc": "AAH00002", "room": "DZ", "ext_code": "AAH305", "currency": "EUR", "meal": "F", "checkin": "20141126", "adt": 2, "chd": 0, "prices":[{"stay": 1, "total": 42.38},{"stay": 2, "total": 84.77},{"stay": 3, "total": 127.16}]}
{"index":{"_id":"AAH000022014112720DZF", "_type":"ari"}}
{ "tlc": "AAH", "mhc": "AAH00002", "room": "DZ", "ext_code": "AAH305", "currency": "EUR", "meal": "F", "checkin": "20141127", "adt": 2, "chd": 0, "prices":[{"stay": 1, "total": 42.38},{"stay": 2, "total": 84.77}]}
{"index":{"_id":"AAH000022014112820DZF", "_type":"ari"}}
{ "tlc": "AAH", "mhc": "AAH00002", "room": "DZ", "ext_code": "AAH305", "currency": "EUR", "meal": "F", "checkin": "20141128", "adt": 2, "chd": 0, "prices":[{"stay": 1, "total": 42.38}]}
{"index":{"_id":"AAH000022014113020DZF", "_type":"ari"}}
{ "tlc": "AAH", "mhc": "AAH00002", "room": "DZ", "ext_code": "AAH305", "currency": "EUR", "meal": "F", "checkin": "20141130", "adt": 2, "chd": 0, "prices":[{"stay": 1, "total": 42.38}]}
```
</description><key id="50188032">8668</key><summary>NullPointerException on top_hits subaggregation of nested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">pzol</reporter><labels /><created>2014-11-26T14:56:28Z</created><updated>2014-11-27T16:52:57Z</updated><resolved>2014-11-27T16:52:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-26T15:23:49Z" id="64661100">Hey @pzol on what ES version did you run into this?

Also I see that you put a `top_hits` aggregation under a `nested` aggregation. This isn't supported yet, but will be from ES version 1.5: #7164
</comment><comment author="pzol" created="2014-11-26T15:29:43Z" id="64661995">Ah thanks, that's 1.4.0

The error is misleading if it's not supported.  I'll be waiting for 1.5 then :)
</comment><comment author="martijnvg" created="2014-11-26T15:32:57Z" id="64662547">I just verified that the `top_hits` agg under the `nested` agg is really causing the NPE. 

I agree that this error is misleading. I'll make sure that we fail at parse time instead of with a cryptic NPE for the next 1.4 release.
</comment><comment author="pzol" created="2014-11-26T16:05:07Z" id="64667974">Shall I close this issue or you want to keep it?
</comment><comment author="martijnvg" created="2014-11-26T16:25:22Z" id="64671416">Lets keep it around until #8673 gets merged in.
</comment><comment author="martijnvg" created="2014-11-27T16:52:57Z" id="64812529">Implemented via: #8673
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>QueryBuilders cleanup (add and deprecate)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8667</link><project id="" key="" /><description>Some QueryBuilders are missing or have a different naming than the other ones.

This patch is applied to branch 1.x and master (elasticsearch 1.5 and 2.0):
## Added
- `templateQuery(...)`
- `commonTermsQuery(...)`
- `queryStringQuery(...)`
- `simpleQueryStringQuery(...)`
## Deprecated
- `commonTerms(...)`
- `queryString(...)`
- `simpleQueryString(...)`

Closes #8721
</description><key id="50186493">8667</key><summary>QueryBuilders cleanup (add and deprecate)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>breaking</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-26T14:43:00Z</created><updated>2015-06-06T15:46:24Z</updated><resolved>2014-12-01T14:12:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-27T09:53:36Z" id="64768057">LGTM

Would it make sense to delay the removals of textPhrase, ... to 2.0? Hard removals in a minor version can be a bit frustrating from a user perspective even if they have been deprecated for a long time.
</comment><comment author="dadoonet" created="2014-11-27T11:33:51Z" id="64779378">I agree we can report the removal to 2.0.
If no objection with that, I'll update the PR accordingly.
</comment><comment author="dadoonet" created="2014-11-30T15:31:51Z" id="64988802">@jpountz PR updated accordingly. WDYT?
</comment><comment author="jpountz" created="2014-12-01T09:37:23Z" id="65039830">LGTM
</comment><comment author="dadoonet" created="2014-12-01T13:21:57Z" id="65063418">@clintongormley I added the `breaking` label as it's a breaking change in 2.0.0. Should I update `migrate_2_0.asciidoc` in master? Or should we have an equivalent file to `migrate_2_0.asciidoc` in java api doc? 
</comment><comment author="dadoonet" created="2014-12-01T13:39:06Z" id="65065288">@clintongormley I opened a new issue to split properly both operations: deprecate in 1.5 then remove in 2.0. See #8721. So I removed here the `breaking` label and added it to #8721.
</comment><comment author="clintongormley" created="2014-12-01T16:11:14Z" id="65087741">&gt; @clintongormley I added the breaking label as it's a breaking change in 2.0.0. Should I update migrate_2_0.asciidoc in master? Or should we have an equivalent file to migrate_2_0.asciidoc in java api doc?

You can just add a section about breaking changes in the Java API to the migration page in the main docs, I'd say.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file></files><comments><comment>java: QueryBuilders cleanup: remove deprecated</comment></comments></commit></commits></item><item><title>Add File.java to forbidden APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8666</link><project id="" key="" /><description>This commit cuts over all of core (not quite all tests) to java.nio.Path
It also adds the file class to the core forbidden APIs to prevent its usage
</description><key id="50168996">8666</key><summary>Add File.java to forbidden APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-26T13:29:26Z</created><updated>2015-06-07T11:54:14Z</updated><resolved>2014-12-02T20:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-27T14:37:14Z" id="64797340">@rmuir I addressed your comments and added the missing lines to the signature files. I also moved away from ZipFile. If you have time I'd love to have another review...
</comment><comment author="rjernst" created="2014-12-01T17:45:23Z" id="65104100">LGTM.
</comment><comment author="dakrone" created="2014-12-02T13:07:33Z" id="65227846">LGTM, left a lot of little comments.
</comment><comment author="s1monw" created="2014-12-02T13:32:32Z" id="65230552">thx @dakrone I pushed fixes
</comment><comment author="dakrone" created="2014-12-02T13:36:43Z" id="65231082">Fixes look good to me, still has my LGTM
</comment><comment author="rmuir" created="2014-12-02T14:50:51Z" id="65241433">looks good, i am fine with the change! The suggestions i listed are pre-existing conditions i think, we can do them as a followup.
</comment><comment author="dadoonet" created="2014-12-02T15:04:38Z" id="65243644">Left some comments. Most of them are not really related to this change but while you are at it :)
</comment><comment author="s1monw" created="2014-12-02T20:13:26Z" id="65296379">thanks guys I pushed fixes for the comments. I think it's ready... 
</comment><comment author="dadoonet" created="2014-12-02T20:28:52Z" id="65298651">LGTM. Do you plan to push this in 1.x?
</comment><comment author="s1monw" created="2014-12-02T20:34:55Z" id="65299548">no this won't go into 1.x
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>src/test/java/org/elasticsearch/plugins/PluginServiceTests.java</file><file>src/test/java/org/elasticsearch/plugins/loading/classpath/InClassPathPlugin.java</file></files><comments><comment>Plugins: Plugin failed to load since #8666</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/common/Base64.java</file><file>src/main/java/org/elasticsearch/common/Names.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobStore.java</file><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/common/io/Streams.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file><file>src/main/java/org/elasticsearch/http/HttpServer.java</file><file>src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/store/support/AbstractIndexStore.java</file><file>src/main/java/org/elasticsearch/indices/analysis/HunspellService.java</file><file>src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsService.java</file><file>src/main/java/org/elasticsearch/monitor/fs/JmxFsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/SigarFsProbe.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>src/main/java/org/elasticsearch/repositories/Repository.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/FileChangesListener.java</file><file>src/main/java/org/elasticsearch/watcher/FileWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>src/test/java/org/apache/lucene/util/AbstractRandomizedTest.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java</file><file>src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java</file><file>src/test/java/org/elasticsearch/common/io/FileSystemUtilsTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/store/SimpleDistributorTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>src/test/java/org/elasticsearch/stresstest/fullrestart/FullRestartStressTest.java</file><file>src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java</file><file>src/test/java/org/elasticsearch/watcher/FileWatcherTest.java</file></files><comments><comment>Add File.java to forbidden APIs</comment></comments></commit></commits></item><item><title>Add explainable script again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8665</link><project id="" key="" /><description>explainable scripts were removed in #7245 but they were used.
This commit adds them again.

closes #8561 

@ursvasan could you take a look and see if this is sufficient for your use case?
</description><key id="50156243">8665</key><summary>Add explainable script again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-26T11:42:27Z</created><updated>2015-06-07T17:07:44Z</updated><resolved>2015-01-19T13:09:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-26T13:46:49Z" id="64617987">LGTM
</comment><comment author="maxjakob" created="2015-01-09T14:56:31Z" id="69344178">+1 for bringing `ExplainableSearchScript` back.
</comment><comment author="brwe" created="2015-01-19T13:09:02Z" id="70489736">closed with 366ddfc89a9265e05d23291b02a829f4da1fcbdd
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>{1.4.0}: Initialization Failed ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8664</link><project id="" key="" /><description>I updated "elasticsearch" from 1.3 to 1.4 by using "yum", and I see this error message show up. What do you think the reason is?

[root@www3073gk elasticsearch]# ./bin/elasticsearch
Failed to configure logging...
org.elasticsearch.ElasticsearchException: Failed to load logging configuration
        at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:125)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:81)
        at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:94)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/config
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
        at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
        at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:97)
        at java.nio.file.Files.readAttributes(Files.java:1686)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:109)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
        at java.nio.file.Files.walkFileTree(Files.java:2602)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:115)
        ... 4 more
log4j:WARN No appenders could be found for logger (node).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{1.4.0}: Initialization Failed ...
1) NoSuchMethodError[org.elasticsearch.rest.BaseRestHandler.&lt;init&gt;(Lorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/client/Client;)V]
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoSuchMethodError: org.elasticsearch.rest.BaseRestHandler.&lt;init&gt;(Lorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/client/Client;)V
  at org.codelibs.elasticsearch.reindex.rest.ReindexRestAction.&lt;init&gt;(Unknown Source)
  while locating org.codelibs.elasticsearch.reindex.rest.ReindexRestAction

1 error
        at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:197)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:203)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.lang.NoSuchMethodError: org.elasticsearch.rest.BaseRestHandler.&lt;init&gt;(Lorg/elasticsearch/common/settings/Settings;Lorg/elasticsearch/client/Client;)V
        at org.codelibs.elasticsearch.reindex.rest.ReindexRestAction.&lt;init&gt;(ReindexRestAction.java:30)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        ... 9 more

[root@www3073gk elasticsearch]# rpm -qa | grep elastic
elasticsearch-1.4.0-1.noarch
</description><key id="50125774">8664</key><summary>{1.4.0}: Initialization Failed ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">negabaro</reporter><labels /><created>2014-11-26T07:03:54Z</created><updated>2014-11-26T08:44:36Z</updated><resolved>2014-11-26T08:44:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-26T08:44:28Z" id="64531085">It looks like a bug in the reindexing plugin that you are using.  I'd suggest opening a ticket there instead.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error org.elasticsearch.transport.RemoteTransportException: r$type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8663</link><project id="" key="" /><description>---

| Error org.elasticsearch.transport.RemoteTransportException: r$type
| Error Caused by: org.elasticsearch.transport.ResponseHandlerFailureTransportException: r$type
| Error Caused by: java.lang.NoSuchFieldError: r$type

---

Why is this error occured when I hit the request. Every time I compile (grails) app, the error get fixed with expected response. But when some changes are made in code (eg, adding a comment or simply printing out something) and hit the request, the error occurs. 

Please help me with this problem. What is this error? and Why this error occured?
</description><key id="50124349">8663</key><summary>Error org.elasticsearch.transport.RemoteTransportException: r$type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BlackRabbitt</reporter><labels /><created>2014-11-26T06:40:50Z</created><updated>2014-11-26T08:48:25Z</updated><resolved>2014-11-26T08:40:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-26T08:40:22Z" id="64530673">@BlackRabbitt This looks like some error in your code (which you haven't provided), not in Elasticsearch.   Please ask questions like these in the mailing list (http://www.elasticsearch.org/community) and not on the GithHub issues list.

thanks
</comment><comment author="BlackRabbitt" created="2014-11-26T08:45:51Z" id="64531197">Ya just now I solved the issue. This was something related with access modifier to the inner class in my code (I solved it but don't know how. How does changing access modifier of inner class from private to protected solved the issue). 
And, 
What I am asking is not how to solve this. I am asking about the error. What is the meaning of that error. For example NullPointerException raised when you try to pass the null value. Like that what does RemoteTransportException stands for.
</comment><comment author="clintongormley" created="2014-11-26T08:46:44Z" id="64531290">Please ask these questions in the mailing list.
</comment><comment author="BlackRabbitt" created="2014-11-26T08:48:25Z" id="64531439">Okay As you wish. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Separately log file deletions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8662</link><project id="" key="" /><description>Today you can turn on lucene.iw's TRACE logging, but that gives you a metric ***load of output.

This change breaks out lucene.iw.ifd separately (TRACE), and also index.store.deletes.

The logging is unfortunately redundant, meaning if Lucene deletes a file, you'll see log output from both lucene.iw.ifd and in index.store.deletes; I did this so that if something else (not Lucene) deletes a file through the Store.directory(), we'd see it logged as well.

I found only 2 places in ES that seem to be deleting files associated with a Lucene index, and I added logging there as well.

Unfortunately, these produce a lot of output ... so I did not turn them on by default.

Closes #8603 
</description><key id="50092936">8662</key><summary>Separately log file deletions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T22:27:08Z</created><updated>2015-06-07T10:48:49Z</updated><resolved>2014-11-26T10:10:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-26T09:51:57Z" id="64539097">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/LoggerInfoStream.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file></files><comments><comment>Core: separately log file deletions</comment></comments></commit></commits></item><item><title>Very large longs appear to be truncated in _source </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8661</link><project id="" key="" /><description>To reproduce (I was using Postman from inside Chrome, but it doesn't seem to matter how you do this)

1) Use the following mapping:

{
    "event" : {
         _id: {path: "id"},
        "properties" : {"message" : {"type" : "string"},"id" : {"type" : "long"}}
    }
}

2) Then add the following value to the index:

{
"id": 4446965025534723766, 
"message": "4446965025534723766"
}
- Query the index/mapping with an empty query ({})

Expected: _source is exactly as was supplied 

Actual:

```
        {
            "_index": "test",
            "_type": "event",
            "_id": "4446965025534723766",
            "_score": 1,
            "_source": {
                "id": 4446965025534723600,
                "message": "4446965025534723766"
            }
        }
```

Note that "id" is 4446965025534723600, not 4446965025534723766

Only the value returned in _source appears to be the problem - the entry can be correctly found by id=4446965025534723766, and the document id that is based on this field is also correct.

This seems to be consistent for very large longs - 15+ decimal digit (lower values appear to be handled correctly)

I can reproduce it in 1.2.0 and 1.3.2 - haven't tried later versions
</description><key id="50079514">8661</key><summary>Very large longs appear to be truncated in _source </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oleglvovitch</reporter><labels /><created>2014-11-25T20:28:30Z</created><updated>2014-11-26T08:27:47Z</updated><resolved>2014-11-26T08:27:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-26T08:27:47Z" id="64529451">Hi @oleglvovitch 

There is no support yet for BigIntegers.  If you want to store long IDs, you should store them as strings instead.  Closing in favour of #5683
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Alerts with datemath are stuck with static values calculated at registration time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8660</link><project id="" key="" /><description>This is documented behavior so not a bug but an enhancement request. Currently the datemath in alerts is converted to static values at registration time. Hence these date-ranges don't work with alerts on datasets where date is a changing variable. 

For e.g. functionality where user can get alerted on records (like scheduled events) matching his criteria in the next month in a rolling pattern without having to either re-ope index/re-start ES or re-register alert queries. 

Thank you!
</description><key id="50071057">8660</key><summary>Alerts with datemath are stuck with static values calculated at registration time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajhalani</reporter><labels /><created>2014-11-25T19:15:09Z</created><updated>2014-11-25T19:21:14Z</updated><resolved>2014-11-25T19:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T19:21:14Z" id="64455302">@ajhalani you're in luck - see #8534 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DiskThresholdDecider#remain(...) should take shards relocating away into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8659</link><project id="" key="" /><description>Let the disk threshold decider take into account shards moving away from a node in order to determine if a shard can remain.

By taking this into account we can prevent that we move too many shards away than is necessary.

PR for #8538
</description><key id="50055815">8659</key><summary>DiskThresholdDecider#remain(...) should take shards relocating away into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T17:07:17Z</created><updated>2015-06-06T19:04:28Z</updated><resolved>2014-11-26T09:08:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-25T20:19:49Z" id="64464246">@martijnvg left a couple of comments
</comment><comment author="dakrone" created="2014-11-25T20:21:22Z" id="64464478">Also, 1.3.x doesn't include the take-relocations-into-account feature, so this may be pretty tough to backport without backporting the original feature, not sure if this is something we should stick in just 1.4.x and up.
</comment><comment author="martijnvg" created="2014-11-25T21:54:31Z" id="64477780">@dakrone Good point, didn't realise that. I removed the 1.3.6 version label. I'll update the PR.
</comment><comment author="s1monw" created="2014-11-25T21:59:27Z" id="64478609">left one comment - thanks mvg for getting this in!!
</comment><comment author="martijnvg" created="2014-11-25T22:22:35Z" id="64481973">@dakrone @s1monw Thanks for the feedback, I've updated the PR.
</comment><comment author="dakrone" created="2014-11-26T08:36:37Z" id="64530307">LGTM, left one comment about a misspelling
</comment><comment author="martijnvg" created="2014-11-26T08:53:15Z" id="64531902">oops, I'll change that.

On 26 November 2014 at 09:36, Lee Hinman notifications@github.com wrote:

&gt; LGTM, left one comment about a misspelling
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/8659#issuecomment-64530307
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file></files><comments><comment>Core: Let the disk threshold decider take into account shards moving away from a node in order to determine if a shard can remain.</comment></comments></commit></commits></item><item><title>Instead of top_terms_N, no rewrite method exists to get first_terms_N</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8658</link><project id="" key="" /><description>There are multiple rewrite methods present. 
Scoring/Constant boolean will hit maxClauseLimit if terms expanded are more than 1024 (default). The only possible expansion (which i can use to extract terms) is top_terms_N. But this iterates through every term to find the "top" terms. There should be an alternative to get the first N expanded terms (irrespective of how good the match is). This way the query execution time improves when the number of possible terms are way too huge (in my case).
</description><key id="50055182">8658</key><summary>Instead of top_terms_N, no rewrite method exists to get first_terms_N</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makam</reporter><labels><label>discuss</label></labels><created>2014-11-25T17:03:11Z</created><updated>2016-11-26T12:57:14Z</updated><resolved>2016-11-26T12:57:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T18:41:38Z" id="64449246">Hi @makam

Could you provide more details about your use case and where you'd like to use this rewrite method? Most queries which use rewrite already have a `max_expansions` parameter which do exactly what you describe.
</comment><comment author="makam" created="2014-11-27T17:56:15Z" id="64818036">So basically my use case:
I have written my custom plugins for highlighter. In the highlighter, i end up doing a rewrite so that i can extract the terms and highlight it my way. I currently use top_terms_boost_100. But this is quite slow as  explained in the original post. So i'd like to use a rewrite method which directly gives me the first 100 terms that got matched.

And you just mentioned that most queries have max_expansions. Its just the fuzzy and match_phrase which have max_expansions right? And each of them have a custom implementation of rewrite (neither of them use the standard rewrite method i.e MultiTermQuery.Rewrite). Am i missing some point here? Is there an inbuilt solution that i can leverage in my plugin?
</comment><comment author="clintongormley" created="2014-11-28T13:07:55Z" id="64892115">Hi @makam 

Sorry if I'm being a bit slow here, but I'm struggling to follow exactly what you're doing.  Are you talking about expanding fuzzy/wildcard/prefix/regex terms in the query for highlighting purposes? 

You say "gives me the first 100 terms that got matched". What do you mean by "matched" here? Matched in the term dictionary or matched by the query?
</comment><comment author="makam" created="2014-11-28T17:31:06Z" id="64915895">Thanks for being patient... I'l re-explain the use case.
I have a custom highlighter which returns the offset details of the matched hit instead of returning snippets for every hit in a document. To get the offset data for a wildcard query, i first rewrite the query using top_terms_100 to extract the terms out of it, and then find the occurrences of these 100 words in the documents being returned. 
Now i want better performance. So i would prefer using some other rewrite method which again gives me only 100 words (possible expansions of wildcard term), but slightly faster. To be precise, return the first 100 terms that match the wildcard automaton instead of evaluating every word. 

Regarding your two questions,
Yes, i am expanding wildcard/prefix terms in the query for highlighting purpose.
And I meant 100 terms in the term dictionary.
</comment><comment author="clintongormley" created="2014-11-28T17:43:46Z" id="64916780">@makam ok - i understand perfectly now :) thanks

I'll put this issue up for discussion, but if you'd be interested in sending a PR it may speed things along.
</comment><comment author="clintongormley" created="2016-11-26T12:57:14Z" id="263062126">Two years have passed since this issue was opened and it hasn't gained traction.  I'm hoping you figured out a way to do this yourself.  If not, you may want to look at the new unified highlighter (#21621) which seems to handle wildcards etc efficiently.  Perhaps there are clues to be gained there about how to implement this.

If you still feel that this rewrite method should be provided, feel free to reopen.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adds documentation for indices.exists_template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8657</link><project id="" key="" /><description>Adds a small section about how to check if a template exists to the `indices-templates` page.

The documentation was built and checked with the [docs](https://github.com/elasticsearch/docs) project.
</description><key id="50048124">8657</key><summary>Adds documentation for indices.exists_template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justahero</reporter><labels /><created>2014-11-25T16:14:31Z</created><updated>2014-12-03T13:11:19Z</updated><resolved>2014-11-26T10:07:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T18:37:52Z" id="64448659">thanks @justahero - merged.

I changed the `[exists]` to something which will be unique across all the reference docs, otherwise it would break the docs build.
</comment><comment author="justahero" created="2014-11-26T10:07:25Z" id="64541006">@clintongormley thx!

Changes are applied, closing this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Adds documentation for indices.exists_template</comment></comments></commit></commits></item><item><title>Fixes documentation URLs in spec files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8656</link><project id="" key="" /><description>This PR fixes a couple of documentation URLs in the JSON spec files that point either to a `404` page or the wrong documentation page.
</description><key id="50047916">8656</key><summary>Fixes documentation URLs in spec files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justahero</reporter><labels /><created>2014-11-25T16:13:11Z</created><updated>2014-12-03T13:11:17Z</updated><resolved>2014-11-25T18:33:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T18:33:49Z" id="64448077">thanks @justahero - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Spec: Fixes URL links to documentation</comment></comments></commit></commits></item><item><title>Fix date_histogram issues during a timezone DST switch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8655</link><project id="" key="" /><description>The problem was in the difference between the two responses of the `preTz.getOffset` call. To solve this I only call it once.

The behaviour of `roundKey` changed that it is now returning the actual rounded timestamp rounded timestamp offsetted to UTC. The exact opposite is true for `valueForKey` which used to accept the UTC offsetted timestamp but now needs to work on the normal timestamp.

The tests for rounding are all passing, including the tests I added where the issue was shown.

Closes #8339

/cc @jpountz 
</description><key id="50043710">8655</key><summary>Fix date_histogram issues during a timezone DST switch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thanodnl</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T15:46:30Z</created><updated>2015-06-07T17:48:53Z</updated><resolved>2014-11-25T16:08:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-25T16:08:18Z" id="64423834">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor metric reducers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8654</link><project id="" key="" /><description>Try to remove duplicate code for metrics that create single value.
I only make this pull request so you know where this is going.
</description><key id="50042550">8654</key><summary>Refactor metric reducers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-11-25T15:38:25Z</created><updated>2015-02-20T11:47:10Z</updated><resolved>2015-02-20T11:47:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-11-27T11:39:20Z" id="64779876">I added basic tests for the implemented metrics now, so max, min etc work now. 
The "factories" I implemented for MetricOps and MetricResults are a little clumsy now but I think it might make sense to wait with fixing this until we settled on a final framework. 
Metrics now only work for aggregations which only contain one array of buckets. 
More then one dimension is not implemented, see https://github.com/elasticsearch/elasticsearch/pull/8654/files#diff-8bae1a7656e97f852987f6d42a92fed2R123. However, this should not be too hard to do now.
</comment><comment author="colings86" created="2014-11-28T15:08:02Z" id="64902881">@britta I like this change. I left some comments but I don't think any of them are particularly problematic. 
Two things I want to check (I think I have them right but want to make sure:
1. Metric reducers are neither required to output a single value or are restricted to outputting numeric values. I want to make sure this is true as in the aggregation we now have geo_bounds, scripted_metric and top_hits, none of which output a numeric value (i.e. don't implement NumericMetricAggregation.*)
2. To implement a new metric reducer I need to subclass the following:
- MetricOp
- MetricResult
- is there a class to implement for the Parser to validate the parameters I pass to my metric reducer?
</comment><comment author="jpountz" created="2014-11-29T00:06:27Z" id="64935645">I like the sharing of code on the server-side, however I think it could be helpful to keep different APIs on the client-side (Min, MinBuilder, Avg, AvgBuilder, etc.) so that we can freely add or remove properties to these objects in the future?
</comment><comment author="brwe" created="2014-12-01T12:55:28Z" id="65060750">Thanks a lot for the review! I addressed all comments, although one I did not understand: https://github.com/elasticsearch/elasticsearch/pull/8654#discussion-diff-21034097 , answered inline.

@colings86 

&gt; 1. Metric reducers are neither required to output a single value or are restricted to outputting numeric values.

Yes, I totally missed that. I changed it now: InternalMetric is now just an InternalMetricsAggregation. As a proof of concept I added an aggregation which is not a numeric aggregation to see if it fits in the current design, see https://github.com/brwe/elasticsearch/commit/ec78541efe7e148ae8b1daa56af554aa88b910cc .  Let me know if this is generic enough!

&gt; 1. To implement a new metric reducer I need to subclass the following:

I added another example that has a custom builder custom parsing and custom output here: https://github.com/brwe/elasticsearch/commit/9401fd021f2a68e919ad6caa87fb39b299bce115
It is a very simple line fit to an array of values. I hope this shows what has to be done now to implement a new reducer.
The Factories and parser registration is still far from ideal, we should probably make this pluggable later.

@jpountz 

&gt;  I think it could be helpful to keep different APIs on the client-side (Min, MinBuilder, Avg, AvgBuilder, etc.) so that we can freely add or remove properties to these objects in the future?

I changed it now to have separate builders for the different metrics. However, I would still prefer having the InternalMetric separated from the actual functionality. Currently, the InternalMetric has a MetricResult which holds all the results. Is this what you meant?
</comment><comment author="colings86" created="2014-12-02T09:07:53Z" id="65200978">@brwe only thing I am wondering is whether we should have something to validate the parameters passed into the metric so we error if an invalid parameter is passed in rather than just dropping it and continuing?
</comment><comment author="jpountz" created="2014-12-02T11:05:33Z" id="65214531">&gt; I changed it now to have separate builders for the different metrics. However, I would still prefer having the InternalMetric separated from the actual functionality. Currently, the InternalMetric has a MetricResult which holds all the results. Is this what you meant?

I would rather have a different class or interface for every type of reducer, so that we can make API changes to one reducer without impacting all the other reducers. However, I have no objection to make all APIs implemented by a single class for now since they all give back the same results. For instance we could have an Avg and a Sum interface that would both be implemented by the same internal class.

Something else that is important I think, is to avoid using internal classes (eg. InternalMetric, these classes prefixed with "Internal" are supposed to only be used internally, not by the clients) in integration tests in order to make sure that the supported API that we propose exposes all the information that is needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] split base settings in ClusterDiscoveryConfiguration between node and transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8653</link><project id="" key="" /><description>The default settings that are currently applied to the transport client are about discovery and gateway, modules that are not even loaded on the transport client. We can now remove the local gateway as it's the default one anyway. Also, let's make sure that the discovery setting is only applied to the node, as it is not relevant for transport client.
</description><key id="50042139">8653</key><summary>[TEST] split base settings in ClusterDiscoveryConfiguration between node and transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T15:35:32Z</created><updated>2014-11-27T07:16:58Z</updated><resolved>2014-11-27T07:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-26T15:08:56Z" id="64658810">LGTM. Left two minor comments.
</comment><comment author="javanna" created="2014-11-26T16:20:35Z" id="64670596">Updated @bleskes thanks for the review
</comment><comment author="bleskes" created="2014-11-26T16:21:20Z" id="64670722">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java</file></files><comments><comment>[TEST] split base settings in ClusterDiscoveryConfiguration between node and transport client</comment></comments></commit></commits></item><item><title>starter script ignores errors when running Elasticsearch in background</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8652</link><project id="" key="" /><description>Hi!

`bin/elasticsearch` script does not handle errors from java app properly.
The example errors can be: inaccessible/missing log dir, data dir or conf file.

This causes init scripts to give false positives - eg. on CentOS6, `/etc/init.d/elasticsearch start` happily announces that daemon has started while it's not under above conditions like missing dirs or permissions.

Misbehaviour like this can have various funny consequences, like: invalidating logic inside Chef cookbooks and making services which depend on ElasticSearch go wild (depending on your setup/ Chef env).

The offending fragment is here:
https://github.com/elasticsearch/elasticsearch/blob/1664355f3/bin/elasticsearch#L153-L155

First, you can't do `exec` here as you did in the foreground case. I believe this `exec` could've been just copypasted from the foreground case few lines above :)

Second, there should be some check that the process is still running and behaving properly - for some values of "properly".
A most dumb trick would be something like `sleep 2; kill -0 $!` hoping that any early errors would make the background process exited.

A more elegant approach would be to handle daemonizing in an app only _after_ it validated it actually can run as a daemon, but that's a story Java does not handle well I guess.
</description><key id="50039296">8652</key><summary>starter script ignores errors when running Elasticsearch in background</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wkhere</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2014-11-25T15:16:48Z</created><updated>2015-11-21T21:41:12Z</updated><resolved>2015-11-21T21:41:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="t-lo" created="2014-12-01T15:13:10Z" id="65078117">@herenowcoder I disagree - using `exec` there works since ES is backgrounded (`&amp;` at the end of that line). The backgrounded shell then `exec`s.

However, other things can be improved with the daemonization code: elasticsearch, even when daemonized, is the child of the current shell. Being a daemon it should rather start its own session (e.g. by using `setsid`). Also, stdin is closed in the script but stdout and stderr remain open (rather unusual for daemons), but I suppose ES requires those for logging purposes?

A cheap shot on the daemonization / initialization problem would be to use the PID file to signal successful start-up. The start script then would wait a pre-defined time slot (are 2s reasonable?) for the PID file to appear after ES was background-started and return an error if no file appears. In cases where no PID file was requested by the user the script could simply use `mktemp` to create a temporary file.

Apart from the obvious changes in `bin/elasticsearch` this would require to move the PID file creation code in `src/main/java/org/elasticsearch/bootstrap/Bootstrap.java` from the beginning (line ~150) to the end (line ~250) of the `main` method.
</comment><comment author="wkhere" created="2014-12-05T15:46:45Z" id="65807697">@t-lo well there are two usages of `exec` there.
First, I agree in the foreground case `exec` is valid.

However in the case of going background, I disagree that `exec` is valid. The init.d script relies on the return value from line 155 which is never called.

In this case if the `exec`uted Elasticsearch process fails, like: there are no directories for logs created for example, then init.d script falsely shows that the service was successfully started, while it wasn't.

That's the real world case I'm describing.
</comment><comment author="t-lo" created="2014-12-05T16:14:16Z" id="65811970">@herenowcoder  But yes, the return is called :) As I wrote, since the whole line is backgrounded (`&amp;` at the end of the line) `exec` will run in the backgrounded sub-shell.

More precisely, there is a notable difference between

``` (sh)
exec elasticsearch
```

and

``` (sh)
exec elasticsearch &amp;
```

The former will overwrite the current process (as used in the foreground case). The latter will spawn a background process (because of the `&amp;`) which will then be overwritten by the `exec`. The command will return immediately.

By the way, I opened a pull request which fixes the "ignore error if backgrounded" issue: https://github.com/elasticsearch/elasticsearch/pull/8742. Take a look at the changes if you like - I tested those, they work as expected and will fix your issue. Note that the exec is still there (and it works :) ).
</comment><comment author="bdharrington7" created="2015-10-21T17:15:53Z" id="149966514">Hi, what happened to this? is this still going to be merged? I'm running 1.7.1 on CentOS and still having this issue where elasticsearch will fail silently, and no logs pick anything up. 
</comment><comment author="clintongormley" created="2015-11-21T21:41:12Z" id="158684166">Closing in favour of https://github.com/elastic/elasticsearch/pull/13643
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scripts only executed when corresponding file stored locally on master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8651</link><project id="" key="" /><description>We are using lang-python scripting module(1), and we would rather not enabled dynamic scripting.

We've noted that scripts could only be executed when the corresponding file is stored locally in the /scripts (2) directory of our master node, and calling a script which is stored in a non-master node only would lead to an error:

```
(0, [{u'update': {u'status': 400, u'_type': u'user', u'_id': u'1', u'error': u'ElasticsearchIllegalArgumentException[failed to execute script]; nested: ScriptException[dynamic scripting for [python] disabled]; ', u'_index': u'index_name'}}])
```

If we turn off the masters, then the remaining nodes which become masters are able to execute the locally stored scripts, the ones which could not be executed before, and the scripts which were on the previous masters (now regular nodes) cannot be executed anymore.

We should be able to do what we want by updating the script files on all nodes, thus on the masters ones, but is this an expected behaviour? 

Is there any kind of synchronisation process to ensure all nodes of the same cluster always have the same scripts? Or do we only we need to write them on all nodes just in case one of these nodes became a master?

(1) lang-python is installed on all nodes of our cluster
(2) either /etc/elasticsearch/scripts if runnig as a service or {tardir}/config/scripts if running from tar
</description><key id="50031797">8651</key><summary>Scripts only executed when corresponding file stored locally on master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Thibaut-Fatus</reporter><labels /><created>2014-11-25T14:17:04Z</created><updated>2014-11-26T13:03:31Z</updated><resolved>2014-11-25T18:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T18:04:47Z" id="64443765">Hi @Thibaut-Fatus 

Scripts are always executed locally, so you need to have the same scripts on all nodes. The master nodes are only involved if they are executing a script - there is nothing special about their role in scripting. You could e.g. use a shared file system to ensure that all nodes have the same scripts.
</comment><comment author="Thibaut-Fatus" created="2014-11-26T10:40:07Z" id="64544831">Thank you for the insight,

However I'm very surprised.. Lets say our configuration was:
-- node0 with script0
-- node1 with script1

where script{i} are only stored locally on node{i}

We were testing this on our development hardware, each of us connected to his own node running on our localhost, on the same cluster. (2 different machines, we were _not_ using a connection pool when firing requests)

Our understanding was that scripts are executed locally, and thus need to be locally stored on each nodes _which you confirm_. 

What we've seen is that:
**if node{i} is master, both users could execute script{i}, none of them could exectute script{j}.**

######
A bit more details:
-- node0 (master) can only execute script0 (this is expected)
-- **node1 (slave) can only execute script0 which is not stored locally (this is not expected ?)**
-- none of them can exectute script1

And shutting down node0 to have node1 as a master leads to the opposite situation where:
-- **node0 (slave) can only execute script1 (this is surprising)**
-- node1 (master) can only execute script1 (this is expected)
</comment><comment author="clintongormley" created="2014-11-26T11:02:25Z" id="64547420">@Thibaut-Fatus I think where you are getting confused is this: the node that you send the request to may or may not be the node that executes the request.

The node receiving the request acts as the "coordinating node", eg for a search request, it will forward a per-shard search request to a copy of every shard in the index (which may or may not be on the same node).  Each shard will execute the request (including scripts) then return the results to the coordinating node, which then reduces the shard-level results to the final results returned to the user.

Scripts in files are never transferred between nodes by Elasticsearch, and master nodes have nothing at all to do with scripting.
</comment><comment author="clintongormley" created="2014-11-26T11:03:43Z" id="64547569">To add to the above: if you send the same search request to the same node repeatedly, the coordinating node will round-robin through shard copies, eg the first request might execute on node 1, and the second request on node 2 etc
</comment><comment author="Thibaut-Fatus" created="2014-11-26T11:33:00Z" id="64573838">@clintongormley I'm aware that the coordinating node could be any node in the cluster, and more than that that it can change from one request to the next one. And even if the fact that script files were not transferred between nodes was a question (we thought it might exist some synchro process between nodes), we also were aware that this wouldn't be done during a request.

I'm still not fully confident that this explains our observation..

There is a point I can't understand regarding my comprehension of how ES works:

(FYI we have 2 nodes, replication factor 2, 1 master, and the scripts update only one document, no params passed, it just tries to add a new entry to the json object)

Our replication factor is 1, we had 2 nodes, green, thus the data must be stored on each of them. One explaination I see, and I hope there is another, is that the coordinator always selects the master to execute the request, thus requests with scripts stored on master work. **But I'm very concerned with this since it means that a coordinator could possibly chose always the same node to execute a request even if replication is &gt; 0 ?**

What scares me is that this explaination is compliant with the fact that we _always_ see an error when the request asks to execute a script stored only on slave node. 

It might be that the coordinator asks the 2 nodes, master failed when the script does not exist locally, thus the coordinator refuses the response from the slave node even if it was correct. This would be a satisfying explaination.
</comment><comment author="clintongormley" created="2014-11-26T13:00:05Z" id="64600688">@Thibaut-Fatus Ah you're talking about scripted updated.  OK, these are a bit different.  The script is always executed on the primary shard, then the new document is replicated in its entirety to the replica shard (so the replica doesn't execute the script).

In fact, there is an issue open (https://github.com/elasticsearch/elasticsearch/issues/8369) to change this behaviour to work better in a distributed environment.
</comment><comment author="Thibaut-Fatus" created="2014-11-26T13:03:31Z" id="64601066">@clintongormley indeed this sounds like what we've seen ! Thanks a lot, hope this would get better with time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping potentially lost with `"dynamic" : "strict"`, `_default_` mapping and failed document index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8650</link><project id="" key="" /><description>The effect is very similar to https://github.com/elasticsearch/elasticsearch/pull/5623
When a document is indexed that does have a dynamic field then the indexing fails as expected. 
However, the type is created locally in the mapper service of the node but never updated on master, see https://github.com/brwe/elasticsearch/commit/340f5c5de207a802085f23aeb984dbd98349301a#diff-defbaaff93b959a2f9a93e7167f6f345R165

This can cause several problems:
1. `_default_` mappings are applied locally and can potentially later not be updated anymore, see https://github.com/brwe/elasticsearch/commit/340f5c5de207a802085f23aeb984dbd98349301a#diff-ed65252ffbbf8656bf257a8cd6251420R68 (thanks @pkoenig10 for the test, https://github.com/elasticsearch/elasticsearch/issues/8423#issuecomment-64395503)
2. Mappings that were created via `_default_` mappings when indexing a document can be lost, see https://github.com/brwe/elasticsearch/commit/340f5c5de207a802085f23aeb984dbd98349301a#diff-defbaaff93b959a2f9a93e7167f6f345R187
</description><key id="50028608">8650</key><summary>Mapping potentially lost with `"dynamic" : "strict"`, `_default_` mapping and failed document index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label></labels><created>2014-11-25T13:51:45Z</created><updated>2015-05-29T17:09:00Z</updated><resolved>2015-05-22T12:27:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="miccon" created="2014-11-27T09:28:49Z" id="64765359">Will this also be fixed on the 1.4 branch?
</comment><comment author="brwe" created="2014-11-27T17:30:39Z" id="64816010">Yes. 

To fix this, there is two options:
1. make sure the type is not created if indexing fails
2. update the mapping on master even if indexing of doc failed

Option 1 is rather tricky to implement and I do not see why the type should not be created in the mapping so I'll make a pr for option 2 shortly.
</comment><comment author="miccon" created="2014-11-28T08:15:37Z" id="64866445">I just checked 0992e7f, but https://gist.github.com/miccon/a4869fe04f9010015861 still fails.

I suppose that when the mapping is created after the failed indexing request, the _all mapping is set to true (by default) and then the _all cannot by set to false anymore.

IMHO not creating the type at all would be the cleaner solution, because even when the default mapping is set to strict creating the type can prevent you from updating the mapping later on (as the _all mapping is created automatically).
</comment><comment author="brwe" created="2015-02-24T16:57:49Z" id="75796189">I pushed the fix for the lost mappings but as @rjernst pointed out, not updating the mapping can only be done once https://github.com/elasticsearch/elasticsearch/issues/9365 is done so I'll leave this issue open.
</comment><comment author="brwe" created="2015-05-22T12:27:16Z" id="104648151">fixed by https://github.com/elastic/elasticsearch/pull/10634
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/WriteFailureException.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingTests.java</file></files><comments><comment>mappings: update cluster state with type mapping also for failed indexing request</comment></comments></commit></commits></item><item><title>Fixed p/c filters not being able to be used in alias filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8649</link><project id="" key="" /><description>PR for #8628

The bug was introduced by #5916
</description><key id="50026831">8649</key><summary>Fixed p/c filters not being able to be used in alias filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T13:38:28Z</created><updated>2015-06-08T00:14:26Z</updated><resolved>2014-11-25T13:46:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-25T13:40:30Z" id="64400471">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw IndexShardClosedException if shard is closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8648</link><project id="" key="" /><description>Today we throw a generic ElasticsearchException when a recovery is cancled. This
causes verbose logging and send shard failures and additional unnecessary cluster state
events. We can just throw IndexShardClosedException which prevents the send shard failures.

we see lots of these exception in the logs which are misleading - this was introduced lately and never released:

```
1&amp;gt; [2014-11-25 11:07:40,742][DEBUG][index.service            ] [node_0] [test2] [3] closed (reason: [recovery failure [RecoveryFailedException[[test2][3]: Recovery failed from [node_1][9pORZ5dyT6C0Y8oZxZ9q-w][ip-10-255-15-175][local[3]]{mode=local} into [node_0][_ZbAkLDCSCa1GB_dEv7nUQ][ip-10-255-15-175][local[2]]{mode=local}]; nested: RemoteTransportException[[node_0][local[2]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[test2][3] Phase[3] Execution failed]; nested: ElasticsearchException[recovery was canceled reason [shard is closed]]; ]])
  1&amp;gt; [2014-11-25 11:07:40,742][WARN ][indices.cluster          ] [node_0] [test2][3] sending failed shard after recovery failure
  1&amp;gt; org.elasticsearch.indices.recovery.RecoveryFailedException: [test2][3]: Recovery failed from [node_1][9pORZ5dyT6C0Y8oZxZ9q-w][ip-10-255-15-175][local[3]]{mode=local} into [node_0][_ZbAkLDCSCa1GB_dEv7nUQ][ip-10-255-15-175][local[2]]{mode=local}
  1&amp;gt;         at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:245)
  1&amp;gt;         at org.elasticsearch.indices.recovery.RecoveryTarget.access$500(RecoveryTarget.java:64)
  1&amp;gt;         at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:485)
  1&amp;gt;         at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
  1&amp;gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&amp;gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&amp;gt;         at java.lang.Thread.run(Thread.java:744)
  1&amp;gt; Caused by: org.elasticsearch.transport.RemoteTransportException: [node_0][local[2]][internal:index/shard/recovery/start_recovery]
  1&amp;gt; Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [test2][3] Phase[3] Execution failed
  1&amp;gt;         at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1182)
  1&amp;gt;         at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:675)
  1&amp;gt;         at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:127)
  1&amp;gt;         at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:51)
  1&amp;gt;         at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:148)
  1&amp;gt;         at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:134)
  1&amp;gt;         at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:267)
  1&amp;gt;         at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
  1&amp;gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&amp;gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&amp;gt;         at java.lang.Thread.run(Thread.java:744)
  1&amp;gt; Caused by: org.elasticsearch.ElasticsearchException: recovery was canceled reason [shard is closed]
  1&amp;gt;         at org.elasticsearch.indices.recovery.ShardRecoveryHandler$CancelableThreads.failIfCanceled(ShardRecoveryHandler.java:640)
  1&amp;gt;         at org.elasticsearch.indices.recovery.ShardRecoveryHandler$CancelableThreads.remove(ShardRecoveryHandler.java:661)
  1&amp;gt;         at org.elasticsearch.indices.recovery.ShardRecoveryHandler$CancelableThreads.run(ShardRecoveryHandler.java:655)
  1&amp;gt;         at org.elasticsearch.indices.recovery.ShardRecoveryHandler.phase3(ShardRecoveryHandler.java:430)
  1&amp;gt;         at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1178)
  1&amp;gt;         ... 10 more
  1&amp;gt; [2014-11-25 11:07:40,743][WARN ][cluster.action.shard     ] [node_0] [test2][3] sending failed shard for [test2][3], node[_ZbAkLDCSCa1GB_dEv7nUQ], [R], s[INITIALIZING], indexUUID [sUuEvRUORCeBRQgKKtg4UQ], reason [Failed to start shard, message [RecoveryFailedException[[test2][3]: Recovery failed from [node_1][9pORZ5dyT6C0Y8oZxZ9q-w][ip-10-255-15-175][local[3]]{mode=local} into [node_0][_ZbAkLDCSCa1GB_dEv7nUQ][ip-10-255-15-175][local[2]]{mode=local}]; nested: RemoteTransportException[[node_0][local[2]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[test2][3] Phase[3] Execution failed]; nested: ElasticsearchException[recovery was canceled reason [shard is closed]]; ]]
  1&amp;gt; [2014-11-25 11:07:40,743][WARN ][cluster.action.shard     ] [node_0] [test2][3] received shard failed for [test2][3], node[_ZbAkLDCSCa1GB_dEv7nUQ], [R], s[INITIALIZING], indexUUID [sUuEvRUORCeBRQgKKtg4UQ], reason [Failed to start shard, message [RecoveryFailedException[[test2][3]: Recovery failed from [node_1][9pORZ5dyT6C0Y8oZxZ9q-w][ip-10-255-15-175][local[3]]{mode=local} into [node_0][_ZbAkLDCSCa1GB_dEv7nUQ][ip-10-255-15-175][local[2]]{mode=local}]; nested: RemoteTransportException[[node_0][local[2]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[test2][3] Phase[3] Execution failed]; nested: ElasticsearchException[recovery was canceled reason [shard is closed]]; ]]
```
</description><key id="50018011">8648</key><summary>Throw IndexShardClosedException if shard is closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T12:10:28Z</created><updated>2015-06-07T17:05:02Z</updated><resolved>2014-11-25T13:35:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-25T12:54:49Z" id="64395257">Left one comment otherwise LGTM
</comment><comment author="s1monw" created="2014-11-25T13:16:26Z" id="64397659">@dakrone replied to your comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search Template - sort field not rendering correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8647</link><project id="" key="" /><description>It seems the sort field, when using search template, does not support array template:

```
curl -XGET localhost:9200/_search/template\?pretty -d '{"template":{"sort":["{{#sort}}","{{.}}","{{/sort}}"]}, "params":{"sort":["created_at"]}}'
```

Server logs:

```
Parse Failure [Failed to parse source [{"sort":["","created_at",""]}]]
```

Doing this works (sorting by one field without specifying sort order):

```
curl -XGET localhost:9200/_search/template\?pretty -d '{"template":{"sort":["{{sort}}"]}, "params":{"sort":"created_at"}}'
```

But specifying order (desc | asc) is not supported:

```
curl -XGET localhost:9200/_search/template\?pretty -d '{"template":{"sort":["{{sort}}"]}, "params":{"sort":{"publishDate":"desc"}}}'
```

Logs:

```
Failed to parse source [{"sort":["{publishDate=desc}"]}]
```
</description><key id="49982696">8647</key><summary>Search Template - sort field not rendering correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chenfisher</reporter><labels /><created>2014-11-25T06:49:28Z</created><updated>2014-11-27T10:14:22Z</updated><resolved>2014-11-27T10:14:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GaelTadh" created="2014-11-25T10:25:28Z" id="64378983">You can achieve the last example by specifying two template fields 

```
curl -XGET localhost:9200/_search/template\?pretty -d '{"template":{"sort":["{{sort}}","{{order}}"]}, "params":{"sort":"created_at","order":"desc"}}'
```

The array issue should be solved in 1.5 see #8255 
</comment><comment author="chenfisher" created="2014-11-26T19:21:30Z" id="64697501">Great, Thanks!
</comment><comment author="s1monw" created="2014-11-26T21:50:00Z" id="64715712">@GaelTadh can we close this issue given #8255 
</comment><comment author="GaelTadh" created="2014-11-27T10:14:21Z" id="64770527">Yes, closing now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide template usage information on index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8646</link><project id="" key="" /><description>Closes #7421
</description><key id="49973140">8646</key><summary>Provide template usage information on index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pkoenig10</reporter><labels><label>:Index Templates</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T03:36:25Z</created><updated>2014-11-28T16:33:34Z</updated><resolved>2014-11-28T16:33:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T17:37:56Z" id="64439502">@markharwood could you take a look at this please?
</comment><comment author="markharwood" created="2014-11-26T10:17:19Z" id="64542181">The "closes" link is to "7241" which is a seemingly unrelated (and previously committed) PR rather than an issue - is this right? 
</comment><comment author="clintongormley" created="2014-11-26T11:13:10Z" id="64548625">@markharwood sorry - that was my typo.  Should have been #7421. Fixed
</comment><comment author="markharwood" created="2014-11-28T15:37:27Z" id="64905662">LGTM. There's no Junit test for this feature but hard to write given the change is a new logfile entry. 
I tested it with mixed file-based template and template and it logged the template names OK.
All other tests pass on latest master.
</comment><comment author="markharwood" created="2014-11-28T16:33:34Z" id="64911084">Pushed to 1.x https://github.com/elasticsearch/elasticsearch/commit/1d519248cffacd35233f5cf8a225c95845193ecb and master https://github.com/elasticsearch/elasticsearch/commit/76157b8b903e1cce91645a78bc5a18b3d782bd25

Thanks, @pkoenig10 !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make `size` property parsing inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8645</link><project id="" key="" /><description>Closes #6061
</description><key id="49973127">8645</key><summary>Make `size` property parsing inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pkoenig10</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T03:36:14Z</created><updated>2015-06-06T19:02:42Z</updated><resolved>2014-11-25T08:01:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-25T08:01:02Z" id="64322477">Merged, thanks! For future contributions, you might want to configure your [`user.name`](https://help.github.com/articles/setting-your-username-in-git/) so that it displays properly (the commit that you attached to this PR has your email address, but "unknown" as a username).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes InternalIndexShard callback handling of failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8644</link><project id="" key="" /><description>Closes #5945
</description><key id="49973118">8644</key><summary>Fixes InternalIndexShard callback handling of failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">pkoenig10</reporter><labels><label>:Engine</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T03:36:04Z</created><updated>2015-06-07T18:02:15Z</updated><resolved>2014-12-05T23:06:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T17:35:23Z" id="64439066">@s1monw could you take a look at this?
</comment><comment author="s1monw" created="2014-11-26T22:02:57Z" id="64717175">I mean the change looks ok to me but what is the bug that gets fixed here? can you maybe explain the intention behind the change or make the commit message more verbose?
</comment><comment author="s1monw" created="2014-11-26T22:04:49Z" id="64717374">oh I see - it relates to #5945 no idea how I missed that :P nevemind it's late here... I will look again tomorrow.
</comment><comment author="s1monw" created="2014-12-05T23:06:07Z" id="65869113">pushed thansk
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Let Lucene kick off merges normally</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8643</link><project id="" key="" /><description>With the last Lucene 5.0.0 snapshot upgrade, I turned off CMS's hard
stalling of incoming threads when merging can't keep up, since ES does
its own index throttling to deal with this.

This means we can simplify a lot of ES's custom wrapper code for
handling segment merges: remove EnableMergeScheduler, MERGE thread
pool, InternalIndexShard.EngineMerger, and the settings
index.merge.force_async_merge and index.merge.async_interval.

Ie, we should now just let Lucene kick off merges as soon as they are
needed, instead of polling every 1.0 second and doing it ourselves.
</description><key id="49962957">8643</key><summary>Let Lucene kick off merges normally</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-25T00:44:43Z</created><updated>2015-06-07T11:54:27Z</updated><resolved>2014-11-25T10:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-25T07:51:26Z" id="64321656">+1 I just left minor comments/questions.
</comment><comment author="jpountz" created="2014-11-25T09:07:33Z" id="64328687">LGTM
</comment><comment author="mikemccand" created="2014-11-25T09:09:28Z" id="64328897">Thanks @jpountz I'll push to master, and backport the "sd" alias fix down to 1.4, 1.x.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/merge/EnableMergeScheduler.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/ConcurrentMergeSchedulerProvider.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/MergeSchedulerProvider.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Core: let Lucene kick off merges</comment></comments></commit></commits></item><item><title>ThreadLeakError by adding groovy language support to ElasticsearchIntegrationTest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8642</link><project id="" key="" /><description>Hello,

we upgraded our maven dependency from 1.3.1 to 1.4.0. By upgrading we had to migrate the MWEL scripts to groovy. With this change we also wanted our elastic search tests passed. We are using the elasticsearch test-jar also on 1.4.0. Both, test-jar and 1.4.0 itself do not bring any library with groovy support. This library has to be included on its own.

We used the groovy-all and elasticsearch-groovy-lang dependency. Both have the same problem. By using the ElasticsearchIntegrationTest.
Our tests are configured with Scope.SUITE and node settings with node.local true and groovy.sandbox.receiver_whitelist.

Can you point me to the problem? I am totally clueless because without the groovy support, we will receive the error: ElasticsearchIllegalArgumentException[script_lang not supported [groovy]].

Any help is appreciated.

Thank you.

```
Nov 24, 2014 5:29:34 PM com.carrotsearch.randomizedtesting.ThreadLeakControl checkThreadLeaks
WARNING: Will linger awaiting termination of 52 leaked thread(s).
Nov 24, 2014 5:29:39 PM com.carrotsearch.randomizedtesting.ThreadLeakControl checkThreadLeaks
SEVERE: com.carrotsearch.randomizedtesting.ThreadLeakError: 52 threads leaked from SUITE scope at de.somepackage.search.MyFunnySortingTest: 
   1) Thread[id=203, name=HystrixTimer-15, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   2) Thread[id=225, name=RxComputationThreadPool-19, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   3) Thread[id=86, name=HystrixTimer-3, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   4) Thread[id=52, name=RxComputationThreadPool-1, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   5) Thread[id=177, name=HystrixTimer-10, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   6) Thread[id=155, name=HystrixTimer-7, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   7) Thread[id=185, name=RxComputationThreadPool-11, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   8) Thread[id=215, name=RxComputationThreadPool-17, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
   9) Thread[id=178, name=hystrix-ElasticMonitorInconsistencies-10, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  10) Thread[id=208, name=HystrixTimer-16, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  11) Thread[id=235, name=RxComputationThreadPool-21, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  12) Thread[id=228, name=HystrixTimer-20, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  13) Thread[id=193, name=HystrixTimer-13, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  14) Thread[id=149, name=RxComputationThreadPool-6, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  15) Thread[id=174, name=RxComputationThreadPool-9, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  16) Thread[id=188, name=HystrixTimer-12, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1081)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  17) Thread[id=220, name=RxComputationThreadPool-18, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  18) Thread[id=195, name=RxComputationThreadPool-13, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  19) Thread[id=114, name=hystrix-ElasticMonitorInconsistencies-4, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  20) Thread[id=48, name=HystrixTimer-1, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  21) Thread[id=205, name=RxComputationThreadPool-15, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  22) Thread[id=158, name=RxComputationThreadPool-7, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  23) Thread[id=75, name=RxComputationThreadPool-2, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  24) Thread[id=165, name=hystrix-ElasticMonitorInconsistencies-8, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  25) Thread[id=180, name=RxComputationThreadPool-10, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  26) Thread[id=138, name=hystrix-ElasticMonitorInconsistencies-5, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  27) Thread[id=183, name=HystrixTimer-11, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  28) Thread[id=213, name=HystrixTimer-17, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  29) Thread[id=167, name=RxComputationThreadPool-8, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  30) Thread[id=87, name=hystrix-ElasticMonitorInconsistencies-3, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  31) Thread[id=156, name=hystrix-ElasticMonitorInconsistencies-7, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  32) Thread[id=190, name=RxComputationThreadPool-12, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  33) Thread[id=113, name=HystrixTimer-4, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  34) Thread[id=116, name=RxComputationThreadPool-4, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  35) Thread[id=223, name=HystrixTimer-19, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  36) Thread[id=69, name=HystrixTimer-2, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  37) Thread[id=137, name=HystrixTimer-5, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  38) Thread[id=172, name=hystrix-ElasticMonitorInconsistencies-9, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  39) Thread[id=164, name=HystrixTimer-8, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  40) Thread[id=230, name=RxComputationThreadPool-20, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  41) Thread[id=233, name=HystrixTimer-21, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  42) Thread[id=200, name=RxComputationThreadPool-14, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  43) Thread[id=70, name=hystrix-ElasticMonitorInconsistencies-2, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  44) Thread[id=89, name=RxComputationThreadPool-3, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  45) Thread[id=198, name=HystrixTimer-14, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  46) Thread[id=210, name=RxComputationThreadPool-16, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  47) Thread[id=146, name=HystrixTimer-6, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  48) Thread[id=140, name=RxComputationThreadPool-5, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  49) Thread[id=218, name=HystrixTimer-18, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  50) Thread[id=49, name=hystrix-ElasticMonitorInconsistencies-1, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  51) Thread[id=171, name=HystrixTimer-9, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088)
        at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
  52) Thread[id=147, name=hystrix-ElasticMonitorInconsistencies-6, state=WAITING, group=TGRP-MyFunnySortingTest]
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:458)
        at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
        at java.util.concurrent.SynchronousQueue.take(SynchronousQueue.java:924)
        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
    at __randomizedtesting.SeedInfo.seed([59002843EE9DD22A]:0)
```
</description><key id="49947448">8642</key><summary>ThreadLeakError by adding groovy language support to ElasticsearchIntegrationTest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">tcompart</reporter><labels><label>feedback_needed</label></labels><created>2014-11-24T21:52:26Z</created><updated>2014-12-07T18:19:32Z</updated><resolved>2014-12-07T18:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T17:01:06Z" id="64433408">@dakrone any ideas what is needed here?
</comment><comment author="dakrone" created="2014-11-25T19:53:35Z" id="64460320">@clintongormley no idea

@tcompart can you tell us more about your configuration? Is this a java project using Elasticsearch as a dependency? Maybe you can share the pom.xml and an example of a test that causes this.

Are you creating groovy scripts in your tests or sending them to an Elasticsearch server?

Does your test create threads that are not being joined?
</comment><comment author="tcompart" created="2014-11-27T23:38:47Z" id="64837936">Hey sorry for the late response.
I try to give you a basic setup to reproduce the error.
- We have groovy script, which are send to the elasticsearch cluster
- We do not create threads. Everything is handled straight and directly by one thread only (main thread)
- We have an indexing class, we have a searching class
- Three objects are created and indexed with different group numbers. 
- We wait until all objects are index (calling a refresh on the index)
- We want to order the search results by group number
- We use a groovy script like the following to do this:

```
return SortBuilders.scriptSort(
        "n = _source.groupNumber; if(n==null){return 0;} else {return n;}",
        "number").order(SortOrder.DESC);
```
- With MWEL this works, with Elasticsearch 1.4.0 and groovy it also works in "production"
- With elasticsearch test-jar the test is green, however the error output marks the test as failed
- The pom looks like the following:

```
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
            &lt;artifactId&gt;lucene-test-framework&lt;/artifactId&gt;
            &lt;version&gt;4.10.2&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;com.carrotsearch.randomizedtesting&lt;/groupId&gt;
            &lt;artifactId&gt;randomizedtesting-runner&lt;/artifactId&gt;
            &lt;version&gt;2.1.10&lt;/version&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;junit&lt;/groupId&gt;
                    &lt;artifactId&gt;junit&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
            &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;
            &lt;version&gt;4.10.2&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;type&gt;test-jar&lt;/type&gt;
            &lt;scope&gt;test&lt;/scope&gt;
            &lt;version&gt;1.4.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch-lang-groovy&lt;/artifactId&gt;
            &lt;version&gt;2.2.0&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;1.4.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework&lt;/groupId&gt;
            &lt;artifactId&gt;spring-beans&lt;/artifactId&gt;
            &lt;version&gt;4.1.0.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;
            &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;
            &lt;version&gt;2.3.4&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.springframework&lt;/groupId&gt;
            &lt;artifactId&gt;spring-context&lt;/artifactId&gt;
            &lt;version&gt;4.1.0.RELEASE&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;    
```

Any ideas? 
My test looks like this:

```
@ClusterScope(scope = Scope.SUITE, numDataNodes = 1)
public class ElasticsearchIntegrationTestBase extends ElasticsearchIntegrationTest {
@Before
  public void createIndex() throws IOException {
    prepareCreate(ELASTIC_INDEX).setSettings(
      ImmutableSettings.builder().loadFromSource(settings()).build())
    .addMapping(ElasticIndexer.TYPE, mapping())
    .execute()
    .actionGet(timeValueSeconds(10));

    ElasticsearchIntegrationTest.client()
    .admin()
    .cluster()
    .prepareHealth()
    .setWaitForActiveShards(NUMBER_OF_SHARDS)
    .setWaitForGreenStatus()
    .execute()
    .actionGet();
  }
}
```
</comment><comment author="dakrone" created="2014-11-28T08:55:06Z" id="64869919">@tcompart from the look of the original failure, your test suite/infrastructure is leaking threads from `HystrixTimer` and the `RxComputationThreadPool`, it looks like you need to shut down the threadpool and Hystrix before your test suite ends so that they don't leak.
</comment><comment author="tcompart" created="2014-12-07T00:23:23Z" id="65920947">@dakrone 
Thank you for your answer. It seems you are right.
With an additional @After Hystrix.reset() I could kill all existing HystrixThreads, however, the RxComputationThreadPool threads remain. 

I currently have no idea where they all are coming from.
I never ever started Hystrix or Rx on my own. Lucenes RandomizedRunner is finding those open threads by its own: ThreadLeakControl.

Do you have any ideas how to disable the check?
Or how to kill open threads of RxComputationThreadPool. Is just me with this kind of problems? Thats really crazy.
</comment><comment author="s1monw" created="2014-12-07T10:00:09Z" id="65932242">I'd check who pulls in the `RxComputationThreadPool` and take it from there. This is actually a feature not a bug to be honest. You are leaving threads behind which is no-good. You can disable it by using
`@ThreadLeakScope(Scope.NONE)` on your test class...
</comment><comment author="tcompart" created="2014-12-07T18:19:31Z" id="65947205">Thank you Simon.
`@ThreadLeakScope(ThreadLeakScope.Scope.NONE)` acuatelly fixed the error. We did not know about this feature, and will investigate our tests to find the remaining RxComputationThreadPool-Threads. 

Thank you so far, and I will close this issue, because we have now an idea about the error report, and we know how to fix the errors.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clarify index removal log message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8641</link><project id="" key="" /><description /><key id="49943363">8641</key><summary>Clarify index removal log message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-24T21:15:22Z</created><updated>2015-06-06T19:19:57Z</updated><resolved>2014-11-26T03:44:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-24T22:56:31Z" id="64280734">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] SimpleRecoveryLocalGatewayTests.testLatestVersionLoaded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8640</link><project id="" key="" /><description>http://build-us-00.elasticsearch.org/job/es_core_master_debian/2468/

```
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/2/no master];
    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:151)
    at org.elasticsearch.action.admin.indices.refresh.TransportRefreshAction.checkGlobalBlock(TransportRefreshAction.java:126)
    at org.elasticsearch.action.admin.indices.refresh.TransportRefreshAction.checkGlobalBlock(TransportRefreshAction.java:50)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.&lt;init&gt;(TransportBroadcastOperationAction.java:115)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:71)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:47)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
    at org.elasticsearch.client.node.NodeIndicesAdminClient.execute(NodeIndicesAdminClient.java:77)
    at org.elasticsearch.client.support.AbstractIndicesAdminClient.refresh(AbstractIndicesAdminClient.java:414)
    at org.elasticsearch.action.admin.indices.refresh.RefreshRequestBuilder.doExecute(RefreshRequestBuilder.java:48)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65)
    at org.elasticsearch.gateway.local.SimpleRecoveryLocalGatewayTests$2.doAfterNodes(SimpleRecoveryLocalGatewayTests.java:304)
    at org.elasticsearch.test.InternalTestCluster.restartAllNodes(InternalTestCluster.java:1215)
    at org.elasticsearch.test.InternalTestCluster.fullRestart(InternalTestCluster.java:1267)
    at org.elasticsearch.gateway.local.SimpleRecoveryLocalGatewayTests.testLatestVersionLoaded(SimpleRecoveryLocalGatewayTests.java:297)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:827)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:877)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49941968">8640</key><summary>[CI Failure] SimpleRecoveryLocalGatewayTests.testLatestVersionLoaded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>jenkins</label></labels><created>2014-11-24T21:02:07Z</created><updated>2015-10-23T07:24:09Z</updated><resolved>2015-10-23T07:24:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-03-05T09:52:42Z" id="77335772">Another build failure: http://build-us-00.elasticsearch.org/job/es_core_14_centos/3004/

and looks to be the same failure.
</comment><comment author="s1monw" created="2015-10-23T07:24:09Z" id="150496259">this hasn't failed in a long time.... closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove unnecessary index removal on index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8639</link><project id="" key="" /><description>After one of the refactoring we started to remove index immediately after index creation instead of only on index creation failure.

That doesn't seem to be necessary and creates confusing debugging information in log files.
</description><key id="49937515">8639</key><summary>Remove unnecessary index removal on index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Core</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-24T20:23:20Z</created><updated>2015-06-07T18:02:34Z</updated><resolved>2014-11-24T20:38:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Support query and filter in `function_score`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8638</link><project id="" key="" /><description>Function score is an excellent addition to ES. However, it doesn't support query + filter, which works in most other parts of ES.

Currently, `function_score` takes either a filter OR a query, but not both. Allowing support for both query and filter would be good because:
1. It would align better with the rest of ES (it took me a while to figure out why the filter wasn't applied). Overall the API is fairly consistent and well-designed, which I like. Smoothing out this rough edge would make it even better!
2. It would be very useful.

Below is a simple use-case.

```
# This query is run against two indices, 'users' and 'speakers'.
# But I think the situation would be the same if it was a single index, 
# with two different types in it.
# 
# I get the following error:
# `QueryParsingException[[users] Unknown field [mentioned_at]];`
#
# All speakers has the `mentioned_at` field, and no user has it.

query_body = {
  :query =&gt; {
    :bool =&gt; {
      :should =&gt; [
        {
          :function_score =&gt; {
            :query =&gt; {
              :multi_match =&gt; {
                :query =&gt; "MY QUERY HERE",
                :type =&gt; :phrase_prefix,
                :fields =&gt; ['name']
              }
            },
            :filter =&gt; { :type =&gt; { :value =&gt; 'speaker' } },
            :functions =&gt; [
              {
                :gauss =&gt; {
                  'mentioned_at' =&gt; {
                    :scale =&gt; '180d',
                    :offset =&gt; '5d',
                    :decay =&gt; 0.5,
                  }
                }
              }
            ]
          }
        },
        {
          :filtered =&gt; {
            :query =&gt; {
              :multi_match =&gt; {
                :query =&gt; "MY QUERY HERE",
                :type =&gt; :phrase_prefix,
                :fields =&gt; ['name']
              }
            },
            :filter =&gt; { :type =&gt; { :value =&gt; 'user' } },
          }
        }
      ]
    }
  }
}
```

_Reference_

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html
</description><key id="49926191">8638</key><summary>Support query and filter in `function_score`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">sandstrom</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-24T18:43:24Z</created><updated>2015-01-19T12:42:14Z</updated><resolved>2015-01-19T12:42:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T13:43:38Z" id="64400854">Hi @sandstrom 

You can use a `filtered` query instead, which will do what you need.  However, I agree that silently choosing just one of `filter` or `query` is incorrect.  I see no need why we can't combine both.

@brwe ?
</comment><comment author="sandstrom" created="2014-11-25T20:26:22Z" id="64465175">@clintongormley Awesome, thanks for looking into this! :boat: 
</comment><comment author="brwe" created="2014-11-26T15:21:57Z" id="64660789">easiest would be to just convert given query and filter to a filtered_query. I'll make a pr shortly.
</comment><comment author="sandstrom" created="2014-11-26T18:50:22Z" id="64693219">@brwe Awesome!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreTests.java</file></files><comments><comment>function_score: use query and filter together</comment></comments></commit></commits></item><item><title>CBOR: Improve recognition of CBOR data format and update to Jackson 2.4.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8637</link><project id="" key="" /><description>- Update pom to 2.4.3 from 2.4.2
- Enable the CBOR data header (aka tag) from the CBOR Generator to provide binary identification like the Smile format
- Check for the CBOR header and ensure that the data sent in represents a "major type" that is an object
- Cleans up `JsonVsCborTests` unused imports

Closes #7640
</description><key id="49916943">8637</key><summary>CBOR: Improve recognition of CBOR data format and update to Jackson 2.4.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>bug</label></labels><created>2014-11-24T17:19:56Z</created><updated>2015-03-19T10:18:38Z</updated><resolved>2014-11-25T19:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-11-24T17:20:25Z" id="64228964">@s1monw I think you wanted this ASAP
</comment><comment author="jpountz" created="2014-11-24T18:29:55Z" id="64239912">Just left one question/comment about the detection of short json documents, otherwise it looks good!
</comment><comment author="jpountz" created="2014-11-25T18:02:24Z" id="64443393">@pickypg Just left a minor comment, I think it's close
</comment><comment author="jpountz" created="2014-11-25T18:13:56Z" id="64445100">LGTM
</comment><comment author="pickypg" created="2014-11-25T19:26:23Z" id="64456066">Merged 7523d0b150ff7b05aa37b5e6edf8216341671b38 (forgot to change message to `Closes ...`)
</comment><comment author="clintongormley" created="2014-11-26T07:58:45Z" id="64526785">PR reverted for now
</comment><comment author="s1monw" created="2014-12-01T21:53:22Z" id="65141506">@pickypg any news on this?
</comment><comment author="pickypg" created="2014-12-02T18:48:39Z" id="65282288">@s1monw Nothing substantial. It looks like I'll have time today to dive deeper.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure the service is started after network is up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8636</link><project id="" key="" /><description>I encountered an issue with the systemd process ordering. The network target starts after ES then it crashes at boot time because the interface wasn't up.

In the rpm package, the systemd service doesn't ensure that the network is up before starting ES.

I suggest to add an After param : 

[Unit]
Description=Starts and stops a single elasticsearch instance on this system
&lt;b&gt;After=network.target&lt;/b&gt;
Documentation=http://www.elasticsearch.org
[...]
</description><key id="49906349">8636</key><summary>Ensure the service is started after network is up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">bennysan</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2014-11-24T16:00:33Z</created><updated>2015-06-17T07:29:39Z</updated><resolved>2015-06-17T07:29:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="t-lo" created="2014-12-03T11:46:00Z" id="65395690">According to http://www.freedesktop.org/wiki/Software/systemd/NetworkTarget/, this should be

``` ini
[Unit]
Description=Starts and stops a single elasticsearch instance on this system
Wants=network-online.target
After=network-online.target
Documentation=http://www.elasticsearch.org
...
```

but things seem to be a bit more complicated. I'll check how `network-online.target` works on my Fedora 21 box.
</comment><comment author="t-lo" created="2014-12-03T11:58:43Z" id="65396938">Looking at `elasticsearch.service`, there's room for more improvement. In the service configuration `bin/elasticsearch` is told to fork and to provide a PID file. Both is unnecessary when systemd is available (see http://0pointer.de/blog/projects/systemd-for-admins-3.html). Part of the intention to write systemd in the first place was to move daemonizing / PID handling complexity from start scripts into the init service. So `src/rpm/systemd/elasticsearch.service` can be simplified quite a bit.
</comment><comment author="t-lo" created="2014-12-03T13:07:40Z" id="65403891">Changes are implemented, tested w/ Fedora 21, works as expected.
</comment><comment author="tlrx" created="2015-06-17T07:29:38Z" id="112692551">Thanks for reporting and your contribution.

I agree with @t-lo it should be `network-online.target` and it how it is configured now (see https://github.com/elastic/elasticsearch/blob/master/core/src/packaging/common/systemd/elasticsearch.service). It has been tested on various distributions including Fedora21, OpenSUSE13, Ubuntu15.04, Debian8, CentOS7.

Closing this one, feel free to reopen if needed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation: Additional info about _score calculation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8635</link><project id="" key="" /><description>Description taken from http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/multi-query-strings.html / 110_Multi_Field_Search/05_Multiple_query_strings.asciidoc
</description><key id="49899286">8635</key><summary>Documentation: Additional info about _score calculation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmamat</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2014-11-24T15:09:37Z</created><updated>2014-11-28T15:25:56Z</updated><resolved>2014-11-28T12:55:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:59:18Z" id="64395751">Hi @pmamat 

I've added some comments to make the text fit better on this page.  Also, please could you sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) so that I can merge your changes in (once you have made the changes above)

thanks
</comment><comment author="pmamat" created="2014-11-25T14:13:50Z" id="64404740">Hi @clintongormley,

I'm happy to have signed the CLA. Please feel free to merge the changes any time.

Regards,
 Philipp
</comment><comment author="clintongormley" created="2014-11-25T17:51:00Z" id="64441509">Hi @pmamat 

Thanks for signing the CLA. Could you update the PR to include the changes I suggested and push again? Then I can go ahead and merge.
</comment><comment author="pmamat" created="2014-11-27T17:27:44Z" id="64815778">Hi @clintongormley 
I merged your first suggestion and discarded the second sentence completely, because it did not make sense any more and I think that your suggestion "The more should clauses that match, the higher the score." does not fit either (there could be one fuzzy matching should clause with a very high score in one hit and two fuzzy matching should clauses with very low scores in another hit).
</comment><comment author="clintongormley" created="2014-11-28T12:55:02Z" id="64891084">LGTM - thanks @pmamat. Merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Additional info about _score calculation</comment></comments></commit></commits></item><item><title>[TEST] Extend unicast ports generation to support more concurrent clusters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8634</link><project id="" key="" /><description>The current `UnicastZen` only supports up to 10 different clusters per jvm, after which it generates ports that overlap between different jvms.

Make it possible to run multiple tests with unicast configuration, by assigning ports based on their test scope.
Every jvm still gets its own port range based on the jvm id, but we now make sure that the different jvms ranges never overlap. The global cluster gets a reserved port range, while SUITE and TEST scopes are treated equally, just assuming that they never run concurrently on the same jvm, thus ports can be safely reused.
</description><key id="49895033">8634</key><summary>[TEST] Extend unicast ports generation to support more concurrent clusters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-24T14:30:54Z</created><updated>2014-11-27T08:21:32Z</updated><resolved>2014-11-27T08:19:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-26T17:01:49Z" id="64677285">I like it. Thx Luca for looking into this. Left two comments.
</comment><comment author="javanna" created="2014-11-27T07:46:24Z" id="64755899">Thanks @bleskes updated
</comment><comment author="bleskes" created="2014-11-27T07:52:23Z" id="64756337">@javanna two little minor details and we're done. thx.
</comment><comment author="javanna" created="2014-11-27T07:58:01Z" id="64756736">thanks @bleskes can you have another look please?
</comment><comment author="bleskes" created="2014-11-27T07:58:47Z" id="64756801">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptions.java</file><file>src/test/java/org/elasticsearch/discovery/ZenUnicastDiscoveryTests.java</file><file>src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java</file></files><comments><comment>[TEST] Extend unicast ports generation to support more concurrent clusters</comment></comments></commit></commits></item><item><title>Make a much smaller Java API that includes transport client.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8633</link><project id="" key="" /><description>This may exist so apologies if I couldn't find it and am duplicating here: 

Would it be possible to break out the code for a client only API that doesn't include the entire elastic search code base?

Currently we have a lot of modular web services modules that make calls to elastic search and because we want to include the advantages of the transport client we have to use the full elastic search codebase.  This means that each web service war file is +30 megabytes.  

Ideally we'd like a client jar file that only includes the API and the transport code.
</description><key id="49894279">8633</key><summary>Make a much smaller Java API that includes transport client.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">willwarren</reporter><labels><label>discuss</label></labels><created>2014-11-24T14:22:58Z</created><updated>2015-04-10T17:37:44Z</updated><resolved>2015-04-10T17:34:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="osman" created="2014-11-28T15:17:05Z" id="64903747">:+1:
</comment><comment author="javanna" created="2015-03-19T19:12:58Z" id="83721258">relates to #6170
</comment><comment author="dakrone" created="2015-04-10T17:34:38Z" id="91630563">Closing as duplicate of #8633
</comment><comment author="dakrone" created="2015-04-10T17:37:44Z" id="91631784">Whoops, meant to say closing as duplicate of #6170
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add notes about scripting and bulk api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8632</link><project id="" key="" /><description>I think it would be good to encourage use of scripts that take parameters versus unique script for each document, so that the script cache is effective. Its currently not very intuitive to the user that this can have a big impact on indexing performance.
</description><key id="49883181">8632</key><summary>Add notes about scripting and bulk api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2014-11-24T12:12:53Z</created><updated>2016-07-13T12:48:04Z</updated><resolved>2016-07-13T12:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-11-24T12:52:26Z" id="64189724">+1, I've seen a couple cases where apps were accidentally creating a new script for each update.
</comment><comment author="nik9000" created="2014-11-24T13:19:02Z" id="64192309">Is the number of scripts compiled exposed for monitoring?  That could be
useful.
On Nov 24, 2014 7:52 AM, "Michael McCandless" notifications@github.com
wrote:

&gt; +1, I've seen a couple cases where apps were accidentally creating a new
&gt; script for each update.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8632#issuecomment-64189724
&gt; .
</comment><comment author="dakrone" created="2014-11-24T13:21:38Z" id="64192584">What if we were to return a key in the response about whether the script
were newly compiled, or if it were reused from the cache?

That could help people realize when they were sending a new script every
time

On Nov 24, 2014 2:19 PM, "Nik Everett" notifications@github.com wrote:

&gt; Is the number of scripts compiled exposed for monitoring? That could be
&gt; useful.
&gt; On Nov 24, 2014 7:52 AM, "Michael McCandless" notifications@github.com
&gt; wrote:
&gt; 
&gt; &gt; +1, I've seen a couple cases where apps were accidentally creating a
&gt; &gt; new
&gt; &gt; script for each update.
&gt; &gt; 
&gt; &gt; —
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; &lt;
&gt; &gt; https://github.com/elasticsearch/elasticsearch/issues/8632#issuecomment-64189724&gt;
&gt; &gt; 
&gt; &gt; .
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="nik9000" created="2014-11-24T22:07:55Z" id="64273854">&gt; What if we were to return a key in the response about whether the script were newly compiled, or if it were reused from the cache? That could help people realize when they were sending a new script every time

I was thinking about that but figured it might be easier to use the counter.  Also, might be more useful because it'd be easy to graph.
</comment><comment author="clintongormley" created="2015-11-21T21:39:05Z" id="158684049">Script compilation stats were added in https://github.com/elastic/elasticsearch/pull/12733

Docs could still be improved.
</comment><comment author="clintongormley" created="2016-07-13T12:48:04Z" id="232344480">The docs were improved in https://github.com/elastic/elasticsearch/pull/18132
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Only primary IP address in `_cat/nodes` ouput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8631</link><project id="" key="" /><description>I have set `-Des.network.bind_host` and `-Des.network.publish_host` to use IP address of an non-primary interface, but the `_cat/nodes?h=ip,name` still gives me primary IP.

``` JSON
curl '10.10.1.1:9200/_nodes/_local?pretty'
{
  "cluster_name" : "elasticsearch",
  "nodes" : {
    "wxDJnCJfQjKjsm0dQvqIVQ" : {
      "name" : "Algrim the Strong",
      "transport_address" : "inet[/10.10.1.1:9300]",
      "host" : "d1edb4b54c7f",
      "ip" : "10.1.0.11",
      "version" : "1.4.0",
      "build" : "bc94bd8",
      "http_address" : "inet[/10.10.1.1:9200]",
      "settings" : {
        "name" : "Algrim the Strong",
        "path" : {
          "logs" : "/usr/elasticsearch-1.4.0/logs",
          "home" : "/usr/elasticsearch-1.4.0"
        },
        "cluster" : {
          "name" : "elasticsearch"
        },
        "client" : {
          "type" : "node"
        },
        "foreground" : "yes",
        "network" : {
          "bind_host" : "_ethwe:ipv4_",
          "publish_host" : "_ethwe:ipv4_"
        }
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 1,
        "cpu" : {
          "vendor" : "Intel",
          "model" : "Core(TM) i7-4870HQ CPU @ 2.50GHz",
          "mhz" : 2498,
          "total_cores" : 1,
          "total_sockets" : 1,
          "cores_per_socket" : 1,
          "cache_size_in_bytes" : 6144
        },
        "mem" : {
          "total_in_bytes" : 1046568960
        },
        "swap" : {
          "total_in_bytes" : 0
        }
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 1,
        "max_file_descriptors" : 1048576,
        "mlockall" : false
      },
      "jvm" : {
        "pid" : 1,
        "version" : "1.8.0_25",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "25.25-b02",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1416827899445,
        "mem" : {
          "heap_init_in_bytes" : 268435456,
          "heap_max_in_bytes" : 1065025536,
          "non_heap_init_in_bytes" : 2555904,
          "non_heap_max_in_bytes" : 0,
          "direct_max_in_bytes" : 1065025536
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Metaspace", "Compressed Class Space", "Par Eden Space", "Par Survivor Space", "CMS Old Gen" ]
      },
      "thread_pool" : {
        "percolate" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : "1k"
        },
        "bench" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "listener" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : "200"
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : "1k"
        },
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 3,
          "max" : 3,
          "queue_size" : "1k"
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "get" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : "1k"
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : "50"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "10.1.0.11",
          "name" : "eth0",
          "mac_address" : "02:42:0A:01:00:0B"
        }
      },
      "transport" : {
        "bound_address" : "inet[/10.10.1.1:9300]",
        "publish_address" : "inet[/10.10.1.1:9300]"
      },
      "http" : {
        "bound_address" : "inet[/10.10.1.1:9200]",
        "publish_address" : "inet[/10.10.1.1:9200]",
        "max_content_length_in_bytes" : 104857600
      },
      "plugins" : [ ]
    }
  }
}
```

Although the above has correct addresses in most places, but the `nodes.*.ip`...

```
curl '10.10.1.1:9200/_cat/nodes?h=ip,name,http_address'
10.1.0.6  Thane Ector       
10.1.0.12 Steel Spider      
10.1.0.5  Aguja             
10.1.0.11 Algrim the Strong 
```

At least it would be great to be able to do `_cat/nodes?h=http_address` as sort of a work-around, I guess, but doesn't look like it's currently exposed.
</description><key id="49882033">8631</key><summary>Only primary IP address in `_cat/nodes` ouput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">errordeveloper</reporter><labels /><created>2014-11-24T11:57:58Z</created><updated>2014-11-25T12:44:33Z</updated><resolved>2014-11-25T12:44:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:44:33Z" id="64394097">HI @errordeveloper 

Thanks for opening - this looks like a duplicate of #5065.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregate terms sum of array data getting wrong result </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8630</link><project id="" key="" /><description>Hi,
I have data like follows

```
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "index_testing",
      "_type" : "status",
      "_id" : "535733748264759296",
      "_score" : 1.0,
      "_source":{"text":"sreejith testing big data , testing smap, testing ES","link":[],"hashtag":[],"senti_data":[{"count":1,"sentiment_score":0,"word":"data"},{"count":3,"sentiment_score":0,"word":"testing"},{"count":1,"sentiment_score":0,"word":"big"},{"count":1,"sentiment_score":0,"word":"sreejith"},{"count":1,"sentiment_score":0,"word":"es"},{"count":1,"sentiment_score":0,"word":"smap"}],"truncated":false,"SentimentScore":1,"Sentiment":"positive","source":"&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Twitter Web Client&lt;/a&gt;","gender":"female","retweet_count":0,"created_at":"2014-11-21T09:57:33.000Z","mention":[],"language":"en","user":{"id":166819950,"profile_image_url_https":"https://pbs.twimg.com/profile_images/458931356345434112/KkMSDG_P_normal.jpeg","location":"","description":"am what am... :P","name":"Sreejith","screen_name":"sreejithsga","profile_image_url":"http://pbs.twimg.com/profile_images/458931356345434112/KkMSDG_P_normal.jpeg"}}
    }, {
      "_index" : "index_testing",
      "_type" : "status",
      "_id" : "535733934550577152",
      "_score" : 1.0,
      "_source":{"text":"sreejith testing hadoop","link":[],"hashtag":[],"senti_data":[{"count":1,"sentiment_score":0,"word":"hadoop"},{"count":1,"sentiment_score":0,"word":"testing"},{"count":1,"sentiment_score":0,"word":"sreejith"}],"truncated":false,"SentimentScore":-1,"Sentiment":"negative","source":"&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Twitter Web Client&lt;/a&gt;","gender":"female","retweet_count":0,"created_at":"2014-11-21T09:58:18.000Z","mention":[],"language":"en","user":{"id":166819950,"profile_image_url_https":"https://pbs.twimg.com/profile_images/458931356345434112/KkMSDG_P_normal.jpeg","location":"","description":"am what am... :P","name":"Sreejith","screen_name":"sreejithsga","profile_image_url":"http://pbs.twimg.com/profile_images/458931356345434112/KkMSDG_P_normal.jpeg"}}
    } ]
  }
}
```

I tried the following query 

```
POST http://localhost:9200/index_testing/_search -d'
{
            "query": {
              "query_string": {
                 "default_field" : "text",
                 "query" :  "testing"
              }

            },
             "filter": {
                "exists": {
                  "field": "senti_data"
                }
            },
            "sort": [
               {
                  "created_at": {
                     "order": "desc"
                  }
               }
            ],
           "aggs": {
                "prod_type": {
                    "terms": {
                        "field": "senti_data.word"
                    },
                    "aggs": {
                        "count": {
                            "sum": {
                                "field": "senti_data.count"
                            }
                        }
                    }
                }
            },
            "size": 0



}'
```

while running this query am getting the following result

```
{
   "took": 6,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "prod_type": {
         "buckets": [
            {
               "key": "sreejith",
               "doc_count": 2,
               "count": {
                  "value": 5
               }
            },
            {
               "key": "testing",
               "doc_count": 2,
               "count": {
                  "value": 5
               }
            },
            {
               "key": "big",
               "doc_count": 1,
               "count": {
                  "value": 4
               }
            },
            {
               "key": "data",
               "doc_count": 1,
               "count": {
                  "value": 4
               }
            },
            {
               "key": "es",
               "doc_count": 1,
               "count": {
                  "value": 4
               }
            },
            {
               "key": "hadoop",
               "doc_count": 1,
               "count": {
                  "value": 1
               }
            },
            {
               "key": "smap",
               "doc_count": 1,
               "count": {
                  "value": 4
               }
            }
         ]
      }
   }
}
```

Issue found 
it returns the incorrect  aggregate sum of "count"  
</description><key id="49878881">8630</key><summary>Aggregate terms sum of array data getting wrong result </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sreejithsrk</reporter><labels><label>feedback_needed</label></labels><created>2014-11-24T11:17:43Z</created><updated>2015-02-28T04:51:36Z</updated><resolved>2015-02-28T04:51:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:36:52Z" id="64393279">@sreejithsrk Do you mean that it looks like your filter is not being taken into account?  You're using the top-level `filter` (now renamed `post_filter`) which is only applied to the `hits` AFTER aggregations have been calculated.  If you want it to apply to aggregations, then you should use a `filtered` query instead.

If this isn't your issue, then please provide a complete recreation (including creating the index, putting the mapping, inserting docs, running the query, and tell us what results you are expecting and how the existing results are different).

thanks
</comment><comment author="clintongormley" created="2015-02-28T04:51:36Z" id="76510579">No more info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException with empty string filter &amp; Java Driver</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8629</link><project id="" key="" /><description>I currently have a NullPointerException when using the Java Driver (1.4.0).
It is displayed client side, but it happens on the server side.

```
2014-11-24 11:42:34,347 [elasticsearch[Zeus][transport_client_worker][T#8]{New I/O worker #8}] DEBUG type - [Zeus] [myindex][3], node[0tdeHG7LSMKLKyqn7g0ifw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@69a9f314] lastShard [true]
org.elasticsearch.transport.RemoteTransportException: [Ancient One][inet[/192.168.200.103:9300]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.search.SearchParseException: [myindex][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"filtered":{"filter":{"term":{"source":""}}}}}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:681) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:537) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:509) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:264) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:776) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:767) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275) ~[elasticsearch-1.4.0.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_25]
Caused by: java.lang.NullPointerException: null
    at java.nio.HeapCharBuffer.&lt;init&gt;(HeapCharBuffer.java:70) ~[na:1.7.0_25]
    at java.nio.CharBuffer.wrap(CharBuffer.java:369) ~[na:1.7.0_25]
    at org.elasticsearch.common.xcontent.json.JsonXContentParser.utf8Bytes(JsonXContentParser.java:91) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.common.xcontent.json.JsonXContentParser.objectBytes(JsonXContentParser.java:116) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.TermFilterParser.parse(TermFilterParser.java:97) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:315) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:296) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:252) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33) ~[elasticsearch-1.4.0.jar:na]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:665) ~[elasticsearch-1.4.0.jar:na]
    ... 9 common frames omitted
```

Note that the query is successfully parsed the first time it is executed and no error is thrown, but always fail after...
I was not able to reproduce it using curl + shell.

Below is an extract of the code to help reproduce this behaviour:

```
   org.elasticsearch.node.Node node = NodeBuilder.nodeBuilder().client(true).node();
   Client client = node.client();

   FilterBuilder emptyFilter = FilterBuilders.termFilter("source", "");

   SearchResponse response1 = client.prepareSearch("myindex").setTypes("mytype").setQuery(QueryBuilders.filteredQuery(null, emptyFilter)).execute().actionGet();
   logger.info("First attempt");

   SearchResponse response2 = client.prepareSearch("myindex").setTypes("mytype").setQuery(QueryBuilders.filteredQuery(null, emptyFilter)).execute().actionGet();
   logger.info("Done");
```
</description><key id="49878326">8629</key><summary>NullPointerException with empty string filter &amp; Java Driver</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/masaruh/following{/other_user}', u'events_url': u'https://api.github.com/users/masaruh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/masaruh/orgs', u'url': u'https://api.github.com/users/masaruh', u'gists_url': u'https://api.github.com/users/masaruh/gists{/gist_id}', u'html_url': u'https://github.com/masaruh', u'subscriptions_url': u'https://api.github.com/users/masaruh/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/572174?v=4', u'repos_url': u'https://api.github.com/users/masaruh/repos', u'received_events_url': u'https://api.github.com/users/masaruh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/masaruh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'masaruh', u'type': u'User', u'id': 572174, u'followers_url': u'https://api.github.com/users/masaruh/followers'}</assignee><reporter username="">fabienrousseau</reporter><labels><label>bug</label></labels><created>2014-11-24T11:11:17Z</created><updated>2015-04-06T03:41:57Z</updated><resolved>2015-04-06T03:41:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-11-25T19:37:44Z" id="64457852">@fabienrousseau Just to be sure (since it's happening on the server): the server is also running 1.4.0, right?
</comment><comment author="fabienrousseau" created="2014-11-26T09:39:44Z" id="64537661">@pickypg Yep. client &amp; server are both on 1.4.0
</comment><comment author="tveimo" created="2015-02-10T10:41:53Z" id="73678959">I also see this, with es 1.4.2 client &amp; server. My query and exceptions is as follows;

org.elasticsearch.search.SearchParseException: [karriere][4]: from[0],size[36]: Parse Failure [Failed to parse source [{"from":0,"size":36,"query":{"filtered":{"query":{"match_all":{}},"filter":{"and":{"filters":[{"term":{"regions":""}}]}}}},"sort":[{"adType":{"order":"asc","missing":"_last"}},{"updated":{"order":"desc","missing":"_first"}}],"facets":{"adType":{"terms":{"field":"adType","size":200,"all_terms":true},"facet_filter":{"and":{"filters":[{"term":{"regions":""}}]}},"global":true},"regions":{"terms":{"field":"regions","size":200,"all_terms":true},"facet_filter":{"and":{"filters":[]}},"global":true},"industry":{"terms":{"field":"industry","size":200,"all_terms":true},"facet_filter":{"and":{"filters":[{"term":{"regions":""}}]}},"global":true},"postPlace":{"terms":{"field":"postPlace","size":200,"all_terms":true},"facet_filter":{"and":{"filters":[{"term":{"regions":""}}]}},"global":true},"stateProvince":{"terms":{"field":"stateProvince","size":200,"all_terms":true},"facet_filter":{"and":{"filters":[{"term":{"regions":""}}]}},"global":true},"educationalQualification":{"terms":{"field":"educationalQualification","size":200,"all_terms":true},"facet_filter":{"and":{"filters":[{"term":{"regions":""}}]}},"global":true},"workTime":{"terms":{"field":"workTime","size":200,"all_terms":true},"facet_filter":{"and":{"filters":[{"term":{"regions":""}}]}},"global":true},"numemployed":{"statistical":{"field":"numemployed"}}}}]]

I have just upgraded from 0.20.6 (we had to stay on this version for some time due to hard lucene dependencies elsewhere), where the query was working ok. Is there anything I can do to help diagnose this issue?
</comment><comment author="nicktindall" created="2015-02-17T03:12:21Z" id="74611090">This still occurs in 1.4.3 (client and server)
</comment><comment author="masaruh" created="2015-02-19T06:23:17Z" id="75004578">I think I know what's happening here. It's due to the way Jackson parses input. Long story short, Jackson's `CBORParser` (which is used to parse query) has cache and if it finds cached value, it returns null as a result of `getTextCharacters`, which is called from `JsonXContentParser.utf8Bytes` in the stack trace, while it's parsing empty input (If it doesn't, it returns empty array)

First query doesn't get NPE since nothing is cached.

I submitted a PR (https://github.com/FasterXML/jackson-core/pull/183) to make it return empty array instead of null.
(It looks a fix for something different but it fixes this case too, where `_currentSegment` is null)
</comment><comment author="javanna" created="2015-03-19T18:46:32Z" id="83714587">Seems related to #6172 if not exactly the same.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java</file><file>src/test/java/org/elasticsearch/common/xcontent/cbor/CborXContentParserTests.java</file></files><comments><comment>Queries: Avoid NPE during query parsing</comment></comments></commit></commits></item><item><title>Alias filter with has_child</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8628</link><project id="" key="" /><description>I am using elasticsearch 1.4.0 and I want to create a filtered alias. All is fine until I try to add a has_child filter. The response returned from the server is: 

`{
   "error": "ElasticsearchIllegalArgumentException[failed to parse filter for alias [&lt;alias_name&gt;]]; nested: QueryParsingException[[&lt;index_name&gt;] [has_child] query and filter requires a search context]; ",
   "status": 400
}`

The filter works fine if I use it to filter the documents in the index. Are has_child filters supported when creating aliases?
</description><key id="49878206">8628</key><summary>Alias filter with has_child</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mikiot</reporter><labels /><created>2014-11-24T11:09:36Z</created><updated>2014-11-25T13:49:47Z</updated><resolved>2014-11-25T13:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:29:46Z" id="64392578">@martijnvg please take a look at this?
</comment><comment author="martijnvg" created="2014-11-25T12:59:32Z" id="64395778">@mikiot Thanks for reporting, I think that this is a regression bug.
</comment><comment author="martijnvg" created="2014-11-25T13:49:47Z" id="64401652">@mikiot The bug has been fixed. The fix for this issue will be included in the next upcoming release. 

This bug was introduced in version `1.1.2` via #5916.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryParserUtils.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file></files><comments><comment>Parent/child: Fixed parent/child not being able to be used in alias filters.</comment></comments></commit></commits></item><item><title>Fix iterator over global ordinals.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8627</link><project id="" key="" /><description>Our iterator over global ordinals is currently incorrect since it does NOT
return -1 (NO_MORE_ORDS) when all ordinals have been consumed. This bug does
not strike immediately with elasticsearch since we always consume ordinals in
a random-access fashion. However it strikes when consuming ordinals through
Lucene helpers such as DocValues#docsWithField.

Close #8580
</description><key id="49877551">8627</key><summary>Fix iterator over global ordinals.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-24T11:01:13Z</created><updated>2015-06-07T17:49:10Z</updated><resolved>2014-11-24T18:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-24T15:26:00Z" id="64209325">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Missing aggregation should not use global ordinals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8626</link><project id="" key="" /><description>It seems that the missing aggregation is using global ordinals (#8580) which is wasteful since we only need to know whether a document has a value. We should just use segment-level fielddata.
</description><key id="49876506">8626</key><summary>Aggregations: Missing aggregation should not use global ordinals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label></labels><created>2014-11-24T10:48:08Z</created><updated>2014-11-24T10:48:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Make dependency on `which` in the binary package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8625</link><project id="" key="" /><description>If I install ES without `which` rpm installed, this happens

```
[root@dev ~]# service elasticsearch start
/etc/init.d/elasticsearch: line 57: which: command not found
Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME
```

Anyone with the access to the .spec could please add the dependency for `which`, please?
</description><key id="49873971">8625</key><summary>Make dependency on `which` in the binary package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grepwood</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-24T10:18:39Z</created><updated>2014-12-05T10:35:06Z</updated><resolved>2014-12-05T10:35:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="t-lo" created="2014-12-01T12:22:19Z" id="65057346">It seems to me that pull request #7598 would fix this issue - it introduces dependencies for both 'which' and the JRE required by elasticsearch to run. However it seems that #7598 has not been merged for almost three months?
</comment><comment author="grepwood" created="2014-12-01T13:15:22Z" id="65062766">I would advocate against introducing a package dependency on JRE/JDK. CentOS/RHEL/Fedora come with up to 3 different JDK packages: 1.6, 1.7 and 1.8.
In my case, elasticsearch works with both `java-1.7.0-openjdk` and `java-1.8.0-openjdk`. When both are installed, ES prefers the older version, which may not be desirable by some users.
</comment><comment author="t-lo" created="2014-12-01T13:21:22Z" id="65063371">Then why not depend on the desired version?
</comment><comment author="grepwood" created="2014-12-01T13:44:53Z" id="65065948">Yeah but which one? There's more than 1. This isn't something that a user can alter in the .rpm that will go to elasticsearch.org's download section.
</comment><comment author="t-lo" created="2014-12-01T14:18:23Z" id="65070074">Oh wait, I misread your previous comment:

&gt; In my case, elasticsearch works with both java-1.7.0-openjdk and java-1.8.0-openjdk.
&gt; When both are installed, ES prefers the older version, which may not be desirable by some users.

Imho this is not a package dependency issue. When multiple providers for java are installed the user would use `update-alternatives` to select a specific one system-wide. Alternatively one may use `$JAVA_HOME` to select a java VM for a specific process.

Also,  #7598 requires jre 1.7 _or better_, i.e. users using 1.8 or above are fine, too.
</comment><comment author="colings86" created="2014-12-05T10:35:06Z" id="65772590">#7598 has been merged so I will close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decay function and optional params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8624</link><project id="" key="" /><description>Hi!

Here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#_decay_functions

It says that `offset` and `decay` are optional params (offset=0 and decay=0.5). What about `origin` and `scale`? It says that `origin` is set to the current time if it is not given, so it is optional as well I guess?

What about `scale`? It does not blow up if not passed, so what value does it get by default? Is it also optional? :)
</description><key id="49872807">8624</key><summary>Decay function and optional params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">Linuus</reporter><labels /><created>2014-11-24T10:04:18Z</created><updated>2014-11-26T14:26:42Z</updated><resolved>2014-11-26T14:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:27:24Z" id="64392352">@brwe please take a look
</comment><comment author="Linuus" created="2014-11-26T10:30:31Z" id="64543751">Any updates here?
</comment><comment author="brwe" created="2014-11-26T14:18:19Z" id="64651313">&gt;  It says that origin is set to the current time if it is not given, so it is optional as well I guess?

`origin` is required for numeric and geo fields. Only date fields have a default origin which is `now`. I will update the documentation to make this more clear.

&gt; What about scale? It does not blow up if not passed, so what value does it get by default?

It should. Can you give an example where it does not?
</comment><comment author="Linuus" created="2014-11-26T14:26:41Z" id="64652481">@brwe Indeed, it does blow up without the `scale` parameter. I guess I did something weird last time.

Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[Docs] Fix missing new line</comment></comments></commit></commits></item><item><title>Enable aggregations for scripts that depend on nested and parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8623</link><project id="" key="" /><description>Hi,

I came across an interesting use case that does not seem to work currently. I have the following (simplified) schema:

``` json
{
        "properties": {
            "rates": {
                "type": "object"
            },

            "details": {
                "type": "nested",
                "properties": {
                    "amount": {
                        "type": "integer"
                    },
                                        "type": {
                                               "type": "string"
                                       }
                }
            }
        }
    }
```

rates is an object that contains various exchange rates for different currencies. The details property is nested because I need to do sums on the amount and filter by type.

What I want is, for each amount in the nested property, being able to write a script (in Groovy, Expression...) where the amount is multiplied by a rate:

``` json
{
            "aggs": {
                "details": {
                    "nested": {
                        "path": "details"
                    },

                    "aggs": {
                        "filter": {
                            "term": {"type": "a"}
                        },

                        "aggs": {
                            "sum_amount": {
                                "sum": {
                                    "script": "doc['details.amount'].value * doc['rates.usd'].value
                                }
                            }
                        }
                    }
                }
            }    
        }
```

This does not work however, because I cannot fetch the rates property (and returns 0). If I do a reverse nested, then I can (logically) access again the rates, the I loose access to the amount.

Thanks
</description><key id="49868372">8623</key><summary>Enable aggregations for scripts that depend on nested and parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bakura10</reporter><labels><label>discuss</label></labels><created>2014-11-24T09:06:23Z</created><updated>2015-11-21T21:37:41Z</updated><resolved>2015-11-21T21:37:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:25:58Z" id="64392208">Hi @bakura10 

Yes, this is indeed a problem. I've labelled this "discuss" but I can't see any easy solution to this given that the parent document and each nested object are separate documents.  The only things I can suggest are:
- access the `details.amount` field using the `_source` instead (which will be slow)
- store the `details.amount` value in each nested document as well  (my preferred solution)
</comment><comment author="bakura10" created="2014-11-25T12:44:38Z" id="64394110">Hi,

Thanks for your answer. The second solution is not really easy. The problem is that the rates contain up to 150 different currencies. Duplicating this info in each detail will be a bit messy :(.
</comment><comment author="clintongormley" created="2015-11-21T21:37:41Z" id="158683967">No further ideas during the last year. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>wrong error message in DateFieldMapper.parseStringValue(String value)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8622</link><project id="" key="" /><description>no use of a String pattern converter, but the error message give you a hint to use a format 
and not only long numbers.
</description><key id="49866558">8622</key><summary>wrong error message in DateFieldMapper.parseStringValue(String value)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">proofy</reporter><labels /><created>2014-11-24T08:38:22Z</created><updated>2014-11-24T12:47:32Z</updated><resolved>2014-11-24T12:47:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="proofy" created="2014-11-24T12:47:32Z" id="64189207">I can't reproduce it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>connection refused from localhost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8621</link><project id="" key="" /><description>Hi,

I am trying to implement a basic search feature in an angularjs framework. When I am running my server (and elasticsearch), I receive the error message:

ERR_CONNECTION_REFUSED
(anonymous function)angular.js:9501 sendReqangular.js:9218 serverRequestangular.js:12988      processQueueangular.js:13004 (anonymous function)angular.js:14204 Scope.$evalangular.js:14020 Scope.$digestangular.js:14308 Scope.$applyangular.js:20669 $$debounceViewValueCommitangular.js:20641 $setViewValueangular.js:19429 listenerjquery.js:4409 jQuery.event.dispatchjquery.js:4095 elemData.handle

Here is the code that I am using in my controller to make the request:

angular.module('canaryApp')
  .controller('SearchCtrl', function ($scope, $http) {
    var getRecipes = function(searchTerm) {
        $http.get('http://localhost:\9200/recipes/_search?q=' + searchTerm).
        success(function(data) {
            $scope.recipes = data.recipes;
            });
    };
    $scope.searchTerm = '';
    $scope.$watch('searchTerm', getRecipes, true);
});

Do you know what I might be doing wrong? I uncommented the security clause in the yaml file, but this did not address the error. Is there another config setting that I need to adjust?

Thanks so much for your help!
-Mars
</description><key id="49857682">8621</key><summary>connection refused from localhost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MarsWilliams</reporter><labels /><created>2014-11-24T05:31:17Z</created><updated>2014-11-25T12:19:38Z</updated><resolved>2014-11-25T12:19:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:19:38Z" id="64391596">Hi @MarsWilliams 

I'm not familiar with angular at all, but possibly you are running into a CORS issue?  CORS is disabled by default in 1.4.0.  See the CORS config options here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-http.html#_settings_2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Switch to forbidden-apis 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8620</link><project id="" key="" /><description>This pull request switches to forbidden-apis 1.7, see https://code.google.com/p/forbidden-apis/wiki/Changes for details. The main solved issue in addition to new Ant features and improved documentation is fixes in ignoring unresolvable signatures (like depecated Java 8 signatures did not work with Java 9, leading to errors in Lucene trunk)
</description><key id="49848069">8620</key><summary>Switch to forbidden-apis 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">uschindler</reporter><labels><label>build</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-24T01:10:51Z</created><updated>2014-11-24T09:14:28Z</updated><resolved>2014-11-24T08:59:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-24T08:59:09Z" id="64166158">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster unresponsive when ec2 node is shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8619</link><project id="" key="" /><description>I'm not sure if this is a configuration error, hoping someone can shed some light.

I have a cluster with 3 nodes, and when I shut down one ec2 instance the other nodes become unresponsive. I am running 3 c3.large AMI (2014.09) ec2 instances with a pretty standard Elasticsearch configuration. Java version is "1.7.0_71", and ES version is "1.4.0". I'm using version "2.4.0" of the aws cloud plugin for node discovery. Each instance is running one copy of Elasticsearch on port 9200/9300, and I am using nginx as a reverse proxy. The cluster works great, new nodes are added without any problem, the only problem is when a node is shutting down. For a minute or two while a node is shutting down the requests to any other node timeout.  Once the node shuts down completely, the other two nodes become responsive again and the cluster goes back online. 

**How to repeat.**
- launch 3 instances (instance1, instance2, instance3)
- when all 3 nodes are online and the cluster is active add some data to an index
- stop ec2 instance1 and run a `ab` against instance2
  
  ```
  ab -n 30000 -c 5 -A test:test "http://&lt;ip address of instance 2&gt;/twitter/tweet/1/"
  ```

The benchmarking will halt at some point while `instance1` is shutting down and print out the errors:

```
Completed 3000 requests
Completed 6000 requests
apr_socket_recv: Connection timed out (110)
apr_poll: The timeout specified has expired (70007)
```

This is also visisble when making web requests to any of the nodes. The request will not finish or the status of the cluster will be "503".

Here are the config files I am using:

**elasticseach**

```
# Additional Java OPTS
#ES_JAVA_OPTS=

# Maximum number of open files
MAX_OPEN_FILES=65535

# Maximum amount of locked memory
MAX_LOCKED_MEMORY=unlimited

# Maximum number of VMA (Virtual Memory Areas) a process can own
MAX_MAP_COUNT=262144

# Elasticsearch log directory
LOG_DIR=/var/log/elasticsearch

# Elasticsearch data directory
DATA_DIR=/tmp

# Elasticsearch work directory
WORK_DIR=/tmp/elasticsearch

# Elasticsearch conf directory
CONF_DIR=/etc/elasticsearch

# Elasticsearch configuration file (elasticsearch.yml)
CONF_FILE=/etc/elasticsearch/elasticsearch.yml

# User to run as, change this to a specific elasticsearch user if possible
# Also make sure, this user can write into the log directories in case you change them
# This setting only works for the init script, but has to be configured separately for systemd startup
ES_USER=elasticsearch

# Configure restart on package upgrade (true, every other setting will lead to not restarting)
RESTART_ON_UPGRADE=true
```

**elasticsearch.yml**

```
cluster.name: ls-elasticsearch
node.master: true
node.data: true
path.data: /tmp
plugin.mandatory: cloud-aws

bootstrap.mlockall: true
transport.tcp.port: 9300
http.port: 9200
http.max_content_length: 100mb
gateway.expected_nodes: 3

discovery.type: ec2
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.multicast.enabled: false

cloud.aws.access_key: &lt;removed&gt;
cloud.aws.secret_key: &lt;removed&gt;
cloud.aws.region: us-west-2

discovery.ec2.groups: ls-elasticsearch
discovery.ec2.host_type: private_ip

index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.trace: 500ms

index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.trace: 200ms

index.indexing.slowlog.threshold.index.warn: 10s
index.indexing.slowlog.threshold.index.info: 5s
index.indexing.slowlog.threshold.index.debug: 2s
index.indexing.slowlog.threshold.index.trace: 500ms

action.disable_delete_all_indices: true
```

The only errors I see in the logs of instance2, while instance1 is shutting down are connection timeout exceptions. I have the log set to debug.

```
[2014-11-23 21:59:11,869][TRACE][discovery.zen.ping.unicast] [search5] [1] failed to connect to [#zen_unicast_6_#cloud-i-793f208d-0#][search5][inet[/10.27.134.11:9300]]
org.elasticsearch.transport.ConnectTransportException: [][inet[/10.27.134.11:9300]] connect_timeout[30s]
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:772)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:737)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:709)
        at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:154)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:372)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: /10.27.134.11:9300
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        ... 3 more
```

Does anybody see any issues here?
</description><key id="49845826">8619</key><summary>Cluster unresponsive when ec2 node is shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ChrisZieba</reporter><labels /><created>2014-11-24T00:05:22Z</created><updated>2014-12-03T22:34:52Z</updated><resolved>2014-11-25T07:05:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ChrisZieba" created="2014-11-25T07:05:04Z" id="64318448">Setting the instance `network.bind_host` to the private IP resolved this.
</comment><comment author="nariman-haghighi" created="2014-12-03T22:34:52Z" id="65504138">Hi ChrisZieba

We've started to notice the same in the last few weeks (running on Azure, long-standing setup that's been very stable). Sometime during the 1.4.0/1.4.1 upgrades, we started to notice the exact same phenomenon on 2-node cluster (when node 0 would shut-down, node 1 became unresponsive for 30-60 seconds). We are using unicast discovery where hosts specifies the private IP of both nodes. I'm curious if you think explicitly setting network.bind_host to the private IP of the node will resolve this and if so why?

Regards,
N.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fresh setup under virtualbox: replicas not allocated to new nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8618</link><project id="" key="" /><description>Hello,

I have a fresh setup inside a virtual box. I'm starting a master node and creating
a 1/1 index. The master now holds the primary shard but replica shard is now marked as unassigned. Bringing up another node makes it join the cluster but the replica is not moved there.

My setup is pretty much default. I thought it might be because of auto assigned IPs
so I changed `network.publish_host` to all the IPs available with `ifconfig` 
(except for loopbacks). In every case it worked yet the replica was not moved to the
new node and stayed unassigned.

I'm out of ideas, so can please someone help in any way?

Thanks a lot :)
</description><key id="49829449">8618</key><summary>Fresh setup under virtualbox: replicas not allocated to new nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dmitrybelyakov</reporter><labels /><created>2014-11-23T16:02:14Z</created><updated>2014-11-23T17:25:50Z</updated><resolved>2014-11-23T17:05:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-11-23T16:21:13Z" id="64123445">You might be running out of disk space. After 85% usage by default, replicas won't be assigned.
</comment><comment author="dmitrybelyakov" created="2014-11-23T16:55:13Z" id="64124808">@dadoonet Thank you for your response! Can this be configured somehow or turned off?
</comment><comment author="dadoonet" created="2014-11-23T16:59:43Z" id="64125001">See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#disk
</comment><comment author="dmitrybelyakov" created="2014-11-23T17:05:29Z" id="64125213">@dadoonet That really did it :) I was struggling with it for the second day in a row. I can now happily continue with the book. Thanks a lot and have a great day !
</comment><comment author="dadoonet" created="2014-11-23T17:21:36Z" id="64125913">You are welcomed but please prefer the mailing list if you have questions.
</comment><comment author="dmitrybelyakov" created="2014-11-23T17:25:50Z" id="64126047">Sure, sorry for that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> Memory leak related issue with Threadlocals in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8617</link><project id="" key="" /><description> have a web service, where every time a button is pressed from UI, it connects to elastic search and fires a query. This is the code which is executed every time. The issue is that after a while, intermittently, the UI hangs.

private static final String CONFIG_CLUSTER_NAME = "cluster.name";

private static final String CLUSTER_NAME = "sample_es";
private static final String[] transportAddress = {
    //Machine details
};

private static final int transportPort = 9300;

```
    public static Client initClient(){
    settings = ImmutableSettings.settingsBuilder().put(CONFIG_CLUSTER_NAME, CLUSTER_NAME).build();

    Client client = new TransportClient(settings);
    for (int i=0 ; i &lt; transportAddress.length-1 ; i++){
        ((TransportClient)client).addTransportAddress(new InetSocketTransportAddress(transportAddress[i], transportPort));
    }
    logger.info("TransportClient Created"); 
    return client;  
```

}

public static int query( String query) throws Exception
{
    Client client = null;
    try{

```
        client = initClient();

        //Query search code

}catch(Exception e){
        e.printStackTrace();
}finally{
    if(client != null){
        client.close();
        logger.info("TransportClient Closed");
    }
}
    return result_count;
```

}

Whenever we restart the tomcat server, this is the error message we are seeing in logs. How should we fix this?

[org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom$1](value
[org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom$1@5838ce3e]) and a value of type
[org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom](value 
[org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom@796c75b1]) but failed to remove it
when the web application was stopped. This is very likely to create a memory leak.
</description><key id="49828780">8617</key><summary> Memory leak related issue with Threadlocals in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prachicsa</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-11-23T15:39:51Z</created><updated>2015-08-29T11:36:59Z</updated><resolved>2015-08-29T11:36:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T12:12:43Z" id="64390937">Duplicate of #283
</comment><comment author="clintongormley" created="2015-08-29T11:36:58Z" id="135973924">Closing in favour of #283
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregation woes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8616</link><project id="" key="" /><description>Here is the mapping of my index - https://gist.github.com/NodexTech/02c8b95eef3abc3f1a44
Please note the "facets" object and that the fields inside it are NOT analyzed and have doc_values:true which should give me values such as non case folded or tokenized bucket values. This works as expected UNTIL an aggregation (ranged in this case) is added to the query at which point the return reverts back to retrieving the bucket values from the index and NOT the document.

Here is the query - https://gist.github.com/NodexTech/a673df667fed9255898f

Sometimes the query fails and returns NO aggregations for "contract_type" (arbitrary field) and sometimes it gives the right answers which leads me to believe it's cache related as I can change the date on the query (i/e go back 3 months) and it will work as expected.

It's very odd behavior, I am fairly new to ES and I am surprised that this would be a bug and not found by unit testing etc but I picked it up in a matter of using ES for less than a week so it might have slipped someone by!
</description><key id="49826630">8616</key><summary>Aggregation woes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NodexTech</reporter><labels><label>feedback_needed</label></labels><created>2014-11-23T14:10:04Z</created><updated>2014-12-31T16:52:56Z</updated><resolved>2014-12-31T16:52:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T09:42:27Z" id="64170308">Hi @NodexTech 

We need more info here, eg what docs are you indexing, what do you consider to be wrong or right results.  You have a number of issues, eg 
- you have `fitler` instead of `filter` in your `lowercase_search` definition.  
- using a `keyword` tokenizer at search time for fields that are tokenized with the `standard` tokenizer will almost never work.
- your `doc_values` will store the original value of the field, not tokenized and not case-folded

Could you put together a small recreation of what it is you're seeing?
</comment><comment author="clintongormley" created="2014-12-31T16:52:56Z" id="68453788">No more info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch fails when paths contain spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8615</link><project id="" key="" /><description>Currently Elasticsearch will fail to start if the install path contains spaces, with:

```
You must set the ES_CLASSPATH var
```

We need to fix these errors and add tests
</description><key id="49826603">8615</key><summary>Elasticsearch fails when paths contain spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-11-23T14:08:52Z</created><updated>2014-12-01T18:30:32Z</updated><resolved>2014-12-01T18:19:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="t-lo" created="2014-12-01T15:37:58Z" id="65081980">It does? On which platform? Works for me on Linux (tested with both bash and dash).
`user@host $ /tmp/es\ with\ spaces/bin/elasticsearch` starts w/o problems, and
`user@host $ "/tmp/es with spaces/bin/elasticsearch"` works fine, too.
</comment><comment author="clintongormley" created="2014-12-01T18:19:02Z" id="65109384">@t-lo Hmm - just tried again with the zip file and it worked just fine in 1.4.0 and master.  Not sure what happened before.

thanks. closing
</comment><comment author="t-lo" created="2014-12-01T18:30:32Z" id="65111216">Maybe it would be good future practice to include commit hashes in issues (especially bug issues) to ease triaging?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Strange terms on field with same name on different types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8614</link><project id="" key="" /><description>Using ES 1.4.0. 

Problem description (details to follow): A field with the same name is defined under two separate types in the same index. Once as a string, the other as a long. The values in this field are always numbers. Doing a terms aggregation on this field shows strange values. 

Steps to reproduce:

Create a type with a string field:

```
PUT rotem-test/_mapping/t1
{
  "t1": {
    "_all": {
      "enabled": false
    },
    "properties": {
      "time.total": {
        "type": "string",
        "index": "not_analyzed"
      }
    }
  }
}
```

Index some objects with _numeric_ values.

```
POST rotem-test/t1/2
{
  "time.total": "26"
}

POST rotem-test/t1/3
{
  "time.total": 10
}
```

Create another type with the same field as long

```
PUT rotem-test/_mapping/t2
{
  "t2": {
    "_all": {
      "enabled": false
    },
    "properties": {
      "time.total": {
        "type": "long"
      }
    }
  }
}
```

Create objects in the second type

```
POST rotem-test/t2/2
{
  "time.total": 10
}

POST rotem-test/t2/3
{
  "time.total": "5"
}

POST rotem-test/t2/4
{
  "time.total": 5
}

```

Do a terms agg on this field in the second type.

```
POST rotem-test/t2/_search?search_type=count
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "terms": {
      "terms": {
        "field": "time.total"
      }
    }
  }
}
```

This is what you get:

```
{
 "took" : 38,
 "timed_out" : false,
 "_shards" : {
  "total" : 4,
  "successful" : 4,
  "failed" : 0
 },
 "hits" : {
  "total" : 6,
  "max_score" : 0.0,
  "hits" : []
 },
 "aggregations" : {
  "terms" : {
   "doc_count_error_upper_bound" : 0,
   "sum_other_doc_count" : 0,
   "buckets" : [{
     "key" : "0 \u0000\u0000\u0000\u0000\u0000\u0000",
     "doc_count" : 6
    }, {
     "key" : "@\b\u0000\u0000\u0000\u0000",
     "doc_count" : 6
    }, {
     "key" : "P\u0002\u0000\u0000",
     "doc_count" : 6
    }, {
     "key" : " \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0005",
     "doc_count" : 4
    }, {
     "key" : " \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
     "doc_count" : 2
    }
   ]
  }
 }
}

```
</description><key id="49824625">8614</key><summary>Strange terms on field with same name on different types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rore</reporter><labels /><created>2014-11-23T12:59:59Z</created><updated>2014-11-25T10:50:39Z</updated><resolved>2014-11-25T10:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-25T10:20:10Z" id="64373260">Hi @rore 

You are running into a known problem: fields with the same name in different types must have the same mapping.  We are planning on enforcing this in 2.0 with #4081
</comment><comment author="rore" created="2014-11-25T10:50:39Z" id="64382230">Hi @clintongormley 

This is quite a worrying remark. Enforcing the same mapping on fields with the same name under different types is quite breaking the understanding of how the mapping is encapsulated under a type. We are counting on the type separation to allow us to have different mapping for fields with the same name. What you are saying destroys one of the main reasons for having different types under an index. This is going to have serious implications. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Treatment of special characters in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8613</link><project id="" key="" /><description>I use the following analyzer:

curl -XPUT 'http://localhost:9200/sample/' -d '
{
  "settings" : {
  "index": {
    "analysis": {
      "analyzer": {
        "default": {
         "type": "custom",
         "tokenizer": "keyword",
         "filter": ["trim", "lowercase"]}
      }
    }
  }
  }
}'

Then when I try to insert some documents which contain special characters like % and etc, it converts in to hex.

1%2fPJJP3JV2C24iDfEu9XpHBaYxXh%2fdHTbmchB35SDznXO2g8Vz4D7GTIvY54iMiX_149c95f02a8 -&gt; actual value

1&amp;#x25;2fPJJP3JV2C24iDfEu9XpHBaYxXh&amp;#x25;2fdHTbmchB35SDznXO2g8Vz4D7GTIvY54iMiX&amp;#x5f;149c95f02a8 

-&gt; stored value.

Sample:

curl -XPUT 'http://localhost:9200/sample/strom/1' -d '{
    "user" : "user1",
    "message" : "1%2fPJJP3JV2C24iDfEu9XpHBaYxXh%2fdHTbmchB35SDznXO2g8Vz4D7GTIvY54iMiX_149c95f02a8"
}'

The problem started occurring only once the data crossed some million documents. Earlier it used store it as it is.

Now if I try to search using,

1%2fPJJP3JV2C24iDfEu9XpHBaYxXh%2fdHTbmchB35SDznXO2g8Vz4D7GTIvY54iMiX_149c95f02a8

it is not able to retrieve the document. How do I deal with this? The behavior seems to non-deterministic in converting special character to hex.

I am unable to replicate the same issue on localmachine.

Can someone explain the mistake I am making?
</description><key id="49813266">8613</key><summary>Treatment of special characters in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prachicsa</reporter><labels /><created>2014-11-23T03:33:43Z</created><updated>2014-11-24T20:20:41Z</updated><resolved>2014-11-24T20:20:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T20:20:41Z" id="64257396">Hi @prachicsa 

Elasticsearch doesn't do any hex en/decoding in the JSON body.  I don't know what you're doing wrong, and you haven't provided all the steps that you are using to demonstrate this problem.

I suggest trying the `explain` API to figure out why your document isn't matching: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-explain.html#search-explain

If you can recreate a bug, please reopen this ticket.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms Aggregation: Handle a list of order options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8612</link><project id="" key="" /><description>It would be nice if you could use a list of ordering options instead of a single order.  This is useful for when you need a secondary sort because the first one is of low cardinality.

Doesn't seem too hard to add after looking at the code.  `InternalOrder` could be extended with with a class called `InternalOrderChain` that can take a list of `InternalOrder` objects in it's constructor.  The `comparator` could use Guava's `Ordering.compound`:

```
    protected Comparator&lt;Bucket&gt; comparator(final Aggregator aggregator) {
        return Ordering.compound(FluentIterable
                .from(orders)
                .transform(new Function&lt;InternalOrder, Comparator&lt;? super Bucket&gt;&gt;() {
                    @Override public Comparator&lt;? super Bucket&gt; apply(InternalOrder input) {
                        return input.comparator(aggregator);
                    }
                })
                .toList()
        );
    }
```

The only thing that's a bit strange is the `InternalOrder.Streams` handling and any check to see if the `InternalOrder` is an `Aggregation`...
</description><key id="49811628">8612</key><summary>Terms Aggregation: Handle a list of order options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mtraynham</reporter><labels /><created>2014-11-23T02:07:44Z</created><updated>2014-11-23T03:04:32Z</updated><resolved>2014-11-23T03:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mtraynham" created="2014-11-23T03:04:32Z" id="64104786">Closing in favor of #6917
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cut over to Path API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8611</link><project id="" key="" /><description>This commit also refactors `RAFReference` to work on top of a Channel instead of fetching the channel from the RandomAccessFile. The channel is created to truncate the file it opens too. This likely needs some documentation or we should allow the caller to pass the open options but I wanted to get this change out for review.
</description><key id="49807527">8611</key><summary>Cut over to Path API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-22T22:56:13Z</created><updated>2015-06-07T11:55:40Z</updated><resolved>2014-11-25T11:45:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-22T22:56:59Z" id="64098759">@dakrone I assigned you here but I'd love @rmuir to look at this too...
</comment><comment author="dakrone" created="2014-11-24T09:04:35Z" id="64166707">@s1monw left a couple of comments but other than that it looks good!
</comment><comment author="dakrone" created="2014-11-24T10:55:36Z" id="64177676">LGTM
</comment><comment author="dakrone" created="2014-11-24T11:09:51Z" id="64179040">LGTM (still :) )
</comment><comment author="s1monw" created="2014-11-24T15:06:50Z" id="64206201">@dakrone I had to do another round of unfucking here sorry - can you take another look?
</comment><comment author="dakrone" created="2014-11-25T11:34:46Z" id="64387135">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/CloseableIndexComponent.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexGateway.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/MergePolicyProvider.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/store/IndexStore.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/store/support/AbstractIndexStore.java</file><file>src/main/java/org/elasticsearch/index/translog/ChecksummedTranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/LegacyTranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/Translog.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStream.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogStreams.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/BufferingFsTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/ChannelReference.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsChannelSnapshot.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslogFile.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/SimpleFsTranslogFile.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>src/main/java/org/elasticsearch/plugins/AbstractPlugin.java</file><file>src/main/java/org/elasticsearch/plugins/Plugin.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/translog/AbstractSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/TranslogVersionTests.java</file><file>src/test/java/org/elasticsearch/index/translog/fs/FsBufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/fs/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file></files><comments><comment>[TRANSLOG] Cut over to Path API</comment></comments></commit></commits></item><item><title>Remove _state directory if index has been deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8610</link><project id="" key="" /><description>Today we try to delete the index directory if all shard locks have been
acquired. Yet, if this fails due to still running recoveries etc. We might
re-import the index as dangeling which also can happen if the node is restarted.
In contrast to the shard direcotries we can safely delete the metastate which is used
to import dangling indices while leaving the shard directories untouched.
</description><key id="49804366">8610</key><summary>Remove _state directory if index has been deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-22T20:58:06Z</created><updated>2015-06-07T18:02:46Z</updated><resolved>2014-11-24T15:05:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-24T11:34:31Z" id="64181328">Left really minor comments but LGTM
</comment><comment author="s1monw" created="2014-11-24T13:48:48Z" id="64195618">@dakrone can you take another look?
</comment><comment author="dakrone" created="2014-11-24T14:32:32Z" id="64201198">LGTM, thanks for making the changes!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cut over MetaDataStateFormat to Path API in Gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8609</link><project id="" key="" /><description /><key id="49803938">8609</key><summary>Cut over MetaDataStateFormat to Path API in Gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-22T20:40:55Z</created><updated>2015-06-07T11:55:59Z</updated><resolved>2014-11-25T14:12:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-23T17:35:02Z" id="64126402">looks good to me.
</comment><comment author="s1monw" created="2014-11-24T09:49:09Z" id="64170926">@rmuir pushed another round can you look?
</comment><comment author="s1monw" created="2014-11-25T13:39:25Z" id="64400330">@rmuir pushed a new commit removeing all File references from this test
</comment><comment author="rmuir" created="2014-11-25T14:05:17Z" id="64403608">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file><file>src/test/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormatTest.java</file></files><comments><comment>[GATEWAY] Cut over MetaDataStateFormat to Path API</comment></comments></commit></commits></item><item><title>[CORE] Wait for pending shard removal on IndexService close</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8608</link><project id="" key="" /><description>When an index service gets closed due to a delete index event it's possible
that shards are already removed from the service that are currently in
recovery. The recovery source applies the delete index first and causes
the target to fail its shard. This shard gets removed from the service
but it's content doesn't get deleted if the actual delete index is faster
applied. This commit makes sure we wait for the pending removals that
are in-flight before we close the index an delete it's content too once
the lock is released.
</description><key id="49802458">8608</key><summary>[CORE] Wait for pending shard removal on IndexService close</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2014-11-22T19:43:55Z</created><updated>2014-12-29T15:18:04Z</updated><resolved>2014-12-29T15:17:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-24T15:11:50Z" id="64207018">@bleskes if you have time I'd love you to review this - assinging you
</comment><comment author="bleskes" created="2014-11-27T14:55:35Z" id="64799401">I looked at it and my main concern is complexity. @s1monw had some ideas about simplifying it and is working on it.
</comment><comment author="s1monw" created="2014-12-29T15:17:53Z" id="68265493">closing in favor of #9083
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[SPEC] Fixes a few locations of the params object in JSON specification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8607</link><project id="" key="" /><description>Moves the `params` block in a couple of JSON specification files, they were not located in the `url` block.
</description><key id="49793937">8607</key><summary>[SPEC] Fixes a few locations of the params object in JSON specification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justahero</reporter><labels /><created>2014-11-22T15:04:15Z</created><updated>2014-12-03T13:11:14Z</updated><resolved>2014-11-24T19:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T19:46:02Z" id="64251861">Thanks @justahero - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Spec: Fixes a few locations of the params array in JSON specification</comment></comments></commit></commits></item><item><title>Fixes description of thor commands in Readme</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8606</link><project id="" key="" /><description>Fixes the description of how to generate JSON API files and matching ruby files.
- adjusts default option of `input` argument to match existing folder
- fixes description of thor usage.
</description><key id="49791927">8606</key><summary>Fixes description of thor commands in Readme</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/karmi/following{/other_user}', u'events_url': u'https://api.github.com/users/karmi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/karmi/orgs', u'url': u'https://api.github.com/users/karmi', u'gists_url': u'https://api.github.com/users/karmi/gists{/gist_id}', u'html_url': u'https://github.com/karmi', u'subscriptions_url': u'https://api.github.com/users/karmi/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4790?v=4', u'repos_url': u'https://api.github.com/users/karmi/repos', u'received_events_url': u'https://api.github.com/users/karmi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/karmi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'karmi', u'type': u'User', u'id': 4790, u'followers_url': u'https://api.github.com/users/karmi/followers'}</assignee><reporter username="">justahero</reporter><labels /><created>2014-11-22T13:58:47Z</created><updated>2014-12-03T13:11:12Z</updated><resolved>2014-11-25T14:34:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2014-11-25T14:35:01Z" id="64407744">Hello, thanks for the interest in the generators! :) I have migrated them to the [elasticsearch-ruby](https://github.com/elasticsearch/elasticsearch-ruby) repository, where it makes more sense now.

Please check out https://github.com/elasticsearch/elasticsearch-ruby/tree/master/elasticsearch-api/utils
</comment><comment author="justahero" created="2014-11-25T14:57:55Z" id="64411415">Thx, that's a good idea.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[SPEC] Removed the utilities for generating REST specs/code</comment></comments></commit></commits></item><item><title>Fix error in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8605</link><project id="" key="" /><description>Indexation does not fail if no timestamp provided when there is a default value defined in mapping.
</description><key id="49767368">8605</key><summary>Fix error in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">A21z</reporter><labels><label>docs</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-22T01:23:46Z</created><updated>2014-11-23T13:06:15Z</updated><resolved>2014-11-23T13:06:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-22T16:13:45Z" id="64085241">can you sign the CLA so we can pull it in? --&gt; http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="A21z" created="2014-11-22T18:59:31Z" id="64091173">done
</comment><comment author="s1monw" created="2014-11-23T13:06:15Z" id="64117323">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Need the ability to limit the number of terms in phrase queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8604</link><project id="" key="" /><description>Phrase queries with hundreds and hundreds of terms can take a ton of power to run.  It'd be nice to be able to reject these queries.  Example:

``` bash
#!/bin/bash

export TEXT="0 "
for i in $(seq 1 10); do
  export TEXT=$(echo "$TEXT $TEXT")
done
curl -XDELETE "http://localhost:9200/phrase_crush"
curl -XPUT "http://localhost:9200/phrase_crush"
for i in $(seq 1 2000); do
  printf "%5d  " $i
  curl -XPOST "http://localhost:9200/phrase_crush/doc" -d '{"text": "'"$TEXT$TEXT"'"}'
  echo
done
time curl -s -XPOST "http://localhost:9200/phrase_crush/_search?pretty" -d '{
  "query": {
    "query_string": {
      "query": "\"'"$TEXT"'\"",
      "default_field": "text"
    }
  }
}' | head
```
</description><key id="49756948">8604</key><summary>Need the ability to limit the number of terms in phrase queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2014-11-21T22:38:36Z</created><updated>2015-05-17T21:58:31Z</updated><resolved>2014-12-02T14:07:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T19:18:37Z" id="64247483">I've labelled this "discuss", but my feeling is that you shouldn't expose the query string query to anybody who isn't trusted anyway, as there are so many ways to hurt your cluster.
</comment><comment author="nik9000" created="2014-11-24T20:04:51Z" id="64254935">I hate to be whiny about this one but its a bit important to me and I'd like to get it fixed soon.  I'm probably going to fix this in a plugin locally and temporarily and then port that to a pull request.

I'm thinking of limiting the number of positions allowed in phrase queries and adding two options:

`max_positions_per_phrase_query`: the maximum number of positions allowed in phrase queries

`phrase_too_large_action`: what to do with phrase queries that are too large.  Options are `throw_exception` and `convert_to_term_queries`.

Does that sound sensible?
</comment><comment author="nik9000" created="2014-11-24T20:22:26Z" id="64257633">&gt; I've labelled this "discuss", but my feeling is that you shouldn't expose the query string query to anybody who isn't trusted anyway, as there are so many ways to hurt your cluster.

The problem is the same with the match query:

``` bash
time curl -s -XPOST "http://localhost:9200/phrase_crush/_search?pretty" -d '{
  "query": {
    "match_phrase": {
      "text": "'"$TEXT"'"
    }
  }
}' | head
```

This takes just as long.

I renamed the issue - it effects all ways of making phrase queries.
</comment><comment author="nik9000" created="2014-11-25T20:44:14Z" id="64467684">Just realized I had to change from `max_positions_per_phrase_query` to `max_terms_per_phrase_query`.  They amount to the same thing for regular phrase queries but for multiphrase queries they the cost looks to be per temr not per position.
</comment><comment author="nik9000" created="2014-11-25T21:51:21Z" id="64477342">My proposal in plugin patch set form:
https://gerrit.wikimedia.org/r/#/c/175853

It only covers `query_string` because that's all I need.  If I ported it to Elasticsearch core I'd do `match_phrase`, `match_phrase_prefix`, and `simple_query_string` as well.
</comment><comment author="nik9000" created="2014-11-26T22:29:48Z" id="64720082">Ignore that patch for now.  I think I might just do this in a plugin entirely with a reasonable degree of safety.
</comment><comment author="clintongormley" created="2014-11-27T09:30:09Z" id="64765497">Hi @nik9000 

The reason I'm feeling edgy about adding these options is that these are only some of the ways that a query can blow up.  We would then need to add 50 other options to control other things that could go wrong.  I'd much rather have generic solutions that solves the  problem for any too-heavy query, eg the request circuit breakers that @dakrone has added, and the generic timeout handler proposed by @markharwood (https://github.com/elasticsearch/elasticsearch/pull/4586).

Although there hasn't been any movement on that PR for a while, it has not been forgotten. Currently, the code for checking timeouts is too invasive, meaning that it will have a performance impact esp on aggregations (and esp with doc values).  We still need to figure out the right way of solving this.
</comment><comment author="nik9000" created="2014-12-02T14:07:46Z" id="65235042">I'll do this in a plugin.  I can't wait for better timeouts and circuit breakers don't catch this.  The plugin analyzes wraps a query and statically analyzes it then degrades or fails based on the number of phrase clauses.  Its pretty simple and fast but it sure isn't fool proof.  Timeouts would be better.

Anyway, its proposed [here](https://gerrit.wikimedia.org/r/#/c/175853) and will be released in a few days I imagine.

If it takes 50 options to plug all the holes we find before timeouts are ready then it takes 50 options.
</comment><comment author="nemobis" created="2015-05-17T18:39:08Z" id="102833891">@nik9000 is there any way to have that available for elastica library/extension users like ttmserver?
</comment><comment author="nik9000" created="2015-05-17T21:58:31Z" id="102863428">Hey Nemo, yeah. The plugin is publicly released and installed on the wmf
cluster. You just have to use it.

For those following along its the wikimedia extra plugin. We're talking
about the `safer` query.
On May 17, 2015 2:39 PM, "nemobis" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 is there any way to have that
&gt; available for elastica library/extension users like ttmserver?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/8604#issuecomment-102833891
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Implement default and more granular logging on deleted files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8603</link><project id="" key="" /><description>We have seen various occurrences of shard initialization failures due to file not found exceptions (potentially corrupted indices), eg. 

```
[2014-11-19 19:59:06,126][WARN ][index.store              ] [node_name] [index_name][3] Can't open file to read checksums
java.io.FileNotFoundException: No such file [_ym1.fdt]
    at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:176)
    at org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:144)
    at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:542)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:480)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:456)
    at org.elasticsearch.index.store.Store.getMetadata(Store.java:154)
    at org.elasticsearch.index.store.Store.getMetadata(Store.java:143)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:728)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:580)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:184)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:444)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
```

Example other files affected:
java.io.FileNotFoundException: No such file [_ym1_es090_0.blm]
java.io.FileNotFoundException: No such file [_zrj_Lucene49_0.dvd]
java.io.FileNotFoundException: No such file [_yie_es090_0.pos]

The default logging does not indicate when files are deleted by Lucene or Elasticsearch.  Without this information, it will be difficult to speculate on whether the missing file exceptions are due to intentional deletes by Lucene or ES, or if there is a potential file system issue.

@rmuir mentioned that Lucene already provides a way to get "info stream" of deleted files
but logging for this is not enabled by default in ES and also not as granular/targeted as desired for debugging this type of situation.

So this is a request to improve our default logging on deleted files, eg.
- Provide a way to tap into Lucene's info stream specific to the deletion of files
- Provide a similar logging facility for the ES side of things when we delete non-Lucene managed files like .blm, etc..
</description><key id="49754103">8603</key><summary>Implement default and more granular logging on deleted files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label></labels><created>2014-11-21T22:05:12Z</created><updated>2014-11-26T13:16:37Z</updated><resolved>2014-11-26T10:10:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-21T22:12:26Z" id="64046327">one idea i had here was to configure more fine-grained integration with infostream.

rather than just setting `lucene.iw` to TRACE, maybe if we could set `lucene.iw.ifd`, then it would filter by "ifd" infostream component. this would give us the ability to have a default that tells us the important stuff (maybe deleting files and merges, for example).
</comment><comment author="mikemccand" created="2014-11-21T22:48:15Z" id="64050360">&gt; one idea i had here was to configure more fine-grained integration with infostream.

+1, that's a nice idea.
</comment><comment author="mikemccand" created="2014-11-25T10:36:24Z" id="64380329">Work in progress: https://github.com/mikemccand/elasticsearch/tree/log_file_deletes

I think ideally for this to be effective as a "why is this index file missing on corruption", we need to have file deleting logged by default?  This may be too noisy though...

As a start, I broke out a separate "lucene.iw.ifd", but then I'm not sure how to handle other places in ES that delete files.  E.g. I added a "logger.info" in RecoveryTarget.CleanFilesRequestHandler, but then you can't easily turn it off w/o turning off all INFO level logging for RecoveryTarget.

Maybe we should be logging down inside Directory?  E.g. we'd wrap the shard's Directory instance, and in its deleteFile, go log the delete to a new logger?  And I suppose also fix all file deletion outside of Lucene's Directory (i.e. direct calls to Files.delete) to tell this new logger as well...
</comment><comment author="mikemccand" created="2014-11-25T11:02:14Z" id="64383803">Related: I fixed a couple places in Lucene (IndexWriter) that Rob found, which were logging unnecessarily and causing tons of output, at least in the "many shards" case ... we'll get that on next Lucene upgrade.
</comment><comment author="rmuir" created="2014-11-26T12:41:16Z" id="64598683">&gt; Maybe we should be logging down inside Directory? E.g. we'd wrap the shard's Directory instance, and in its deleteFile, go log the delete to a new logger? And I suppose also fix all file deletion outside of Lucene's Directory (i.e. direct calls to Files.delete) to tell this new logger as well...

is it useful to just log that a file was deleted? If you are debugging a missing file, you dont need a log file to tell you this!!!! its having the right amount of context around _why_ it was deleted that is important. 
</comment><comment author="mikemccand" created="2014-11-26T13:16:37Z" id="64602478">&gt; is it useful to just log that a file was deleted? 

I think it is useful to know that it was Directory.deleteFile that did the deleting, vs. e.g. Files.delete elsewhere, or vs. something outside of ES doing the deletions?

We do also get context from lucene.iw.ifd (it was Lucene that deleted), and 2 places in ES that seemed to delete Lucene index files (I added separate logging).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/LoggerInfoStream.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file></files><comments><comment>Core: separately log file deletions</comment></comments></commit></commits></item><item><title>Provide option to allow dynamic creation of fields but not if it is one with different casing in the name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8602</link><project id="" key="" /><description>We currently have a dynamic strict setting that prevents the automatic creation of fields within a type.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-dynamic-mapping.html#mapping-dynamic-mapping

Would be nice to provide an additional option which allows for the dynamic creation of fields within a type, but disallow dynamic creation of the same field but with a different casing (eg. "body" vs. "Body")
</description><key id="49746951">8602</key><summary>Provide option to allow dynamic creation of fields but not if it is one with different casing in the name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-11-21T20:52:39Z</created><updated>2014-11-24T19:11:26Z</updated><resolved>2014-11-24T19:11:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-21T20:54:35Z" id="64036067">body_text vs bodyText vs BodyText too?

On Fri, Nov 21, 2014 at 3:52 PM, Pius notifications@github.com wrote:

&gt; We currently have a dynamic strict setting that prevents the automatic
&gt; creation of fields within a type.
&gt; 
&gt; http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-dynamic-mapping.html#mapping-dynamic-mapping
&gt; 
&gt; Would be nice to provide an additional option which allows for the dynamic
&gt; creation of fields within a type, but disallow dynamic creation of the same
&gt; field but with a different casing (eg. "body" vs. "Body")
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8602.
</comment><comment author="ppf2" created="2014-11-21T20:58:53Z" id="64036654">Yah not quite sure about the body_text scenario, but for bodytext,Bodytext,bodyText,BodyText, certainly for this use case. Thx.
</comment><comment author="clintongormley" created="2014-11-24T19:11:26Z" id="64246352">Meh I don't like this at all.  It introduces so many problems elsewhere.  Field names are case sensitive, and that's that.  Problems like this should be handled on the application side.

Sorry, but I'm closing this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added note that ES packages automatically change vm.max_map_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8601</link><project id="" key="" /><description /><key id="49745820">8601</key><summary>Added note that ES packages automatically change vm.max_map_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">tristanbob</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-21T20:41:10Z</created><updated>2014-11-25T17:26:02Z</updated><resolved>2014-11-25T17:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T19:08:37Z" id="64245912">Hi @tristanbob 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="tristanbob" created="2014-11-25T02:13:01Z" id="64299936">Done.  Thanks!

On Mon, Nov 24, 2014 at 12:09 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @tristanbob https://github.com/tristanbob
&gt; 
&gt; Thanks for the PR. Please could I ask you to sign the CLA so that I can
&gt; merge this in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; thanks
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/8601#issuecomment-64245912
&gt; .

## 

Tristan Rhodes
</comment><comment author="clintongormley" created="2014-11-25T17:26:01Z" id="64437523">thanks @tristanbob - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Added note that ES packages automatically change vm.max_map_count</comment></comments></commit></commits></item><item><title>Packaging: Added a command to start elasticsearch on bootup.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8600</link><project id="" key="" /><description /><key id="49742130">8600</key><summary>Packaging: Added a command to start elasticsearch on bootup.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tristanbob</reporter><labels><label>:Packaging</label><label>docs</label><label>v1.3.7</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T20:03:48Z</created><updated>2014-12-16T11:32:33Z</updated><resolved>2014-12-05T10:18:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T19:07:02Z" id="64245667">Is this command true for all debian and ubuntu systems? Or does it depend on which startup system is in use?
</comment><comment author="clintongormley" created="2014-11-24T19:07:36Z" id="64245755">Also, if we're going to add these details for the apt package, would be good to do so for the RPMs as well.
</comment><comment author="tristanbob" created="2014-11-25T02:08:55Z" id="64299614">Yes, this command should work for all Debian based systems.

If the elasticsearch package maintainer changes the init script to
something else (like Upstart) then this would have to be re-evaluated.
With the current packages, this command is explicitly listed in the
installation output.

I am not sure of the correct commands for RPM distros, but this change is
under the APT section.

On Mon, Nov 24, 2014 at 12:08 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Also, if we're going to add these details for the apt package, would be
&gt; good to do so for the RPMs as well.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/8600#issuecomment-64245755
&gt; .

## 

Tristan Rhodes
</comment><comment author="t-lo" created="2014-12-02T09:29:34Z" id="65203386">`update-rc.d` is specific to Debian / SystemV init. For the upcoming Debian 8 ("Jessie") - which will use systemd init - the corresponding command is `systemctl enable elasticsearch.service`.

For Red Hat / SysV init (deprecated, last used in RHEL 5 - RHEL 6 uses upstart, RHEL 7 uses systemd) the corresponding command would be `chkconfig --add elasticsearch`.

IOW the documentation addendum for Apt / Debian systems looks good :)
</comment><comment author="jpountz" created="2014-12-05T10:19:54Z" id="65770964">@tristanbob Merged thanks!
@t-lo Thanks for the note, I added some information about doing the same on RHEL/Fedora.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Added a command to start elasticsearch on bootup on Debian.</comment></comments></commit></commits></item><item><title>Use Lucene checksums if segment version is &gt;= 4.9.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8599</link><project id="" key="" /><description>We started to use the lucene CRC32 checksums instead of the legacy Adler32
in `v1.3.0` which was the first version using lucene `4.9.0`. We can safely
assume that if the segment was written with this version that checksums
from lucene can be used even if the legacy checksum claims that it has a Adler32
for a given file / segment.

Closes #8587 

NOTE: this PR is against 1.x but will apply to master too but since it needs to pass BWC tests opened it against 1.x
</description><key id="49741947">8599</key><summary>Use Lucene checksums if segment version is &gt;= 4.9.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>resiliency</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T20:02:15Z</created><updated>2015-06-08T00:41:37Z</updated><resolved>2014-11-21T20:48:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-21T20:20:20Z" id="64031747">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove legacy settings for restricting date rounding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8598</link><project id="" key="" /><description>The setting `mapping.date.round_ceil` affects how date ranges are parsed, but only on the upper end of the range, and when the end is inclusive.  It causes a slightly different date parsing function to be used, which will first try to parse as a date, and then try as a timestamp if the year is very large.  If we want to support this timestamp fallback, it should happen for all date parsing, regardless of which end of the range it came from, or whether the end is inclusive.

Also, the setting `index.mapping.date.parse_upper_inclusive` appears to be from before 1.0, and is no longer documented.  This setting is currently parsed and the value used as the default for `mapping.date.round_ceil`.  Both of these settings should be removed to ensure date ranges are parsed symmetrically.
</description><key id="49728951">8598</key><summary>Remove legacy settings for restricting date rounding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Settings</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T18:07:20Z</created><updated>2014-12-15T21:14:27Z</updated><resolved>2014-12-15T21:14:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file><file>src/test/java/org/elasticsearch/count/simple/SimpleCountTests.java</file><file>src/test/java/org/elasticsearch/exists/SimpleExistsTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/search/simple/SimpleSearchTests.java</file></files><comments><comment>Settings: Remove `mapping.date.round_ceil` setting for date math parsing</comment></comments></commit></commits></item><item><title>Fix wrong error messages in MultiMatchQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8597</link><project id="" key="" /><description>There is an issue with the error messages of `multi_match`:

``` json
{
    "multi_match": {
        "query":    "full text search",
        "fields":   [ "title", "body" ]
    }
}
```

If I do not have any "title" or "body" fields in my mapping, it threw a parse error: `No fields specified for match_all query`. Which is wrong both ways:
- There are fields in my query
- my query is not a "match_all" :)

This PR fix this last issue and a small typo in the doc block.
</description><key id="49727153">8597</key><summary>Fix wrong error messages in MultiMatchQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">damienalexandre</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T17:52:19Z</created><updated>2015-06-08T00:18:21Z</updated><resolved>2014-11-21T18:29:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-21T18:29:49Z" id="64015242">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file></files><comments><comment>Fix wrong error messages in MultiMatchQueryParser.</comment></comments></commit></commits></item><item><title>Prefix expansions should be based on term frequency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8596</link><project id="" key="" /><description>It appears (and I read somewhere) that prefix expansions (for multi-match phrase prefix queries for instance) are sorted alphabetically. This means that the max_expansions limits limit can get hit quickly for short text.

An example I was looking at was searching for "related pos". Should match a bunch of docs I have mentioning "related posts", but matches nothing unless I set max expansions to 5000. My filters are limiting us to a search of about 3000 docs in an index of many millions of docs, so there are a lot of words starting with "pos" that are going to get in the way of the prefix query.

I think a better sorting of which terms to expand on would be to start with the terms with the lowest document frequency since they will be the most discerning of importance. I think this should be a doable change. Any pointers on if there is a reader to get the terms with frequencies?

Even better would be to limit which terms are chosen based on the filters being applied. I'm not sure that's possible though given the query execution order.
</description><key id="49723546">8596</key><summary>Prefix expansions should be based on term frequency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels><label>discuss</label></labels><created>2014-11-21T17:20:04Z</created><updated>2015-11-21T21:36:56Z</updated><resolved>2015-11-21T21:36:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-21T17:36:26Z" id="64007750">&gt; It appears (and I read somewhere) that prefix expansions (for multi-match phrase prefix queries for instance) are sorted alphabetically. This means that the max_expansions limits limit can get hit quickly for short text.

I haven't looked at the code in a while but I imagine they are collected alphabetically because that is how the index is stored.  If max_expansions acts as a limit to walking the terms dictionary then it'd terminate early.  You could certainly have a mode where you plop the terms into a priority queue as you walk the dictionary and then only search those terms.  You'd have to walk all the matches in the dictionary but you'd still only get the as many term queries as you asked for. 

&gt; An example I was looking at was searching for "related pos". Should match a bunch of docs I have mentioning "related posts", but matches nothing unless I set max expansions to 5000. My filters are limiting us to a search of about 3000 docs in an index of many millions of docs, so there are a lot of words starting with "pos" that are going to get in the way of the prefix query.
&gt; 
&gt; I think a better sorting of which terms to expand on would be to start with the terms with the lowest document frequency since they will be the most discerning of importance. I think this should be a doable change. Any pointers on if there is a reader to get the terms with frequencies?

I'm pretty sure that's possible but maybe backwards.  You could end up throwing out tons of matches that way.  I guess it depends.  It'd be best if you could signal the user that that portion of the query couldn't complete but I think that change would be much harder.

&gt; Even better would be to limit which terms are chosen based on the filters being applied. I'm not sure that's possible though given the query execution order.

I believe that's effectively impossible due to execution order and performance issues.
</comment><comment author="gibrown" created="2014-11-21T18:17:34Z" id="64013480">Ya, you're probably right, most frequent terms may be more sensible. Would at least need some minimum cutoff frequency if sorting by least frequent.
</comment><comment author="s1monw" created="2014-11-23T13:24:51Z" id="64117811">I think `MultiPhrasePrefixQuery` should be a `MultiTermQuery` and then we can expose the rewrite method for it and get all the flexibility. I also think we should port this class to Lucene... @rmuir @rjernst WDYT?
</comment><comment author="rmuir" created="2014-11-23T13:31:19Z" id="64117992">Do we really need another way for the user to use wildcards? My vote: nuke it
</comment><comment author="rjernst" created="2014-11-23T18:55:00Z" id="64129514">This seems like it would eliminate the protection that `max_expansions` provides? It would require buffering up all terms that match the prefix, so that you could sort on this property (e.g doc freq). It wouldn't be as bad as actually matching on all of these terms, but wildcards are already hacky, so why do this?  It would be better to model the searches based on something other than prefixes.
</comment><comment author="gibrown" created="2014-11-24T14:28:15Z" id="64200609">Oh, I see. A wildcard query can do almost exactly what I want using something like "rewrite": "top_terms_boost_500". Unfortunately doesn't work on multiple fields out of the box, so it does require a more complex query on the client side. Feels like the rewrite syntax could be improved also.

Maybe the prefix match queries should be reworked to use wildcard queries under the hood. I assume they would have the same performance characteristics as a prefix query?
</comment><comment author="rmuir" created="2014-11-24T14:38:10Z" id="64202002">In cases where you want both partial matching and proximity, consider n-grams...
</comment><comment author="clintongormley" created="2014-12-31T17:58:58Z" id="68458196">The `top_terms_N` rewrite method doesn't seem to work in the way described, ie collecting the top N scoring terms from all matching prefixes.  Instead, it seems to examine only the first N terms (alphabetically):

```
DELETE t

PUT t
{
  "settings": {
    "number_of_shards": 1
  }
}

POST /t/t/_bulk
{ "index":{}}
{ "field": "foo1"}
{ "index":{}}
{ "field": "foo1"}
{ "index":{}}
{ "field": "foo2"}
{ "index":{}}
{ "field": "foo2"}
{ "index":{}}
{ "field": "foo3"}
{ "index":{}}
{ "field": "foo3"}
{ "index":{}}
{ "field": "foo4"}
{ "index":{}}
{ "field": "foo4"}
{ "index":{}}
{ "field": "foo5"}

POST t/_optimize?max_num_segments=1
```

This query will only return `foo1`..`foo4`:

```
GET _search
{
  "query": {
    "wildcard": {
      "field": {
        "value": "foo*",
        "rewrite": "top_terms_4"
      }
    }
  }
}
```

Changing the rewrite to `top_terms_5` includes the best scoring term `foo5`.
</comment><comment author="rmuir" created="2015-01-01T18:40:46Z" id="68494273">&gt; The top_terms_N rewrite method doesn't seem to work in the way described, ie collecting the top N scoring terms from all matching prefixes. Instead, it seems to examine only the first N terms (alphabetically):

It works correctly, but a wildcard query "scores" all terms the same. Because of that, this top-N sorting tie-breaks on term order. Things are different when the query actually assigns a score to terms (such as fuzzy query).
</comment><comment author="clintongormley" created="2015-11-21T21:36:56Z" id="158683931">Given that this change could potentially make wildcards significantly heavier than they already are today, I'm going to close this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8595</link><project id="" key="" /><description>Hi,

I've recently upgraded to `1.4.0` and I have some transient errors, like this one : 

```
ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference@143d90f]
```

It's very strange since it happens on various indices. Some of them are very stable (a handful of update each day), some of them are very volatile (thousands of concurrent read/writes during the day, then left alone).

I can't reproduce those errors. I can't pinpoint any specific query.
I've been logging a lot and each time a query raise this error, I re-execute it and it runs well.

I've not tried to downgrade to `1.3.x` yet. Maybe I will.

I've been raking my brain for many days, with quite some frustration hence this evasive bug report.

If anyone has the beginning of an idea, I'd be happy to try and give as much information as I can.
</description><key id="49722131">8595</key><summary>Failed to derive xcontent from org.elasticsearch.common.bytes.ChannelBufferBytesReference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlecour</reporter><labels><label>bug</label><label>feedback_needed</label></labels><created>2014-11-21T17:08:46Z</created><updated>2017-04-25T10:18:58Z</updated><resolved>2015-04-26T19:44:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jlecour" created="2014-11-21T17:16:00Z" id="64004762">A couple of details to add :

I get those errors with
- Elasticsearch `1.4.0`
- Debian stable `7.7`
- OpenJDK `7u65-2.5.1-5~deb7u1`
- Ruby `2.1.5`
- elasticsearch gem `1.0.6`

I don't get any error with
- Elasticsearch `1.3.4`
- Ruby `2.1.2`

There is also a couple of things that has changed in my application. I'll try to exclude those changes.
</comment><comment author="s1monw" created="2014-11-23T13:30:15Z" id="64117960">can we get more of the stacktrace for this error - do you know when this happens?
</comment><comment author="jlecour" created="2014-11-25T09:07:04Z" id="64328642">I've found this in my Elasticsearch log : 

```
[2014-11-25 10:03:22,940][WARN ][http.netty               ] [Bis] Caught exception while handling client http traffic, closing connection [id: 0xf14bdac7, /127.0.0.1:54741 =&gt; /127.0.0.1:9200]
java.lang.IllegalArgumentException: empty text
        at org.elasticsearch.common.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:97)
        at org.elasticsearch.common.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.elasticsearch.common.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.elasticsearch.common.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.elasticsearch.common.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

PS : my log level is `DEBUG` and there is nothing before/after this message.
</comment><comment author="clintongormley" created="2014-11-25T17:42:48Z" id="64440236">@jlecour I can reproduce this error if I telnet to port 9200 and send some non-HTTP message, eg `FOO\n``

Do you have some process which is pinging your server to check that it is alive?
</comment><comment author="jlecour" created="2014-11-25T19:41:45Z" id="64458456">@clintongormley No I don't have such activity. Also, I have this error in my Ruby client's log, after `_bulk` requests. But as I've said, the request that trigger such an error can be further executed as is and work perfectly.

I may have more information in a few hours.
</comment><comment author="epackorigan" created="2015-01-14T06:21:54Z" id="69874202">i was seeing this earlier today with a slightly malformed JSON block (trying to setup some transient settings). In my case, i had extra spaces at the beginning, before the opening bracket.

this worked:

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.disk.threshold_enabled" : false
    }
}'
```

this didn't (note the extra space on the beginning of the line containing "transient")

```
curl -XPUT localhost:9200/_cluster/settings -d '
 { "transient" : {
        "cluster.routing.allocation.disk.threshold_enabled" : false
    }
}'
```
</comment><comment author="clintongormley" created="2015-01-15T20:03:42Z" id="70151789">@epackorigan Can you recreate this?

i tried setting up two nodes and running your malfunctioning request, and all worked as expected.
</comment><comment author="epackorigan" created="2015-01-17T06:27:56Z" id="70356764">I will try to reproduce on Monday and reconfirm.
</comment><comment author="epackorigan" created="2015-01-22T00:19:43Z" id="70949165">not quite monday, but here is what i have:

I was trying to migrate data off one node, so i can service it (add ram to it).
i tried the following:

```
curl --silent -XPUT localhost:9200/_cluster/settings -s '{
  "transient" : {
    "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
  }
}'
```

note: there are two spaces in front of transient, and 4 in front of cluster. i received the following error:
  "error" : "ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@1]",

more variations on the spacing, or formatting of the submitted data, pretty much always yielded that same error:

```
curl --silent -XPUT localhost:9200/_cluster/settings?pretty -s '{ "transient" : { "cluster.routing.allocation.exclude._ip" : "10.0.0.1" } }'
{
  "error" : "ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@1]",
  "status" : 400
}
```

until i had exactly 4/8 spaces:

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
    }
}'
{"acknowledged":true,"persistent":{},"transient":{"cluster":{"routing":{"allocation":{"exclude":{"_ip":"10.0.0.1"}}}}}}
```

it could be that the parser there is very picky about the format of the JSON data (i'm not even sure if that's valid JSON or not).

Note: I'm running on Debian Wheezy, with OpenJDK-1.7, ES 1.4.2 (all installed from packages)
</comment><comment author="clintongormley" created="2015-01-22T13:57:29Z" id="71023316">@epackorigan The formatting of your JSON is just fine.  It's your use of parameters which is flakey ;)

Your version which doesn't work:

```
curl -XPUT localhost:9200/_cluster/settings -s '{
```

Your version which **does** work:

```
curl -XPUT localhost:9200/_cluster/settings -d '{
```

With the first version, you're not passing a body at all, which is why it is failing with Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@1
</comment><comment author="epackorigan" created="2015-01-22T17:26:29Z" id="71060192">&lt;facepalm&gt;Doh! sorry for the noise!&lt;/facepalm&gt;
</comment><comment author="mausch" created="2015-02-24T17:45:05Z" id="75807087">I just got this error from copying and pasting a request from somewhere into Marvel/Sense, which left me with a space after the request path. I googled the error and this was the first result, which was a good thing because I could fix it right away, but perhaps a "less scary" error might help other users (I imagine this must be a quite common mistake).
</comment><comment author="bleskes" created="2015-02-24T17:51:22Z" id="75808334">@mausch that's annoying. FYI We have this noted for Sense and we'll fix it to trim the request properly (putting aside the question of a better error message 
</comment><comment author="bleskes" created="2015-03-05T20:46:12Z" id="77448078">@mausch FYI - the trailing space issue is fixed in marvel 1.3.1 , released today.
</comment><comment author="clintongormley" created="2015-04-26T19:44:00Z" id="96426160">No more info on this ticket. Closing. Feel free to reopen if you're still seeing this
</comment><comment author="GamingCoder" created="2015-07-29T15:47:01Z" id="125995116">I saw this error in the ES JavaScript client and documented the problem at https://github.com/elastic/elasticsearch-js/issues/245
</comment><comment author="bamarni" created="2016-01-27T10:24:20Z" id="175533608">I'm getting the same error with version `1.7.4` and the official php client.

It is with the bulk api too, even though I get this error it looks like everything worked normally, very similar to what @jlecour reported.

[EDIT] : actually it was due to an empty document in the batch, my bad.
</comment><comment author="ulkas" created="2016-03-08T09:37:11Z" id="193688548">@bamarni 
the same at me, also en empty body in the batch json
</comment><comment author="serg3ant" created="2017-04-25T10:18:19Z" id="296986453">It seem to happen when bulk is empty (no actions / documents at all)</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added utility method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8594</link><project id="" key="" /><description /><key id="49717636">8594</key><summary>Added utility method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ananich</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T16:28:38Z</created><updated>2015-03-19T10:09:41Z</updated><resolved>2014-11-21T20:08:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T19:34:16Z" id="64025256">hey can you please sign the CLA http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="ananich" created="2014-11-21T19:45:53Z" id="64026903">Done
</comment><comment author="s1monw" created="2014-11-21T20:09:23Z" id="64030280">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java</file></files><comments><comment>Added utility method to CountRequestBuilder</comment></comments></commit></commits></item><item><title>Custom Function Scripts Throw UnsupportedOperationException in 1.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8593</link><project id="" key="" /><description>We just upgraded to 1.4 and noticed that all of our native custom function scripts throw this error. We also noticed that the score() function can no longer be found. 
https://github.com/elasticsearch/elasticsearch/commit/a4640b4a8a2feefc9d10743ba68fd7a26c81fd2b#diff-2fe61cedf4859b4ca85e7b05ab021a3b

I notice that on Nov 10 you fixed this issue, but that doesn't appear to be in 1.4.
https://github.com/elasticsearch/elasticsearch/commit/ce8a7297522868485dab887baa94fcb30f61d9ee

(File: src/main/java/org/elasticsearch/script/AbstractSearchScript.java)
Is there a way to get around this problem in 1.4 or do we need to downgrade and wait for 1.4.1 to come out?
</description><key id="49716634">8593</key><summary>Custom Function Scripts Throw UnsupportedOperationException in 1.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acampagna</reporter><labels /><created>2014-11-21T16:22:43Z</created><updated>2014-11-21T22:43:27Z</updated><resolved>2014-11-21T22:43:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T22:43:27Z" id="64049845">this is fixed and 1.4.1 will come pretty soon too. I don't know if there is a way around your problem without knowing what your scripts trying to do. I will close this issue for now since the bug is fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix script fields to be returned as a multivalued field when they produce a list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8592</link><project id="" key="" /><description>This change is essentially the same as #3015 but on script fields.
</description><key id="49711022">8592</key><summary>Fix script fields to be returned as a multivalued field when they produce a list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T15:44:18Z</created><updated>2015-06-06T16:20:00Z</updated><resolved>2014-11-24T08:53:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-22T00:41:04Z" id="64060150">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsFetchSubPhase.java</file><file>src/test/java/org/elasticsearch/script/IndexLookupTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>Core: Fix script fields to be returned as a multivalued field when they produce a list.</comment></comments></commit></commits></item><item><title>Return new lists on calls to getValues.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8591</link><project id="" key="" /><description>Scripts currently share the same list across invocations to getValues. This
caused a bug in script fields where all documents coming from the same segment
would get the same values (basically, for the next document for which script
values have been requested). Scripts now return a fresh new list on every
invocation to `getValues`.

Close #8576
</description><key id="49704969">8591</key><summary>Return new lists on calls to getValues.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T14:56:08Z</created><updated>2015-06-08T00:25:06Z</updated><resolved>2014-11-25T21:30:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T22:51:39Z" id="64050704">I wonder if we should fix this Interface and return `Iterable&lt;T&gt;` instead such that folks need to make copies of it if they really need and we can just lazily fetch the values?
</comment><comment author="jpountz" created="2014-11-21T23:02:58Z" id="64051859">I am not sure it would fix the issue? Someone could still keep references to the Iterable instance?

I think it is the right trade-off to have these interfaces potentially slow but easy to use. It avoids surprises, and if someone needs better performance it is still possible to access directly the fielddata interfaces (via the `getInternalValues` method).
</comment><comment author="s1monw" created="2014-11-23T12:32:18Z" id="64116344">&gt; I am not sure it would fix the issue? Someone could still keep references to the Iterable instance?

yes they should becuse each call to `getValues` would return a new iterable. but it would produce the values each time. The common case here is that stuff iterated once if you need it more often you should copy it?
</comment><comment author="jpountz" created="2014-11-24T18:41:50Z" id="64241760">@s1monw I updated the patch with your suggestion. So now we have `doc['field']` which implements `java.util.List` and its scope is limited to the current document. And we also have `doc['field'].values` which returns a copy so that it can survive changes of scope. So for instance `doc['field'].values` is required in script fields for things to work correctly but it is perfectly fine to use `doc['field']` in the context of an aggregation since we don't keep references to the produced list. Is it what you had in mind?
</comment><comment author="s1monw" created="2014-11-25T16:04:56Z" id="64423189">awesome - LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cannot connect to webchat.freenode #elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8590</link><project id="" key="" /><description>I was trying to connect to the "#elasticsearch" irc channel as directed on http://www.elasticsearch.org/community/
The server gives an error on connect:
`#elasticsearch Cannot join channel (+r) - you need to be identified with services`

I remember, that the process worked before. Also I can connect to other channels on freenode just fine. Is there a configuration problem on your side?
</description><key id="49704450">8590</key><summary>Cannot connect to webchat.freenode #elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels /><created>2014-11-21T14:51:09Z</created><updated>2014-11-21T22:43:58Z</updated><resolved>2014-11-21T22:43:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T22:43:58Z" id="64049899">afaik this has been fixed now - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make the memory stats use Lucene's Accountable APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8589</link><project id="" key="" /><description>In addition to reporting the memory usage of a given object, the Accountable API also allows to return sub-information about where this memory usage comes from.

We should move memory usage reporting to this API in order to have more fine-grained understanding of where the memory goes. For instance, this could allow the parent/child field data memory report to break down memory usage by type, etc.
</description><key id="49682266">8589</key><summary>Make the memory stats use Lucene's Accountable APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-21T10:43:20Z</created><updated>2016-01-22T18:28:04Z</updated><resolved>2015-11-23T18:34:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-21T13:02:42Z" id="63966886">I tried to do some of this when upgrading lucene.

Part of it is that ES has these crazy "maps" that arent actually maps, used everywhere, so I think i TODO'd this. Can this map be fixed? 
</comment><comment author="clintongormley" created="2015-11-21T21:34:55Z" id="158683822">@jpountz is this still something we should pursue?
</comment><comment author="jpountz" created="2015-11-23T18:34:58Z" id="159022119">I don't think this is relevant anymore now that the things that use memory (fielddata, query cache) are in Lucene and implement Accountable. The only thing which is left in Elasticsearch is the request cache, but we don't even report memory usage as it's meant to be tiny (1% memory usage by default).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to current Lucene 5.0.0 snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8588</link><project id="" key="" /><description>A few recent lucene changes to fold in:

We've removed IndexWriter.unLock in Lucene, so I also removed "clear
lock on startup" in Elasticsearch.  A couple test cases were missing
engine.close() before opening a new engine.

I exposed the new SerbianNormalizationFilterFactory.

I noticed that we silently use NoLockFactory if you have a typo in
your index.store.fs.fs_lock; I changed this to throw
StoreException instead.  I think it's scary to be lenient
here?

The trickiest change was DistributorDirectory, and how it tracks the
file created by the lock factory.  For the writeFiles boolean, I had
to remove the check that "lockFactory instanceof FSLockFactory"; maybe
we could use reflection to get this, if it's really necessary?  (Do we
really support using NoLockFactory w/ FSDirectory?).  Or maybe instead
we could check if the class name of the lock/factory is NoLock, but
MockDirWrapper wraps in AssertingLock...

I added some harmless "synchronized" to methods (all callers of these
private methods are already sync'd) just to make it clear on quick
look that all access/changes to nameDirMapping are in fact sync'd.

I also fixed ConcurrentMergeSchedulerProvider to disable Lucene's CMS
stalling (override w/ an empty maybeStall method).  We can simplify
things here: remove EnableMergeScheduler, the separate MERGE thread
pool, let IndexWriter launch merges when they are needed (not
separately, once every second), etc., since Lucene won't stall
indexing threads anymore ... I'll open a separate issue for this.
</description><key id="49677462">8588</key><summary>Upgrade to current Lucene 5.0.0 snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T09:48:50Z</created><updated>2015-08-25T13:25:44Z</updated><resolved>2014-11-24T10:25:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-23T12:35:45Z" id="64116440">left once comment - LGTM otherwise
</comment><comment author="rmuir" created="2014-11-23T20:21:52Z" id="64132906">looks good to me. Maybe @uschindler is curious about lockfactory changes.
</comment><comment author="mikemccand" created="2014-11-23T21:16:09Z" id="64135090">Thank you for all the feedback, I folded it all in @uschindler 
</comment><comment author="uschindler" created="2014-11-23T21:33:15Z" id="64135807">See my comments above. Otherwise its correct API-wise. It also took me a long time while refactoring LockFactory in Lucene that you don't need one in most cases. LockFactory is just an implementation detail of BaseDirectory which should only be used for actual "real" directory implementations. Everything that wraps or filters should use Directory (for complex delegation like FileSwitchDirectory) or FilterDirectory (for simple delagation).
</comment><comment author="mikemccand" created="2014-11-24T09:39:39Z" id="64170003">I pushed another commit w/ feedback from @uschindler (thanks Uwe!); I think it's ready.
</comment><comment author="s1monw" created="2014-11-24T09:40:19Z" id="64170073">LGTM
</comment><comment author="uschindler" created="2014-11-24T09:47:59Z" id="64170830">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisModule.java</file><file>src/main/java/org/elasticsearch/index/analysis/SerbianNormalizationFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/merge/scheduler/ConcurrentMergeSchedulerProvider.java</file><file>src/main/java/org/elasticsearch/index/store/DistributorDirectory.java</file><file>src/main/java/org/elasticsearch/index/store/fs/FsDirectoryService.java</file><file>src/main/java/org/elasticsearch/search/suggest/context/GeolocationContextMapping.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/index/store/distributor/DistributorTests.java</file></files><comments><comment>Core: upgrade to current Lucene 5.0.0 snapshot</comment></comments></commit></commits></item><item><title>Recovery detects false corruption if legacy checksums are present for a new written segment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8587</link><project id="" key="" /><description>BWC tests run into a failure this morning which is caused by the verification of the old adler 32 checksums we added recently. 

http://build-us-00.elasticsearch.org/job/es_bwc_1x/5047/CHECK_BRANCH=tags%2Fv1.2.4,jdk=JDK7,label=bwc/

the problem here is the following
- index was created with es `1.2.4` which still records Adler32 checksums in the legacy checksum file
- we flush but apparently the last segment `_h` didn't make it into the commit but was recorded in the checksums file. 
- we uprade the node to `1.4.1-SNAPSHOT` - the index opens just fine
- we apply the transaction log and IndexWriter starts writing a segment `_h`
  - note: now we have a Adler32 checksum for `_h` in the checksum file but the files are actually not the once that where checksummed.
- since we have a replica we initiate a recovery and in our `Store.java` code line `#638` we prefer the adler checksum even though we could get the original checksum from lucene.
- on recovery we now compare the checksums and they obviously don't match - in turn fail the primary :-1: 

I think the fix here is to prefer new checksums since they are taken from the file if we know we have them....
</description><key id="49675582">8587</key><summary>Recovery detects false corruption if legacy checksums are present for a new written segment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>blocker</label><label>bug</label><label>PITA</label><label>resiliency</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-21T09:26:36Z</created><updated>2015-03-19T20:26:19Z</updated><resolved>2014-11-21T21:39:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T18:38:31Z" id="64016428">just as a sidenote - the code that has the problem was never released to this is not affecting `1.4.0`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file></files><comments><comment>[STORE] Use Lucene checksums if segment version is &gt;= 4.9.0</comment></comments></commit></commits></item><item><title>Wrong recovery indices stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8586</link><project id="" key="" /><description>1. Have an ES node with some docs (1 index, 1 shard per index, no replicas).
2. Take a snapshot.
3. Start restore.
4. Get a Recovery Status.

``` java
RecoveryState.Index index = client.admin().indices().prepareRecoveries().get()
.shardResponses().get("index").get(0).recoveryState().getIndex();
index.numberOfRecoveredBytes() // will return a negative number
index.percentBytesRecovered() // will return 0 though the shard is already DONE
```
</description><key id="49641588">8586</key><summary>Wrong recovery indices stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abaxanean</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>bug</label></labels><created>2014-11-21T01:34:20Z</created><updated>2016-11-26T12:51:22Z</updated><resolved>2016-11-26T12:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-26T12:51:22Z" id="263061883">This appears to be fixed in recent versions</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update DiskThresholdDecider javadoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8585</link><project id="" key="" /><description>Since v1.3.0, and issue #6201, the default values in code and documentation have been updated to 85% and 90% for low and high watermarks. However, the related javadoc still contains the initial values : this commit fix this.
</description><key id="49627197">8585</key><summary>Update DiskThresholdDecider javadoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">tcucchietti</reporter><labels><label>docs</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T22:30:40Z</created><updated>2014-11-21T09:41:15Z</updated><resolved>2014-11-21T09:25:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-21T09:25:40Z" id="63944873">Merged this to master, 1.x, and 1.4. Thanks for the fix @tcucchietti!
</comment><comment author="s1monw" created="2014-11-21T09:39:41Z" id="63946510">@dakrone can you backport this to 1.3.6 too?
</comment><comment author="dakrone" created="2014-11-21T09:40:03Z" id="63946553">Yep, will do (done)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix Bloom filter ram usage calculation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8584</link><project id="" key="" /><description>BloomFilter actually returned the size of the bitset as the
size in bytes so off by factor 8 plus a constant :)

Closes #8564
</description><key id="49622589">8584</key><summary>Fix Bloom filter ram usage calculation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Stats</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T21:48:26Z</created><updated>2015-06-08T00:39:01Z</updated><resolved>2014-11-20T22:00:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-11-20T21:55:27Z" id="63887235">LGTM
</comment><comment author="rmuir" created="2014-11-20T21:56:13Z" id="63887353">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added swift openstack repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8583</link><project id="" key="" /><description /><key id="49616539">8583</key><summary>Added swift openstack repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">matthughes</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-20T20:52:09Z</created><updated>2014-11-25T12:50:08Z</updated><resolved>2014-11-25T12:49:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T13:20:35Z" id="64192483">Hi @matthughes 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="matthughes" created="2014-11-24T13:45:01Z" id="64195144">K, signed.
</comment><comment author="matthughes" created="2014-11-24T13:47:00Z" id="64195376">@clintongormley Hmm, I put the wrong github username down.  I wrote 'mhughes' which is my usual username, but I forgot on github it's matthughes.
</comment><comment author="clintongormley" created="2014-11-25T12:50:08Z" id="64394734">thanks @matthughes - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Added swift openstack repository</comment></comments></commit></commits></item><item><title>Typo: changed "5% or the real words" to "5% of the real words"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8582</link><project id="" key="" /><description /><key id="49602396">8582</key><summary>Typo: changed "5% or the real words" to "5% of the real words"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haneytron</reporter><labels><label>adoptme</label><label>Awaiting CLA</label></labels><created>2014-11-20T18:46:44Z</created><updated>2014-11-25T12:15:50Z</updated><resolved>2014-11-25T12:15:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-23T13:28:06Z" id="64117895">can you sign the CLA so we can pull it in? --&gt; http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="haneytron" created="2014-11-23T16:33:21Z" id="64123907">Signed.
</comment><comment author="clintongormley" created="2014-11-25T12:15:44Z" id="64391228">thanks @ironyx - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Typo: changed "5% or the real words" to "5% of the real words"</comment></comments></commit></commits></item><item><title>ParentChildStressTest broken with MasterNotDiscoveredException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8581</link><project id="" key="" /><description>I tried to run ParentChildStressTest , but it failed with an exception like below

```
Exception in thread "main" org.elasticsearch.discovery.MasterNotDiscoveredException: waited for [30s]
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$4.onTimeout(TransportMasterNodeOperationAction.java:164)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:239)
    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:497)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

How do you guys run this test? 
</description><key id="49597342">8581</key><summary>ParentChildStressTest broken with MasterNotDiscoveredException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coderplay</reporter><labels /><created>2014-11-20T18:14:52Z</created><updated>2014-11-24T19:50:10Z</updated><resolved>2014-11-24T18:47:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-21T12:00:21Z" id="63961047">ParentChildStressTest assumes that there is another data node running. This test only starts a client node. It isn't a test that runs when building elasticsearch, but a test that is used manually.
</comment><comment author="coderplay" created="2014-11-21T16:23:42Z" id="63994630">@martijnvg 

How do you launch another data node separately? I am a newbie of elasticsearch.

Have you tested it before release?  Actually, I modified the code of ParentChildStress, programmatically launched 4 data nodes before client starting,  but still failed with another parsing exception.  It's because that  "field" query was deprecated. 
</comment><comment author="clintongormley" created="2014-11-24T18:47:48Z" id="64242693">@coderplay Please ask questions like these about how to use Elasticsearch in the mailing list: http://elasticsearch.org/community

thanks
</comment><comment author="coderplay" created="2014-11-24T19:36:33Z" id="64250376">@clintongormley 

Actually, I do assure this is a bug. "field" query was deprecated. you can't go through this test. 
</comment><comment author="clintongormley" created="2014-11-24T19:50:10Z" id="64252520">Hi @coderplay 

Yes I see that you are correct.  This is a very old test that was launched as an ordinary Java console app.  It doesn't use the junit test framework and has not assertions.  We don't run this test anymore.  In fact parent-child has been rewritten completely since then.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing Aggregation in 1.4.0 throws ArrayOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8580</link><project id="" key="" /><description>This query used to work fine on 1.3.5:

```
{
  "from": 0,
  "size": 0,
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "bool": {
          "must": {
            "term": {
              "closed": false
            }
          },
          "_cache": true
        }
      }
    }
  },
  "aggregations": {
    "missing-external_link-square": {
      "missing": {
        "field": "external_link.square"
      }
    }
  }
}
```

I upgraded the index to 1.4.0, and now it returns this stack trace (on 10 shards):

{
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][0]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][0]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][1]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][1]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][2]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][2]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][3]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][3]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][4]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][4]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][5]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][5]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][6]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][6]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][7]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][7]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][8]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][8]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }{[LBJC8-OYQySmdIMINrn-Ow][radius_2014-11-13-23-39_updated][9]: QueryPhaseExecutionException[[radius_2014-11-13-23-39_updated][9]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException; }]",
"status": 500
}
</description><key id="49593389">8580</key><summary>Missing Aggregation in 1.4.0 throws ArrayOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">tylerprete</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T17:49:19Z</created><updated>2014-11-24T18:44:12Z</updated><resolved>2014-11-24T18:43:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-23T13:27:12Z" id="64117872">@jpountz can you take a look at this?
</comment><comment author="jpountz" created="2014-11-23T20:42:34Z" id="64133731">@tylerprete Can you share the mappings of your `external_link.square` field? If you look at the lines that have been written in the elasticsearch log file at the same time that you got this error, you should see a stack trace for the `ArrayIndexOutOfBoundsException`, this information would be very helpful too. Thanks!
</comment><comment author="tylerprete" created="2014-11-24T04:45:30Z" id="64151985">Thanks for looking into this so quickly!

Mapping for that field is nothing fancy:

```
{
  "external_link": {
    "properties": {
      "square": {
        "type": "string"
      }
    }
  }
}
```

I believe this is the stack trace you're looking for:

```
[2014-11-24 04:41:17,894][DEBUG][action.search.type       ] [10.0.4.38] [radius_2014-11-13-23-39_updated][5], node[AlrEzws1TiiuZ6CiC-9RvQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3e546cc6] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [radius_2014-11-13-23-39_updated][5]: query[filtered(ConstantScore(cache(BooleanFilter(+cache(closed:F)))))-&gt;cache(_type:place)],from[0],size[0]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 728
        at org.apache.lucene.util.packed.Direct16.get(Direct16.java:54)
        at org.apache.lucene.util.packed.PackedLongValues.get(PackedLongValues.java:102)
        at org.apache.lucene.util.packed.PackedLongValues.get(PackedLongValues.java:110)
        at org.elasticsearch.index.fielddata.ordinals.MultiOrdinals$MultiDocs.ordAt(MultiOrdinals.java:166)
        at org.elasticsearch.index.fielddata.AbstractRandomAccessOrds.nextOrd(AbstractRandomAccessOrds.java:41)
        at org.apache.lucene.index.DocValues$5.get(DocValues.java:171)
        at org.elasticsearch.search.aggregations.bucket.missing.MissingAggregator.collect(MissingAggregator.java:59)
        at org.elasticsearch.search.aggregations.BucketCollector$2.collect(BucketCollector.java:81)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucketNoCounts(BucketsAggregator.java:74)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:63)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectBucket(BucketsAggregator.java:55)
        at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.collect(FilterAggregator.java:61)
        at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:117)
        ... 7 more
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/AbstractRandomAccessOrds.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataImplTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/SortedSetDVStringFieldDataTests.java</file></files><comments><comment>Fielddata: Fix iterator over global ordinals.</comment></comments></commit></commits></item><item><title>Ensure shards are deleted under lock on close</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8579</link><project id="" key="" /><description>Today there is a race condition between the actual deletion of
the shard and the release of the lock in the store. This race can cause
rare imports of dangeling indices if the cluster state update loop
tires to import the dangeling index in that particular windonw. This commit
adds more safety to the import of dangeling indices and removes the race
condition by holding on to the lock on store closing while the listener
is notified.
</description><key id="49590757">8579</key><summary>Ensure shards are deleted under lock on close</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T17:33:30Z</created><updated>2015-06-06T19:20:06Z</updated><resolved>2014-11-21T11:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-21T09:01:31Z" id="63942516">@s1monw left a couple of comments
</comment><comment author="s1monw" created="2014-11-21T10:13:43Z" id="63950644">@dakrone  updated based on your comments
</comment><comment author="dakrone" created="2014-11-21T11:19:28Z" id="63957407">LGTM, thanks for adding the documentation
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Not able to setup analyzers with 1.2.1 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8578</link><project id="" key="" /><description>Hi, 
We are using 0.90.x version of ES with java api and it was working fine..
but this is not working with 1.2.1

index settings:
{
    "index": {
        "analysis": {
            "analyzer": {
                "keyword_lowercase": {
                    "type": "custom",
                    "tokenizer": "keyword",
                    "filter": "lowercase"
                },
                "standard_lowercase": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "filter": "lowercase"
                }
            }
        }
    }
}

client.admin().indices().prepareCreate(indexName).setSettings(_).execute().actionGet()

Now this is throwing 
Root type mapping not empty after parsing
</description><key id="49587080">8578</key><summary>Not able to setup analyzers with 1.2.1 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samatha-kankipati</reporter><labels><label>non-issue</label></labels><created>2014-11-20T17:12:47Z</created><updated>2014-11-24T20:15:35Z</updated><resolved>2014-11-24T20:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T22:46:58Z" id="64050221">the problem seems to be your mapping not the index settings can we see your mapping?
</comment><comment author="samatha-kankipati" created="2014-11-21T23:20:19Z" id="64053604">Here is my mapping
{
    "syncRequestDoc":{
        "_source":{
            "enabled":true
        },
        "properties":{
            "id":{
                "type":"string"
            },
            "documentType":{
                "type":"string",
                "analyzer":"keyword_lowercase"
            },
            "documentIds":{
                "type":"string"
            },
            "status":{
                "type":"string",
                "analyzer":"keyword_lowercase"
            },
            "documentLastUpdatedAt":{
                "type":"date",
                "format":"date_time"
            },
            "createdAt": {
                "type":"date",
                "format":"date_time"
            },
            "lastRunAt": {
                "type":"date",
                "format":"date_time"
            },
            "lastRunDetails": {
                "type": "string"
            }
        }
    }
}
</comment><comment author="samatha-kankipati" created="2014-11-21T23:22:50Z" id="64053838">Here is the exception

org.elasticsearch.index.mapper.MapperParsingException: mapping [index]
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:405)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: 
org.elasticsearch.index.mapper.MapperParsingException: Root type mapping not empty after parsing! Remaining fields:   [analysis : {analyzer={keyword_lowercase={filter=lowercase, type=custom, tokenizer=keyword}}}]
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:276)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:190)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:444)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:317)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:402)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</comment><comment author="s1monw" created="2014-11-22T16:12:13Z" id="64085179">hmm can you share the code how you create your index too?
</comment><comment author="samatha-kankipati" created="2014-11-22T21:29:33Z" id="64096191">Here is the code.. We are using 0.90.2..I tried different versions.. It is working fine till 1.1.0 and started failing from 1.2.1

   private void initMappings() {
        client.admin().cluster().prepareHealth().setWaitForYellowStatus().execute().actionGet()
        indicesJson = readJsonFile(indicesStructureJsonFile)
        if (indicesJson) {
            initializeIndicesAndMappings();
        }
    }

```
private void initializeIndicesAndMappings() {
    indicesJson.indices.each { index -&gt;
        setIndex(index.name)
        index.types.each {documentType -&gt;
            setIndexMappings(index.name, documentType)
        }
    }
}

private void setIndex(String indexName) {
    def readIndexSettings = {index -&gt; readJsonFile("${configLocation}/${mappingsPath}/${indexName}/${indexSettingsFileName}") }

    logger.info("Index does not exist. Creating the Index ${indexName} ")
    def indicesClient = client.admin().indices()
    if (!isIndexExists(indexName)) {
        indicesClient.prepareCreate(indexName).setSettings(readIndexSettings(indexName)).execute().actionGet()
    }
    else {
        indicesClient.prepareClose(indexName).execute().actionGet();
        indicesClient.prepareUpdateSettings(indexName).setSettings(readIndexSettings(indexName)).execute().actionGet();
        indicesClient.prepareOpen(indexName).execute().actionGet();
    }
    logger.info("Successfully setup Index ${indexName} ")
}

private void setIndexMappings(String indexName, String docType) {
    String source = readJsonFile("${configLocation}/${mappingsPath}/${indexName}/${docType}.json")
    if (source != null) {
        logger.info("Setting up Mapping for [${indexName}]/[${docType}]: ${source}")
        client.admin().indices().preparePutMapping(indexName).setIgnoreConflicts(false).setType(docType).setSource(source).execute().actionGet()
    }
}

public JSONObject readJsonFile(String url) throws Exception {
    logger.info("About to read index setting file : ${url}")
    final File jsonFile = new File(url);
    if (jsonFile.exists() &amp;&amp; jsonFile.canRead()) {
        new JsonSlurper().parse(new FileReader(jsonFile)) as JSONObject
    }
    else {
        InputStream ips = getClass().getResourceAsStream(url);
        new JsonSlurper().parse(new InputStreamReader(ips)) as JSONObject
    }
}

protected Boolean isIndexExists(String indexName) {
    ClusterStateResponse response = client.admin().cluster().prepareState().execute().actionGet();
    response.getState().metaData().hasIndex(indexName)

}
```
</comment><comment author="clintongormley" created="2014-11-24T20:15:35Z" id="64256595">Hi @samatha-kankipati 

I have no idea what your code is doing or what the files you're slurping contain, but it looks like you are passing settings to the mapping API.  Previously, this was ignored.  Now we catch the error instead.

You'll need to debug this to figure out where you're going wrong, but that's the root cause.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Take into account `_default_ mapping` in index template when parsing alias filter in index templates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8577</link><project id="" key="" /><description>By also taking into account the type for an index request that indexes into index that doesn't exist in the automatic create index request, the mapping for the new type will take over the `_default_` mapping. Alias filters that then point to fields in the `_default_` mapping will then not fail. 

PR for #8473
</description><key id="49583339">8577</key><summary>Take into account `_default_ mapping` in index template when parsing alias filter in index templates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aliases</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T16:55:31Z</created><updated>2015-06-07T17:51:13Z</updated><resolved>2014-11-24T17:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T08:37:49Z" id="63940248">Left a small comment otherwise LGTM
</comment><comment author="martijnvg" created="2014-11-21T11:18:02Z" id="63957284">Updated the PR to not chain the create index request.
</comment><comment author="s1monw" created="2014-11-23T12:55:38Z" id="64116992">thanks LGTM
</comment><comment author="s1monw" created="2014-11-24T15:26:23Z" id="64209399">@martijnvg can you please get this in?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scripting: ScriptDocValues.getValues() returns an reused list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8576</link><project id="" key="" /><description>Using ElasticSearch 1.4.0 from your official .deb, given an index with:

```
"plays": {
    "mappings": {
        "play": {
            "_source": {
                "enabled": false
            },
            "_timestamp": {
                "default": null,
                "enabled": true
            },
            "dynamic": "strict",
            "properties": {
                "artist": {
                    "index": "not_analyzed",
                    "type": "string"
                },
```

And:

```
"settings": {
    "index": {
        "creation_date": "1416491787644",
        "merge": {
            "scheduler": {
                "max_thread_count": "1"
            }
        },
        "number_of_replicas": "1",
        "number_of_shards": "1",
        "refresh_interval": "-1",
        "uuid": "XDRbDOoLSR-cMFjQAm9TjQ",
        "version": {
            "created": "1040099"
        }
    }
}
```

A search like:

```
{
    "script_values": {
        "artist": {
            "script": "_doc['artist'].values"
        }
    }
}
```

Will return a result set whose hits contain an `artist` array whose elements correspond to the elements for the last search result, i.e. all previous hits `artist` arrays assume the same contents as that of the last hit. It looks like an object is being reused somehow, although casting a glance at ScriptDocValues.java I can't see how. Is it possible `listLoaded` is not being reset somehow?

A trivial workaround is:

```
{
    "script_values": {
        "artist": {
            "script": "_doc['artist'].values.take(100)"
        }
    }
}
```
</description><key id="49567282">8576</key><summary>Scripting: ScriptDocValues.getValues() returns an reused list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dw</reporter><labels /><created>2014-11-20T15:30:50Z</created><updated>2014-11-25T21:31:49Z</updated><resolved>2014-11-25T17:02:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dw" created="2014-11-20T15:41:09Z" id="63827061">Aah, clearly the "problem" is that `list` is reset and reused for every doc in the query, which looks like a performance thing. I'm happy if this was marked as a doc bug rather than a functional bug, the current behaviour seems fair.
</comment><comment author="jpountz" created="2014-11-20T16:09:55Z" id="63832190">@dw I tried to reproduce your issue without success. Here is what I ran:

```
DELETE plays 

PUT plays
{
  "mappings": {
    "play": {
      "dynamic": "strict",
      "properties": {
        "artist": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  },
  "settings": {
    "number_of_shards": 1
  }
}

PUT plays/play/1
{
  "artist": [ "me", "you" ]
}

PUT plays/play/2
{
  "artist": [ "you", "him" ]
}

GET plays/_search
{
  "script_fields": {
    "artist": {
      "script": "doc['artist'].values"
    }
  }
}
```

which returned

```
{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "plays",
            "_type": "play",
            "_id": "1",
            "_score": 1,
            "fields": {
               "artist": [
                  [
                     "me",
                     "you"
                  ]
               ]
            }
         },
         {
            "_index": "plays",
            "_type": "play",
            "_id": "2",
            "_score": 1,
            "fields": {
               "artist": [
                  [
                     "him",
                     "you"
                  ]
               ]
            }
         }
      ]
   }
}
```

Can you maybe guide me a bit so that I can reproduce the issue? I used a fresh install of elasticsearch 1.4.0.
</comment><comment author="dw" created="2014-11-20T17:03:43Z" id="63841972">@jpountz huh strange. How many shards does your index have? If &gt;1 and both docs went to a different shard, and the script_value is executed in the context of a shard, that might explain why you don't see it
</comment><comment author="dw" created="2014-11-20T17:04:04Z" id="63842026">(I've only been using ElasticSearch a week, so still guessing about things quite heavily)
</comment><comment author="dw" created="2014-11-20T17:07:18Z" id="63842579">Ah, doh, sorry, didn't notice you'd set the shards to 1. Hmm, not sure
</comment><comment author="jpountz" created="2014-11-20T17:10:20Z" id="63843093">OK I found the reason: this bug only occurs on a per-segment basis, so you need to run an optimize to reproduce the bug. Here is a recreation for the bug:

```
DELETE plays 

PUT plays
{
  "mappings": {
    "play": {
      "dynamic": "strict",
      "properties": {
        "artist": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  },
  "settings": {
    "number_of_shards": 1
  }
}

PUT plays/play/1
{
  "artist": [ "me", "you" ]
}

PUT plays/play/2
{
  "artist": [ "you", "him" ]
}

POST plays/_optimize?max_num_segments=1

GET plays/_search
{
  "script_fields": {
    "artist": {
      "script": "doc['artist'].values"
    }
  }
}
```

which returns

```
{
   "took": 7,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "plays",
            "_type": "play",
            "_id": "2",
            "_score": 1,
            "fields": {
               "artist": [
                  [
                     "me",
                     "you"
                  ]
               ]
            }
         },
         {
            "_index": "plays",
            "_type": "play",
            "_id": "1",
            "_score": 1,
            "fields": {
               "artist": [
                  [
                     "me",
                     "you"
                  ]
               ]
            }
         }
      ]
   }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/SlicedDoubleList.java</file><file>src/main/java/org/elasticsearch/common/util/SlicedLongList.java</file><file>src/main/java/org/elasticsearch/common/util/SlicedObjectList.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java</file><file>src/test/java/org/elasticsearch/common/util/SlicedDoubleListTests.java</file><file>src/test/java/org/elasticsearch/common/util/SlicedLongListTests.java</file><file>src/test/java/org/elasticsearch/common/util/SlicedObjectListTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/PercentileRanksTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/PercentilesTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>Scripts: Return new lists on calls to getValues.</comment></comments></commit></commits></item><item><title>Docs: Fix a typo in a javadoc comment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8575</link><project id="" key="" /><description /><key id="49566914">8575</key><summary>Docs: Fix a typo in a javadoc comment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels><label>docs</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T15:28:52Z</created><updated>2014-12-05T13:36:56Z</updated><resolved>2014-11-21T18:29:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-21T18:29:55Z" id="64015251">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file></files><comments><comment>Fix a typo in a javadoc comment in MapperService.</comment></comments></commit></commits></item><item><title>Fix geohash grid doc counts computation on multi-valued fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8574</link><project id="" key="" /><description>Close #8512
</description><key id="49559020">8574</key><summary>Fix geohash grid doc counts computation on multi-valued fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T14:36:59Z</created><updated>2015-06-07T17:49:19Z</updated><resolved>2014-11-21T10:02:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T08:44:22Z" id="63940862">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter cache: add a `_cache: auto` option and make it the default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8573</link><project id="" key="" /><description>Up to now, all filters could be cached using the `_cache` flag that could be
set to `true` or `false` and the default was set depending on the type of the
`filter`. For instance, `script` filters are not cached by default while
`terms` are. For some filters, the default is more complicated and eg. date
range filters are cached unless they use `now` in a non-rounded fashion.

This commit adds a 3rd option called `auto`, which becomes the default for
all filters. So for all filters a cache wrapper will be returned, and the
decision will be made at caching time, per-segment. Here is the default logic:
- if there is already a cache entry for this filter in the current segment,
  then return the cache entry.
- else if the current segment comes from a flush (as opposed to a merge),
  then do not cache.
- else if the doc id set cannot iterate (eg. script filter) then do not cache.
- else if the doc id set is already cacheable and it has been used twice or
  more in the last 1000 filters then cache it.
- else if the doc id set is not cacheable and it has been used 5 times or more
  in the last 1000 filters, then load it into a cacheable set and cache it.
- else return the uncached set.

So for instance geo-distance filters and script filters are going to use this
new default and are not going to be cached because of their iterators.

Similarly, date range filters are going to use this default all the time, but
it is very unlikely that those that use `now` in a not rounded fashion will get
reused so in practice they won't be cached.

`terms`, `range`, ... filters produce cacheable doc id sets with good iterators
so they will be cached as soon as they have been used twice.

Filters that don't produce cacheable doc id sets such as the `term` filter will
need to be used 5 times before being cached. This ensures that we don't spend
CPU iterating over all documents matching such filters unless we have good
evidence of reuse.

One last interesting point about this change is that it also applies to compound
filters. So if you keep on repeating the same `bool` filter with the same
underlying clauses, it will be cached on its own while up to now it used to
never be cached by default.

`_cache: true` has been changed to only cache on large segments, in order to not
pollute the cache since small segments should not be the bottleneck anyway.
However `_cache: false` still has the same semantics.

Close #8449
</description><key id="49556249">8573</key><summary>Filter cache: add a `_cache: auto` option and make it the default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T14:16:15Z</created><updated>2015-06-07T17:14:25Z</updated><resolved>2014-12-18T15:02:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T10:24:05Z" id="63951927">&gt; else if the doc id set is already cacheable and it has been used twice or more in the last 1000 filters then cache it.

should this be `else if the doc id set is already cached a`?

I'd actually add to the list that we don't cache fitlers from readers that are created by a flush since they might be merged away quickly and pollute the cache? These filters should be very fast anyway and we can get this information from the reader easily?

I really like this issue!!
</comment><comment author="jpountz" created="2014-11-21T18:03:28Z" id="64011568">&gt; should this be else if the doc id set is already cached a?

There are two cases here: if the doc id set is already cached then we always return the cached set. However if the filter produces a cacheable doc id set (eg. range filter or terms filter, which build bit sets), then we wait for the filter to have been used twice before caching.

I just pushed more commits in order to:
- add more tests/improve the existing ones
- no cache filters on flush segments (by looking up the source of the segment into the diagnostics)
- track most used filters on a per-index basis (while the first commit did it on a per node basis)
</comment><comment author="rjernst" created="2014-11-22T03:35:04Z" id="64067043">LGTM.
</comment><comment author="rmuir" created="2014-11-22T17:00:14Z" id="64086851">The history size is 1000, but if we imagine 30 or 60 segments, then the actual history is much much shorter because entries are tracked per-segment? This seems too small to be effective, and tracking this history per-segment only hurts instead of helping. As a workaround for now maybe the history could just be longer, based on a more succinct datastructure (e.g. int hashCode).
</comment><comment author="rmuir" created="2014-11-22T17:16:30Z" id="64087414">alternatively, since we already discard flushed segments before even tracking, then a per-segment history might really not be too much overhead and would keep things really simple. But i would still look at the memory efficiency of the history structure.
</comment><comment author="jpountz" created="2014-11-26T14:16:04Z" id="64650990">@mikemccand @rjernst I opened https://issues.apache.org/jira/browse/LUCENE-6077.
</comment><comment author="mikemccand" created="2014-11-26T14:37:59Z" id="64654125">Wonderful, thanks @jpountz!
</comment><comment author="jpountz" created="2014-12-16T16:31:07Z" id="67187531">I pushed a new commit. Most of this change has been pushed to Lucene and this pull request now only does the integration with elasticsearch. Some more informations about the change:
- only the filter hash codes are tracked for usage statistics, that said collisions are not too likely given that we consider all bits of the hash code (on the contrary to eg. hash tables)
- usage statistics are computed on a per-index basis, not per-segment
- usage statistics are computed correctly even for aliases filters which are parsed/cached only once and then reused because stats are incremented when a filter produces a DocIdSet on the first leaf on an index
- `_cache: true` now only caches on large segments in order to not pollute the cache with data that will soon need to be removed because of merging

The next step would be to replace the filter cache with the one in Lucene but there is more work that needs to be done so I'd like to do it in another PR.
</comment><comment author="rjernst" created="2014-12-16T18:55:09Z" id="67211452">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disable bloom filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8572</link><project id="" key="" /><description>See #8571 and #8564 

I make the "es090" postings format read-only, just to support old segments. There is a test version that subclasses it with write-capability for testing. 
</description><key id="49556096">8572</key><summary>Disable bloom filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>bug</label><label>PITA</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T14:15:14Z</created><updated>2015-06-07T18:02:55Z</updated><resolved>2014-11-23T13:39:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-20T14:25:06Z" id="63814943">it looks awesome - I know that we have `StoreDirectory#codecService()` that is only used by the bloom stuff, I also think we should at least note this in migration doc for `2.0` and I wonder what happens to fields that have bloom configured as their postings format - should we auto-upgrade those fields?
</comment><comment author="rmuir" created="2014-11-20T14:29:19Z" id="63815552">oh, yes this is no longer used, lemme see if i can clean it up more. 

to answer your question, es090 fields will read with the old code, but we never load blooms. we just read the delegate PF name from the .blm file.

as far as "auto-upgrade", they will be converted by normal merging and by the upgrade api in 2.0. For 1.x we would need additional logic to force the upgrade API to target these, since the lucene codec major version will not have changed, it will think they are up to date.
</comment><comment author="rmuir" created="2014-11-20T14:36:58Z" id="63816642">pushed commit removing StoreDirectory.codecService(). DirectoryUtils.getStoreDirectory is still used by other code.

As far as migration doc, I am unsure what, if any text we should add. We will be able to read the old segments so there is not much for the user to do?
</comment><comment author="s1monw" created="2014-11-20T14:41:24Z" id="63817331">&gt; As far as migration doc, I am unsure what, if any text we should add. We will be able to read the old segments so there is not much for the user to do?

All I'd add is something like this:

```
The `bloom` postingsformat has been removed in 2.0. This postingsformat can't be used in mappings anymore.
```

&gt; as far as "auto-upgrade", they will be converted by normal merging and by the upgrade api in 2.0. For 1.x we would need additional logic to force the upgrade API to target these, since the lucene codec major version will not have changed, it will think they are up to date.

sorry I was unclear... I meant if somebody has `bloom` in it's mapping we need to replace it with whatever is default since we can't write it anymore. The upgrade API doesn't do that so I think we need to add some code to the mapping that logs a warning and replaces "bloom" with whatever is our default.
</comment><comment author="rmuir" created="2014-11-20T14:44:23Z" id="63817764">I'm confused, where is this 'bloom' option in the mappings? there is no reference to it in the documentation.
</comment><comment author="s1monw" created="2014-11-20T14:56:37Z" id="63819713">bq. I'm confused, where is this 'bloom' option in the mappings? there is no reference to it in the documentation.

yeah nevermind I was talking about the SPI loader etc...
</comment><comment author="mikemccand" created="2014-11-20T15:26:32Z" id="63824647">We could maybe remove StoreDirectory entirely?  Does it have a purpose beyond passing that bloom setting?
</comment><comment author="s1monw" created="2014-11-20T15:29:11Z" id="63825074">&gt; We could maybe remove StoreDirectory entirely? Does it have a purpose beyond passing that bloom setting?

yes it does I already spoke to rob about how to remove it I think we can but it might requrie some modification to lucene too to allow associating a shard ID with and index reader / leaf reader
</comment><comment author="rmuir" created="2014-11-20T15:31:21Z" id="63825407">Mike there is other code using this stuff (unfortunately). The reason is, some code wants to know what shard a segment reader belongs to. we should think about a better way to do this, but i think its out of scope here.
</comment><comment author="mikemccand" created="2014-11-20T15:38:14Z" id="63826543">OK let's not do it here.
</comment><comment author="mikemccand" created="2014-11-20T21:20:32Z" id="63881766">LGTM
</comment><comment author="s1monw" created="2014-11-20T21:21:40Z" id="63881954">LGTM
</comment><comment author="s1monw" created="2014-11-23T12:36:43Z" id="64116462">@rmuir did you merge this? I think it can be closed?
</comment><comment author="rmuir" created="2014-11-23T13:39:04Z" id="64118163">yes, i only closed the issue, or the pull request, or whichever this one isnt. Github is a mess
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove Bloom filter and friends entirely </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8571</link><project id="" key="" /><description>We have several issues related to the bloom filter mainly related to memory consumption etc. We disabled it by default on the loading end in `1.4.0` which hasn't show any performance downsides yet. IMO we should remove the support of writing the bloomfilter entirely. In-fact since it's just delegating to another postings format we can even drop the reading code and remove this feature entirely. We should do this for `2.0` and `1.x`
</description><key id="49549800">8571</key><summary>Remove Bloom filter and friends entirely </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T13:36:56Z</created><updated>2014-11-21T02:11:06Z</updated><resolved>2014-11-21T02:11:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingFormats.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/index/codec/CodecTests.java</file><file>src/test/java/org/elasticsearch/index/codec/postingformat/DefaultPostingsFormatTests.java</file><file>src/test/java/org/elasticsearch/index/codec/postingformat/Elasticsearch090RWPostingsFormat.java</file><file>src/test/java/org/elasticsearch/index/codec/postingformat/ElasticsearchPostingsFormatTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Disable bloom filters.</comment></comments></commit></commits></item><item><title>Backport `ShardRecoveryHandler`refactorings to 1.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8570</link><project id="" key="" /><description>this is a backport of several commits and since it's a biggy I think we should do some review cycles here.
</description><key id="49547696">8570</key><summary>Backport `ShardRecoveryHandler`refactorings to 1.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.5.0</label></labels><created>2014-11-20T13:25:10Z</created><updated>2015-03-19T10:18:38Z</updated><resolved>2014-11-20T13:38:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-20T13:36:49Z" id="63808403">Left one question, other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add before/after indexDeleted callbacks to IndicesLifecycle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8569</link><project id="" key="" /><description>In order to implement #8551 correctly without causing problems of relocating
shards we need to be informed if an index is actually deleted. This commit adds
more callbacks to the listener and makes deleteIndex a dedicated method on IndicesService
</description><key id="49544366">8569</key><summary>Add before/after indexDeleted callbacks to IndicesLifecycle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T13:02:58Z</created><updated>2015-06-06T19:20:14Z</updated><resolved>2014-11-20T15:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-20T13:15:31Z" id="63805733">This looks good to me, but I think we need some serious documentation around the order in which the `InternalIndicesLifecycle` methods are called.

For example, I would have expected `beforeIndexDeleted` to be called right **after** `beforeIndexClosed`, but it's called right before the index has been closed, noting the order of operations in javadocs somewhere would be extremely helpful.
</comment><comment author="dakrone" created="2014-11-20T13:30:13Z" id="63807558">Is there a way we can add tests for this to ensure it doesn't break in a future commit?
</comment><comment author="s1monw" created="2014-11-20T14:18:48Z" id="63814053">added a new commit @dakrone 
</comment><comment author="dakrone" created="2014-11-20T14:32:11Z" id="63815960">LGTM (regardless of if you want to add the logging), thanks for the test and docs!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Note empty query string parsing difference..</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8568</link><project id="" key="" /><description>..between query_string and simple_query_string. I can't actually find
source code or docs to back up this paragraph, however it appears to be
the way things work.
</description><key id="49544108">8568</key><summary>Note empty query string parsing difference..</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dw</reporter><labels><label>feedback_needed</label></labels><created>2014-11-20T13:01:04Z</created><updated>2015-03-24T11:35:21Z</updated><resolved>2015-03-24T10:51:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-21T09:31:48Z" id="63945585">Hmm.. I'm not sure if we want to unify this behavior between `query_string` and `simple_query_string` (which would mean this is a bug), what do you think @clintongormley?
</comment><comment author="clintongormley" created="2014-11-24T13:37:35Z" id="64194284">@dw Why do you think this is true? The following:

```
GET _validate/query?explain
{
  "query": {
    "simple_query_string": {
      "fields": [
        "text"
      ],
      "query": "  "
    }
  }
}
```

returns an explanation of `MatchNoDocsQuery` for me.  Can you demonstrate the issue?

@dakrone One difference between qs and simple_qs is that a query of  `*` matches nothing in simple_qs, but matches all docs in query_string.
</comment><comment author="s1monw" created="2015-03-24T09:57:43Z" id="85428457">@dw any updates on this?
</comment><comment author="dw" created="2015-03-24T10:51:51Z" id="85448617">Sorry for the huge delay, I never got a chance to retest this and I no longer have a working ES setup to try against. Closing this for now, it seems likely based on the comments above the behaviour I observed related to my configuration rather than a trait of ES, and the issue was pretty minor to begin with.
</comment><comment author="s1monw" created="2015-03-24T11:35:21Z" id="85458270">thanks for revisiting!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Switched back to using 1.35, but getting Lucene NoSuchDirectoryException: pointing at directory under 1.4 install now deleted.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8567</link><project id="" key="" /><description>My ES installation is just a single node on localhost.
I've been experimenting (more like grinding teeth) with both mongodb_river and mongo-connector to try to get attachments/files indexed into ES. All this was previously working with ES 1.3.1 and Mongo 2.4.11.  Tried getting it running using a ES 1.4.0 / Mongo 2.6.5 combination, but facing same issues as reported here: https://github.com/richardwilly98/elasticsearch-river-mongodb/issues/412
Then tried moving back to ES 1.3.5 with requisite installs for mapper-attachment and river_mongodb, but now not even able to open the 'head' plugin (although I can see that ES has started). Part of the error is reproduced below. Why would an ES 1.3.5 installation reference a _river from my ES 1.4.0 data directory? I shut down the mongodb replicaset thinking that maybe that was the source of the problem ... but ES 1.3.5 is still not available to the head plugin because of this lucene NoSuchDirectoryException.

Any thoughts? Thanks.

[2014-11-20 04:38:22,503][DEBUG][action.admin.cluster.node.stats] [Diamond Lil] failed to execute on node [V3oubzV6THmie2DR5LSReg]
org.elasticsearch.transport.RemoteTransportException: [DJ][inet[/10.211.55.6:9300]][cluster/nodes/stats/n]
Caused by: org.elasticsearch.ElasticsearchException: io exception while building 'store stats'
    at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:542)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
    at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
    at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:212)
    at org.elasticsearch.node.service.NodeService.stats(NodeService.java:156)
    at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:96)
    at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:44)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:278)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:269)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.lucene.store.NoSuchDirectoryException: directory '/home/mccoole/Development/Tools/ES/elasticsearch-1.4.0/data/elasticsearch/nodes/0/indices/_river/0/index' does not exist
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
    at org.apache.lucene.store.FileSwitchDirectory.listAll(FileSwitchDirectory.java:87)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.common.lucene.Directories.estimateSize(Directories.java:40)
    at org.elasticsearch.index.store.Store.stats(Store.java:213)
    at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:540)
</description><key id="49542792">8567</key><summary>Switched back to using 1.35, but getting Lucene NoSuchDirectoryException: pointing at directory under 1.4 install now deleted.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Analect</reporter><labels /><created>2014-11-20T12:50:09Z</created><updated>2014-11-24T13:54:58Z</updated><resolved>2014-11-24T13:54:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ZeAleks" created="2014-11-20T14:56:11Z" id="63819649">Hello Analect,

I'm facing some performance issue with 1.4 version of Elasticsearch. So I was wondering if I should rollback the installation.
But I red that : http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-upgrade.html
1.4 is using new version of lucene. So, maybe, new documents that you inserted has new formats and cannot be used anymore with 1.3 version of Elasticsearch.

If this is the reason for your exception, you have two choices :
- run 1.4 and try to correct what can be
- reindex everything

Keep us in touch.

Good luck.
Alek
</comment><comment author="Analect" created="2014-11-20T15:37:52Z" id="63826491">@ZeAleks 
Thanks for your response.
I don't think I'm affected by what you describe ... as I'm even having this issue on a blank database when I first try to start up a version of ES. I'm not even trying to move around an existing database between versions ... which is why I'm puzzled by why one version of ES (ie. 1.3.5) would want to be referencing a _river that had been used in testing another version (1.4.0).  The only explanation I can think of is that it somehow pulled down some reference to the river from a running mongodb replica-set, but I don't know enough about how the mechanism works to be sure if that is a possibility. Even with that replicaset closed down, the same Exception was occurring on the startup of version 1.3.5, where the 'head' plugin is left hanging.
</comment><comment author="s1monw" created="2014-11-20T15:43:31Z" id="63827473">@Analect do you have the 1.4 instance still running by any chance? 

@ZeAleks what kind of perf issues are you seeing?
</comment><comment author="ZeAleks" created="2014-11-20T15:54:54Z" id="63829456">@s1monw Here is the corresponding post for my perf issues : https://github.com/elasticsearch/elasticsearch/issues/8553
</comment><comment author="Analect" created="2014-11-20T15:55:12Z" id="63829516">@s1monw No, I make sure I only have a single version running at any time to avoid any possibility of conflicts. For the 1.4 version, because I was having a problem getting that running again (I can't recall the detail .. but it might have been something related to the river_mongodb too), I ended up deleting the elasticsearch folder under /data, in expectation that this would clear things up and allow me to start with a new blank database. It obviously hadn't the desired effect ... and then I had this related problem in 1.3.5, where it was looking for a _river definition in that folder I had deleted under the 1.4 installation ... eventhough I had only installed 1.3.5 and hadn't even added any mappings to it yet ... and certainly not any _river mappings.
</comment><comment author="Analect" created="2014-11-20T16:09:36Z" id="63832138">@s1monw This is odd.
I had completely removed ES 1.4.0 from my machine
I downloaded a new version of the tar and expanded it.
The ES starts fine:
{
  "status" : 200,
  "name" : "Mr. Justice",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.4.0",
    "build_hash" : "bc94bd81298f81c656893ab1ddddd30a99356066",
    "build_timestamp" : "2014-11-05T14:26:12Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.2"
  },
  "tagline" : "You Know, for Search"
}

However, 'head' is not working ... for this reason, it appears:
https://github.com/mobz/elasticsearch-head/issues/170

What I find really odd though, is that when I try to navigate to 'head' (not yet fixing it as per the link above), I still get a reference to a _river (see last line of logs below) ... eventhough on this new installation of version 1.4, the only plugin I have installed is 'head'.

[2014-11-20 08:06:13,551][INFO ][node                     ] [Louise Mason] version[1.4.0], pid[8512], build[bc94bd8/2014-11-05T14:26:12Z]
[2014-11-20 08:06:13,552][INFO ][node                     ] [Louise Mason] initializing ...
[2014-11-20 08:06:13,556][INFO ][plugins                  ] [Louise Mason] loaded [], sites [head]
[2014-11-20 08:06:15,541][INFO ][node                     ] [Louise Mason] initialized
[2014-11-20 08:06:15,542][INFO ][node                     ] [Louise Mason] starting ...
[2014-11-20 08:06:15,631][INFO ][transport                ] [Louise Mason] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.211.55.6:9301]}
[2014-11-20 08:06:15,642][INFO ][discovery                ] [Louise Mason] elasticsearch/jNaGt-FdSLOtJzOrzbVF8A
[2014-11-20 08:06:18,685][INFO ][cluster.service          ] [Louise Mason] detected_master [DJ][V3oubzV6THmie2DR5LSReg][ubuntu][inet[/10.211.55.6:9300]], added {[DJ][V3oubzV6THmie2DR5LSReg][ubuntu][inet[/10.211.55.6:9300]],}, reason: zen-disco-receive(from master [[DJ][V3oubzV6THmie2DR5LSReg][ubuntu][inet[/10.211.55.6:9300]]])
[2014-11-20 08:06:18,721][INFO ][http                     ] [Louise Mason] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.211.55.6:9200]}
[2014-11-20 08:06:18,722][INFO ][node                     ] [Louise Mason] started
[2014-11-20 08:06:25,859][DEBUG][action.admin.cluster.node.stats] [Louise Mason] failed to execute on node [V3oubzV6THmie2DR5LSReg]
org.elasticsearch.transport.RemoteTransportException: [DJ][inet[/10.211.55.6:9300]][cluster:monitor/nodes/stats[n]]
Caused by: org.elasticsearch.ElasticsearchException: io exception while building 'store stats'
    at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:542)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
    at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
    at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:212)
    at org.elasticsearch.node.service.NodeService.stats(NodeService.java:156)
    at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:96)
    at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:44)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:278)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:269)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.lucene.store.NoSuchDirectoryException: directory '/home/mccoole/Development/Tools/ES/elasticsearch-1.4.0/data/elasticsearch/nodes/0/indices/_river/0/index' does not exist
</comment><comment author="Analect" created="2014-11-20T16:27:40Z" id="63835443">@s1monw 
FYI, I made the fix for 'head', as per issue 170 linked above.  However, head still won't work and so it might be related to the NoSuchDirectoryException I'm having. Am I right in understanding that there's a separate lucene instance associated with each ES database?
</comment><comment author="Analect" created="2014-11-21T10:14:48Z" id="63950766">@s1monw 
Any suggestions how I can fix this org.apache.lucene.store.NoSuchDirectoryException. I'm not concerned with losing data, as this was a blank database ... I just want to be able to get ES 1.4 up and running again and visible on head ... but somehow (I think!) this directory exception is preventing that on my linux machine. As explained above, I've tried a fresh install, but that doesn't appear to fix, as it still searches for this rogue _river/0/index directory that was deleted long ago. On my windows machine, the 1.4 install with the CORS fix in the *.yml allows me to get head working, so I'm stumped as to why I can't get the linux one working.
</comment><comment author="clintongormley" created="2014-11-24T13:54:53Z" id="64196324">@analect you are definitely running more than one node:

```
[2014-11-20 08:06:18,685][INFO ][cluster.service ] [Louise Mason] detected_master [DJ]
```

and they're running on the same machine. You can see that this new node started with ports 9301/9201, not 9300/9200

I suggest you do a `killall java` and start again :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove reference to imaginary "no_docs_query"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8566</link><project id="" key="" /><description>No reference to it in the source code except this file.
</description><key id="49542430">8566</key><summary>Remove reference to imaginary "no_docs_query"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dw</reporter><labels /><created>2014-11-20T12:47:17Z</created><updated>2014-11-23T12:56:52Z</updated><resolved>2014-11-23T12:56:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-23T12:41:43Z" id="64116580">I like this @clintongormley WDYT?
</comment><comment author="clintongormley" created="2014-11-23T12:56:52Z" id="64117025">I like it. thanks @dw - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Remove reference to imaginary "no_docs_query"</comment></comments></commit></commits></item><item><title>Missing quote in the example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8565</link><project id="" key="" /><description /><key id="49529263">8565</key><summary>Missing quote in the example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">barbasa</reporter><labels><label>docs</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-20T10:57:58Z</created><updated>2014-11-23T13:05:45Z</updated><resolved>2014-11-23T13:05:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-23T12:43:53Z" id="64116641">I know it might sound awful but can you sign the CLA http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="barbasa" created="2014-11-23T13:01:31Z" id="64117180">Done!

2014-11-23 12:44 GMT+00:00 Simon Willnauer notifications@github.com:

&gt; I know it might sound awful but can you sign the CLA
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/8565#issuecomment-64116641
&gt; .
</comment><comment author="s1monw" created="2014-11-23T13:05:45Z" id="64117309">pushed thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>bloom filter overestimates ram usage by order of magnitude</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8564</link><project id="" key="" /><description>I found this by dropping bloom filter code into lucene test suite. There are consistent failures:

java.lang.AssertionError: Actual RAM usage 136880, but got 915568, -568.8836937463471% error

I think we should just stop writing these?
</description><key id="49486184">8564</key><summary>bloom filter overestimates ram usage by order of magnitude</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>PITA</label></labels><created>2014-11-20T03:13:56Z</created><updated>2014-11-20T22:00:12Z</updated><resolved>2014-11-20T22:00:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/BloomFilter.java</file><file>src/test/java/org/elasticsearch/index/codec/postingformat/ElasticsearchPostingsFormatTest.java</file></files><comments><comment>[BLOOM] Fix Bloom filter ram usage calculation</comment></comments></commit></commits></item><item><title>bin/plugin output for empty plugins isn't machine-friendly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8563</link><project id="" key="" /><description>The default output when you don't have any plugins:

```
% bin/plugin --list
Installed plugins:
    - No plugin detected in /usr/local/elasticsearch/elasticsearch-2.0.0-SNAPSHOT/plugins
```

looks too much like when you _do_ have plugins:

```
% bin/plugin --list
Installed plugins:
    - kibana
```

Can we just have a `\n`-separated list with nothing else?  And just no output if there aren't any installed?  Like so:

```
% bin/plugin --list
%
% bin/plugin -i elasticsearch/kibana/latest
% bin/plugin --list
kibana
%
```

At the very least can we change `- No plugin ...` to not have the same syntax as a plugin that _is_ installed?
</description><key id="49464751">8563</key><summary>bin/plugin output for empty plugins isn't machine-friendly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-19T22:49:25Z</created><updated>2016-11-06T07:42:37Z</updated><resolved>2016-11-06T07:42:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nellicus" created="2015-06-10T08:51:55Z" id="110659086">@tlrx 

can we have this to work in conjunction with --verbose?
today there is no diff

```
abonuccelli@w530 /opt/elk/PRODSEC/node1/elasticsearch-1.6.0/bin $ ./plugin -l
Installed plugins:
    - watcher
    - marvel
    - license
    - shield


abonuccelli@w530 /opt/elk/PRODSEC/node1/elasticsearch-1.6.0/bin $ ./plugin -l -v
Installed plugins:
    - watcher
    - marvel
    - license
    - shield
```

Can we have -v to print version and location of each plugin 

@drewr I disagree on "no output if there aren't any installed? " this is very user unfriendly.
</comment><comment author="drewr" created="2015-06-10T11:19:08Z" id="110704589">If you type `ls` in an empty directory, you get no output.  If you list plugins and there aren't any installed why is no output not a valid response?  It's what makes things like `for i in $(bin/plugin -l); do ... done`  work.
</comment><comment author="clintongormley" created="2016-11-06T07:42:37Z" id="258665579">Fixed in 5.0.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test opening very old indexes fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8562</link><project id="" key="" /><description>On master (ie 2.0) loading indexes based on lucene 3.x (ie 0.20) will fail with an `IndexFormatTooOldException`.  However, our static index bwc tests don't check this, because the exception is hidden away during recovery as the cluster starts up.  We should some how propagate this error back up to verify the index is in fact not openable.  Similar lucene bwc tests exists for very old indexes.
</description><key id="49463134">8562</key><summary>Test opening very old indexes fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>adoptme</label><label>test</label></labels><created>2014-11-19T22:34:14Z</created><updated>2015-12-08T07:50:08Z</updated><resolved>2015-12-08T07:50:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>native script custom explain feature removed in elasticsearch 1.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8561</link><project id="" key="" /><description>I see that in issue #7245 , the interface ExplainableSearchScript and its use in ScriptScoreFunction is removed. I use that feature to customize (add all attributes in addition to query and parameters i use  to compute the score in my script) the explain plan of my custom native script. It was fairly useful. 

I am wondering why was it removed. Is there a different way to add my custom details to the explanation of script function ?
</description><key id="49455209">8561</key><summary>native script custom explain feature removed in elasticsearch 1.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">ursvasan</reporter><labels><label>:Search</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-19T21:23:43Z</created><updated>2015-02-23T16:56:35Z</updated><resolved>2015-01-19T13:08:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-11-25T17:10:29Z" id="64435062">I was eager to remove code and apparently got a little overboard. I will make a pr shortly to add it again. I currently see now other way to add details from a search script to the explanation.
</comment><comment author="ghiron" created="2015-01-19T16:56:48Z" id="70525983">@brwe : Could you add it back to v1.4 branch too please ?
</comment><comment author="brwe" created="2015-01-19T17:20:44Z" id="70529478">@ghiron ok, pushed 5b031f67b3dbc908f03a2cbc6ba9e0d7fcd08630 to 1.4
</comment><comment author="ghiron" created="2015-01-19T17:23:17Z" id="70529868">thank you @brwe !
</comment><comment author="ursvasan" created="2015-02-13T04:35:53Z" id="74204112">thanks for adding it back @brwe. LGTM 
</comment><comment author="maxjakob" created="2015-02-23T14:14:02Z" id="75546987">Since [ExplainableSearchScript.explain now takes a `float`](https://github.com/elasticsearch/elasticsearch/issues/8561) it is not possible any more to print the full tree of explanations.
@brwe Is there a plan to revert #7245 completely?
</comment><comment author="brwe" created="2015-02-23T16:56:35Z" id="75582549">@maxjakob there is now: https://github.com/elasticsearch/elasticsearch/pull/9826
Can you check if that is what you need?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java</file><file>src/main/java/org/elasticsearch/script/ExplainableSearchScript.java</file><file>src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptPlugin.java</file><file>src/test/java/org/elasticsearch/search/functionscore/ExplainableScriptTests.java</file></files><comments><comment>scripts: add explainable script again</comment></comments></commit></commits></item><item><title>How to register streams for custom aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8560</link><project id="" key="" /><description>I am developing a custom metric aggregator using ES 1.3.4 similar to the Stats aggregator. On a 2 node cluster, when I execute a query with this custom aggregation, I get the following warning in ES logs:

```
[2014-11-20 00:50:14,319][WARN ][transport.netty          ] [Blue Streak] Message not fully read (response) for [313] handler org.elasticsearch.search.action.SearchServiceTransportAction$6@27f233b2, error [false], resetting
```

On debugging, I found that a NPE is thrown by the following line:

```
InternalAggregation aggregation = AggregationStreams.stream(type).readResult(in);
```

in `InternalAggregations.java`. 

Seems like I've not registered streams for my custom aggregation and I am not able to figure a way to do it. FYI, I do not have any module defined for my custom aggregation. I am simply adding a custom `AggregatorParser` in `onModule(AggregationModule)` method. This method is defined in a class that extends `AbstractPlugin`. Can you help me register streams for my custom aggregation?
</description><key id="49442287">8560</key><summary>How to register streams for custom aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels /><created>2014-11-19T19:32:34Z</created><updated>2014-11-30T07:35:35Z</updated><resolved>2014-11-30T07:35:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bittusarkar" created="2014-11-20T09:42:34Z" id="63783132">For further clarity, I'll mention the code structure of my custom aggregation below:

```
public class CustomPlugin extends AbstractPlugin {
    ...
    public static void onModule(AggregationModule aggregationModule) {
        aggregationModule.addAggregatorParser(CustomAggregationParser.class);
    }
    ...
}

class InternalCustomAggregation extends InternalNumericMetricsAggregation.MultiValue implements CustomAggregation {
    ...
    public static final AggregationStreams.Stream STREAM = new AggregationStreams.Stream() {
        @Override
        public InternalCustomAggregation readResult(StreamInput in) throws IOException {
            InternalCustomAggregation result = new InternalCustomAggregation();
            result.readFrom(in);
            return result;
        }
    };

    public static void registerStreams() {
        AggregationStreams.registerStream(STREAM, TYPE.stream());
    }
    ...
}

interface CustomAggregation extends Aggregation {
    ...
}

class CustomAggregator extends NumericMetricsAggregator.MultiValue {
    ...
}

class CustomAggregationBuilder extends ValuesSourceMetricsAggregationBuilder&lt;CustomAggregationBuilder&gt; {
    ...
}

class CustomAggregationParser implements Aggregator.Parser {
    ...
}
```

I could not find a way to get the method `InternalPayloads.registerStreams()` called by ES for registering streams for my custom aggregation. Is there something that I am missing here? Let me know if you need more information.
</comment><comment author="bittusarkar" created="2014-11-30T07:35:35Z" id="64978231">I finally figured it out. The solution was rather very simple. I updated the `onModule()` method as below:

```
public static void onModule(AggregationModule aggregationModule) {
    aggregationModule.addAggregatorParser(CustomAggregationParser.class);
    InternalCustomAggregation.registerStreams();
}
```

I came to this solution by reading the `Elasticsearch Cardinality Plugin` code at https://github.com/algolia/elasticsearch-cardinality-plugin/tree/1.0.X (see branch `1.0.X` and not `master`)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Script Context (ctx) missing documentation and no access to _version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8559</link><project id="" key="" /><description>I am trying to change a document field ONLY if a document is currently at a specific version AND leave the version itself untouched. I assumed being able to use `version_type=force` and script something like `if(ctx._version==NUMBER){ DO_SOMETHING;}`.

Sadly that does not work and sadly i cannot even find any kind of documentation as to what I can expect to find in the script context. Looking through examples i did find out there is at least `ctx._source` and `ctx.op` other than that i cannot find anything in the documentation.

Can anybody shed some light on this ?
</description><key id="49436909">8559</key><summary>Script Context (ctx) missing documentation and no access to _version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">matthiasg</reporter><labels /><created>2014-11-19T18:47:34Z</created><updated>2014-12-23T01:19:21Z</updated><resolved>2014-11-20T21:31:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-20T16:07:12Z" id="63831693">We recently merged https://github.com/elasticsearch/elasticsearch/pull/5724 to add more context variables and document what they are, it will be part of 1.5.0
</comment><comment author="matthiasg" created="2014-11-20T21:31:29Z" id="63883515">@dakrone that looks good .. Hope it will be released soon then.
</comment><comment author="xmarcos" created="2014-12-22T15:56:36Z" id="67852905">@dakrone thanks for the update, is there any place (roadmap?) where we can check or be informed when 1.5 will be released? Thanks!
</comment><comment author="dakrone" created="2014-12-22T16:34:15Z" id="67857914">@xmarcos there isn't a solid roadmap for when 1.5 will be released unfortunately. The easiest way to check is to check (through RSS most likely) the [elasticsearch blog](http://www.elasticsearch.org/blog/)
</comment><comment author="xmarcos" created="2014-12-23T01:19:21Z" id="67912139">@dakrone will do! Thanks for the quick response :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Offsets spuriously lost from field after indexing a separate type with matching field name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8558</link><project id="" key="" /><description>If there are two mappings within the index that have a matching field name - one configured with and  one without `index_options: offsets` then we can add a document of the first type (i.e. the mapping with offsets), and query it successfully using the postings highlighter.

But after adding several documents of the second type (i.e. the mapping without offsets), the same query (returning the same single original document only) fails with the exception:

`IllegalArgumentException[field 'title' was indexed without offsets, cannot highlight`

This failure takes about 10 seconds to reproduce when using refresh_interval: 1s, and indexing the documents with a 1s pause between documents. This suggests that perhaps it is something in the index refresh processing that is getting confused between the mappings when updating the index.

The failure only happens after adding several extra documents of the second type over a period spanning several refresh_intervals. Attempts to recreate the issue with a longer refresh_interval resulted in many more documents of the second type having to be added before the issue arose.

The following gist is a short shell script that demonstrates this issue with elasticsearch 1.4.0:

https://gist.github.com/robinhughes/4b1f381a94dd905d71a6
</description><key id="49427641">8558</key><summary>Offsets spuriously lost from field after indexing a separate type with matching field name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robinhughes</reporter><labels /><created>2014-11-19T17:38:37Z</created><updated>2014-11-24T11:33:33Z</updated><resolved>2014-11-24T11:33:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T11:33:33Z" id="64181245">Hi @robinhughes 

You're running into a known issue: fields with the same name in different types must have the same mapping.  In the next version we're planning on enforcing this requirement.  Closing in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IndexService - synchronize  close to prevent race condition with shard creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8557</link><project id="" key="" /><description>During node shutdown we have a race condition between processing cluster state updates (creating shards) and closing down the index service. This may cause shards to leak and not be closed properly.

 This commit removes the concurrency in shard closing as InternalIndexService.removeShard has been synchronized for a long time now.

 On the other hand, the commit restores the parallel shutdown of indices lost in 7e1d8a6ca3b23c07cbaebe72831c7ef4a201bd2b

This is the cause of http://build-us-00.elasticsearch.org/job/es_core_1x_strong/1406/testReport/junit/org.elasticsearch.gateway.local/LocalGatewayIndexStateTests/testDanglingIndicesNoAutoImport/

```
java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open locks: {write.lock=java.lang.RuntimeException: lock "write.lock" was not released}

Caused by: java.lang.RuntimeException: lock "write.lock" was not released
    at org.apache.lucene.store.MockLockFactoryWrapper$MockLock.obtain(MockLockFactoryWrapper.java:74)
    at org.apache.lucene.store.Lock.obtain(Lock.java:77)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:753)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1453)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:733)
```
</description><key id="49427519">8557</key><summary>IndexService - synchronize  close to prevent race condition with shard creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-19T17:37:48Z</created><updated>2015-06-07T18:19:28Z</updated><resolved>2014-11-19T20:33:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-19T20:05:13Z" id="63704160">left a tiny comment LGTM otherwise
</comment><comment author="bleskes" created="2014-11-19T20:33:29Z" id="63708460">Tiny comment applied. Thx!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file></files><comments><comment>Internal: IndexService - synchronize close to prevent race condition with shard creation</comment></comments></commit></commits></item><item><title>DateMath: Fix semantics of rounding with inclusive/exclusive ranges.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8556</link><project id="" key="" /><description>Date math rounding currently works by rounding the date up or down based
on the scope of the rounding.  For example, if you have the date
`2009-12-24||/d` it will round down to the inclusive lower end
`2009-12-24T00:00:00.000` and round up to the non-inclusive date
`2009-12-25T00:00:00.000`.

The range endpoint semantics work as follows:
- `gt` - round D down, and use &gt; that value
- `gte` - round D down, and use &gt;= that value
- `lt` - round D down, and use &lt;
- `lte` - round D up, and use &lt;=

There are 2 problems with these semantics:
- `lte` ends up including the upper value, which should be non-inclusive
- `gt` only excludes the beginning of the date, not the entire rounding scope

This change makes the range endpoint semantics symmetrical.  First, it
changes the parser to round up and down using the first (same as before)
and last (1 ms less than before) values of the rounding scope.  This
makes both rounded endpoints inclusive. The range endpoint semantics
are then as follows:
- `gt` - round D up, and use &gt; that value
- `gte` - round D down, and use &gt;= that value
- `lt` - round D down, and use &lt; that value
- `lte` - round D up, and use &lt;= that value

closes #8424
</description><key id="49426625">8556</key><summary>DateMath: Fix semantics of rounding with inclusive/exclusive ranges.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-19T17:32:27Z</created><updated>2015-01-21T23:22:20Z</updated><resolved>2014-11-21T17:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-20T15:53:23Z" id="63829173">LGTM
</comment><comment author="jpountz" created="2014-11-21T09:12:04Z" id="63943498">LGTM^2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RangeQueryParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file><file>src/test/java/org/elasticsearch/count/simple/SimpleCountTests.java</file><file>src/test/java/org/elasticsearch/exists/SimpleExistsTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/search/simple/SimpleSearchTests.java</file></files><comments><comment>Settings: Remove `mapping.date.round_ceil` setting for date math parsing</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Fix test failures caused by #8556</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file></files><comments><comment>Fix compile error from bad merge in #8556</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file></files><comments><comment>DateMath: Fix semantics of rounding with inclusive/exclusive ranges.</comment></comments></commit></commits></item><item><title>Allow to cancel recovery sources when shards are closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8555</link><project id="" key="" /><description>Today recovery sources are not cancled if a shard is closed. The recovery target
is already cancled when shards are closed but we should also cleanup and cancel
the sources side since it holds on to shard locks / references until it's closed.
</description><key id="49421782">8555</key><summary>Allow to cancel recovery sources when shards are closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-19T16:59:52Z</created><updated>2015-06-07T11:56:11Z</updated><resolved>2014-11-19T21:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-19T20:58:56Z" id="63712299">@dakrone pushed a new commit
</comment><comment author="bleskes" created="2014-11-19T21:32:40Z" id="63717667">I left some cosmetics related comments. O.w. LGTM. I lovel the CancelableThreads - will come handy in other places.
</comment><comment author="s1monw" created="2014-11-19T22:14:15Z" id="63724149">haven't ported this to 1.5 yet... 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch Cluster is not recovering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8554</link><project id="" key="" /><description>Hello,

After upgrading our Production cluster to Version 1.3.5 , the cluster is  constantly  in red state with :
{
  "cluster_name" : "totango_prod_hist",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 26,
  "number_of_data_nodes" : 20,
  "active_primary_shards" : 100,
  "active_shards" : 200,
  "relocating_shards" : 0,
  "initializing_shards" : 2,
  "unassigned_shards" : 2 
}

When i  went over the logs i found this exception:

2014-11-19 16:21:01,253][WARN ][index.engine.internal    ] [elasticsearch-prod-hist08] [2014_11][5] failed engine [refresh failed]
[2014-11-19 16:21:01,401][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist08] [2014_11][5] sending failed shard for [2014_11][5], node[fiY35nZBRLOZzQ_AUBOcQA], [P], s[INITIALIZING], indexUUID [Ms7CtS7kRg6pth9YXgPSEg], reason [engine failure, message [refresh failed][IllegalStateException[parentFilter must return FixedBitSet; got org.apache.lucene.search.BitsFilteredDocIdSet@739f95fa]]]
[2014-11-19 16:23:52,294][WARN ][index.engine.internal    ] [elasticsearch-prod-hist08] [2014_11][5] failed engine [refresh failed]
[2014-11-19 16:23:52,569][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist08] [2014_11][5] sending failed shard for [2014_11][5], node[fiY35nZBRLOZzQ_AUBOcQA], [P], s[INITIALIZING], indexUUID [Ms7CtS7kRg6pth9YXgPSEg], reason [engine failure, message [refresh failed][IllegalStateException[parentFilter must return FixedBitSet; got org.apache.lucene.search.BitsFilteredDocIdSet@503839bb]]]
[2014-11-19 16:26:42,678][WARN ][index.engine.internal    ] [elasticsearch-prod-hist08] [2014_11][5] failed engine [refresh failed]
[2014-11-19 16:26:42,934][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist08] [2014_11][5] sending failed shard for [2014_11][5], node[fiY35nZBRLOZzQ_AUBOcQA], [P], s[INITIALIZING], indexUUID [Ms7CtS7kRg6pth9YXgPSEg], reason [engine failure, message [refresh failed][IllegalStateException[parentFilter must return FixedBitSet; got org.apache.lucene.search.BitsFilteredDocIdSet@3674e0a0]]]
[2014-11-19 16:29:32,599][WARN ][index.engine.internal    ] [elasticsearch-prod-hist08] [2014_11][5] failed engine [refresh failed]
[2014-11-19 16:29:32,855][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist08] [2014_11][5] sending failed shard for [2014_11][5], node[fiY35nZBRLOZzQ_AUBOcQA], [P], s[INITIALIZING], indexUUID [Ms7CtS7kRg6pth9YXgPSEg], reason [engine failure, message [refresh failed][IllegalStateException[parentFilter must return FixedBitSet; got org.apache.lucene.search.BitsFilteredDocIdSet@5a27fe79]]]
[2014-11-19 16:32:23,194][WARN ][index.engine.internal    ] [elasticsearch-prod-hist08] [2014_11][5] failed engine [refresh failed]
[2014-11-19 16:32:23,442][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist08] [2014_11][5] sending failed shard for [2014_11][5], node[fiY35nZBRLOZzQ_AUBOcQA], [P], s[INITIALIZING], indexUUID [Ms7CtS7kRg6pth9YXgPSEg], reason [engine failure, message [refresh failed][IllegalStateException[parentFilter must return FixedBitSet; got org.apache.lucene.search.BitsFilteredDocIdSet@39c4890a]]]

Please advise ?

Thanks,

Costya.
</description><key id="49418044">8554</key><summary>Elasticsearch Cluster is not recovering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cregev</reporter><labels /><created>2014-11-19T16:34:45Z</created><updated>2015-01-20T10:00:39Z</updated><resolved>2014-11-19T20:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-19T20:04:25Z" id="63704004">Hi Costya,

This is a duplicate of #8390 , which was solved and will be released with 1.3.6. As a sanity check - I assume you are using nested docs and delete by query, correct? If not please reopen.

Thanks for reporting it!
</comment><comment author="Yakx" created="2014-11-19T20:10:58Z" id="63705112">exactly
</comment><comment author="ron-totango" created="2014-11-19T20:11:12Z" id="63705141">So what's the workaround?
The cluster is red now, we can't wait for 1.3.6
</comment><comment author="bleskes" created="2014-11-19T20:23:51Z" id="63707027">you can delete the translog files on the shards that are stuck. That will cause the shard to successfully recover as they won't need to run the delete by query again. Note that you may have slight data loss, if unlucky. 
</comment><comment author="ron-totango" created="2014-11-19T20:25:08Z" id="63707221">how to you delete the translog files?
</comment><comment author="bleskes" created="2014-11-19T20:35:53Z" id="63708844">you have to do it manually by deleting all of the files in folder like {clustername}/nodes/{id}/indices/{index_name}/{shard}/translog/  under your data folders.
</comment><comment author="ron-totango" created="2014-11-19T20:47:51Z" id="63710588">Thanks for the help
</comment><comment author="cregev" created="2014-11-19T20:53:48Z" id="63711469">Thank you for the help.
</comment><comment author="dimzak" created="2015-01-19T14:55:30Z" id="70505591">Sorry for the late post.
We have the same problem after performing a ''Delete by query'' (https://github.com/elasticsearch/elasticsearch/pull/8390).  Deletion of translogs made cluster green again but we still need to delete those documents. 
Is there a workaround for that?
</comment><comment author="clintongormley" created="2015-01-20T10:00:39Z" id="70630170">@dimzak once you've upgraded, you should be able to go ahead and use delete-by-query again
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Poor indexing performance with 1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8553</link><project id="" key="" /><description>Hello,

I just upgraded my 1.3.2 cluster with 1.4.0 version of Elasticsearch.
As I have some performance counter on some indexing I run, I immediately saw that performances was really bad now.

![image](https://cloud.githubusercontent.com/assets/4868394/5109222/2efda50e-7010-11e4-9d98-25622741941c.png)

I don't now where and what to look to find out where is the problem.
I didn't change anything else than the Elasticsearch version.

Does someone have and idea about what can cause this issue ?

Thank you.
Alek
</description><key id="49415959">8553</key><summary>Poor indexing performance with 1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ZeAleks</reporter><labels /><created>2014-11-19T16:21:19Z</created><updated>2014-11-26T16:12:24Z</updated><resolved>2014-11-21T10:04:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-20T15:57:58Z" id="63830008">Alex can you explain what your graph shows?
</comment><comment author="ZeAleks" created="2014-11-20T16:08:41Z" id="63831955">Yes.
I have some code that index documents (like an ETL).
Every single column is the time taken by one bulk index.

The most revelant and important for me are greens one. About every 10 minutes, I get a CSV file that I parse and index every single lines. I bulk insert 1 000 lines at a time.
So every green column represent the time taken to index 1 000 lines.
</comment><comment author="mikemccand" created="2014-11-20T16:11:52Z" id="63832531">Are you adding new documents, or updating them (replacing previous versions)?  Small or large docs? See this blog post for ways to speed up indexing: http://www.elasticsearch.org/blog/performance-considerations-elasticsearch-indexing/
</comment><comment author="ZeAleks" created="2014-11-20T16:29:44Z" id="63835799">Hello Michael,
Thank you for your replay.

I do updating existing nested documents to be exact.
The problem is that everything was fine until I upgraded Elasticsearch.
</comment><comment author="eliasah" created="2014-11-20T16:32:16Z" id="63836283">It might be an error in the visualization framework. You might want to use a load tester like Tsung or Apache JMeter to benchmark your indexing. That would give more insight about where there bottleneck is. Since we can't reproduce your performance issues as for now that would be a good start.
</comment><comment author="mikemccand" created="2014-11-20T17:25:58Z" id="63845557">Can you capture hot threads while bulk-indexing one of your set of 1000 docs and post here?
</comment><comment author="mikemccand" created="2014-11-20T17:29:48Z" id="63846162">Also are you using the default index.refresh_interval (1 second)?
</comment><comment author="ZeAleks" created="2014-11-20T19:00:36Z" id="63860257">Michael,

Everytime I start indexing a file, I set refresh_interval to "60s". "-1" was not working for me.

Here is a capture of hot thread (GET /_nodes/hot_threads) :

&lt;tabl&gt;
&lt;tr&gt;&lt;td&gt;::: [S3DEV-BI-ES07][rii902ScRAyuAMiogRhTTQ][S3DEV-BI-ES07][inet[/10.199.67.45:9300]]{master=false}&lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
   49.4% (246.7ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES07][bulk][T#2]'
     2/10 snapshots sharing following 28 elements
       java.lang.ClassLoader.defineClass1(Native Method)
       java.lang.ClassLoader.defineClass(ClassLoader.java:800)
       java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
       groovy.lang.GroovyClassLoader.access$300(GroovyClassLoader.java:56)
       groovy.lang.GroovyClassLoader$ClassCollector.createClass(GroovyClassLoader.java:478)
       groovy.lang.GroovyClassLoader$ClassCollector.onClassNode(GroovyClassLoader.java:495)
       groovy.lang.GroovyClassLoader$ClassCollector.call(GroovyClassLoader.java:499)
       org.codehaus.groovy.control.CompilationUnit$16.call(CompilationUnit.java:814)
       org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1047)
       org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:583)
       org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:561)
       org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:538)
       groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:286)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:259)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     5/10 snapshots sharing following 15 elements
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:257)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 38 elements
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:637)
       org.codehaus.groovy.control.ResolveVisitor.transformBinaryExpression(ResolveVisitor.java:902)
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:641)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitExpressionStatement(ClassCodeExpressionTransformer.java:139)
       org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
       org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:163)
       org.codehaus.groovy.control.ResolveVisitor.visitBlockStatement(ResolveVisitor.java:1259)
       org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitConstructorOrMethod(ClassCodeExpressionTransformer.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitConstructorOrMethod(ResolveVisitor.java:167)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
       org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1063)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitClass(ResolveVisitor.java:1202)
       org.codehaus.groovy.control.ResolveVisitor.startResolving(ResolveVisitor.java:142)
       org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:643)
       org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:923)
       org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:585)
       org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:534)
       groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:286)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:259)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
   35.1% (175.3ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES07][bulk][T#1]'
     5/10 snapshots sharing following 19 elements
       groovy.lang.GroovyClassLoader.clearCache(GroovyClassLoader.java:952)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.scriptRemoved(GroovyScriptEngineService.java:97)
       org.elasticsearch.script.ScriptService$ScriptCacheRemovalListener.onRemoval(ScriptService.java:509)
       org.elasticsearch.common.cache.LocalCache.processPendingNotifications(LocalCache.java:1956)
       org.elasticsearch.common.cache.LocalCache$Segment.runUnlockedCleanup(LocalCache.java:3460)
       org.elasticsearch.common.cache.LocalCache$Segment.postWriteCleanup(LocalCache.java:3436)
       org.elasticsearch.common.cache.LocalCache$Segment.put(LocalCache.java:2891)
       org.elasticsearch.common.cache.LocalCache.put(LocalCache.java:4149)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.put(LocalCache.java:4754)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:342)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 56 elements
       java.security.AccessController.doPrivileged(Native Method)
       java.net.URLClassLoader.findClass(URLClassLoader.java:354)
       java.lang.ClassLoader.loadClass(ClassLoader.java:425)
       sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
       java.lang.ClassLoader.loadClass(ClassLoader.java:412)
       groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:655)
       groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:523)
       org.codehaus.groovy.control.ClassNodeResolver.tryAsLoaderClassOrScript(ClassNodeResolver.java:183)
       org.codehaus.groovy.control.ClassNodeResolver.findClassNode(ClassNodeResolver.java:168)
       org.codehaus.groovy.control.ClassNodeResolver.resolveName(ClassNodeResolver.java:124)
       org.codehaus.groovy.control.ResolveVisitor.resolveToOuter(ResolveVisitor.java:617)
       org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:269)
       org.codehaus.groovy.control.ResolveVisitor.resolveFromStaticInnerClasses(ResolveVisitor.java:363)
       org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:269)
       org.codehaus.groovy.control.ResolveVisitor.resolveFromModule(ResolveVisitor.java:579)
       org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:269)
       org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:237)
       org.codehaus.groovy.control.ResolveVisitor.transformPropertyExpression(ResolveVisitor.java:774)
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:637)
       org.codehaus.groovy.control.ResolveVisitor.transformBinaryExpression(ResolveVisitor.java:902)
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:641)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitExpressionStatement(ClassCodeExpressionTransformer.java:139)
       org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
       org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:163)
       org.codehaus.groovy.control.ResolveVisitor.visitBlockStatement(ResolveVisitor.java:1259)
       org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitConstructorOrMethod(ClassCodeExpressionTransformer.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitConstructorOrMethod(ResolveVisitor.java:167)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
       org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1063)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitClass(ResolveVisitor.java:1202)
       org.codehaus.groovy.control.ResolveVisitor.startResolving(ResolveVisitor.java:142)
       org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:643)
       org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:923)
       org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:585)
       org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:534)
       groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:286)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:259)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 41 elements
       org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:269)
       org.codehaus.groovy.control.ResolveVisitor.resolve(ResolveVisitor.java:237)
       org.codehaus.groovy.control.ResolveVisitor.transformPropertyExpression(ResolveVisitor.java:774)
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:637)
       org.codehaus.groovy.control.ResolveVisitor.transformBinaryExpression(ResolveVisitor.java:902)
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:641)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitExpressionStatement(ClassCodeExpressionTransformer.java:139)
       org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
       org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:163)
       org.codehaus.groovy.control.ResolveVisitor.visitBlockStatement(ResolveVisitor.java:1259)
       org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitConstructorOrMethod(ClassCodeExpressionTransformer.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitConstructorOrMethod(ResolveVisitor.java:167)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
       org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1063)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitClass(ResolveVisitor.java:1202)
       org.codehaus.groovy.control.ResolveVisitor.startResolving(ResolveVisitor.java:142)
       org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:643)
       org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:923)
       org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:585)
       org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:534)
       groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:286)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:259)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:651)
       org.codehaus.groovy.control.ResolveVisitor.transformPropertyExpression(ResolveVisitor.java:761)
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:637)
       org.codehaus.groovy.control.ResolveVisitor.transformBinaryExpression(ResolveVisitor.java:902)
       org.codehaus.groovy.control.ResolveVisitor.transform(ResolveVisitor.java:641)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitExpressionStatement(ClassCodeExpressionTransformer.java:139)
       org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
       org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:163)
       org.codehaus.groovy.control.ResolveVisitor.visitBlockStatement(ResolveVisitor.java:1259)
       org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
       org.codehaus.groovy.ast.ClassCodeExpressionTransformer.visitConstructorOrMethod(ClassCodeExpressionTransformer.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitConstructorOrMethod(ResolveVisitor.java:167)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
       org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1063)
       org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
       org.codehaus.groovy.control.ResolveVisitor.visitClass(ResolveVisitor.java:1202)
       org.codehaus.groovy.control.ResolveVisitor.startResolving(ResolveVisitor.java:142)
       org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:643)
       org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:923)
       org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:585)
       org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:561)
       org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:538)
       groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:286)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:259)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (212.5micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES07][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
&lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
::: [S3DEV-BI-ES03][K0lsh5b4SdOU03R5kQmuEw][S3DEV-BI-ES03][inet[/10.199.31.17:9300]]{master=false}
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (99.4micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES03][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (86.4micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES03][transport_client_worker][T#3]{New I/O worker #3}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (27.4micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES03][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)
&lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
::: [S3DEV-BI-ES02][zHj9goFrT_C1oBNT2PR6xQ][s3dev-bi-es02][inet[/10.199.31.16:9300]]{data=false, master=true}
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (162.6micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES02][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (67.7micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES02][transport_client_worker][T#2]{New I/O worker #2}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (29.2micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES02][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)
&lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
::: [S3DEV-BI-ES04][-4gSCl-kQQKHRniQjNjRGw][S3DEV-BI-ES04][inet[/10.199.31.18:9300]]{master=false}
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (207.9micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES04][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (76.7micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES04][transport_client_worker][T#3]{New I/O worker #3}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (28.7micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES04][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)
&lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
::: [S3DEV-BI-ES06][N2xcaOFLQ2C6F2UcYu7sSA][s3dev-bi-es06][inet[/10.199.31.20:9300]]{master=false}
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
   54.5% (272.6ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES06][bulk][T#2]'
     4/10 snapshots sharing following 16 elements
       groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:286)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:259)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 19 elements
       groovy.lang.GroovyClassLoader.clearCache(GroovyClassLoader.java:952)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.scriptRemoved(GroovyScriptEngineService.java:97)
       org.elasticsearch.script.ScriptService$ScriptCacheRemovalListener.onRemoval(ScriptService.java:509)
       org.elasticsearch.common.cache.LocalCache.processPendingNotifications(LocalCache.java:1956)
       org.elasticsearch.common.cache.LocalCache$Segment.runUnlockedCleanup(LocalCache.java:3460)
       org.elasticsearch.common.cache.LocalCache$Segment.postWriteCleanup(LocalCache.java:3436)
       org.elasticsearch.common.cache.LocalCache$Segment.put(LocalCache.java:2891)
       org.elasticsearch.common.cache.LocalCache.put(LocalCache.java:4149)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.put(LocalCache.java:4754)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:342)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 14 elements
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 7 elements
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
   50.1% (250.6ms out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES06][bulk][T#1]'
     8/10 snapshots sharing following 14 elements
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:245)
       groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:203)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.compile(GroovyScriptEngineService.java:119)
       org.elasticsearch.script.ScriptService.getCompiledScript(ScriptService.java:353)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:339)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 19 elements
       groovy.lang.GroovyClassLoader.clearCache(GroovyClassLoader.java:952)
       org.elasticsearch.script.groovy.GroovyScriptEngineService.scriptRemoved(GroovyScriptEngineService.java:97)
       org.elasticsearch.script.ScriptService$ScriptCacheRemovalListener.onRemoval(ScriptService.java:509)
       org.elasticsearch.common.cache.LocalCache.processPendingNotifications(LocalCache.java:1956)
       org.elasticsearch.common.cache.LocalCache$Segment.runUnlockedCleanup(LocalCache.java:3460)
       org.elasticsearch.common.cache.LocalCache$Segment.postWriteCleanup(LocalCache.java:3436)
       org.elasticsearch.common.cache.LocalCache$Segment.put(LocalCache.java:2891)
       org.elasticsearch.common.cache.LocalCache.put(LocalCache.java:4149)
       org.elasticsearch.common.cache.LocalCache$LocalManualCache.put(LocalCache.java:4754)
       org.elasticsearch.script.ScriptService.compile(ScriptService.java:342)
       org.elasticsearch.script.ScriptService.executable(ScriptService.java:463)
       org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:183)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardUpdateOperation(TransportShardBulkAction.java:535)
       org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:240)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
       org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (58.9micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES06][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)
&lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
::: [S3DEV-BI-ES05][RCNDd2psQ9mJdqOGjsap5g][S3DEV-BI-ES05][inet[/10.199.31.19:9300]]{master=false}
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (189.5micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES05][scheduler][T#1]'
     10/10 snapshots sharing following 9 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
       java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
       java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (73.3micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES05][transport_client_worker][T#3]{New I/O worker #3}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (27.8micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES05][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)
&lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
::: [S3DEV-BI-ES01][ALCO_0FLRsaunFGeC7CkQw][s3dev-bi-es01][inet[/10.199.31.15:9300]]{data=false, master=true}
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (102.4micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES01][[transport_server_worker.default]][T#3]{New I/O worker #8}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (33.2micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES01][[transport_server_worker.default]][T#1]{New I/O worker #6}'
     10/10 snapshots sharing following 15 elements
       sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
       sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
       sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
       sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
       sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
       org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
       org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
       org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
       java.lang.Thread.run(Thread.java:745)
   &lt;/td&gt;&lt;/tr&gt;
   &lt;tr&gt;&lt;td&gt;
    0.0% (30.5micros out of 500ms) cpu usage by thread 'elasticsearch[S3DEV-BI-ES01][transport_client_timer][T#1]{Hashed wheel timer #1}'
     10/10 snapshots sharing following 5 elements
       java.lang.Thread.sleep(Native Method)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
       org.elasticsearch.common.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
       org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       java.lang.Thread.run(Thread.java:745)
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
</comment><comment author="rmuir" created="2014-11-20T19:17:32Z" id="63862884">Hi, can you tell us your exact jvm version?
</comment><comment author="dakrone" created="2014-11-20T20:17:48Z" id="63872308">@ZeAleks can you also share the script and/or a full request you are using to update the documents?
</comment><comment author="ZeAleks" created="2014-11-20T20:39:12Z" id="63875507">@rmuir : Here is the Java version
java version "1.7.0_65"
OpenJDK Runtime Environment (rhel-2.5.1.2.el6_5-x86_64 u65-b17)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)

@dakrone : The script is in C#. Do you really want to see it ? It is just looping to everyline of a CSV file. Lines contains 3 columns with int values. It creates a concatenated string of bulk request that will be send to ES. And Every 1 000 lines, it sends the request, empty the string and continue.
Here is an example of what is sent to Elasticsearch in bulk (1 line) : 

{"update":{"_id":4242}}
{"script":"ctx._source.StockEvolution+=part;ctx._source.LastStockMovement='20141120-213620';ctx._source.CurrentInitial=42;ctx._source.CurrentSold=42;ctx._source.CurrentReal=42;","params":{"part":[{Date:"20141120-213620",Initial:42,Sold:42,Real:42}]}}
</comment><comment author="mikemccand" created="2014-11-20T22:02:27Z" id="63888297">@ZeAleks your update scripts change every time?  So they must be recompiled for each update?  It's better to use the same script and just change the params... I think (not certain) ES will compile them once and reuse that for future updates that have the same script.

With #6571 we switched from mvel to groovy in 1.4.0 so it looks likely that compiling scripts is slower for groovy than it was for mvel, maybe?
</comment><comment author="nik9000" created="2014-11-21T03:37:25Z" id="63920281">I know that the first point is correct. I'm reasonably sure your second
(compiling is slower) is also correct.

Its OK to have a couple of different scripts but not one per operation.
On Nov 20, 2014 5:02 PM, "Michael McCandless" notifications@github.com
wrote:

&gt; @ZeAleks https://github.com/ZeAleks your update scripts change every
&gt; time? So they must be recompiled for each update? It's better to use the
&gt; same script and just change the params... I think (not certain) ES will
&gt; compile them once and reuse that for future updates that have the same
&gt; script.
&gt; 
&gt; With #6571 https://github.com/elasticsearch/elasticsearch/pull/6571 we
&gt; switched from mvel to groovy in 1.4.0 so it looks likely that compiling
&gt; scripts is slower for groovy than it was for mvel, maybe?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8553#issuecomment-63888297
&gt; .
</comment><comment author="ZeAleks" created="2014-11-21T08:29:53Z" id="63939493">OK.
I will try immediately to use parameters in my script.

But, if that is true, so that it mean that mvel has better performance than groovy ?
</comment><comment author="dakrone" created="2014-11-21T09:07:45Z" id="63943087">@ZeAleks MVEL compiles faster than Groovy (which is why you are seeing compilation show up in the hot threads of your system), however Groovy executes much faster than MVEL. Moving all the changing parts of the script to parameters will ensure that ES only compiles the script once per node, and not every time a request is sent
</comment><comment author="s1monw" created="2014-11-21T10:04:38Z" id="63949485">@ZeAleks can you try to optimize your script to use parameters? I will close the issue for now - if it's a different cause we can reopen...
</comment><comment author="ZeAleks" created="2014-11-21T11:00:53Z" id="63955713">Hey guys,

You were right. The problem was cause by my request.
I change it to : 
{"update":{"_id":1}}
{"script":"ctx._source.StockEvolution+=part;ctx._source.LastStockMovement=part[0].Date;ctx._source.CurrentInitial=part[0].Initial;ctx._source.CurrentSold=part[0].Sold;ctx._source.CurrentReal=part[0].Real;","params":{"part":[{Date:"2009-11-15T16:22:12",Initial:53,Sold:54,Real:52}]}}

And now it is much better :

![capture](https://cloud.githubusercontent.com/assets/4868394/5141434/c122da7e-7175-11e4-969c-1a6a34485620.PNG)

But, it is not as fast as it was with 1.3.2 and mvel.
I will try to investigate why.

Anyway, think you all for your support !
</comment><comment author="osykora" created="2014-11-25T15:39:12Z" id="64418374">I experienced the same issue when I switched to ES 1.4, thus to Groovy, since MVEL is no longer supported. The bulk update script executed slowly, so I tried to parameterized the script and it helped, but it's not as fast as it was using MVEL for the very same script.

Here's my pseudo code for a script that is updating a list in a list:
​{update: {"_id" : "myObjectID" }} 
{"script": "for(item in ctx._source.myListOfValues){ if(item.get('field1') == param1 &amp;&amp; item.get('field2') == param2 &amp;&amp; item.get('field3') == param3) {item.innerListOfValues+=list; break; }} ,"params":{"param1":"someValue", "param2":"someValue2", "param3":"someValue3", "list": ['a', 'b', 'c']}}
</comment><comment author="ZeAleks" created="2014-11-26T16:12:24Z" id="64669233">Hi osykora,

What I did after my last post was :
- Upgrade to Java 1.8
- Restart every node of my cluster

After that, my queries are as fast as before.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] use logger level from test class annotation also in external node...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8552</link><project id="" key="" /><description>...s
</description><key id="49405283">8552</key><summary>[TEST] use logger level from test class annotation also in external node...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-11-19T15:16:17Z</created><updated>2014-12-08T11:46:37Z</updated><resolved>2014-12-08T11:46:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-19T17:02:43Z" id="63674368">LGTM
</comment><comment author="s1monw" created="2014-11-19T17:02:57Z" id="63674415">please label that PR accordingly
</comment><comment author="javanna" created="2014-11-19T18:15:17Z" id="63686274">LGTM thanks @brwe 
</comment><comment author="brwe" created="2014-11-20T19:18:49Z" id="63863073">Pushed a little too early, the commit was actually a mess. External settings were not picked up anymore in classes that override externalNodeSettings() such as UnicastBackwardsCompatibilityTest, which then failed. @javanna would you mind taking another look?
</comment><comment author="javanna" created="2014-11-21T07:30:08Z" id="63933650">left a small comment @brwe 
</comment><comment author="s1monw" created="2014-11-23T12:40:23Z" id="64116548">left one comment other than that LGTM
</comment><comment author="s1monw" created="2014-12-08T11:46:27Z" id="66105789">closing in favor of https://github.com/elasticsearch/elasticsearch/pull/8820
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/ElasticsearchBackwardsCompatIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/junit/listeners/LoggingListener.java</file></files><comments><comment>[TEST] Pass class level test logging to external nodes</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/search/functionscore/FunctionScoreBackwardCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchBackwardsCompatIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/junit/listeners/LoggingListener.java</file></files><comments><comment>[TEST] use logger level from test class annotation also in external nodes</comment></comments></commit></commits></item><item><title>Free pending search contexts if index is closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8551</link><project id="" key="" /><description>Today we hold on to search context reference if they are not cleaned
up for a while until a reaper thread trashes them if they timed out.
This commit removes all pending contexts once the index is closed to release
resources and filehandles immediatly once the index is closed.
</description><key id="49404487">8551</key><summary>Free pending search contexts if index is closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-19T15:10:10Z</created><updated>2015-06-06T19:20:46Z</updated><resolved>2014-11-19T20:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-19T15:12:24Z" id="63654142">Nice one! LGTM
</comment><comment author="s1monw" created="2014-11-19T23:13:26Z" id="63732591">I had to back this out since it's causing failures http://build-us-00.elasticsearch.org/job/es_core_1x_metal/4815/consoleFull the problem is that `close != remove` and if all shards are moved away from a node the index is `closed` on this node. Yet I was trying to remove stuff once the index is deleted....
</comment><comment author="s1monw" created="2014-11-20T15:15:14Z" id="63822666">After fixing #8569 I pushed these back in with the fix
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/IndicesLifecycle.java</file><file>src/main/java/org/elasticsearch/indices/IndicesService.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesLifecycle.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerSingleNodeTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchSingleNodeTest.java</file></files><comments><comment>[INDEX] Add before/after indexDeleted callbacks to IndicesLifecycle</comment></comments></commit></commits></item><item><title>Fix example in logging daily rotate configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8550</link><project id="" key="" /><description>PR #8464 come with a bug in the example provided.

First, the current log file is not compressed so it should not end with `.gz`.
Second, conversion pattern was removing all the log content but was printing only the log date.
Then, the current log filename was hardcoded to `elasticsearch` instead of the cluster name.
</description><key id="49402115">8550</key><summary>Fix example in logging daily rotate configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Logging</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-19T14:51:46Z</created><updated>2015-06-10T09:16:23Z</updated><resolved>2014-11-19T15:01:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-19T14:57:18Z" id="63651747">LGTM
</comment><comment author="dadoonet" created="2014-11-19T15:01:40Z" id="63652452">Closed by d202b540f4374ef5b30e3c740f6146045d996642 in 1.x and cbced948c413b12e24575999c4e369d16ae37fb3 in master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>how to insert data into es quickly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8549</link><project id="" key="" /><description>i use flume-ng to  insert data into es, i found it is not quickly（about 6M/s）
i want to optimize the efficiency to 15M/s or more quickly

how should i to do ?

there is one es node in my environment，and the refresh_interval is set to -1 ..
</description><key id="49399213">8549</key><summary>how to insert data into es quickly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">balibaba12</reporter><labels /><created>2014-11-19T14:32:47Z</created><updated>2014-11-21T22:52:30Z</updated><resolved>2014-11-21T22:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T22:52:30Z" id="64050775">please use the mailing list for questions like this - the issue tracker is only for bugs and improvements / features
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI] Malformed input or input contains unmappable characters: test-weird-index-中文</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8548</link><project id="" key="" /><description>Occasionally our CI runs fail with the following:

```
expected [2xx] status code but api [index] returned [400 Bad Request] [{"error":"IndexCreationException[[test-weird-index-中文] failed to create index]; nested: InvalidPathException[Malformed input or input contains unmappable characters: test-weird-index-中文]; ","status":400}]
```

See: http://build-us-00.elasticsearch.org/job/es_core_master_debian/2401/testReport/org.elasticsearch.test.rest/ElasticsearchRestTests/test__yaml_index_10_with_id_Index_with_ID_/
See: http://build-us-00.elasticsearch.org/job/es_rest_compat_1x/CHECK_BRANCH=origin/1.2,jdk=JDK7,label=commit/869/

I was unable to reproduce this locally, but I think it might be caused by an incorrectly set `LANG` environment variable, as seen by:

```
hinmanm@Xanadu 0 ~()% LANG=C groovysh -Dfile.encoding=ANSI_X3.4-1968
Groovy Shell (2.3.7, JVM: 1.8.0_20)
Type ':help' or ':h' for help.
-------------------------------------------------------------------------------
groovy:000&gt; (new File("/tmp/myindex")).toPath().resolve("test-weird-index-中文")
===&gt; /tmp/myindex/test-weird-index-??????

hinmanm@Xanadu 0 ~()% LANG=en_US.UTF-8 groovysh -Dfile.encoding=ANSI_X3.4-1968
Groovy Shell (2.3.7, JVM: 1.8.0_20)
Type ':help' or ':h' for help.
-------------------------------------------------------------------------------
groovy:000&gt; (new File("/tmp/myindex")).toPath().resolve("test-weird-index-中文")
(new File("/tmp/myindex")).toPath().resolve("test-weird-index-中文")
===&gt; /tmp/myindex/test-weird-index-中文
```

We fixed this for `bin/elasticsearch` in https://github.com/elasticsearch/elasticsearch/pull/6047 by setting the var, however, I think we may need to do the same thing in our CI infrastructure for the tests.
</description><key id="49398422">8548</key><summary>[CI] Malformed input or input contains unmappable characters: test-weird-index-中文</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>jenkins</label></labels><created>2014-11-19T14:27:10Z</created><updated>2014-11-19T17:34:42Z</updated><resolved>2014-11-19T17:34:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-19T14:27:32Z" id="63647127">@mrsolo do you have any thoughts/opinions on this?
</comment><comment author="mrsolo" created="2014-11-19T17:13:48Z" id="63676281">This may be the side effected of Jenkins master migration.  The Debian image that Jenkins is using does not have LANG explicitly set.  

http://build-us-00.elasticsearch.org/computer/debian-7-64-0-metal/ now have 'LANG=en_US.UTF-8'
</comment><comment author="dakrone" created="2014-11-19T17:34:42Z" id="63679781">Closing this then, will re-open if it happens again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ability to set external request id and enhanced logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8547</link><project id="" key="" /><description>I miss setRequestId() method in ES API. It would be good to bind API requests to some value specified on the client side. For example:

client.prepareSearch().setRequestId("SOME_STRING").execute().actionGet();

What I would like to achieve is an ability to group logs from various system components.

Assuming that we have a system containing 3 components:
- A - HTTP frontend
- B - My App 
- C - Elasticsearch

and communication chain is as follows:

CLIENT -&gt; A (generation of unique request id happens here) -&gt; B (with given request id) -&gt; C (with given request id)

Now if there is an exception or warning logged in one of those systems it will be logged with request id bound to the request. So assuming that we have centralized logging infrastructure it would be very easy to track the root causes and side effects of problems that occur in the system.
</description><key id="49389370">8547</key><summary>Ability to set external request id and enhanced logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kmakowski</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-19T13:29:21Z</created><updated>2016-11-26T12:40:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rayward" created="2016-08-30T23:34:19Z" id="243614609">Would love to see this attached to the slow query log so that I can trace bad queries back to an originating http request.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When i using elasticsearch to search a long field encountered a strange question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8546</link><project id="" key="" /><description>hi,
  when i use es to search,i experience a problem.there is a field,type is long,and store as array.like this:
![1](https://cloud.githubusercontent.com/assets/9320176/5102980/3df819ca-700d-11e4-96ef-a93f445b5cea.png)

but when i use two item in the array to search,one term can search ,but one term could not search any result.like this:
![2](https://cloud.githubusercontent.com/assets/9320176/5102981/439f2896-700d-11e4-9e89-ba731e812752.png)
![3](https://cloud.githubusercontent.com/assets/9320176/5102983/48de1844-700d-11e4-956d-fad27f7d9921.png)
</description><key id="49350893">8546</key><summary>When i using elasticsearch to search a long field encountered a strange question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haochun</reporter><labels /><created>2014-11-19T08:58:30Z</created><updated>2014-11-26T05:10:15Z</updated><resolved>2014-11-24T11:23:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T11:23:58Z" id="64180341">Hi @haochun 

The number `64488939508378000000` is a big integer, not a long.  There is currently no support for big integers in Elasticsearch.

Closing in favour of #5683
</comment><comment author="haochun" created="2014-11-26T05:10:15Z" id="64515914">@clintongormley i use new Random().nextLong() to generate this number.If so, I can only use the string to replace it is?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restore with `wait_for_completion:true` should wait for succesfully restored shards to get started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8545</link><project id="" key="" /><description>This commit ensures that restore operation with wait_for_completion=true doesn't return until all successfully restored shards are started. Before it was returning as soon as restore operation was over, which cause some shards to be unavailable immediately after restore completion.

Fixes #8340
</description><key id="49322370">8545</key><summary>Restore with `wait_for_completion:true` should wait for succesfully restored shards to get started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-19T02:49:55Z</created><updated>2015-06-08T00:37:10Z</updated><resolved>2014-11-25T01:55:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-11-19T03:16:20Z" id="63586251">Left some minor comments. LGTM
</comment><comment author="s1monw" created="2014-11-21T09:08:48Z" id="63943174">I left a couple of comments
</comment><comment author="s1monw" created="2014-11-21T09:38:26Z" id="63946360">@imotov should this go into `1.3.6` too?
</comment><comment author="s1monw" created="2014-11-24T15:34:14Z" id="64210701">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update plugins.asciidoc on river plugins section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8544</link><project id="" key="" /><description>Adding links to Amazon S3 and Google Drive river plugins
</description><key id="49310820">8544</key><summary>Update plugins.asciidoc on river plugins section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lbroudoux</reporter><labels /><created>2014-11-18T23:58:24Z</created><updated>2014-11-24T11:15:28Z</updated><resolved>2014-11-24T11:15:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T11:15:27Z" id="64179520">thanks @lbroudoux - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update plugins.asciidoc on river plugins section</comment></comments></commit></commits></item><item><title>GeoPolyFilter and NumberFormatException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8543</link><project id="" key="" /><description>Some combination of geo points, geo_polygon filters, and partial mapping definitions can cause a parsing failure during search.

The exception looks like this:

```
[2014-11-18 16:06:41,272][DEBUG][action.search.type       ] [Human Fly] [test][0], node[2NmTpg-7RyKkKlYMEXjT-A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4ce649ca]
org.elasticsearch.search.query.QueryPhaseExecutionException: [test][0]: query[filtered(ConstantScore(GeoPolygonFilter(point, [[0.0, 0.0], [0.0, 1.0], [1.0, 1.0], [0.0, 0.0]])))-&gt;cache(_type:foo)],from[0],size[1000]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:356)
        at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
        at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchException: java.lang.NumberFormatException: For input string: "?R"
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:80)
        at org.elasticsearch.index.search.geo.GeoPolygonFilter.getDocIdSet(GeoPolygonFilter.java:59)
        at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:46)
        at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:157)
        at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:542)
        at org.apache.lucene.search.FilteredQuery$FilterStrategy.filteredBulkScorer(FilteredQuery.java:504)
        at org.apache.lucene.search.FilteredQuery$1.bulkScorer(FilteredQuery.java:150)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
        ... 7 more
Caused by: org.elasticsearch.common.util.concurrent.UncheckedExecutionException: java.lang.NumberFormatException: For input string: "?R"
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
        at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:174)
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:74)
        ... 20 more
Caused by: java.lang.NumberFormatException: For input string: "?R"
        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)
        at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)
        at java.lang.Double.parseDouble(Double.java:538)
        at org.elasticsearch.index.fielddata.plain.AbstractIndexGeoPointFieldData$GeoPointEnum.next(AbstractIndexGeoPointFieldData.java:68)
        at org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayIndexFieldData.loadDirect(GeoPointDoubleArrayIndexFieldData.java:82)
        at org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayIndexFieldData.loadDirect(GeoPointDoubleArrayIndexFieldData.java:42)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:187)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:174)
        at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 24 more
```

The following script requires elasticsearch to be running locally and usually reproduces the problem on my machine:

``` bash
#!/bin/bash

data=$(cat &lt;&lt;EOF
20.0,-131.0
86.0,29.1
-50.1,18.2
4.0,-157.3
-16.2,159.0
-14.5,-109.2
-29.4,-44.6
37.6,76.3
66.2,-119.3
-17.7,138.3
13.1,78.0
-41.1,88.9
-11.11,107.7
23.1,40.1
-43.6,141.6
2.4,-128.8
-10.11,-51.6
-77.6,-35.13
72.12,13.1
42.14,31.14
-77.6,-139.19
23.16,-22.7
-20.19,-45.11
-14.0,4.13
15.18,-151.1
EOF
)

#echo $data
curl -XDELETE 'http://localhost:9200/test/'

set -e
sleep 2
curl -XPOST localhost:9200/test -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "foo" : {
            "_source" : { "enabled" : true },
            "properties" : {
                "point" : { "type" : "geo_point"}
            }
        }
    }
}'
i=0
for x in $data
do
    i=$(expr $i + 1)
    echo
    echo Line $i $x

    LAT=$(echo  $x | awk -F , '{{print $1}}')
    LON=$(echo  $x | awk -F , '{{print $2}}')
    curl -s -XPOST localhost:9200/test/foo/ -d "{
      point: [$LON, $LAT]
    }" &gt; /dev/null


    curl -s -XPOST localhost:9200/test/another_mapping -d "{
      name: \"FOOBAR\",
      point:  [$LON, $LAT]}" &gt; /dev/null

    curl --fail -XGET localhost:9200/test/foo/_search -d '{ "size": 1000,
         "query" : {"filtered" : { "query" : {"match_all" : {}},
                      "filter" :
                         {"geo_polygon" :
                            {"point" :{"points":
                                       [{"lat": 0, "lon": 0},
                                        {"lat": 0, "lon": 1},
                                        {"lat": 1, "lon": 1}]}}}}}}'

done
```
</description><key id="49300480">8543</key><summary>GeoPolyFilter and NumberFormatException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryfow</reporter><labels /><created>2014-11-18T22:10:37Z</created><updated>2014-11-24T11:11:47Z</updated><resolved>2014-11-24T11:11:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T11:11:47Z" id="64179194">Hi @ryfow 

You're hitting a known issue.  Fields in different types with the same name must have the same mapping, otherwise you see problems like this.  You have `another_mapping.point` which is mapped as a `double`, and the two datatypes are being confused. 

In the next version we're going to enforce that requirement.  Closing in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Position offset gap doesn't work in object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8542</link><project id="" key="" /><description>I have an impression that position_offset_gap doesn't work in object

For this example

```
{
    "field": {
        "type": "object",
        "properties": {
            "subfield": {
                "type": "string",
                "position_offset_gap": 500
            }
        },
        "position_offset_gap": 5000
    }
}
```

When I build document 

```
{"field" : [{"subfield" : ["foo", "bar"]}, {"subfield" : ["baz"]}]}
```

positiont offset gap 5000 is not applied between "bar" and "baz"
</description><key id="49299943">8542</key><summary>Position offset gap doesn't work in object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels /><created>2014-11-18T22:05:36Z</created><updated>2015-02-12T15:18:35Z</updated><resolved>2014-11-24T10:54:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-24T10:54:38Z" id="64177598">Hi @prog8 

That's correct.  `position_offset_gap` is only applied to concrete fields (ie not objects/nested) with positions enabled.  In fact in master, the mapping you have provided will throw an exception:

```
MapperParsingException[Mapping definition for [field] has unsupported parameters:  [position_offset_gap : 5000]
```

If you want to separate objects in this way, you should use `nested` fields instead. That way each object will be (internally) indexed into its own separate document.
</comment><comment author="prog8" created="2014-11-24T12:04:42Z" id="64185108">Hi @clintongormley,
I'm not sure if nested is a solution. From one side is brings big overhead when I have a lot of nested objects and from second side it doesn't let me to handle span queries over different sections of document. For me this task is something which is quite important in some cases.
</comment><comment author="yehosef" created="2015-02-12T11:05:48Z" id="74053736">It seems you can set a position_offset_gap  if you apply it to concrete fields in the subobject.  Check out this gist - https://gist.github.com/yehosef/eaf9584d4394198e4f4f
I create three types with a nested object.  The first I didn't apply a gap, the second I applied it at the object level, and the third I applied it on the field in the nested object level.  I had two nested object with the text "my kitchen" and "sink repair" and I searched for "kitchen sink"

The first two types do return results, while the third does not (indicating that the gap is working).

So in the example given, they won't have a gap of 5000 but they will have a gap of 500 - you don't need to resort to subobjects (unless I'm missing something.)  At it seems the newer code will throw an error when applied to the object field level - which is good since it doesn't do anything.
</comment><comment author="prog8" created="2015-02-12T11:13:48Z" id="74054720">Exactly "they won't have a gap of 5000 but they will have a gap of 500" but the case is to have gap 5000 on one level and 500 on second level. Without it how span near queries can be used for documents with multiple sections levels (like eg. level0(sentence), level1 (paragraph), level2 (chapter))?
</comment><comment author="yehosef" created="2015-02-12T15:17:57Z" id="74087503">I understand the use case now.  Good question.  I'm also not sure if nested documents would solve the problem.  @clintongormley would you be able to accomplish what  @prog8 is asking with nested documents?

My initial thoughts are to see if there is a way to artificially boost the gap.  Eg, perhaps you could put a extra line between each paragraph that has 5000 periods (or some other char that artificially create the extra offset you need).  Then you would store the sentences as regular embedded objects and you'll have the 500 offset and 5000 offsets you need.

I think supporting the position offset gap at the nested offset level would make sense and just increase the offset of any field with position information between objects.  It seems like a valid use case.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect Highlighting when combining with match and prefix queries on not analized fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8541</link><project id="" key="" /><description>This is happening in ES version 1.3.2 in our production environment.

Steps to reproduce:
1) Create an Index with the following mapping:

``` javascript
{
    "type1":{
        "properties":{
            "field1":{"type":"string"},
            "field2":{"type":"string","index":"not_analyzed"},
            "field3":{"type":"string","index":"not_analyzed"}
        }
    }
}
```

2) Add the following document to the Index:

``` javascript
{
"field1" : "This is a test 100",
"field2" : "9999999",
"field3" : "1009999"
}
```

3) If you do an explain on this query and the just indexed document you will see that the field1 is the only one that matches:

``` javascript
{
"query": {
        "bool": {
            "should": [
                {
                    "match": {
                        "field1": {
                            "query": "100",
                            "type": "boolean"
                        }
                    }
                },
                {
                    "prefix": {
                        "field2": {
                            "prefix": "100"
                        }
                    }
                },
                {
                    "match": {
                        "field3": {
                            "query": "100",
                            "type": "boolean"
                        }
                    }
                }
            ]
        }
    }
}
```

4) Now run the query with highlighting and see how the field3 is highlighted but it shouldn't.
Query:

``` javascript
{
    "query": {
        "bool": {
            "should": [
                {
                    "match": {
                        "field1": {
                            "query": "100",
                            "type": "boolean"
                        }
                    }
                },
                {
                    "prefix": {
                        "field2": {
                            "prefix": "100"
                        }
                    }
                },
                {
                    "match": {
                        "field3": {
                            "query": "100",
                            "type": "boolean"
                        }
                    }
                }
            ]
        }
    },
    "highlight": {
        "fields": {
            "field1": {
            },
            "field2": {
            }
            ,
            "field3": {
            }
        }
    }
}
```

Result:

``` javascript
{
   "took":0,
   "timed_out":false,
   "_shards":{
      "total":3,
      "successful":3,
      "failed":0
   },
   "hits":{
      "total":1,
      "max_score":0.025092505,
      "hits":[
         {
            "_index":"test1",
            "_type":"type1",
            "_id":"1",
            "_score":0.025092505,
            "_source":{
               "field1":"This is a test 100",
               "field2":"9999999",
               "field3":"1009999"
            },
            "highlight":{
               "field3":[
                  "&lt;em&gt;1009999&lt;/em&gt;"
               ],
               "field1":[
                  "This is a test &lt;em&gt;100&lt;/em&gt;"
               ]
            }
         }
      ]
   }
}
```

This does not happen if the query doesn't include the prefix subquery on field2:

``` javascript
{
    "query": {
        "bool": {
            "should": [
                {
                    "match": {
                        "field1": {
                            "query": "100",
                            "type": "boolean"
                        }
                    }
                },
                {
                    "match": {
                        "field3": {
                            "query": "100",
                            "type": "boolean"
                        }
                    }
                }
            ]
        }
    },
    "highlight": {
        "fields": {
            "field1": {},
            "field2": {},
            "field3": {}
        }
    }
}
```

Result:

``` javascript
{
   "took":0,
   "timed_out":false,
   "_shards":{
      "total":3,
      "successful":3,
      "failed":0
   },
   "hits":{
      "total":1,
      "max_score":0.043088365,
      "hits":[
         {
            "_index":"test1",
            "_type":"type2",
            "_id":"1",
            "_score":0.043088365,
            "_source":{
               "field1":"This is a test 100",
               "field2":"9999999",
               "field3":"1009999"
            },
            "highlight":{
               "field1":[
                  "This is a test &lt;em&gt;100&lt;/em&gt;"
               ]
            }
         }
      ]
   }
}
```
</description><key id="49299513">8541</key><summary>Incorrect Highlighting when combining with match and prefix queries on not analized fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">carofe82</reporter><labels><label>:Highlighting</label></labels><created>2014-11-18T22:01:52Z</created><updated>2016-11-24T19:03:56Z</updated><resolved>2016-11-24T19:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="carofe82" created="2015-01-26T22:15:27Z" id="71550085">This issue has been opened for a while. Is there any hope to see a fix any time soon?
</comment><comment author="clintongormley" created="2016-11-24T19:03:56Z" id="262832966">This happened because `require_field_match` used to default to `false`.  It now defaults to `true`</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not take deleted documents into account in aggregations filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8540</link><project id="" key="" /><description>Since aggregators are only called on documents that match the query, it never
gets called on deleted documents, so by specifying `null` as live docs, we very
likely remove a BitsFilteredDocIdSet layer.
</description><key id="49296612">8540</key><summary>Do not take deleted documents into account in aggregations filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-18T21:36:24Z</created><updated>2015-06-06T19:02:59Z</updated><resolved>2014-11-20T09:36:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-18T22:46:50Z" id="63560300">LGTM. Maybe we could have a simple test verifying deleted docs don't start getting counted with this change?
</comment><comment author="jpountz" created="2014-11-19T08:13:21Z" id="63605383">@rjernst Thanks, I pushed a new commit that indexes deleted docs that match the filters.
</comment><comment author="rjernst" created="2014-11-19T19:14:44Z" id="63695777">Thanks, LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FilterTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersTests.java</file></files><comments><comment>Aggregations: Do not take deleted documents into account in aggregations filters.</comment></comments></commit></commits></item><item><title>ElasticsearchIntegrationTest circular dependency from Client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8539</link><project id="" key="" /><description>We're having a big issue upgrading plugins to 1.4.0. We use `ElasticsearchIntegrationTest` to run our integration tests. In 1.4.0, if any class in your plugin injects `Client` it creates a circular dependency. @dr2014 put together a simple example with 4 `.java` files that demonstrates the issue.

https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/KHXzKLWoBrQ

https://github.com/dr2014/ES1.4_IntegrationTest_Issue

If you switch back to 1.3.x everything works fine.
</description><key id="49281682">8539</key><summary>ElasticsearchIntegrationTest circular dependency from Client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">erikringsmuth</reporter><labels><label>regression</label><label>v1.4.1</label></labels><created>2014-11-18T19:23:53Z</created><updated>2014-11-21T09:04:40Z</updated><resolved>2014-11-21T09:04:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2014-11-19T10:59:29Z" id="63623439">Hi @erikringsmuth 

Thanks for the sample project, it helps a lot.

I'm not sure where the problem is located, but as a workaround I suggest you to use

```
.put("plugins." + PluginsService.LOAD_PLUGIN_FROM_CLASSPATH, true)
```

Instead of

```
.put("plugin.types", TestESPlugin.class.getName())
```

In your SampleTest class. This should correct the pb, please let me know if it works for you.

Still investigating on my side.
</comment><comment author="dr2014" created="2014-11-19T19:34:43Z" id="63699121">@tlrx 

The es-plugin.properties file in the Sample project was not pointing to the correct class. Please get the latest changes for the Sample project.

The workaround suggested by you is not working.
</comment><comment author="tlrx" created="2014-11-21T09:04:40Z" id="63942807">Ok, you hit a problem already fixed by @martijnvg in #8415. The test passed on elasticsearch 1.4.1-SNAPSHOT.

This has been backported to 1.4.x and should be integrated in 1.4.1.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>disk.watermark.high relocates all shards creating a relocation storm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8538</link><project id="" key="" /><description>I have a relatively large cluster running ES 1.3.5 and we recently started to get low on disk space. The cluster has roughly 110 TB usable space of which about 86TB is used. We have 93 indices of which 90 are rotating daily indices and the other 3 are permanent. With this setup we end up with shards that range in size from a few MB to over 160GB and we're finding the shard count based allocation strategy results in some nodes starting to run out of space earlier than expected which trips the cluster.routing.allocation.disk.watermark.high setting.

It appears when this happens that "all" of the shards from that node are relocated. This then puts more pressure on the other nodes and at some point another node will trip cluster.routing.allocation.disk.watermark.high and relocate all of its shards too. This then goes on and on and on and never stops until we intercede and manually cancel the allocations. At times we've seen more than 200 relocations in progress at the same time.

Is the relocation of "all" shards really the intended behavior? In looking at the source code it appears to do a reroute in this scenario but this seems like the last thing you would want to do. You're almost guaranteed that if one node is tripping that setting then others are going to be close as well. In our cluster we have room on many of the other nodes and if it selectively moved a shard or two everything would be fine but moving all of them is really problematic.

It also seems that when all these relocations occur that cluster.routing.allocation.disk.watermark.low doesn't prevent the overallocation of other nodes. I believe this is occurring because of the number of relocations that are started at once so the node gets asked multiple times to accept shards within a short time period and all requests succeed because no pending relocations are considered in respect to disk utilization. This pretty much guarantees that node will eventually trip cluster.routing.allocation.disk.watermark.high and continue the cycle even though there are other nodes in the cluster with hundreds of GB free. 

We're in the process of adding additional capacity to our cluster so we're not at risk of bumping into these limits but otherwise this behavior seems quite problematic.
</description><key id="49273518">8538</key><summary>disk.watermark.high relocates all shards creating a relocation storm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">kstaken</reporter><labels><label>blocker</label><label>bug</label><label>PITA</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-18T18:23:32Z</created><updated>2014-11-26T09:14:27Z</updated><resolved>2014-11-26T09:14:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-19T17:01:37Z" id="63674150">Okay, this looks like this is because the `DiskThresholdDecider`'s `canRemain` method needs to take currently relocating shards into account, subtracting them from the limit when deciding whether to relocate more shards.

We currently take relocations _in_ to the node into account, now we need to take relocations _out_ of the node into account, and only in the `canRemain` method.
</comment><comment author="s1monw" created="2014-11-23T12:54:14Z" id="64116941">@dakrone I think we only need to change the default for `cluster.routing.allocation.disk.include_relocations` here  which is true. we can also think about adding an enum here where you can specify where you wanna take it into account like 
`cluster.routing.allocation.disk.include_relocations = never | allocation | moves` the naming is hard here I guess.. :)
</comment><comment author="dakrone" created="2014-11-24T11:04:19Z" id="64178533">@s1monw I don't follow how that will help? I think it will still see all shards as failing the `canRemain()` decision and relocating them away from the node that goes over the high watermark
</comment><comment author="s1monw" created="2014-11-24T15:47:39Z" id="64212949">oh well I misunderstood the code - it only takes the incoming into account :/
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file></files><comments><comment>Core: Let the disk threshold decider take into account shards moving away from a node in order to determine if a shard can remain.</comment></comments></commit></commits></item><item><title>Remove NoneGateway, NoneGatewayAllocator, &amp; NoneGatewayModule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8537</link><project id="" key="" /><description>Always use the LocalGateway\* equivalents

The reason of this change is that we already check in the `LocalGateway` whether a node is a client node, or is not master-eligible, and skip writing the state there. This allows us to remove this code that was previously used only for tribe nodes (which are not master eligible anyway and wouldn't write state) and in tests (which can shake more bugs out).
</description><key id="49240494">8537</key><summary>Remove NoneGateway, NoneGatewayAllocator, &amp; NoneGatewayModule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Internal</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-11-18T14:52:49Z</created><updated>2015-06-06T16:20:27Z</updated><resolved>2014-11-24T11:22:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-18T17:08:23Z" id="63506101">I haven't started looking but I already like the title and the diff stats. :-)
</comment><comment author="jpountz" created="2014-11-18T18:16:21Z" id="63517850">This looks good to me, maybe we could try to have a static variable in ElasticsearchTestCase holding a no-op gateway allocator so that we don't have to create a new anonymous instance whenever we need it? Would also be nice if someone else could give it a look since it's a part of elasticsearch that I don't kno well.
</comment><comment author="bleskes" created="2014-11-18T19:46:49Z" id="63532616">LGTM. Left some minor comments + a question about removed test.
</comment><comment author="javanna" created="2014-11-19T13:02:03Z" id="63636104">nice work @dakrone ! I just bumped into the [gateway documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html), I think it should be updated as part of this PR as `gateway: none` is not an option anymore.
</comment><comment author="javanna" created="2014-11-19T13:06:56Z" id="63636637">One more thing, if I understand correctly the `gateway.type` can only have `local` value after this change. Does the setting remain configurable? Would it make sense to remove it from the default `elasticsearch.yml` (and also look for any other place where it's mentioned)?
</comment><comment author="dakrone" created="2014-11-19T13:33:47Z" id="63639805">@javanna good catch! I've updated the documentation and `elasticsearch.yml` for the removal of the none gateway.

The setting remains configurable, but only for internal (plugin) use only.
</comment><comment author="bleskes" created="2014-11-19T16:21:51Z" id="63666638">&gt; Would it make sense to remove it from the default elasticsearch.yml

+1 

&gt; The setting remains configurable, but only for internal (plugin) use only.

+1 (we may want to mock it at some point)

Left one minor comment LGTM (with the change to elasticsearch.yml)
</comment><comment author="dakrone" created="2014-11-24T11:22:33Z" id="64180217">Merged to master
</comment><comment author="s1monw" created="2014-11-24T11:22:50Z" id="64180245">w00t
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `memory`/ `ram` store</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8536</link><project id="" key="" /><description>The RAM store is discuraged for production usage anyway and
we don't test it in our randomized infrastructure. This commit
removes it for `2.0`
</description><key id="49238067">8536</key><summary>Remove `memory`/ `ram` store</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-18T14:36:06Z</created><updated>2015-08-11T17:21:18Z</updated><resolved>2014-11-20T14:17:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-18T18:27:14Z" id="63519638">LGTM, also nice to move a couple more methods to Paths instead of Files
</comment><comment author="kimchy" created="2014-11-19T13:51:33Z" id="63641933">+1 on removing it, but lets give people more time to comment if possible
</comment><comment author="dakrone" created="2014-11-19T13:57:56Z" id="63642761">+1, it's still possible to use ram disks if people _absolutely_ need them and removing them reduces complexity
</comment><comment author="clintongormley" created="2014-11-19T14:13:03Z" id="63644981">+1
</comment><comment author="djschny" created="2015-08-10T23:56:10Z" id="129649852">This was a great benefit for unit testing when running an embedded version of elasticsearch as one did not have to clean up data afterwards. It's a shame to see it removed when it still has purpose. I'm trying to understand the stance here since it is clear what a memory/ram store does.
</comment><comment author="clintongormley" created="2015-08-11T13:32:15Z" id="129874543">@djschny we've had issues in the past where there were bugs and inconsistencies in the ram store.  If e provide it, we should also test it, otherwise we have no idea if it works correctly.  Given that it is not recommended for production use, we decided to clear it out of our code base so that we can focus on the important stuff.
</comment><comment author="djschny" created="2015-08-11T17:21:18Z" id="129977524">Thanks @clintongormley - missed the part where it was known to be buggy.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] MemoryCircuitBreakerTests.testThreadedUpdatesToChildBreakerWithParentLimit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8535</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_master_regression/867/

```
testThreadedUpdatesToChildBreakerWithParentLimit(org.elasticsearch.common.breaker.MemoryCircuitBreakerTests)
REPRODUCE WITH  : mvn clean test -Dtests.seed=6124793851234CFA -Dtests.class=org.elasticsearch.common.breaker.MemoryCircuitBreakerTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Berlin -Dtests.method="testThreadedUpdatesToChildBreakerWithParentLimit" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.processors=8
Throwable:
com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=1982, name=Thread-39, state=RUNNABLE, group=TGRP-MemoryCircuitBreakerTests]
Caused by: java.lang.AssertionError: tripped too many times: 3
Expected: &lt;false&gt;
     got: &lt;true&gt;

    __randomizedtesting.SeedInfo.seed([6124793851234CFA]:0)
    [...org.junit.*]
    org.elasticsearch.common.breaker.MemoryCircuitBreakerTests$5.run(MemoryCircuitBreakerTests.java:186)
    java.lang.Thread.run(Thread.java:745)
```
</description><key id="49237698">8535</key><summary>[CI Failure] MemoryCircuitBreakerTests.testThreadedUpdatesToChildBreakerWithParentLimit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>jenkins</label></labels><created>2014-11-18T14:33:14Z</created><updated>2014-11-21T23:04:56Z</updated><resolved>2014-11-21T23:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:04:56Z" id="64052070">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Resolve `now` in date ranges in percolator and alias filters at search time instead of parse time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8534</link><project id="" key="" /><description>PR for #8474. Percolator queries and index alias filters are parsed once and reused as long as they exist on a node. If they contain time based range filters with a `now` expression then the alias filters and percolator queries are going to be incorrect from the moment these are constructed (depending on the date rounding).

 If a `range` filter or `range` query is constructed as part of adding a percolator query or a index alias filter then these get wrapped in special query or filter wrappers that defer the resolution of now at last possible moment instead of during parse time. In the case of the range filter a special Resolvable Filter makes sure that `now` is resolved when the DocIdSet is pulled and in the case of the range query `now` is resolved at query rewrite time. Both occur at the time the range filter or query is used as apposed when the query or filter is constructed during parse time.

Note this PR is against the 1.x branch.
</description><key id="49229367">8534</key><summary>Resolve `now` in date ranges in percolator and alias filters at search time instead of parse time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Dates</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-18T13:40:54Z</created><updated>2015-06-07T18:06:32Z</updated><resolved>2014-11-19T21:30:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-19T13:43:42Z" id="63640988">left some comments
</comment><comment author="martijnvg" created="2014-11-19T14:33:03Z" id="63647923">@s1monw I applied the feedback and left one comment.
</comment><comment author="s1monw" created="2014-11-19T16:24:13Z" id="63667101">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ResolvableFilter.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterCachingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Core: Added query/filter wrapper that builds the actual query to be executed on the last possible moment to aid with index aliases and percolator queries using `now` date expression.</comment></comments></commit></commits></item><item><title>[CI Failure] DateHistogramOffsetTests.singleValue_WithPreOffset_MinDocCount</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8533</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_1x_regression/803/

```
REPRODUCE WITH  : mvn clean test -Dtests.seed=76338BDFEAB372C -Dtests.class=org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Berlin -Dtests.method="singleValue_WithPreOffset_MinDocCount" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.processors=8
Throwable:
java.lang.AssertionError: 
Expected: &lt;5L&gt;
     got: &lt;4L&gt;

    __randomizedtesting.SeedInfo.seed([76338BDFEAB372C:95FAB45878660D92]:0)
    [...org.junit.*]
    org.elasticsearch.search.aggregations.bucket.DateHistogramOffsetTests.singleValue_WithPreOffset_MinDocCount(DateHistogramOffsetTests.java:124)
    [...sun.*, com.carrotsearch.randomizedtesting.*, java.lang.reflect.*]
    org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
```
</description><key id="49225162">8533</key><summary>[CI Failure] DateHistogramOffsetTests.singleValue_WithPreOffset_MinDocCount</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>jenkins</label></labels><created>2014-11-18T13:13:54Z</created><updated>2014-11-21T23:05:06Z</updated><resolved>2014-11-21T23:05:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-11-18T13:18:54Z" id="63469311">Looks like it might be down to failure to create a shard:

```
Failed to create shard, message [IndexShardCreationException[[idx2][3] failed to create shard]; 
nested: LockObtainFailedException[Can't lock shard [idx2][3], timed out after 5000ms        
```
</comment><comment author="markharwood" created="2014-11-18T15:27:03Z" id="63487524">A round up of related issues - "Lock obtain" issues of this form:

```
org.apache.lucene.store.LockObtainFailedException: Can't lock shard [test][0], timed out after 0ms
```

are found in the following test results (grouped by issues raised so far):

https://github.com/elasticsearch/elasticsearch/issues/8533 :
    http://build-us-1.elasticsearch.org/job/es_core_1x_regression/803/

https://github.com/elasticsearch/elasticsearch/issues/8523 :
    http://build-us-1.elasticsearch.org/job/es_core_master_centos/1374/

https://github.com/elasticsearch/elasticsearch/issues/8522 : 
    http://build-us-1.elasticsearch.org/job/es_bwc_1x/4950/

https://github.com/elasticsearch/elasticsearch/issues/8515 :
    http://build-us-1.elasticsearch.org/job/es_core_master_metal/5354/ 

https://github.com/elasticsearch/elasticsearch/issues/8514 :
    http://build-us-1.elasticsearch.org/job/es_core_master_metal/5324/

https://github.com/elasticsearch/elasticsearch/issues/8511 :
    http://build-us-1.elasticsearch.org/job/es_bwc_1x/4941/
    http://build-us-1.elasticsearch.org/job/es_bwc_1x/4967/

https://github.com/elasticsearch/elasticsearch/issues/8509 :
    http://build-us-1.elasticsearch.org/job/es_core_master_debian/2402/
    http://build-us-1.elasticsearch.org/job/es_core_master_window-2008/603/ 

https://github.com/elasticsearch/elasticsearch/issues/8505 
    http://build-us-1.elasticsearch.org/job/es_core_master_metal/5320/
</comment><comment author="s1monw" created="2014-11-21T23:05:06Z" id="64052091">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Update execution hint docs for Significant terms agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8532</link><project id="" key="" /><description>copied over the relevant pieces from the terms agg
</description><key id="49216296">8532</key><summary>Docs: Update execution hint docs for Significant terms agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>docs</label></labels><created>2014-11-18T12:14:41Z</created><updated>2014-11-18T19:56:50Z</updated><resolved>2014-11-18T19:55:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-18T12:18:39Z" id="63461701">LGTM
</comment><comment author="markharwood" created="2014-11-18T13:32:53Z" id="63470897">When I've experienced slow-running significant terms recently it was fixed by setting the execution hint to `map` and my queries then ran radically faster. 
I'm not sure I had "very few" matching documents in these cases so the advice in the docs here may be misleading. It would be worth quantifying the numbers better perhaps? Even if it is using language like "hundreds", "thousands" or "millions". Maybe a matrix for matching doc/value number ranges and the preferred algo choices?
</comment><comment author="markharwood" created="2014-11-18T14:27:28Z" id="63478154">@bleskes Scratch my last comments. I checked back on the cases where `map` helped most and it looks like it was where there was a lot of matching docs but a top-level agg breaking them down into many buckets which then had a significant_terms agg analysing "few" docs.  The documentation looks good
</comment><comment author="bleskes" created="2014-11-18T19:56:28Z" id="63534118">Thx. Pushed to 1.3,1.4, 1.x and master
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update execution hint docs for Significant terms agg</comment></comments></commit></commits></item><item><title>Index corruption with delete by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8531</link><project id="" key="" /><description>I have index with parent "model" and child "sell" documents at one index with millions of documents and heavy concurrent indexing.

All nodes works with Oracle Hotspot 8 with 30gb heap, ElasticSearch 1.3.5.

I'm execute delete by query like this:

```
curl -XDELETE 'http://127.0.0.1:9200/sells/sell/_query' -d '{"query":{"filtered":{"query":{"match_all":{}},"filter":{"and":{"filters":[{"term":{"client_id":123}}]}}}}}]}'
```

and our index is corrupted with this error message and exception stack trace in logs:

```
[2014-11-18 12:49:33,650][DEBUG][action.deletebyquery     ] [elastic1] [sells][3], node[hv2718pkQm-5SZq7agcY9g], [P], s[STARTED]: Failed to execute [delete_by_query {[sells][sell], query [{"query":{"filtered":{"query":{"match_all":{}},"filter":{"and":{"filters":[{"term":{"client_id":123}}]}}}}}]}] 
org.elasticsearch.index.engine.RefreshFailedEngineException: [sells][3] Refresh failed 
        at org.elasticsearch.index.engine.internal.InternalEngine.refresh(InternalEngine.java:789) 
        at org.elasticsearch.index.engine.internal.InternalEngine.delete(InternalEngine.java:686) 
        at org.elasticsearch.index.shard.service.InternalIndexShard.deleteByQuery(InternalIndexShard.java:465) 
        at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnPrimary(TransportShardDeleteByQueryAction.java:123) 
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:535) 
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:434) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
        at java.lang.Thread.run(Thread.java:745) 
Caused by: java.lang.IllegalStateException: parentFilter must return FixedBitSet; got org.apache.lucene.search.BitsFilteredDocIdSet@74ebc73d 
        at org.elasticsearch.index.search.nested.IncludeNestedDocsQuery$IncludeNestedDocsWeight.scorer(IncludeNestedDocsQuery.java:123) 
        at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59) 
        at org.apache.lucene.index.BufferedUpdatesStream.applyQueryDeletes(BufferedUpdatesStream.java:554) 
        at org.apache.lucene.index.BufferedUpdatesStream.applyDeletesAndUpdates(BufferedUpdatesStream.java:287) 
        at org.apache.lucene.index.IndexWriter.applyAllDeletesAndUpdates(IndexWriter.java:3322) 
        at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:3313) 
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:425) 
        at org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:292) 
        at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:267) 
        at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:257) 
        at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:171) 
        at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:118) 
        at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58) 
        at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:176) 
        at org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:225) 
        at org.elasticsearch.index.engine.internal.InternalEngine.refresh(InternalEngine.java:779) 
        ... 8 more
```

I cannot reproduce this error on our test stand cluster, sorry. It's seems like a concurrent access error.

What's a kind of problem? Does it just bug?
</description><key id="49207122">8531</key><summary>Index corruption with delete by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mitallast</reporter><labels /><created>2014-11-18T10:55:00Z</created><updated>2014-11-18T11:16:29Z</updated><resolved>2014-11-18T11:16:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-18T11:16:29Z" id="63455370">duplicate of #8390

Thank you for the report.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can't sort on field with custom date format mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8530</link><project id="" key="" /><description>I'm running elasticsearch 1.4

``` javascript
{

    "status": 200,
    "name": "Bevatron",
    "cluster_name": "poc",
    "version": {
        "number": "1.4.0",
        "build_hash": "bc94bd81298f81c656893ab1ddddd30a99356066",
        "build_timestamp": "2014-11-05T14:26:12Z",
        "build_snapshot": false,
        "lucene_version": "4.10.2"
    },
    "tagline": "You Know, for Search"

}
```

I have a type with the following mapping:

``` javascript
{
   "bopo": {
      "mappings": {
         "ProductionTypeCapacity": {
            "properties": {
               "AddedDate": {
                  "type": "date",
                  "format": "dateOptionalTime"
               },
               "ChangedDate": {
                  "type": "date",
                  "format": "dateOptionalTime"
               },
               "CorporateBrandCapacities": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "HadesScriptUsed": {
                  "type": "string"
               },
               "Key": {
                  "type": "string"
               },
               "Month": {
                  "type": "date",
                  "format": "yyyy-MM"
               },
               "ProductionUnitId": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "SupplierId": {
                  "type": "string",
                  "index": "not_analyzed"
               }
            }
         }
      }
   }
}
```

A hit looks like this:

``` javascript
  {
            "_index": "bopo",
            "_type": "ProductionTypeCapacity",
            "_id": "ProductionTypeCapacity-120886-120288-2014-12",
            "_score": 1,
            "_source": {
               "Key": "120886-120288-2014-12",
               "AddedDate": "2014-03-04T02:33:04.9430000",
               "ChangedDate": "2014-03-04T10:12:24.3830000",
               "CorporateBrandCapacities": "Home",
               "Month": "2014-12",
               "ProductionUnitId": "120288",
               "SupplierId": "120886",
               "HadesScriptUsed": "CapacityRegistrationScript"
            }
         }
```

When I execute the following query:

``` javascript
POST bopo/ProductionTypeCapacity/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "Month": {
        "order": "desc"
      }
    }
  ]
}
```

I get this error:

``` javascript
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[81vkOCltQm6PWmlJSmiVog][bopo][0]: QueryPhaseExecutionException[[bopo][0]: query[ConstantScore(cache(_type:ProductionTypeCapacity))],from[0],size[10],sort[&lt;custom:\"Month\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@149bf6c8&gt;!]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.elasticsearch.index.fielddata.plain.AbstractAtomicOrdinalsFieldData$1 cannot be cast to org.elasticsearch.index.fielddata.AtomicNumericFieldData]; }{[81vkOCltQm6PWmlJSmiVog][bopo][1]: QueryPhaseExecutionException[[bopo][1]: query[ConstantScore(cache(_type:ProductionTypeCapacity))],from[0],size[10],sort[&lt;custom:\"Month\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@603af23&gt;!]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.elasticsearch.index.fielddata.plain.AbstractAtomicOrdinalsFieldData$1 cannot be cast to org.elasticsearch.index.fielddata.AtomicNumericFieldData]; }{[81vkOCltQm6PWmlJSmiVog][bopo][2]: QueryPhaseExecutionException[[bopo][2]: query[ConstantScore(cache(_type:ProductionTypeCapacity))],from[0],size[10],sort[&lt;custom:\"Month\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@695b5d68&gt;!]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.elasticsearch.index.fielddata.plain.AbstractAtomicOrdinalsFieldData$1 cannot be cast to org.elasticsearch.index.fielddata.AtomicNumericFieldData]; }{[81vkOCltQm6PWmlJSmiVog][bopo][3]: QueryPhaseExecutionException[[bopo][3]: query[ConstantScore(cache(_type:ProductionTypeCapacity))],from[0],size[10],sort[&lt;custom:\"Month\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@4627c21b&gt;!]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.elasticsearch.index.fielddata.plain.AbstractAtomicOrdinalsFieldData$1 cannot be cast to org.elasticsearch.index.fielddata.AtomicNumericFieldData]; }{[81vkOCltQm6PWmlJSmiVog][bopo][4]: QueryPhaseExecutionException[[bopo][4]: query[ConstantScore(cache(_type:ProductionTypeCapacity))],from[0],size[10],sort[&lt;custom:\"Month\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@446d2a0b&gt;!]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.elasticsearch.index.fielddata.plain.AbstractAtomicOrdinalsFieldData$1 cannot be cast to org.elasticsearch.index.fielddata.AtomicNumericFieldData]; }]",
   "status": 500
}
```
</description><key id="49204978">8530</key><summary>Can't sort on field with custom date format mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jensbengtsson</reporter><labels /><created>2014-11-18T10:33:33Z</created><updated>2016-04-28T17:44:35Z</updated><resolved>2014-11-25T13:44:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-23T14:00:11Z" id="64118787">Hi @jensbengtsson 

Are you creating mappings manually when you create the index, or are you relying on dynamic mapping to add fields for you?
</comment><comment author="jensbengtsson" created="2014-11-23T16:56:20Z" id="64124857">Manual mapping
</comment><comment author="clintongormley" created="2014-11-25T12:16:57Z" id="64391338">@jpountz please can you take a look
</comment><comment author="jpountz" created="2014-11-25T12:26:20Z" id="64392250">This looks to me like a conflict between several types, one having `Month` mapped as a string which has been used to build fielddata, and another one mapping `Month` as a date.

@jensbengtsson Does your `bopo` index have another type that maps `Month` as a string? Note that this issue would also occur if you first indexed some data with `Month` mapped as a string, deleted the type (only the type, not the whole index), and then indexed data again with `Month` mapped as a date.
</comment><comment author="jensbengtsson" created="2014-11-25T12:37:24Z" id="64393335">@jpountz
OK, I thought deleting the mapping would make me able to change the type definition. But then it makes sense because the entire index has not been recreated, only the specific type mapping. 
</comment><comment author="jpountz" created="2014-11-25T13:44:28Z" id="64400949">@jensbengtsson I am closing this issue now, but please reopen if recreating the whole index doesn't solve the issue.
</comment><comment author="bimalkeeth" created="2016-04-27T05:59:48Z" id="214979160">Hi  
Restricting Deleting type cause huge bottle neck in parent type indexing as is allways required to create new index. we can not drop  particuler type and recreate.
</comment><comment author="jpountz" created="2016-04-27T07:11:26Z" id="214990428">@bimalkeeth We do not deny that deleting types would be useful. However, there is no way to properly support it at the Lucene level, so we had no choice but to remove this feature which was causing trouble in the past since Lucene's field infos could go out of sync with elasticsearch's mappings.
</comment><comment author="clintongormley" created="2016-04-28T17:44:35Z" id="215507169">@bimalkeeth the new [reindex api](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-reindex.html) makes reindexing really easy
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix for geohash neighbors when geohash length is even.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8529</link><project id="" key="" /><description>We don't have to set XLimit and YLimit depending on the level (even or odd), since semantics of x and y are already swapped on each level.
XLimit is always 7 and YLimit is always 3.

Close #8526
</description><key id="49203574">8529</key><summary>Fix for geohash neighbors when geohash length is even.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">clement-tourriere</reporter><labels><label>:Geo</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-18T10:18:51Z</created><updated>2015-06-07T18:10:40Z</updated><resolved>2014-11-25T19:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-23T12:42:44Z" id="64116615">@nknize can you take a look at this?
</comment><comment author="nknize" created="2014-11-24T19:58:23Z" id="64253904">This is a correct and simple fix to a legit bug.  As the comment mentions, x/y semantics were already swapped on each geohash level.  xLimit and yLimit were reversing the semantics and the unit tests did not catch this boundary condition.  I wrote an additional test case to cover the odd and even boundary condition.  
</comment><comment author="clintongormley" created="2014-11-25T16:58:06Z" id="64432889">@nknize in that case you want to rebase and merge it in to 1.3, 1.4, 1.x and master please?
</comment><comment author="nknize" created="2014-11-25T19:38:50Z" id="64458026">@clement-tourriere proposed pr merged at request of @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] IndexStatsTests.throttleStats "Delete Index failed - not acked"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8528</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_1x_metal/4765

```
  1&gt; Throwable:
  1&gt; java.lang.AssertionError: Delete Index failed - not acked
  1&gt; Expected: &lt;true&gt;
  1&gt;      but: was &lt;false&gt;
  1&gt;     __randomizedtesting.SeedInfo.seed([567E75887CCCE111:EB7D7A75ECB82CA5]:0)
  1&gt;     org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
  1&gt;     org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:114)
  1&gt;     org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:110)
  1&gt;     org.elasticsearch.test.TestCluster.wipeIndices(TestCluster.java:138)
  1&gt;     org.elasticsearch.test.TestCluster.wipe(TestCluster.java:75)
  1&gt;     org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:616)
  1&gt;     org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1788)
```

This failure has plagued us for some time.  It happens during test cleanup, when we ask ES to delete the index, but that delete takes &gt; 30 seconds (times out) and causes the not acked.

The question is why the delete would take so long.

We recently (#7730) changed MockFSDirectoryService to do a flush when an index is closed, in order to run check index, and for this test that flush sometimes (rarely) takes &gt; 30 seconds.

On the most recent iteration, I added "dump all threads" when this assert trips, so we got all threads in this failure, but ... it does not reveal much (no deadlock).  I see one thread stuck in fsync:

```
  1&gt;   421) Thread[id=5152, name=elasticsearch[node_s0][generic][T#2], state=RUNNABLE, group=TGRP-IndexStatsTests]
  1&gt;         at sun.nio.ch.FileDispatcherImpl.force0(Native Method)
  1&gt;         at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)
  1&gt;         at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:386)
  1&gt;         at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:316)
  1&gt;         at org.apache.lucene.store.FSDirectory.fsync(FSDirectory.java:415)
  1&gt;         at org.apache.lucene.store.FSDirectory.sync(FSDirectory.java:310)
  1&gt;         at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:74)
  1&gt;         at org.elasticsearch.index.store.DistributorDirectory.sync(DistributorDirectory.java:115)
  1&gt;         at org.apache.lucene.store.MockDirectoryWrapper.sync(MockDirectoryWrapper.java:233)
  1&gt;         at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:74)
  1&gt;         at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:4519)
  1&gt;         at org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2994)
  1&gt;         at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3097)
  1&gt;         at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3064)
  1&gt;         at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:924)
  1&gt;         at org.elasticsearch.index.shard.service.InternalIndexShard.flush(InternalIndexShard.java:628)
  1&gt;         at org.elasticsearch.test.store.MockFSDirectoryService$1.beforeIndexShardClosed(MockFSDirectoryService.java:89)
```

And then, because MockDirectoryWrapper is heavily sync'd, I see other threads in Lucene (one merging, one trying to do refresh) blocked in MDW's methods.

Net/net this seems to indicate that fsync of the many small files created by this test is just too slow.
</description><key id="49201867">8528</key><summary>[CI Failure] IndexStatsTests.throttleStats "Delete Index failed - not acked"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>jenkins</label></labels><created>2014-11-18T10:00:22Z</created><updated>2014-11-18T16:56:36Z</updated><resolved>2014-11-18T12:23:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-11-18T12:24:41Z" id="63462515">OK from everything I can see, fsync() is just taking too long for this test sometimes ... so I pushed the workaround of optimizing &amp; flushing in the test before handing things over to cleanup ...
</comment><comment author="kimchy" created="2014-11-18T12:30:31Z" id="63463455">@mikemccand I wonder if we should add "store" stats around how long fsync takes? I think its a good indication how well the storage device is doing?
</comment><comment author="mikemccand" created="2014-11-18T15:45:12Z" id="63490808">@kimchy I like that idea, I'll work on that.
</comment><comment author="mikemccand" created="2014-11-18T16:56:36Z" id="63503793">@kimchy Actually we already have flush stats (count + msec); I'm not sure tracking store.fsync time will really add much value over that?

Also, in order to track fsync time, I think we'd need to wrap all our Directory instances with FilterDirectory so we can spy on the fsync calls, and adding more delegation especially at this low level makes me nervous...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file></files><comments><comment>Test: force merge index in the end of IndexStatsTests.throttleStats</comment></comments></commit></commits></item><item><title>Parser throws NullPointerException when Filter aggregation clause is empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8527</link><project id="" key="" /><description>Added Junit test that recreates the error and fixed FilterParser to default to using a MatchAllDocsFilter if the requested filter clause is left empty.
Closes #8438
</description><key id="49201585">8527</key><summary>Parser throws NullPointerException when Filter aggregation clause is empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-18T09:57:27Z</created><updated>2015-06-07T17:49:27Z</updated><resolved>2014-11-20T14:12:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-18T22:12:36Z" id="63555218">The fix looks good, I think we need the same fix in the `filters` (with an `s`) aggregation?
</comment><comment author="jpountz" created="2014-11-20T09:28:29Z" id="63781491">LGTM
</comment><comment author="markharwood" created="2014-11-20T14:12:51Z" id="63813228">Pushed to 1.x, 1.4 and master https://github.com/elasticsearch/elasticsearch/commit/0c9431499671d214d2c64931779a00b50cf7e9c4
</comment><comment author="markharwood" created="2014-11-21T12:29:28Z" id="63963629">Pushed to 1.3, minus the changes for the Filters (with an "s") agg as it doesn't exist there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo: incorrect neighbours computation in GeoHashUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8526</link><project id="" key="" /><description>GeoHashUtils.neighbor produces bad neighbours for even level geohash (when geohash length is even).

For instance : 

For geohash `u09tv` : 

http://geohash.gofreerange.com/ (this geohash is in Paris, France).

Real neighbours for this geohash are `[u09wh, u09wj, u09wn, u09tu, u09ty, u09ts, u09tt, u09tw]`

GeoHashUtils.neigbors returns `[u09qh, u09wj, u09yn, u09mu, u09vy, u09ks, u09st, u09uw]`
</description><key id="49199633">8526</key><summary>Geo: incorrect neighbours computation in GeoHashUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">clement-tourriere</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2014-11-18T09:36:25Z</created><updated>2014-11-25T19:32:12Z</updated><resolved>2014-11-25T19:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>src/test/java/org/elasticsearch/index/search/geo/GeoHashUtilsTests.java</file></files><comments><comment>Fix for geohash neighbors when geohash length is even.</comment><comment>We don't have to set XLimit and YLimit depending on the level (even or odd), since semantics of x and y are already swapped on each level.</comment><comment>XLimit is always 7 and YLimit is always 3.</comment></comments></commit></commits></item><item><title>Update update-settings.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8525</link><project id="" key="" /><description>Inconsistent indentation
</description><key id="49180392">8525</key><summary>Update update-settings.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdzor</reporter><labels /><created>2014-11-18T04:44:13Z</created><updated>2014-11-23T13:46:11Z</updated><resolved>2014-11-23T13:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-23T13:46:07Z" id="64118354">thanks @mdzor - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update update-settings.asciidoc</comment></comments></commit></commits></item><item><title>[CI Failure] SimpleRecoveryLocalGatewayTests.testReusePeerRecovery failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8524</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_1x_metal/4745/testReport/junit/org.elasticsearch.gateway.local/SimpleRecoveryLocalGatewayTests/testReusePeerRecovery/

```
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];
    at org.elasticsearch.cluster.block.ClusterBlocks.indexBlockedException(ClusterBlocks.java:171)
    at org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction.checkBlock(TransportCreateIndexAction.java:70)
    at org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction.checkBlock(TransportCreateIndexAction.java:41)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction.innerExecute(TransportMasterNodeOperationAction.java:96)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction.doExecute(TransportMasterNodeOperationAction.java:88)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction.doExecute(TransportMasterNodeOperationAction.java:43)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$TransportHandler.messageReceived(TransportMasterNodeOperationAction.java:251)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$TransportHandler.messageReceived(TransportMasterNodeOperationAction.java:235)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49158353">8524</key><summary>[CI Failure] SimpleRecoveryLocalGatewayTests.testReusePeerRecovery failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>jenkins</label></labels><created>2014-11-17T23:08:57Z</created><updated>2014-11-21T23:04:31Z</updated><resolved>2014-11-21T23:04:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:04:31Z" id="64052002">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] SearchWithRandomExceptionsTests.testRandomDirectoryIOExceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8523</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_master_centos/1374/

```
java.lang.AssertionError: 
Expected: &lt;88L&gt;
     got: &lt;82L&gt;

  at __randomizedtesting.SeedInfo.seed([61E81BB8263D784B:28147790387B93EA]:0)
  at org.junit.Assert.assertThat(Assert.java:780)
  at org.junit.Assert.assertThat(Assert.java:738)
  at org.elasticsearch.search.basic.SearchWithRandomExceptionsTests.testRandomDirectoryIOExceptions(SearchWithRandomExceptionsTests.java:174)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:827)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:877)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
  at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49157245">8523</key><summary>[CI Failure] SearchWithRandomExceptionsTests.testRandomDirectoryIOExceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>jenkins</label></labels><created>2014-11-17T22:56:30Z</created><updated>2014-11-21T23:05:29Z</updated><resolved>2014-11-21T23:05:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:05:29Z" id="64052115">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] FunctionScoreBackwardCompatibilityTests.testSimpleFunctionScoreParsingWorks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8522</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_bwc_1x/4950/

```
java.lang.AssertionError: Expected different hit count.  Total shards: 8 Successful shards: 8 &amp; 0 shard failures:
Expected: &lt;10&gt;
     but: was &lt;9&gt;
  at __randomizedtesting.SeedInfo.seed([A61FCF132EF99935:6BFFD5C3497A986E]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertOrderedSearchHits(ElasticsearchAssertions.java:174)
  at org.elasticsearch.search.functionscore.FunctionScoreBackwardCompatibilityTests.checkFunctionScoreStillWorks(FunctionScoreBackwardCompatibilityTests.java:109)
  at org.elasticsearch.search.functionscore.FunctionScoreBackwardCompatibilityTests.testSimpleFunctionScoreParsingWorks(FunctionScoreBackwardCompatibilityTests.java:94)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:827)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:877)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
```
</description><key id="49156197">8522</key><summary>[CI Failure] FunctionScoreBackwardCompatibilityTests.testSimpleFunctionScoreParsingWorks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>jenkins</label></labels><created>2014-11-17T22:46:12Z</created><updated>2014-11-21T23:05:37Z</updated><resolved>2014-11-21T23:05:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:05:37Z" id="64052128">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[GEO] Fix for geo_shape query with polygon from -180/90 to 180/-90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8521</link><project id="" key="" /><description>This fix adds a simple consistency check that intersection edges appear pairwise. Polygonal boundary tests were passing (false positive) on the Eastern side of the dateline simply due to the initial order (edge direction) of the intersection edges.  Polygons in the Eastern hemispehere (which were not being tested) were correctly failing inside of JTS due to an attempt to connect incorrect intersection edges (that is, edges that were not even intersections). While this patch fixes issue/8467 (and adds broader test coverage) it is not intented as a long term solution.  The mid term fix (in work) will refactor all geospatial computational geometry to use ENU / ECF coordinate systems for higher accuracy and eliminate brute force mercator checks and conversions.

Closes #8467
</description><key id="49156002">8521</key><summary>[GEO] Fix for geo_shape query with polygon from -180/90 to 180/-90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0</label></labels><created>2014-11-17T22:44:20Z</created><updated>2014-11-24T15:32:28Z</updated><resolved>2014-11-24T15:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2014-11-20T21:12:47Z" id="63880591">Removed IntersectionOrder.SENTINEL per @colings86 request.  No need to add extra connection check as its taken care of by the intersection function.  

Note: This is not a catch all boundary condition fix.  There are discrepancies in the ordering of the vertexes for a polygon.  GeoJSON has no strict rule for vertex ordering where OGC WKT uses the right-hand rule (counter-clockwise).  JTS, on the other hand, implements left-hand rule (clockwise).  Due to these discrepancies current date-line wrapping logic will still throw topology exceptions on certain polygons that wrap the dateline.  Since this is typically &lt;3% of polygon cases its rarely an issue, but one that needs to be noted and addressed in a separate issue.
</comment><comment author="colings86" created="2014-11-24T09:50:09Z" id="64171022">Left one minor comment, otherwise LGTM
</comment><comment author="colings86" created="2014-11-24T13:43:56Z" id="64194994">LGTM
</comment><comment author="nknize" created="2014-11-24T15:32:28Z" id="64210397">Merged to Master, 1.4, 1.x
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] UpgradeTest.testUpgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8520</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_bwc_1x/4949/

```
java.lang.AssertionError: timed out waiting for green state
Expected: &lt;false&gt;
     got: &lt;true&gt;

  at __randomizedtesting.SeedInfo.seed([802A6DBD6BF2603E:D4665F04E4E303CE]:0)
  at org.junit.Assert.assertThat(Assert.java:780)
  at org.elasticsearch.test.ElasticsearchIntegrationTest.ensureGreen(ElasticsearchIntegrationTest.java:950)
  at org.elasticsearch.test.ElasticsearchIntegrationTest.ensureGreen(ElasticsearchIntegrationTest.java:935)
  at org.elasticsearch.rest.action.admin.indices.upgrade.UpgradeTest.testUpgrade(UpgradeTest.java:144)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:827)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:877)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
  at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49155445">8520</key><summary>[CI Failure] UpgradeTest.testUpgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>jenkins</label></labels><created>2014-11-17T22:38:45Z</created><updated>2014-11-21T23:06:47Z</updated><resolved>2014-11-21T23:06:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:06:47Z" id="64052276">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Register data.path for all nodes on close in InternalTestCluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8519</link><project id="" key="" /><description>We need to register those data paths otherwise we might miss path that
need to get cleaned when using local gatway etc. which can otherwise
cause imports of dangeling indices.
</description><key id="49145841">8519</key><summary>[TEST] Register data.path for all nodes on close in InternalTestCluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-17T21:11:34Z</created><updated>2015-03-19T10:18:37Z</updated><resolved>2014-11-18T12:40:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-18T09:48:58Z" id="63445342">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Added new elasticsearch-river-kafka plugin to the documentation (u...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8518</link><project id="" key="" /><description>...ses latest version of Kafka, EL Bulk API, and supports concurrent requests)

The differences between existing and this new plugin:
- uses Kafka 0.8.1.1 (latest version)
- uses Elasticsearch 1.4.0 (latest version)
- makes use of Bulk API (to have better performance)
- can be executed in concurrent requests
- abstracts the consuming mechanism (can consume Kafka messages from multiple brokers and partitions)

@clintongormley please review (a follow up on the closed PL - https://github.com/elasticsearch/elasticsearch/pull/8443)
</description><key id="49142593">8518</key><summary>Docs: Added new elasticsearch-river-kafka plugin to the documentation (u...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mariamhakobyan</reporter><labels /><created>2014-11-17T20:41:50Z</created><updated>2014-11-23T11:30:08Z</updated><resolved>2014-11-23T11:30:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-23T11:30:01Z" id="64114805">thanks @mariamhakobyan - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Added new elasticsearch-river-kafka plugin to the documentation (uses latest version of Kafka, EL Bulk API, and supports concurrent requests)</comment></comments></commit></commits></item><item><title>[CI Failure] CorruptedFileTest.testCorruptFileAndRecover</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8517</link><project id="" key="" /><description>Occurred in same test run as #8516 and #8515

http://build-us-1.elasticsearch.org/job/es_core_13_debian/2379/

``` java
java.lang.NullPointerException
    at org.elasticsearch.test.ElasticsearchIntegrationTest.client(ElasticsearchIntegrationTest.java:603)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:541)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1566)
    at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:884)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49137349">8517</key><summary>[CI Failure] CorruptedFileTest.testCorruptFileAndRecover</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">stormpython</reporter><labels><label>jenkins</label></labels><created>2014-11-17T19:53:00Z</created><updated>2015-04-28T09:21:18Z</updated><resolved>2015-04-28T09:21:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-11-17T20:26:39Z" id="63369723">Also http://build-us-1.elasticsearch.org/job/es_core_13_debian/2379/
</comment><comment author="s1monw" created="2014-11-21T23:11:30Z" id="64052782">I will look into this - this seems like happening more often lately but always with a delete file...
</comment><comment author="s1monw" created="2015-04-28T09:21:17Z" id="96985606">this has been fixed long ago... closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] CorruptedFileTest.testCorruptFileThenSnapshotAndRestore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8516</link><project id="" key="" /><description>Occurred in same test run as #8517 and #8515

http://build-us-1.elasticsearch.org/job/es_core_13_debian/2379/

``` java
java.lang.NullPointerException
    at org.elasticsearch.test.ElasticsearchIntegrationTest.client(ElasticsearchIntegrationTest.java:603)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:541)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1566)
    at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:884)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49137081">8516</key><summary>[CI Failure] CorruptedFileTest.testCorruptFileThenSnapshotAndRestore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stormpython</reporter><labels><label>jenkins</label></labels><created>2014-11-17T19:50:33Z</created><updated>2014-11-21T23:09:45Z</updated><resolved>2014-11-21T23:09:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-11-17T20:26:18Z" id="63369672">Also http://build-us-1.elasticsearch.org/job/es_core_13_debian/2379/
</comment><comment author="s1monw" created="2014-11-21T23:09:45Z" id="64052595">closed in favor of #8517
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] CorruptedFileTest.testCorruptPrimaryNoReplica</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8515</link><project id="" key="" /><description>Occurred in same test run as #8517 and #8516.

http://build-us-1.elasticsearch.org/job/es_core_13_debian/2379/

``` java
java.lang.AssertionError: 
Expected: is &lt;RED&gt;
     got: &lt;GREEN&gt;

    at __randomizedtesting.SeedInfo.seed([103D0E2E850F24A2:3501B89069A010B5]:0)
    at org.junit.Assert.assertThat(Assert.java:780)
    at org.junit.Assert.assertThat(Assert.java:738)
    at org.elasticsearch.index.store.CorruptedFileTest.testCorruptPrimaryNoReplica(CorruptedFileTest.java:265)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:826)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:862)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:876)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49136898">8515</key><summary>[CI Failure] CorruptedFileTest.testCorruptPrimaryNoReplica</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stormpython</reporter><labels><label>jenkins</label></labels><created>2014-11-17T19:49:03Z</created><updated>2014-11-21T23:05:48Z</updated><resolved>2014-11-21T23:05:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-11-17T20:26:02Z" id="63369647">Also http://build-us-1.elasticsearch.org/job/es_core_13_debian/2379/
</comment><comment author="markharwood" created="2014-11-18T11:14:18Z" id="63455129">Same test but different error: "is still locked after 5 sec waiting"

```
org.elasticsearch.transport.ConnectTransportException: [node_s3][inet[/178.63.88.246:9609]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:807)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:741)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:714)
    at org.elasticsearch.test.transport.MockTransportService$LookupTestTransport.connectToNode(MockTransportService.java:293)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:156)
    at org.elasticsearch.discovery.zen.ZenDiscovery.joinElectedMaster(ZenDiscovery.java:465)
    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:417)
```

 in http://build-us-1.elasticsearch.org/job/es_core_master_metal/5354/

```
REPRODUCE WITH  : mvn clean test -Dtests.seed=ABAC0878EA9900C0 -Dtests.class=org.elasticsearch.index.store.CorruptedFileTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Berlin -Dtests.method="testCorruptPrimaryNoReplica" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.security.manager=true -Dtests.nightly=true -Dtests.heap.size=528m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:-UseCompressedOops" -Dtests.processors=8
```
</comment><comment author="s1monw" created="2014-11-21T23:05:48Z" id="64052152">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] BulkTests.testThatInvalidIndexNamesShouldNotBreakCompleteBulkRequest failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8514</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_master_metal/5324/

```
java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_0.fdx=1, _0.fdt=1}
    __randomizedtesting.SeedInfo.seed([73AA1235A385C7E4:C12CDFCF864B9133]:0)
    org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:746)
    org.elasticsearch.test.store.MockDirectoryHelper$ElasticsearchMockDirectoryWrapper.close(MockDirectoryHelper.java:140)
    org.apache.lucene.store.FilterDirectory.close(FilterDirectory.java:95)
    org.elasticsearch.index.store.Store$StoreDirectory.innerClose(Store.java:562)
    org.elasticsearch.index.store.Store$StoreDirectory.access$100(Store.java:539)
    org.elasticsearch.index.store.Store.closeInternal(Store.java:359)
    org.elasticsearch.index.store.Store.access$000(Store.java:82)
    org.elasticsearch.index.store.Store$1.closeInternal(Store.java:101)
    org.elasticsearch.common.util.concurrent.AbstractRefCounted.decRef(AbstractRefCounted.java:64)
    org.elasticsearch.index.store.Store.decRef(Store.java:334)
    org.elasticsearch.index.engine.internal.InternalEngine$RecoveryCounter.endRecovery(InternalEngine.java:1693)
    org.elasticsearch.index.engine.internal.InternalEngine$RecoveryCounter.close(InternalEngine.java:1700)
    org.elasticsearch.common.lease.Releasables.close(Releasables.java:45)
    org.elasticsearch.common.lease.Releasables.closeWhileHandlingException(Releasables.java:70)
    org.elasticsearch.common.lease.Releasables.closeWhileHandlingException(Releasables.java:75)
    org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1169)
    org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:676)
    org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
    org.elasticsearch.indices.recovery.RecoverySource.access$000(RecoverySource.java:42)
    org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:126)
    org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:112)
    org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:274)
    org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: unclosed IndexOutput: _0.fdx
    org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:630)
    org.apache.lucene.store.MockDirectoryWrapper.createOutput(MockDirectoryWrapper.java:602)
    org.apache.lucene.store.FilterDirectory.createOutput(FilterDirectory.java:64)
    org.apache.lucene.store.TrackingDirectoryWrapper.createOutput(TrackingDirectoryWrapper.java:44)
    org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.&lt;init&gt;(CompressingStoredFieldsWriter.java:109)
    org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsWriter(CompressingStoredFieldsFormat.java:115)
    org.apache.lucene.index.DefaultIndexingChain.initStoredFieldsWriter(DefaultIndexingChain.java:81)
    org.apache.lucene.index.DefaultIndexingChain.startStoredFields(DefaultIndexingChain.java:263)
    org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:307)
    org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:240)
    org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:455)
    org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1398)
    org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1133)
    org.elasticsearch.index.engine.internal.InternalEngine.innerIndex(InternalEngine.java:603)
    org.elasticsearch.index.engine.internal.InternalEngine.index(InternalEngine.java:532)
    org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:447)
    org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:439)
    org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:150)
    org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511)
    org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419)
    java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    java.lang.Thread.run(Thread.java:745)
```
</description><key id="49132071">8514</key><summary>[CI Failure] BulkTests.testThatInvalidIndexNamesShouldNotBreakCompleteBulkRequest failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>jenkins</label></labels><created>2014-11-17T19:06:21Z</created><updated>2014-11-21T23:06:00Z</updated><resolved>2014-11-21T23:06:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:06:00Z" id="64052182">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix geohash grid aggregation on multi-valued fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8513</link><project id="" key="" /><description>This aggregation creates an anonymous fielddata instance that takes geo points
and turns them into a geo hash encoded as a long. A bug was introduced in 1.4
because of a fielddata refactoring: the fielddata instance tries to populate
an array with values without first making sure that it is large enough.

Close #8507
</description><key id="49122420">8513</key><summary>Fix geohash grid aggregation on multi-valued fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>:Geo</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-17T17:50:25Z</created><updated>2015-06-07T18:10:47Z</updated><resolved>2014-11-20T09:38:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-17T23:29:22Z" id="63396642">@rjernst I pushed a new commit. Is it what you had in mind?
</comment><comment author="rjernst" created="2014-11-18T02:13:28Z" id="63411968">Yes, thanks.  LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Geohash grid should count documents only once</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8512</link><project id="" key="" /><description>If two geo-hashes of the same document fall into the same bucket, then the document count for this bucket will be incremented by 2 instead of 1. Here is a reproduction:

```
DELETE test

PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "points": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT test/test/1
{
  "points": [
    "1,2",
    "2,3"
  ]
}

GET /test/_search?search_type=count
{
  "aggs": {
    "grid": {
      "geohash_grid": {
        "field": "points",
        "precision": 2
      }
    }
  }
}
```

which returns:

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "grid": {
         "buckets": [
            {
               "key": "s0",
               "doc_count": 2
            }
         ]
      }
   }
}
```
</description><key id="49121380">8512</key><summary>Aggregations: Geohash grid should count documents only once</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>:Geo</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-17T17:41:22Z</created><updated>2014-11-21T10:02:17Z</updated><resolved>2014-11-21T10:02:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ejain" created="2014-11-17T18:45:16Z" id="63353368">btw the geohash grid aggregation fails in 1.4.0 if documents with more than one value are present, see https://github.com/elasticsearch/elasticsearch/issues/8507
</comment><comment author="jpountz" created="2014-11-17T22:56:38Z" id="63392544">Indeed, I found this issue while working on a fix for that issue! :) https://github.com/elasticsearch/elasticsearch/pull/8513
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file></files><comments><comment>Aggregations: Fix geohash grid doc counts computation on multi-valued fields.</comment></comments></commit></commits></item><item><title>[CI Failure] BasicBackwardsCompatibilityTest.testExistsFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8511</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_bwc_1x/4941/
org.elasticsearch.bwcompat.BasicBackwardsCompatibilityTest.testExistsFilter

``` java
java.lang.Exception: Test abandoned because suite timeout was reached.
    at __randomizedtesting.SeedInfo.seed([9E5FFB0C75D9795B]:0)
```

junit.framework.TestSuite.org.elasticsearch.bwcompat.BasicBackwardsCompatibilityTest

``` java
java.lang.Exception: Suite timeout exceeded (&gt;= 1200000 msec).
    at __randomizedtesting.SeedInfo.seed([9E5FFB0C75D9795B]:0)
```

junit.framework.TestSuite.org.elasticsearch.bwcompat.BasicBackwardsCompatibilityTest

``` java
java.lang.RuntimeException: Suite replaced Thread.defaultUncaughtExceptionHandler. It's better not to touch it. Or at least revert it to what it was before. Current: class org.elasticsearch.test.ElasticsearchTestCase$ElasticsearchUncaughtExceptionHandler
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:576)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.run(RandomizedRunner.java:435)
    at com.carrotsearch.ant.tasks.junit4.slave.SlaveMain.execute(SlaveMain.java:199)
    at com.carrotsearch.ant.tasks.junit4.slave.SlaveMain.main(SlaveMain.java:310)
    at com.carrotsearch.ant.tasks.junit4.slave.SlaveMainSafe.main(SlaveMainSafe.java:12)
```
</description><key id="49120027">8511</key><summary>[CI Failure] BasicBackwardsCompatibilityTest.testExistsFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stormpython</reporter><labels><label>jenkins</label></labels><created>2014-11-17T17:29:46Z</created><updated>2014-11-21T23:06:10Z</updated><resolved>2014-11-21T23:06:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-11-18T12:30:52Z" id="63463517">Another timeout in the same test: http://build-us-1.elasticsearch.org/job/es_bwc_1x/4967/

```
java.lang.AssertionError: timed out waiting for yellow
Expected: &lt;false&gt;
     got: &lt;true&gt;

    at __randomizedtesting.SeedInfo.seed([7091587BBB69F086:A0817F9CF3A145C0]:0)
    at org.junit.Assert.assertThat(Assert.java:780)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.ensureYellow(ElasticsearchIntegrationTest.java:1072)
    at org.elasticsearch.bwcompat.BasicBackwardsCompatibilityTest.testExistsFilter(BasicBackwardsCompatibilityTest.java:420)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
```
</comment><comment author="s1monw" created="2014-11-21T23:06:10Z" id="64052196">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: SingleBucketAggregation should create a single bucket </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8510</link><project id="" key="" /><description>`SingleBucketAggregations` (like filter aggregation) have no method `getBuckets()` and also the json response contains no buckets array. This saves some space but also makes it harder to traverse the aggregation tree because when looking at the result one always has to know if the aggregation that produced the current level was a `SingleBucketAggregation` or a `MultiBucketAggregation` (like terms agg).

Example for json:

Request with top level multi bucket agg:

```
POST testidx/_search?size=0
{
  "aggs": {
    "terms": {
      "terms": {
        "field": "label",
        "size": 10
      },
      "aggs": {
        "histogram": {
          "histogram": {
            "field": "num",
            "interval": 1
          }
        }
      }
    }
  }
}
```

Request with top level single bucket agg:

```
POST testidx/_search?size=0
{
  "aggs": {
    "terms": {
      "filter": {
        "term": {
          "label": "label_a"
        }
      },
      "aggs": {
        "histogram": {
          "histogram": {
            "field": "num",
            "interval": 1
          }
        }
      }
    }
  }
}

```

multi bucket yields:

```
"aggregations": {
      "terms": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "label_a",
               "doc_count": 3,
               "histogram": {
                  "buckets": [
                     {
                        "key": 1,
                        "doc_count": 1
                     },
                    ...

```

single bucket yields:

```

"aggregations": {
      "terms": {
         "doc_count": 3,
         "histogram": {
            "buckets": [
               {
                  "key": 1,
                  "doc_count": 1
               },
               ...

```

although the two requests have the same level of "nestedness". If I was to post process the result I would have to change whichever application is consuming it when I change the top level aggregation from single to multibucket or the other way round.

The following would be more convenient for the second request:

```
"aggregations": {
    "terms": {
      "doc_count": 3,
      "buckets": [
        {
          "key": "some_key_that_makes_sense",
          "histogram": {
            "buckets": [
              {
                "key": 1,
                "doc_count": 1
              },
            ...

```

This also affects the coming soon `getProperty` method for aggregations which is currently implemented to be consistent with the different behavior of single and multi buckets: https://github.com/elasticsearch/elasticsearch/pull/8421#discussion_r20424480
</description><key id="49119675">8510</key><summary>Aggregations: SingleBucketAggregation should create a single bucket </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>discuss</label><label>high hanging fruit</label><label>stalled</label></labels><created>2014-11-17T17:26:40Z</created><updated>2017-06-26T15:36:10Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2014-11-17T18:15:41Z" id="63348799">+1

While I agree that it makes sense to have `SingleBucketAggregation` return the same structure as `MultiBucketAggregation`, you could also leverage the new `meta` field which was added in #8279 to specifically to deal with scenarios like this.

This is especially an issue in NEST, since it's a strongly-typed client and we need to know the exact C# type to deserialize the agg response to.

The `meta` field will greatly simplify this code in particular: https://github.com/elasticsearch/elasticsearch-net/blob/develop/src/Nest/Resolvers/Converters/Aggregations/AggregationConverter.cs#L26, since we'll be able to associate each agg request with the response type in the `meta` field.

cc @Mpdreamz 
</comment><comment author="Mpdreamz" created="2014-11-17T18:58:52Z" id="63355698">+1, being able to program against both in a similar fashion would be great. Something being either a leave or node is an easier mental model then having two types of nodes where the shape is dependent on parent metadata not actually returned in the response.

That said: `"key": "some_key_that_makes_sense",` is that a trivial problem? If we can't give it any meaningful key for `SingleBucketAggregation`'s, leaving it out might also cause code that looks for buckets and keys to misbehave. Having said this I remember generated keys being discussed somewhere but my memory is drawing a blank on where and what the conclusion was :)
</comment><comment author="uboness" created="2014-11-18T09:47:56Z" id="63445219">-1 on the JSON side ... Single bucket ages both semantically and logically hold no buckets... It'd be like saying that java shouldn't have objects and instead you should always work with collections.... And to work with a single object you'll need to have a collection of size 1... Just so you'd work with objects the same way you work with collections.

Yes... You need to know what you're working with... On the lang clients on the other hand it might make sense to generalize the tree traversal 
</comment><comment author="clintongormley" created="2014-11-18T09:51:17Z" id="63445609">@uboness i think it depends on the consumer.  If a user writes a single agg then it makes sense to return that value without the intermediate bucket.  For general purpose tools like Kibana it may make more sense to have a standard representation.  I'm wondering if this more verbose syntax should be a query time option.
</comment><comment author="uboness" created="2014-11-18T12:48:57Z" id="63465956">even with the generic consumers (e.g. kibana), they still need to distinguish between metrics and buckets right? and I also wonder how generic is it? For example, I'm sure the kibana is very much aware of the different types of aggs it exposes to the users and so there's dedicated code for each anyway (visualize the different aggs differently regardless of their nature).

One of the things I always liked in the json structure of aggs is that it's very intuitive and human readable. Navigating like `last_year.monthly_purchases.january.avg_price`... is more intuitive than `last_year.xxxxx.monthly_purchases.january.avg_price` where `last_year` is a filter agg (no clue what this `xxxxx` is as it has not logical explanation)
</comment><comment author="brwe" created="2014-11-18T15:06:07Z" id="63484082">&gt; the json structure of aggs is that it's very intuitive and human readable. 

Cannot say that for me but if enough people feel that way then maybe @clintongormley proposal to have query time parameter flag for that might make sense.

&gt; Yes... You need to know what you're working with...

This is actually the problem and why I opened the issue. It would be great to have the result in a such generic way that one would not have to know the type of aggregation because the structure is always the same. 
Metrics and buckets would still have to be distinguished with this change and I wonder if this is not also something we could change in a second step but have no smart idea right now. 

As for Kibana, I am unfamiliar with the implementation details and how much of a relief this change would actually be, so I summon @rashidkpc to join the discussion.
</comment><comment author="javanna" created="2014-11-19T18:32:41Z" id="63689046">Interesting discussion, I am +1 on making the structure generic if possible. I think it would help a lot those users who write client code that interacts with elasticsearch, by making their life easier and their code better. There is a little price in terms of readability for the human eye, if that's a big concern we could have some kind of `human` flag as Clint mentioned...not sure how much that would end up complicating the codebase on our end though. Would be interesting to hear opinions from actual users here!
</comment><comment author="rashidkpc" created="2014-11-19T19:50:24Z" id="63701698">From a Kibana perspective @javanna hits the nail on the head. Dealing with aggregations that don't have a buckets array is a real pain. While yes, we have different code for different aggs it would be really nice to be able to treat at least all bucket aggs the same
</comment><comment author="uboness" created="2014-11-19T21:28:16Z" id="63716903">well.. for me, having an output for a simple filter agg like the following qualifies as wrong &amp; misleading:

``` json
"aggregations": {
    "last_year": {
        "doc_count": 130,
        "buckets": [
            {
                "avg_price" : { "value" : 56.3 }
            }
        ]
    }
}
```
- wrong - because filter aggs should not have buckets (it's meaningless)
- misleading - because the response indicates that filter agg can have more than one
  bucket. 

Further more, a common functionality in multi bucket aggs is that every bucket has an identifying key... what is the key in the example above? One can also request the buckets to be returned as an object (instead of an array) where each bucket is keyed by its key... and now what? how would the response look like? will we have a fake key for single bucket aggs? a fixed fake key? or require the user to provide a key? and if we do... what will the user call it.. I can't think of any key that would make sense, cause we're trying to name something that has no name.

The way I look at JSON responses is that their structure should be as self documenting as possible. We don't have schemas (and we don't want schemas) so the output needs to be semantically correct - that what makes a good API IMO. Now, perhaps you can write tools around this API that make your life easier if you really need to treat all agg responses the same (as I mentioned above, you can always have this code on the client side). I don't consider this to be human vs. machine - returning semantically correct structures is not for "humans" only.. at least IMO.
</comment><comment author="jpountz" created="2014-11-20T14:26:51Z" id="63815182">I am not a fan of returning a `buckets` element for all aggregations in the `json` either. However, I don't know if this would help, but I would be ok with having a `getBuckets` helper method on the Java API for all bucket aggregations that would return a list with a single element (the agg itself) for single-bucket aggregations.
</comment><comment author="polyfractal" created="2014-11-24T18:04:51Z" id="64235986">+1 making the result consistent, or at the very least providing a flag that enables the behavior

&gt; Single bucket ages both semantically and logically hold no buckets
&gt; wrong - because filter aggs should not have buckets (it's meaningless)

Conceptually, I think it is the exact opposite.  When I explain buckets to people, I tell them a bucket is simply a criteria that documents can match.  If the document matches the criteria, it is added to the bucket.  Some buckets dynamically add criteria as they encounter new values (`terms`, `histogram`), while other's are hard-coded upfront (`range`)

A filter bucket has a single criteria: does this doc match the filter?  If yes, add to the bucket.  It's a boolean operation, so it only has one bucket, but conceptually it behaves identically to any other bucket.  Ditto for Global bucket (criteria is "all").

For most practical applications, you don't really know what the results coming back are.  The only way to definitively know is by keeping the original request and serializing each level based on the request. Alternatively, you get to play the introspection game at every level. 

To make the point:  if we followed that logic to it's extreme, the `range` bucket should change it's output when only one range is requested, since we are expecting the user to know that only one range is coming back.

### Real life example

Here is a very real, simple example that you might find on an ecommerce site.  This isn't a "generic" tool like Kibana, but would definitely benefit from generic responses.  When you hit the main page, you get a tree of all colors and all brands:

``` json
{
   "aggs":{
      "colors":{
         "terms":{
            "field":"color",
            "size":10
         },
         "aggs":{
            "brand":{
               "terms":{
                  "field":"brand",
                  "size":10
               },
               "aggs":{
                  "avgPrice":{
                     "avg":{  "field":"price" }
                  }
               }
            }
         }
      }
   }
}
```

Now imagine a user selects `"Red"` and `"Toyota"`.  The aggregation changes to:

``` json
{
"aggs":{
   "colors":{
      "filter":{
         "term":{
            "color":"Red"
         }
      },
      "aggs":{
         "brand":{
            "filter":{
               "term":{
                  "brand":"Toyota"
               }
            },
            "aggs":{
               "avgPrice":{
                  "avg":{ "field":"price" }
               }
            }
         }
      }
   }
}
```

The `terms` buckets are replaced with a `filter` bucket for those two selected terms.  Presently, to parse both of these responses, the application needs to introspect each level to determine if it switches from `terms` to `filter`:

``` php
$first = $response['aggs']['colors'];

// Using `terms` for `color`
if ($first['buckets'] !== null) {
  foreach ($first['buckets'] as $color) {
    $second = $color;

    //  using `terms` for `brand`
    if ($second['buckets'] !== null) {
      foreach ($second['buckets'] as $brand) {
        $avgPrice = $brand['avgPrice']['value'];
        // ... do business logic here ...
      }
    } else {   //  using `filter` for `brand`
      $avgPrice = $second['avgPrice']['value'];
      // ... do business logic here ...
    }
  }
} else {  // using `filter` for `color`
  $second = $first['brand'];              // NOTE: we have to hardcode the agg name here!

  //  using `terms` for `brand`
  if ($second['buckets'] !== null) {
    foreach ($second['buckets'] as $brand) {
      $avgPrice = $brand['avgPrice']['value'];
      // ... do business logic here ...
    }
  } else {   //  using `filter` for `brand`
      $avgPrice = $second['avgPrice']['value'];
      // ... do business logic here ...
    }
} 
```

If everything came back as a bucket, you can just iterate over everything:

``` php
foreach ($response['aggs']['colors']['buckets'] as $color) {
  foreach ($color['brand']['buckets'] as $brand) {
    $avgPrice = $brand['avgPrice']['value'];   
    // ... do business logic here ...
  }
}
```
</comment><comment author="uboness" created="2014-11-26T02:23:03Z" id="64505832">@polyfractal I'm totally with you on how you explain aggs (I invented it :D)

to your example, the way I look at it, this agg is wrongly constructed:

``` json
{
"aggs":{
   "colors":{
      "filter":{
         "term":{
            "color":"Red"
         }
      },
      "aggs":{
         "brand":{
            "filter":{
               "term":{
                  "brand":"Toyota"
               }
            },
            "aggs":{
               "avgPrice":{
                  "avg":{ "field":"price" }
               }
            }
         }
      }
   }
}
```

it should be:

``` json
{
"aggs":{
   "red":{
      "filter":{
         "term":{
            "color":"Red"
         }
      },
      "aggs":{
         "toyota":{
            "filter":{
               "term":{
                  "brand":"Toyota"
               }
            },
            "aggs":{
               "avgPrice":{
                  "avg":{ "field":"price" }
               }
            }
         }
      }
   }
}
```

because with the second agg, you're effectively asking for the avg price of a red toyota, and when the response comes back you do:

``` php
$response['aggs'][$selected_color][$selected_brand]['avgPrice']['value']
```

perhaps it's just a purist way of looking at things, but it is semantically correct. That said, I understand the practical aspect of having it and as I mentioned above, like @jpountz I'm fine if the APIs provide helper methods there (so @polyfractal you can add it to your php client)... but in the JSON, it just feels wrong to me.
</comment><comment author="javanna" created="2017-05-05T19:08:10Z" id="299550622">Looking at the discussion that happened on this ticket, I would say that there was no consensus on making this change hence we may want to close it. Or should we discuss it again on the next FixItFriday @clintongormley ?</comment><comment author="clintongormley" created="2017-05-08T13:35:10Z" id="299868376">@javanna Should probably be a discussion for the aggs team - part of the decision about whether we need to refactor the aggs framework and how we'd do it.  Marking has high hanging fruit</comment><comment author="javanna" created="2017-06-16T16:49:46Z" id="309076985">cc @colings86 </comment><comment author="colings86" created="2017-06-26T15:36:04Z" id="311096091">We spoke about this in the aggs team meeting and although there was almost a consensus feeling that this would be good to fix, there currently isn't a way to change the response format of an aggregation without making a hard braking change with no period of deprecation which makes this change very tricky and we didn't feel the current arguments for this change warrant such a harsh break int eh product on a major version or not. If/When we implement https://github.com/elastic/elasticsearch/issues/11184 we should revisit this since it would provide a path for this change to be made but until then this is effectively stalled</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] MinimumMasterNodesTests.multipleNodesShutdownNonMasterNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8509</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_1x_small/967/

``` java
java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_0.cfs=1, _1.cfs=1}
    at __randomizedtesting.SeedInfo.seed([400BD79F07D4ECE4:C685D546BE64EF40]:0)
    at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:701)
    at org.elasticsearch.test.store.MockDirectoryHelper$ElasticsearchMockDirectoryWrapper.close(MockDirectoryHelper.java:140)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllFilesClosed(ElasticsearchAssertions.java:675)
    at org.elasticsearch.test.TestCluster.assertAfterTest(TestCluster.java:85)
    at org.elasticsearch.test.InternalTestCluster.assertAfterTest(InternalTestCluster.java:1672)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:617)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1788)
    at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:885)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.RuntimeException: unclosed IndexInput: _0.cfs
    at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:585)
    at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:629)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:566)
    at org.apache.lucene.store.CompoundFileDirectory.&lt;init&gt;(CompoundFileDirectory.java:104)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:108)
    at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:108)
    at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:145)
    at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:239)
    at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:104)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:422)
    at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:112)
    at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
    at org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1535)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:309)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:733)
    at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:271)
    at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:255)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:274)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    ... 1 more
```
</description><key id="49119157">8509</key><summary>[CI Failure] MinimumMasterNodesTests.multipleNodesShutdownNonMasterNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stormpython</reporter><labels><label>jenkins</label></labels><created>2014-11-17T17:21:50Z</created><updated>2014-11-21T23:06:25Z</updated><resolved>2014-11-21T23:06:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2014-11-17T23:01:32Z" id="63393197">This failure seems related: http://build-us-1.elasticsearch.org/job/es_core_master_debian/2402/

```
org.elasticsearch.ElasticsearchIllegalStateException: directory close threw IOException
  at __randomizedtesting.SeedInfo.seed([AE945E90952FC469:281A5C492C9FC7CD]:0)
  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllFilesClosed(ElasticsearchAssertions.java:677)
  at org.elasticsearch.test.TestCluster.assertAfterTest(TestCluster.java:85)
  at org.elasticsearch.test.InternalTestCluster.assertAfterTest(InternalTestCluster.java:1711)
  at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:621)
  at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1812)
  at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:885)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
  at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.NoSuchFileException: /home/jenkins/workspace/es_core_master_debian/target/J4/data/TEST-Debian-77-wheezy-64-minimal-CHILD_VM=[4]-CLUSTER_SEED=[8071130832789151926]-HASH=[1890EA09AEFF2]/nodes/1/indices/test/0/index
  at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
  at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
  at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
  at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:426)
  at java.nio.file.Files.newDirectoryStream(Files.java:545)
  at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:202)
  at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:222)
  at org.apache.lucene.store.FileSwitchDirectory.listAll(FileSwitchDirectory.java:89)
  at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
  at org.elasticsearch.index.store.DistributorDirectory.assertConsistency(DistributorDirectory.java:195)
  at org.elasticsearch.index.store.DistributorDirectory.close(DistributorDirectory.java:131)
  at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:877)
  at org.elasticsearch.test.store.MockDirectoryHelper$ElasticsearchMockDirectoryWrapper.close(MockDirectoryHelper.java:140)
  at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllFilesClosed(ElasticsearchAssertions.java:675)
```
</comment><comment author="tlrx" created="2014-11-18T09:27:27Z" id="63442845">Same error as the one pointed by @gmarz: 
http://build-us-1.elasticsearch.org/job/es_core_master_window-2008/603/
 in org.elasticsearch.indexing.IndexActionTests.testAutoGenerateIdNoDuplicates

```
org.elasticsearch.ElasticsearchIllegalStateException: directory close threw IOException
    at __randomizedtesting.SeedInfo.seed([1F10B764CF6D05FA:2C48C335F1CBF30B]:0)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllFilesClosed(ElasticsearchAssertions.java:677)
    at org.elasticsearch.test.TestCluster.assertAfterTest(TestCluster.java:85)
    at org.elasticsearch.test.InternalTestCluster.assertAfterTest(InternalTestCluster.java:1711)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:621)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1812)
    at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:885)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.NoSuchFileException: Y:\jenkins\workspace\es_core_master_window-2008\target\J0\data\d0\GLOBAL-WIN-DFOOL81193R-CHILD_VM=[0]-CLUSTER_SEED=[2238490658407777786]-HASH=[7785E10F5AF79C]\nodes\0\indices\test\1\index
    at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)
    at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
    at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
    at sun.nio.fs.WindowsDirectoryStream.&lt;init&gt;(WindowsDirectoryStream.java:86)
    at sun.nio.fs.WindowsFileSystemProvider.newDirectoryStream(WindowsFileSystemProvider.java:510)
    at java.nio.file.Files.newDirectoryStream(Files.java:589)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:202)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:222)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.index.store.DistributorDirectory.assertConsistency(DistributorDirectory.java:195)
    at org.elasticsearch.index.store.DistributorDirectory.close(DistributorDirectory.java:131)
    at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:877)
    at org.elasticsearch.test.store.MockDirectoryHelper$ElasticsearchMockDirectoryWrapper.close(MockDirectoryHelper.java:140)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAllFilesClosed(ElasticsearchAssertions.java:675)
    ... 36 more
```
</comment><comment author="s1monw" created="2014-11-21T23:06:25Z" id="64052237">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: give information about accuracy in the response from cardinality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8508</link><project id="" key="" /><description>This is a feature request that has come up a couple of times on the [mailing list](https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/zx9UFt2JPNY/6RWURKQMhR8J) and from the Kibana team: it would be useful to get a sense of how accurate responses from the cardinality aggregations are, similarly to what we are doing with the terms aggregation.
</description><key id="49115921">8508</key><summary>Aggregations: give information about accuracy in the response from cardinality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-17T16:55:41Z</created><updated>2015-11-21T21:20:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-02-18T14:46:18Z" id="74875002">Thinking out loud:
- We can provide theoretical estimates, since those are well defined
- If all the HLL sketches are still in linear counting mode, and the union of all sketches doesn't upgrade to HLL mode, the error can safely be reported as 0%
- ~~When collecting values, we can increment a counter whenever the HLL sketch changes due to an update.  This would provide a mostly accurate per-shard cardinality*.  The counts could then be summated to provide a worst-case upper error bound, which assumes all shards have unique sets of values.~~

Edit: last bullet is nonsense, that would count the number of times registers changed (due to longer runs of zeros), which i imagine would vastly under-estimate the true cardinality
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exception from geohash_grid aggregation with array of points (ES 1.4.0)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8507</link><project id="" key="" /><description>Using a geohash_grid aggregation used to work on arrays of points, but with ES 1.4.0 an exception occurs instead. The following curl commands reproduce the issue on a clean installation of ES:

```
# create index with geo_point mapping
curl -XPUT localhost:9200/test -d '{
  "mappings": {
    "test": {
      "properties": {
        "points": {
          "type": "geo_point",
          "geohash_prefix": true
        }
      }
    }
  }
}'

# insert documents
curl -XPUT localhost:9200/test/test/1?refresh=true -d '{ "points": [[1,2], [2,3]] }'
curl -XPUT localhost:9200/test/test/2?refresh=true -d '{ "points": [[2,3], [3,4]] }'

# perform aggregation
curl -XGET localhost:9200/test/test/_search?pretty -d '{
  "size": 0,
  "aggs": {
    "a1": {
      "geohash_grid": {
        "field": "points",
        "precision": 3
      }
    }
  }
}'
```

On Elasticsearch 1.3.5 this produces the expected result:

```
...
  "aggregations" : {
    "a1" : {
      "buckets" : [ {
        "key" : "s09",
        "doc_count" : 2
      }, {
        "key" : "s0d",
        "doc_count" : 1
      }, {
        "key" : "s02",
        "doc_count" : 1
      } ]
    }
  }
...
```

However on Elasticsearch 1.4.0 this triggers a failure:

```
{
  "took" : 76,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 3,
    "failed" : 2,
    "failures" : [ {
      "index" : "test",
      "shard" : 2,
      "status" : 500,
      "reason" : "QueryPhaseExecutionException[[test][2]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[1]; "
    }, {
      "index" : "test",
      "shard" : 3,
      "status" : 500,
      "reason" : "QueryPhaseExecutionException[[test][3]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[1]; "
    } ]
  },
  "hits" : {
    "total" : 0,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "a1" : {
      "buckets" : [ ]
    }
  }
}
```

The log contains this exception:

```
[2014-11-17 17:28:25,121][DEBUG][action.search.type       ] [Franklin Storm] [test][3], node[-S9ijRKKQH-IEAr3sUW14Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1fa40d49]
org.elasticsearch.search.query.QueryPhaseExecutionException: [test][3]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
    at org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser$GeoGridFactory$CellValues.setDocument(GeoHashGridParser.java:154)
    at org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridAggregator.collect(GeoHashGridAggregator.java:73)
    at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)
    at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:117)
    ... 7 more
```
</description><key id="49113008">8507</key><summary>Exception from geohash_grid aggregation with array of points (ES 1.4.0)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mvdz</reporter><labels><label>bug</label></labels><created>2014-11-17T16:34:21Z</created><updated>2014-11-20T09:36:44Z</updated><resolved>2014-11-20T09:36:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-17T16:58:26Z" id="63336500">We did quite a significant refactoring of fielddata in 1.4 in order to better integrate with Lucene, I'm wondering that this could be related. I'm looking into it...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/SortingNumericDocValues.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSource.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/values/ScriptLongValues.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file></files><comments><comment>Aggregations: Fix geohash grid aggregation on multi-valued fields.</comment></comments></commit></commits></item><item><title>Missing parent routing causes NullPointerException in Bulk API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8506</link><project id="" key="" /><description>Now each error is reported in bulk response rather than causing entire bulk to fail.
Added a Junit test but the use of TransportClient means the error is manifested differently to a REST based request - instead of a NullPointer the whole of the bulk request failed with a RoutingMissingException. Changed TransportBulkAction to catch this exception and treat it the same as the existing logic for a ElasticsearchParseException - the individual bulk request items are flagged and reported individually rather than failing the whole bulk request.

Closes #8365
</description><key id="49111232">8506</key><summary>Missing parent routing causes NullPointerException in Bulk API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-17T16:22:02Z</created><updated>2015-06-08T00:14:46Z</updated><resolved>2014-11-17T18:20:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-17T16:23:17Z" id="63330292">LGTM
</comment><comment author="markharwood" created="2014-11-17T16:33:48Z" id="63332177">Implements the alternative approach to fixing the NullPointerException outlined here: https://github.com/elasticsearch/elasticsearch/pull/8378#issuecomment-63131673
</comment><comment author="markharwood" created="2014-11-17T18:20:25Z" id="63349492">Pushed commits https://github.com/elasticsearch/elasticsearch/commit/d1c6d3b7b090479a5b58c3cc7bdccbaf8381b019 and https://github.com/elasticsearch/elasticsearch/commit/6f79d67f810f406d456ef9dab8aa3b8a44b97419 to master, 1.x and 1.4 
</comment><comment author="markharwood" created="2014-11-21T11:53:12Z" id="63960393">Pushed to 1.3 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] SimpleClusterStateTests.testLargeClusterStatePublishing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8505</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_master_metal/5320/

```
java.lang.AssertionError: CreateIndexResponse failed - not acked
Expected: &lt;true&gt;
     but: was &lt;false&gt;
    at __randomizedtesting.SeedInfo.seed([98495D58CB7FF3B3:83DF14E12EEE2C90]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:105)
    at org.elasticsearch.cluster.SimpleClusterStateTests.testLargeClusterStatePublishing(SimpleClusterStateTests.java:148)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:483)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:827)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:877)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="49110098">8505</key><summary>[CI Failure] SimpleClusterStateTests.testLargeClusterStatePublishing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>jenkins</label></labels><created>2014-11-17T16:14:16Z</created><updated>2014-11-21T23:06:36Z</updated><resolved>2014-11-21T23:06:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:06:36Z" id="64052258">this was likely caused by the shard locking issues that are now resovled
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Wildcard search on string </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8504</link><project id="" key="" /><description>I am having a problem with results not being as expected.

I basicly log a bunch of information about a specific file. When I try to filter my results by using 'filename:"*.css"' to get only css files, everything works fine. Using *.js or *.php however doesn't return any results, eventhough files with that ending exist!
Using just \* or the specific complete filename works. I have no idea what the cause could be.

Log-files don't show anything.

Mapping of the index in question:

```
{
    "stats-2014.11.16": {
        "mappings": {
            "resource": {
                "properties": {
                    "@timestamp": {
                        "type": "date",
                        "format": "dateOptionalTime"
                    },
                    "domain": {
                        "type": "string"
                    },
                    "dur": {
                        "type": "long"
                    },
                    "exec": {
                        "type": "long"
                    },
                    "filename": {
                        "type": "string"
                    },
                    "node": {
                        "type": "string"
                    },
                    "recv": {
                        "type": "long"
                    },
                    "size": {
                        "type": "long"
                    },
                    "start": {
                        "type": "long"
                    },
                    "status": {
                        "type": "long"
                    },
                    "url": {
                        "type": "string"
                    }
                }
            }
        }
    }
}
```
</description><key id="49105812">8504</key><summary>Wildcard search on string </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2014-11-17T15:44:54Z</created><updated>2014-11-17T15:59:58Z</updated><resolved>2014-11-17T15:59:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-17T15:59:58Z" id="63326274">Hi @MakaHost 

Please ask questions like these on the mailing list: http://www.elasticsearch.org/community. The GitHub issues list is for bug reports and feature requests.

Also, you should probably read the chapter on Partial Matching from the book: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/partial-matching.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix typo in the javadoc for #tieBreaker()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8503</link><project id="" key="" /><description /><key id="49092681">8503</key><summary>Fix typo in the javadoc for #tieBreaker()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels /><created>2014-11-17T14:02:13Z</created><updated>2014-11-18T10:41:20Z</updated><resolved>2014-11-17T15:58:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-17T15:57:44Z" id="63325892">Thanks @ankon - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryBuilder.java</file></files><comments><comment>Docs: Fix typo in the javadoc for #tieBreaker()</comment></comments></commit></commits></item><item><title>[Bugreport] wrong work of minimum_should_match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8502</link><project id="" key="" /><description>## How to reproduce bug:
### Analyzer settings:

``` javascript
PUT test
{
    "settings" : {
        "analysis" : {
            "analyzer" : {
                "myGramAn" : {
                    "type" : "custom",
                    "tokenizer" : "standard",
                    "filter" : [
                        "myGram"
                    ]
                }
            },
            "filter" : {
                "myGram" : {
                    "type" : "nGram",
                    "min_gram" : 3,
                    "max_gram" : 3
                }
            }
        }
    }
}
```
### Mapping:

``` javascript
PUT test/myType/_mapping
{
    "properties" : {
        "myField" : {
            "type" : "string",
            "analyzer" : "myGramAn"
        }
    }
}
```
### Documents:

``` javascript
PUT test/myType/1
{
    "myField" : "12345"
}

PUT test/myType/2
{
    "myField" : "12345678"
}

PUT test/myType/3
{
    "myField" : "123 678"
}
```

Now I want to find documents, which contain those four grams: "123", "234", "567", and "678". All of them occur only in second document. So, if I will use terms query, it would return right answer:

``` javascript
GET test/myType/_search
{
    "query" : {
        "terms" : {
            "myField" : [ "123", "234", "567", "678" ],
            "minimum_should_match" : "4"
        }
    }
}
//-------------------
{
    "hits" : [
        {
            "_index" : "test",
            "_type" : "myType",
            "_id" : "2",
            "_score" : 0.61370564,
            "_source" : {
                "myField" : "12345678"
            }
        }
    ]
}
```

But if I change it to match query, result will be wrong:

``` javascript
GET test/myType/_search
{
    "query" : {
        "match" : {
            "myField" : {
                "query" : "1234 5678",
                "minimum_should_match" : "4"
            }
        }
    }
}
//-------------------
{
    "hits" : [
        {
            "_index" : "test",
            "_type" : "myType",
            "_id" : "2",
            "_score" : 0.61370564,
            "_source" : {
                "myField" : "12345678"
            }
        }, 
        {
            "_index" : "test",
            "_type" : "myType",
            "_id" : "3",
            "_score" : 0.07956372,
            "_source" : {
                "myField" : "123 678"
            }
        }
    ]
}
```

But they should be the same:

``` javascript
GET test/_analyze?analyzer=myGramAn&amp;text=1234 5678
//-------------------
{
    "tokens" : [{
            "token" : "123",
            "start_offset" : 0,
            "end_offset" : 4,
            "type" : "word",
            "position" : 1
        }, {
            "token" : "234",
            "start_offset" : 0,
            "end_offset" : 4,
            "type" : "word",
            "position" : 1
        }, {
            "token" : "567",
            "start_offset" : 5,
            "end_offset" : 9,
            "type" : "word",
            "position" : 2
        }, {
            "token" : "678",
            "start_offset" : 5,
            "end_offset" : 9,
            "type" : "word",
            "position" : 2
        }
    ]
}
```

Result will be identical, even if I’ll change minimum_should_match to "100%" or "2", "3", "-1", "-2".

The root of this mistake lies at `org.apache.lucene.util.QueryBuilder:287` (lucene 4.10.1). Method createFieldQuery divides terms by position increment into two should queries and wraps it with another should with ours minimum_should_match: `(myField:123 myField:234)(myField:567 myField:678)`. That's why results are identical with other values of minimum_should_match.
All other kinds of *match\* queries are affected too.
</description><key id="49087463">8502</key><summary>[Bugreport] wrong work of minimum_should_match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kamapcuc</reporter><labels><label>discuss</label></labels><created>2014-11-17T13:13:33Z</created><updated>2015-11-21T21:19:33Z</updated><resolved>2015-11-21T21:19:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-17T13:45:02Z" id="63305987">Hi @Kamapcuc 

The problem with changing the current logic is that eg synonyms will no longer work as expected.  For instance, let's say that you have "jumps" as a synonym for "leaps", using query time synonym expansion.

A query for "the quick fox jumps" would become "the quick fox (jumps|leaps)" - unless you have also used index time expansion, this query will never match the indexed docs.  Instead we only require one token in each position.  

Token filters are not allowed to change positions or offsets, so the ngram token filter will always produce stacked tokens.

However, you can use the `ngram` tokenizer to do exactly what you need.  In your example, change the analyzer to the following:

```
PUT test
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "myGramTok": {
          "type": "ngram",
          "min_gram": 3,
          "max_gram": 3,
          "token_chars": [
            "letter","digit"
          ]
        }
      },
      "analyzer": {
        "myGramAn": {
          "tokenizer": "myGramTok"
        }
      }
    }
  }
}
```

With this in place, your `match` query works as expected.
</comment><comment author="Kamapcuc" created="2014-11-17T14:35:02Z" id="63312652">Hi, Clinton!

Of course I understand, that QueryBuilder:287 are made for synonyms (we use them, and they are awesome). But the reason doesn't change that those situation with `minimum_should_match` is a bug.
It works not like intended without any excuse. [Here](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-minimum-should-match.html) there is not any word about that it doesn't work with grams. (and it takes me several hours to find why our big, complicated, based on `multi_match` query sometimes returns very strange results)

Your solution is not appropriable, because we use snowball token filter. Also, I can't totally remove n-gram because when I search "conductor" I want to find "semiconductor" too.

Maybe there is some option to remove TF/IDF and replace it with constant? For example each term adds 1 to score, so I'll be able to make cutoff with `min_score` instead?

Or maybe you will change work of analyzer and position becomes array? For example:

``` javascript
GET test/_analyze?analyzer=myGramAn&amp;text=1234 5678
//-------------------
{
    "tokens" : [{
            "token" : "123",
            "start_offset" : 0,
            "end_offset" : 4,
            "type" : "word",
            "position" : [1, 1]
        }, {
            "token" : "234",
            "start_offset" : 0,
            "end_offset" : 4,
            "type" : "word",
            "position" : [1, 2]
        }, {
            "token" : "567",
            "start_offset" : 5,
            "end_offset" : 9,
            "type" : "word",
            "position" : [2, 1]
        }, {
            "token" : "678",
            "start_offset" : 5,
            "end_offset" : 9,
            "type" : "word",
            "position" : [2, 2]
        }
    ]
}
```

Or something like that...
</comment><comment author="Kamapcuc" created="2014-12-24T13:17:43Z" id="68051696">Is there any progress in that problem?
</comment><comment author="clintongormley" created="2014-12-24T13:23:08Z" id="68051949">No. I spoke to @mikemccand about this and there is no way to fix this other than what I have already suggested.  You say you can't use the ngram tokenizer because you're using the snowball filter.  Honestly it doesn't make sense to combine those two - the ngrams tokenizer makes the snowball filter redundant.

Regardless, any major change to analysis like this would have to go into Lucene first.
</comment><comment author="Kamapcuc" created="2014-12-24T13:31:33Z" id="68052369">We use synonyms only with expand token filter in index analyzer and don't use them in search analyzer. So, at that time i just brute force it: https://github.com/Kamapcuc/lucene-solr/commit/33cb9491788980d47f88bb8df1bf5aed179cbecf
But i think other people can confront with those proberm too.
</comment><comment author="clintongormley" created="2015-11-21T21:19:33Z" id="158682958">Nothing to do here
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Installing ES 1.4.0 on Windows fails if there are space in the path to ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8501</link><project id="" key="" /><description>I have extracted ES 1.4.0 to the following folder:
C:\Program Files\ElasticSearch\elasticsearch-1.4.0
Then, from cmd I do the following:
- cd "C:\Program Files\ElasticSearch\elasticsearch-1.4.0\bin"
- service.bat install elasticsearch_140

This results in an error:
Command "C:\Program" not found. However, the installation continues. It even reports that installation was successful, but ES will fail to start.

I have already found the reasons for this inside the service.bat script:
- CALL %ES_HOME%\bin\elasticsearch.in.bat

It should be:
- CALL "%ES_HOME%\bin\elasticsearch.in.bat"

Cheers, Curly060 =;-&gt;
</description><key id="49078103">8501</key><summary>Installing ES 1.4.0 on Windows fails if there are space in the path to ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Curly060</reporter><labels /><created>2014-11-17T11:36:19Z</created><updated>2014-11-17T11:53:59Z</updated><resolved>2014-11-17T11:53:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-17T11:53:59Z" id="63294477">Fixed by #8428
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] UpdateTests.testContextVariables failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8500</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_master_metal/5306/

```
org.elasticsearch.ElasticsearchIllegalArgumentException: failed to execute script
    at __randomizedtesting.SeedInfo.seed([C76216C8FF38338C:62E6C0522E52A40E]:0)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:201)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:176)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:170)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction$1.run(TransportInstanceSingleOperationAction.java:187)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: AssertionError[ttl is not within acceptable range. Expression: ((111211211 - ctx._ttl) &lt;= (now - 1416211700028)). Values: now = 1416211700146]
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:197)
```
</description><key id="49071857">8500</key><summary>[CI Failure] UpdateTests.testContextVariables failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>jenkins</label></labels><created>2014-11-17T10:32:05Z</created><updated>2014-11-18T16:01:59Z</updated><resolved>2014-11-18T16:01:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-11-18T00:59:57Z" id="63405669">http://build-us-1.elasticsearch.org/job/es_core_master_metal/5335/testReport/junit/org.elasticsearch.update/UpdateTests/testContextVariables/
</comment><comment author="mikemccand" created="2014-11-18T01:03:04Z" id="63405975">http://build-us-1.elasticsearch.org/job/es_core_master_metal/5333/testReport/junit/org.elasticsearch.update/UpdateTests/testContextVariables/
</comment><comment author="tlrx" created="2014-11-18T11:26:05Z" id="63456318">http://build-us-1.elasticsearch.org/job/es_core_1x_metal/4768/testReport/junit/org.elasticsearch.update/UpdateTests/testContextVariables/
</comment><comment author="dakrone" created="2014-11-18T11:36:52Z" id="63457482">I merged the PR in that added this, so I'll take a look when I get a chance.
</comment><comment author="dakrone" created="2014-11-18T15:34:28Z" id="63488825">I ran the test with seed individually for each of these builds, then ran the full suite of tests with each seed without encountering any failures, was anyone able to reproduce these failures?
</comment><comment author="dakrone" created="2014-11-18T16:01:59Z" id="63493831">Pushed a fix for this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>[TEST] Give tests for ctx._ttl more leeway</comment></comments></commit></commits></item><item><title>[CI Failure] BasicBackwardsCompatibilityTest.testIndexUpgradeSingleNode failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8499</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_bwc_1x/4932/

```
java.lang.AssertionError: Count is 69 but 138 was expected.  Total shards: 2 Successful shards: 2 &amp; 0 shard failures:
    at __randomizedtesting.SeedInfo.seed([514E8FB3D0AF27A6:A545CB315D6FFB24]:0)
    at org.junit.Assert.fail(Assert.java:93)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:184)
    at org.elasticsearch.bwcompat.BasicBackwardsCompatibilityTest.testIndexUpgradeSingleNode(BasicBackwardsCompatibilityTest.java:307)
```
</description><key id="49071472">8499</key><summary>[CI Failure] BasicBackwardsCompatibilityTest.testIndexUpgradeSingleNode failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>jenkins</label></labels><created>2014-11-17T10:28:11Z</created><updated>2014-11-21T23:10:49Z</updated><resolved>2014-11-21T23:10:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-21T23:10:49Z" id="64052710">this also seems related to the shard locking issues everything stable again - closing 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nodes fail to transfer shards to other ones</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8498</link><project id="" key="" /><description>ES Version: 0.20.6
Java version: 

```
java -version
java version "1.7.0_65"
Java(TM) SE Runtime Environment (build 1.7.0_65-b17)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)
```

The exception:

```
[2014-11-17 10:08:40,932][WARN ][action.index             ] [Ringo Kid] Failed to perform index on replica [ticket_comments][2]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.StreamCorruptedException: unexpected end of block data
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1369)
    at java.io.ObjectInputStream.access$300(ObjectInputStream.java:205)
    at java.io.ObjectInputStream$GetFieldImpl.readFields(ObjectInputStream.java:2132)
    at java.io.ObjectInputStream.readFields(ObjectInputStream.java:537)
    at java.net.InetSocketAddress.readObject(InetSocketAddress.java:282)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1872)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1894)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1894)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:369)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:169)
    ... 23 more
[2014-11-17 10:08:40,933][WARN ][cluster.action.shard     ] [Ringo Kid] sending failed shard for [ticket_comments][2], node[jacKtNEwSE6djr0Zsgv2jg], [R], s[INITIALIZING], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2014-11-17 10:08:40,935][WARN ][cluster.action.shard     ] [Ringo Kid] sending failed shard for [ticket_comments][2], node[jacKtNEwSE6djr0Zsgv2jg], [R], s[INITIALIZING], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2014-11-17 10:08:40,935][WARN ][transport.netty          ] [Ringo Kid] Message not fully read (response) for [8459] handler org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4@6e2ac102, error [true], resetting
[2014-11-17 10:08:40,935][WARN ][transport.netty          ] [Ringo Kid] Message not fully read (response) for [8458] handler org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4@38e5a2cf, error [true], resetting
```
</description><key id="49070944">8498</key><summary>Nodes fail to transfer shards to other ones</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajardan</reporter><labels /><created>2014-11-17T10:22:42Z</created><updated>2014-11-17T11:44:52Z</updated><resolved>2014-11-17T11:44:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajardan" created="2014-11-17T10:23:12Z" id="63284624">I checked Java and ES version on all nodes, it is the same.
</comment><comment author="clintongormley" created="2014-11-17T11:44:52Z" id="63293526">Hi @ajardan 

Different versions of Java would have been my first guess.  But either way, you're using a VERY old and unsupported version of elasticsearch.  You really need to upgrade :)

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[CI Failure] IndicesRequestTests.testIndicesStats failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8497</link><project id="" key="" /><description>http://build-us-1.elasticsearch.org/job/es_core_14_window-2012/304/

```
java.lang.AssertionError: org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction$IndexShardStatsRequest
Expected: ["test1", "test0", "test0"]
     got: []

    at __randomizedtesting.SeedInfo.seed([C97289D5AE27CFBB:D82FFC40EC13878D]:0)
    at org.junit.Assert.assertThat(Assert.java:780)
    at org.elasticsearch.action.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:816)
    at org.elasticsearch.action.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:801)
    at org.elasticsearch.action.IndicesRequestTests.testIndicesStats(IndicesRequestTests.java:485)
```
</description><key id="49070166">8497</key><summary>[CI Failure] IndicesRequestTests.testIndicesStats failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>jenkins</label></labels><created>2014-11-17T10:15:31Z</created><updated>2015-09-30T10:38:52Z</updated><resolved>2015-09-30T10:38:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-30T10:38:52Z" id="144350690">I can't repro this anymore, I think we can close for now and reopen if it fails again, given that it didn't happen for a long while it probably got fixed meanwhile.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index API: BWC layer for GetIndex should not block in a listener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8496</link><project id="" key="" /><description>Today we execute BWC calls against nodes that have not GetIndex API
in a action listeners #onFailure method. These calls are blocking today
and might be executed on a bounded thread-pool which might deadlock the
call depending on how many threads are in the pool and how the pool is
setup. These calls should run async as well.
</description><key id="49059884">8496</key><summary>Index API: BWC layer for GetIndex should not block in a listener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label></labels><created>2014-11-17T08:47:54Z</created><updated>2015-03-19T16:40:45Z</updated><resolved>2014-11-17T10:44:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-17T09:31:48Z" id="63278991">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Wireshark protocol dissection support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8495</link><project id="" key="" /><description>Hi guys, I've finished my Wireshark dissector for Elasticsearch. It is now merged into their master. See https://code.wireshark.org/review/#/c/4948/
</description><key id="49019019">8495</key><summary>Add Wireshark protocol dissection support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ryandoyle</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-16T22:46:16Z</created><updated>2014-11-17T12:07:38Z</updated><resolved>2014-11-17T12:07:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-17T11:41:16Z" id="63293133">Nice work @ryandoyle 

Please could I ask you to sign the CLA so that I can merge your PR in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="ryandoyle" created="2014-11-17T11:53:07Z" id="63294375">Hi @clintongormley, CLA has been signed. 
</comment><comment author="clintongormley" created="2014-11-17T11:56:42Z" id="63294742">HI @ryandoyle. Did you sign the corporate CLA?  We can only check the personal CLA's automatically, and only if your email is the same as the one on github. The corp version has to be checked manually every time (for future PRs too).
</comment><comment author="ryandoyle" created="2014-11-17T12:01:05Z" id="63295160">Hi @clintongormley, I signed the personal CLA with ryan@doylenet.net to only just realise that my email address was wrong in my github account so I've changed it to what it should be (ryan@doylenet.net) on github. I've probably now just made this process harder than it was supposed to be!
</comment><comment author="clintongormley" created="2014-11-17T12:07:26Z" id="63295793">Success! Merged, thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add Wireshark protocol dissection support</comment></comments></commit></commits></item><item><title>Ban all usage of Future#cancel(true)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8494</link><project id="" key="" /><description>Interrupting a thread while blocking on an NIO Read / Write Operation
can cause a file to be closed due to the interrupts. This can have unpredictable
effects when files are open by index readers etc. we should prevent interruptions
across the board if possible.
</description><key id="49012580">8494</key><summary>Ban all usage of Future#cancel(true)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-16T19:33:30Z</created><updated>2015-06-06T19:20:57Z</updated><resolved>2014-11-18T13:23:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-17T09:01:17Z" id="63275992">LGTM, left two minor comments
</comment><comment author="s1monw" created="2014-11-17T11:17:01Z" id="63290572">@dakrone thanks for the review I push changes
</comment><comment author="dakrone" created="2014-11-17T12:48:33Z" id="63299817">LGTM!
</comment><comment author="s1monw" created="2014-11-18T12:46:20Z" id="63465640">@kimchy any comments on that
</comment><comment author="kimchy" created="2014-11-18T12:49:35Z" id="63466031">LGTM
</comment><comment author="dakrone" created="2014-11-19T17:09:31Z" id="63675568">@s1monw I think this didn't make it into 1.x? (I can't find it and `FutureUtils` doesn't exist on 1.x), can you backport it?
</comment><comment author="s1monw" created="2014-11-19T20:06:50Z" id="63704448">@dakrone yeah I wanted it to bake in a bit - will backport
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataDeleteIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/metrics/MeterMetric.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/FutureUtils.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/translog/TranslogService.java</file><file>src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java</file><file>src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/transport/TransportService.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file></files><comments><comment>[CORE] Ban all useage of Future#cancel(true)</comment></comments></commit></commits></item><item><title>Lack of systemd init files in deb package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8493</link><project id="" key="" /><description>Hello,

Is it possible to add files which are needed to start the service with systemd in elasticsearch deb package?
This will allow to start "normally" elasticsearch under latest Debian-based Linux distributions which switched recently from system-V to systemd init system. As I understand such files (mostly elasticsearch.service) are included already in RPM packages and they have to be just slightly modified.

Thanks,
  Alexandr 
</description><key id="49011727">8493</key><summary>Lack of systemd init files in deb package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">abravorus</reporter><labels /><created>2014-11-16T19:12:08Z</created><updated>2014-12-09T13:30:42Z</updated><resolved>2014-12-05T11:29:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abravorus" created="2014-11-17T12:54:26Z" id="63300426">Hello,

I prepared a set of files (based on files in RPM package) needed to start elasticsearch with systemd under Debian jessie (testing) and was able to start it normally.

1) /etc/systemd/system/elasticsearch.service

```
[Unit]
Description=Starts and stops a single elasticsearch instance on this system
Documentation=http://www.elasticsearch.org

[Service]
Type=forking
EnvironmentFile=/etc/default/elasticsearch
User=elasticsearch
Group=elasticsearch
PIDFile=/var/run/elasticsearch/elasticsearch.pid
ExecStart=/usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR
# See MAX_OPEN_FILES in /etc/default/elasticsearch
LimitNOFILE=65535
# See MAX_LOCKED_MEMORY in /etc/default/elasticsearch, use "infinity" when     MAX_LOCKED_MEMORY=unlimited and using bootstrap.mlockall: true
#LimitMEMLOCK=infinity
# Shutdown delay in seconds, before process is tried to be killed with KILL (if configured)
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

2)  /usr/lib/sysctl.d/elasticsearch.conf

```
vm.max_map_count=262144
```

3) /usr/lib/tmpfiles.d/elasticsearch.conf

```
d    /run/elasticsearch   0755 elasticsearch elasticsearch - -
```

You should also check the content of /etc/default/elasticsearch for default values for your configuration, for example to change ES_HEAP_SIZE value.

You also need to make all systemd exercises ones (as root or via sudo):

systemd-tmpfiles --create elasticsearch.conf (to created a folder with proper permissions for PID file)
systemctl daemon-reload
systemctl enable elasticsearch.service

After that elasticsearch should start automatically after reboot or you can start it manually via:
systemctl start elasticsearch.service
or
/etc/init.d/elasticsearch start (which actually run the previous command)

Of course, I'm not sure that all above is errors free but it works for me.

Alexandr
</comment><comment author="t-lo" created="2014-12-03T10:21:45Z" id="65385408">Debian stable ("Wheezy") still uses sysv-init; "Jessie" is still in "testing" (i.e. not released), and thusly will not receive timely security updates until its release in 2015 (see https://www.debian.org/releases/jessie/).

So, does elasticsearch support `.deb` packages for "testing" (i.e. unreleased) versions?
</comment><comment author="abravorus" created="2014-12-03T11:22:54Z" id="65393507">Hi,

I can just say that in my experience official elasticsearch .deb package
can be installed without any problems on Debian testing ("Jessie") and
could be run on it smootly. The only question is that "Jessie" switched
already to systemd (instead of sysv-init) and I had to prepare
elasticsearch systemd config files by myself.

I have quite a simple elasticsearch cluster with two non-heavily-loaded
nodes both running under Debian "Jessie". Upgrade from ES 1.4.0 to 1.4.1
was done successfully by standard apt-get update/apt-get upgrade procedure
(having ES systemd files in place already, of course).

Security updates for "Jessie" are done via the following repository record
in /etc/apt/sources.list:
deb http://security.debian.org/ jessie/updates main non-free contrib

There are only 4 weeks till 2015, time goes fast :)

Alexandr

On Wed, Dec 3, 2014 at 1:22 PM, Thilo Fromm notifications@github.com
wrote:

&gt; Debian stable ("Wheezy") still uses sysv-init; "Jessie" is still in
&gt; "testing" (i.e. not released), and thusly will not receive timely security
&gt; updates until its release in 2015 (see
&gt; https://www.debian.org/releases/jessie/).
&gt; 
&gt; So, does elasticsearch support .deb packages for "testing" (i.e.
&gt; unreleased) versions?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8493#issuecomment-65385408
&gt; .
</comment><comment author="t-lo" created="2014-12-03T11:37:05Z" id="65394793">@abravorus Is there a generic way to build a `.deb` package which provides both a `.service` file for systemd and a sysv-script, and only installs the correct ones (i.e. init script on Deb &lt; 8, and service file on Deb &gt;= 8)? Or do you think it would be best to provide a `.deb` w/ init script for Debian &lt; 8, and a separate `.deb` for Debian &gt;= 8?

Concerning sec updates, quoting from https://www.debian.org/releases/jessie/:

&gt; Please note that security updates for testing distribution are not yet managed by the security team.
&gt; Hence, testing does not get security updates in a timely manner. You are encouraged to switch
&gt; your sources.list entries from testing to wheezy for the time being if you need security support. See
&gt; also the entry in the Security Team's FAQ for the testing distribution.
</comment><comment author="abravorus" created="2014-12-03T12:00:40Z" id="65397153">Hi Thilo,

I do not know is it possible to build such a single .deb package. May be
possible if it will look on installed packages in pre-install phase, but I
never did it by myself. I think that more simple and robust way is to build
two separate packages for Debian &lt; 8 and for Debian &gt;= 8.

Keeping in mind that some other Linux distros are switched already to
systemd and ES has already systemd support for RPM there is a sense to be
prepared for that for Debian too.

You are right about security updates for Debian testing. I've just noted
that they are available in principle.

Alexandr

PS: Looks like GitHub web-site is blocked in Russia, I can answer via
e-mail only.

On Wed, Dec 3, 2014 at 2:37 PM, Thilo Fromm notifications@github.com
wrote:

&gt; @abravorus https://github.com/abravorus Is there a generic way to build
&gt; a .deb package which provides both a .service file for systemd and a
&gt; sysv-script, and only installs the correct ones (i.e. init script on Deb &lt;
&gt; 8, and service file on Deb &gt;= 8)? Or do you think it would be best to
&gt; provide a .deb w/ init script for Debian &lt; 8, and a separate .deb for
&gt; Debian &gt;= 8?
&gt; 
&gt; Concerning sec updates, quoting from
&gt; https://www.debian.org/releases/jessie/:
&gt; 
&gt; Please note that security updates for testing distribution are not yet
&gt; managed by the security team.
&gt; Hence, testing does not get security updates in a timely manner. You are
&gt; encouraged to switch
&gt; your sources.list entries from testing to wheezy for the time being if you
&gt; need security support. See
&gt; also the entry in the Security Team's FAQ for the testing distribution.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8493#issuecomment-65394793
&gt; .
</comment><comment author="t-lo" created="2014-12-03T12:03:49Z" id="65397456">Ping @clintongormley does ES want two separate `.deb`s, one for systemd, one for sysv-init?
</comment><comment author="t-lo" created="2014-12-03T13:09:24Z" id="65404070">@abravorus Currently there are some improvements to the ES `.service` file in the works. Most notably the ES start script doesn't daemonize (so backgrounding is left to systemd), and we use `After=network-online.target` to wait for networking before starting ES. I checked with Fedora 21 and it behaves as expected. 
Could you please test the new service file w/ Debian 8?

```
[Unit]
Description=Starts and stops a single elasticsearch instance on this system
Documentation=http://www.elasticsearch.org
Wants=network-online.target
After=network-online.target

[Service]
EnvironmentFile=/etc/sysconfig/elasticsearch
User=elasticsearch
Group=elasticsearch
ExecStart=/usr/share/elasticsearch/bin/elasticsearch            \
                            -Des.default.config=$CONF_FILE      \
                            -Des.default.path.home=$ES_HOME     \
                            -Des.default.path.logs=$LOG_DIR     \
                            -Des.default.path.data=$DATA_DIR    \
                            -Des.default.path.work=$WORK_DIR    \
                            -Des.default.path.conf=$CONF_DIR
# See MAX_OPEN_FILES in sysconfig
LimitNOFILE=65535
# See MAX_LOCKED_MEMORY in sysconfig, use "infinity" when MAX_LOCKED_MEMORY=unlimited and using bootstrap.mlockall: true
#LimitMEMLOCK=infinity
# Shutdown delay in seconds, before process is tried to be killed with KILL (if configured)
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

Regards,
Thilo
</comment><comment author="clintongormley" created="2014-12-03T13:13:46Z" id="65404549">@t-lo We'd definitely want to provide just a single .deb package, so supporting both in one would be awesome.
</comment><comment author="abravorus" created="2014-12-03T13:17:55Z" id="65405026">Hi Thilo,

I will test new .service file tomorrow and let you know.

One note: There is no /etc/sysconfig folder in Debian (both &lt;8 and &gt;=8), I
use /etc/default/elasticsearch instead. Or this folder should be created by
.deb , but I do not see that now.

Alexandr

On Wed, Dec 3, 2014 at 4:10 PM, Thilo Fromm notifications@github.com
wrote:

&gt; @abravorus https://github.com/abravorus Currently there are some
&gt; improvements to the ES .service file in the works. Most notably the ES
&gt; start script doesn't daemonize (so backgrounding is left to systemd), and
&gt; we use After=network-online.target to wait for networking before starting
&gt; ES. I checked with Fedora 21 and it behaves as expected.
&gt; Could you please test the new service file w/ Debian 8?
&gt; 
&gt; [Unit]
&gt; Description=Starts and stops a single elasticsearch instance on this system
&gt; Documentation=http://www.elasticsearch.org
&gt; Wants=network-online.target
&gt; After=network-online.target
&gt; 
&gt; [Service]
&gt; EnvironmentFile=/etc/sysconfig/elasticsearch
&gt; User=elasticsearch
&gt; Group=elasticsearch
&gt; ExecStart=/usr/share/elasticsearch/bin/elasticsearch            \
&gt;                             -Des.default.config=$CONF_FILE      \
&gt;                             -Des.default.path.home=$ES_HOME     \
&gt;                             -Des.default.path.logs=$LOG_DIR     \
&gt;                             -Des.default.path.data=$DATA_DIR    \
&gt;                             -Des.default.path.work=$WORK_DIR    \
&gt;                             -Des.default.path.conf=$CONF_DIR
&gt; 
&gt; # See MAX_OPEN_FILES in sysconfig
&gt; 
&gt; LimitNOFILE=65535
&gt; 
&gt; # See MAX_LOCKED_MEMORY in sysconfig, use "infinity" when MAX_LOCKED_MEMORY=unlimited and using bootstrap.mlockall: true
&gt; 
&gt; #LimitMEMLOCK=infinity
&gt; 
&gt; # Shutdown delay in seconds, before process is tried to be killed with KILL (if configured)
&gt; 
&gt; TimeoutStopSec=20
&gt; 
&gt; [Install]
&gt; WantedBy=multi-user.target
&gt; 
&gt; Regards,
&gt; Thilo
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8493#issuecomment-65404070
&gt; .
</comment><comment author="electrical" created="2014-12-03T13:23:54Z" id="65405711">@abravorus the /etc/default/elasticsearch file should be created by the package. weird that it doesn't in your case.
</comment><comment author="t-lo" created="2014-12-03T13:34:16Z" id="65406854">@electrical I think @abravorus pointed out a problematic line in my (RPM specific) `.service` file: `EnvironmentFile=/etc/sysconfig/elasticsearch`. And I think he's right - for Debianesque systems this needs to be `EnvironmentFile=/etc/default/elasticsearch`.
</comment><comment author="abravorus" created="2014-12-03T13:35:19Z" id="65406987">Richard, it's created, no problem. But proposed .service file uses
/etc/sysconfig/elasticsearch
file which doesn't exists in .deb . And I use, in my handmade .service
file /etc/default/elasticsearch
instead.

Alexandr

On Wed, Dec 3, 2014 at 4:24 PM, Richard Pijnenburg &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; @abravorus https://github.com/abravorus the /etc/default/elasticsearch
&gt; file should be created by the package. weird that it doesn't in your case.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8493#issuecomment-65405711
&gt; .
</comment><comment author="electrical" created="2014-12-03T13:35:36Z" id="65407020">Ahh i see. sorry. yeah, we will need to create a separate service file indeed.
</comment><comment author="abravorus" created="2014-12-03T13:36:28Z" id="65407121">Thilo, exactly. Probably this is due to the difference in RPM and .deb
folder structure.

Alexandr

On Wed, Dec 3, 2014 at 4:34 PM, Thilo Fromm notifications@github.com
wrote:

&gt; @electrical https://github.com/electrical I think @abravorus
&gt; https://github.com/abravorus pointed out a problematic line in the (RPM
&gt; specific) .service file: EnvironmentFile=/etc/sysconfig/elasticsearch.
&gt; And I think he's right - for Debianesque systems this needs to be
&gt; EnvironmentFile=/etc/default/elasticsearch.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8493#issuecomment-65406854
&gt; .
</comment><comment author="t-lo" created="2014-12-03T13:37:10Z" id="65407216">@abravorus thanks for pointing this out; I missed that :)
</comment><comment author="abravorus" created="2014-12-04T06:07:02Z" id="65542154">Hi Thilo,

I checked your .service file on one of my ES nodes under Debian 8. It
works, service starts/stops as expected.
But I didn't check it with full machine reboot because I can't do it now
(and probably two weeks more). If this is not urgently I will let you know
about full reboot test as soon as I'll be able to do it.

Alexandr

On Wed, Dec 3, 2014 at 4:10 PM, Thilo Fromm notifications@github.com
wrote:

&gt; @abravorus https://github.com/abravorus Currently there are some
&gt; improvements to the ES .service file in the works. Most notably the ES
&gt; start script doesn't daemonize (so backgrounding is left to systemd), and
&gt; we use After=network-online.target to wait for networking before starting
&gt; ES. I checked with Fedora 21 and it behaves as expected.
&gt; Could you please test the new service file w/ Debian 8?
&gt; 
&gt; [Unit]
&gt; Description=Starts and stops a single elasticsearch instance on this system
&gt; Documentation=http://www.elasticsearch.org
&gt; Wants=network-online.target
&gt; After=network-online.target
&gt; 
&gt; [Service]
&gt; EnvironmentFile=/etc/sysconfig/elasticsearch
&gt; User=elasticsearch
&gt; Group=elasticsearch
&gt; ExecStart=/usr/share/elasticsearch/bin/elasticsearch            \
&gt;                             -Des.default.config=$CONF_FILE      \
&gt;                             -Des.default.path.home=$ES_HOME     \
&gt;                             -Des.default.path.logs=$LOG_DIR     \
&gt;                             -Des.default.path.data=$DATA_DIR    \
&gt;                             -Des.default.path.work=$WORK_DIR    \
&gt;                             -Des.default.path.conf=$CONF_DIR
&gt; 
&gt; # See MAX_OPEN_FILES in sysconfig
&gt; 
&gt; LimitNOFILE=65535
&gt; 
&gt; # See MAX_LOCKED_MEMORY in sysconfig, use "infinity" when MAX_LOCKED_MEMORY=unlimited and using bootstrap.mlockall: true
&gt; 
&gt; #LimitMEMLOCK=infinity
&gt; 
&gt; # Shutdown delay in seconds, before process is tried to be killed with KILL (if configured)
&gt; 
&gt; TimeoutStopSec=20
&gt; 
&gt; [Install]
&gt; WantedBy=multi-user.target
&gt; 
&gt; Regards,
&gt; Thilo
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8493#issuecomment-65404070
&gt; .
</comment><comment author="t-lo" created="2014-12-04T08:45:58Z" id="65553259">@abravorus Thanks for testing!
</comment><comment author="jpountz" created="2014-12-05T11:29:20Z" id="65778213">Fixed via #8765
</comment><comment author="abravorus" created="2014-12-05T11:38:09Z" id="65779033">Great, thanks!

Alexandr

On Fri, Dec 5, 2014 at 2:30 PM, Adrien Grand notifications@github.com
wrote:

&gt; Closed #8493 https://github.com/elasticsearch/elasticsearch/issues/8493.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8493#event-203209705
&gt; .
</comment><comment author="abravorus" created="2014-12-09T13:26:47Z" id="66281841">Hi Thilo, 

Just to let you know.
.service file works as expected during full machine reboot too. Thanks to our electricians for the occasional 4-hours blackout :)

Alexandr
</comment><comment author="t-lo" created="2014-12-09T13:30:42Z" id="66282310">@abravorus Heh :D Thanks again for testing!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: Skip bwc analysis tests if using turkish or azuri locale.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8492</link><project id="" key="" /><description>On 1.x, the pre built pattern analyzer has issues with turkish and azuri
locales.  We should just skip the test if using these locales.  On
master, the issue is fixed by only using the Elasticsearch version,
which avoids the default locale issue lucene's PatternAnalyzer is
susceptable to.
</description><key id="49006686">8492</key><summary>Tests: Skip bwc analysis tests if using turkish or azuri locale.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-16T18:00:01Z</created><updated>2015-01-21T23:22:21Z</updated><resolved>2014-11-20T06:14:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-19T21:28:44Z" id="63716969">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reword note regarding _source for accuracy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8491</link><project id="" key="" /><description>Previously it suggested _source was always present, when that is not the case.
</description><key id="48955371">8491</key><summary>Reword note regarding _source for accuracy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dw</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-16T06:55:00Z</created><updated>2014-11-24T11:20:16Z</updated><resolved>2014-11-24T11:20:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dw" created="2014-11-19T00:20:51Z" id="63571205">CLA signed
</comment><comment author="clintongormley" created="2014-11-24T11:20:13Z" id="64179993">thanks @dw - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Reword note regarding _source for accuracy</comment></comments></commit></commits></item><item><title>Date Range Filter unclear rounding behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8490</link><project id="" key="" /><description>If I run the query :

```
{
  "size": 14,
  "_source": "context.data.eventTime",
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "context.data.eventTime": {
                  "gte": "2014-11-12T14:54:59.000Z",
                  "lte": "2014-11-12T14:55:00.000Z"
                }
              }
            }
          ]
        }
      }
    }
  }
}
```

I get :

```
{
    "took": 0,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "failed": 0
    },
    "hits": {
        "total": 3,
        "max_score": 1,
        "hits": [
            {
                "_index": "test",
                "_type": "request",
                "_id": "4a5acb28-e6e8-4dfc-b86f-80926ed82fd7",
                "_score": 1,
                "_source": {
                    "context": {
                        "data": {
                            "eventTime": "2014-11-12T14:55:00.1458607Z"
                        }
                    }
                }
            },
            {
                "_index": "test",
                "_type": "request",
                "_id": "22a6d539-06a7-4dd9-860c-2ea5ed0956cf",
                "_score": 1,
                "_source": {
                    "context": {
                        "data": {
                            "eventTime": "2014-11-12T14:55:00.5976447Z"
                        }
                    }
                }
            },
            {
                "_index": "test",
                "_type": "request",
                "_id": "554b25f3-36a9-4f84-b2a3-69f257cf1ac0",
                "_score": 1,
                "_source": {
                    "context": {
                        "data": {
                            "eventTime": "2014-11-12T14:55:00.9979586Z"
                        }
                    }
                }
            }
        ]
    }
}
```

eventTime has mappings :

```
"eventTime": {
                "type": "date",
                "format": "dateOptionalTime"
              }
```

Shouldn't no records match ?

Interestingly if I run the same query with the range filter using 'lt' instead of a 'lte' I get no records matched.
</description><key id="48843314">8490</key><summary>Date Range Filter unclear rounding behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">aaneja</reporter><labels><label>:Dates</label><label>bug</label></labels><created>2014-11-14T21:44:18Z</created><updated>2015-06-07T18:06:47Z</updated><resolved>2014-12-11T01:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-17T11:30:16Z" id="63291998">Hi @aaneja 

Closing in favour of #8424
</comment><comment author="aaneja" created="2014-11-17T17:50:14Z" id="63344776">I read the related issue and the documentation for date rounding (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html#date-math)

It appears that date rounding is to the nearest second - if this is so, and I want millisecond precision is my only option passing range values as UNIX millisecond timestamps ?
(If I use : 

```
{
  "range": {
    "context.data.eventTime": {
      "gte": "1415804099000",
      "lte": "1415804100000"
    }
  }
}
```

I get right results - no records match)
</comment><comment author="clintongormley" created="2014-11-17T18:00:33Z" id="63346471">Rounding only happens if you specify it, eg "some_date/d"

What's happening is that the `date_format` that you're using (`dateOptionalTime`) doesn't include milliseconds. So milliseconds are being ignored both in the values that you index AND in the range clause.  Try using `basic_date_time`.  See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html#built-in
</comment><comment author="aaneja" created="2014-11-17T18:32:21Z" id="63351396">I think `dateOptionalTime` does include milliseconds on indexing. As you see from above one of the records has timestamp : "2014-11-12T14:55:00.1458607Z" which in UNIX milliseconds is 1415804100145.

I ran a query with 

```
{
  "range": {
    "context.data.eventTime": {
      "gte": "1415804100145",
      "lte": "1415804100145"
    }
  }
}
```

and this record matched. If I try the same range filter with 1415804100146 - 1415804100149 no records match 
</comment><comment author="clintongormley" created="2014-11-17T18:34:42Z" id="63351754">Hmm you may be right - I'll have to take another look at this one tomorrow
</comment><comment author="aaneja" created="2014-11-19T17:50:45Z" id="63682373">Ping. Any updates on what the expected behavior of `dateOptionalTime` should be ?
</comment><comment author="clintongormley" created="2014-11-24T11:51:26Z" id="64183925">Hi @aaneja 

So you're correct: `dateOptionalTime` does include milliseconds.  But I can't replicate your findings.  Your query correctly returns no results when I run it.  I tried on 1.2.1, 1.3.4, and 1.4.0.

Do you perhaps have another field with the same name but a different mapping?  Can you provide a working recreation of this issue? What version of Elasticsearch are you running?
</comment><comment author="aaneja" created="2014-12-05T00:50:34Z" id="65731094">Here's a repro -
`GET /`
Response  -

```
{
   "status": 200,
   "name": "Terror",
   "version": {
      "number": "1.3.2",
      "build_hash": "dee175dbe2f254f3f26992f5d7591939aaefd12f",
      "build_timestamp": "2014-08-13T14:29:30Z",
      "build_snapshot": false,
      "lucene_version": "4.9"
   },
   "tagline": "You Know, for Search"
}
```

Then -

```
DELETE /megacorp/tweet

PUT /megacorp/_mapping/tweet
{
    "tweet" : {
        "properties": {
              "eventTime": {
                "type": "date",
                "format": "dateOptionalTime"
              }
            }
    }
}

PUT /megacorp/tweet/1?pretty
{
  "eventTime": 1415832900146
}

PUT /megacorp/tweet/2?pretty
{
  "eventTime": 1415832900597
}


POST /megacorp/tweet/_search
{
  "size": 14,
  "_source": "eventTime",
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "eventTime": {
                  "gte": "2014-11-12T22:54:00Z",
                  "lte": "2014-11-12T22:55:00Z"
                }
              }
            }
          ]
        }
      }
    }
  }
}


POST /megacorp/tweet/_search
{
  "size": 14,
  "_source": "eventTime",
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "eventTime": {
                  "gte": 1415832840000,
                  "lte": 1415832900000
                }
              }
            }
          ]
        }
      }
    }
  }
}
```

The last two queries should be, IMO, equivalent.
However I get incorrect results in the first one; the second one correctly matches no records
</comment><comment author="rjernst" created="2014-12-05T22:08:58Z" id="65862589">I've tracked this down to a bug in the date parser and am working on a fix.
</comment><comment author="rjernst" created="2014-12-08T23:24:26Z" id="66207623">I have a fix in #8834.  Note that this is really a first step towards eliminating the two different parsing functions in the date math parser (see #8598).
</comment><comment author="rjernst" created="2014-12-09T22:16:14Z" id="66368652">After discussing the further, I think the best immediate fix here is to set `mapping.date.round_ceil = false`.  This should fix the problem described above (essentially doing via configuration what my patch in #8834 does).  In order to not introduce breaking behavior, I am going to drop #8834 entirely and work on the full removal of `mapping.date.round_ceil` for #8598.
</comment><comment author="rjernst" created="2014-12-11T01:18:01Z" id="66554752">I'm closing this in favor of #8598 which will be fixed for 2.0.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>1.4 disk allocation decider logging and error handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8489</link><project id="" key="" /><description>  Moved to 1.4, all looked well, but things started failing in our docker CI environment during a call to bootstrap the cluster to create some search templates. 

Basically, would just see the following error flow:

```
application_1   | [info] application - Creating search template: testTemplate, file: test.json, type: search
elasticsearch_1 | [2014-11-14 02:53:55,763][INFO ][cluster.metadata         ] [Visimajoris] [.scripts] creating index, cause [auto(index api)], shards [1]/[0], mappings []
application_1   | [.scripts][0] Primary shard is not active or isn't assigned is a known node. Timeout: [1m], request: index {[.scripts][mustache][testTemplate], source[{
```

This was a huge mystery until the logging on the search cluster was bumped up to DEBUG:

```
elasticsearch_1 | [2014-11-14 19:27:33,523][INFO ][cluster.metadata         ] [XXX] [.scripts] creating index, cause [auto(index api)], shards [1]/[0], mappings []
elasticsearch_1 | [2014-11-14 19:27:33,539][DEBUG][cluster.routing.allocation.decider] [XXX] Node [IdXCumBrSYOTpgesR2QQ3Q] has 9.009641449850488% free disk
elasticsearch_1 | [2014-11-14 19:27:33,539][DEBUG][cluster.routing.allocation.decider] [XXX] Less than the required 10.0 free bytes threshold (9.009641449850488 bytes free) on node IdXCumBrSYOTpgesR2QQ3Q, preventing allocation even though primary has never been allocated
elasticsearch_1 | [2014-11-14 19:27:33,542][DEBUG][cluster.routing.allocation.decider] [XXX] Node [IdXCumBrSYOTpgesR2QQ3Q] has 9.009641449850488% free disk
elasticsearch_1 | [2014-11-14 19:27:33,542][DEBUG][cluster.routing.allocation.decider] [XXX] Less than the required 10.0 free bytes threshold (9.009641449850488 bytes free) on node IdXCumBrSYOTpgesR2QQ3Q, preventing allocation even though primary has never been allocated
elasticsearch_1 | [2014-11-14 19:27:33,543][DEBUG][indices                  ] [XXX] [.scripts] closing ... (reason [failed to create index])
```

I think there are a few issues going on here:
- The current log message is confusing since one line says 9.009641449850488% free disk and next line says 10.0 free bytes threshold (9.009641449850488 bytes free)
- This message is only logged at DEBUG level. If you can't create an index because of low disk, that is an error. 
- It would be nice if the error returned to the client indicated the reason the index was unassigned. 
</description><key id="48834219">8489</key><summary>1.4 disk allocation decider logging and error handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2014-11-14T20:09:48Z</created><updated>2014-11-15T01:26:58Z</updated><resolved>2014-11-15T01:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-14T20:51:47Z" id="63127090">Hi @ppearcy, we've addressed this in #8270 and #8382, which should be part of 1.4.1
</comment><comment author="ppearcy" created="2014-11-15T01:26:58Z" id="63153895">Ah, great thanks! Will close out and open a new one if there are still issues. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Do not force the post-filter to be loaded into a BitSet.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8488</link><project id="" key="" /><description>If the filter does not support random-access, we should use the iterator instead of loading it into a BitSet.
</description><key id="48819099">8488</key><summary>Do not force the post-filter to be loaded into a BitSet.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-14T17:55:34Z</created><updated>2015-06-07T11:56:28Z</updated><resolved>2014-11-17T14:49:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-14T21:04:33Z" id="63128759">LGTM (nice that we now can easily switch modes during execution, thanks to the per segment collection api)
</comment><comment author="jpountz" created="2014-11-17T14:00:18Z" id="63307867">@rjernst I pushed a new commit to avoid the null check
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/FilteredCollector.java</file></files><comments><comment>Search: Do not force the post-filter to be loaded into a BitSet.</comment></comments></commit></commits></item><item><title>ElasticSearch 1.3.4 recovery slow on larger clusters (50+ total nodes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8487</link><project id="" key="" /><description>We are seeing a situation on clusters running 1.3.4 with greater than 50 total nodes where shard recovery/allocation is either failing or is VERY slow.

Full details:
I currently have 6 clusters, 4 with 24 total nodes and 2 larger clusters with 53 &amp; 63 nodes respectively.  Everything is run on VMs running Windows Server 2012 R2 within Azure.

All clusters (except the 63 node) were upgraded from 1.3.2 to 1.3.4 using an offline method of shutting down the cluster clean, swapping out the x64 Windows Service, then restarting all of the nodes.  The 63 node cluster was built last week clean with 1.3.4.

For the 24 node clusters, the clusters returned to a green status after the upgrade in less than 30 minutes and are performing very well.  I also did a rolling reboot of all of the machines to apply OS updates where I have automation that sets allocation to new_primaries, shuts down the service, reboots the machine, waits for the node to rejoin the cluster, sets allocation back to all, then waits for the cluster to return to green before proceeding to the next node.  Again for each of the 24 node clusters, it completed the entire process in just shy of 2 hours.

The 53 node cluster is our oldest cluster (with 144 indices &amp; 3330 shards) and has under gone several upgrades using the offline method and in each scenario, the cluster returned to yellow within 15 minutes and green within 40 minutes.  Monday night we upgraded this cluster and it took 3 hours to get to yellow and 6 hours to get to green.

The 63 node cluster is our newest cluster (currently has 12 indices &amp; 272 shards).  Last night &amp; today while performing a rolling reboot, each node has a max of 3 shards on it, after the machine rebooted, it was taking &gt; 10 minutes for the node to rejoin the cluster and randomly some shards would never finish initialization.

When I query _cat/recovery?pretty=true&amp;v&amp;active_only=true, no shards are listed however _cat/shards would show 1 or 2 shards as INITIALIZING.  If I issued the command for reroute cancel on the initializing shards, they would almost immediately allocation and the cluster would turn green.
</description><key id="48817717">8487</key><summary>ElasticSearch 1.3.4 recovery slow on larger clusters (50+ total nodes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">TrentStewart</reporter><labels /><created>2014-11-14T17:42:53Z</created><updated>2015-11-21T21:18:52Z</updated><resolved>2015-11-21T21:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-17T12:47:07Z" id="63299645">&gt; The 53 node cluster is our oldest cluster (with 144 indices &amp; 3330 shards) and has under gone several upgrades using the offline method and in each scenario, the cluster returned to yellow within 15 minutes and green within 40 minutes. Monday night we upgraded this cluster and it took 3 hours to get to yellow and 6 hours to get to green.

When you bring a node back up and it has replicas on disk, ES will sync the replicas with the current primaries. That can't take varying amount of time, depending on how different the local segments file are. We are working on making it faster.

&gt; It was taking &gt; 10 minutes for the node to rejoin the cluster

With 1.4.0 we  considerably improved the joining process by using batching- it should be much faster. See #7493 

&gt; When I query _cat/recovery?pretty=true&amp;v&amp;active_only=true, no shards are listed however _cat/shards would show 1 or 2 shards as INITIALIZING. 

This is worrying. Can you reproduce the issue? Are there any errors in the logs? I'm looking for something like ConcurrentModificationException (but it may be something else!).
</comment><comment author="portante" created="2014-12-04T22:10:22Z" id="65712958">I have a small one node ES instance (1.4.1, RHEL 7, openjdk 1.7.0), with
256GB of memory, 4 sockets, 10 cores per socket (no hyper-threads), and
with 623 indexes (no replication, one shard per index), the cluster becomes
unresponsive with Java running out of memory (spends all its time in
garbage collection) on ES startup.  We just upgraded from 1.3.2-1.

I modified my startup sequence to immediately close all indexes after
restarting elasticsearch and then open them in sets of 10, waiting for the
cluster health to go green.  That process ends up taking &gt;10 min per set
to open those indexes, but eventually dies mid-way through the indexes.

We then opened them one at a time, from the most recent to oldest, for
only about 60 indexes, to see if we can limp along until we find a fix for
this problem.  The indexes are all around 11-13 GB in size, about 150M
documents.

Does this sound familiar to anyone?

On Mon, Nov 17, 2014 at 7:47 AM, Boaz Leskes notifications@github.com
wrote:

&gt; The 53 node cluster is our oldest cluster (with 144 indices &amp; 3330 shards)
&gt; and has under gone several upgrades using the offline method and in each
&gt; scenario, the cluster returned to yellow within 15 minutes and green within
&gt; 40 minutes. Monday night we upgraded this cluster and it took 3 hours to
&gt; get to yellow and 6 hours to get to green.
&gt; 
&gt; When you bring a node back up and it has replicas on disk, ES will sync
&gt; the replicas with the current primaries. That can't take varying amount of
&gt; time, depending on how different the local segments file are. We are
&gt; working on making it faster.
&gt; 
&gt; It was taking &gt; 10 minutes for the node to rejoin the cluster
&gt; 
&gt; With 1.4.0 we considerably improved the joining process by using batching-
&gt; it should be much faster. See #7493
&gt; https://github.com/elasticsearch/elasticsearch/pull/7493
&gt; 
&gt; When I query _cat/recovery?pretty=true&amp;v&amp;active_only=true, no shards are
&gt; listed however _cat/shards would show 1 or 2 shards as INITIALIZING.
&gt; 
&gt; This is worrying. Can you reproduce the issue? Are there any errors in the
&gt; logs? I'm looking for something like ConcurrentModificationException (but
&gt; it may be something else!).
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8487#issuecomment-63299645
&gt; .
</comment><comment author="TrentStewart" created="2014-12-05T21:42:29Z" id="65859130">&gt; &gt; The 53 node cluster is our oldest cluster (with 144 indices &amp; 3330 shards) and has under gone several upgrades using the offline method and in each scenario, the cluster returned to yellow within 15 minutes and green within 40 minutes. Monday night we upgraded this cluster and it took 3 hours to get to yellow and 6 hours to get to green.
&gt; 
&gt; When you bring a node back up and it has replicas on disk, ES will sync the replicas with the current primaries. That can't take varying amount of time, depending on how different the local segments file are. We are working on making it faster.

True, there appears to be a greater percentage of the recovery time is spent in the translog stage.

&gt; &gt; It was taking &gt; 10 minutes for the node to rejoin the cluster
&gt; 
&gt; With 1.4.0 we considerably improved the joining process by using batching- it should be much faster. See #7493 
&gt; 
&gt; &gt; When I query _cat/recovery?pretty=true&amp;v&amp;active_only=true, no shards are listed however _cat/shards would show 1 or 2 shards as INITIALIZING. 
&gt; 
&gt; This is worrying. Can you reproduce the issue? Are there any errors in the logs? I'm looking for something like ConcurrentModificationException (but it may be something else!).

Yes, it's not 100% reproducible, but it does happen frequently.  There are no exceptions in the logs that I have discovered. 

Since I posted this, I done several rolling reboots and node failure recoveries of our clusters.  With 1.3.4 vs 1.3.2, a full recovery has gone from ~40 minutes to over 4 hours.  Prior to 1.3.4 I never paid any attention to pending_tasks, it's part of my problem triage process now, so I'm not able to do an apples to apples comparison. 

When _cat/recovery does not show anything, pending_tasks will grow to over 2000 tasks while its stuck on a single task, usually its shard-started that wasn't able to allocate to a node.  Some times I can cancel the allocation, most of the time the cancel would timeout as well and the cluster is left in a limbo waiting and waiting.
</comment><comment author="portante" created="2014-12-06T03:54:06Z" id="65884332">https://github.com/elasticsearch/elasticsearch/issues/8394#issuecomment-65871792 addressed the problem I saw above.
</comment><comment author="bleskes" created="2014-12-17T10:32:06Z" id="67304131">@TrentStewart sorry for not returning to you earlier

&gt; When _cat/recovery does not show anything, pending_tasks will grow to over 2000 tasks while its stuck on a single task, usually its shard-started that wasn't able to allocate to a node. 

When this happen again (master stuck on a single task), can you run the hot threads api on it? that will tell us what it's busy doing.

&gt; Some times I can cancel the allocation, most of the time the cancel would timeout as well and the cluster is left in a limbo waiting and waiting.

Cancel is in itself a task, albeit with highest priority. It needs to wait until the current task is done before having effect.
</comment><comment author="julien51" created="2015-02-23T16:13:09Z" id="75573480">We are seeing a similar issue where recovery seems to be stuck in the "translog" state.
When checking `/&lt;idx&gt;/_recovery?pretty=true&amp;active_only=true` we do find that translog recovery is in progress:

```
{
      "id" : 2,
      "type" : "GATEWAY",
      "stage" : "TRANSLOG",
      "primary" : true,
      "start_time_in_millis" : 1424704351606,
      "stop_time_in_millis" : 0,
      "total_time_in_millis" : 3657431,
      "source" : {
        "id" : "lv6TBuiTSoS0nz1qQ9a1Bg",
        "host" : "elastic5.&lt;host&gt;.com.",
        "transport_address" : "inet[/192.168.162.33:9300]",
        "ip" : "104.237.137.221",
        "name" : "elastic5.&lt;host&gt;.com"
      },
      "target" : {
        "id" : "lv6TBuiTSoS0nz1qQ9a1Bg",
        "host" : "elastic5.&lt;host&gt;.com.",
        "transport_address" : "inet[/192.168.162.33:9300]",
        "ip" : "104.237.137.221",
        "name" : "elastic5.&lt;host&gt;.com"
      },
      "index" : {
        "files" : {
          "total" : 647,
          "reused" : 647,
          "recovered" : 647,
          "percent" : "100.0%"
        },
        "bytes" : {
          "total" : 8834720289,
          "reused" : 8834720289,
          "recovered" : 8834720289,
          "percent" : "100.0%"
        },
        "total_time_in_millis" : 48
      },
      "translog" : {
        "recovered" : 36274,
        "total_time_in_millis" : 0
      },
      "start" : {
        "check_index_time_in_millis" : 0,
        "total_time_in_millis" : 1021
      }
    }
```

But it is extremely slow (the 0 value in translog.total_time_in_millis is a lie!)... and we have no ETA idea. What does `recovered` mean in the context of translog? Are these bytes?
</comment><comment author="bleskes" created="2015-02-23T16:15:42Z" id="75573974">@julien51 those are translog operations . To find out how many there are in total and what the size of the translog is, you can run `GET /&lt;idx&gt;/_stats?level=shards&amp;human` and check the primary shard. I'm working on a change that will supply those as part of the recovery output.  Which version are you on?
</comment><comment author="julien51" created="2015-02-23T16:19:20Z" id="75574709">Version: 1.4.2
The problem: this is a primary shard initializing (after a recovery), so it looks like the number of operations is not available :(
Also, in about 5 minutes, we progressed by ~1500 operations... so slow!
</comment><comment author="bleskes" created="2015-02-23T16:25:03Z" id="75575911">@julien51 I see, this is your primary shard? In this case it's hard to tell indeed. You can check the file system for the size of a file called "translog-????.recovering" (again, working on improving this). 

When you shut down the cluster (did you?) did you have any relocations/recoveries going on? (if you know).
</comment><comment author="julien51" created="2015-02-23T16:29:00Z" id="75576706">Unfortunaty, there was a bit of a disaster and the shutdown of the server was unexpected (still trying to understand what happend). We had 2 servers failing (out of 5) and of course they had the primary and secondary for one of our indices. But yes, based on the logs, I believe there was some relocation going on.

The file size of 1752285717 bytes. Any idea of how that roughly converts to number of operations? 
Is there one operation per line? The file has 511,014 lines... so if that's 1 line per operation, we're not even 10% of the way :( any idea how to make this _much_ faster? (because at this point it will take 16hours to recover a file of less than 2GB)

I should add that it's similarly slow for the 2ndary shards which are also initializing and in the "translog" state.
</comment><comment author="julien51" created="2015-02-23T17:07:06Z" id="75584877">Extra stupid question: this would in theory be an absolutely bad time to upgrade to 1.4.4, but I see 1.4.3 does bring improvements on the recovery front.  
What if I prevent the cluster from doing any re-alloc and then upgrade the node on which this slow initilizating is going on?
</comment><comment author="julien51" created="2015-02-23T23:25:05Z" id="75660646">After 6 hours, it was still processing the translog, but at an excruciatingly slow pace... so I restarted the node, and well, it finished in a matter of seconds on another host. We seem to have the same number of documents. 
Yet, we'll start re-indexing from our main datastore, just in case we miss anything. 
</comment><comment author="bleskes" created="2015-02-24T13:23:35Z" id="75755052">&gt; The file size of 1752285717 bytes. Any idea of how that roughly converts to number of operations? 
&gt; Is there one operation per line? The file has 511,014 lines... so if that's 1 line per operation, 

The translog is binary. New lines are just there by accident (if viewed as text)

&gt;  this would in theory be an absolutely bad time to upgrade to 1.4.4, but I see 1.4.3 does bring improvements on the recovery front.

1.4.3 helps by being more aggressive in trimming the translog post recovery but we're still chasing this issue. It is very rare but do occur (as you sadly noticed).

&gt; I restarted the node, and well, it finished in a matter of seconds on another host. 
&gt; We seem to have the same number of documents. 

Yes. The translog on replica is being flushed during recovery. It's only the primary than can grow, because we need it a safety measure to catch all the documents indexed between starting to copy lucene files and starting the translog phase. Depending on what exactly happened, there might be none but it is not guaranteed. 
</comment><comment author="clintongormley" created="2015-11-21T21:18:52Z" id="158682873">The never-ending-translog bug was fixed several versions ago.  I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Order by scripted_metric sub aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8486</link><project id="" key="" /><description>Since there is a new Scripted metric aggregation (scripted_metric) in 1.4, it is possible to do a lot of amazing stuff. 
For example it is possible to implement Weighted Average aggregation, which we were missing before.

Now we are really missing a **possibility to sort by scripted_metric results**.

**Live example**:

We calculate **weightedAvgVis** with scripted_metric and want to get ids with **TOP 5 values of weightedAvgVis**. Since script returns double, it looks logically possible. 

``` json
{  
   "from":0,
   "size":0,
   "query":{  
      "match_all":{   }
   },
   "aggregations":{  
      "idNodes":{  
         "terms":{  
            "field":"id",
            "size":5,
            "order":{  
               "weightedAvgVis":"asc"
            }
         },
         "aggregations":{  
            "weightedAvgVis":{  
               "scripted_metric":{  
                  "init_script":"_agg['weightedSum'] = 0d; _agg['countSum'] = 0L;",
                  "map_script":"_agg['weightedSum'] = _agg.weightedSum + _source['avgVis'] * _source['count']; _agg['countSum'] = _agg.countSum + _source['count'];",
                  "reduce_script":"weightedSum = 0d; countSum = 0L; for(a in _aggs) {weightedSum += a.weightedSum; countSum += a.countSum;};if(countSum == 0L) {return 0d;} else {return weightedSum / countSum}"
               }
            }
         }
      }
   }
}
```
</description><key id="48807850">8486</key><summary>Order by scripted_metric sub aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eryabitskiy</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label><label>feedback_needed</label><label>high hanging fruit</label></labels><created>2014-11-14T16:18:46Z</created><updated>2016-12-07T17:34:18Z</updated><resolved>2016-12-07T14:37:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-17T12:42:49Z" id="63299236">@eryabitskiy this is something that I have been thinking about but requires a few outstanding issues to be resolved first. Specifically these are https://github.com/elasticsearch/elasticsearch/pull/8421 and https://github.com/elasticsearch/elasticsearch/issues/8434. These would allow us to specify much more powerful order paths and the getProperty method on the scripted metric aggregation could be used to retrieve arbitrary properties of the scripts results.
</comment><comment author="snikch" created="2014-12-04T20:49:31Z" id="65701144">Hi there, I'm not sure if this is appropriate but I thought you may want to gauge interest. We're very keen to see this as well. Currently we end up doing a lot of sorting on oversized resultsets in Go, whereas being able to sort on scripted metrics would save us this hassle.
</comment><comment author="vb3" created="2015-05-08T23:18:23Z" id="100392288">+1 This would be great if solved! 
</comment><comment author="felipegs" created="2015-05-08T23:38:48Z" id="100394488">+1
</comment><comment author="concordiadiscors" created="2015-05-12T14:48:18Z" id="101306715">+1
</comment><comment author="rmruano" created="2015-05-12T17:26:46Z" id="101358012">:+1: sort on scripted metrics aggregations would be a killer feature. In the meantime we're also doing client-side sorting on oversized results.
</comment><comment author="hendrik-schumacher" created="2015-05-13T13:36:16Z" id="101665258">+1
</comment><comment author="aaneja" created="2015-05-18T19:27:07Z" id="103182369">+1
</comment><comment author="edigu" created="2015-05-20T14:03:15Z" id="103898012">+1
</comment><comment author="brianstebar2" created="2015-05-27T14:47:32Z" id="105940362">+1
</comment><comment author="tekwiz" created="2015-06-15T13:34:46Z" id="112071621">+1 Aside from the other use-cases mentioned here, this feature would give the Kibana product some major strength, particularly for rapid prototyping.
</comment><comment author="genme" created="2015-06-26T08:12:43Z" id="115568942">+1
</comment><comment author="adimasuhid" created="2015-08-10T02:09:53Z" id="129276057">+1
</comment><comment author="iantruslove" created="2015-08-26T19:49:31Z" id="135151602">(hopping on the "+1 to this feature" bandwagon)
</comment><comment author="mpereira" created="2015-08-26T19:52:20Z" id="135152189">+1
</comment><comment author="vhiroki" created="2015-09-10T20:50:20Z" id="139375083">+1
</comment><comment author="EvgenyGusarov" created="2015-09-23T12:32:16Z" id="142584902">+1
</comment><comment author="lquerel" created="2015-10-14T18:58:15Z" id="148156581">+1
</comment><comment author="bkj" created="2015-10-15T13:32:21Z" id="148386584">+1
</comment><comment author="mariomara" created="2015-10-20T11:44:53Z" id="149536126">+1
</comment><comment author="ssickles" created="2015-11-10T04:22:32Z" id="155282179">+1
</comment><comment author="antmat" created="2015-11-11T11:54:28Z" id="155753625">+1
</comment><comment author="vasiliy-kiryukhin" created="2015-12-02T22:05:45Z" id="161448453">+1
</comment><comment author="lquerel" created="2016-01-06T00:01:46Z" id="169173594">Regarding the computation of weighted average I sent a pull request to extend the current avg aggregation in order to support this kind of average natively without scripted metric.

Here the issue created on github:
https://github.com/elastic/elasticsearch/issues/15731

Here the pull request (only the first approach is implemented):
https://github.com/elastic/elasticsearch/pull/15781
</comment><comment author="lquerel" created="2016-01-06T00:05:22Z" id="169174155">Regarding the support of the order clause for scripted metric I sent a pull request to support that.

https://github.com/elastic/elasticsearch/pull/15718
</comment><comment author="orrchen" created="2016-01-18T07:40:36Z" id="172452450">+1
</comment><comment author="justinmpier" created="2016-02-01T03:42:16Z" id="177742769">+1
</comment><comment author="jayhahn" created="2016-02-01T20:00:22Z" id="178163006">+1
</comment><comment author="gchen" created="2016-02-03T12:05:59Z" id="179188528">+1
</comment><comment author="rajagopals" created="2016-02-10T06:03:17Z" id="182212776">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>unexpected results from aggregartions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8485</link><project id="" key="" /><description>Hi. I am trying to use elasticsearch to store metrics of our cluster. The collection is done using sensu and the values in elasticsearch still have the fields "key","value" and "timestamp" that you would see in graphite. When I create a average/max/min histogram over our metrics I get weird results. The average cpu utilisation of our cluster is 1.22907852E-315 according to elasticsearch. At first glance I suspected a overflow issue, but there are simply not enough results to cause that.

The json I am using is the following:

```
{
  "size": 5,
  "query": {
    "bool": {
      "must": [ { "match": { "host": "poolnode-03" } },
                { "match": { "metric": "cpu_metrics" } },
                { "range": {
                    "@timestamp": {
                       "gte" : "2014-11-12T17:50:00",
                       "lte" : "2014-11-12T17:51:00"
                    }
                  }
                }]
    }     
  },        
  "aggs": {
    "cpu_histogram": {
      "date_histogram": {
        "field": "@timestamp",
        "interval": "5000ms",
        "min_doc_count": 1
      },
      "aggs": {
        "avg_cpu": { "avg": { "field": "value" } },
        "max_cpu": { "max": { "field": "value" } },
        "min_cpu": { "min": { "field": "value" } }
      }
    }
  }
}
```

The result looks like this: 

```
{
  "took" : 13,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 408,
    "max_score" : 3.4773107,
    "hits" : [ {
      "_index" : "sensu",
      "_type" : "cpu_metrics",
      "_id" : "AUmlIK4cRrcdtiDjFlbQ",
      "_score" : 3.4773107,
      "_source":{"host":"poolnode-03","metric":"cpu_metrics","name":"poolnode-03.cpu.total.iowait","value":11637,"@timestamp":"2014-11-12T17:50:22+00:00"}
    }, {
      "_index" : "sensu",
      "_type" : "cpu_metrics",
      "_id" : "AUmlIK4cRrcdtiDjFlbV",
      "_score" : 3.4773107,
      "_source":{"host":"poolnode-03","metric":"cpu_metrics","name":"poolnode-03.cpu.cpu0.user","value":15686755,"@timestamp":"2014-11-12T17:50:22+00:00"}
    }, {
      "_index" : "sensu",
      "_type" : "cpu_metrics",
      "_id" : "AUmlIK4cRrcdtiDjFlbe",
      "_score" : 3.4773107,
      "_source":{"host":"poolnode-03","metric":"cpu_metrics","name":"poolnode-03.cpu.cpu1.user","value":12541923,"@timestamp":"2014-11-12T17:50:22+00:00"}
    }, {
      "_index" : "sensu",
      "_type" : "cpu_metrics",
      "_id" : "AUmlIK4cRrcdtiDjFlbj",
      "_score" : 3.4773107,
      "_source":{"host":"poolnode-03","metric":"cpu_metrics","name":"poolnode-03.cpu.cpu1.irq","value":3,"@timestamp":"2014-11-12T17:50:22+00:00"}
    }, {
      "_index" : "sensu",
      "_type" : "cpu_metrics",
      "_id" : "AUmlIK4cRrcdtiDjFlbo",
      "_score" : 3.4773107,
      "_source":{"host":"poolnode-03","metric":"cpu_metrics","name":"poolnode-03.cpu.ctxt","value":4585398711,"@timestamp":"2014-11-12T17:50:22+00:00"}
    } ]
  },
  "aggregations" : {
    "cpu_histogram" : {
      "buckets" : [ {
        "key_as_string" : "2014-11-12T17:50:00.000Z",
        "key" : 1415814600000,
        "doc_count" : 68,
        "min_cpu" : {
          "value" : 0.0
        },
        "avg_cpu" : {
          "value" : 1.22906508E-315
        },
        "max_cpu" : {
          "value" : 2.2995982737E-314
        }
      }, {
        "key_as_string" : "2014-11-12T17:50:10.000Z",
        "key" : 1415814610000,
        "doc_count" : 68,
        "min_cpu" : {
          "value" : 0.0
        },
        "avg_cpu" : {
          "value" : 1.229071915E-315
        },
        "max_cpu" : {
          "value" : 2.2996008E-314
        }
      }, {
        "key_as_string" : "2014-11-12T17:50:20.000Z",
        "key" : 1415814620000,
        "doc_count" : 68,
        "min_cpu" : {
          "value" : 0.0
        },
        "avg_cpu" : {
          "value" : 1.229073614E-315
        },
        "max_cpu" : {
          "value" : 2.29960331E-314
        }
      }, {
        "key_as_string" : "2014-11-12T17:50:30.000Z",
        "key" : 1415814630000,
        "doc_count" : 68,
        "min_cpu" : {
          "value" : 0.0
        },
        "avg_cpu" : {
          "value" : 1.22907528E-315
        },
        "max_cpu" : {
          "value" : 2.2996057756E-314
        }
      }, {
        "key_as_string" : "2014-11-12T17:50:40.000Z",
        "key" : 1415814640000,
        "doc_count" : 68,
        "min_cpu" : {
          "value" : 0.0
        },
        "avg_cpu" : {
          "value" : 1.229076895E-315
        },
        "max_cpu" : {
          "value" : 2.2996081945E-314
        }
      }, {
        "key_as_string" : "2014-11-12T17:50:50.000Z",
        "key" : 1415814650000,
        "doc_count" : 68,
        "min_cpu" : {
          "value" : 0.0
        },
        "avg_cpu" : {
          "value" : 1.22907852E-315
        },
        "max_cpu" : {
          "value" : 2.2996107306E-314
        }
      } ]
    }
  }
}
```

I checked the mapping of the type and everything looks good:

```
{
  "sensu" : {
    "mappings" : {
      "cpu_metrics" : {
        "properties" : {
          "@timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "host" : {
            "type" : "string"
          },
          "metric" : {
            "type" : "string"
          },
          "name" : {
            "type" : "string"
          },
          "value" : {
            "type" : "long"
          }
        }
      }
    }
  }
}
```

I suspect I misunderstood some of the syntax and the error is on my side, but after studying the documentation for quite a while I still cannot figure out what is wrong with my search.

Thanks,
Mathias
</description><key id="48793648">8485</key><summary>unexpected results from aggregartions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mruediger</reporter><labels><label>:Mapping</label><label>bug</label><label>feedback_needed</label></labels><created>2014-11-14T14:13:15Z</created><updated>2014-11-17T11:11:04Z</updated><resolved>2014-11-17T11:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-14T14:30:13Z" id="63071430">Hi @mruediger 

Did you map the the `value` field explicitly, or just rely on dynamic mapping?  Currently there is an issue where a new field can be added to two shards at the same time, with different mappings (eg `double` vs `long`).  One mapping will win, but the other mapping continues to exist on the other shard.

I think this is what you're running into.  When you run the aggregations, some shards are returning longs while other shards are returning doubles, which is causing this mixup.

We plan on fixing this mapping issue, but in the meantime you can handle this by explicitly specifying the field type.
</comment><comment author="mruediger" created="2014-11-14T14:33:25Z" id="63071873">Hey @clintongormley , thanks for the quick reply.

I suspected something like this and ran
`curl -XPUT http://localhost:9200/sensu/_mapping/cpu_metrics/ -d '{ "cpu_metrics" : { "properties" : { "value" : { "type" : "long" }}}}'`

I don't now if that is enough to configure the mapping. Is it even possible to set the mapping for data that is already stored? Sorry, I am pretty new to elasticsearch.

-Mathias
</comment><comment author="clintongormley" created="2014-11-14T14:36:14Z" id="63072256">@mruediger no worries :)

It isn't sufficient to fix existing mappings.  You will need to reindex.

btw, a colleague has just clarified my description of the problem, as it is not quite as clear cut as a described.  In your case the `long` mapping won.  The problem shows up when the primary or replica shard with the internal `double` mapping is moved to another node, which then interprets the existing data as `long`, when really it has been indexed as a `double`.

Reindexing is the only way forward here I'm afraid. Perhaps try a subset of the data first, to confirm that it is the issue.
</comment><comment author="mruediger" created="2014-11-14T14:53:00Z" id="63074649">I will try that. Thanks!
</comment><comment author="clintongormley" created="2014-11-14T14:53:30Z" id="63074727">please let us know if that works - if it doesn't then there is another issue
</comment><comment author="mruediger" created="2014-11-14T16:00:05Z" id="63085308">It seems to be working now. Thanks :-)
</comment><comment author="mruediger" created="2014-11-14T16:08:56Z" id="63086792">Its normal that the min/max and avg results are returned as a floating point number (e.g. 4.654463936E9)?

Mathias
</comment><comment author="clintongormley" created="2014-11-17T11:11:03Z" id="63289957">Hi @mruediger 

Yes, these aggs always return doubles.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>More consistent naming for term vector[s]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8484</link><project id="" key="" /><description>We speak of the term vectors of a document, where each field has an associated
stored term vector. Since by default we are requesting all the term vectors of
a document, the HTTP request endpoint should rather be called `_termvectors`
instead of `_termvector`. The usage of `_termvector` is now deprecated, as
well as the transport client call to termVector and prepareTermVector.
</description><key id="48791517">8484</key><summary>More consistent naming for term vector[s]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-11-14T13:53:17Z</created><updated>2015-06-06T17:32:54Z</updated><resolved>2014-11-21T13:18:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-17T15:58:34Z" id="63326047">The change looks good to me. @clintongormley Could you just confirm that it's ok to break it for 2.0?
</comment><comment author="clintongormley" created="2014-11-17T16:01:49Z" id="63326605">@jpountz +1 - the old `/_termvector` URL is still supported (just not documented)
</comment><comment author="dakrone" created="2014-11-21T13:20:50Z" id="63968726">Shouldn't this be marked as "breaking" since it breaks the Java API compatibility?
</comment><comment author="alexksikes" created="2014-11-21T13:21:33Z" id="63968786">The old usage is still supported though.
</comment><comment author="dakrone" created="2014-11-21T13:22:26Z" id="63968872">Yes, for the REST side, on the Java side, TermVectorRequestBuilder became TermVectorsRequestBuilder and TermVectorResponse became TermVectorsResponse, which will cause compilation errors for Java API users
</comment><comment author="alexksikes" created="2014-11-21T13:30:46Z" id="63969652">On the Java client side you can still call on Client#termVector, but I agree for the other classes. So I'll mark it as breaking. Thanks!  
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsItemResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/MultiTermVectorsShardResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsFields.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TermVectorsWriter.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportMultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportShardMultiTermsVectorAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/TransportTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/DfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/ShardDfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/TransportDfsOnlyAction.java</file><file>src/main/java/org/elasticsearch/action/termvectors/dfs/package-info.java</file><file>src/main/java/org/elasticsearch/action/termvectors/package-info.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/morelikethis/MoreLikeThisFetchService.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsModule.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorsService.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/termvectors/RestMultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/termvectors/RestTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/transport/ActionNames.java</file><file>src/test/java/org/elasticsearch/action/IndicesRequestTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/AbstractTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsCheckDocFreqTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/GetTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/MultiTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/action/termvectors/TermVectorsUnitTests.java</file><file>src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/routing/SimpleRoutingTests.java</file><file>src/test/java/org/elasticsearch/transport/ActionNamesTests.java</file></files><comments><comment>Term Vectors: More consistent naming for term vector[s]</comment></comments></commit></commits></item><item><title>`copy_to` in object fields added by dynamic templates adds the full object mapping too late</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8483</link><project id="" key="" /><description>I have two issues:
- copy_to should copy values to upper nested_types by dot notation
- copy_to should copy values inside of just created dynamic fields defined with dynamic_template

see https://gist.github.com/hbartnick/9954ef86a1f3285ff5ba
</description><key id="48788267">8483</key><summary>`copy_to` in object fields added by dynamic templates adds the full object mapping too late</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">elasticjava</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2014-11-14T13:22:35Z</created><updated>2015-10-23T18:20:01Z</updated><resolved>2015-10-23T18:20:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-14T14:08:49Z" id="63068762">Hi @hbartnick

You'll be pleased to hear that in 1.4.0, all but one of your cases works as expected.  The only issue that fails can be simplified to the following:

```
DELETE /test
PUT /test/
{
  "mappings": {
    "test": {
      "dynamic_templates": [
        {
          "foo": {
            "match": "foo*",
            "mapping": {
              "type": "object",
              "properties": {
                "one": {
                  "type": "string",
                  "copy_to": [
                    "{name}.two"
                  ]
                },
                "two": {
                  "type": "string"
                }
              }
            }
          }
        }
      ]
    }
  }
}
```

Fails with: MapperParsingException[attempt to copy value to non-existing object [foo.two]]

```
PUT /test/test/1
{
  "foo": { "one": "bar"}
}
```

Succeeds and creates the correct mapping

```
PUT /test/test/1
{
  "foo": { "three": "bar"}
}
```

The first request now works correctly:

```
PUT /test/test/1
{
  "foo": { "one": "bar"}
}
```

So it looks like an issue with the order that dynamic mappings are updated.
</comment><comment author="elasticjava" created="2014-11-18T08:39:01Z" id="63437703">Hi @clintongormley - thanks a lot for your prompt reply!

So trying to adopt the suggestions I first insert a dummy document for the uncreated sections:
but now I realize, that data is merged together in the intentional nested sections. I'm confused...

see: https://gist.github.com/hbartnick/41b683e019862f916d3c
</comment><comment author="clintongormley" created="2014-11-23T13:54:14Z" id="64118592">Hi @hbartnick 

The problem is that your query is not doing what you think.  The `match` query does not support a query syntax (eg `Beitragstitel AND Arbeitstitel*`).  That's the `query_string` query.
</comment><comment author="elasticjava" created="2014-11-24T12:54:21Z" id="64189906">Hi @clintongormley,
thanks a lot! my fault....
</comment><comment author="elasticjava" created="2015-01-20T10:09:32Z" id="70631246">we compensate the error as follows:
1. after index creation we index only one document, containing all simplified structures of object-fields and delete this document immediately. the necessary structures in elasticsearch are now present.
2. for those fields we don´t know at index creation time we react on the MapperParsingException in a compensating loop, extract the information for a dummy-document we immediately index and delete in the same manner as before and try again the last index request:

``` java
    public static String createInitializerJson(MapperParsingException e) {
        String message = e.getMessage();
        String field = message.substring(message.indexOf('[') + 1, message.indexOf(']'));
        String initializerJsonPrefix = "{\"" + field.replace(".", "\": { \"");
        int countOpens = initializerJsonPrefix.length() - initializerJsonPrefix.replace("{", "").length();
        return initializerJsonPrefix + "\": \"dummy\"" + stringOfSize(countOpens, '}');
    }
    public static String stringOfSize(int size, char ch) {
        final char[] array = new char[size];
        Arrays.fill(array, ch);
        return new String(array);
    }
```
</comment><comment author="konradkonrad" created="2015-05-07T09:18:44Z" id="99788266">It seems to me, the problem was already expected in this comment?

``` java
if (mapper == null) {
    //TODO: Create an object dynamically?
    throw new MapperParsingException("attempt to copy value to non-existing object [" + field + "]");
}
```

https://github.com/elastic/elasticsearch/blob/4f14af21c5b9213e4b1add84d6d880eb20451e7d/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L638
</comment><comment author="konradkonrad" created="2015-10-22T23:56:37Z" id="150390617">possible duplicate #11237 ?
</comment><comment author="clintongormley" created="2015-10-23T18:20:01Z" id="150653469">Agreed - closing in favor of #11237
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Marvel does not work with rest.action.multi.allow_explicit_index: false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8482</link><project id="" key="" /><description>To prevent users from overriding the index which has been specified in the URL
i set 
`rest.action.multi.allow_explicit_index: false`
in elasticsearch.yml (using ES version 1.3.4) 

See: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/url-access-control.html
and https://www.found.no/foundation/elasticsearch-security/

After restarting elasticseach we are getting errors in the logs coming from Marvel:

```
2014-11-14 13:33:36,883 ERROR marvel.agent.exporter 
- [Nameless One] remote target          didn't respond with 200 OK response code [400 Bad Request].
content: [:) error ElasticsearchIllegalArgumentException[explicit index in bulk is not allowed]
```

See also https://groups.google.com/forum/#!topic/elasticsearch/hGy-S-jJdyQ
</description><key id="48785223">8482</key><summary>Marvel does not work with rest.action.multi.allow_explicit_index: false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">henrikno</reporter><labels /><created>2014-11-14T12:55:14Z</created><updated>2014-12-20T08:50:32Z</updated><resolved>2014-11-15T17:10:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-15T17:10:47Z" id="63179670">Hi @henrikno . This has been fixed and will be part of the next marvel release. Thx for reporting.
</comment><comment author="bleskes" created="2014-12-20T08:50:32Z" id="67729556">@henrikno you can give marvel 1.3.0 a try. This issue was solved there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add CBOR-friendly bulk format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8481</link><project id="" key="" /><description>The current bulk format in JSON uses the `\n` as end-of-line markers, allowing the coordinating node to parse just the metadata line, then skip over the body line.

CBOR doesn't have any disallowed characters or character sequences, so this mechanism will not work.  Instead we should have a CBOR specific format which looks like this:

```
{ metadata line }
int_length_of_body_line
{ body line }
```

The `int_length_of_body_line` would allow us to skip to the next metadata line.  

For the `delete` action, no `int_length_of_body_line` would be required, as we already know that it can't have a body.

Depends on #7640
</description><key id="48782157">8481</key><summary>Add CBOR-friendly bulk format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Bulk</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-14T12:25:56Z</created><updated>2016-08-16T13:54:37Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tarass" created="2016-08-12T23:56:51Z" id="239586580">Any updates?

CBOR would be great compliment to improved number handling in 5.0 release considering it has variable size numerics.
</comment><comment author="tlrx" created="2016-08-16T07:52:45Z" id="240029051">Since CBOR allows embedded CBOR data item, maybe we could have a CBOR specific bulk format that is basically an array of data items but use https://tools.ietf.org/html/rfc7049#section-2.4.4.1 for the body item?
</comment><comment author="clintongormley" created="2016-08-16T13:54:33Z" id="240108706">@tlrx that's an interesting idea... not sure how easy it'd be to fit into our bulk parsing code.  Please could you investigate when you have some time?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support VirtualLock on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8480</link><project id="" key="" /><description>On *nix boxes, we have the `mlockall` option, but that isn't supported by Windows.  Instead, Windows has [VirtualLock](http://msdn.microsoft.com/en-us/library/aa366895%28VS.85%29.aspx)

Could/should we support this option? Could we make the `bootstrap.mlockall` setting use VirtualLock on Windows boxes?
</description><key id="48766383">8480</key><summary>Support VirtualLock on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Settings</label></labels><created>2014-11-14T10:09:21Z</created><updated>2015-06-09T19:32:21Z</updated><resolved>2015-05-11T15:15:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-14T10:09:59Z" id="63036632">@costin @gmarz @Mpdreamz any ideas about this?
</comment><comment author="Mpdreamz" created="2014-11-14T15:29:44Z" id="63080061">No but sounds like something fun to investigate:

http://edc.tversu.ru/elib/inf/0088/0596003943_secureprgckbk-chp-13-sect-3.html
</comment><comment author="gmarz" created="2014-11-14T15:53:22Z" id="63084188">@clintongormley I've wondered this myself.  I knew VirtualLock existed, but have never done anything with it.  I'll start looking into it.
</comment><comment author="gmarz" created="2014-12-22T16:47:41Z" id="67859639">So after looking into this a bit, it does look possible to leverage `VirtualLock`.  However, it's important to note that `VirtualLock` works a bit more like `mlock` than it does `mlockall`, and thus is more tedious to implement as it requires more information and setup upfront (size of region to lock, memory needs to be allocated, etc).  Unfortunately, there is no equivalent `VirtualLockAll` in Windows.

In short, there are two key steps that need to be taken before calling `VirtualLock`:
- Increase working set size of the process using [SetProcessWorkingSetSize](http://msdn.microsoft.com/en-us/library/windows/desktop/ms686234%28v=vs.85%29.aspx).  By default, Windows limits the working set size of a process to ~30 pages.

&gt; This limit is intentionally small to avoid severe performance degradation. Applications that need to lock larger numbers of pages must first call the SetProcessWorkingSetSize function to increase their minimum and maximum working set sizes.
- Allocate the region of memory to lock using [VirtualAlloc](http://msdn.microsoft.com/en-us/library/windows/desktop/aa366887%28v=vs.85%29.aspx).

I've started implementing this in my own branch, and so far the results look good.  It'll probably be much easier to discuss this once we have some code to look at, so I'll open a PR once I'm confident with the changes and we can take it from there.
</comment><comment author="clintongormley" created="2014-12-22T17:34:57Z" id="67865552">nice work @gmarz 
</comment><comment author="gmarz" created="2015-01-07T19:22:39Z" id="69075882">Need to make a slight amendment to my previous comment above.

After digging a bit further, I've learned that there is no need for `VirtualAlloc`.  It's only useful for allocating and locking _new_ memory, where in this case, we need to lock the current memory already allocated by the JVM (Xms).

The only way I've found this possible is to walk the JVM address space using [VirtualQueryEx](http://msdn.microsoft.com/en-us/library/windows/desktop/aa366907) and lock each page individually.
</comment><comment author="s1monw" created="2015-05-11T07:32:15Z" id="100797730">I reverted both 1.x and master commits since tests are failing
</comment><comment author="rmuir" created="2015-05-11T13:00:34Z" id="100898844">I think its just a silly test bug? The failing test asserts that mlockall didn't succeed on windows, but now it can since its supported:

```
public void testMlockall() {
    if (Constants.WINDOWS) {
        assertFalse("Memory locking is not available on Windows platforms", Natives.LOCAL_MLOCKALL);
    }
    ...
```
</comment><comment author="gmarz" created="2015-05-11T13:28:57Z" id="100907444">@rmuir Yup.  I'm fixing this now.
</comment><comment author="rmuir" created="2015-05-11T13:55:58Z" id="100916460">I think we should just remove that whole assert. 
</comment><comment author="rmuir" created="2015-05-11T13:56:43Z" id="100916636">+1, my browser was a bit out of date.
</comment><comment author="gmarz" created="2015-05-11T15:15:05Z" id="100940024">I've put back the change and fixed the tests in [1.x](https://github.com/elastic/elasticsearch/commit/1aa5c77d4460d1382aa23c19ffe2b1a389f38295) and [master](https://github.com/elastic/elasticsearch/commit/cc3f02c0a03686b915b846d1aff04cd90cf29914).
</comment><comment author="javadevmtl" created="2015-06-08T22:09:18Z" id="110158046">I would gladly love to test this. Currently getting 100% usage in Windows with mapped files :)
I have 128GB server and twice made it up to 100% mem usage and unresponssive ES node.

Some details here: https://discuss.elastic.co/t/100-windows-memory-usage-but-no-oom/2089/4
And here: http://serverfault.com/questions/697511/setting-memory-mapped-file-limit-for-windows
</comment><comment author="gmarz" created="2015-06-09T19:32:21Z" id="110478145">Hey @javadevmtl this is available in [1.6](https://www.elastic.co/blog/elasticsearch-1-6-0-released), which we just released today.

Would be great if you could test this and provide any feedback.  Really interested in seeing what effect it has on the issue your experiencing.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/common/jna/Kernel32Library.java</file><file>src/main/java/org/elasticsearch/common/jna/Natives.java</file><file>src/main/java/org/elasticsearch/common/jna/SizeT.java</file></files><comments><comment>VirtualLock implementation for Windows (mlockall equivalent)</comment></comments></commit></commits></item><item><title>Allow configuration of the GC log file via an environment variable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8479</link><project id="" key="" /><description>Enabling GC logging works now by setting the environment variable ES_GC_LOG_FILE
to the full path to the GC log file. Missing directories will be created as needed.

The ES_USE_GC_LOGGING environment variable is no longer used.

Closes #8471
Closes #8479

Note that the default name of the file was already proper for Windows, so the fix for #8471 effectively is "do what windows did".
</description><key id="48762150">8479</key><summary>Allow configuration of the GC log file via an environment variable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ankon</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-14T09:37:27Z</created><updated>2015-06-08T15:25:40Z</updated><resolved>2015-02-12T16:14:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-23T14:05:13Z" id="64118932">Hi @ankon 

Thanks for this PR.  I'm not much of a shell scripter, but I'm wondering if those assignments need to be wrapped in quotes, in case a path contains spaces?
</comment><comment author="clintongormley" created="2014-11-23T14:09:38Z" id="64119109">In fact, a number of (existing) things fail if a path contains spaces: https://github.com/elasticsearch/elasticsearch/issues/8615
</comment><comment author="spinscale" created="2014-12-05T10:09:23Z" id="65769917">Hey @ankon 

this PR makes a lot of sense and we would like to get it in, but changes the default path of the `gc.log` to something else than `/var/log/` means that it will not work for our packages? Can you maybe add that `ES_GC_LOG_FILE` variable to `src/rpm/sysconfig/elasticsearch` and `src/deb/default/elasticsearch` as well, so we can fall back to `/var/log/elasticsearch/gc.log` as path in our packages?

would you be willing to update this PR (as well as quoting the paths in the batch script)? We are happy to help though!
</comment><comment author="ankon" created="2014-12-05T10:11:06Z" id="65770099">Missed the notifications, will have a look for the two raised issues.
</comment><comment author="ankon" created="2014-12-05T13:16:37Z" id="65788175">@spinscale can you have a look over 669d99f? I hope I found all places where this should be referenced, but I don't use debian and didn't figure out how to build the RPM to test. Also, it seems ES_USE_GC_LOGGING is actually off in the packages, which maybe is something to be changed?

@clintongormley I saw similar issues to #8615, b657693 and 9611e72 fix those for me. I tested this by building master, and then running `_JAVA_LAUNCHER_DEBUG=true ES_USE_GC_LOGGING=true sh -x bin/elasticsearch` to see that all arguments are properly parsed. I did leave the fixes on this branch though, but could pull them into a separate PR if you think that makes more sense.
</comment><comment author="spinscale" created="2014-12-08T10:40:44Z" id="66091356">@ankon if you do not set `USE_GC_LOGGING` in the init scripts (in case the `ES_GC_LOG_FILE` is set), then logging will not be activated, right? So this should also be a parameter in the defaults file I think
</comment><comment author="spinscale" created="2014-12-08T10:49:01Z" id="66099333">also, thinking about the current need for two env vars, would it potentially make more sense to replace `ES_USE_GC_LOGGING` with `ES_GC_LOG_FILE` completely? If it is set, we log somewhere, without the need to set an extra environment variable for enabling/disabling. What do you think?

Yes, this would break bwc, but I think it simplifies things.
</comment><comment author="ankon" created="2014-12-09T10:30:35Z" id="66262636">@spinscale, yes, you will need to explicitly enable GC logging before setting the name of the log file will do anything. 

For the package case I think this should be the default (you do have a reasonable place to put the log, and there is no reason for not having the logs: worst situation ever to run out of memory and then figure out you don't have logs to show, _and_ devs might not even be able to help you enable it because they don't know the package you're using).

For cases where packages aren't used the same argument holds for having logs enabled by default, but in those cases you have the choice where to have the log: default in the `logs/` directory, if variable is set then where that one points to.

IOW: Yes, I'd be very much in favor of dropping the `ES_USE_GC_LOGGING` by enabling GC logging by default. :)
</comment><comment author="spinscale" created="2014-12-12T08:04:15Z" id="66742701">Hey,

I took your PR as a base and fixed some things (and still need to test the packages), check it out here https://github.com/spinscale/elasticsearch/commit/671fe15a141f03c854e8d170ccfe1501864faad2
- Removed the `USE_GC_LOGGING` environment variable completely
- Made sure that the RPM does not do GC logging by default
- Added docs about the existence of the `ES_GC_LOG_FILE` environment variable

If you are good with this and we finished testing, we could get your PR in, if you add my changes. What do you think?
</comment><comment author="ankon" created="2014-12-12T10:57:53Z" id="66758822">If I understand the packaging scripts correctly then for debian we now have GC logging enabled by default (to `/var/log/elasticsearch/gc.log`), and it can be disabled by setting the default explicitly to an empty value.

For the rpms it looks reverse: GC logging is disabled by default, and can be enabled by uncommenting the line in the sysconfig file. 

I think it would make sense to have the packages behave in the same way, which according to the doc changes in spinscale@671fe15 would be that `src/deb/init.d/elasticsearch` should have it commented?

Re combining/adding: sure, what's the procedure for that? Merge your commit(s) into my branch?
</comment><comment author="spinscale" created="2014-12-12T15:44:30Z" id="66789414">good catch, fixed that in https://github.com/spinscale/elasticsearch/commit/ef8bd1346a1de7b9ea2fcbcceac51d91ad4045a9

I'd be happy if you merge my commit into your branch, as I dont want to take your credit away here by just pushing my commit, as it is merely based on your work with minor modifications. Does that work for you?
</comment><comment author="ankon" created="2014-12-15T16:06:59Z" id="67017112">Sounds good. I'm still learning my ways through git, but I just cherry-picked that commit now, and it looks good in the diff :)
</comment><comment author="ankon" created="2015-02-10T10:51:31Z" id="73680287">Anything on my side to be done to get this merged? 
</comment><comment author="spinscale" created="2015-02-12T11:09:29Z" id="74054196">Hey,

looks good. Can you rebase &amp; squash and then its ready from my point of view.
Sorry, this one slipped under my rader for too long.
</comment><comment author="ankon" created="2015-02-12T14:36:37Z" id="74080267">Hi, I squashed them together on top of the (then-current :D) master.

While doing that I fixed a minor issue for Windows: it didn't create the directories. This was tested with a Windows 10 cmd.exe, and worked as expected. Note that while the bat code handles them, the JVM does not actually allow paths with spaces for the GC log.
</comment><comment author="spinscale" created="2015-02-12T16:15:43Z" id="74099036">just merged it in, thx a lot for helping and your patience!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Allow configuration of the GC log file via an environment variable</comment></comments></commit></commits></item><item><title>Mapper Attachment content not decoded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8478</link><project id="" key="" /><description>I follwed the steps in this tutorial:
https://github.com/elasticsearch/elasticsearch-mapper-attachments/tree/v2.3.2/#version-232-for-elasticsearch-13

I am unable to GET text that is in decoded format. For example:
 "file": "IkdvZCBTYXZlIHRoZSBRdWVlbiIgKGFsdGVybmF0aXZlbHkgIkdvZCBTYXZlIHRoZSBLaW5nIg=="

should be decoded to  "God Save the Queen" (alternatively "God Save the King" but I am getting the same encoded content under _source. Where have I gone wrong?
</description><key id="48746273">8478</key><summary>Mapper Attachment content not decoded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tudit</reporter><labels /><created>2014-11-14T07:32:49Z</created><updated>2014-11-14T07:39:06Z</updated><resolved>2014-11-14T07:39:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-11-14T07:39:06Z" id="63020123">Hi @tudit 

You should ask this on the mailing list as we try to use github issues only for reporting issues and asking for feature requests.
Also, in this case, if you think you are hitting an issue, you should prefer reporting it in the mapper attachment plugin repo not in elasticsearch core.

That said I think your document has been decoded as expected and indexed as expected. What probably surprises you is that you can't see this string decoded in `_source` document. Actually, we never modify source document. That would explain what you are seeing.

You could store the document content and then ask for it when searching using `fields` option and you will be able to see the result.

Would be happy to follow up this discussion on the mailing list in case you need to discuss this.

Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch is constantly falling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8477</link><project id="" key="" /><description>Hello,

We are running on Elasticsearch Version 1.3.4 having 3 dedicated Master nodes , 3 Nodata nodes , 15 Data nodes. Since the upgrade to 1.3\* series we have suffered from a major instability issues with our ES cluster.

Basically what happens is that one of the master nodes gets  a JavaOutOfMemory ,then the clusters elects a new master and he gets  a JavaOutOfMemory too,in parallel one of the data nodes gets a JavaOutOfMemory. 

The Strange thing is that each node that gets JavaOutOfMemory,the process of Elasticsearch is immediately killed and restarted(After the restart the node rejoins the cluster) but the cluster is unresponsive from this point(The point where a Master and a Data node get  JavaOutOfMemory).
Nothing Helps to make the cluster recover from it's unresponsive status except a Full cluster restart.
Moreover, when looking at the cluster health using Es api "curl -XGET localhost:9200/_cluster/health?pretty" it shows that the cluster's health is yellow whereas when i take a look at the marvel dashboard it shows that all the indexes are not reporting for X time.

Having said that i can't understand how can it be that The health api shows the cluster is in yellow status, whereas marvel shows that all the indexes are not reporting.

Attached the log from today crash:
[2014-11-13 09:19:44,374][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
    at java.util.HashMap$KeySet.iterator(HashMap.java:912)
    at java.util.HashSet.iterator(HashSet.java:172)
    at java.util.Collections$UnmodifiableCollection$1.&lt;init&gt;(Collections.java:1099)
    at java.util.Collections$UnmodifiableCollection.iterator(Collections.java:1098)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:119)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:83)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:19:45,292][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 09:19:45,292][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 09:20:16,184][WARN ][index.merge.scheduler    ] [elasticsearch-prod-hist06] [2014_11][3] failed to merge
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 09:20:20,503][ERROR][index.engine.internal    ] [elasticsearch-prod-hist06] [2014_11][3] failed to acquire searcher, source search
org.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed
    at org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:98)
    at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:711)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:647)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:508)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:688)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:677)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:20:20,530][ERROR][index.engine.internal    ] [elasticsearch-prod-hist06] [2014_11][3] failed to acquire searcher, source search
org.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed
    at org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:98)
    at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:711)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:647)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:508)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:688)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:677)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:20:20,522][ERROR][index.engine.internal    ] [elasticsearch-prod-hist06] [2014_11][3] failed to acquire searcher, source search
org.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed
    at org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:98)
    at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:711)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:647)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:508)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:688)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:677)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:28:41,136][INFO ][action.admin.cluster.node.shutdown] [elasticsearch-prod-hist06] shutting down in [200ms]
[2014-11-13 09:28:41,350][INFO ][action.admin.cluster.node.shutdown] [elasticsearch-prod-hist06] initiating requested shutdown...
[2014-11-13 09:28:41,351][INFO ][node                     ] [elasticsearch-prod-hist06] stopping ...
[2014-11-13 09:28:41,468][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [transport disconnected (with verified connect)]
[2014-11-13 09:28:41,488][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [failed to perform initial connect [[elasticsearch-prod-hist-master02][inet[/10.179.174.119:9300]] connect_timeout[30s]]]
[2014-11-13 09:28:41,488][INFO ][cluster.service          ] [elasticsearch-prod-hist06] master {new [elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}, previous [elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}}, removed {[elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})
[2014-11-13 09:28:41,497][WARN ][discovery.ec2            ] [elasticsearch-prod-hist06] not enough master nodes after master left (reason = failed to perform initial connect [[elasticsearch-prod-hist-master02][inet[/10.179.174.119:9300]] connect_timeout[30s]]), current nodes: {[elasticsearch-prod-hist11][SD_0-pzqRV6Nd5sLEWklKg][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][l0oz1_vRQOuM2tgv9jlBVQ][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][NMHy5dyvTZeyZEaODIrr7A][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist10][2MKPMbOtTraoVgP-tz_60Q][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist06][Ekp72e2-Sl2wXZWIP7L22w][elasticsearch-prod-hist06.totango.com][inet[ip-10-63-144-12.ec2.internal/10.63.144.12:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][HuHAIiKWR4-r_-Y01y7uyQ][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][vuHFjWwoR46_7Mbu-hwJ_g][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist09][y6eEqkI_Tqmw-CI9UH3zCw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][PHir_YKIQMu_TWzT9XfL_Q][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd02][dtiEiaMmQgGYgcFiFsJpTA][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][uhrR_7uAT2ml1pYeWdQLKw][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist01][aTACc8Y6StGq5tLEQwPTVw][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist04][1FMiCjVOSXSSdowYKClEIA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][HhAGRdUaSpKKV4VxjfVymg][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd01][SARDt4MpRv2nXSBZ3IbB3Q][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist02][NjMPR4fRSUi0mIkgfuaoJw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist13][5c4jQFWiSyKGeVLneeiqXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][BcPwD4AtRd-gNdw3UHBXEA][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}
[2014-11-13 09:28:41,498][INFO ][cluster.service          ] [elasticsearch-prod-hist06] removed {[elasticsearch-prod-hist11][SD_0-pzqRV6Nd5sLEWklKg][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][l0oz1_vRQOuM2tgv9jlBVQ][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][NMHy5dyvTZeyZEaODIrr7A][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist10][2MKPMbOtTraoVgP-tz_60Q][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][HuHAIiKWR4-r_-Y01y7uyQ][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][vuHFjWwoR46_7Mbu-hwJ_g][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist09][y6eEqkI_Tqmw-CI9UH3zCw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][PHir_YKIQMu_TWzT9XfL_Q][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist-nd02][dtiEiaMmQgGYgcFiFsJpTA][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][uhrR_7uAT2ml1pYeWdQLKw][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist01][aTACc8Y6StGq5tLEQwPTVw][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist04][1FMiCjVOSXSSdowYKClEIA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][HhAGRdUaSpKKV4VxjfVymg][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd01][SARDt4MpRv2nXSBZ3IbB3Q][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist02][NjMPR4fRSUi0mIkgfuaoJw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist13][5c4jQFWiSyKGeVLneeiqXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][BcPwD4AtRd-gNdw3UHBXEA][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})
[2014-11-13 09:28:41,505][WARN ][action.bulk              ] [elasticsearch-prod-hist06] Failed to perform bulk/shard on remote replica [elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false}[2014_11][1]
org.elasticsearch.transport.NodeDisconnectedException: [elasticsearch-prod-hist15][inet[/10.203.179.73:9300]][bulk/shard/replica] disconnected
[2014-11-13 09:28:41,505][WARN ][action.bulk              ] [elasticsearch-prod-hist06] Failed to perform bulk/shard on remote replica [elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false}[2014_11][1]
org.elasticsearch.transport.NodeDisconnectedException: [elasticsearch-prod-hist15][inet[/10.203.179.73:9300]][bulk/shard/replica] disconnected
[2014-11-13 09:28:41,506][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist06] can't send shard failed for [2014_11][1], node[5KzNeqYLQuqLIVYdhVgc8Q], [R], s[STARTED]. no master known.
[2014-11-13 09:28:41,507][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist06] can't send shard failed for [2014_11][1], node[5KzNeqYLQuqLIVYdhVgc8Q], [R], s[STARTED]. no master known.
[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-c361d029-0][elasticsearch-prod-hist06.totango.com][inet[/10.7.144.161:9300]]]
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist03][inet[/10.7.144.161:9300]][discovery/zen/unicast]
Caused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-e76adb0d-0][elasticsearch-prod-hist06.totango.com][inet[/10.30.193.43:9300]]]
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist05][inet[/10.30.193.43:9300]][discovery/zen/unicast]
Caused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-3a6edfd0-0][elasticsearch-prod-hist06.totango.com][inet[/10.69.17.49:9300]]]
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist14][inet[/10.69.17.49:9300]][discovery/zen/unicast]
Caused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-bd51e057-0][elasticsearch-prod-hist06.totango.com][inet[/10.179.146.242:9300]]]
org.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist13][inet[/10.179.146.242:9300]][discovery/zen/unicast]
Caused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:28:42,207][INFO ][node                     ] [elasticsearch-prod-hist06] stopped
[2014-11-13 09:28:42,207][INFO ][node                     ] [elasticsearch-prod-hist06] closing ...
[2014-11-13 09:28:42,221][INFO ][node                     ] [elasticsearch-prod-hist06] closed
[2014-11-13 09:34:14,165][INFO ][node                     ] [elasticsearch-prod-hist06] version[1.3.4], pid[27430], build[a70f3cc/2014-09-30T09:07:17Z]
[2014-11-13 09:34:14,165][INFO ][node                     ] [elasticsearch-prod-hist06] initializing ...
[2014-11-13 09:34:14,380][INFO ][plugins                  ] [elasticsearch-prod-hist06] loaded [cloud-aws, marvel], sites [marvel]
[2014-11-13 09:34:18,354][INFO ][node                     ] [elasticsearch-prod-hist06] initialized
[2014-11-13 09:34:18,354][INFO ][node                     ] [elasticsearch-prod-hist06] starting ...
[2014-11-13 09:34:18,479][INFO ][transport                ] [elasticsearch-prod-hist06] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/10.63.144.12:9300]}
[2014-11-13 09:34:18,542][INFO ][discovery                ] [elasticsearch-prod-hist06] totango_prod_hist/AdEjYlsIRw6W0p-NahHGKA
[2014-11-13 09:34:30,557][INFO ][cluster.service          ] [elasticsearch-prod-hist06] detected_master [elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}, added {[elasticsearch-prod-hist13][7iNunaxMQdGB8dic_Y2lXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][M2XgBd9JSmWHqRQZ5cgj5Q][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][Xp6HPZorQJKsZM4PjnXBog][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][v_8fEEtiRSW70Uj4PKJJvg][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist09][ZcjG4XaeQr6gh-a_DkNIWw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist-nd02][DauYY04MQei6CfVw6Xvkqw][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-nd01][Ch6aYSnDQkO3H_-aXXDckw][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist15][qAn5cAEGSLS9YvcI7pDoOA][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist01][HfAEifKqS86YIuTy0yLyxQ][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist07][BqAauJRMS0yDcSgXkdrGIQ][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][WXMjYXo-QbKE1eWOQebciw][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist11][VbuIH5Y3THKLeJlXO2kIug][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][e9hJgHCnQQuhdZPu5ImWjw][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist02][ZRjhMm4QShu8EEuR8Q-HVw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])
[2014-11-13 09:34:30,809][INFO ][http                     ] [elasticsearch-prod-hist06] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.63.144.12:9200]}
[2014-11-13 09:34:30,810][INFO ][node                     ] [elasticsearch-prod-hist06] started
[2014-11-13 09:34:31,550][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist10][E4y5yg4YRCiEkgiiz2t9HQ][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])
[2014-11-13 09:34:32,559][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist04][oSanqbQPQA2Oi0yxLEa9hA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])
[2014-11-13 09:34:33,569][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist12][mNgZpVfrT5-IrtDGPWfURQ][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])
[2014-11-13 09:34:34,580][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist03][lAj9n3NDQUOv9IbWtlLq4A][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])
[2014-11-13 09:57:47,420][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.common.netty.buffer.ChannelBuffers.buffer(ChannelBuffers.java:134)
    at org.elasticsearch.common.netty.buffer.HeapChannelBufferFactory.getBuffer(HeapChannelBufferFactory.java:68)
    at org.elasticsearch.common.netty.buffer.AbstractChannelBufferFactory.getBuffer(AbstractChannelBufferFactory.java:48)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:80)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
[2014-11-13 09:59:03,811][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 09:59:04,613][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 09:59:04,643][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:02:30,821][WARN ][transport.netty          ] [elasticsearch-prod-hist06] exception caught on transport layer [[id: 0x7eaa6ee8, /10.179.174.119:48642 =&gt; /10.63.144.12:9300]], closing connection
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:02:40,859][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:02:40,860][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:02:40,859][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:02:49,551][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:02:51,862][WARN ][transport.netty          ] [elasticsearch-prod-hist06] exception caught on transport layer [[id: 0x7eaa6ee8, /10.179.174.119:48642 =&gt; /10.63.144.12:9300]], closing connection
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:04:16,167][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:04:16,167][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:04:16,168][WARN ][transport.netty          ] [elasticsearch-prod-hist06] exception caught on transport layer [[id: 0x1b5ad6c8, /10.63.144.12:53190 =&gt; /10.179.146.242:9300]], closing connection
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:04:17,117][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:04:16,167][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:06:42,143][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2014-11-13 10:07:13,079][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2014-11-13 10:07:13,755][INFO ][cluster.service          ] [elasticsearch-prod-hist06] master {new [elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}, previous [elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}}, removed {[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})
[2014-11-13 10:07:18,907][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [no longer master]
[2014-11-13 10:07:18,908][WARN ][discovery.ec2            ] [elasticsearch-prod-hist06] not enough master nodes after master left (reason = no longer master), current nodes: {[elasticsearch-prod-hist13][7iNunaxMQdGB8dic_Y2lXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][M2XgBd9JSmWHqRQZ5cgj5Q][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][mNgZpVfrT5-IrtDGPWfURQ][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][lAj9n3NDQUOv9IbWtlLq4A][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][Xp6HPZorQJKsZM4PjnXBog][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][v_8fEEtiRSW70Uj4PKJJvg][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist09][ZcjG4XaeQr6gh-a_DkNIWw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist10][E4y5yg4YRCiEkgiiz2t9HQ][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd02][DauYY04MQei6CfVw6Xvkqw][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-nd01][Ch6aYSnDQkO3H_-aXXDckw][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist04][oSanqbQPQA2Oi0yxLEa9hA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][qAn5cAEGSLS9YvcI7pDoOA][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist01][HfAEifKqS86YIuTy0yLyxQ][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist06][AdEjYlsIRw6W0p-NahHGKA][elasticsearch-prod-hist06.totango.com][inet[ip-10-63-144-12.ec2.internal/10.63.144.12:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][BqAauJRMS0yDcSgXkdrGIQ][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][WXMjYXo-QbKE1eWOQebciw][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist11][VbuIH5Y3THKLeJlXO2kIug][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][e9hJgHCnQQuhdZPu5ImWjw][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist02][ZRjhMm4QShu8EEuR8Q-HVw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}
[2014-11-13 10:07:18,919][INFO ][cluster.service          ] [elasticsearch-prod-hist06] removed {[elasticsearch-prod-hist13][7iNunaxMQdGB8dic_Y2lXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][M2XgBd9JSmWHqRQZ5cgj5Q][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][mNgZpVfrT5-IrtDGPWfURQ][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][lAj9n3NDQUOv9IbWtlLq4A][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][Xp6HPZorQJKsZM4PjnXBog][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][v_8fEEtiRSW70Uj4PKJJvg][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist09][ZcjG4XaeQr6gh-a_DkNIWw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist10][E4y5yg4YRCiEkgiiz2t9HQ][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist-nd02][DauYY04MQei6CfVw6Xvkqw][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-nd01][Ch6aYSnDQkO3H_-aXXDckw][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist04][oSanqbQPQA2Oi0yxLEa9hA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][qAn5cAEGSLS9YvcI7pDoOA][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist01][HfAEifKqS86YIuTy0yLyxQ][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][BqAauJRMS0yDcSgXkdrGIQ][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][WXMjYXo-QbKE1eWOQebciw][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist11][VbuIH5Y3THKLeJlXO2kIug][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][e9hJgHCnQQuhdZPu5ImWjw][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist02][ZRjhMm4QShu8EEuR8Q-HVw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})

Please advise?

Thanks,
Costya.
</description><key id="48691127">8477</key><summary>Elasticsearch is constantly falling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">cregev</reporter><labels /><created>2014-11-13T21:00:02Z</created><updated>2014-11-17T11:38:32Z</updated><resolved>2014-11-17T11:38:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-14T09:13:21Z" id="63027798">Hi @CostyaRegev 

Nothing that you show from the logs here indicates the root of the problem.  However, from previous conversations with your team, i know that you have very high filter cache eviction rates.  In other words, you are caching lots of filters which you never reuse.

I think you're running into #8249.  The filter cache size doesn't take the cache key into account, just the value.  I think you're filling up your heap with all of these cache keys, where the key is much bigger than the value.  Then you're never reusing these filters, so you keep adding more filters, until you OOM.

We made a change in 1.3.5 to make each cached entry count for a minimum amount, even if it was smaller, so that each cached entry has more weight.  This should help you.

But the real fix (as we've told you before) is to figure out which filters should be cached and which should not, and to disable caching for those filters. Building a cached filter is more expensive than just using a filter, so it only makes sense to do on those filters which are frequently reused.
</comment><comment author="cregev" created="2014-11-14T10:22:42Z" id="63038262">Hi @clintongormley 

We have disabled most of our caching by using doc_values for those fields(btw , we reindex all our cluster with new mapping settings (most of the fields are doc_values), but we are still facing problems with the cluster.

For example every time we restart our cluster our Indecies ID Cache starts with a relatively high number 
which does not make sense because we are not using a parent-child queries, thus  i need to run Elasticsearch's api for cleaning the ID cache  after every restart.

How this situation can happen ?

Thanks,
Costya
</comment><comment author="clintongormley" created="2014-11-14T11:03:31Z" id="63043886">@CostyaRegev doc values is to do with fielddata, not filter caching.  When you see your memory usage rising, try clearing the filter cache:

```
POST /_cache/clear?filter
```

And see if your heap drops (it may take up to a minute).  That will confirm my diagnosis.

For the ID cache - that is built eagerly, not at query time.  And there is no point in clearing it because it will just be rebuilt (at least for new segments).  However, if you are not using parent/child queries at all (which makes me wonder why you have the feature enabled) then you can set fielddata loading to `lazy`.

But really, if you need parent/child, then you need the memory to hold the IDs, in which case you either need more RAM or more nodes.
</comment><comment author="cregev" created="2014-11-16T13:36:42Z" id="63219136">Hi Clinton ,

Our cluster crashed today couple of times , and indeed it helped clearing the filter cache after i cleared the filter cache the cluster was back to normal , but something strange that happened is:
One of the data nodes got JavaOutOfMemory , then i cleared the filter cache but the data nodes= did not return the cluster , moreover then that i entered the host and checked that the service of elasticsearch was running but when i tried "curl -XGET localhost:9200/_cluster/health?pretty " is did  not succeed connecting the cluster i had to kill -9 the process in order to restart the service and only then the node joined back the cluster.

My question is this : when a node gets a JavaOutOfMemory should not it rejoin the cluster again after the process is restarted ? 

Another anomaly that i found is that when i use "curl -XPOST 'http://localhost:9200/_shutdown'" the shutdown api it does not shutdown all the nodes in the cluster , only part of them...
Who can this situation happen?

Thanks,
Costya.
</comment><comment author="clintongormley" created="2014-11-17T11:38:31Z" id="63292832">Hi @CostyaRegev 

After an OOM, the node is in an undefine state and has to be restarted.  It sounds like you are getting into a split brain condition.  You need to deal with the underlying problem which sounds like it is the cache key issue.  Upgrading will help, but really you need to stop caching filters which are not reused.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>CircuitBreakingException occurs without plausible reasons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8476</link><project id="" key="" /><description>I saw a CircuitBreakingException when running the following query yesterday:
`{"size":1,"query":{"filtered":{"filter":{"bool":{"must":[{"term":{"&lt;string field&gt;":"&lt;string&gt;"}},{"range":{"&lt;date field&gt;":{"gte":"2014-10-20","lte":"2014-10-21"}}}]}}}},"aggs":{"1":{"value_count":{"field":"request.count"}}}}`

request.count is a long field.
(FYI : The same query without any aggs does not match any documents.)

Response :
`{"error":"SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[G6x3HS9FTnidgrGxRG4U_g][rollup-2014-10-20][5]: RemoteTransportException[[&lt;node name&gt;][inet[/10.0.64.113:9300]][search/phase/query+fetch]]; nested: ElasticsearchException[org.elasticsearch.common.breaker.CircuitBreakingException: Data too large, data for field [request.count] would be larger than limit of [11998016307/11.1gb]]; nested: UncheckedExecutionException[org.elasticsearch.common.breaker.CircuitBreakingException: Data too large, data for field [request.count] would be larger than limit of [11998016307/11.1gb]]; nested: CircuitBreakingException[Data too large, data for field [request.count] would be larger than limit of [11998016307/11.1gb]]; }]","status":500}`

When I re-ran this query today, it took really long (148s) and returned without errors with the expected response.
Marvel tells me that the fielddata size went up by 300 megabytes.
I checked node stats and all the nodes where this index lives are 7-8 gig.

My question is why did the breaker hit yesterday given that the aggs was a simple value_count over a long field ? 
I'll be happy to include more logs if someone can tell me where to look for them.
</description><key id="48686888">8476</key><summary>CircuitBreakingException occurs without plausible reasons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aaneja</reporter><labels /><created>2014-11-13T20:20:10Z</created><updated>2014-11-24T19:03:29Z</updated><resolved>2014-11-14T08:54:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-14T08:54:44Z" id="63026106">Hi @aaneja 

The circuit breaker trips when there is not enough memory to serve the request.  In this case, it was unable to load the fielddata that it needed for the `request.count` field.  Presumably you have a lot of other data already loaded in fielddata.

Note: it doesn't just depend on that single query - it looks at how much heap is in use at the time. So if you have other queries running which are also using heap, then it may throw the exception at that moment, but later work just fine.

I suggest you look at what is consuming your fielddata:

```
GET /_nodes/stats/indices/fielddata?fields=*
```

Also consider changing those fields to use doc values instead (fielddata on disk), which then removes the problem of heap usage.

Also, given that your query took 148s, I'm betting that you have garbage collection issues.  Make sure that you have swap disabled.  See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory
</comment><comment author="aaneja" created="2014-11-20T00:31:09Z" id="63741425">So I reran the query today a bunch of times and queried the marvel reported node_stats for the failing nodes. The query -

```
GET /.marvel-2014.11.19/node_stats/_search?search_type=count
{
  "_source": [
    "@timestamp",
    "indices.fielddata.*",
    "fielddata_breaker.*",
    "indices.search.*"
  ],
  "aggs": {
    "ByIp": {
      "terms": {
        "field": "node.ip",
        "size": 0
      },
      "aggs": {
        "FiledDataBytes": {
          "stats": {
            "field": "indices.fielddata.memory_size_in_bytes"
          }
        },
        "QueryCurrent": {
          "stats": {
            "field": "indices.search.query_current"
          }
        },
        "QueryTotal": {
          "stats": {
            "field": "indices.search.query_total"
          }
        },
        "Tripped": {
          "stats": {
            "field": "fielddata_breaker.tripped"
          }
        },
        "Evictions": {
          "stats": {
            "field": "fielddata.evictions"
          }
        }
      }
    }
  },
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "@timestamp": {
                  "gte": "2014-11-19T20:20:48Z",
                  "lt": "2014-11-19T20:27:48Z"
                }
              }
            },
            {
              "bool": {
                "should": [
                  {
                    "term": {
                      "node.ip": "10.0.65.59"
                    }
                  },
                  {
                    "term": {
                      "node.ip": "10.0.64.243"
                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  },
  "sort": [
    {
      "@timestamp": {
        "order": "asc"
      }
    }
  ]
}
```

Here are the aggregation results -

```
"aggregations": {
      "ByIp": {
         "buckets": [
            {
               "key": "10.0.65.59",
               "doc_count": 40,
               "Evictions": {
                  "count": 0,
                  "min": null,
                  "max": null,
                  "avg": null,
                  "sum": null
               },
               "FiledDataBytes": {
                  "count": 40,
                  "min": 8185812512,
                  "max": 8185812512,
                  "avg": 8185812512,
                  "sum": 327432500480
               },
               "QueryTotal": {
                  "count": 40,
                  "min": 53021,
                  "max": 53534,
                  "avg": 53287.175,
                  "sum": 2131487
               },
               "QueryCurrent": {
                  "count": 40,
                  "min": 0,
                  "max": 9,
                  "avg": 1.4,
                  "sum": 56
               },
               "Tripped": {
                  "count": 40,
                  "min": 2885,
                  "max": 2915,
                  "avg": 2904.125,
                  "sum": 116165
               }
            },
            {
               "key": "10.0.64.243",
               "doc_count": 39,
               "Evictions": {
                  "count": 0,
                  "min": null,
                  "max": null,
                  "avg": null,
                  "sum": null
               },
               "FiledDataBytes": {
                  "count": 39,
                  "min": 8361789544,
                  "max": 8361789544,
                  "avg": 8361789544,
                  "sum": 326109792216
               },
               "QueryTotal": {
                  "count": 39,
                  "min": 48028,
                  "max": 48544,
                  "avg": 48303.333333333336,
                  "sum": 1883830
               },
               "QueryCurrent": {
                  "count": 39,
                  "min": 0,
                  "max": 0,
                  "avg": 0,
                  "sum": 0
               },
               "Tripped": {
                  "count": 39,
                  "min": 8841,
                  "max": 8884,
                  "avg": 8864.333333333334,
                  "sum": 345709
               }
            }
         ]
      }
   }
```

My take on these numbers : I would've imagined that when we have few parallel queries, then already loaded fielddata would not be a factor in determining if CircuitBreaker should trip or not.
This is not the case above.
There were no evictions either, in fact I have never seen evictions on these nodes over the past 2 days

Can you provide a more detailed explanation of how CircuitBreaker estimations work ?
</comment><comment author="aaneja" created="2014-11-21T19:02:13Z" id="64020721">Ping. Any updates ? 
</comment><comment author="clintongormley" created="2014-11-24T19:03:29Z" id="64245075">@aaneja Evictions will only happen if you set a maximum size to the fielddata cache.  Already loaded fielddata is taken into account.  Apparently, loading the data you requested would have pushed you over the configured circuit breaker limits, given the amount of heap space you have available.  

The details of what you can configure are available here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-fielddata.html

If you want to understand more low level details, your best bet would be to delve into the code itself.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix for ArithmeticException[/ by zero] when parsing a polygon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8475</link><project id="" key="" /><description>While this commit is primariy a fix for issue/8433 it adds more rigor to ShapeBuilder for parsing against the GeoJSON specification. Specifically, this adds LinearRing and LineString validity checks as defined in http://geojson.org/geojson-spec.html to ensure valid polygons are specified. The benefit of this fix is to provide a gate check at parse time to avoid any further processing if an invalid GeoJSON is provided.  More parse checks like these will be necessary going forward to ensure full compliance with the GeoJSON specification.

Closes #8433
</description><key id="48669940">8475</key><summary>Fix for ArithmeticException[/ by zero] when parsing a polygon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-13T18:02:52Z</created><updated>2015-06-07T18:10:54Z</updated><resolved>2014-11-19T14:25:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-14T09:10:48Z" id="63027563">@nknize I left a few comments. I think it's looking pretty good though
</comment><comment author="colings86" created="2014-11-14T14:40:04Z" id="63072787">@nknize left a couple of minor comments but I think its almost ready
</comment><comment author="colings86" created="2014-11-14T21:08:58Z" id="63129285">LGTM
</comment><comment author="s1monw" created="2014-11-21T09:43:48Z" id="63947010">@nknize @colings86 I think this should be ported to the `1.3` branch too and have a `1.3.6` label?
</comment><comment author="nknize" created="2014-11-24T14:28:43Z" id="64200683">pushed to branch `1.3` and added `1.3.6` tag &amp; label
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filtered aliases which use `now()` should not be cached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8474</link><project id="" key="" /><description>If an alias has a filter with a date-range expression containing `now()`, it is rewritten to use the concrete value that `now()` has at the time the filter is parsed, and is then cached.  Worse than that, if a shard is moved from one node to another, the filter is reparsed, meaning that `now()` on the new node can be substantially different from the value on other nodes.

Instead, we should not cache filters that use `now()`, and just reparse them on every query.
</description><key id="48668812">8474</key><summary>Filtered aliases which use `now()` should not be cached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Aliases</label><label>adoptme</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-13T17:53:43Z</created><updated>2015-04-05T17:43:01Z</updated><resolved>2014-11-19T22:22:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="swarzech" created="2015-04-02T19:00:09Z" id="89010010">Is there anyway to work around this issue in elasticsearch v 0.90?
</comment><comment author="clintongormley" created="2015-04-05T17:43:01Z" id="89817799">@swarzech no, you'll need to upgrade
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ResolvableFilter.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterCachingTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeTimezoneTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Core: Added query/filter wrapper that builds the actual query to be executed on the last possible moment to aid with index aliases and percolator queries using `now` date expression.</comment></comments></commit></commits></item><item><title>filtered aliases in templates do not inherit mappings from aliased index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8473</link><project id="" key="" /><description>Related to #6110 and #8431. This behavior is new since 1.4.0.

```
#reset logstash templates
DELETE /_template/logstash
DELETE /_template/filtered_logstash

# ensure index does not exist
DELETE logstash-2014.11.11

#create template for logstash indexes
PUT /_template/logstash
{
  "template": "logstash-*",
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "string_fields": {
            "mapping": {
              "index": "not_analyzed",
              "type": "string"
            },
            "match_mapping_type": "string",
            "match": "*"
          }
        }
      ],
      "properties": {
        "@timestamp": {
          "type": "date"
        },
        "clientip": {
          "index": "not_analyzed",
          "type": "string"
        }
      },
      "_all": {
        "enabled": true
      }
    }
  }
}

PUT _template/filtered_logstash
{
  "template": "logstash-*",
  "order": 1,
  "aliases": {
    "filtered-{index}": {
      "filter": {
        "bool": {
          "must_not": [
            {
              "terms": {
                "clientip": [
                  "1.2.3.4",
                  "2.3.4.5"
                ]
              }
            }
          ]
        }
      }
    }
  }
}

# (FAILS) insert doc to dynamically create index
POST /logstash-2014.11.11/event/1
{
  "@timestamp": "2014-11-11T00:02:14.000Z",
  "clientip": "3.4.5.6"
}

```

I would expect this to work, but instead I get `Strict field resolution and no field mapping can be found for the field with name [clientip]` when I try to `POST` that event.
</description><key id="48663309">8473</key><summary>filtered aliases in templates do not inherit mappings from aliased index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">loren</reporter><labels /><created>2014-11-13T17:10:14Z</created><updated>2015-03-12T19:57:42Z</updated><resolved>2014-11-24T17:27:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T17:50:37Z" id="62935208">@loren thanks for opening this.  The recreation can be simplified to a single template. It appears that the `_default_` mapping is only consulted after the aliases have been created:

```
DELETE _all 
DELETE /_template/*

PUT /_template/logstash
{
  "template": "logstash-*",
  "mappings": {
    "_default_": {
      "properties": {
        "clientip": {
          "index": "not_analyzed",
          "type": "string"
        }
      }
    }
  },
  "aliases": {
    "filtered-{index}": {
      "filter": {
        "bool": {
          "must_not": [
            {
              "terms": {
                "clientip": [
                  "1.2.3.4",
                  "2.3.4.5"
                ]
              }
            }
          ]
        }
      }
    }
  }
}
```

 Throws `Strict field resolution and no field mapping can be found for the field with name [clientip]`:

```
POST /logstash-2014.11.11/event/1
{
  "clientip": "3.4.5.6"
}
```

However, if you change the `_default_` mapping in the template to `event`,  then this works just fine.

@martijnvg could you have a look please?
</comment><comment author="shivangshah" created="2015-03-11T03:04:18Z" id="78191851">I apologize in advance for opening this conversation up after months of it being resolved, but I still see this problem with version 1.4.4. I have detailed the usecase in question and the corresponding files here: http://stackoverflow.com/questions/28977610/elastic-search-filtered-aliases-with-unmapped-fields
All help is appreciated and I apologize in advance if it is something silly that I missed !
</comment><comment author="eugenp" created="2015-03-12T19:57:42Z" id="78592580">Quick note - I am seeing the same problem after upgrading to 1.4.4. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file></files><comments><comment>Core: Fields defined in the `_default_` mapping of an index template should be picked up when an index alias filter is parsed if a new index is introduced when a document is indexed into an index that doesn't exist yet.</comment></comments></commit></commits></item><item><title>Improved Doc Routing </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8472</link><project id="" key="" /><description>Currently if you want to take control of routing ES does a typical hash &amp; mod operation on the routing parameter to determine which shard to rout data to. One would typically rout by userId, but a large user would of course cause issues with hotspotting a shard. I think this could be improved. 

If you could actually specify a shard # when indexing/searching then one could design their own data partitioning algorithm to split a user's data across a subset of the total shards in an index. 

Alternatively the ES API could 1st class this capability by accepting 2 additional routing parameters. EG take in a secondary routing param (for hashing) to determine which of x# shards the user's data is split across. If you had 10 shards, x = 3, then user's data is split across 3 shards with consistent hashing on the 2ndary routing param to pick which of the 3 holds that document. Searches need only provide the 1st routing param and x = 3, and all 3 shards would get lit up.
</description><key id="48653948">8472</key><summary>Improved Doc Routing </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewinin</reporter><labels><label>:CRUD</label><label>discuss</label><label>feedback_needed</label></labels><created>2014-11-13T16:03:22Z</created><updated>2016-11-26T12:20:09Z</updated><resolved>2016-11-26T12:20:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ddorian" created="2016-05-11T17:37:12Z" id="218533047">Any recommendation on how(if possible) to do this on the client-side ? Meaning how to make sure "user:1shard:1" is not on the same shard as "user:1:shard:2" ?
</comment><comment author="drewinin" created="2016-05-11T18:35:42Z" id="218549824">@ddorian We reverse engineered the routing mechanism to do it client side for the time being. You can check it out here https://github.com/MyPureCloud/elasticsearch-lambda/blob/master/src/main/java/com/inin/analytics/elasticsearch/index/routing/ElasticsearchRoutingStrategyV1.java
</comment><comment author="s1monw" created="2016-05-12T07:57:49Z" id="218686208">your reply brought up my attention to this ticket - I wonder why you don't simply use multiple indices and make the decisions yourself client side? I mean you have full control over where a document goes, full control on how many shards the docs are distributed across and you can even put them on stronger machines. The last point is what makes me think we shouldn't implement this feature. You will get in-balanced shards if you use routing in a way you use it which is tricky since shard balancing doesn't take this into account (it's hard to do that really) if you have this on an index level as I suggested you have all the control over it yourself which you would lose on a shard level. Yet, if you feel this won't work for you can you please explain why? 
</comment><comment author="ddorian" created="2016-05-12T08:24:30Z" id="218691661">@s1monw Do 2-indexes made of 1-shard have the same overhead as 1-index made of 2-shards ? I assume the 2-shards-index has less overhead on the cluster.
</comment><comment author="s1monw" created="2016-05-12T08:31:25Z" id="218693141">@ddorian there is certainly overhead here but what are we talking about, like some MB of heap space for data-strucutures to hold the metadata? From a search/indexing perspective there is no overhead at all.
</comment><comment author="ddorian" created="2016-05-12T08:46:29Z" id="218696445">@s1monw 
In my case, I have monthly indexes. And having many monthly indexes for many big clients may have more overhead compared to 1-monthly-index with many shards ?

From a search-index perspective, having per-client indexes will actually be faster since you don't have to maintain a "client_id" indexed field.

Interested how the big log-as-a-service users do this. Maybe they do 1-client-index and maintain multiple clusters (so not all customers on 1 cluster) ?
</comment><comment author="s1monw" created="2016-05-12T08:51:20Z" id="218697448">I think it depends on your distribution of customers you don't need an index per customer. You can have like 3 indices `small, medium, large` where you can use routing in the `small` index distribute over 3 shards int the `medium` and over 6 shards in the `large` one and you classify your customers into these categories. It's also easier to move customers from one to another in the next month and you might even end up with less shards eventually?
</comment><comment author="s1monw" created="2016-05-12T08:52:15Z" id="218697654">&gt; Interested how the big log-as-a-service users do this. Maybe they do 1-client-index and maintain multiple clusters (so not all customers on 1 cluster) ?

that is a good idea in general - you won't loose all customers on an incident, it's likely easier to migrate to new versions, etc.
</comment><comment author="ddorian" created="2016-05-12T09:01:21Z" id="218699744">@s1monw 
Won't the small,medium,large situation be better by just having 1 index with the same number of shards as small,medium,large combined ?

By per-customer-index, I'm talking only about large customers. Small ones will go into shared indexes.

&gt; It's also easier to move customers from one to another in the next month and you might even end up with less shards eventually?

I can't delete old-indexes (like log companies do) so probably not.
</comment><comment author="s1monw" created="2016-05-12T09:24:54Z" id="218705033">&gt; Won't the small,medium,large situation be better by just having 1 index with the same number of shards as small,medium,large combined ?

the entire issue is about that it can be problematic since if you use routing you get in-balanced shards? Why don't you just not use one big index without routing then? as far as I can tell you need to do more research on what is really your bottleneck. I am waiting for @drewinin before I close this issue
</comment><comment author="ddorian" created="2016-05-12T09:32:47Z" id="218706880">@s1monw 
I don't use 1 big index without routing because ALL my queries filter on user_id, so I want to route on the those shard(s) and not scatter-gather on 30 shards(size of the shared index) for big clients but to keep on 2-3-4 etc shards depending on their size.
</comment><comment author="s1monw" created="2016-05-12T09:36:13Z" id="218707689">see that is exactly why I recommend multiple indices for customers that don't fit into one shard
</comment><comment author="drewinin" created="2016-05-12T12:12:38Z" id="218738851">@s1monw That'd work if we had predictable usage patterns. We have a lot of customers that ramp up usage 100x without warning. So they might be "small customer" for most of the year and then explode for a couple days. We do daily log indexes btw. We'd get caught by surprise having them in the "small customer index". It seemed simpler to have 1 index per day and balance customers across shards than trying to predict who's going to have a busy day.
</comment><comment author="clintongormley" created="2016-11-26T12:20:09Z" id="263060469">Closing in favour of #21585</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Non-existing location for default gc.log file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8471</link><project id="" key="" /><description>`bin/elasticsearch.in.sh` specifies that the GC log file should be stored at `/var/log/elasticsearch/gc.log` (when `ES_USE_GC_LOGGING` is set).

This location would not exist by default, and I also think it is a rather odd place given that other logs end up in the (existing) `logs` directory.

Could this be changed to 
1. by default use $ES_HOME/logs/gc.log 
2. be overrideable by another setting so one doesn't need to modify the script file

This issue has now caused about 50% of my node restarts when updating ... I always forget about it, see the error, wait for the cluster to be reasonable, and then restart again :)
</description><key id="48652527">8471</key><summary>Non-existing location for default gc.log file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label></labels><created>2014-11-13T15:55:19Z</created><updated>2015-02-12T16:14:37Z</updated><resolved>2015-02-12T16:14:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ankon" created="2014-11-18T10:37:32Z" id="63451139">PR #8479 has a fix for this issue. 

Note that the windows .bat file already did the right thing, so the PR makes windows and !windows behave in the same way, and then adds the ability to configure the name of the log file from the outside.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Allow configuration of the GC log file via an environment variable</comment></comments></commit></commits></item><item><title>Set default node name to hostname</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8470</link><project id="" key="" /><description>I can't find any discussion on it. Can this be done? It would be convenient.
</description><key id="48652081">8470</key><summary>Set default node name to hostname</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">printercu</reporter><labels><label>:Packaging</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2014-11-13T15:52:50Z</created><updated>2015-04-17T08:17:56Z</updated><resolved>2015-04-17T08:17:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T16:51:13Z" id="62924662">You can configure `elasticsearch.yml` to set the node name to an environment variable, eg:

```
node.name: ${FOO}
```

then start Elasticsearch with:

```
FOO=foo.bar.com bin/elasticsearch
```

You could also use `${HOSTNAME}`, but be aware that even though `echo $HOSTNAME` works, it is not a real environment variable (at least in `bash`).  However, if you update the config to:

```
node.name: ${HOSTNAME}
```

then the following line works:

```
HOSTNAME=$HOSTNAME bin/elasticsearch
```
</comment><comment author="printercu" created="2014-11-13T17:12:57Z" id="62929020">I know. As in the book (http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_important_configuration_changes.html) said it's recommended to rename host, I think having default node name to hostname will be 'stable by default'.

Having comic heroes names is cool, but it's not useful in production.
</comment><comment author="clintongormley" created="2014-11-13T17:13:49Z" id="62929174">The problem is that you can start more than one node on the same host.
</comment><comment author="printercu" created="2014-11-13T17:31:26Z" id="62932020">Yeah, haven't thought of it. Thank you.

However maybe it'll be useful to have magic variable (like `_local_` or `_non_loopback_` for network) to set name to hostname without adding `export HOSTNAME` to init.d scripts.
</comment><comment author="clintongormley" created="2014-11-13T17:43:24Z" id="62933989">@printercu I agree, which is why I had left this ticket open :)  I'll reopen.
</comment><comment author="AndreKR" created="2015-01-03T20:00:26Z" id="68607448">Please do so. :)
</comment><comment author="clintongormley" created="2015-01-05T11:28:25Z" id="68696540">I think all we need to do here is to export `HOSTNAME` as a real environment variable, then users can use it easily in their config
</comment><comment author="HariSekhon" created="2015-04-01T16:51:06Z" id="88551820">How about just setting the default node name to `$HOSTNAME:&lt;port&gt;` - that will account for multiple instances per host.

This is similar to what SolrCloud does (they actually do `&lt;ip&gt;:&lt;port&gt;` which is also unique but much less useful unless you only have a few servers and know all their IPs off by heart or enjoy doing DNS lookups - I changed those to hostname:port).

The Marvel names are interesting but were starting to make debugging more tedious so I've set them to hostnames too since I only have one instance per server.

In order to work around the

```
node.name ${HOSTNAME}
```

variable not being available you can put this in /etc/sysconfig/elasticsearch

```
export HOSTNAME=$(hostname -s)
```

which is easier to auto-manage across a cluster of nodes than say `HOSTNAME=$HOSTNAME bin/elasticsearch` if you're using automation and not starting nodes by hand. Note that the `export` keyword is required in /etc/sysconfig/elasticsearch unless you want to go around editing/copying your init scripts to do an explicit `export HOSTNAME` afterwards after sourcing that file. Most decent automation folks will already be managing elasticsearch.yml and sysconfig/elasticsearch anyway.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update terms-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8469</link><project id="" key="" /><description>It looks like the reference editor forgot to remove some draft notes. These two sentences look more like a version history or like TODOs than the reference info.
</description><key id="48649977">8469</key><summary>Update terms-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">golubev</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-13T15:38:40Z</created><updated>2015-01-21T10:18:11Z</updated><resolved>2015-01-21T09:37:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T15:42:59Z" id="62910740">Whoops - that'd be me.  Thanks for spotting. Could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="golubev" created="2014-11-13T15:45:01Z" id="62911332">Hi @clintongormley 

I've already signed the ICLA and my company LUN.UA LLC signed the CCLA.
</comment><comment author="clintongormley" created="2014-11-13T15:50:02Z" id="62912221">Ah right - I remember now. Unfortunately our CLA check tool can only read the personal CLAs, not the company CLAs, which means that we're going to ask you the same thing every time (unless you also sign a personal CLA).

merged, thanks
</comment><comment author="golubev" created="2014-11-13T15:55:36Z" id="62913724">@clintongormley thanks!

You mean the personal CLA - not the ICLA under the CCLA?

I had an e-mail confirmation as below:

```
Individual Contributor License Agreement under CCLA between elasticsearch BV and LUN UA LLC is Signed and Filed!

From: CLA Committers (elasticsearch BV) 
To: Sergii Golubiev
```
</comment><comment author="clintongormley" created="2014-11-13T15:58:07Z" id="62914514">@golubev correct - the echosign API doesn't give us visibility to the CCLA ones :(
</comment><comment author="clintongormley" created="2015-01-21T09:37:15Z" id="70808757">Thanks @golubev - closed by 78afe0003e463df4d021b3cd24d4cdd69566ec2a
</comment><comment author="golubev" created="2015-01-21T10:18:11Z" id="70813794">Thanks @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting not working when min_gram set to 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8468</link><project id="" key="" /><description>Hi,
I use elasticsearch 1.4 and I discovered a problem with highlighting in the following scenario, where min_gram is set to 1 for an ngram tokenizer:

``` json
PUT test_highlighting
{
  "settings": {
    "analysis": {
      "analyzer": {
        "nGram_analyzer": {
          "alias": "default_index",
          "tokenizer": "nGram_tokenizer",
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      },
      "tokenizer": {
        "nGram_tokenizer": {
          "type": "nGram",
          "min_gram": 1,
          "max_gram": 25,
          "token_chars": [ "letter", "digit" ]
        }
      }
    }
  }
}

POST test_highlighting/doc/
{
  "content": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua."
}

GET test_highlighting/_search
{
  "query": {
    "match": {
      "content": "a"
    }
  },
  "highlight": {
    "fields": {
      "*": { }
    }
  }
}
```

For the search query at the end, I get this result:

``` json
{
   "took": 6,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.039754517,
      "hits": [
         {
            "_index": "test_highlighting",
            "_type": "doc",
            "_id": "AUmpZaAOCwTCfR9MAewK",
            "_score": 0.039754517,
            "_source": {
               "content": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua."
            },
            "highlight": {
               "content": [
                  "Lorem ipsum dolor sit &lt;em&gt;a&lt;/em&gt;met, consetetur s&lt;em&gt;a&lt;/em&gt;dipscing elitr, sed di&lt;em&gt;a&lt;/em&gt;m nonumy eirmod tempor invidunt u",
                  "t l&lt;em&gt;a&lt;/em&gt;bore et dolore m&lt;em&gt;agna&lt;/em&gt; &lt;em&gt;aliquya&lt;/em&gt;m er&lt;em&gt;a&lt;/em&gt;t, sed di&lt;em&gt;a&lt;/em&gt;m voluptu&lt;em&gt;a&lt;/em&gt;."
               ]
            }
         }
      ]
   }
}
```

You can see that "agna" in "magna" and "aliquya" in "aliquyam" get highlighted. I think this is a bug.
I only get these wrong results when min_gram is set to 1, the search term consists of only one char, and the search term appears two times closely together in one word (like "a" in "magna" or "aliquyam").

Using an ngram token filter instead, with version 4.1 (as suggested in #3137), also does not work for me. I tried these index settings:

``` json
PUT test_highlighting
{
  "settings": {
    "analysis": {
      "analyzer": {
        "nGram_analyzer": {
          "alias": "default_index",
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "asciifolding",
            "nGram_filter"
          ]
        }
      },
      "filter": {
        "nGram_filter": {
          "type": "nGram",
          "min_gram": 1,
          "max_gram": 25,
          "version": "4.1"
        }
      }
    }
  }
}
```

Then highlighting works correctly when searching for single chars. But when I search for multiple chars:

``` json
GET test_highlighting/_search
{
  "query": {
    "match": {
      "content": "mag"
    }
  },
  "highlight": {
    "fields": {
      "*": { }
    }
  }
}
```

Then there is another highlighting problem ("mag" gets repeated in "magna"):

``` json
{
   "took": 7,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.011986438,
      "hits": [
         {
            "_index": "test_highlighting",
            "_type": "doc",
            "_id": "AUmpbwK1CwTCfR9MAe9w",
            "_score": 0.011986438,
            "_source": {
               "content": "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua."
            },
            "highlight": {
               "content": [
                  " labore et dolore magn&lt;em&gt;mag&lt;/em&gt;a aliquyam erat, sed diam voluptua."
               ]
            }
         }
      ]
   }
}
```

Could anyone please help me out?
</description><key id="48640463">8468</key><summary>Highlighting not working when min_gram set to 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sschuerz</reporter><labels><label>:Highlighting</label></labels><created>2014-11-13T14:17:37Z</created><updated>2016-11-24T19:01:32Z</updated><resolved>2016-11-24T19:01:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-13T14:24:05Z" id="62896756">Its certainly a bug but highlighting doesn't see a whole ton of love so I wouldn't count on it being high on anyone's list.

In the mean time you can try to use the [fast-vector-highlighter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#fast-vector-highlighter) instead of the plain highlighter and see if that fixes things.  You can also try the [experimental highlighter plugin](https://github.com/wikimedia/search-highlighter) to see if that helps.

Full disclosure: I maintain the experimental highlighter plugin.  It is "experimental" in the sense that if is built to be easy to modify.  Maybe modular would have been a better name.  Whatever.  Despite the low version number its pretty stable and happily serves several million searches a day.
</comment><comment author="sschuerz" created="2014-11-13T14:39:40Z" id="62899061">Thank you, @nik9000. I know the experimental hightlighter plugin. I've used it with elasticsearch 1.3 and I got good results (a least when not using "*" in the "fields" clause).

However, when I try to use the experimental hightlighter plugin in elasticsearch 1.4:

``` json
GET test_highlighting/_search
{
  "query": {
    "match": {
      "content": "mag"
    }
  },
  "highlight": {
    "fields": {
      "content": { "type": "experimental" }
    }
  }
}
```

I get this error: ElasticsearchIllegalArgumentException[unknown highlighter type [experimental] for the field [content]]

@nik9000, could this be a compatibility issue?

I think the plugin is installed correctly, because it shows up in the plugins list on command line.

Anyway, I'll give the fast vector highlighter a try now.
</comment><comment author="nik9000" created="2014-11-13T15:45:19Z" id="62911388">Yeah, experimental is 1.3 only at this point because we haven't moved to
1.4 and it basically follows our cluster to make things easy on me. Give
the fast vector highlighter a chance then. We'll get the experimental to
1.4 in a few weeks probably.
On Nov 13, 2014 9:39 AM, "Severin Schürz" notifications@github.com wrote:

&gt; Thank you, @nik9000 https://github.com/nik9000. I know the experimental
&gt; hightlighter plugin. I've used it with elasticsearch 1.3 and I got good
&gt; results (a least when not using "*" in the "fields" clause).
&gt; 
&gt; However, when I try to use the experimental hightlighter plugin in
&gt; elasticsearch 1.4:
&gt; 
&gt; GET test_highlighting/_search
&gt; {
&gt;   "query": {
&gt;     "match": {
&gt;       "content": "mag"
&gt;     }
&gt;   },
&gt;   "highlight": {
&gt;     "fields": {
&gt;       "content": { "type": "experimental" }
&gt;     }
&gt;   }
&gt; }
&gt; 
&gt; I get this error: ElasticsearchIllegalArgumentException[unknown
&gt; highlighter type [experimental] for the field [content]]
&gt; 
&gt; @nik9000 https://github.com/nik9000, could this be a compatibility
&gt; issue?
&gt; 
&gt; I think the plugin is installed correctly, because it shows up in the
&gt; plugins list on command line.
&gt; 
&gt; Anyway, I'll give the fast vector highlighter a try now.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8468#issuecomment-62899061
&gt; .
</comment><comment author="sschuerz" created="2014-11-14T08:50:48Z" id="63025758">I've tried the fast-vector-highlighter now and it does the highlighting correctly.

The only problem (apart from the bigger index size) I have is that I need to put every searchable field into the mapping to specify `"term_vector": "with_positions_offsets"`. The mapping should however be dynamically created. I found no documented way to change the default value of `term_vector` on type level or even globally ... does anyone have an idea how this could be achieved?
</comment><comment author="clintongormley" created="2016-11-24T19:01:32Z" id="262832719">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>geo_shape query with polygon from -180/90 to 180/-90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8467</link><project id="" key="" /><description>Hi,
Since 1.4.0 i have issues running the following query:

```
{
  "size" : 0,
  "query" : {
    "bool" : {
      "must" : [ 
 {
        "geo_shape" : {
          "loc" : {
            "shape" : {
              "type" : "polygon",
              "coordinates" : [ [ 
[ -180, 90 ], 
[ 180, 90 ], 
[ 180, -90 ], 
[ -180, -90 ], 
[ -180, 90 ] 
] ]
            }
          }
        }
      } ]
    }
  },
  "aggregations" : {
    "ship" : {
      "terms" : {
        "field" : "entity_id",
        "size" : 0
      }
    }
  }
}
```

The error is :

```
 TopologyException[found non-noded intersection between LINESTRING ( -180.0 -90.0, -180.0 90.0 ) and LINESTRING ( Infinity Infinity, -180.0 -90.0 ) [ (-180.0, -90.0, NaN) ]]; }]",
```

The same query was running with 1.3.4
If i change the points to : 179/89 this works.

I am reading the polygon from PostGIS DB as :
0103000000010000000500000000000000008066C0000000000080564000000000008066400000000000805640000000000080664000000000008056C000000000008066C000000000008056C000000000008066C00000000000805640

st_asgeojson:
{"type":"Polygon","coordinates":[[[-180,90],[180,90],[180,-90],[-180,-90],[-180,90]]]}
</description><key id="48637729">8467</key><summary>geo_shape query with polygon from -180/90 to 180/-90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">j0r0</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2014-11-13T13:51:22Z</created><updated>2014-11-24T14:34:30Z</updated><resolved>2014-11-24T14:34:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T15:35:18Z" id="62908326">@nknize could you have a look at this please?
</comment><comment author="nknize" created="2014-11-14T14:33:02Z" id="63071816">Sure thing!  Will check this out today.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/BasePolygonBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/test/java/org/elasticsearch/common/geo/ShapeBuilderTests.java</file></files><comments><comment>[GEO] Fix for geo_shape query with polygon from -180/90 to 180/-90</comment></comments></commit></commits></item><item><title>Why I have some unassigned shards?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8466</link><project id="" key="" /><description>I put a elasticsearch file on D drive and another elasticsearch file on E drive.
I modify their elasticsearch.yml to change the http.port and tcp.port.
But when I start two nodes on my PC, there are some unassigned shards.  
![00](https://cloud.githubusercontent.com/assets/8230560/5028158/9ca8fa60-6b72-11e4-9590-d784e12262a7.png)

How can I fix this problem?
Thanks in advance.
</description><key id="48630088">8466</key><summary>Why I have some unassigned shards?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JiayiFu</reporter><labels /><created>2014-11-13T12:22:25Z</created><updated>2014-11-13T13:34:21Z</updated><resolved>2014-11-13T13:27:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T13:27:04Z" id="62889645">@JiayiFu Please ask these questions in the mailing list instead: http://elasticsearch.org/community. You'll need to provide more information than you have done here.  I suggest looking in the log files of each node for more information.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documents missing after adding a new node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8465</link><project id="" key="" /><description>1. Create a new Elasticsearch cluster with 2 nodes and 5000 documents.
2. Add  a new node with zone awareness in a different zone. 
3. Exclude IPs and delete the first 2 nodes.
4. Delete 20 documents.
5. Add 2 new nodes with zone awareness in a different zone.
6. Exclude IPs and delete the old single node.
7. Add back the same 20 documents.
8. Add a new node with zone awareness in a different zone.

We can now see inconsistent document counts returned from each of the nodes. If we flush we see the correct 5000 documents. But if we exclude IPs and delete the 2 old nodes we see data loss and only 4980 documents. The newly added 20 documents go missing.
</description><key id="48626571">8465</key><summary>Documents missing after adding a new node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">aadithya</reporter><labels><label>feedback_needed</label></labels><created>2014-11-13T11:38:58Z</created><updated>2015-04-01T13:26:50Z</updated><resolved>2015-04-01T13:26:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T13:25:05Z" id="62889407">Hi @aadithya 

I got confused by your description, eg "Exclude IPs".  Could you be more explicit about exactly what requests you are running on which nodes?
</comment><comment author="aadithya" created="2014-11-13T14:52:11Z" id="62900912">We are using "cluster.routing.allocation.exclude._ip" to decommission the old nodes. We exclude the old nodes and wait till all the shards are available in the new ones, before shutting down the old nodes. 
</comment><comment author="clintongormley" created="2014-11-13T15:37:05Z" id="62908887">@aadithya when i asked for more detail, i mean MORE detail :) I don't know what nodes you are setting what on.  Spell it out for me 
</comment><comment author="clintongormley" created="2014-11-13T15:37:39Z" id="62909069">I want to know how many nodes are up, when, which nodes, what the cluster health says, what you do next, etc
</comment><comment author="aadithya" created="2014-11-14T11:06:43Z" id="63044838">We are able to consistently reproduce this scenario with the following steps-

The cluster health is always green. 

1) Create a cluster using Elasticsearch version 1.3.2 on two instances of Amazon EC2 m3.xlarge with the EC2 discovery plugin. All settings are set to defaults except  
 cluster.routing.allocation.awareness.attributes: 0

2) Upload 5000 documents using es.index() API in ElasticsearchPy client.
3) Add a new instance[ip3] of m3.xlarge which joins the cluster. 
    cluster.routing.allocation.awareness.attributes: 1

4) Set cluster.routing.allocation.exclude._ip: ip1, ip2 where ip1 and ip2 are the ips of the first 2 nodes.
5) Wait for shard state to be STARTED for all shards in ip3 which are also part of ip1,ip2.
6) Terminate ip1 and ip2.
7) Delete 20 documents.
8) Add two new instances ip4, ip5.
     cluster.routing.allocation.awareness.attributes: 2

9) Set cluster.routing.allocation.exclude._ip: ip3. 
10) Wait for shard state to be STARTED for all shards in ip4, ip5 which are also part of ip3.
11) Terminate ip3.
12) Add 20 documents. ( The same 20 that were deleted.)
13) Add a new instance ip6.
       cluster.routing.allocation.awareness.attributes: 3

Hope this helps. 
</comment><comment author="clintongormley" created="2014-11-14T11:33:31Z" id="63049197">@aadithya much clearer, thanks.

Couple of questions: 
- you are setting `cluster.routing.allocation.awareness.attributes` but you don't say what attributes you are setting on which nodes.  If you don't set any attributes, then your index can't be assigned.  Could you add this info please?
- When you delete 20 documents, are you using delete-by-query?
</comment><comment author="aadithya" created="2014-11-15T10:09:26Z" id="63167172">Sorry, I misunderstood awareness attributes. We are setting:

cluster.routing.allocation.awareness.attributes: di_number

We are setting a different di_number for every wave of deployment. 
[ip1, ip2] - node.di_number : 0
[ip3]          node.di_number: 1
[ip4, ip5]   node.di_number: 2
[ip6]          node.di_number: 3

We are using this function from ElasticsearchPy, es.delete(index=index, doc_type=doc_type, id=id) to delete documents. I guess this is deleting documents by ID. 
</comment><comment author="muralikpbhat" created="2014-11-26T10:25:28Z" id="64543161">Is it possible that if both the primary and replica node gets excluded (via cluster.routing.allocation.exclude._ip setting) for a particular shard, the un-flushed data can be lost? Does 'excluding' an ip ensure that the new writes to that shard are going to a non-excluded ip as well after the shard relocation has started?
</comment><comment author="aadithya" created="2015-04-01T13:26:49Z" id="88479601">This was an issue with our test. Adding version numbers to our test fixed the issue. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add log4j-extras dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8464</link><project id="" key="" /><description>Close #7927
</description><key id="48618879">8464</key><summary>Add log4j-extras dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-13T10:12:49Z</created><updated>2015-06-07T10:48:56Z</updated><resolved>2014-11-13T12:45:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-11-13T11:29:05Z" id="62877468">LGTM
</comment><comment author="spinscale" created="2014-11-13T12:43:49Z" id="62884920">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java</file></files><comments><comment>Fix example in logging daily rotate configuration</comment></comments></commit></commits></item><item><title>Internal: Don't pass acceptdocs down to nonNestedDocsFilter in ParentsFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8463</link><project id="" key="" /><description>PR for #8461. The bug can only occur in 1.3.5, but should be backported to the other branches as well.

The PR is against 1.3 branch
</description><key id="48617853">8463</key><summary>Internal: Don't pass acceptdocs down to nonNestedDocsFilter in ParentsFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label></labels><created>2014-11-13T10:02:09Z</created><updated>2015-05-18T23:29:27Z</updated><resolved>2014-11-14T09:56:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-13T17:42:38Z" id="62933857">Looks good!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Breaking Changes 1.4 is missing mvel changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8462</link><project id="" key="" /><description>In 1.3.5 it was possible to use mvel scripting out-of-the-box, in 1.4.0 however one explicitly needs to install the https://github.com/elasticsearch/elasticsearch-lang-mvel plugin.

I think this change should be mentioned on 
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/breaking-changes-1.4.html.
</description><key id="48611970">8462</key><summary>Breaking Changes 1.4 is missing mvel changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels /><created>2014-11-13T08:57:52Z</created><updated>2014-11-13T12:51:56Z</updated><resolved>2014-11-13T12:51:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T12:51:56Z" id="62885742">@ankon thanks for spotting - fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ClassCastException in ParentIdsFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8461</link><project id="" key="" /><description>Similar to #8390, 

```
ParentIdsFilter.java#144
nonNestedDocs = (FixedBitSet) nonNestedDocsFilter.getDocIdSet(context, acceptDocs);
```

If you pass acceptDocs, then the result may not be a FixedBitSet.  Not sure if the solution in #8390 works here too.
</description><key id="48584183">8461</key><summary>ClassCastException in ParentIdsFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">fizx</reporter><labels /><created>2014-11-13T00:51:23Z</created><updated>2014-11-20T14:58:18Z</updated><resolved>2014-11-20T14:58:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fizx" created="2014-11-13T00:51:48Z" id="62823195">@rmuir ^
</comment><comment author="martijnvg" created="2014-11-13T09:35:08Z" id="62864467">@fizx I assume you encountered this bug on version 1.3.5?
</comment><comment author="nz" created="2014-11-13T15:02:00Z" id="62902386">@martijnvg Yes, that's in 1.3.5 (I work with @fizx)
</comment><comment author="martijnvg" created="2014-11-20T14:58:17Z" id="63820022">Fixed via PR #8463
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping illegalargument exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8460</link><project id="" key="" /><description>Hello, I seem to have an issue with mapping on my index, this is causing bulk failure, which is causing my client to close and interrupting my river!!!!!!

The error logs, (posted below) seem to be freaking out anytime the values in the field core_log have a : in them(some do and some do not, the field contains our error messages/stack traces), which would make sense to me, except I have the mapping set to string so it shouldnt affect it? I have also tried setting dynamic: strict, but this seemed to change nothing, am I missing something about this? if not it seems like a bug

```
Put core42/_mapping/jdbc
{
    "jdbc":{
        "dynamic":"false",
        "properties": {
        "CLASS": {
                  "type": "string"
               },
               "CORE_LEVEL": {
                  "type": "string"
               },
               "CORE_LOG": {
                  "type": "string"
               },
               "CORE_USER": {
                  "type": "string"
               },
               "CUSTOMER": {
                  "type": "string"
               },
               "DATETIME": {
                  "type": "date",
                  "format": "dateOptionalTime"
               },
               "SERVER_NAME": {
                  "type": "string"
               },
               "WEBAPP": {
                  "type": "string"
               }
        }
    }
}


[2014-11-12 10:12:31,996][ERROR][river.jdbc.BulkNodeClient] bulk [29] failed with 3 failed items, failure message = failure in bulk execution:[58]: index [core42], type [jdbc], id [103756], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][62]: index [core42], type [jdbc], id [103760], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][147]: index [core42], type [jdbc], id [103767], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:12:32,061][ERROR][river.jdbc.BulkNodeClient] bulk [29] failed with 3 failed items, failure message = failure in bulk execution:[38]: index [core42], type [jdbc], id [103756], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][42]: index [core42], type [jdbc], id [103760], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][127]: index [core42], type [jdbc], id [103767], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:13:12,003][ERROR][river.jdbc.BulkNodeClient] bulk [37] failed with 1 failed items, failure message = failure in bulk execution:[99]: index [core42], type [jdbc], id [103747], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:13:12,077][ERROR][river.jdbc.BulkNodeClient] bulk [37] failed with 1 failed items, failure message = failure in bulk execution:[71]: index [core42], type [jdbc], id [103747], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:13:31,618][ERROR][river.jdbc.BulkNodeClient] bulk [29] failed with 3 failed items, failure message = failure in bulk execution:[29]: index [core42], type [jdbc], id [103756], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][33]: index [core42], type [jdbc], id [103760], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][118]: index [core42], type [jdbc], id [103767], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:13:31,660][ERROR][river.jdbc.BulkNodeClient] bulk [29] failed with 2 failed items, failure message = failure in bulk execution:[60]: index [core42], type [jdbc], id [103756], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][64]: index [core42], type [jdbc], id [103760], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:13:36,665][ERROR][river.jdbc.BulkNodeClient] bulk [30] failed with 1 failed items, failure message = failure in bulk execution:[4]: index [core42], type [jdbc], id [103767], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:14:11,633][ERROR][river.jdbc.BulkNodeClient] bulk [37] failed with 1 failed items, failure message = failure in bulk execution:[59]: index [core42], type [jdbc], id [103747], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:14:11,710][ERROR][river.jdbc.BulkNodeClient] bulk [37] failed with 1 failed items, failure message = failure in bulk execution:[91]: index [core42], type [jdbc], id [103747], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:14:30,195][ERROR][river.jdbc.BulkNodeClient] bulk [29] failed with 3 failed items, failure message = failure in bulk execution:[45]: index [core42], type [jdbc], id [103756], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][49]: index [core42], type [jdbc], id [103760], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][134]: index [core42], type [jdbc], id [103767], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:14:30,256][ERROR][river.jdbc.BulkNodeClient] bulk [29] failed with 3 failed items, failure message = failure in bulk execution:[55]: index [core42], type [jdbc], id [103756], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][59]: index [core42], type [jdbc], id [103760], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ][144]: index [core42], type [jdbc], id [103767], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:15:10,204][ERROR][river.jdbc.BulkNodeClient] bulk [37] failed with 1 failed items, failure message = failure in bulk execution:[82]: index [core42], type [jdbc], id [103747], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:15:10,280][ERROR][river.jdbc.BulkNodeClient] bulk [37] failed with 1 failed items, failure message = failure in bulk execution:[94]: index [core42], type [jdbc], id [103747], message [MapperParsingException[failed to parse [CORE_LOG]]; nested: ElasticsearchIllegalArgumentException[unknown property [d]]; ]
[2014-11-12 10:15:58,523][ERROR][river.jdbc.RiverThread ] interrupted while shutdownjava.io.IOException: interrupted while shutdown at org.xbib.elasticsearch.plugin.jdbc.pipeline.executor.SimplePipelineExecutor.shutdown(SimplePipelineExecutor.java:145) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.afterPipelineExecutions(RiverThread.java:140) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.run(RiverThread.java:130) at java.lang.Thread.run(Unknown Source) at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) at java.util.concurrent.FutureTask.run(Unknown Source) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source)
[2014-11-12 10:15:58,525][ERROR][river.jdbc.RiverThread ] interrupted while shutdownjava.io.IOException: interrupted while shutdown at org.xbib.elasticsearch.plugin.jdbc.pipeline.executor.SimplePipelineExecutor.shutdown(SimplePipelineExecutor.java:145) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.afterPipelineExecutions(RiverThread.java:140) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.run(RiverThread.java:130) at java.lang.Thread.run(Unknown Source) at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) at java.util.concurrent.FutureTask.run(Unknown Source) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source)
[2014-11-12 10:15:58,524][ERROR][river.jdbc.RiverThread ] interrupted while shutdownjava.io.IOException: interrupted while shutdown at org.xbib.elasticsearch.plugin.jdbc.pipeline.executor.SimplePipelineExecutor.shutdown(SimplePipelineExecutor.java:145) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.afterPipelineExecutions(RiverThread.java:140) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.run(RiverThread.java:130) at java.lang.Thread.run(Unknown Source) at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) at java.util.concurrent.FutureTask.run(Unknown Source) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source)
[2014-11-12 10:15:58,523][ERROR][river.jdbc.RiverThread ] interrupted while shutdownjava.io.IOException: interrupted while shutdown at org.xbib.elasticsearch.plugin.jdbc.pipeline.executor.SimplePipelineExecutor.shutdown(SimplePipelineExecutor.java:145) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.afterPipelineExecutions(RiverThread.java:140) at org.xbib.elasticsearch.plugin.jdbc.RiverThread.run(RiverThread.java:130)
```
</description><key id="48568959">8460</key><summary>Mapping illegalargument exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blakekp</reporter><labels /><created>2014-11-12T22:03:30Z</created><updated>2014-11-13T12:17:42Z</updated><resolved>2014-11-13T12:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sushanthku" created="2014-11-12T22:07:23Z" id="62803813">Can you paste an example of the document you are trying to index
</comment><comment author="blakekp" created="2014-11-12T22:17:21Z" id="62805317">actually apologies, upon trying to isolate the issue I have found this has to do with logstash's elasticsearch plugin, where should I place this question?

my logstash config file is as follows, the above posted stack trace however is inside of elasticsearch, which is leading me to beleive it is on elasticsearches execution being enacted by the logstash plugin, apologies for the lack of understanding of the issue previous to now

```
input {
    #stdin{}
    elasticsearch {
    host =&gt; "10.14.1.157"
    port =&gt; "9200"
    index =&gt; "core42"
    query =&gt; "license error"
  }
}

output {

    file {
    path =&gt; "C:\Users\username\Desktop\logfile.txt"
    }
   email {
    from =&gt; "logstash.alert@nowhere.com"
    subject =&gt; "Testing"
    to =&gt; "someone@somewhere.com"
    via =&gt; "smtp"
    body =&gt; "Here is the event line that occured:%{[core_log]}"
    htmlbody =&gt; "&lt;h2&gt;Alert&lt;/h2&gt;&lt;br/&gt;&lt;br/&gt;&lt;h3&gt;you have been automatically alerted about the following event:&lt;/h3&gt;&lt;br/&gt;&lt;br/&gt;&lt;div align='center'&gt;%{[core_log]}&lt;/div&gt;"
    options =&gt; { smtpIporHost =&gt; "smtp.gmail.com"
                         port =&gt; 587
                         domain =&gt; "gmail.com"
                         userName =&gt; "somemail@gmail.com"
                         password =&gt; "somepass"
                         starttls =&gt; true                                                                                                             
                         authenticationType =&gt; login
                        }
    }

}
```
</comment><comment author="clintongormley" created="2014-11-13T12:17:42Z" id="62882049">Hi @blakekp 

Something is feeding bad data to Elasticsearch. I don't know if it is the JDBC plugin or Logstash.  I suggest you try each of them separately to narrow it down, then open the issue in the relevant project.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem while indexing a document with routing after upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8459</link><project id="" key="" /><description>We are in the process of upgrading to ES 1.3.4 from version 1.0.0. But we are having problems while indexing. 

Say we have an index - "test1" with  the following mapping:   

```
{
  "mappings": {
    "visit": {
      "dynamic": "strict",
      "_all": {
        "enabled": false
      },
      "_routing": {
        "required": true,
        "path": "visitor.id"
      },
      "properties": {
        "visitor.id": {
          "type": "string"
        }
      }
    }
  }
}

```

You can see here that we use custom routing (using the "visitor.id" field). An insert of the type:

```
{
"visitor.id" : abc
}
```

 would work in version 1.0.0 but fails in version 1.3.4 with the following exception (because of the dot notation in the field name):
"MapperParsingException[The provided routing value [abc] doesn't match the routing key stored in the document: [null]]",

There wasn't a standard regarding field names and an attempt has been made here: https://github.com/elasticsearch/elasticsearch/issues/6736

But the routing check while indexing is causing problems for us. Is there any solution without having to reindex?
</description><key id="48564439">8459</key><summary>Problem while indexing a document with routing after upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sushanthku</reporter><labels /><created>2014-11-12T21:24:28Z</created><updated>2014-11-13T16:57:32Z</updated><resolved>2014-11-13T12:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-13T12:03:45Z" id="62880734">Hi @sushanthku 

Yes, there were (and still are) a number of corner cases that weren't handled correctly.  As you point out, with #6736 we're planning on tightening all of this up.  I'm afraid that there is nothing to do here but reindex.

Also, note that we are planning on removing the ability to extract routing values from within the document body (#6730) so you may want to take that into account when reindexing.
</comment><comment author="sushanthku" created="2014-11-13T15:54:44Z" id="62913449">I do specify the routing value while indexing, but the dot notation causes it to try and find a nested field.  
This bug is not going to be fixed you say?
</comment><comment author="clintongormley" created="2014-11-13T16:57:31Z" id="62925763">Hi @sushanthku 

&gt; I do specify the routing value while indexing, but the dot notation causes it to try and find a nested field.

To be clear, you need to specify the routing value in the query string (or as part of an alias), not as part of the document body.  If you do that already, then there is no need for the `path` parameter.

&gt; This bug is not going to be fixed you say?

Correct - the real bug is allowing field names with dots in them. That will be going away.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multiple versions of mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8458</link><project id="" key="" /><description>We are facing a situation where document structure in number of types is not quite stable and keeps changing.

At first we attempted a dynamic mapping approach but it failed us with a need for more specific mapping such as in case of case-insensitive mapping, guid treated as a whole string, and phrase sorting rather than a term sorting.

Our second attempt is at the static mapping and batch-like job that validates the mappings and, if needs to be, builds a new index with new mappings and then copies over the old documents. The process is long and tedious, often enough the process times out before completing the index rebuild and then starts over at the next scheduled opportunity.

We it be feasible to avoid such massive rebuild of the index by maintaining older versions of the mapping until such time that all documents (of an older version) and due to naturally attrition through document updates would have migrated away to newer versions?
</description><key id="48546029">8458</key><summary>Multiple versions of mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">VKhazinGladiator</reporter><labels /><created>2014-11-12T18:43:27Z</created><updated>2014-11-12T19:48:23Z</updated><resolved>2014-11-12T19:48:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T19:48:23Z" id="62781354">Hi @VKhazinGladiator 

&gt; We it be feasible to avoid such massive rebuild of the index by maintaining older versions of the mapping until such time that all documents (of an older version) and due to naturally attrition through document updates would have migrated away to newer versions?

You have a few options, depending on your use case.  Really it depends on whether you need the new mappings applied to old documents or not.  If you do, then you have no option but to reindex.  If you don't then:
- you can always add new fields to existing type mappings
- you can add new "sub-fields" to existing field mappings eg to analyze existing fields in a different way (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#_multi_fields_3)
- if your data is append-only, then you can create a new index for the new documents, and use an alias which points to the old and new index to search across both of them

With options 1 and 2, you can also reindex existing documents into the same index, which will fill in the newly added fields for you.

We are also planning to add a reindex API which will make the reindex process more reliable and less difficult.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Index corruption with ES1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8457</link><project id="" key="" /><description>I've got a 2-node cluster, both with 8G of heap and 16G ram total running on Java7 with Centos7
Today, during a batch insert job I've begun noticing "CorruptIndexException"s like these:

```
[2014-11-12 17:42:06,305][WARN ][cluster.action.shard     ] [Cluemaster] [somtoday-iridium-straat1-2014-10-05][1] received shard failed for [somtoday-iridium-straat1-2014-10-05][1], node[W4q8
nityRu6TsZQacyWTvQ], [R], s[INITIALIZING], indexUUID [nqd-mNyZTpqvD-rjml098g], reason [Failed to start shard, message [CorruptIndexException[[somtoday-iridium-straat1-2014-10-05][1] Preexisti
ng corrupted index [corrupted_FEmwcRlwQMWvhBuhU3kZzw] caused by: CorruptIndexException[codec footer mismatch: actual footer=-1637713527 vs expected footer=-1071082520 (resource: NIOFSIndexInp
ut(path="/home/elasticsearch/gotham/nodes/0/indices/somtoday-iridium-straat1-2014-10-05/1/index/_0.cfs"))]
org.apache.lucene.index.CorruptIndexException: codec footer mismatch: actual footer=-1637713527 vs expected footer=-1071082520 (resource: NIOFSIndexInput(path="/home/elasticsearch/got
ham/nodes/0/indices/somtoday-iridium-straat1-2014-10-05/1/index/_0.cfs"))
        at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:235)
        at org.apache.lucene.codecs.CodecUtil.retrieveChecksum(CodecUtil.java:228)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:586)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:519)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:502)
        at org.elasticsearch.index.store.Store.getMetadata(Store.java:183)
        at org.elasticsearch.index.store.Store.getMetadataOrEmpty(Store.java:147)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:726)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:578)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:182)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:431)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

I'm inserting at one node over REST and replicating with default settings to the other node.
Will it automatically recover or do I need to do something about it?

---

java version "1.7.0_71"
OpenJDK Runtime Environment (rhel-2.5.3.1.el7_0-x86_64 u71-b14)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)
</description><key id="48539729">8457</key><summary>Index corruption with ES1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thomasmarkus</reporter><labels /><created>2014-11-12T17:44:26Z</created><updated>2014-11-12T22:01:11Z</updated><resolved>2014-11-12T21:51:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T18:04:38Z" id="62762988">Hi @thomasmarkus 

Was this index created on an older version of Elasticsearch?  Recent versions have added more checksumming, so it is possible that you're seeing corruptions that occurred in an earlier version.
</comment><comment author="thomasmarkus" created="2014-11-12T18:09:02Z" id="62763653">Hi @clintongormley 

I've recently upgraded from 1.4.0beta1 -&gt; 1.4.0 final. Does that count as an 'upgrade' in this case?
</comment><comment author="clintongormley" created="2014-11-12T18:10:16Z" id="62763830">@thomasmarkus the index was first created on 1.4.0.beta1?
</comment><comment author="thomasmarkus" created="2014-11-12T18:14:18Z" id="62764439">@clintongormley correct. I've browsed through a few hundred of these exceptions and they all refer to a set of indices created with beta1. Indices created after 1.4.0beta1 do not appear in the list of exceptions and seem fine.
</comment><comment author="thomasmarkus" created="2014-11-12T18:26:19Z" id="62766332">After employing more grep-foo it seems that only a few shards of only 1 index are effected. The logging however is quite repetitive (if you account for the different shards) and excessive for this case.
</comment><comment author="thomasmarkus" created="2014-11-12T18:28:49Z" id="62766744">Am I right to assume that if the other node had a replica _without_ corruption that the issue would resolve itself automatically? This doesn't seem to be the case, so I'm wondering how to best recover from this state (aside from the obvious re-indexing of the data)
</comment><comment author="s1monw" created="2014-11-12T18:47:27Z" id="62769644">it seems like something got corrupted maybe during your upgrade and what ES does in this case is mark your shard as corrupted. If ES finds this corruption marker it will spit out this exception. If you have a copy of that shard sitting somewhere else you can safely wipe the index directory `/home/elasticsearch/gotham/nodes/0/indices/somtoday-iridium-straat1-2014-10-05/1/index/` then you should get rid of the annoying exception. The reason we don't do this ourself is that this is usually and exceptional case we want the user to resolve instead of blindly wiping your data.
</comment><comment author="thomasmarkus" created="2014-11-12T18:50:55Z" id="62771770">I agree that user intervention is probably desirable, but the logspam is pretty excessive. I would expect a few lines after which it marks the index as corrupted and ignores it. The current behavior of ES is to keep logging this fact several times a second.

Can I wipe the index folder while the ES node is running or should I restart?
</comment><comment author="s1monw" created="2014-11-12T18:52:18Z" id="62772019">you can wipe this particular folder at runtime but restart would be the safe option
</comment><comment author="s1monw" created="2014-11-12T20:17:16Z" id="62786015">@thomasmarkus can you report back once you did the change please?
</comment><comment author="thomasmarkus" created="2014-11-12T20:19:39Z" id="62786377">I removed the reportedly faulty shard while ES was running and the excessive exception logging has ceased immediately.
</comment><comment author="thomasmarkus" created="2014-11-12T20:21:41Z" id="62786677">A few minutes later I'm seeing this logged by ES (possibly unrelated issue, but I'm including it nonetheless to get the full picture)

```
[2014-11-12 21:19:28,449][WARN ][index.engine.internal    ] [Blackmask] [somtoday-iridium-straat2-2014-08-17][0] failed to read latest segment infos on flush
java.nio.file.NoSuchFileException: /home/elasticsearch/gotham/nodes/0/indices/somtoday-iridium-straat2-2014-08-17/0/index/_0.si
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
    at java.nio.channels.FileChannel.open(FileChannel.java:287)
    at java.nio.channels.FileChannel.open(FileChannel.java:334)
    at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:81)
    at org.apache.lucene.store.FileSwitchDirectory.openInput(FileSwitchDirectory.java:172)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:443)
    at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:113)
    at org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoReader.read(Lucene46SegmentInfoReader.java:51)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:358)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:454)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:906)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)
    at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:121)
    at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:112)
    at org.elasticsearch.index.engine.internal.InternalEngine.readLastCommittedSegmentsInfo(InternalEngine.java:329)
    at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:943)
    at org.elasticsearch.index.engine.internal.InternalEngine.updateIndexingBufferSize(InternalEngine.java:239)
    at org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:168)
    at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:489)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

I have a matching exception for the same shard on the master node:

```
[2014-11-12 20:19:21,521][DEBUG][action.admin.indices.stats] [Cluemaster] [somtoday-iridium-straat2-2014-08-17][0], node[bPxJWDIORm-WT15ztGU_xA], [R], s[STARTED]: failed to execute [org.e
lasticsearch.action.admin.indices.stats.IndicesStatsRequest@73afbf81]
org.elasticsearch.transport.RemoteTransportException: [Blackmask][inet[/192.168.56.106:9300]][indices:monitor/stats[s]]
Caused by: org.elasticsearch.ElasticsearchException: io exception while building 'store stats'
        at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:542)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
        at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:205)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:56)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:339)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:325)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.lucene.store.NoSuchDirectoryException: directory '/home/elasticsearch/gotham/nodes/0/indices/somtoday-iridium-straat2-2014-08-17/0/index' does not exist
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
        at org.apache.lucene.store.FileSwitchDirectory.listAll(FileSwitchDirectory.java:87)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.elasticsearch.common.lucene.Directories.estimateSize(Directories.java:40)
        at org.elasticsearch.index.store.Store.stats(Store.java:213)
        at org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:540)
        ... 10 more
```

This also smells like index corruption and may share a related root cause. However, this index was created with ES 1.4.0 final and not the beta1 build.
Background info: there are currently ~2500 shards that still are transferring from my 'master' node to the other cluster node.
</comment><comment author="s1monw" created="2014-11-12T20:28:10Z" id="62787600">wait, which directory did you remove? did you remove `/home/elasticsearch/gotham/nodes/0/indices/somtoday-iridium-straat2-2014-08-17/0/index/` or just `/home/elasticsearch/gotham/nodes/0/indices/somtoday-iridium-straat1-2014-10-05/1/index/`?
</comment><comment author="thomasmarkus" created="2014-11-12T20:29:57Z" id="62787852">@s1monw  I removed all the shards of that specific index, because I had a similar exception for each of those shards.
</comment><comment author="s1monw" created="2014-11-12T20:31:28Z" id="62788063">ok so what I'd recommend you to do is shutdown the node, wait until replicas allocated on other nodes, wipe the data on the "broken" node and restart. The second exception appears like you wiped the index of a live shard...not really a corruption
</comment><comment author="thomasmarkus" created="2014-11-12T20:40:26Z" id="62789374">Sadly in this case that is not an option, because this is a simple 2-node setup. If I understand the issue correctly it appears that my 'master' node that holds some unreplicated data is probably corrupted.
</comment><comment author="s1monw" created="2014-11-12T20:45:17Z" id="62790093">ok so lemme get this straight - you cluster is in state `RED` it has unallocated primary shards? If not you can recovery from this you can for instance switch the replicas to `0` this should give you the ability to move all your shards to one node clean up the other, bring it back and then flip it up to `1`. I am not sure what you did exactly and which shards you wiped. When you said you have one shard that has these problems I didn't expect you to remove a bunch of them since the exception showed it's a replica. BTW is this a production env since you are coming from a beta release?
</comment><comment author="thomasmarkus" created="2014-11-12T20:53:21Z" id="62791337">I indeed have unallocated primary shards that are corrupted. Therefore recovery through other replica's isn't an option in this scenario. I've wiped all the shards that ES (either on the master node or the replica-node) that were logged as corrupted (initially 5 in total all in the same index), that data is gone as it wasn't replicated which is OK, because it _could_ have been caused by switching from beta1 -&gt; final.

The setup I'm describing is a dev env that we are using to stress-test ES before putting it in production. It's just 2 nodes with about 200G of indexed data.

(as you may have noticed, I'm relatively new to ES so please forgive my unintentional vagueness)
</comment><comment author="s1monw" created="2014-11-12T21:02:36Z" id="62792811">&gt; (as you may have noticed, I'm relatively new to ES so please forgive my unintentional vagueness)

no worries... 

&gt;  that data is gone as it wasn't replicated which is OK, because it could have been caused by switching from beta1 -&gt; final.

can you tell me how you upgraded from beta1 to final? like what was your procedure?
</comment><comment author="thomasmarkus" created="2014-11-12T21:06:35Z" id="62793440">1) stop one node in the cluster
2) upgrade the RPM
3) start the node
- repeat for the 2nd node in the cluster.

(note: it is by no means confirmed that the issue is caused by moving from beta1 to final)
</comment><comment author="thomasmarkus" created="2014-11-12T21:51:00Z" id="62801336">I probably just uncovered the root cause to all of the above issues: disk space problems. More specifically related to XFS behavior when growing the filesystem without a remount. This can cause it to disallow the creation of new files, but still report lots of 'free' disk space. This led to problems allocating new files (hence the type of errors I've been reporting).
</comment><comment author="s1monw" created="2014-11-12T22:01:11Z" id="62802888">oh man thanks for bringing this up!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ElasticSearch needs faster, smarter recovery from downtime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8456</link><project id="" key="" /><description>Hi,

I've been using ES for about a month now on a 6-node cluster.  There are a few issues ES exhibits when coming up from a failed-node or failed-cluster scenario.  These are pretty major, IMHO, because they result in longer downtime than is absolutely necessary.

Please forgive me if this is a duplicate issue.  I'm dealing with ES recovery as I write this, so time is at a premium.

One thing I'm noticing is that ES begins relocating shards as soon as the cluster is started up.  This is bad, because it results in unnecessary delays getting the cluster to the yellow, then green state.  If there are any unassigned or uninitialized shards, or the cluster status is anything other than green, shard relocation should be delayed until everything else is done, all shards are initialized and running, and the cluster state is green.

An even bigger issue, and you may want to split this into two issues, is that sometimes ES leaves shards unassigned.  It just doesn't assign them.  Even if you have to start up another thread to look for unassigned shards and assign them, under no circumstances should ES ever leave a shard unassigned, period.  Doing this forces the cluster state to red, and forces the admin to manually assign those shards.  This is something ES should take care of automatically.  So...NEVER leave a shard in the unassigned state.

Other than these two bugs in how ES recovers from a downed node or a complete cluster restart, ES has been fabulous for us.  Certainly a night-and-day improvement over Riak.  Keep up the awesome work!

Oh...we're running the latest release, downloaded just yesterday, elasticsearch-1.4.0-1.noarch, RPM version, under Amazon Linux (RHEL), x86-64 architecture, on java version 1.7.0_71 (OpenJDK).
</description><key id="48533990">8456</key><summary>ElasticSearch needs faster, smarter recovery from downtime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roncemer</reporter><labels /><created>2014-11-12T16:56:22Z</created><updated>2014-11-12T19:36:37Z</updated><resolved>2014-11-12T19:36:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T19:36:37Z" id="62779460">Hi @roncemer 

&gt; One thing I'm noticing is that ES begins relocating shards as soon as the cluster is started up. This is bad, because it results in unnecessary delays getting the cluster to the yellow, then green state. If there are any unassigned or uninitialized shards, or the cluster status is anything other than green, shard relocation should be delayed until everything else is done, all shards are initialized and running, and the cluster state is green.

The cluster doesn't know how many nodes you are planning on starting, unless you tell it.  You should look at this section of the docs, which will help you to configure these settings:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html#recover-after

If you plan on doing a rolling restart, then you should disable shard allocation as described here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#rolling-upgrades

Also, we're trying to implement another feature which will greatly speed up recovery: https://github.com/elasticsearch/elasticsearch/issues/6069

&gt; An even bigger issue, and you may want to split this into two issues, is that sometimes ES leaves shards unassigned. It just doesn't assign them. Even if you have to start up another thread to look for unassigned shards and assign them, under no circumstances should ES ever leave a shard unassigned, period. Doing this forces the cluster state to red, and forces the admin to manually assign those shards. This is something ES should take care of automatically. So...NEVER leave a shard in the unassigned state.

There are good reasons for leaving shards unassigned, eg you have no disk space, or the only copy of your shard is corrupt (but you may have another copy on an unstarted node or in a snapshot somewhere).  If we were to say "just go ahead and assign empty shards regardless" then you will suffer data loss which you could have avoided.

That said, the logs should always tell you **why** a shard is unassigned, which will help you to figure out what to do about it.  1.4.0 isn't noisy enough about nearly full disks, we are working on improving that in 1.4.1 (https://github.com/elasticsearch/elasticsearch/pull/8382).  If we're missing logging other reasons, open issues about them so that we can fix those too.

&gt; Other than these two bugs in how ES recovers from a downed node or a complete cluster restart, ES has been fabulous for us. Certainly a night-and-day improvement over Riak. Keep up the awesome work!

Glad to hear it is working out :)  thanks for letting us know.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Expand logging documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8455</link><project id="" key="" /><description>Updated log4j link so it doesn't point to log4j 2.0 but version 1.2. Clarified which formats are supported and briefly explained what loggers and appenders are, plus added a link to the log4j docs.

Closes #5305
</description><key id="48531897">8455</key><summary>[DOCS] Expand logging documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>docs</label></labels><created>2014-11-12T16:39:39Z</created><updated>2014-11-13T10:10:08Z</updated><resolved>2014-11-13T10:10:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T19:22:20Z" id="62777041">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Expand logging documentation</comment></comments></commit></commits></item><item><title>Change nested agg to execute in doc id order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8454</link><project id="" key="" /><description>By executing in docid order the child level filters don't require random access bitset any more and can use normal docid set iterators. Also the child filters don't need the bitset cache anymore and can rely on the normal filter cache.

Note: PR is against 1.x branch
</description><key id="48527393">8454</key><summary>Change nested agg to execute in doc id order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-12T16:04:55Z</created><updated>2015-06-07T17:49:40Z</updated><resolved>2014-11-14T20:41:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-14T14:04:42Z" id="63068253">@martijnvg I like the change and just left one minor comment about a potential simplification.
</comment><comment author="martijnvg" created="2014-11-14T16:00:38Z" id="63085409">@jpountz I updated the PR with your simplification and it looks much better now!
</comment><comment author="jpountz" created="2014-11-14T16:01:37Z" id="63085582">LGTM, thanks @martijnvg !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/NestedTests.java</file></files><comments><comment>Nested aggregator: Fix handling of multiple buckets being emitted for the same parent doc id.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file></files><comments><comment>Aggregations: Changed child filters to not require a random access based bitset in `nested` agg.</comment><comment>Also the nested agg now requires docs to be consumed / scored in order.</comment></comments></commit></commits></item><item><title>Remove the "all-trusting" TrustManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8453</link><project id="" key="" /><description>Closes #8444
</description><key id="48520689">8453</key><summary>Remove the "all-trusting" TrustManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">ankon</reporter><labels><label>:Plugins</label><label>review</label></labels><created>2014-11-12T15:12:47Z</created><updated>2015-10-08T12:59:25Z</updated><resolved>2015-10-08T12:59:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T16:28:34Z" id="62746600">Hi @ankon 

This PR would break things for people who are using self signed certs etc.  Why not make the trusting mode a command line opt-in option?
</comment><comment author="ankon" created="2014-11-12T16:53:00Z" id="62750998">As mentioned in https://github.com/elasticsearch/elasticsearch/issues/8444#issuecomment-62698524: Self-signed certificates may be a valid reason where the trust manager needs to be modified.

Some reasoning behind removing it completely from my end:
- I think it would be better to force the user in those cases to be explicit and actually provide the certificates. That's a bigger change though, so I opted for the manager first and then wait if some people actually need insecurity :)
- I'm not sure about the current user base whether self-signed are actually used: this would mean downloading plugins from a custom URL, and it would be as easy as providing the binary blob directly. No idea how to verify that assumption from my side, would be good to know if this actually happens in practice.
- Conditionalizing the "all-trusting" trust manager under a command line option would be possible, but I'd really have this off by default then. This would still be a breaking change, as people would need to update the calls to `bin/plugin` in scripts etc.

What do you think?
</comment><comment author="dakrone" created="2014-11-12T23:08:57Z" id="62812579">&gt; Why not make the trusting mode a command line opt-in option?

I think it would be _much_ better to add an `--insecure` option and make security opt-out, rather than opting in to security. Blindly trusting all sources for plugin installs is a bad idea.
</comment><comment author="clintongormley" created="2014-11-13T12:33:44Z" id="62883847">@dakrone we're saying the same thing - "trusting" mode is insecure, but you came up with a good name for the option :)

@ankon if you add the `--insecure` option, then I'd be happy to accept this PR.  (Also, we'll need you to sign the CLA please: http://www.elasticsearch.org/contributor-agreement/)

thanks
</comment><comment author="dakrone" created="2014-11-13T12:34:35Z" id="62883937">@clintongormley hah, you're right  we are, much better to make it explicit with the "insecure" terminology so it's not confusing :)
</comment><comment author="ankon" created="2014-11-13T15:08:43Z" id="62903465">Alright, there we go: Added `--insecure` command line option, and moved the logic into `#main()`

While there I also fixed the `#getAcceptedIssuers()` return value: it should be "non-null but possibly empty" as per https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/X509TrustManager.html#getAcceptedIssuers()

I didn't add a short form of the option, if you think there should be one I'd pick `-k`, based on the same option in `curl(1)` :)

Note that having the option handled by `#main()` means that programmatic use of the plugin manager can install a trust manager prior to using the plugin manager in just the same way.

Some notes that came up while testing with a self-signed certificate:
1. Java is strict when it comes to names, so the hostname of the URL used for downloading must at least be acceptable for the certificate. This could be easily fixed by adding a "hostname verifier" that similarly accepts any name. Didn't do that, as the current version has the same "problem". 
2. Related to 1: Unless the certificate mentions an IP address as "subject alternative name", using an IP address won't work either. Fix for that would be the same hostname verifier.
</comment><comment author="clintongormley" created="2014-11-13T15:39:48Z" id="62909751">Awesome. Thanks @ankon. note: we still need the CLA to be signed.

@tlrx please could you have a look at this?
</comment><comment author="dadoonet" created="2014-11-13T15:48:51Z" id="62912009">Just a note. It would mean as well that we need to rebase this one https://github.com/elasticsearch/elasticsearch/pull/7339 after and review it again. So I'd prefer to have first #7339 merged in and then have another PR or rebase this one.

@clintongormley WDYT?
</comment><comment author="ankon" created="2014-11-13T15:49:26Z" id="62912110">@clintongormley my CLA signature is part of the CCLA for "Collaborne B.V." ... at least it should be :)
</comment><comment author="clintongormley" created="2014-11-13T15:54:35Z" id="62913404">@ankon ah ok - I'll have to check manually.  We can only check the personal CLAs automatically.  Note: worth signing the personal CLA instead, otherwise we're going to ask you the same thing every time :)

@dadoonet #7339 has been open for 3 months and hasn't been touched in almost 4 weeks.  You'll have to rebase it when you are ready to work on it again.
</comment><comment author="tlrx" created="2014-11-14T09:08:14Z" id="63027318">@ankon Thanks for this PR! I made two minor comments.

I like the --insecure option, but now I'm wondering if we also need to set up a all-trusting hostname verifier (see HttpsURLConnection.setDefaultHostnameVerifier()) because I think it's often used together when working with self-signed certificates.

We could have something like:
--trust-all-certificates (or --disable-certificates-verification): installs the all-trusting trust manager
--trust-all-hostnames (or --disable-hostnames-verification): installs the all-trusting hostname verifier
--insecure: installs both

@dadoonet @clintongormley I think we can merge this PR as soon as it's updated. It should be trivial to integrate it in #7339 later
</comment><comment author="dadoonet" created="2014-11-14T09:20:15Z" id="63028488">@tlrx works for me.

BTW I think this should be documented in our docs and this doc update should be part of the PR. @ankon could you add this as well in plugins documentation?
</comment><comment author="ankon" created="2014-11-14T09:25:32Z" id="63029749">@tlrx I can add the verifier, sure. I'm not sure about having separate options for that, IMHO it just makes the installation of plugins more complicated, and I'm still somewhat assuming that users are reasonably sane: either they know the system is "misconfigured", and don't want to bother with the details as they already decided on the misconfiguration being "ok" for them, or they fix the system (and then need to address both hostname/SAN as well as certificate chains anyways).

@dadoonet I added a section in plugins.asciidoc, any other place where this could should be mentioned?
</comment><comment author="dadoonet" created="2014-11-14T09:27:05Z" id="63030117">@ankon I don't know what happened with my browser. I did not see this file at first and now it's there! Apologies...
</comment><comment author="ankon" created="2014-11-14T09:28:48Z" id="63030504">I'm still learning how to best do the "squash and rebase" part of the contributing guidelines, so at the moment I just do it when updating the patch. If it is okay to squash only at the end that's fine with me (makes reading the diffs also easier I think).
</comment><comment author="tlrx" created="2014-11-14T09:32:30Z" id="63031455">@ankon Let's go for --insecure only and ok to squash at the end.
</comment><comment author="ankon" created="2014-11-14T15:38:30Z" id="63081527">Alright, done in 08dfbc86d10e0d41661a6c9b3fc5cfb098791ac1 ... One question: is there any reasonable idea for testing that?
</comment><comment author="ankon" created="2015-02-10T10:40:24Z" id="73678766">Anything I should be doing to get this merged?
</comment><comment author="tlrx" created="2015-02-10T10:44:12Z" id="73679267">@ankon I'm working on the PluginManager right now.

I'll merge this PR asap. Nothing to do on your side :)
</comment><comment author="ankon" created="2015-10-08T12:59:25Z" id="146534270">Looks like @rmuir fixed it quite a while ago in 13636dcfef66d68f61dc4ad76d9bde4f77d54694
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices API: Fix GET index API always running all features</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8452</link><project id="" key="" /><description>Previous to this change all features (_alias,_mapping,_settings,_warmer) are run regardless of which features are actually requested. This change fixes the request object to resolve this bug.

This change is for 1.4 and a.x branches see https://github.com/elasticsearch/elasticsearch/pull/8392 for equivalent change in master
</description><key id="48515284">8452</key><summary>Indices API: Fix GET index API always running all features</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label></labels><created>2014-11-12T14:25:58Z</created><updated>2015-03-19T16:41:11Z</updated><resolved>2014-11-13T13:57:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-13T13:58:23Z" id="62893342">merged into 1.x and 1.4, for master changes see #8392
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Client nodes don't join cluster back post OOM exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8451</link><project id="" key="" /><description>In our setup we are always seeing when client nodes throw OOM, it is booted out of cluster. It is never able to join cluster back post OOM exception.

We saw the following from logs

java.lang.OutOfMemoryError: Java heap space
    at java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:57)
    at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
    at sun.nio.cs.StreamEncoder.&lt;init&gt;(StreamEncoder.java:195)
    at sun.nio.cs.StreamEncoder.&lt;init&gt;(StreamEncoder.java:175)
    at sun.nio.cs.StreamEncoder.forOutputStreamWriter(StreamEncoder.java:68)
    at java.io.OutputStreamWriter.&lt;init&gt;(OutputStreamWriter.java:133)
    at java.io.PrintStream.&lt;init&gt;(PrintStream.java:111)
    at java.io.PrintStream.&lt;init&gt;(PrintStream.java:175)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:384)
    at sun.net.www.http.HttpClient.openServer(HttpClient.java:473)
    at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:203)
    at sun.net.www.http.HttpClient.New(HttpClient.java:290)
    at sun.net.www.http.HttpClient.New(HttpClient.java:306)
    at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:995)
    at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:931)
    at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:849)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:325)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:182)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:248)
    at org.elasticsearch.marvel.agent.exporter.ESExporter.exportNodeStats(ESExporter.java:130)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:349)
    at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:236)
    at java.lang.Thread.run(Thread.java:724)

...............
[2014-11-04 09:34:50,844][DEBUG][action.search.type       ] [ES-NODE-Q02] [3312] Failed to execute fetch phase
org.elasticsearch.transport.NodeDisconnectedException: [ES-NODE-D02][inet[/10.0.0.8:9300]][search/phase/fetch/id] disconnected
...

[2014-11-04 09:41:35,957][DEBUG][action.admin.indices.alias.get] [ES-NODE-Q02] connection exception while trying to forward request to master node [[ES-NODE-M01][K13SBxlOQ06-axx0_UL1bg][es-node-m01][inet[/10.0.0.10:9300]]{updateDomain=0, tag=masternode, data=false, faultDomain=0, master=true}], scheduling a retry. Error: [org.elasticsearch.transport.NodeDisconnectedException: [ES-NODE-M01][inet[/10.0.0.10:9300]][indices/get/aliases] disconnected]

Post OOM exception client node is never able to join cluster back. We needed to restart es service on client node to get this node join back to cluster.

Is it a known issue? Is it expected behavior?
</description><key id="48509307">8451</key><summary>Client nodes don't join cluster back post OOM exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">satishmallik</reporter><labels><label>feedback_needed</label></labels><created>2014-11-12T13:21:39Z</created><updated>2015-02-28T04:50:54Z</updated><resolved>2015-02-28T04:50:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T14:22:21Z" id="62724158">Hi @satishmallik 

After an OOM, the JVM is in an undefined state, and you should restart it.  There is no point in continuing after that.  The more important question that you need to answer is: why are you getting OOMs in the client? 

It looks like you are running the Marvel exported inside your node client, is that correct?
</comment><comment author="clintongormley" created="2014-11-12T14:31:38Z" id="62725586">@satishmallik what else are you doing on this client node? how much heap does it have? I'd love to figure out what is using up all of the memory.

Could you send the output of these requests please:

```
curl localhost:9200/_nodes &gt; nodes_info.json
curl localhost:9200/_nodes/stats &gt; nodes_stats.json
```
</comment><comment author="satishmallik" created="2014-11-12T14:53:12Z" id="62729564">Hi clinton,
We have 9 node production cluster and 3 node marvel cluster. Out of 9 nodes 3 are master, 3 are data and 3 are client nodes. We hit OOM on one of client nodes. We have allocated 2G of RAM for JVM on a 3.5G RAM client machine.

I am not able to attach json file here. Can you share me any location where I can upload it,
</comment><comment author="clintongormley" created="2014-11-12T16:26:39Z" id="62746278">Hi @satishmallik 

You can send them to me at clinton dot gormley at elasticsearch dot com
</comment><comment author="satishmallik" created="2014-11-12T17:24:56Z" id="62756515">I sent it to your email id. Please have a look,
</comment><comment author="satishmallik" created="2014-11-14T05:11:37Z" id="63011808">Marvel exporter is running on each of 9 nodes including client on which we saw OOM.
</comment><comment author="clintongormley" created="2014-11-14T09:20:44Z" id="63028595">Hi @satishmallik 

I looked at your node info/stats.  Couple of things to note:
- You are using the Azul JVM, which is not supported
- You have swap enabled, which can cause slow GCs.  Windows does not support mlockall, so disabling swap is the only way to go here
- Your merges are being heavily throttled, which means that merges are not keeping up with your indexing.  Have you set the merge throttling to some custom value? If not, try allowing higher throughput and see if your disks can keep up. If not, then you need faster disks or more data nodes.

And that's about all I could see.  I didn't see anything unusual in the clients.  Are you sending massive bulk requests through the clients? Or returning massive result sets?  

I'd suggest doing a heap dump of a client that has full memory, to see what is taking up all of the heap.
</comment><comment author="satishmallik" created="2014-11-14T14:49:20Z" id="63074099">Hi Clinton,
Thanks for looking into this,

Few questions
1) Which JVMs are officially supported by Elasticsearch? With Azule we are seeing GC running quite frequently and CPU/Memory usage remains quite high

2) I have to experiment with disabling SWAP on ES nodes,

3) We had earlier deployed ES 1.1.3 which was throttling merges. We already have moved to ES 1.3.4, but we didn't delete our earlier index which was created by ES1.1.3. We are using Azure Blob store for our index storage. 

Our client node is currently acting both as ingestion and query nodes. We are doing heavy bulk indexing and yes we do return 1000 results. We are doing multi term aggregation. But we are planning to move away from multi term aggregation model.

In general how can we figure that merge is throttling or, not during indexing? Apart our segments count and deleted documents (25%) are quite high. We are not doing any optimization on index. Any suggestions on controlling segment count and deleted documents? 

How can we control CPU/Mem usage on data nodes? Data nodes are dual core nodes having 14G of RAM.
</comment><comment author="clintongormley" created="2014-11-14T15:03:16Z" id="63076157">&gt; 1) Which JVMs are officially supported by Elasticsearch? With Azule we are seeing GC running quite frequently and CPU/Memory usage remains quite high

OpenJDK and Oracle's Java

&gt; 3) We had earlier deployed ES 1.1.3 which was throttling merges. We already have moved to ES 1.3.4, but we didn't delete our earlier index which was created by ES1.1.3. We are using Azure Blob store for our index storage.

Not sure what throughput you'll get on Azure, but it is worth trying to play with the throttling.  By default we throttle to 20MB/s, but you can increase this value (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-store.html#store-throttling).  You want to keep an eye on your search latencies, because search will fight with indexing for I/O. That's why we throttle by default. 

&gt; Our client node is currently acting both as ingestion and query nodes. We are doing heavy bulk indexing and yes we do return 1000 results. We are doing multi term aggregation. But we are planning to move away from multi term aggregation model.

Thousand**S** of results? or 1,000 total? If thousands, then you should look at using the scroll API instead.

&gt; In general how can we figure that merge is throttling or, not during indexing? Apart our segments count and deleted documents (25%) are quite high. We are not doing any optimization on index. Any suggestions on controlling segment count and deleted documents?

You can measure how much throttling is happening by looking at:

```
GET /_nodes/stats/indices/store,indexing
```

You shouldn't need to optimize. As long as there is enough I/O available, then the background merge process will handle things just fine.

&gt; How can we control CPU/Mem usage on data nodes? Data nodes are dual core nodes having 14G of RAM.

Use them less? :)  You need to figure out where the memory is being used, etc.  There's no simple answer to this.  I suggest reading the Administration section of the Definitive Guide: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/administration.html
</comment><comment author="clintongormley" created="2015-02-28T04:50:54Z" id="76510556">No more info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>randomizedtesting jar versions mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8450</link><project id="" key="" /><description>Following documentation section here (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/using-elasticsearch-test-classes.html) and running a simple test case results in the following exception:

```
java.lang.NoSuchMethodError: com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(Lcom/carrotsearch/randomizedtesting/Randomness;Ljava/util/concurrent/Callable;)Ljava/lang/Object;
    at __randomizedtesting.SeedInfo.seed([AEC9FCD0DC412E76:F00D66F28CD590AC]:0)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.buildWithPrivateContext(ElasticsearchIntegrationTest.java:543)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.buildAndPutCluster(ElasticsearchIntegrationTest.java:559)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.beforeInternal(ElasticsearchIntegrationTest.java:263)
    at org.elasticsearch.test.ElasticsearchIntegrationTest.before(ElasticsearchIntegrationTest.java:1742)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
```

The problem comes from the fact that Maven will retrieve Lucene 4.10.2's dependency on randomizedtesting artifact version 2.1.6. On the other hand ES 1.4.0 uses API from randomizedtesting that seems to be available in later versions: https://github.com/elasticsearch/elasticsearch/blob/v1.4.0/pom.xml#L62

First reported here: https://groups.google.com/forum/#!topic/elasticsearch/WD9S1NdxeBs
</description><key id="48500806">8450</key><summary>randomizedtesting jar versions mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">astefan</reporter><labels><label>docs</label><label>test</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-12T11:36:13Z</created><updated>2014-12-19T14:45:27Z</updated><resolved>2014-12-06T17:14:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-12-06T17:19:35Z" id="65905954">Thank you for reporting. Fixed.
</comment><comment author="ghiron" created="2014-12-19T14:44:26Z" id="67646151">still have the same kind of exception with 1.4.3-SNAPSHOT (same code works fine with 1.4.2 and same dependencies)

``` xml```
&lt;junit.version&gt;4.10&lt;/junit.version&gt;
&lt;elasticsearch.version&gt;1.4.3-SNAPSHOT&lt;/elasticsearch.version&gt;
&lt;randomizedtesting-runner.version&gt;2.1.11&lt;/randomizedtesting-runner.version&gt;
&lt;lucene.version&gt;4.10.2&lt;/lucene.version&gt;
```

``` java```
java.lang.NoSuchMethodError: org.apache.lucene.store.MMapDirectory.getDirectory()Ljava/nio/file/Path;
at __randomizedtesting.SeedInfo.seed([73D877A9EA36C0AA]:0)
    at org.apache.lucene.store.StoreUtils.toString(StoreUtils.java:36)
    at org.apache.lucene.store.StoreUtils.toString(StoreUtils.java:44)
    at org.apache.lucene.store.RateLimitedFSDirectory.toString(RateLimitedFSDirectory.java:70)
    at java.lang.String.valueOf(String.java:2981)
    at java.lang.StringBuilder.append(StringBuilder.java:131)
    at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:603)
    at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:487)
    at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:113)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:819)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:781)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1445)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:710)
    at org.elasticsearch.index.gateway.none.NoneIndexShardGateway.recover(NoneIndexShardGateway.java:72)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: add randomizedtesting-runner to testing-framework.asciidoc</comment></comments></commit></commits></item><item><title>Search: Track most commonly used filters and cache them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8449</link><project id="" key="" /><description>Whether or not a filter should be cached is currently up to the filter parsers. For instance, we cache `terms` filters because they are costly to build and produce a result that is already cacheable, and do not cache geo-distance filters because they are usually expected to be applied to the position of a user of the system that keeps changing all the time.

We could make the default better by tracking usage statistics of the filters and decide to cache based on the cost of the filter and how often the filter is used.
</description><key id="48498993">8449</key><summary>Search: Track most commonly used filters and cache them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2014-11-12T11:13:59Z</created><updated>2014-12-29T13:42:25Z</updated><resolved>2014-12-18T15:01:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T13:19:34Z" id="62716236">+1
</comment><comment author="mikemccand" created="2014-11-12T14:35:09Z" id="62726099">+1
</comment><comment author="nik9000" created="2014-11-12T14:41:09Z" id="62727094">+1
- Probably should be able to opt in/out of the behavior.
- What about filters who's cache key is ridiculous without being overridden?
- Might be cool to turn _cache support _cache as a number between 0 and 1 that the user can set closed to 1 to signal a priori knowledge about whether the filter is more likely to be reused.  true would map to 1 (always cache) and false to 0 (never cache).  I dunno.  Maybe over complicated.
</comment><comment author="rjernst" created="2014-11-14T21:33:51Z" id="63132476">&gt; Might be cool to turn _cache support _cache as a number between 0 and 1 that the user can set closed to 1 to signal a priori knowledge about whether the filter is more likely to be reused. true would map to 1 (always cache) and false to 0 (never cache). I dunno. Maybe over complicated.

That sounds too complicated.  I think it would be better to have the default based on stats, and then have a special value, say "always" that would bypass the stats and ensure the filter is cached.  Although I'm not sure how much we need the ability to override the stats based approach, since anything that should be cached should quickly rise statistically.
</comment><comment author="clintongormley" created="2014-11-17T11:28:54Z" id="63291871">@rjernst @nik9000 I think the `_cache` setting should accept three parameters only: `auto` (default), `true`, `false`.  
</comment><comment author="rjernst" created="2014-11-19T19:37:38Z" id="63699609">@clintongormley I like that, defaults are the right place for this.  And I'm ok with the "auto" terminology.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/filter/AutoFilterCachingPolicy.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/FilterCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/none/NoneFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/support/CacheKeyFilter.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/support/FilterCacheValue.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/weighted/WeightedFilterCache.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/query/AndFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/BoolFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FQueryFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NotFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/OrFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/PrefixFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>src/main/java/org/elasticsearch/index/shard/IndexShard.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/terms/IndicesTermsFilterCache.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterCachingTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/search/scriptfilter/ScriptFilterSearchTests.java</file><file>src/test/java/org/elasticsearch/test/CompositeTestCluster.java</file><file>src/test/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/TestCluster.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Filter cache: add a `_cache: auto` option and make it the default.</comment></comments></commit></commits></item><item><title>elasticsearch 1.4.0 hanging with rivers starting up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8448</link><project id="" key="" /><description>The elasticsearch-river-mongodb river tries to query elasticsearch whether a specific index already exists in its `#start()` method. If there are many of those rivers elasticsearch's startup essentially hangs: all threads in the 'listener' pool are used by the `RiversService#createRiver()` method, and the listener for the 'indexExists' call never gets executed.

A full thread is available at https://gist.github.com/ankon/e000198100ce3d52cb3b#file-thread-dump-txt, and the issue is filed also in the elasticsearch-river-mongodb project: https://github.com/richardwilly98/elasticsearch-river-mongodb/issues/406

Some questions:
1. What is the proper behavior of a river inside `#start()`, in particular: is it allowed to use the elasticsearch client? Should it do anything special when invoking methods on it?
2. Where is the best place for documenting these requirements?
</description><key id="48493886">8448</key><summary>elasticsearch 1.4.0 hanging with rivers starting up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ankon</reporter><labels /><created>2014-11-12T10:16:43Z</created><updated>2015-08-15T17:41:35Z</updated><resolved>2015-08-15T17:41:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ankon" created="2014-11-12T11:51:12Z" id="62707287">On the river side: moving the code around to avoid running requests to ES inside `#start()` fixes the issue, so this issue is now purely a "needs documentation" issue :)
</comment><comment author="ankon" created="2014-11-14T16:22:28Z" id="63088983">Update the description a bit: this problem isn't specific to the mongodb river, but would happen with any type of river that has a non-trivial `#start()` method invoking elasticsearch listeners again.
</comment><comment author="jprante" created="2014-11-22T15:35:50Z" id="64083949">Hint: the river start() method must not be blocked by a river implementation. It should return quickly so Elasticsearch can continue with startup of the node.
There is no guarantee that Elasticsearch had recovered all the indexes successfully and entered a yellow/green state when river start() method is called. I recommend spawning a new thread and wait there for yellow/green state before using client actions.
</comment><comment author="emptyflask" created="2014-12-05T17:19:26Z" id="65822491">The [CouchDB river plugin is having the same problem](https://github.com/elasticsearch/elasticsearch-river-couchdb/issues/78), so if anyone here is able to help with that project, it would be greatly appreciated.
</comment><comment author="clintongormley" created="2015-08-15T17:41:35Z" id="131405633">Given that rivers have been removed in master, I'm going to close this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Corrected syntax error in search curl cmd</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8447</link><project id="" key="" /><description /><key id="48467134">8447</key><summary>[DOCS] Corrected syntax error in search curl cmd</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">taddeimania</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-12T03:05:53Z</created><updated>2014-11-12T16:24:15Z</updated><resolved>2014-11-12T16:24:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T12:02:49Z" id="62708430">Hi @taddeimania 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="taddeimania" created="2014-11-12T14:45:54Z" id="62728290">Signed.  Thanks!
</comment><comment author="clintongormley" created="2014-11-12T16:23:53Z" id="62745789">thanks @taddeimania - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Corrected syntax error in search curl cmd</comment></comments></commit></commits></item><item><title>[DOCS] Updated threadpool documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8446</link><project id="" key="" /><description>Docs: Threadpools and their limits clarified

Made a few minor changes to this to elaborate on a higher level what the threadpools actually are and also to clarify what the default values are for the various pools that are mentioned.
</description><key id="48445320">8446</key><summary>[DOCS] Updated threadpool documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2014-11-11T22:38:15Z</created><updated>2014-11-12T11:39:53Z</updated><resolved>2014-11-12T11:39:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-11T22:39:38Z" id="62635223">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Updated threadpool documentation to elaborate/clarify what the pools are for and their values</comment></comments></commit></commits></item><item><title>Tribe nodes should return explicit exceptions when attempting to write to the master (eg create index)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8445</link><project id="" key="" /><description>To repro:
1. Create two clusters and a tribe node that is configured to join both.
2. Attempt to use PUT to create a new index.

I'm seeing the following...in this test I first added an index manually to t2, then to t1.  Then I tried to add an index to the tribe node.  This is a serious problem for a Kibana dashboard configured to point to the tribe node in order to span clusters.

[2014-11-11 16:04:16,527][INFO ][tribe                    ] [Matsu'o Tsurayaba] [t2] adding index [testindex]
[2014-11-11 16:04:27,617][DEBUG][action.admin.indices.create] [Matsu'o Tsurayaba] observer: timeout notification from cluster service. timeout setting [30s], time since start [30s]
[2014-11-11 16:04:39,616][INFO ][tribe                    ] [Matsu'o Tsurayaba] [t1] adding index [testindex2]
[2014-11-11 16:04:42,288][DEBUG][action.admin.indices.create] [Matsu'o Tsurayaba] observer: timeout notification from cluster service. timeout setting [30s], time since start [30s]
[2014-11-11 16:04:48,639][DEBUG][action.admin.indices.create] [Matsu'o Tsurayaba] no known master node, scheduling a retry
</description><key id="48442664">8445</key><summary>Tribe nodes should return explicit exceptions when attempting to write to the master (eg create index)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Tribe Node</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-11T22:12:38Z</created><updated>2015-12-07T17:57:26Z</updated><resolved>2015-12-07T17:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T11:57:36Z" id="62707899">Hi @seang-es 

The tribe node cannot be used to create indices, as there is no way of determining which cluster the index should belong to.  This is a documented limitation: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-tribe.html#modules-tribe

However, I think the error message should be more explicit in the tribe node. 
</comment><comment author="seang-es" created="2014-11-12T16:00:12Z" id="62741456">Ok, I was seeing the same error after I manually created the index on one of the nodes.  It still appeared as if some writes were failing.  I will test further.
</comment><comment author="javanna" created="2015-12-07T17:57:26Z" id="162607897">I think this can be closed in favour of #13290.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PluginManager installs "all-trusting" TrustManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8444</link><project id="" key="" /><description>While reading some elasticsearch code I noticed that the PluginManager installs an "all-trusting" TrustManager as default for SSL connections.

I think this a very bad idea from a security point of view: any attacker able to set up a MITM proxy can now just wait for elasticsearch plugin installation requests, and return malicious code that elasticsearch will execute on the node the plugin is installed.

Given that installing plugins can be quite a common occurrence when using continuous deployment processes, and predicting which plugins could be installed on a site (mapper-attachments, cloud-aws for example) this looks "too easy".

Securing elasticsearch by running it as non-root user might be helpful, but nonetheless this vector can give access to machines previously unavailable.
</description><key id="48437278">8444</key><summary>PluginManager installs "all-trusting" TrustManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels><label>:Plugins</label><label>adoptme</label></labels><created>2014-11-11T21:27:06Z</created><updated>2015-10-08T12:58:14Z</updated><resolved>2015-10-08T12:58:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-12T03:40:44Z" id="62665908">+1

That behavior should be optional and off by default.

For what its worth we're too paranoid to use the plugin manager at all and
just jam the files where they go. Its pretty painless.
On Nov 11, 2014 4:27 PM, "Andreas Kohn" notifications@github.com wrote:

&gt; While reading some elasticsearch code I noticed that the PluginManager
&gt; installs an "all-trusting" TrustManager as default for SSL connections.
&gt; 
&gt; I think this a very bad idea from a security point of view: any attacker
&gt; able to set up a MITM proxy can now just wait for elasticsearch plugin
&gt; installation requests, and return malicious code that elasticsearch will
&gt; execute on the node the plugin is installed.
&gt; 
&gt; Given that installing plugins can be quite a common occurrence when using
&gt; continuous deployment processes, and predicting which plugins could be
&gt; installed on a site (mapper-attachments, cloud-aws for example) this looks
&gt; "too easy".
&gt; 
&gt; Securing elasticsearch by running it as non-root user might be helpful,
&gt; but nonetheless this vector can give access to machines previously
&gt; unavailable.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8444.
</comment><comment author="ankon" created="2014-11-12T10:31:10Z" id="62698524">Honestly I'm not sure why the behavior should even be there. If you have a server providing plugins with a b0rked SSL certificate then you can as well just not use SSL.

There might be cases where you use self-signed certificates, but even then it would be better for you to explicitly allow those certificates. At the moment this would require some changes to the script and PluginManager, so one could come up with a patch that loads a keystore from a well-known place (`config` directory of ES?).
</comment><comment author="ankon" created="2014-11-12T15:13:59Z" id="62733006">PR #8453 removes the trustmanager without replacement. This obviously may impact compatibility (previously we would download from sources we now wouldn't anymore), but as mentioned above: we _shouldn't_ have done that.
</comment><comment author="ankon" created="2015-10-08T12:58:14Z" id="146533438">Looks like @rmuir fixed it quite a while ago: 13636dcfef66d68f61dc4ad76d9bde4f77d54694

Thanks! :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Added another elasticsearch-river-kafka plugin to the documentation (use...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8443</link><project id="" key="" /><description>...s latest version of Kafka, EL Bulk API, and supports concurrent requests)
</description><key id="48432290">8443</key><summary>Docs: Added another elasticsearch-river-kafka plugin to the documentation (use...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mariamhakobyan</reporter><labels /><created>2014-11-11T20:41:44Z</created><updated>2014-11-17T11:32:53Z</updated><resolved>2014-11-12T10:03:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-12T10:03:10Z" id="62695240">Hi @mariamhakobyan 

Thanks for the PR.  However I find it confusing that it has exactly the same name as the other Kafka river.  Please could you find a different name and resubmit.  Also, it is polite to add newer plugins after existing plugins.

thanks
</comment><comment author="mariamhakobyan" created="2014-11-12T12:04:25Z" id="62708560">Hi @clintongormley,

Thanks for a quick feedback, I agree I should have added it in the end, the reason of not doing that was to keep the plugins referring to the same technology (e.g. kafka) together, so the users find it easier.

What would you suggest to put as a different name? As far as I see the rivers follow the same simple naming conventions - [elasticsearch-river-{technology}], so I am not sure what to change the name to.
If you have a good suggestion, I would be happy to follow that.

Cheers,
Mariam 
</comment><comment author="clintongormley" created="2014-11-12T14:14:11Z" id="62723042">Hi @mariamhakobyan 

&gt; Thanks for a quick feedback, I agree I should have added it in the end, the reason of not doing that was to keep the plugins referring to the same technology (e.g. kafka) together, so the users find it easier.

I'd just put it after the other kafka one, rather than right at the end.

&gt; What would you suggest to put as a different name? As far as I see the rivers follow the same simple naming conventions - [elasticsearch-river-{technology}], so I am not sure what to change the name to.
&gt; If you have a good suggestion, I would be happy to follow that.

What is different about your river? Why did it need to be a separate river?  Or it just that the other one is unmaintained?  If that's the case, perhaps kafka2?
</comment><comment author="mariamhakobyan" created="2014-11-12T14:51:36Z" id="62729307">The main differences are the following:
- uses Kafka 0.8.1.1 (latest version)
- uses Elasticsearch 1.3.2 (will be updated to the latest version soon)
- makes use of Bulk API (to have better performance)
- can be executed in concurrent requests
- abstracts the consuming mechanism (can consume Kafka messages from multiple brokers and partitions)

These are the main reasons of having a separate river plugin. Of course it's also maintained periodically.
</comment><comment author="mariamhakobyan-zanox" created="2014-11-14T09:51:48Z" id="63034551">Hi @clintongormley,

Any updates on this pull request? How would you suggest to proceed?

Regards,
Mariam
</comment><comment author="clintongormley" created="2014-11-14T10:06:52Z" id="63036262">@mariamhakobyan As I said, choose another name for your plugin, even if it is kafka2.  I gave you some suggestions, but it is your plugin - you get to choose whichever name makes more sense to you.
</comment><comment author="mariamhakobyan" created="2014-11-15T14:59:01Z" id="63174964">@clintongormley, would that be fine if I change the name of the plugin in your documentation only, so it shows up as elasticsearch-river-kafka2, but the underlying link remains the same meaning https://github.com/mariamhakobyan/elasticsearch-river-kafka (Anyway, there are multiple river plugins for kafka in github with the same artifact name already).
</comment><comment author="clintongormley" created="2014-11-17T11:32:53Z" id="63292264">@mariamhakobyan I think it is confusing, and changing a repo name is easy, but as you like
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set bloom default to false even when Directory doesn't have a codecService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8442</link><project id="" key="" /><description>We disabled loading of bloom filters by default a while ago, but in the code we still default to true if the directory is not a StoreDirectory or doesn't have a codecService.  I don't know if this ever happens, but in a standalone test (using my own FSDirectory) I suddenly saw bloom filters being loaded so I want to defensively fix this.
</description><key id="48430483">8442</key><summary>Set bloom default to false even when Directory doesn't have a codecService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-11T20:25:00Z</created><updated>2015-06-06T19:21:05Z</updated><resolved>2014-11-11T22:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-11T21:40:36Z" id="62626783">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Running ES on Windows - spaces issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8441</link><project id="" key="" /><description>```
C:\Program Files\Java\elasticsearch-1.4.0\bin&gt;elasticsearch.bat
'C:\Program' is not recognized as an internal or external command,
operable program or batch file.
Error: Could not find or load main class org.elasticsearch.bootstrap.Elasticsearch
```

I think line 6 on bin\elasticsearch.bat should be

```
CALL "%~dp0elasticsearch.in.bat"
```

instead of

```
CALL %~dp0elasticsearch.in.bat
```

am I right?
</description><key id="48409594">8441</key><summary>Running ES on Windows - spaces issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">francescopeloi</reporter><labels /><created>2014-11-11T17:22:20Z</created><updated>2014-11-13T14:23:24Z</updated><resolved>2014-11-13T14:23:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2014-11-11T18:08:27Z" id="62590005">@francescopeloi Yea, the path should definitely be surrounded by quotes to handle spaces properly.  I was able to reproduce this and can confirm that adding quotes fixes the issue.
</comment><comment author="uschindler" created="2014-11-12T22:02:47Z" id="62803155">Hi,
In Elasticsearch 1.3.x this worked correctly. Since 1.4.0 the windows startup script fails if the path to ES contains spaces. Which happens with my user name and also if I install in "C:\Program Files. So the startup script is more or less unuseable with common configurations (installation in "Program Files"). Also the PATH of JAVA_HOME is in most cases containing spaces, because Oracle installs the JDK in "C:\Program Files".
I will check the patch on my local installation!
</comment><comment author="uschindler" created="2014-11-12T22:04:44Z" id="62803432">Hi, I checked, the quotes around the CALL fixes the issue! Thanks - I fixed my local startup script :-)

+1 to commit!
</comment><comment author="gmarz" created="2014-11-13T14:23:24Z" id="62896672">Closed by #8428
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change default eager loading behaviour for nested fields and parent/child in bitset cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8440</link><project id="" key="" /><description>Only eagerly load bitsets for parent nested object fields. Child nested fields and parent type filters should not get in the bitset filter cache.

This PR moves parent/child away from the bitset filter cache and let it rely on the regular filter cache. P/c doesn't rely on bitset based filters.

PR is made against 1.x
</description><key id="48402504">8440</key><summary>Change default eager loading behaviour for nested fields and parent/child in bitset cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cache</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-11T16:22:53Z</created><updated>2015-06-07T18:03:24Z</updated><resolved>2014-11-14T18:58:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-14T14:54:14Z" id="63074826">Updated the PR to also not eagerly load parent type filters and move p/c away from bitset filter cache.
</comment><comment author="jpountz" created="2014-11-14T14:59:35Z" id="63075642">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java</file><file>src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/TopChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Core: In the bitset cache only eagerly load bitsets for parent nested object fields.</comment></comments></commit></commits></item><item><title>Remove `index.percolator.allow_unmapped_fields` setting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8439</link><project id="" key="" /><description>There should be no way to opt out from strict field resolution for percolator query parsing.
</description><key id="48396982">8439</key><summary>Remove `index.percolator.allow_unmapped_fields` setting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-11T15:37:10Z</created><updated>2015-06-08T14:15:26Z</updated><resolved>2014-11-14T21:46:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-14T15:01:33Z" id="63075931">LGTM, just left a request to add a comment
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file></files><comments><comment>Percolator: Remove `index.percolator.allow_unmapped_fields` setting.</comment></comments></commit></commits></item><item><title>Parse Failure: NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8438</link><project id="" key="" /><description>Query is pretty big and I have no idea what's causing it. I don't think that's relevant anyway, IMHO the code should check for nulls and throw a more specific exception, otherwise we clients are left in the dark about what we're doing wrong.
Here's the stack trace:

```
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:660)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:516)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.search.aggregations.bucket.filter.FilterParser.parse(FilterParser.java:42)
        at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:130)
        at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:120)
        at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:77)
        at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:644)
        ... 9 more
```
</description><key id="48385632">8438</key><summary>Parse Failure: NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">mausch</reporter><labels><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-11-11T13:48:05Z</created><updated>2014-11-21T12:29:54Z</updated><resolved>2014-11-20T13:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-11T14:03:40Z" id="62550443">&gt; Query is pretty big and I have no idea what's causing it. I don't think that's relevant anyway, IMHO the code should check for nulls and throw a more specific exception, otherwise we clients are left in the dark about what we're doing wrong.

Agreed - can you tell us what you were doing when you saw this error? What does the query/agg look like?
</comment><comment author="mausch" created="2014-11-11T14:42:49Z" id="62555795">It was mostly simple terms aggs but also one global agg with a filter. IIRC the query itself was that same filter being applied to the global agg.
</comment><comment author="clintongormley" created="2014-11-11T14:50:54Z" id="62557017">@mausch could you provide some examples? Clearly we're not seeing this in our testing, so it would be helpful to see what you are doing different.
</comment><comment author="mausch" created="2014-11-11T16:03:02Z" id="62568674">Sorry, I don't have that query any more. But I still insist that it shouldn't matter. I'd run some random-testing tool (e.g. https://github.com/pholser/junit-quickcheck ) on `SearchService.parseSource` and `FilterParser.parse`. No input ever should cause a NPE in my opinion, otherwise it's a bug.
</comment><comment author="clintongormley" created="2014-11-11T16:22:38Z" id="62571996">@mausch I agree with you - we don't wilfully leave NPEs in our code. The trick is finding out what is causing it. 
</comment><comment author="vaibhavkulkar" created="2014-11-11T16:33:20Z" id="62573900">I am new to this community and I think I can fix this one to start with. Has some one already fixed it ?
</comment><comment author="mausch" created="2014-11-11T16:45:35Z" id="62576016">@clintongormley Indeed, especially with complex code. Sorry I lost the original query. The only thing I can advice is to start applying randomized testing (if you haven't already, I don't know much about the ES codebase). It's great to find this kind of bugs. IIRC Lucene has started using it a couple of years ago.

@vaibhavkulkar unless there's already some randomized testing in place in the project this isn't trivial to fix.
</comment><comment author="s1monw" created="2014-11-11T16:55:21Z" id="62577788">can you tell what filters you are using?
</comment><comment author="mausch" created="2014-11-11T17:22:37Z" id="62582373">IIRC it was an And filter with a single terms filter within.
</comment><comment author="clintongormley" created="2014-11-11T18:11:13Z" id="62590423">@mausch also, what version of Elasticsearch did you see this on?
</comment><comment author="martijnvg" created="2014-11-12T11:19:11Z" id="62704004">The filters aggregation should in at FilterParser.java line 41 check for null. A null value is a valid return value for a filter parser (and for example the `and` filter does this).
</comment><comment author="mausch" created="2014-11-12T12:48:08Z" id="62712964">@clintongormley version 1.3.4
</comment><comment author="wcong" created="2014-11-17T02:41:51Z" id="63253810">I found the same NPE.
the version is 1.3.2.
In my case,it happened when i used FilterAggregation ,but sent no filter.
In java client,it looks like this.

``` java
        FilterBuilder fb = new AndFilterBuilder();
        FilterAggregationBuilder fab = AggregationBuilders
                .filter("filter")
                .filter(fb)
                .subAggregation(AggregationBuilders.terms("brandId").field("brand_id").size(0))
                .subAggregation(AggregationBuilders.terms("allAttr").field("all_attr").size(0));
```

In http query it looks like this.
![elasticsearch npe](https://cloud.githubusercontent.com/assets/1748926/5064466/7748cd16-6e44-11e4-9a16-239cd4538f30.png)
</comment><comment author="markharwood" created="2014-11-17T10:48:26Z" id="63287403">@martijnvg I can see where you say check for null but do you believe the correct response is then to throw a parse exception with a friendly message or to process the query using something like a MatchNoDocsFilter?
</comment><comment author="martijnvg" created="2014-11-17T10:51:22Z" id="63287752">@markharwood I think if a filter parser returns null then the filters agg should interpret this as nothing is going to match, so `MatchNoDocsFilter` should be used.
</comment><comment author="s1monw" created="2014-11-17T10:57:13Z" id="63288400">We had the same issue with bool query and what we do there is: 

``` Java
  if (clauses.isEmpty()) {
    return new MatchAllDocsQuery();
  }
```

I think bool filter should be consistent here - I am not sure which one we should change to be honest...
</comment><comment author="markharwood" created="2014-11-17T11:24:14Z" id="63291403">&gt; I am not sure which one we should change to be honest...

Tough one. My gut feel on the empty filters array would be that it would mean "match none" but if we have a precedent for this on the query side which is "match all" then I guess we have these ugly options:
1) API Inconsistency (query=match all, filter = match none)
2) Consistently ALL (but empty filters matching all might feel weird)
3) Consistently NONE ( introduces a backwards compatibility issue here for change in query behaviour)
4) Parser error (avoids any ambiguity by forcing users to declare logic more explicitly)
</comment><comment author="clintongormley" created="2014-11-17T11:52:19Z" id="63294284">The `bool` query change was made to make it easier to build such queries in tools like Kibana, ie they have a placeholder to which clauses can be added, but which doesn't fail if none are added.

If we don't specify a `query` (eg in `filtered` query) then it defaults to `match_all`.  I think the same logic works for filters here.  We're not specifying leaf filters against any fields, there is just no leaf filter.  I'd lean towards matching ALL.
</comment><comment author="s1monw" created="2014-11-17T14:27:18Z" id="63311522">+1 for what clint said. I thinks the deal here is `no restrictions` == `match all` so I think translating it to match all it the right thing. I personally think it's not perfectly fine for a filter parser to return `null` btw. I think the actual query/filter parser should handle these cases and return  valid filter instead. @martijnvg what do you think?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FilterTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FiltersTests.java</file></files><comments><comment>Parser throws NullPointerException when Filter aggregation clause is empty.</comment><comment>Added Junit test that recreates the error and fixed FilterParser to default to using a MatchAllDocsFilter if the requested filter clause is left empty.</comment><comment>Also added fix and test for the Filters (with an "s") aggregation.</comment></comments></commit></commits></item><item><title>Docs: note about confusing disk threshold settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8437</link><project id="" key="" /><description /><key id="48384098">8437</key><summary>Docs: note about confusing disk threshold settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">itsadok</reporter><labels><label>docs</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-11T13:30:37Z</created><updated>2014-11-13T05:34:59Z</updated><resolved>2014-11-12T08:24:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-11T18:04:16Z" id="62589336">@dakrone can you check this please?
</comment><comment author="itsadok" created="2014-11-12T05:33:47Z" id="62672640">Relevant code is [here](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java#L210-L214) and also on lines 307,316, 339 and 349.
</comment><comment author="dakrone" created="2014-11-12T08:24:58Z" id="62684547">Merged to 1.3, 1.4, 1.x, and master branches.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce shard level locks to prevent concurrent shard modifications</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8436</link><project id="" key="" /><description>Today it's possible that the data directory for a single shard is used by more than on
IndexShard-&gt;Store instances. While one shard is already closed but has a concurrent recovery
running and a new shard is creating it's engine files can conflict and data can potentially
be lost. We also remove shards data without checking if there are still users of the files
or if files are still open which can cause pending writes / flushes or the delete operation
to fail. If the latter is the case the index might be treated as a dangeling index and is brought
back to life at a later point in time.

This commit introduces a shard level lock that prevents modifications to the shard data
while it's still in use. Locks are created per shard and maintained in NodeEnvironment.java.
In contrast to most java concurrency primitives those locks are not reentrant.

This commit also adds infrastructure that checks if all shard locks are released after tests.
</description><key id="48381754">8436</key><summary>Introduce shard level locks to prevent concurrent shard modifications</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-11T13:01:43Z</created><updated>2015-06-06T19:21:15Z</updated><resolved>2014-11-16T13:32:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-11T15:21:42Z" id="62561749">pushed more changes @dakrone 
</comment><comment author="kimchy" created="2014-11-11T19:00:27Z" id="62597924">@s1monw quick question regarding the path of the lock file, does it make sense to have it encapsulated in the shard location itself (similar to write.lock in Lucene)? I think its nicer since then there is a single place that holds the shard data.

Update: I think I get why its a different directory, the ability to delete while holding the lock, if thats the case, then it makes sense. On crappy internet, will try and complete the review later tonight....
</comment><comment author="s1monw" created="2014-11-11T21:44:49Z" id="62627443">&gt; Update: I think I get why its a different directory, the ability to delete while holding the lock, if thats the case, then it makes sense. On crappy internet, will try and complete the review later tonight....

correct that is the only reason.
</comment><comment author="bleskes" created="2014-11-11T22:46:13Z" id="62636092">I like the change. Left comments here and there. 

I might have missed, but where do we clean up lock files of indices /shards that were deleted/relocated and everything is fine? I assume we can only delete a file if someone else doesn't have a lock. O.w. this doesn't work.

One more thing I was wondering is whether InternalNode.close should try and acquire all shard locks (and wait for that) to ensure all background tasks complete gracefully. Once acquired, I think we can also delete the lock files to clean up the directory? (ala the current code for startup)
</comment><comment author="s1monw" created="2014-11-12T08:53:15Z" id="62687220">&gt; I might have missed, but where do we clean up lock files of indices /shards that were deleted/relocated and everything is fine? I assume we can only delete a file if someone else doesn't have a lock. O.w. this doesn't work.

we do only do that when we create the `NodeEnvironment` ie on node-startup. Unfortunately there is a race so we can't really delete these files while we are running the node.

&gt; One more thing I was wondering is whether InternalNode.close should try and acquire all shard locks (and wait for that) to ensure all background tasks complete gracefully. Once acquired, I think we can also delete the lock files to clean up the directory? (ala the current code for startup)

can you explain the benefit of this?
</comment><comment author="dakrone" created="2014-11-12T11:01:04Z" id="62702006">Is it possible to add unit testing of `NodeEnvironment`'s `ShardLock` locking and unlocking methods, so threaded locking and unlocking can be tested for race conditions?
</comment><comment author="s1monw" created="2014-11-12T14:36:51Z" id="62726344">@dakrone @bleskes I pushed a lot of changes along your comments - I didn't fix the unittests yet for the locks.... will do tonight or tomorrow.
</comment><comment author="s1monw" created="2014-11-12T22:02:23Z" id="62803087">@dakrone added some unittests for NodeEnviroment too 
</comment><comment author="bleskes" created="2014-11-13T14:50:38Z" id="62900681">I love the new listeners and the fact that we moved to in memory locks. left some comments about left over clean ups due to the many iterations.
</comment><comment author="dakrone" created="2014-11-13T15:55:43Z" id="62913762">Left lots of comments, @s1monw does it make sense to have `ShardLock` implement `java.util.concurrent.locks.Lock` so it can use the `.lock`, `.tryLock` and `.unlock` naming conventions?
</comment><comment author="s1monw" created="2014-11-13T22:08:06Z" id="62975526">&gt; Left lots of comments, @s1monw does it make sense to have ShardLock implement java.util.concurrent.locks.Lock so it can use the .lock, .tryLock and .unlock naming conventions?

thanks for the comments - I think I addressed them all... Regarding `Lock` I don't really want anybody to lock this outside of `NodeEnironment` it really has the semantics of a file based lock not like  monitor. not sure how this would look like to be honest so no I don't think we should make it more complicated.
</comment><comment author="s1monw" created="2014-11-15T14:05:09Z" id="63173419">I fixed all comments - I think it's ready.. refactorings can happen later this really goes out of date quickly...
</comment><comment author="bleskes" created="2014-11-15T15:34:28Z" id="63176105">Left two comments about log levels. LGTM other wise. Agreed we need to push it and continue from here.
</comment><comment author="dakrone" created="2014-11-15T19:58:06Z" id="63187087">LGTM!
</comment><comment author="kimchy" created="2014-11-15T20:41:21Z" id="63191295">LGTM, I love the move to in memory locks.
</comment><comment author="s1monw" created="2014-11-16T15:39:46Z" id="63223507">thanks everybody for the intense &amp; time consuming reviews it's a pretty low level change and lots of places are involved.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor term analysis for simple_query_string prefix queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8435</link><project id="" key="" /><description>Also fixes an issue where the token stream could be non-closed if an
exception occured.

I used `org.apache.lucene.util.QueryBuilder` as a template for refactoring this.
</description><key id="48377167">8435</key><summary>Refactor term analysis for simple_query_string prefix queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Search</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-11T12:02:18Z</created><updated>2015-03-19T09:46:40Z</updated><resolved>2014-11-11T12:12:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-11T12:04:46Z" id="62537964">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use JSONPath syntax for aggregation paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8434</link><project id="" key="" /><description>@jpountz  pointed out that we could use JSONPath to define paths to properties in aggregations. This would be a new syntax and would replace the old `a&gt;b.c` style syntax. We could add a config option to enable the older syntax for bwc. The new syntax would allow for complex aggregation paths for use in reducers to, for example, get properties from a particular bucket in an aggregation, or retrieve all the values of the property `c` in all the buckets even if c is three levels down the aggregation tree. It would also give a standard way to access the root of the agg tree (with '$').

for reference see: http://goessner.net/articles/JsonPath/
</description><key id="48376632">8434</key><summary>Use JSONPath syntax for aggregation paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2014-11-11T11:54:49Z</created><updated>2015-08-12T08:29:22Z</updated><resolved>2015-05-05T14:18:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-05-05T14:18:55Z" id="99092333">After looking at JSONPath it's not 100% good fit for aggregation paths and there are other efforts looking at JSONPath-like functionality elsewhere in the product, so closing this for now
</comment><comment author="thomax" created="2015-08-11T21:18:59Z" id="130080796">&gt; After looking at JSONPath it's not 100% good fit for aggregation paths and there are other efforts looking at JSONPath-like functionality elsewhere in the product, so closing this for now

Have any of these other efforts given results? JSONPath or JSONPath-like functionality would be useful to me, so I'd be very interested to hear about this.
</comment><comment author="colings86" created="2015-08-12T08:29:21Z" id="130215622">So far just response filtering has been implemented (https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering) which uses a simplified syntax for referencing paths in JSON.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"ArithmeticException[/ by zero]" when parsing a "polygon" "geo_shape" query with one pair of coordinates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8433</link><project id="" key="" /><description>For a query like this one:

``` json
{
  "query": {
    "geo_shape": {
      "location": {
        "shape": {
          "type": "polygon",
          "coordinates": [[["4.901238","52.36936"]]]
        }
      }
    }
  }
}
```

An "ArithmeticException: / by zero" is returned enclosed in a SearchParseException:

```
Caused by: java.lang.ArithmeticException: / by zero
    at org.elasticsearch.common.geo.builders.ShapeBuilder$Edge.ring(ShapeBuilder.java:460)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.createEdges(BasePolygonBuilder.java:442)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.coordinates(BasePolygonBuilder.java:129)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.buildGeometry(BasePolygonBuilder.java:170)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.build(BasePolygonBuilder.java:146)
    at org.elasticsearch.index.query.GeoShapeQueryParser.getArgs(GeoShapeQueryParser.java:173)
    at org.elasticsearch.index.query.GeoShapeQueryParser.parse(GeoShapeQueryParser.java:155)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:252)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)
```

If I increase the number of coordinates for the polygon in the query to two, a more acceptable and meaningful exception is being thrown: "IllegalArgumentException[Invalid number of points in LinearRing (found 2 - must be 0 or &gt;= 4)]". Probably, the same exception should be thrown and returned in case of just one set of coordinates.
</description><key id="48363895">8433</key><summary>"ArithmeticException[/ by zero]" when parsing a "polygon" "geo_shape" query with one pair of coordinates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">astefan</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0</label></labels><created>2014-11-11T09:26:39Z</created><updated>2014-11-19T14:25:32Z</updated><resolved>2014-11-19T14:25:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchGeoAssertions.java</file></files><comments><comment>[GEO] Add LinearRing and LineString validity checks as defined by http://geojson.org/geojson-spec.html to ensure valid polygons are specified at parse time.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchGeoAssertions.java</file></files><comments><comment>[GEO] Fix for ArithmeticException[/ by zero] when parsing a "polygon" with one pair of coordinates</comment></comments></commit></commits></item><item><title>NPE enclosed in a SearchParseException for a "point" type "geo_shape" filter or query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8432</link><project id="" key="" /><description>Given the following mapping and data:

``` json
PUT /my_index
{
  "mappings": {
    "landmark": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type": "geo_shape"
        }
      }
    }
  }
}

POST /my_index/landmark/1
{"name":"Dam Square, Amsterdam","location":{"type":"polygon","coordinates":[[[4.89218,52.37356],[4.89205,52.37276],[4.89301,52.37274],[4.89392,52.3725],[4.89431,52.37287],[4.89331,52.37346],[4.89305,52.37326],[4.89218,52.37356]]]}}
```

The following query, either a "geo_shape" filter or query, fails with a SearchParseException enclosing a NullPointerException:

``` json
GET /my_index/landmark/_search
{
  "query": {
    "geo_shape": {
      "location": {
        "shape": {
          "type": "point",
          "coordinates": [["4.901238","52.36936"]]
        }
      }
    }
  }
}
```

And the exception in logs:

```
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.geo.builders.PointBuilder.build(PointBuilder.java:59)
    at org.elasticsearch.common.geo.builders.PointBuilder.build(PointBuilder.java:29)
    at org.elasticsearch.index.query.GeoShapeQueryParser.getArgs(GeoShapeQueryParser.java:173)
    at org.elasticsearch.index.query.GeoShapeFilterParser.parse(GeoShapeFilterParser.java:178)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:315)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:296)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:252)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)
```
</description><key id="48362593">8432</key><summary>NPE enclosed in a SearchParseException for a "point" type "geo_shape" filter or query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">astefan</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.2</label></labels><created>2014-11-11T09:11:53Z</created><updated>2014-12-10T22:45:48Z</updated><resolved>2014-12-10T22:45:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="astefan" created="2014-11-11T09:15:03Z" id="62520700">Another NPE happens for a query type "polygon":

``` json
{
  "query": {
    "geo_shape": {
      "location": {
        "shape": {
          "type": "polygon",
          "coordinates": ["4.901238","52.36936"]
        }
      }
    }
  }
}
```

and the stacktrace:

```
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.geo.builders.ShapeBuilder$GeoShapeType.parsePolygon(ShapeBuilder.java:644)
    at org.elasticsearch.common.geo.builders.ShapeBuilder$GeoShapeType.parse(ShapeBuilder.java:597)
    at org.elasticsearch.common.geo.builders.ShapeBuilder.parse(ShapeBuilder.java:235)
    at org.elasticsearch.index.query.GeoShapeQueryParser.parse(GeoShapeQueryParser.java:86)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:252)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)
```
</comment><comment author="s1monw" created="2014-12-01T21:50:03Z" id="65140964">@nknize can you take a look at this?
</comment><comment author="nknize" created="2014-12-04T19:33:07Z" id="65688708">The following is probably obvious (and likely the intent of the test):

The GeoJSON in the query request is invalid.  e.g., 

``` java
"shape": {
          "type": "point",
          "coordinates": [["4.901238","52.36936"]]
        }
```

Should either be:

``` java
"type": "multipoint",
...
```

or:

``` java
...
"coordinates": ["4.901238","52.36936"]}
```

Same with the polygon - it should be an array of LinearRings (closed LineStrings).   

I'll add a more useful parse error message instead of the "give up" NPE.  

FYI, most of the shape parsing logic can use better error handling so I'm certain this isn't the last NPE parse error.  I'll be adding better error handling as I go so add the issues as you find them.
</comment><comment author="s1monw" created="2014-12-04T19:35:21Z" id="65689057">if ripping it out and redo is a potential better solution I am all for it @nknize especially if we can add unittests to it as well :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/test/java/org/elasticsearch/common/geo/GeoJSONShapeParserTests.java</file></files><comments><comment>[GEO] Fix for NPE enclosed in SearchParseException for a "geo_shape" filter or query</comment></comments></commit></commits></item><item><title>index.query.parse.allow_unmapped_fields setting does not seem to allow unmapped fields in alias filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8431</link><project id="" key="" /><description>Should I be able to create a alias filter on an unmapped field if I have index.query.parse.allow_unmapped_fields set to true?

Here is a gist of the problem:
https://gist.github.com/loren/cde1aded2f86950f2cc4
</description><key id="48353622">8431</key><summary>index.query.parse.allow_unmapped_fields setting does not seem to allow unmapped fields in alias filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">loren</reporter><labels><label>bug</label><label>docs</label></labels><created>2014-11-11T06:48:48Z</created><updated>2014-11-20T14:59:02Z</updated><resolved>2014-11-20T14:59:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-11T09:33:58Z" id="62522652">Hi @loren 

I can replicate this behaviour - it is indeed a bug.  That said, strict parsing is saving you from filters that don't actually work.

@martijnvg please could you take a look at this?
</comment><comment author="martijnvg" created="2014-11-11T15:07:23Z" id="62559467">@loren The `index.query.parse.allow_unmapped_fields` doesn't have an effect on alias filters. When alias filters are being created it forcefully overwrites the `index.query.parse.allow_unmapped_fields` setting to false.

The documentation is wrong about how the `index.query.parse.allow_unmapped_fields` affects parsing for when adding an alias or a percolator query. I'll change the docs.  
</comment><comment author="loren" created="2014-11-12T14:41:31Z" id="62727175">Well, I actually don't even want to set `index.query.parse.allow_unmapped_fields` to true. I got here because I ran into a problem when I upgraded to 1.4.0. I have a `logstash` template and I have a `filtered_logstash` template that just contains a filtered alias. When the next day's `logstash` events started rolling in, all the writes failed with the `Strict field resolution and no field mapping can be found for the field with name [clientip]` error.

That confused me because the `clientip` field is explicitly mapped in the `logstash` template. I thought maybe setting `index.query.parse.allow_unmapped_fields` to true would solve my problem. It did not.

It turns out @javanna 's workaround in #6110 is what helped me out. In my `filtered_logstash` template, I put in an arbitrary `mappings` section along with the filtered alias:

```
  "mappings": {
      "workaround" : {
        "properties": {
          "confusing" : {
            "type" : "string"
          }  
        } 
      }
    }
```

Even though I have no type `workaround` and no property `confusing`, putting this mapping in place in the `filtered_logstash` template let me write a document of type `search` with property `clientip`.
</comment><comment author="clintongormley" created="2014-11-12T16:20:19Z" id="62745107">@loren From your explanation I think I understand, but am not quite sure. This issue is closed. Could I ask you to open a new issue giving a full recreation?  I'd like to understand what is going on here, to see if there is something we need to do better.

thanks
</comment><comment author="martijnvg" created="2014-11-20T14:59:02Z" id="63820149">Documentation has been updated, to be clear on what `index.query.parse.allow_unmapped_fields` setting does.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add static index based backcompat tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8430</link><project id="" key="" /><description>I'm looking for initial feedback on this approach to solve #8065. I refactored in such a way that the upgrade tests can use the same framework.  I am currently still trying to get 0.90 indexes to build correctly (explicit flush doesn't seem to work?), but I think the basics are all here.

closes #8065
</description><key id="48333299">8430</key><summary>Add static index based backcompat tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rjernst</reporter><labels /><created>2014-11-11T00:32:51Z</created><updated>2015-01-21T23:22:20Z</updated><resolved>2014-11-20T22:58:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-11T13:19:37Z" id="62545324">this is a great start! I left some minor comments other than that LGTM
</comment><comment author="s1monw" created="2014-11-12T20:48:51Z" id="62790624">@rjernst I left some more comments
</comment><comment author="rjernst" created="2014-11-18T02:40:31Z" id="63414074">@s1monw The only remaining issue now is the `testTooOldIndexes()`, which just has 0.20.5 on master.  The `IndexFormatTooOldException` is hidden down inside the cluster, and so starting the node just times out on `ensureGreen()` in `loadIndex()`.  Can you think of a way this can be propagated, or at least checked from a test (that the index failed to load)?
</comment><comment author="rjernst" created="2014-11-19T23:27:17Z" id="63734411">Ok I removed the very old index testing, and spun off a separate issue to address that: #8562.

I'll push this shortly.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete all documents for a Range, based on Nested field - Not Working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8429</link><project id="" key="" /><description>Trying to delete documents of type "tweets" that are older than 30 days.
Running the query further below as a GET returns results as expected.
However, when I try and attempt a DELETE based on the same query, it returns with result:

```
{"ok":true,"found":false,"_index":"twitter","_type":"tweets","_id":"_search","_version":1}
```

Nothing is deleted.
Any suggestions on how to delete documents based on a nested field for a given range, would be really appreciated.

```
curl -XDELETE 'http://localhost:9200/twitter/tweets/_search' -d '{
    "query": {
        "nested": {
            "path": "tweet",
            "query": {

                    "range": {
                        "tweet.metadata.crawl_time" : {
                            "lte": "now-30d"
                        }
                    }
            }
        }
    }
}'
```
</description><key id="48322427">8429</key><summary>Delete all documents for a Range, based on Nested field - Not Working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jstoor</reporter><labels><label>feedback_needed</label></labels><created>2014-11-10T22:25:03Z</created><updated>2014-11-11T12:14:39Z</updated><resolved>2014-11-11T12:14:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-11T09:12:35Z" id="62520479">Hi @jstoor 

This works for me.  Are you sure you're using `DELETE ..../_query`, not `DELETE ..../_search`?  What version of Elasticsearch are you on?

```
DELETE twitter

PUT twitter
{
  "mappings": {
    "tweets": {
      "properties": {
        "tweet": {
          "type": "nested"
        }
      }
    }
  }
}

PUT /twitter/tweets/1
{
  "tweet": {
    "metadata": {
      "crawl_time": "2014-09-01"
    }
  }
}

GET /twitter/tweets/_search
{
  "query": {
    "nested": {
      "path": "tweet",
      "query": {
        "range": {
          "tweet.metadata.crawl_time": {
            "lte": "now-30d"
          }
        }
      }
    }
  }
}

DELETE /twitter/tweets/_query
{
  "query": {
    "nested": {
      "path": "tweet",
      "query": {
        "range": {
          "tweet.metadata.crawl_time": {
            "lte": "now-30d"
          }
        }
      }
    }
  }
}
```

No documents returned now:

```
GET /twitter/_search
```
</comment><comment author="jstoor" created="2014-11-11T10:33:42Z" id="62529165">@clintongormley thank you for responding.

I'm on "0.90.9" and we don't currently have permission to update to a newer version.
</comment><comment author="clintongormley" created="2014-11-11T12:14:39Z" id="62538854">Hi @jstoor 

I confirm that this doesn't work on 0.90.9, but we aren't doing any more releases in the 0.90 branch.  This is fixed in 1.x , so I'm going to close this ticket.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added quotes to allow spaces in installation path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8428</link><project id="" key="" /><description>Closes #8441
</description><key id="48321271">8428</key><summary>Added quotes to allow spaces in installation path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">vidarkongsli</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T22:13:33Z</created><updated>2015-06-08T00:10:56Z</updated><resolved>2014-11-13T14:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-11T09:06:48Z" id="62519943">Hi @vidarkongsli 

Thanks for the PR. Please could I ask you to sign the CLA so that we can merge it in?
http://www.elasticsearch.org/contributor-agreement/

@gmarz please could you review this?
</comment><comment author="vidarkongsli" created="2014-11-11T10:10:24Z" id="62526640">@clintongormley Jumped through the hoop: signed :)
</comment><comment author="gmarz" created="2014-11-11T18:18:59Z" id="62591647">LGTM.  Thanks @vidarkongsli :+1: 
</comment><comment author="s1monw" created="2014-11-12T22:05:27Z" id="62803528">@gmarz looks like you can pull this in you got policeman approval on #8441
</comment><comment author="uschindler" created="2014-11-12T22:06:56Z" id="62803752">LOL
</comment><comment author="s1monw" created="2014-11-12T22:07:50Z" id="62803884">@uschindler :+1: 
</comment><comment author="gmarz" created="2014-11-13T14:22:01Z" id="62896513">Merged.  Thanks again @vidarkongsli
</comment><comment author="s1monw" created="2014-11-21T09:47:55Z" id="63947482">@gmarz can you make sure this issue doesn't exist in our `1.3` branch?
</comment><comment author="uschindler" created="2014-11-21T09:59:22Z" id="63948875">@s1monw In 1.3.x this worked fine for me in the past, but maybe you have a similar problem in later 1.3 branches.
</comment><comment author="gmarz" created="2014-11-21T16:42:09Z" id="63998516">@s1monw Confirmed, this is not an issue in `1.3`.
</comment><comment author="s1monw" created="2014-11-21T19:32:27Z" id="64024998">thx @gmarz 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DNS round robin assisted discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8427</link><project id="" key="" /><description>If all nodes in a cluster are added to a round robin DNS record and the following changes are made:
- `discovery.zen.ping.unicast.hosts` has the DNS RR name and Zen tries connecting to all the IPs 
- clients initially connect to the cluster via the DNS RR name

Why? Modern HTTP clients will get a random node in the cluster, but if it fails they'll automatically try the rest until one works [1], when it can then discover the rest of the cluster in the normal way. Currently AFAIK to guarantee a client will always work on startup (ie. before it has a chance to discover all the other nodes in the cluster) and for unicast zen to always work, it has to be supplied with a _current_ list of at least half (?) of the master-eligible nodes.

This would make the cluster rely on DNS to operate, although I expect most people refer to their nodes via DNS anyway, so doesn't add any further dependency for people who'd want to take advantage of this.

[1] http://blog.engelke.com/2011/06/07/web-resilience-with-round-robin-dns/
</description><key id="48308326">8427</key><summary>DNS round robin assisted discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">jimmyjones2</reporter><labels><label>:Network</label><label>adoptme</label><label>docs</label></labels><created>2014-11-10T20:13:00Z</created><updated>2016-12-14T10:29:39Z</updated><resolved>2016-12-14T10:29:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ianwestcott" created="2015-09-15T00:37:54Z" id="140241830">I just attempted to build a cluster using round robin DNS, and was surprised to find that it doesn't seem to work. Am I missing something, or is this not a supported discovery method? If not, what is the current limitation on supporting round robin DNS discovery? 
</comment><comment author="JesperTerkelsen" created="2015-09-17T11:49:10Z" id="141052102">:+1: Would be really nice to have this.
</comment><comment author="clintongormley" created="2015-11-21T21:15:45Z" id="158681965">I _think_ this should now work with 2.0.  Want to try it out and let us know?
</comment><comment author="jimmyjones2" created="2015-11-26T20:52:28Z" id="159992934">TL;DR confirmed working with 2.1. Should this now be documented? I think for some use cases this is a great feature!

``` bash
sudo docker run -i -t elasticsearch
```

Now add a load of docker IPs as A records for es.mydomain, making sure one matches the container above. Check:

``` bash
nslookup es.mydomain
Server:     8.8.8.8
Address:    8.8.8.8#53

Non-authoritative answer:
Name:   es.mydomain
Address: 172.17.0.18
Name:   es.mydomain
Address: 172.17.0.16
Name:   es.mydomain
Address: 172.17.0.15
Name:   es.mydomain
Address: 172.17.0.19
Name:   es.mydomain
Address: 172.17.0.17
```

Start up another ES node:

``` bash
sudo docker run -i -t elasticsearch -Des.discovery.zen.ping.unicast.hosts="es.mydomain"
```

It'll give "No route to host" errors for all but one of the IPs associated with es.mydomain and join the other container in a cluster.
</comment><comment author="jwaldrip" created="2016-08-20T22:02:20Z" id="241226500">works like a charm
</comment><comment author="JarenGlover" created="2016-10-21T17:42:40Z" id="255432105">@jwaldrip @ianwestcott it sounds like the TLDR; i can now bootstrap a elasticsearch master cluster using DNS? (ex. consul DNS) 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Document using round-robin DNS for discovery (#21810)</comment></comments></commit></commits></item><item><title>Fix conflict when updating mapping with _all disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8426</link><project id="" key="" /><description>_all reports a conflict since #7377. However, it was not checked if _all
was actually configured in the updated mapping. Therefore whenever _all
was disabled a mapping could not be updated unless _all was again added to the
updated mapping.
Also, add enabled setting to mapping always whenever enabled was set explicitely.

closes #8423
</description><key id="48295745">8426</key><summary>Fix conflict when updating mapping with _all disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T18:17:56Z</created><updated>2015-07-08T14:09:46Z</updated><resolved>2014-11-20T12:47:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-10T19:17:14Z" id="62437943">LGTM.
</comment><comment author="clintongormley" created="2014-11-14T15:47:42Z" id="63083033">@brwe want to get this in?
</comment><comment author="rayward" created="2015-07-07T00:26:44Z" id="119034077">It seems that it's not possible to _disable_ the `_all` field after it was previously enabled:

`{"error":"MergeMappingException[Merge failed with failures {[mapper [_all] enabled is true now encountering false]}]","status":400}`

Is this intentional?
</comment><comment author="clintongormley" created="2015-07-08T14:09:46Z" id="119592259">Hi @rayward 

Yes it is intentional.  In fact from 2.0, the only time you'll be able to enable or disable the _all field is when you create the mapping.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file></files><comments><comment>[root mappers] fix conflict when updating mapping with _all disabled</comment></comments></commit></commits></item><item><title>Performance issues when large number of indices used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8425</link><project id="" key="" /><description>See this thread for more details

https://groups.google.com/forum/#!topic/elasticsearch/cJ2Y6-KQZus

Basically, when there is a large number of indices system performance degrades dramatically.  We reproduced this in a simple YCSB test case that connects to a remote system.  In my case I had 4200 indices created.  Looking at a heap dump, we found that there were 42,890 instances of IndexRoutingTable and 128,630 instances of IndexShardRoutingTable.
</description><key id="48288412">8425</key><summary>Performance issues when large number of indices used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnament</reporter><labels><label>discuss</label></labels><created>2014-11-10T17:11:03Z</created><updated>2015-08-26T15:25:29Z</updated><resolved>2015-08-26T15:25:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-10T17:49:45Z" id="62423918">Hi @johnament

Having read through that thread, it sounds like you are not following best practices.   I'm guessing you're also creating each index with the default of 5 primary shards per index, even for this tiny dataset?

Have a read of this chapter: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/scale.html

In fact, if you're not familiar with the refresh process either, it would be useful for you to read the book from cover to cover :)

As to why there are so many instances of indexRoutingTable and IndexShardRoutingTable, I'm not sure, and will ask somebody else to comment.
</comment><comment author="johnament" created="2014-11-10T18:11:11Z" id="62427382">@clintongormley I'm not sure what you're referring to.  Can you explain what you feel I'm not covering in a "best practice" way?
</comment><comment author="clintongormley" created="2014-11-10T19:27:09Z" id="62439487">@johnament Start by reading the chapter that I linked to - it'll give you more background as to why 4,000 indices for 25k docs is a bad idea, and alternatives that you can use.
</comment><comment author="jimmyjones2" created="2014-11-10T20:18:07Z" id="62447642">@clintongormley I was trying to find what is a reasonable number of indices a while ago and couldn't find any clear guidance. Could the book contain a a rule of thumb, like more than 100 is probably a bad idea (or whatever the rough number is)? I found most monitoring tools struggled for various reasons with a large number.
</comment><comment author="clintongormley" created="2014-11-10T20:21:36Z" id="62448217">@jimmyjones2 there isn't a concrete number because it depends very much on how you use them.  I started trying to provide some guidelines, but gave up because it just didn't do the subject justice.  

The best advice comes down to: measure your hardware and your use case, and extrapolate from there.

See http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/capacity-planning.html
</comment><comment author="jimmyjones2" created="2014-11-10T21:11:42Z" id="62455802">@clintongormley I know, but people are lazy... would it be helpful to say if you want more than 100 indices you should do some testing, otherwise you'll be fine? A bit like the 5 shards thing is good for most use cases, if you want more you should think about it and test?
</comment><comment author="johnament" created="2014-11-10T23:55:21Z" id="62478740">@clintongormley If I read http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/kagillion-shards.html I think you might be thinking that I'm trying this.

I'm not trying this.

What I'm trying is more akin to http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/user-based.html it's just that my initial use case has 10k users each having between 100 and 2000 documents in their store.  The reason 4200 came up was because that was as far as we could get before elasticsearch died.  Each index has fairly unique data in it, the types are generally unique per tenant (where tenant == user in the above scenario).  

What we're actually seeing is that when the number of tenants gets to even 24, performance degrades dramatically.

Based on the listed, would you then recommend maybe running a single shard per tenant?
</comment><comment author="clintongormley" created="2014-11-11T09:21:16Z" id="62521326">@johnament you are indeed trying to run a kagillion shards :)  (also, you didn't mention how many nodes this is on?)

Each shard has a cost - it is a lucene instance in its own right. Also, all of your 25k docs could fit easily into a single shard.  Depending on how big the docs are, how they are analyzed, and how you're searching, you could probably fit 10,000 times your current docs into a single shard quite happily.

So having 4,200 indices x 5 shards (plus replicas == 42,000 shards!) gives you about one document per primary shard!  You can see why that is an anti-pattern.

(also, that number explains the 42,890 instances of IndexRoutingTable, not sure why there are 3x the number of IndexShardRoutingTable instances)
</comment><comment author="clintongormley" created="2014-11-11T09:22:42Z" id="62521467">@jimmyjones2  re:

&gt;  I know, but people are lazy... would it be helpful to say if you want more than 100 indices you should do some testing, otherwise you'll be fine? A bit like the 5 shards thing is good for most use cases, if you want more you should think about it and test?

No, the best I can say is "it depends", and "you should test your use case yourself" :)

It really does depend. People do such different things with their data that there is no one-size-fits-all rule.
</comment><comment author="jpountz" created="2015-08-26T15:25:29Z" id="135062049">Closing: too many indices will cause various problems anyway.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rounded date ranges with `lte` should set `include_upper` to `false`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8424</link><project id="" key="" /><description>As discussed in https://github.com/elasticsearch/elasticsearch/issues/7203 the current behaviour of `lt`/`lte` with date rounding is inconsistent.  For instance, with the following docs indexed:

```
DELETE /t

POST /t/t/_bulk
{"index":{"_id":1}}
{"date":"2014/11/07 00:00:00"}
{"index":{"_id":2}}
{"date":"2014/11/07 01:00:00"}
{"index":{"_id":3}}
{"date":"2014/11/08 00:00:00"}
{"index":{"_id":4}}
{"date":"2014/11/08 01:00:00"}
{"index":{"_id":5}}
{"date":"2014/11/09 00:00:00"}
{"index":{"_id":6}}
{"date":"2014/11/09 01:00:00"}
```

This query with `lt`:

```
GET /_search?sort=date
{
  "query": {
    "range": {
      "date": {
        "lt": "2014/11/08||/d"
      }
    }
  }
}
```

correctly returns `2014/11/07 00:00:00` and `2014/11/07 01:00:00`, but this query with `lte`:

```
GET /_search?sort=date
{
  "query": {
    "range": {
      "date": {
        "lte": "2014/11/08||/d"
      }
    }
  }
}
```

incorrectly returns:
- `2014/11/07 00:00:00` 
- `2014/11/07 01:00:00`
- `2014/11/08 00:00:00` 
- `2014/11/08 01:00:00`
- `2014/11/09 00:00:00` 

It should not include that last document.  The `lte` parameter, when used with date rounding, should use `ceil()` to round the date up, as it does today, but `include_upper` should be set to `false`.
</description><key id="48273319">8424</key><summary>Rounded date ranges with `lte` should set `include_upper` to `false`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>low hanging fruit</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T15:05:21Z</created><updated>2014-11-21T17:30:37Z</updated><resolved>2014-11-21T17:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterDateRangeFormatTests.java</file></files><comments><comment>DateMath: Fix semantics of rounding with inclusive/exclusive ranges.</comment></comments></commit></commits></item><item><title>Mapping update fails if _all.enabled was set to false - MergeMappingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8423</link><project id="" key="" /><description>In ES 1.4.0 it is no longer possible to update the mapping if _all.enabled has been set to false and _all isn't present in the updating request

create index t:

```
http put localhost:9200/t/
```

create mapping:

```
http put localhost:9200/t/_mapping/default default:='{"dynamic": "strict", "_all": {"enabled": false}}'
```

update mapping:

```
http put localhost:9200/t/_mapping/default default:='{"dynamic": "true"}
```

Fails with:

```
 "error": "MergeMappingException[Merge failed with failures {[mapper [_all] enabled is false now encountering true]}]", 
```

If I include _all in the request it works as expected:

```
http put localhost:9200/t/_mapping/default default:='{"dynamic": "true", "_all": {"enabled": false}}'
{
     "acknowledged": true
}
```

In 1.3.5 that was possible. 
</description><key id="48268068">8423</key><summary>Mapping update fails if _all.enabled was set to false - MergeMappingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">mfussenegger</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T14:12:49Z</created><updated>2014-11-25T12:57:06Z</updated><resolved>2014-11-20T12:47:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-10T15:21:51Z" id="62398778">@mfussenegger thanks for reporting. i've confirmed this.

Also, updating the `_default_` mapping will **remove** the `_all` mapping:

```
DELETE _all 

PUT t

PUT /t/_mapping/_default_ 
{
  "dynamic": "strict",
  "_all": {
    "enabled": false
  }
}
```

Here, the `_all` mapping is present:

```
GET /_mapping

PUT /t/_mapping/_default_ 
{
  "dynamic": "true"
}
```

And now it is gone:

```
GET /_mapping
```
</comment><comment author="brwe" created="2014-11-10T18:19:31Z" id="62428643">This is a bug indeed, thanks for reporting. opened pr #8426
</comment><comment author="miccon" created="2014-11-24T17:25:13Z" id="64229717">There seems to be another issue with the `_all` field in combination with the `_default_` mapping:

I recreated the issue here:
https://gist.github.com/miccon/a4869fe04f9010015861#file-strictallmappingtest-java

Failing on 21de386
</comment><comment author="brwe" created="2014-11-25T12:07:40Z" id="64390465">@pkoenig10 thanks a lot for the test, this is incredibly helpful. It seems to me the issue is not in _all mapper but rather that the type is created when indexing the document even though the indexing fails. I will open a separate issue shortly.
</comment><comment author="miccon" created="2014-11-25T12:57:06Z" id="64395503">Thanks for the quick reply. Another thing I also noticed is that closing and reopening the index after the failed indexing makes the test pass. So it may be that the type gets somehow created but not persisted.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file></files><comments><comment>[root mappers] fix conflict when updating mapping with _all disabled</comment></comments></commit></commits></item><item><title>Add option for analyzing wildcard/prefix to simple_query_strinq</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8422</link><project id="" key="" /><description>The query_string query has an option for analyzing wildcard/prefix (#787) by a best effort approach.

This adds `analyze_wildcard` option also to simple_query_string.

The default is set to `false` so the existing behavior of simple_query_string is unchanged.
</description><key id="48260105">8422</key><summary>Add option for analyzing wildcard/prefix to simple_query_strinq</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jprante</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T12:38:12Z</created><updated>2015-06-07T16:55:34Z</updated><resolved>2014-11-11T09:25:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-11T09:25:14Z" id="62521763">Merged this to 1.x and master, thanks @jprante !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added getProperty method to Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8421</link><project id="" key="" /><description>This allows arbitrary properties to be retrieved from an aggregation tree. The property is specified using the same syntax as the
order parameter in the terms aggregation. If a property path contians a multi-bucket aggregation the property values from each bucket will be returned in an array.
</description><key id="48253172">8421</key><summary>Added getProperty method to Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T11:06:42Z</created><updated>2015-06-06T19:03:06Z</updated><resolved>2014-11-25T10:15:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-11-10T14:25:24Z" id="62390462">Not sure if this is related to this pull requests, but when I name my aggregations with for example `"a.b"` then `getProperty` will fail:

```
 @Test
    public void testNestedBucketsWithGetProperty() throws IOException, ExecutionException, InterruptedException {
        indexDocs();
        String firstAgg = "a.b";
        String secondAgg = "c.d";
        SearchResponse searchResponse = client().prepareSearch("index").setQuery(matchAllQuery())
                .addAggregation(terms("class").field("class").subAggregation(histogram(firstAgg).field("num").interval(1).subAggregation(avg(secondAgg).field("num")).subAggregation(avg("max").field("num"))))
                .get();
        assertThat(searchResponse.getHits().getTotalHits(), equalTo(30l));

        Aggregations agg = searchResponse.getAggregations();
        Object o = agg.get("class").getProperty(firstAgg + "." + secondAgg);
    }

    private void indexDocs() throws IOException, ExecutionException, InterruptedException {
        List&lt;IndexRequestBuilder&gt; indexRequests = new ArrayList&lt;&gt;();
        for (int i = 0; i&lt; 3; i++) {
            for (int j = 0; j&lt; 10; j++) {
                indexRequests.add(client().prepareIndex().setSource(jsonBuilder().startObject().field("class", Integer.toString(i)).field("num", j).endObject()).setIndex("index").setType("type"));
            }
        }
        indexRandom(true, indexRequests);
        ensureGreen("index");
    }
```
</comment><comment author="colings86" created="2014-11-10T14:37:16Z" id="62392058">@brwe I was talking to @clintongormley about this the other day. This will need to be fixed but outside of this PR as it also affects sort order for aggregations.  For example, currently you cannot order a terms aggregation using the 99.9th percentile from the percentiles aggregation as you would need to specify your order as `percents.99.9` which would be interpreted as an aggregation named `percents.99` and a value named `9`.
</comment><comment author="jpountz" created="2014-11-10T17:10:50Z" id="62417699">I like this new API as it is very convenient. I left a couple of comments/questions.

I know it's outside of the scope of this PR but I'm wondering if we should make the path syntax compatible with some standard like json path for the next major release, in a way that it would be compatible with calling the same json path expression on the json response from elasticsearch. In addition to making it easier to approach to new users, it would also allow to address data that would be stored in a specific bucket, which is not supported today (although I could imagine sorting or reducers make use of it)?
</comment><comment author="colings86" created="2014-11-11T10:08:47Z" id="62526483">I like the idea of supporting JSON path expressions. From looking at the spec for JSON path, we are not that far away in what we do already, although we need to replace the '&gt;' character with '.' in our current syntax. One thing we would need to sort out is what happens if the aggregation name contains a dot. This currently causes issues and we need to come up with a resolution order for resolving names and values with a dot, or remove support for agg names with a dot.
</comment><comment author="colings86" created="2014-11-13T14:40:04Z" id="62899122">Added a new issue for the JSONPath suggest here: https://github.com/elasticsearch/elasticsearch/issues/8434
</comment><comment author="colings86" created="2014-11-13T14:55:06Z" id="62901351">@brwe @jpountz thanks for reviewing, I have addressed/replied to your comments and pushed a new commit. Would you mind taking another look?
</comment><comment author="jpountz" created="2014-11-25T09:36:05Z" id="64332029">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] add 2d vis for decay functions and parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8420</link><project id="" key="" /><description /><key id="48246743">8420</key><summary>[docs] add 2d vis for decay functions and parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-11-10T09:51:41Z</created><updated>2014-11-10T09:58:29Z</updated><resolved>2014-11-10T09:58:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-10T09:52:01Z" id="62362229">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[docs] add 2d vis for decay functions and parameters</comment></comments></commit></commits></item><item><title>The elasticsearch user's home directory belongs to root for RPM installs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8419</link><project id="" key="" /><description>The home directory for the elasticsearch user is not created with the user, it's created later on during the install and hence gets created as root:root with everything under `/usr/share/elasticsearch` belonging to root as a result. This is from a fresh install on a CentOS 6.5 box:

```
$ cat /etc/issue
CentOS release 6.5 (Final)
Kernel \r on an \m

$ ls -ald /usr/share/elasticsearch/
drwxr-xr-x 4 root root 4096 Nov 10 05:42 /usr/share/elasticsearch/

$ ls -al /usr/share/elasticsearch/
total 44
drwxr-xr-x   4 root root  4096 Nov 10 05:42 .
drwxr-xr-x. 67 root root  4096 Nov 10 05:42 ..
drwxr-xr-x   2 root root  4096 Nov 10 05:42 bin
drwxr-xr-x   3 root root  4096 Nov 10 05:42 lib
-rw-r--r--   1 root root 11358 Jul 28 14:53 LICENSE.txt
-rw-r--r--   1 root root   150 Jul 28 14:53 NOTICE.txt
-rw-r--r--   1 root root  8421 Jul 28 14:53 README.textile
```

Was there a valid reason for this?
</description><key id="48232235">8419</key><summary>The elasticsearch user's home directory belongs to root for RPM installs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">atrepca</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2014-11-10T05:53:30Z</created><updated>2014-12-11T14:39:06Z</updated><resolved>2014-12-11T14:39:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-11-10T11:30:40Z" id="62372238">The initial idea was to only give the elasticsearch user write permission to the data dir, however I see that this leads to the problem, that only root can install plugins actually... I will take a look, if we do the same in the debian package and if we do it might make sense to rethink this strategy.

I guess by opening this ticket, you agree to this approach? :-)
</comment><comment author="atrepca" created="2014-11-11T04:44:39Z" id="62501517">The approach of rethinking the strategy? Sure :smile:

Trying to install a plugin as the elasticsearch user (provided you go around `/sbin/nologin`) fails (as expected) with:

```
$ sudo su -s /bin/bash -c "/usr/share/elasticsearch/bin/plugin install lukas-vlcek/bigdesk" elasticsearch
-&gt; Installing lukas-vlcek/bigdesk...

Failed to install lukas-vlcek/bigdesk, reason: plugin directory /usr/share/elasticsearch/plugins is read only
```

Looks to be the same for the debian package, the `adduser` command is invoked with `--no-create-home` in [postinst](https://github.com/elasticsearch/elasticsearch/blob/master/src/deb/control/postinst#L24-26) so I guess it's created later on as root too.
</comment><comment author="t-lo" created="2014-12-01T16:28:07Z" id="65090795">Both RPM and DEB seem to be affected. Actually, `/usr/share/elasticsearch/plugins` doesn't even exist after a fresh install. It would be rather simple though to create this directory and give ownership to the elasticsearch user. That would be two additional lines in both `src/deb/control/postinst` and `src/rpm/scripts/postinstall`, correspondingly.

Since changing the ownership of `/usr/share/elasticsearch/plugins` suffices to resolve the plugin install issue I recommend to change ownership only for this subdirectory.
</comment><comment author="t-lo" created="2014-12-11T13:51:43Z" id="66621089">This issue was resolved by pull request #8732 which essentially has been merged already.
</comment><comment author="atrepca" created="2014-12-11T14:39:06Z" id="66627310">Thanks for fixing @t-lo, marking as closed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>percolator failed to percolate when there are capital letters in not_analyzed field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8418</link><project id="" key="" /><description>I'm using elasticsearch java client.

I stored log in for example 'test' type.
String log = "{ \"logLevel\":\"WARN\"}";

in which the mapping of logLevel field is "string, not_analyzed"

and if there is a query stored in .percolator type
logLevel:/WARN/

percolate request fails,

if the query was like
logLevel:"WARN"

it succeeded.

for comparison, if the log was like
String log = "{ \"logLevel\":\"warn\"}";

in such case, both two queries are properly percolated
logLevel:/warn/
logLevel:"warn"

think there is not any related issue. hope this will be fixed soon.
</description><key id="48225105">8418</key><summary>percolator failed to percolate when there are capital letters in not_analyzed field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels /><created>2014-11-10T02:49:04Z</created><updated>2014-11-12T13:59:38Z</updated><resolved>2014-11-10T11:00:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sweetest" created="2014-11-10T04:53:23Z" id="62341252">it seems like regex field in a query get lower cased, cannot find query by document with capital letters in not_analyzed field.
</comment><comment author="clintongormley" created="2014-11-10T11:00:19Z" id="62369469">Hi @sweetest 

First, regex queries are slow and heavy.  For the use case that you describe, a simple term filter would be much more efficient.  Anyway, the parameter you're looking for is [`lowercase_expanded_terms`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-dsl-query-string-query) which defaults to `true`.   You want to set this to `false` in this case.  (but i repeat, there are much better ways to do this)
</comment><comment author="sweetest" created="2014-11-12T13:59:38Z" id="62721062">Thanks for the comment, I do appreciate that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add score() back to AbstractSearchScript</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8417</link><project id="" key="" /><description>See #8377
closes #8416
</description><key id="48223017">8417</key><summary>Add score() back to AbstractSearchScript</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T01:57:48Z</created><updated>2015-06-08T00:25:11Z</updated><resolved>2014-11-10T15:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file></files><comments><comment>Scripting: Add score() back to AbstractSearchScript</comment></comments></commit></commits></item><item><title>Add `score()` back to `AbstractSearchScript`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8416</link><project id="" key="" /><description>In #6864, `score()` was removed from `AbstractSearchScript`, in favor of using `doc().score()`.  However, in #7819, `score()` was removed from `DocLookup`.  While native scripts can still override `setScorer(Scorer)`, we should make it easier to access the score by keeping stashing the scorer in `AbstractScoreScript` for them.
</description><key id="48222421">8416</key><summary>Add `score()` back to `AbstractSearchScript`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-10T01:43:20Z</created><updated>2014-11-10T16:00:18Z</updated><resolved>2014-11-10T15:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-10T08:43:10Z" id="62355404">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file></files><comments><comment>Scripting: Add score() back to AbstractSearchScript</comment></comments></commit></commits></item><item><title>Removed unnecessary DiscoveryService reference from LocalDiscovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8415</link><project id="" key="" /><description>This causes Guice circular dependency issues when local discovery used and plugins are loaded 

I noticed this when running plugin tests when `plugin.types` was set and local discovery was used (which was the default).

Closes #8539
</description><key id="48216427">8415</key><summary>Removed unnecessary DiscoveryService reference from LocalDiscovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Discovery</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-09T22:34:18Z</created><updated>2015-06-07T18:07:33Z</updated><resolved>2014-11-12T10:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-10T08:45:31Z" id="62355617">LGTM
</comment><comment author="martijnvg" created="2014-11-21T11:27:38Z" id="63958200">The problem that this PR fixes only occurs on 1.4.0.Beta1 and 1.4.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't eagerly load NestedDocsFilter in bitset filter cache, because it is never used.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8414</link><project id="" key="" /><description>Related #8394
</description><key id="48216177">8414</key><summary>Don't eagerly load NestedDocsFilter in bitset filter cache, because it is never used.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cache</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-09T22:26:52Z</created><updated>2015-06-07T18:03:42Z</updated><resolved>2014-11-09T23:34:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-09T22:40:09Z" id="62323356">If it is never used, should we remove it entirely?
</comment><comment author="martijnvg" created="2014-11-09T22:45:41Z" id="62323591">@jpountz Makes sense. I updated the PR and remove it.
</comment><comment author="jpountz" created="2014-11-09T22:46:21Z" id="62323625">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NestedDocsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NonNestedDocsFilter.java</file></files><comments><comment>Core: Remove NestedDocsFilter, because it isn't used and also don't eagerly load it in bitset filter cache.</comment></comments></commit></commits></item><item><title>Extend refresh-mapping logic to the `_default_` type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8413</link><project id="" key="" /><description>When data nodes receive mapping updates from the master, the parse it and merge it into their own in memory representation (if there). If this results in different bytes then the master sent, the nodes will send a refresh-mapping command to indicate to the master that it's byte level storage of the mapping should be refreshed via the document mappers. This comes handy when the mapping format has changed, in a backwards compatible manner, and we want to make sure we can still rely on the bytes to identify changes.  An example of such a change can be seen at #4760.

  This commit extends the logic to include the `_default_` type, which was never refreshed before. In some unlucky scenarios, this called the _default_ mapping to be parsed with every cluster state update.

PS - I haven't found a good way to test this. Suggestions are welcome.
</description><key id="48214302">8413</key><summary>Extend refresh-mapping logic to the `_default_` type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-09T21:28:13Z</created><updated>2015-06-07T18:04:15Z</updated><resolved>2014-11-10T19:44:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-11-10T16:51:33Z" id="62414428">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Internal: extend refresh-mapping logic to the _default_ type</comment></comments></commit></commits></item><item><title>[TEST] Disable compression in BWC test for version &lt; 1.3.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8412</link><project id="" key="" /><description>The compression bug fixed in #7210 can still strike us since we are
running BWC test against these version. This commit disables compression
forcefully if the compatibility version is &lt; 1.3.2 to prevent debugging
already known issues.
</description><key id="48212840">8412</key><summary>[TEST] Disable compression in BWC test for version &lt; 1.3.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2014-11-09T20:39:17Z</created><updated>2015-06-07T11:46:38Z</updated><resolved>2014-11-11T13:12:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-09T21:26:35Z" id="62320504">LGTM
</comment><comment author="rjernst" created="2014-11-10T16:43:37Z" id="62413107">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Avoid interrupt()'ing NIO channels in TranslogService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8411</link><project id="" key="" /><description /><key id="48209484">8411</key><summary>Avoid interrupt()'ing NIO channels in TranslogService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-11-09T18:47:23Z</created><updated>2014-11-23T12:24:43Z</updated><resolved>2014-11-23T12:24:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-09T20:16:20Z" id="62317509">I really think we need to clean up more of those there are a couple of them that look even worse like the one on the refresh / merge future in `InternalIndexShard.java` maybe we open a branch and fix all of them there and can run CI on them to ensure it doesn't impact test stability?
</comment><comment author="rmuir" created="2014-11-10T12:08:50Z" id="62375724">Why should that prevent me from fixing this?
</comment><comment author="s1monw" created="2014-11-11T15:47:04Z" id="62565914">we can have PRs for each of them if that helps use to make progress I am +1 LGTM
</comment><comment author="s1monw" created="2014-11-12T09:06:13Z" id="62688582">@rmuir do you wanna push this to `1.3` and `1.4` too I think it's a bug
</comment><comment author="jpountz" created="2014-11-17T15:20:56Z" id="63319810">Is this PR superseded by https://github.com/elasticsearch/elasticsearch/pull/8494?
</comment><comment author="s1monw" created="2014-11-17T21:15:51Z" id="63377184">@jpountz I think so
</comment><comment author="clintongormley" created="2014-11-23T11:38:51Z" id="64115004">@rmuir It looks like we can close this PR, now that #8494 has been merged?
</comment><comment author="s1monw" created="2014-11-23T12:24:43Z" id="64116155">yeah we can close this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support _analyzer mapping in nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8410</link><project id="" key="" /><description>The existing [`_analyzer` mapping feature](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-analyzer-field.html) currently works only with top-level documents.

It should be possible to use `_analyzer` mapping on nested document level, same as it is now possible on root document.

In our particular case with eZ Publish CMS, we index Content (our main domain) fields in a nested document **per translation**. Content can have multiple translations -- we currently support 42 languages out of the box, but users are also free to add their own. The fields stored there are dynamic and not known upfront (we implement a kind of metabase).

We use custom `_all` field in translation's nested document, and our requirement here is to be able to configure analyzers per translation.

I'm aware this is possible to achieve with [dynamic templates](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates), and we did implement it through that feature, see https://github.com/ezsystems/ezpublish-kernel/pull/1065.

Short snippet of our `content` document mapping with dynamic templates approach:

``` json
{
    "content": {
        "dynamic": false,
        "_all" : { "enabled": false },
        "properties": {
            "id": { "type": "integer", "index": "not_analyzed" },
            "type_id": { "type": "string", "index": "not_analyzed" },
            ...
            "fields_doc": {
                "type": "nested",
                "dynamic": true,
                "properties": {
                    "meta_all_ara_SA": { "type": "string", "analyzer": "analyzer_ara_SA" },
                    "meta_all_cat_ES": { "type": "string", "analyzer": "analyzer_cat_ES" },
                    "meta_all_chi_CN": { "type": "string", "analyzer": "analyzer_chi_CN" },
                    ...
                }
            }
        },
        "dynamic_templates": [
            { "meta_all_ara_SA": { "path_match": "fields_doc.*_meta_all_ara_SA_*", "mapping": { "index": "no", "copy_to": "fields_doc.meta_all_ara_SA" } } },
            { "meta_all_cat_ES": { "path_match": "fields_doc.*_meta_all_cat_ES_*", "mapping": { "index": "no", "copy_to": "fields_doc.meta_all_cat_ES" } } },
            { "meta_all_chi_CN": { "path_match": "fields_doc.*_meta_all_chi_CN_*", "mapping": { "index": "no", "copy_to": "fields_doc.meta_all_chi_CN" } } },
            ...
        ]
    }
}
```

This obviously has several drawbacks:
1. we now have 42 separate custom `_all` fields per nested document (one per language, and the number will only grow), but only single one is really used
2. our mapping is now dependant on the languages used
3. in fulltext search, we now need to query all custom `_all` fields, instead of a single one

Being able to use `_analyzer` mapping on a nested document our mapping could be made completely unaware of repository's language configuration. Also, we would be able to use only one custom `_all` field in translation document (`fields_doc.meta_all_*` in the snippet) in order to achieve analyzer configuration per translation. This also means single field to query for fulltext search, instead of using the wildcard to cover them all.

Other CMS's implementing search on top of Elasticsearch using nested documents probably hit this same problem as well.
</description><key id="48207531">8410</key><summary>Support _analyzer mapping in nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pspanja</reporter><labels /><created>2014-11-09T17:35:24Z</created><updated>2014-11-10T10:47:45Z</updated><resolved>2014-11-10T10:47:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-10T10:47:45Z" id="62368195">Hi @pspanja 

I view the `_analyzer` field as a mistake - we should never have added it.  Tokens from different analyzers end up in the same field, which messes up term frequencies, idf, conflates tokens from different languages (eg `die` in german and english), etc.  And you can only choose a single analyzer to use at search time, so which one do you choose?  

I think what you are doing already is a good approach.  If you want to do cross-language search that doesn't involve searching 42 fields, then have an additional custom `_all` field which just uses the standard analyzer (or eg the icu tokenizer plus the icu folding filter) and search that field.  That will work reasonably well.

You won't have stemming, synonyms, stopwords etc, but really you can't do that cross language in a single field anyway. 

I know the `_analyzer` approach sounds tempting, but you just end up with a mess of tokens and completely incorrect statistics.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>java api issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8409</link><project id="" key="" /><description>in 1.4.0, if an es client is obtained via node builder and then all of the indexes are dropped and recreated via this client, subsequent data indexing calls appear to fail. 

If you follow this at trace level, each index call results in a message 'primary shard [xxx] is not yet active or we do not know the node it is assigned to [null], scheduling a retry. ' (originating from TransportShardReplicationOperationAction). 

This is not the behaviour in 1.3.4. 

The issue also doesn't manifest itself if a transport client is used instead.
</description><key id="48207279">8409</key><summary>java api issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">rob-tice</reporter><labels><label>feedback_needed</label></labels><created>2014-11-09T17:25:52Z</created><updated>2015-01-05T11:41:42Z</updated><resolved>2015-01-05T11:41:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-10T10:04:11Z" id="62363570">Hi @rob-tice 

Could you provide some code (and the trace output) to recreate this issue please?
</comment><comment author="clintongormley" created="2014-12-30T20:33:08Z" id="68395251">@bleskes any ideas here?
</comment><comment author="bleskes" created="2015-01-04T20:57:27Z" id="68648621">The error message suggest the client node doesn't know where the primaries of the new indices are allocated. It's hard to know why from the current information we have. Since we didn't hear anything so far, I suggest we give it one more week and close it unless we get more info.
</comment><comment author="rob-tice" created="2015-01-04T21:42:14Z" id="68650416">Hi guys.

I added a flag in my app to create a new client if this exception occurred
as I didn't have time to investigate.
I have not seen it in any of the later releases so unless it starts to
occur again I would regard it as closed.

Best

Rob
On 4 Jan 2015 20:58, "Boaz Leskes" notifications@github.com wrote:

&gt; The error message suggest the client node doesn't know where the primaries
&gt; of the new indices are allocated. It's hard to know why from the current
&gt; information we have. Since we didn't hear anything so far, I suggest we
&gt; give it one more week and close it unless we get more info.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8409#issuecomment-68648621
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Synchronize operations that modify file mappings on DistributorDirectory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8408</link><project id="" key="" /><description>The rename(String, String) method doesn't allow this implementation to use a simple
concurrent map. There is a race during a rename operation where files are not fully
renamed but already visible via #listAll(). This inconsistency can lead to problems
when opening commit points since the pending_segments_N as well as segments_N are visible
but not yet atomically renamed.

Yet, non of the methods that are synced are long running such that adding sychronization
doesn't introduce bottlenecks here. The Direcotry#sync(...) method is not synchronized since
it doesn't change any mapping nor does it depend on the mapping.
</description><key id="48206066">8408</key><summary>Synchronize operations that modify file mappings on DistributorDirectory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-09T16:42:34Z</created><updated>2015-06-07T17:36:13Z</updated><resolved>2014-11-09T18:12:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-09T17:47:11Z" id="62312016">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Calculate Alder32 Checksums for legacy files in Store#checkIntegrity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8407</link><project id="" key="" /><description>Previously we didn't calculate this checksums even though we have a checksum
to compare. Since we now also verify checksums for legacy files #checkIntegrity
should also calculate the legacy checksums.
</description><key id="48205598">8407</key><summary>Calculate Alder32 Checksums for legacy files in Store#checkIntegrity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>bug</label><label>resiliency</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-09T16:23:32Z</created><updated>2015-06-08T00:41:45Z</updated><resolved>2014-11-09T17:40:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-09T16:46:50Z" id="62309797">@rmuir pushed an update
</comment><comment author="rmuir" created="2014-11-09T16:54:41Z" id="62310075">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file></files><comments><comment>[STORE] Calculate Alder32 Checksums for legacy files in Store#checkIntegrity</comment></comments></commit></commits></item><item><title>Terms agg with zero-result filtered query searches whole index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8406</link><project id="" key="" /><description>We're using a filtered query followed by a terms agg, as so:

```
{
  "query":{
    "filtered":{
      "query":{ 
        "query_string":{
          "query": "type:web_info_log AND log_namespace:FOO AND message:\"bar\""
        }
      },
      "filter":{
        "bool":{
          "must":[{
            "range":{
              "@timestamp":{
                "from":1415464349189,"to":1415465564077
              }
            }
          }]
        }
      }
    }
  },
  "aggs":{
    "histogram_aggregation":{
      "histogram":{
        "interval":10,"field":"@timestamp"
      },
      "aggs":{
        "terms_bucket":{
          "terms":{
            "field":"log_message","execution_hint":"map"
          }
        }
      }
    }
  }
}
```

Normally the filtered query returns some small number of requests (under 100) and everything is fine.
However, when it returns zero results, the terms aggregations runs on the entire index (in our case, ~4bn docs), causing repeated GCs which take many hours and make the cluster unusable.
It should, in fact, not run at all because it got zero results from the filtered query.
</description><key id="48203891">8406</key><summary>Terms agg with zero-result filtered query searches whole index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">avleen</reporter><labels /><created>2014-11-09T15:17:43Z</created><updated>2014-12-31T17:22:47Z</updated><resolved>2014-12-31T17:22:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="avleen" created="2014-11-09T15:21:13Z" id="62306699">This is on Elasticsearch 1.3.2.
If this is repeatable for others, any chance the fix could make it into 1.4? It's brought our cluster down repeatedly in the last 2 weeks and I can reproduce it here easily.
</comment><comment author="jpountz" created="2014-11-10T10:16:46Z" id="62364882">@avleen This looks wrong indeed. I just tried to reproduce the issue with no success, can you please help me understand a bit more what happens:
- do you actually get a doc_count for every term in the response or did you infer that it ran on all docs because of CPU or I/O activity?
- can you confirm you are not setting [min_doc_count](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_minimum_document_count) to `0`? (this option combined with the `map` execution hint could make elasticsearch search the whole index).
- if you can still reproduce this issue, could you try to capture the output of the [hot threads](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html) while the query is running so that I can see what is calling the aggregation.
</comment><comment author="avleen" created="2014-11-10T17:54:33Z" id="62424669">Hi @jpountz 
Here're the results from the query when searching over 2 days of indices, and running the agg on a field with very low cardinality (maybe a few dozen unique entries):

```
{
   "took": 6148,
   "timed_out": false,
   "_shards": {
      "total": 78,
      "successful": 78,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   },
   "aggregations": {
      "terms": {
         "buckets": []
      }
   }
}
```

When I run it against our `log_message` field, which is `not_analyzed` and has a ton of large strings (up to 10kb long) and the vast majority of the messages are unique:

```
{
   "took": 184340,
   "timed_out": false,
   "_shards": {
      "total": 39,
      "successful": 39,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   },
   "aggregations": {
      "terms": {
         "buckets": []
      }
   }
}
```
</comment><comment author="jpountz" created="2014-11-10T18:57:50Z" id="62434824">Hmm this might be due to fielddata loading. This is expected since we need to load fielddata before knowing what matches but I will check that we are not loading global ordinals with the "map" execution hint since they are not required.

Can you confirm that only the first few queries are slow?
</comment><comment author="avleen" created="2014-11-10T21:21:55Z" id="62457407">This seems to be pretty slow all the time. It's hard to confirm because running the query takes down our production cluster for several minutes. It just happened again and one node fell out of the cluster because it was trying to do garbage collection for several minutes :-)

Running the `filtered query` part is really fast. It rarely takes more than 5 or 10s for ~5bn docs.
Running the `aggs` on top of that, even when are zero results from the query, takes many minutes.
So in this case, why is fielddata being loaded when there were zero results?
</comment><comment author="jpountz" created="2014-11-12T09:10:35Z" id="62689043">I can try to explain a bit more how running works on a shard level: Elasticsearch creates an object called `BulkScorer` which is responsible for finding matching documents and optionally scoring them. This `BulkScorer` takes a `Collector` which is basically a callback that gets called for every matching document. Examples of collectors include `TotalHitCountCollector` (to just count the number of matches), `TopDocsCollector` (to compute the top matching documents) or `AggregationsCollector` (to run aggregations). For instance your request would use both a `TopDocsCollector` and a `AggregationsCollector`. When we create the collector for aggregations, we need to prepare everything to get ready for collecting matches, which includes loading fielddata. But at this point we don't know if the query is going to match anything yet. This is why even queries that do not match any hits load field data.
</comment><comment author="jpountz" created="2014-11-12T09:27:48Z" id="62691010">I just checked that the `map` execution hint does not load global ordinals, as expected. So the only idea I have is that these slow times for queries that match no queries is due to fielddata loading.

However, I'm still concerned that you mentioned that everything works fine when the query returns ~100 matches. Do you mean that requests that match no documents are more harmful to your cluster than those queries that match few documents?
</comment><comment author="avleen" created="2014-11-24T16:54:20Z" id="64224657">It seemed that way but don't take it as gospel. It's possible that during
those instances, field data was already cached.

Could the field data loading be delayed until we know what documents
actually need to be loaded in?
It seems problematic is always load in huge amounts of field data when you
are running a filtered query to reduce the number of documents for
aggregation.

On Wed, Nov 12, 2014, 01:27 Adrien Grand notifications@github.com wrote:

&gt; I just checked that the map execution hint does not load global ordinals,
&gt; as expected. So the only idea I have is that these slow times for queries
&gt; that match no queries is due to fielddata loading.
&gt; 
&gt; However, I'm still concerned that you mentioned that everything works fine
&gt; when the query returns ~100 matches. Do you mean that requests that match
&gt; no documents are more harmful to your cluster than those queries that match
&gt; few documents?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8406#issuecomment-62691010
&gt; .
</comment><comment author="clintongormley" created="2014-11-25T13:10:01Z" id="64396951">@avleen If you want to use a field for sorting/aggregating, then you need it in fielddata.  Perhaps this query doesn't match but the next one will.  Fielddata is loaded from disk by uninverting the index, so it has to happen in one go.  It would be much much much slower if we were to load only matching docs.

Instead, use doc values for this field.  Then the memory requirements go away.
</comment><comment author="avleen" created="2014-11-25T13:15:18Z" id="64397534">We'll give doc values another site Clinton. When we tested them a while
back,we traded lower memory for significantly higher disk IO which slowed
indexing.
We'll test to see if the recent perf improvements with doc values make them
more viable now :)

Thanks!

On Tue, Nov 25, 2014, 14:10 Clinton Gormley notifications@github.com
wrote:

&gt; @avleen https://github.com/avleen If you want to use a field for
&gt; sorting/aggregating, then you need it in fielddata. Perhaps this query
&gt; doesn't match but the next one will. Fielddata is loaded from disk by
&gt; uninverting the index, so it has to happen in one go. It would be much much
&gt; much slower if we were to load only matching docs.
&gt; 
&gt; Instead, use doc values for this field. Then the memory requirements go
&gt; away.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8406#issuecomment-64396951
&gt; .
</comment><comment author="clintongormley" created="2014-11-25T16:54:13Z" id="64432215">@avleen Make sure you aren't using them on the `message.raw` field (in fact, just disable the `message.raw` field completely).  Also, in Lucene 5, doc values merging is better than it is today, which should help as well.  Would be interested in any stats you can give us about the effect of doc values on your indexing.
</comment><comment author="clintongormley" created="2014-12-31T17:22:47Z" id="68455752">Nothing more to do here.  Closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8312
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: SerialMergeScheduler never triggers index throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8405</link><project id="" key="" /><description>This is a deprecated merge scheduler (already removed in master), but is still available in 1.3, 1.4, 1.5.

When you pick this merged scheduler, there is a bug that causes index throttling to not kick in, which means it's possible to accumulate way too many segments in the index.

Furthermore, this can then cause Elasticsearch to take a long time (&gt; 30 seconds default timeout for e.g. a delete index request, resulting in a "Delete Index failed - not acked" from ElasticsearchIntegrationTest) to close the index, since it has a merge thread holding InternalEngine.readLock, stuck in IndexWriter.maybeMerge.  I'll address this issue separately; fixing SMS (here) should fix most of the false test failures.

I also fixed IndexStatsTests.throttleTests to fail if it never saw index throttling kick in after up to 5 minutes of crazy merges.
</description><key id="48198951">8405</key><summary>Core: SerialMergeScheduler never triggers index throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Engine</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label></labels><created>2014-11-09T11:39:09Z</created><updated>2015-03-19T16:42:10Z</updated><resolved>2014-11-09T15:01:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-09T11:40:19Z" id="62300431">LGTM mike gooood catch!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Typo fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8404</link><project id="" key="" /><description /><key id="48180680">8404</key><summary>Typo fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismattmann</reporter><labels /><created>2014-11-08T21:01:04Z</created><updated>2014-11-09T20:42:09Z</updated><resolved>2014-11-09T10:37:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chrismattmann" created="2014-11-08T21:01:34Z" id="62278674">I have no clue why it has the old commits in there, but the one I was trying to send along was https://github.com/chrismattmann/elasticsearch/commit/854b89e6759b4fafb656e110d4f3267f5e6c7999
</comment><comment author="clintongormley" created="2014-11-09T10:35:38Z" id="62298705">Thanks @chrismattmann - merged
</comment><comment author="chrismattmann" created="2014-11-09T18:15:29Z" id="62313081">Thanks @clintongormley !
</comment><comment author="s1monw" created="2014-11-09T20:42:09Z" id="62318616">thanks @chrismattmann 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fix some typos, and sentence structure.</comment></comments></commit></commits></item><item><title>Add extra validation to segments_N files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8403</link><project id="" key="" /><description>lucene has a build-in CRC32 for segments_N files even before Lucene 4.8 we should take advantage of that and check them when those files are recovered.
</description><key id="48179135">8403</key><summary>Add extra validation to segments_N files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>adoptme</label><label>enhancement</label><label>resiliency</label></labels><created>2014-11-08T20:03:09Z</created><updated>2015-11-17T13:26:15Z</updated><resolved>2015-11-17T13:26:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-11-17T13:26:15Z" id="157369952">closing as this seems to be done with the refactoring that was part of 2.0.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve recovery protocol by adding expected file footer to detect truncation more easily</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8402</link><project id="" key="" /><description>We should add an extra layer of checks and send footers that we expect to ensure we can detect file truncation more easily in addition to the plain file length 
</description><key id="48179072">8402</key><summary>Improve recovery protocol by adding expected file footer to detect truncation more easily</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2014-11-08T20:00:47Z</created><updated>2016-03-23T12:34:32Z</updated><resolved>2016-03-23T12:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:50:29Z" id="158680013">@s1monw is this still relevant?
</comment><comment author="s1monw" created="2016-03-23T12:34:22Z" id="200329155">closing - not relevant anymore
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add dedicated unittest for VerifyingIndexOutput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8401</link><project id="" key="" /><description>this is a pretty integral part of our resiliency code and should have dedicated unittests
</description><key id="48178997">8401</key><summary>Add dedicated unittest for VerifyingIndexOutput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-11-08T19:58:28Z</created><updated>2015-04-19T11:30:39Z</updated><resolved>2015-04-19T11:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-04-19T11:30:37Z" id="94267783">we have `LegacyVerificationTests` which is close enough.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix mention of Apache License, version 2.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8400</link><project id="" key="" /><description /><key id="48155844">8400</key><summary>fix mention of Apache License, version 2.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismattmann</reporter><labels /><created>2014-11-08T04:57:54Z</created><updated>2015-03-19T10:18:52Z</updated><resolved>2014-11-08T18:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-08T05:00:05Z" id="62245978">@chrismattmann thank you, are there any addtl docs indicating guidelines regarding this you can provide?
</comment><comment author="chrismattmann" created="2014-11-08T05:04:02Z" id="62246070">Hi @rmuir yeah, @gstein always tells me that it's the Apache License, version 2 ("ALv2"). Looking at the title for this page http://www.apache.org/licenses/LICENSE-2.0.html also shows that.
</comment><comment author="rmuir" created="2014-11-08T05:06:57Z" id="62246145">OK, thank you. I agree with the title, i just dont see the "ALv2" and was confused. I only want to make sure we do the right thing everywhere (in case its mentioned elsewhere). 
</comment><comment author="chrismattmann" created="2014-11-08T05:10:16Z" id="62246226">No worries, @rmuir. Like I said, @gstein was one of the peeps who made the license and has been around the ASF since the beginning. You can Google around for it, ask greg, whatever. Anyways just saw it as Apache 2 License, and Greg has specifically told me that's not what it's called.
</comment><comment author="gstein" created="2014-11-08T05:32:17Z" id="62246693">"ALv2" is shorthand, much like you'll see "GPLv3" or "MPL". That can certainly be omitted from the change in this pull request, but fixing the license name/reference is definitely a good idea.
</comment><comment author="s1monw" created="2014-11-08T08:07:23Z" id="62249779">@chrismattmann  I know it's a small change but I need to ask you to sign the CLA? http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="chrismattmann" created="2014-11-08T15:07:07Z" id="62260873">@s1monw done, thanks.
</comment><comment author="chrismattmann" created="2014-11-08T20:53:16Z" id="62278386">Thanks @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fix mention of Apache License, version 2.</comment></comments></commit></commits></item><item><title>Harden recovery for old segments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8399</link><project id="" key="" /><description>When a lucene 4.8+ file is transferred, Store returns a VerifyingIndexOutput
that verifies both the CRC32 integrity and the length of the file.

However, for older files, problems can make it to the lucene level. This is not great
since older lucene files aren't especially strong as far as detecting issues here.

For example, if a network transfer is closed on the remote side, we might write a
truncated or corrupted file... which old lucene formats may or may not detect.

The idea here is to verify old files with their legacy Adler32 checksum, plus expected
length. If they don't have an Adler32 (segments_N, jurassic elasticsearch?, its optional
as far as the protocol goes), then at least check the length.

We could improve it for segments_N, its had an embedded CRC32 forever in lucene, but this
gets trickier. Long term, we should also try to also improve tests around here, especially
backwards compat testing, we should test that detected corruptions are handled properly.

I added unit tests for the two cases covered here. PR is against 1.x since thats what i'm working with,
but I can forward-port to master once we figure this out.
</description><key id="48155145">8399</key><summary>Harden recovery for old segments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-08T04:23:12Z</created><updated>2015-06-08T00:22:59Z</updated><resolved>2014-11-09T09:14:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-08T06:34:26Z" id="62247969">LGTM!
</comment><comment author="s1monw" created="2014-11-08T07:58:14Z" id="62249611">I didn't look very close but on a high level this looks very good to me - not sure why we haven't done this before to be honest...
</comment><comment author="s1monw" created="2014-11-08T08:03:40Z" id="62249702">left some rather cosmetic comments, LGTM though - honestly I almost classify this as a bugfix so I think we should push it to `1.4` and `1.3` too WDYT?
</comment><comment author="rmuir" created="2014-11-08T11:20:57Z" id="62254391">Thanks for looking @s1monw . Can you look at my comments? I am concerned that those comments are not cosmetic but are too risky for this task.
</comment><comment author="rmuir" created="2014-11-08T11:58:11Z" id="62255288">I tried a way that might make us both happy. I made the irrelevant forwarded methods 'final' and so on.
</comment><comment author="s1monw" created="2014-11-08T19:43:47Z" id="62274095">thanks @rmuir LGTM
</comment><comment author="mikemccand" created="2014-11-08T21:24:14Z" id="62279403">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/LegacyVerification.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/store/VerifyingIndexOutput.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file><file>src/test/java/org/elasticsearch/index/store/TestLegacyVerification.java</file></files><comments><comment>Internal: harden recovery for old segments</comment></comments></commit></commits></item><item><title>Redhat Specific.  Elasticsearch 1.4.0-1.Beta1 is newer than 1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8398</link><project id="" key="" /><description>I'm going to paste the irclogs as i'm not the person with the problem.  But I agree it is a problem.

 The problem is that elasticsearch1.4.0-1.Beta1 is newer than 1.4.0.  Requiring you to --force the action.

```
[15:45] &lt;audiphilth&gt; yum update says nothing to update, manual run of rpm update gives this - package
                     elasticsearch-1.4.0.Beta1-1.noarch (which is newer than elasticsearch-1.4.0-1.noarch) is already
                     installed
[15:51] &lt;Damm&gt; audiphilth, you likely have to remove the beta
[15:51] &lt;Damm&gt; to get to the release
[15:51] &lt;Damm&gt; audiphilth, that or rpm -Uvh to it
[15:51] &lt;Damm&gt; and grab the rpm
[15:51] &lt;Damm&gt; audiphilth, likely better to force it by the command line than remove it heh
[15:51] --- edx is now known as MUCHAHASAQ
[15:51] &lt;audiphilth&gt; ya, i grabbed the rpm and i get - package elasticsearch-1.4.0.Beta1-1.noarch (which is newer than
                     elasticsearch-1.4.0-1.noarch) is already installed
[15:52] &lt;audiphilth&gt; not sure why it thinks the 1.4 beta is newer
[15:53] &lt;audiphilth&gt; rpm -Uvh elasticsearch-1.4.0.noarch.rpm
[15:53] &lt;audiphilth&gt; Preparing...                ########################################### [100%]
[15:53] &lt;audiphilth&gt;         package elasticsearch-1.4.0.Beta1-1.noarch (which is newer than elasticsearch-1.4.0-1.noarch)
                     is already installed
[15:53] &lt;Damm&gt; --force
[15:53] &lt;Damm&gt; i think that works? or something similar
[15:53] &lt;Damm&gt; that's crazy though
[15:53] &lt;Damm&gt; oh 1.4.0-1
[15:53] &lt;Damm&gt; vs 1.4.0
[15:53] &lt;Damm&gt; geeze
[15:53] &lt;audiphilth&gt; right, was hoping not to have to do that, i'll try and see
[15:55] &lt;audiphilth&gt; --force works
[15:55] --- MUCHAHASAQ is now known as edx
[15:56] &lt;Damm&gt; well it appears the beta was a greater version
[15:56] &lt;Damm&gt; so it makes sense
[15:56] &lt;wjimenez_&gt; can anyone shed light on the '_type' and 'type' fields? what is the difference?
[15:57] &lt;audiphilth&gt; not sure how much sense it makes if the release ver. can be trumped by an *older* beta
[16:01] &lt;Damm&gt; someone should file an issue on this
[16:01] &lt;Damm&gt; so I think I will and just paste the irclogs
```
</description><key id="48145689">8398</key><summary>Redhat Specific.  Elasticsearch 1.4.0-1.Beta1 is newer than 1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damm</reporter><labels /><created>2014-11-08T00:04:03Z</created><updated>2014-11-08T20:58:51Z</updated><resolved>2014-11-08T12:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-08T12:26:42Z" id="62256003">@damm yes - this is a duplicate of #8353.  We have deleted the Beta1 package from the RPM repo, but if you have the beta installed then there is nothing to do but force the upgrade.  Going forward we have changed our version formats to avoid this issue.

thanks for reportin
</comment><comment author="damm" created="2014-11-08T20:58:51Z" id="62278580">@clintongormley so 1.4.0-2 in the future? 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>logstash processors keep dying with "nil?" redirect?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8397</link><project id="" key="" /><description>All of our logs go to a kafka cluster, then we have some logstash processors that push them into elasticsearch. We keep noticing a weird error, then the processor tends to die. The process is up mind you. But the CPU/Load goes to about 1% and there is almost no data going in our out of the ethernet port. I believe elasticsearch is telling the processor to go to a server, but is sending bad or missing data about said server. But that's just my theory.

Some info that will hopefully help shine a lot on the problem.

rpm:
logstash-1.4.2-1_2c0f5a1.noarch
logstash-kafka-1.2.1-1.noarch

My conf:
input {
kafka {
zk_connect =&gt; "zoo01:2111,zoo02:2111,zoo03:2111"
group_id =&gt; "access"
topic_id =&gt; "prod.access"
consumer_threads =&gt; 1
}
}

filter {
mutate {
remove_field =&gt; [ "_type", "_id", "_index", "logdate" ]
}

if [type] == "apache_access" {
ruby {
code =&gt; 'event["msec"] = event["usec"] / 1000.0 if event["usec"]'
remove_field =&gt; [ "usec" ]
}
} 
}

output {
elasticsearch {
host =&gt; "es-access"
port =&gt; "9200"
protocol =&gt; "http"
flush_size =&gt; '10000'
index =&gt; "%{type}-%{+YYYY.MM.dd}"
cluster =&gt; "elastic-access"
workers =&gt; "4"
}
}

The error we are seeing:
{:timestamp=&gt;"2014-11-07T00:04:19.463000+0000", :message=&gt;"Failed to flush outgoing items", :outgoing_count=&gt;246, :exception=&gt;#, :backtrace=&gt;["/opt/logstash/vendor/bundle/jruby/1.9/gems/ftw-0.0.39/lib/ftw/agent.rb:336:in execute'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/ftw-0.0.39/lib/ftw/agent.rb:217:inpost!'", "/opt/logstash/lib/logstash/outputs/elasticsearch/protocol.rb:106:in bulk_ftw'", "/opt/logstash/lib/logstash/outputs/elasticsearch/protocol.rb:80:inbulk'", "/opt/logstash/lib/logstash/outputs/elasticsearch.rb:315:in flush'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:219:inbuffer_flush'", "org/jruby/RubyHash.java:1339:in each'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:216:inbuffer_flush'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:193:in buffer_flush'", "/opt/logstash/vendor/bundle/jruby/1.9/gems/stud-0.0.17/lib/stud/buffer.rb:159:inbuffer_receive'", "/opt/logstash/lib/logstash/outputs/elasticsearch.rb:311:in receive'", "/opt/logstash/lib/logstash/outputs/base.rb:86:inhandle'", "/opt/logstash/lib/logstash/outputs/base.rb:78:in `worker_setup'"], :level=&gt;:warn}
</description><key id="48142101">8397</key><summary>logstash processors keep dying with "nil?" redirect?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vehyla</reporter><labels /><created>2014-11-07T23:11:05Z</created><updated>2014-11-08T12:17:49Z</updated><resolved>2014-11-08T12:17:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-08T12:17:49Z" id="62255785">Hi @Vehyla 

Please open your ticket on https://github.com/elasticsearch/logstash/issues instead

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ArrayIndexOutOfBoundsException in murmur hash with has_child</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8396</link><project id="" key="" /><description>Running 1.4.0 it is possible to hang ES.

Steps to reproduce:
Three processes in a while(true) loop.  One is using the _bulk API to insert (and update) a small random number of documents. The other two processes are executing fairly complex filteredQuery's.

I can run any of the two processes concurrently without problems, but when a the third one starts, ES fails with the exception below.

Other times, the exception doesn't occur but instead ES just hangs and I have to kill -9 its jvm.  

```
[2014-11-07 16:46:36,175][DEBUG][action.search.type       ] [Hardcore] [2848] Failed to execute query phase
org.elasticsearch.search.query.QueryPhaseExecutionException: [xxx.public.test.idxtest][2]: query[filtered(ConstantScore(++cache(_xmin:[2882514 TO 2882514]) +cache(_cmin:[* TO 0}) +cache(_xmax:[0 TO 0]) +cache(_xmax:[2882514 TO 2882514]) +cache(_cmax:[0 TO *]) +cache(_xmin_is_committed:T) +cache(_xmax:[0 TO 0]) +cache(_xmax:[2882514 TO 2882514]) +cache(_cmax:[0 TO *]) +NotFilter(cache(_xmax:[2882514 TO 2882514])) +cache(_xmax_is_committed:F) +CustomQueryWrappingFilter(child_filter[data/xact](filtered(ConstantScore(cache(BooleanFilter(_field_names:id))))-&gt;cache(_type:data)))))-&gt;cache(+_type:xact +org.elasticsearch.index.search.nested.NonNestedDocsFilter@38f048bd)],from[0],size[32768]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.executeScan(SearchService.java:245)
    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)
    at org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 13417
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:136)
    at org.elasticsearch.index.search.child.CustomQueryWrappingFilter.getDocIdSet(CustomQueryWrappingFilter.java:72)
    at org.elasticsearch.common.lucene.search.AndFilter.getDocIdSet(AndFilter.java:54)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:46)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:157)
    at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:542)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:136)
    at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:542)
    at org.apache.lucene.search.FilteredQuery$FilterStrategy.filteredBulkScorer(FilteredQuery.java:504)
    at org.apache.lucene.search.FilteredQuery$1.bulkScorer(FilteredQuery.java:150)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)
    ... 7 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 13417
    at org.apache.lucene.util.StringHelper.murmurhash3_x86_32(StringHelper.java:205)
    at org.apache.lucene.util.StringHelper.murmurhash3_x86_32(StringHelper.java:229)
    at org.apache.lucene.util.BytesRef.hashCode(BytesRef.java:143)
    at org.elasticsearch.common.util.BytesRefHash.add(BytesRefHash.java:151)
    at org.elasticsearch.index.search.child.ParentIdsFilter.createShortCircuitFilter(ParentIdsFilter.java:67)
    at org.elasticsearch.index.search.child.ChildrenConstantScoreQuery.createWeight(ChildrenConstantScoreQuery.java:127)
    at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:684)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:133)
    ... 21 more
```
</description><key id="48140359">8396</key><summary>ArrayIndexOutOfBoundsException in murmur hash with has_child</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Parent/Child</label><label>bug</label></labels><created>2014-11-07T22:48:50Z</created><updated>2014-12-24T09:11:21Z</updated><resolved>2014-12-24T09:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-09T22:54:46Z" id="62323991">This looks to me like field data for the _parent field data is buggy and returns an invalid BytesRef that has an `offset+end` that is greater than the length of the wrapped array. I will look into it.
</comment><comment author="jpountz" created="2014-11-10T12:24:56Z" id="62377284">I wrote a basic test hoping it could reproduce the issue, but got no luck. Here it is in case someone would like to iterate on it:

``` java
public class PCTests extends ElasticsearchIntegrationTest {

    private static final int NUM_PARENTS = 50000;
    private static final int NUM_CHILDREN = 500000;

    public void addOrUpdate() {
        final boolean parent = randomBoolean();
        if (parent) {
            final String parentId = Integer.toString(randomInt(NUM_PARENTS));
            client().prepareIndex("test", "parent", parentId).setSource(Collections.&lt;String, Object&gt;emptyMap()).get();
        } else {
            final int id = randomInt(NUM_CHILDREN);
            final int parentId = MathUtils.mod(MurmurHash3.hash(id), NUM_PARENTS);
            client().prepareIndex("test", "child", Integer.toString(id)).setParent(Integer.toString(parentId)).setSource("i", randomInt(1000)).get();
        }
    }

    public void query() {
        SearchResponse resp = client().prepareSearch("test").setSize(1).setQuery(QueryBuilders.hasChildQuery("child", QueryBuilders.rangeQuery("i").to(randomInt(1000)))).get();
        System.out.println(resp);
    }

    public void test() throws Exception {
        createIndex("test");
        client().admin().indices().preparePutMapping("test").setType("parent").setSource(JsonXContent.contentBuilder().startObject().startObject("parent").endObject().endObject()).get();
        client().admin().indices().preparePutMapping("test").setType("child").setSource(JsonXContent.contentBuilder().startObject().startObject("child")
                .startObject("_parent")
                .field("type", "parent")
            .endObject().endObject().endObject()).get();
        for (int i = 0; i &lt;= NUM_PARENTS; ++i) {
            client().prepareIndex("test", "parent", Integer.toString(i)).setSource(Collections.&lt;String, Object&gt;emptyMap()).get();
        }

        final Thread[] indexingThreads = new Thread[3];
        final AtomicInteger running = new AtomicInteger(indexingThreads.length);
        for (int i = 0; i &lt; indexingThreads.length; ++i) {
            indexingThreads[i] = new Thread() {
                public void run() {
                    for (int i = 0; i &lt; 5 * (NUM_CHILDREN + NUM_PARENTS); ++i) {
                        addOrUpdate();
                    }
                    running.decrementAndGet();
                }
            };
        }
        for (Thread t : indexingThreads) {
            t.start();
        }
        while (running.get() &gt; 0) {
            query();
            Thread.sleep(200);
        }
    }

}
```

Can you share more information about the setup that reproduces the issue:
- how many parent/child relations are configured in the mappings, are there some types that are both parent and child?
- how many parents and children in the index?
- what does the query look like? Given the stack trace, I guess it uses the has_child query?
- are there custom plugins installed?
- is the issue still reproducible after restarting elasticsearch?
- can you try to reproduce the failure with assertions enabled? (the `-ea` JVM option)

If sharing the index that triggers this behaviour is possible and the issue is easy to reproduce, that would be great.
</comment><comment author="aleph-zero" created="2014-11-10T18:31:31Z" id="62430561">I'm working with the reporter to get working code to reproduce.
</comment><comment author="clintongormley" created="2014-11-14T15:45:30Z" id="63082638">@aleph-zero any news on this?
</comment><comment author="s1monw" created="2014-11-23T12:55:12Z" id="64116980">@aleph-zero ping?
</comment><comment author="aleph-zero" created="2014-11-23T14:10:07Z" id="64119127">@clintongormley @s1monw I never got a working example from the reporter. I'll ping him back and see if I can at least get the general steps to reproduce.
</comment><comment author="aleph-zero" created="2014-12-03T17:53:51Z" id="65455344">@clintongormley @s1monw Still trying to get a proof-of-concept working
</comment><comment author="eeeebbbbrrrr" created="2014-12-18T17:19:02Z" id="67520993">Original reporter here...

@jpountz if you can provide some instruction how to actually run your test above (I have the ES repo cloned locally) I can fix it for you.  It looks like you're on the right track.

I've provided Chris Earle @ ES a set of shell scripts that recreates the problem (internal support number 6313) by poking at ES with libcurl.

Looking at your code above, you want 1 thread doing updates and two (or more) threads querying (the reverse of what you have now), and you'll want to run the query threads for a few minutes before the exceptions start to appear.

To answer your questions from above:
- two types in the index, only one is configured to be a child
- The query is quite complex (https://gist.github.com/eeeebbbbrrrr/d0491882248160e9afeb)
- We have custom plugins, but they don't need to be installed to re-create this
- Restarting ES "fixes" things until the test is run again
- With asserts enabled:  https://gist.github.com/eeeebbbbrrrr/708305a968acb13b32c3

The shell scripts I've provided to Chris Earle know how to create the exact index involved, with proper mappings, settings, and data.
</comment><comment author="eeeebbbbrrrr" created="2014-12-18T17:31:28Z" id="67522943">So I figured out how to run the test.  :)  Looks like this re-creates it:

``` java
import com.carrotsearch.hppc.hash.MurmurHash3;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.common.math.MathUtils;
import org.elasticsearch.common.xcontent.json.JsonXContent;
import org.elasticsearch.index.query.QueryBuilders;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.Test;

import java.util.Collections;
import java.util.concurrent.atomic.AtomicInteger;

public class PCTests extends ElasticsearchIntegrationTest {

    private static final int NUM_PARENTS = 50000;
    private static final int NUM_CHILDREN = 500000;
    private static final int MAX_QUERIES = 32768;

    public void addOrUpdate() {
        final boolean parent = randomBoolean();
        if (parent) {
            final String parentId = Integer.toString(randomInt(NUM_PARENTS));
            client().prepareIndex("test", "parent", parentId).setSource(Collections.&lt;String, Object&gt;emptyMap()).get();
        } else {
            final int id = randomInt(NUM_CHILDREN);
            final int parentId = MathUtils.mod(MurmurHash3.hash(id), NUM_PARENTS);
            client().prepareIndex("test", "child", Integer.toString(id)).setParent(Integer.toString(parentId)).setSource("i", randomInt(1000)).get();
        }
    }

    public void query() {
        SearchResponse resp = client().prepareSearch("test").setSize(1).setQuery(QueryBuilders.hasChildQuery("child", QueryBuilders.rangeQuery("i").to(randomInt(1000)))).get();
        System.out.println(resp);
    }

    @Test
    public void test() throws Exception {
        createIndex("test");
        client().admin().indices().preparePutMapping("test").setType("parent").setSource(JsonXContent.contentBuilder().startObject().startObject("parent").endObject().endObject()).get();
        client().admin().indices().preparePutMapping("test").setType("child").setSource(JsonXContent.contentBuilder().startObject().startObject("child")
                .startObject("_parent")
                .field("type", "parent")
                .endObject().endObject().endObject()).get();
        for (int i = 0; i &lt;= NUM_PARENTS; ++i) {
            client().prepareIndex("test", "parent", Integer.toString(i)).setSource(Collections.&lt;String, Object&gt;emptyMap()).get();
        }

        logger.info("Bootstrapping index...");
        // bootstrap index with some data
        for (int i = 0; i &lt;NUM_PARENTS; ++i) {
            addOrUpdate();
        }

        final Thread[] indexingThreads = new Thread[3];
        final AtomicInteger running = new AtomicInteger(indexingThreads.length);
        for (int i = 0; i &lt; indexingThreads.length; ++i) {
            indexingThreads[i] = new Thread() {
                public void run() {
                    for (int i = 0; i &lt; MAX_QUERIES; ++i) {
                        query();
                    }
                    running.decrementAndGet();
                }
            };
        }

        logger.info("Starting test");
        for (Thread t : indexingThreads) {
            t.start();
        }
        while (running.get() &gt; 0) {
            addOrUpdate();
            Thread.sleep(200);
        }
    }

}
```

I think IDEA's output window truncated the exception, but you can see that it's there:

``` json
{
  "took" : 15,
  "timed_out" : false,
  "_shards" : {
    "total" : 10,
    "successful" : 9,
    "failed" : 1,
    "failures" : [ {
      "index" : "test",
      "shard" : 9,
      "status" : 500,
      "reason" : "RemoteTransportException[[node_2][local[3]][indices:data/read/search[phase/query/id]]]; nested: QueryPhaseExecutionException[[test][9]: query[child_filter[child/parent](filtered(i:[* TO 204])-&gt;_type:child)],from[0],size[1]: Query Failed [Failed to execute main query]]; nested: RuntimeException[java.lang.AssertionError]; nested: AssertionError; "
    } ]
  },
  "hits" : {
    "total" : 4254,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "parent",
      "_id" : "9280",
      "_score" : 1.0,
      "_source" : { }
    } ]
  }
}
```
</comment><comment author="eeeebbbbrrrr" created="2014-12-18T18:40:14Z" id="67533205">Oh, it's worth mentioning that ES does _not_ hang.  My original report to @aleph-zero said it did, but further diagnosis on my end found that it was my code that was looping indefinitely when it encountered the side effects of this exception (scan+scroll not returning the expected # of results, for example).

The AIOOB is very real, however.  :)
</comment><comment author="jpountz" created="2014-12-22T14:32:32Z" id="67842816">Thanks for making the bug reproducible, it helped a lot!
</comment><comment author="eeeebbbbrrrr" created="2014-12-22T16:01:03Z" id="67853477">I built your fix/ branch and can confirm this indeed fixes up my standalone test cases.

I'm glad you wrapped ParentChildIndexFieldData.java in a threaded test as it definitely missed the concurrency boat.  Makes me wonder how it ever worked?
</comment><comment author="jpountz" created="2014-12-22T16:27:58Z" id="67857050">This is a bug that I introduced in elasticsearch 1.4 since we almost completely rewrote fielddata to be more in-line with Lucene (this removed a lot of useless wrapping when using doc values, and helped improve the performance of doc values when used from elasticsearch).
</comment><comment author="eeeebbbbrrrr" created="2014-12-23T21:21:42Z" id="67999864">That makes sense.  I didn't dig through the commit history but I was thinking that the concurrency responsibly had to be happening elsewhere at some point in the past.

Thanks for taking care of this so quickly.  Happy Holidays!
</comment><comment author="jpountz" created="2014-12-24T09:11:21Z" id="68038648">You're welcome, happy holidays to you too!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java</file></files><comments><comment>Parent/child: Fix concurrency issues of the _parent field data.</comment></comments></commit></commits></item><item><title>Support for shard level caching of term vectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8395</link><project id="" key="" /><description>This commits adds caching support to the Term Vectors API. A new `_cache` body
parameter is introduced in the term vector request. When set to `true`, the
shard query cache is solicited so to keep the same near real-time promise as
uncached requests. This caching mechanism makes sense in a MLT scenario in
which the same term vector request is performed multiple times, once per
shard, or when the request is especially expensive, for example when asking
for term statistics or distributed frequencies.

In order to keep the real-time promise of the term vectors API, caching is to
`false` by default. Additionally term vector requests are now timed.
</description><key id="48112953">8395</key><summary>Support for shard level caching of term vectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>stalled</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T18:10:46Z</created><updated>2015-07-06T22:35:06Z</updated><resolved>2015-07-06T22:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-17T23:26:06Z" id="63396285">I would be ok to push such a work-around temporarily if it is needed for performance reasons (is it or does the fs cache already do a good job?), but I think the right fix would be to parse the mlt query and fetch term vectors on the coordinating node only, and then to send to shards all the information that they need to find similar documents? I know this is a high hanging fruit given the current design, but this would also be useful to other areas of elasticsearch...
</comment><comment author="alexksikes" created="2014-11-20T10:16:08Z" id="63787170">Yes I agree and this would provide a first hand solution to deprecating MLT API. Also note that caching is set to `false` by default. There are also some plans of reducing MLT to TVs with dfs only. Beyond MLT this caching mechanism might be useful, think of a blog that generates a word cloud of its top entries, each time the user hit the front page. @s1monw WDYT?

Should we use the same configuration parameters as Query Cache, or should this have its own independent set of parameters?
</comment><comment author="jpountz" created="2014-11-27T09:33:32Z" id="64765878">Something that worries me a bit here is that this is using the same cache as the query cache. And since cache keys consist of the json request and the reader version, you could have collisions on requests that would be valid for both the `_search` and `_termvectors` apis.
</comment><comment author="s1monw" created="2014-11-28T11:36:51Z" id="64885048">I agree with @jpountz I think we should wrap the TV request with a special cache key prefix to make sure we don't collide with the search request, would that fix your concerns?
</comment><comment author="jpountz" created="2014-11-28T14:08:44Z" id="64897404">Not sure about prefixing, what about adding a new property to IndicesQueryCache.Key to describe what the cache value is? This could eg. be `_search` for search requests and `_termvectors` for termvectors requests?
</comment><comment author="alexksikes" created="2014-11-28T14:15:22Z" id="64898012">That seems to be a cleaner solution, I'm ok either way.
</comment><comment author="jpountz" created="2014-12-04T10:17:04Z" id="65611240">@alexksikes Left minor comments. Can you also add tests to this PR (preferably unit tests)?
</comment><comment author="jpountz" created="2014-12-04T10:20:05Z" id="65611611">I also think we should raise an error when both "realtime" and "cache" are true since they are mutually exclusive?
</comment><comment author="alexksikes" created="2014-12-05T10:11:40Z" id="65770157">Actually they are not quite mutually exclusive as having both "realtime" and "cache" set to true, will generate the term vectors from the document in the transaction log on the first request, if no other request like it has been performed. So it is realtime on first unseen request, and then NRT on subsequent requests because of the cache.
</comment><comment author="jpountz" created="2014-12-05T11:18:48Z" id="65777069">If you specify `realtime: true` and get results for old data, I think it's a bug. I would rather fail requests that ask for both realtime results and caching.
</comment><comment author="alexksikes" created="2014-12-05T13:05:03Z" id="65787039">Sounds good but one issue I'm not completely satisfied about is that since `realtime: true` is the default, then to use the cache, the user would have to set two options `realtime: false` and `_cache: true`. But I think it is fine this way. Thanks for the comments!
</comment><comment author="jpountz" created="2014-12-17T12:08:55Z" id="67313638">To be honest I'm worried about the complexity that we are adding here compared to the value since the option cannot be on by default. I think we already have too much complexity in elasticsearch and should not add more unless necessary. So I'd vote to close this PR and resurrect it if there are performance issues due to multiple calls to the term vectors API that would be solved with the cache.
</comment><comment author="alexksikes" created="2014-12-17T13:08:22Z" id="67319421">That's a possibility but I don't think it adds that much complexity. I'm noting that we go from 30ms (file system cache) to 1ms on most requests when this caching mechanism is on, which it will be by default when performing a MLT query.

Also, one of the raison d'être of this PR is to deprecate the MLT API. So either we push this PR or we fix the problems with the current MLT API with this other PR #8028, or we do both. I let @s1monw do the final vote on this one.
</comment><comment author="alexksikes" created="2015-07-06T22:04:47Z" id="119009698">Closing in favor of https://github.com/elastic/elasticsearch/issues/10217
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Large index no longer initialises under 1.4.0 and 1.4.0 Beta 1 due to OutOfMemoryException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8394</link><project id="" key="" /><description>We have one particularly large index in our cluster - it contains 10s of millions of documents and has quite a lot of nesteds too. Prior to 1.4.0 Beta 1 (including 1.2.x and 1.3.x) the index re-initialised on a node with 8GB allocated to ElasticSearch (16GB+ available in OS). Since 1.4.0 Beta 1 (and still on 1.4.0) we're getting an OOM exception (startup log and exception stack below). At this point, the node ceases recovery (expected, I guess) and becomes unresponsive. All data nodes suffer the same fate and the entire cluster becomes unresponsive.

```
[2014-11-07 17:12:39,895][WARN ][common.jna               ] unable to link C library. native methods (mlockall) will be disabled.
[2014-11-07 17:12:40,077][INFO ][node                     ] [dvlp_FRONTEND2] version[1.4.0], pid[9052], build[bc94bd8/2014-11-05T14:26:12Z]
[2014-11-07 17:12:40,077][INFO ][node                     ] [dvlp_FRONTEND2] initializing ...
[2014-11-07 17:12:40,129][INFO ][plugins                  ] [dvlp_FRONTEND2] loaded [cloud-aws], sites [bigdesk, head, inquisitor, kopf]
[2014-11-07 17:12:45,220][INFO ][node                     ] [dvlp_FRONTEND2] initialized
[2014-11-07 17:12:45,220][INFO ][node                     ] [dvlp_FRONTEND2] starting ...
[2014-11-07 17:12:45,438][INFO ][transport                ] [dvlp_FRONTEND2] bound_address {inet[/0:0:0:0:0:0:0:0:50882]}, publish_address {inet[FRONTEND2/192.168.10.73:50882]}
[2014-11-07 17:12:45,452][INFO ][discovery                ] [dvlp_FRONTEND2] dvlp/C2f-euXcRc-cEv3dnsBnXw
[2014-11-07 17:13:15,451][WARN ][discovery                ] [dvlp_FRONTEND2] waited for 30s and no initial state was set by the discovery
[2014-11-07 17:13:15,468][INFO ][http                     ] [dvlp_FRONTEND2] bound_address {inet[/0:0:0:0:0:0:0:0:50881]}, publish_address {inet[frontend2/192.168.10.73:50881]}
[2014-11-07 17:13:15,468][INFO ][node                     ] [dvlp_FRONTEND2] started
[2014-11-07 17:13:48,552][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]
[2014-11-07 17:14:51,597][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]
[2014-11-07 17:15:54,633][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]
[2014-11-07 17:16:57,647][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]
[2014-11-07 17:18:00,664][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]
[2014-11-07 17:19:03,675][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]
[2014-11-07 17:20:06,684][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]
[2014-11-07 17:20:36,950][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [NodeDisconnectedException[[dvlp_FRONTEND2_coordinator][inet[/192.168.10.73:55591]][internal:discovery/zen/join] disconnected]]
[2014-11-07 17:20:41,171][WARN ][transport.netty          ] [dvlp_FRONTEND2] Message not fully read (response) for [85] handler future(org.elasticsearch.transport.EmptyTransportResponseHandler@2060e2c8), error [true], resetting
[2014-11-07 17:20:41,171][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND1_coordinator][4y8Hh5kAQPK2Ie3gzc58Ww][FRONTEND1][inet[/192.168.10.70:55858]]{datacentrename=site1, data=false, nodename=dvlp_FRONTEND1_coordinator, master=true}], reason [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]
[2014-11-07 17:20:45,520][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [RemoteTransportException[[dvlp_FRONTEND2_coordinator][inet[/192.168.10.73:55591]][internal:discovery/zen/join]]; nested: ElasticsearchIllegalStateException[Node [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[FRONTEND2/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}] not master for join request from [[dvlp_FRONTEND2][C2f-euXcRc-cEv3dnsBnXw][FRONTEND2][inet[/192.168.10.73:50882]]{datacentrename=site2, nodename=dvlp_FRONTEND2, master=false}]]; ], tried [3] times
[2014-11-07 17:20:48,831][INFO ][cluster.service          ] [dvlp_FRONTEND2] detected_master [dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}, added {[dvlp_DEVBATH01.exabre.co.uk_loadbalancer][8i4izXAUQiWeS2arwV9LeA][DEVBATH01][inet[/192.168.10.65:12184]]{datacentrename=site1, data=false, nodename=dvlp_DEVBATH01.exabre.co.uk_loadbalancer, master=true},[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true},[dvlp_FRONTEND2_loadbalancer][joVXc_fGTx-SC_YwJ2YBmQ][FRONTEND2][inet[/192.168.10.73:65341]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_loadbalancer, master=false},[dvlp_FRONTEND1_loadbalancer][snDHwo0YTR6VsAFV9nBcxw][FRONTEND1][inet[/192.168.10.70:55054]]{datacentrename=site1, data=false, nodename=dvlp_FRONTEND1_loadbalancer, master=false},}, reason: zen-disco-receive(from master [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}])
[2014-11-07 17:21:01,937][INFO ][cluster.service          ] [dvlp_FRONTEND2] added {[dvlp_FRONTEND1_coordinator][4y8Hh5kAQPK2Ie3gzc58Ww][FRONTEND1][inet[/192.168.10.70:55858]]{datacentrename=site1, data=false, nodename=dvlp_FRONTEND1_coordinator, master=true},}, reason: zen-disco-receive(from master [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}])
[2014-11-07 17:25:25,598][INFO ][monitor.jvm              ] [dvlp_FRONTEND2] [gc][old][739][27] duration [8s], collections [1]/[9s], total [8s]/[8.8s], memory [7.8gb]-&gt;[7.7gb]/[7.9gb], all_pools {[young] [172.4mb]-&gt;[46.5mb]/[199.6mb]}{[survivor] [24.9mb]-&gt;[0b]/[24.9mb]}{[old] [7.6gb]-&gt;[7.7gb]/[7.7gb]}
[2014-11-07 17:25:46,387][INFO ][monitor.jvm              ] [dvlp_FRONTEND2] [gc][old][746][32] duration [5s], collections [1]/[6s], total [5s]/[23.6s], memory [7.9gb]-&gt;[7.9gb]/[7.9gb], all_pools {[young] [195mb]-&gt;[199.6mb]/[199.6mb]}{[survivor] [0b]-&gt;[10.9mb]/[24.9mb]}{[old] [7.7gb]-&gt;[7.7gb]/[7.7gb]}
[2014-11-07 17:28:16,136][WARN ][index.warmer             ] [dvlp_FRONTEND2] [dvlp_13_67_item_20140410][7] failed to load fixed bitset for [org.elasticsearch.index.search.nested.NonNestedDocsFilter@fd00879d]
org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.getAndLoadIfNotPresent(FixedBitSetFilterCache.java:139)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.access$100(FixedBitSetFilterCache.java:75)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer$1.run(FixedBitSetFilterCache.java:287)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.OutOfMemoryError: Java heap space
    at org.apache.lucene.util.FixedBitSet.&lt;init&gt;(FixedBitSet.java:187)
    at org.apache.lucene.search.MultiTermQueryWrapperFilter.getDocIdSet(MultiTermQueryWrapperFilter.java:104)
    at org.elasticsearch.common.lucene.search.NotFilter.getDocIdSet(NotFilter.java:49)
    at org.elasticsearch.index.search.nested.NonNestedDocsFilter.getDocIdSet(NonNestedDocsFilter.java:46)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:142)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:139)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
    ... 8 more
[2014-11-07 17:28:29,215][INFO ][monitor.jvm              ] [dvlp_FRONTEND2] [gc][old][749][40] duration [22.9s], collections [4]/[2.3m], total [22.9s]/[1m], memory [7.9gb]-&gt;[7.9gb]/[7.9gb], all_pools {[young] [199.5mb]-&gt;[199.6mb]/[199.6mb]}{[survivor] [22.9mb]-&gt;[23.1mb]/[24.9mb]}{[old] [7.7gb]-&gt;[7.7gb]/[7.7gb]}
[2014-11-07 17:28:23,797][WARN ][index.warmer             ] [dvlp_FRONTEND2] [dvlp_13_67_item_20140410][7] failed to load fixed bitset for [org.elasticsearch.index.search.nested.NestedDocsFilter@fd00879d]
org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.getAndLoadIfNotPresent(FixedBitSetFilterCache.java:139)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.access$100(FixedBitSetFilterCache.java:75)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer$1.run(FixedBitSetFilterCache.java:287)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.OutOfMemoryError: Java heap space
    at org.apache.lucene.util.FixedBitSet.&lt;init&gt;(FixedBitSet.java:187)
    at org.apache.lucene.search.MultiTermQueryWrapperFilter.getDocIdSet(MultiTermQueryWrapperFilter.java:104)
    at org.elasticsearch.index.search.nested.NestedDocsFilter.getDocIdSet(NestedDocsFilter.java:50)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:142)
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:139)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
    ... 8 more
```
</description><key id="48111142">8394</key><summary>Large index no longer initialises under 1.4.0 and 1.4.0 Beta 1 due to OutOfMemoryException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">andrassy</reporter><labels><label>feedback_needed</label></labels><created>2014-11-07T17:51:58Z</created><updated>2015-02-28T04:56:22Z</updated><resolved>2015-02-28T04:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrassy" created="2014-11-07T18:15:42Z" id="62188111">A little bit of digging in the code and I came across the "index.load_fixed_bitset_filters_eagerly" setting. Setting this to false seems to avoid my initial problem. Has the default changed? Is this something new? Are there any impacts I might need to look out for in setting this to false?
</comment><comment author="martijnvg" created="2014-11-07T18:17:24Z" id="62188358">Hey @andrassy how many nested object fields do you have in all your mappings?

Since 1.4 we eagerly load the filters and keep them around to make nested query execution as fast as possible. Under the hood the nested query relies on the fact that these filters are in memory as bitsets.

The `index.load_fixed_bitset_filters_eagerly` setting has been added to disable the eager loading, but at some point when running nested queries these filters will end up in the heap as bitsets. Maybe disabling in your case make sense in the case if you have many nested object fields, but not all are used.
</comment><comment author="andrassy" created="2014-11-07T18:31:48Z" id="62190361">_stats reports doc count just above 600 million docs for the index (which includes the nesteds, right?) - 10 shards across 5 data nodes at  present. There are quite a few nested mappings which we do use, but I think that we're probably not hitting the full parent doc set due to other filters being applied when we actually query - would that keep the bit filter caches smaller? It's just that we don't seem to have hit any OOM limits recently, operating with 1.3.x and prior versions for some time. 

We could restructure the data to avoid many of the nested mappings I think but this'll take us some time :( and involve some code changes right the way up our stack. We'll try with the index.load_fixed_bitset_filters_eagerly setting as false and see how we get on. 

Thought it was worth sharing the issue here. Thanks for the rapid response @martijnvg!
</comment><comment author="martijnvg" created="2014-11-07T18:41:08Z" id="62191676">@andrassy Sharing this is really important! ES may needs to change its default behaviour when it comes to eager loading filters associated with nested object fields.

Yes, the doc count does include nested documents. You said you have quite a few nested object fields. Can you give share how many nested fields you have (check the mapping) ? (or an estimation)

In the node stats api we also expose how much the bit set filter is taking (under the fixed_bit_set_memory_in_bytes key). Are you able to check this?
</comment><comment author="andrassy" created="2014-11-07T18:51:20Z" id="62193102">We have three types within the index with 5, 5, and 4 (totalling 14) nested properties.

fixed_bit_set_memory_in_bytes currently says 0, but I only just started to recover with load_fixed_bitset_filters_eagerly set to false. I'll check again once we've seen some traffic - it'll probably be Monday now as it's our DEV box and everyone else went home already :D
</comment><comment author="martijnvg" created="2014-11-07T19:03:39Z" id="62194995">Ok, would be great to know how much `fixed_bit_set_memory_in_bytes` is being reported.

Do you by any chance also have a `_parent` fields configured in your mappings? Each parent type increases the entries in the bitset cache.

Also beyond that do you have any other warming configured? (warmer queries, eager field data loading)
</comment><comment author="martijnvg" created="2014-11-07T19:54:32Z" id="62202379">Also if you are able to share your mappings (or a dummy mapping that show the structure of your nested object fields) that would be helpful to see if we can improve this. Having 14 types and 600M docs shouldn't result in a OOM with your available heap space.
</comment><comment author="martijnvg" created="2014-11-14T20:45:26Z" id="63126327">By changing the default eager loading behaviour and changing the dependency on bitset based filters in nested and parent/child via the following issues: #8454, #8440 and #8414, running out of memory like happened here will not happen anymore.
</comment><comment author="portante" created="2014-12-05T18:45:23Z" id="65835045">I just upgraded from 1.3.2-1 to 1.4.1 and am seeing the following OOMs:

```
[2014-12-05 13:32:14,166][WARN ][index.warmer             ] [Patriots] [foo.bar-20140831][0] failed to load fixed bitset for [org.elasticsearch.index.search.nested.NonNestedDocsFilter@a801f786]  
org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.OutOfMemoryError: Java heap space  
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)  
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)  
        at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)  
        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.getAndLoadIfNotPresent(FixedBitSetFilterCache.java:137)  
        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.access$100(FixedBitSetFilterCache.java:73)  
        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer$1.run(FixedBitSetFilterCache.java:278)  
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  
        at java.lang.Thread.run(Thread.java:745)  
Caused by: java.lang.OutOfMemoryError: Java heap space  
```

Is this related to this problem?  And if so, do I have to change something else for my indexes, or should this change in 1.4.1 have fixed this already?

See also my comment in: https://github.com/elasticsearch/elasticsearch/issues/8487
</comment><comment author="martijnvg" created="2014-12-05T21:46:25Z" id="65859639">@portante ES version 1.4.1 should have fixed OOM issue related to the fixed bitset cache.

if possible can you share the following:
- Your mappings: localhost:9200/_mappings
- Cluster stats: localhost:9200/_cluster/stats?human&amp;pretty
</comment><comment author="portante" created="2014-12-05T22:30:35Z" id="65865124">@martijnvg: Loaded the above in the following gist: https://gist.github.com/portante/711aa2428461a7485384

I did not provide all the mappings for each index, instead I gave you one representative one of each type, sosreport, sar, and marvel (which is already known).

I also provided a /_cat/shards output so you can see the relative sizes of the indexes.  The vos.sar-\* indexes are about 10 - 13 GB, while all the others seem to be in sub-1 GB ranges.

I have successfully loaded all .marvel_, tvos._, vos.sosreport-\* indexes, but have been unsuccessful with the vos.sar-\* indexes.
</comment><comment author="martijnvg" created="2014-12-05T23:34:40Z" id="65871792">I see that the fixed bitset cache already takes 10GB and many of your shards are not started. In total you have assigned 206GB of jvm heap to ES, which feels more than sufficient, so I don't see directly why you would run OOM. However in general this amount of heap for a single node is too high and should be split across more nodes (can be on the same physical machine). That being sad this shouldn't result in the situation you're in now. 

Also the vos.sar-20141019 index has in total 14 unique nested object fields. Do the other indices have the same nested fields? And how many Lucene documents (this different than the number of documents in ES when nested fields have been defined in the mapping) do those indices have in total (more or less)? This can be found in the indices stats api under docs.stats?

As I commented earlier here ES since 1.4 loads a data-structure eagerly in memory in order for nested query/filter and nested aggregations to run fast. (not loading it when it is needed). 

In order to get all shards started I recommend setting `index.load_fixed_bitset_filters_eagerly` to `false` in your elasticsearch.yml file and restart. This disables the eager loading and prevents the OOM caused by the stack trace you send earlier.
</comment><comment author="martijnvg" created="2014-12-05T23:42:08Z" id="65872413">@portante It is better to run the indices stats after you configured the mentioned setting and the cat indices api maybe provides a better view to the metric: localhost:9200/_cat/indices/vos.sar-*
</comment><comment author="portante" created="2014-12-06T03:24:34Z" id="65883607">@martijnvg, can you explain why having more memory is too much?  I can certainly break this up, but that seems counter intuitive.

All the vos.sar-\* indices CAN have 14 unique indexes.  Most about about 6 - 8, if I understand the data set correctly.

In the provided gist, you can see that value: https://gist.github.com/portante/711aa2428461a7485384#file-shards-cat-L71

Each indexed sar document represents one sample collected as reported by the `sadf` command from sysstat.  On some systems they might collect 144 samples a day (10 min intervals), some have 8,600+ samples (10 seconds per day).  What really seems to affect the size of things is the number of nested elements.  We have seen VM hosts with close to 1,000 nics servicing VMs, which ends up have one per net-dev and net-edev docs.  Or they might have 400+ block devices ending up with that many nested docs for disks.

I had disabled the index warmers on those large indexes as a work-around.  After enabling the warmers, and applying the setting above, the instance now takes about a 3 minutes to load up from ES start.

Much better. Thanks!
</comment><comment author="martijnvg" created="2014-12-07T21:27:02Z" id="65955901">@portante This is the reason: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/heap-sizing.html#compressed_oops

Now that all shards are started can you share how much docs all the vos.sar-\* indices have? 
Best way to share this is via running: `curl 'localhost:9200/_cat/shards/vos.sar-*'`. 
This can give a good indication how much heap memory the fixed bitset cache will take if everything is being loaded.
</comment><comment author="portante" created="2014-12-08T16:23:53Z" id="66140869">@martijnvg, I have updated that gist with the output requested above (using wildcards did not work on the _cat command for some reason for me), see https://gist.github.com/portante/711aa2428461a7485384#file-shards-txt

I'll have to think about the compressed_oops and how we can restructure to take advantage of that.  It seems like this would be a nice feature to have for ES where it would break itself up into smaller instances automatically instead of having to require the users to do it.
</comment><comment author="clintongormley" created="2015-02-28T04:56:22Z" id="76510723">I think this ticket can be closed now? Feel free to reopen if more discussion is needed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search Template - parse template if it is provided as a single escaped string.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8393</link><project id="" key="" /><description>I was trying to implement fix for #8308

I think I nailed it down. It seems it is combination of two related issues:

1) The `TemplateQueryParser.java` was not correctly parsing the request when the `template` contained a single `VALUE_STRING` token. I think this could not have been working before (obviously there were no tests for this use case). I improved the main `parse` method to detect string token. **Please review** namely the complex `if` conditions - I am sure there is a way how to express them in more elegant way.

2) The second issue is with `SearchService.java` in `parseTemplate` method where it tries to parse the template for second time. When I commented this part out then all the tests that I added to `TemplateQueryTest.java` started to pass. **Please review** validity of commenting this out. I did not notice any tests that would be broken due to this but the chance is that there were no tests covering the logic behind second parsing pass.

Looking for the feedback.
</description><key id="48106581">8393</key><summary>Search Template - parse template if it is provided as a single escaped string.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>:Search Templates</label></labels><created>2014-11-07T17:16:36Z</created><updated>2015-06-24T06:29:55Z</updated><resolved>2015-06-23T17:55:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2014-11-13T17:56:28Z" id="62936176">Thanks for comments @MaineC I can update this PR accordingly.
Any idea what to do about the code I commented out in `SearchService.java`?
</comment><comment author="lukas-vlcek" created="2014-11-21T15:13:27Z" id="63983133">Bump. Is it my turn now? I think we still need review/comments on the changes I did in `SearchService.java`. I feel really strange that just commenting out some part of the code will do the trick.  I would expect that we can get more clue about what this part of code is used for and get more understanding how to improve it for this use case. Once we know it I will be happy to update this PR and rebase it.
</comment><comment author="MaineC" created="2014-11-24T17:06:19Z" id="64226667">I went back to the original author of the lines you commented out. Turns out those are tested only in the REST test suite, not in the unit tests (probably should add one unit test for them to fail early in such cases). The case this piece of code is supposed to handle are template requests of the form

GET _search/template
{
   "template" : {
     "id" : "mytemplate"
   }
}

You can execute the REST tests yourself by running

mvn clean test -Dtests.class=org.elasticsearch.test.rest.ElasticsearchRestTests
</comment><comment author="lukas-vlcek" created="2014-11-24T18:18:11Z" id="64238104">Thanks @MaineC this will help!
</comment><comment author="MaineC" created="2014-11-25T07:41:57Z" id="64320977">For reference: https://github.com/elasticsearch/elasticsearch/blob/master/TESTING.asciidoc has more information on the test suite itself.

One thing - when running the tests from Maven you might run into complaints at build time due to the way the patch is formatted - there are "please no tabs" etc. warnings built-in.
</comment><comment author="lukas-vlcek" created="2014-11-25T07:53:05Z" id="64321802">Thanks @MaineC I will reformat the code with next PR update (I was aware of this, the goal of the first iteration of this PR was not to deliver perfect code but start the discussion and verify I am on the right track to fix this, which seems to be the case).
</comment><comment author="lukas-vlcek" created="2014-11-25T15:43:49Z" id="64419210">@MaineC I updated this PR, do you think you can give it another look?

I think I found how to update the `SearchService.java`. The trick is that the part I commented out in previous PR is back but it is needed to close the second parsing logic into `try ... catch` clause and _ignore_ specific exception. The bottom line is that in such case the template is a single string value which is not valid JSON (due to conditional clauses - thus the parser fails). However, this string value is later processed by script engine (which is expected). I put relevant comments into the code. I also added relevant unit tests. As of now the `org.elasticsearch.test.rest.ElasticsearchRestTests` is passing for me as well as all the tests I added.

In simple words, I think the issue can be fixed by this PR but honestly the code is getting quite complex and it smells a bit IMO. I can imagine this can be implemented in a better way but I am afraid it would require some higher level refactoring. The `template` thing is a bit messy now.

At least I think I clearly demonstrated what the problem is. Thanks!
</comment><comment author="GaelTadh" created="2014-11-26T15:28:12Z" id="64661774">@MaineC @lukas-vlcek I'm going to spend some time re-thinking the template parsing code here to take this into account and try and do it in a cleaner way.
</comment><comment author="lukas-vlcek" created="2014-11-26T16:01:37Z" id="64667402">@GaelTadh +1
Don't hesitate to ping me if you need any explanation about the tests I added in this PR. I think they are valuable and they could be used in the future.
</comment><comment author="lukas-vlcek" created="2014-12-12T14:27:13Z" id="66778613">@GaelTadh Did you have a chance to look at this?

In the meantime I started using my fix (similar to the one proposed in this PR) and it seems I hit one more issue with search templates. The problem symptom is as follows:

```
Caused by: org.elasticsearch.index.mapper.MapperParsingException: object mapping for [query] tried to parse as object, but got EOF, has a concrete value been provided to it?
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:498) [elasticsearch-patched-1.4.1.jar:]
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:541) [elasticsearch-patched-1.4.1.jar:]
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:490) [elasticsearch-patched-1.4.1.jar:]
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:413) [elasticsearch-patched-1.4.1.jar:]
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:189) [elasticsearch-patched-1.4.1.jar:]
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:511) [elasticsearch-patched-1.4.1.jar:]
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:419) [elasticsearch-patched-1.4.1.jar:]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [rt.jar:1.7.0_55]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [rt.jar:1.7.0_55]
    ... 1 more
```

It seems the problem here is when client try to mix templates in both formats: the one using escaped string value and regular JSON value. It seems that in such case this can clash in the mapping when query is stored internally in Elasticsearch. I wonder if there is an easy solution to this that I could use?

For now I workarounded this on my side by allowing only templates in form of escaped strings. So even if the query does not need to use conditional clauses (which leads to invalid JSON) then the client is required to provide it as escaped string.
</comment><comment author="lukas-vlcek" created="2014-12-12T15:15:33Z" id="66785014">@GaelTadh Sorry about this, the symptom was on my side only.
I was storing the query into ES index. The symptom is gone after I changed mapping of the `query` field to `{ "type" : "object", "enabled" : false }`.
</comment><comment author="lukas-vlcek" created="2015-03-21T16:18:17Z" id="84382693">Bump, is there any plan to have this issue fixed please? `1.5`, `1.6` ... ?
</comment><comment author="MaineC" created="2015-04-20T07:12:58Z" id="94377814">&gt; The template parsing logic was a bit dirty IMO (my changes made it even more messy) 

Actually the template related part of the DSL isn't quite consistent IMHO:

This is how you refer to a template stored in the index:

```
"template": {
    "id": "my_template", 
    "params" : {
        "query_string" : "all about search"
    }
}
```

This is how you refer to a template stored in a file:

```
"template": {
    "file": "my_template", 
    "params" : {
        "query_string" : "all about search"
    }
}
```

This is how you submit a templated query in the request itself:

```
"template": {
    "query": { "match": { "text": "{query_string}" }}},
    "params" : {
        "query_string" : "all about search"
    }
}
```

In each case a template consists of two parts: One being the template string (or a pointer to it), the other being the parameters to fill the template.

For search request templates however we accept the following:

```
"template" : {
      "query": { "match" : { "field" : "value" } }, 
      "size" : "{{my_size}}"
},
"params" : {
    "my_size" : 5
}
```

Having seen the first three examples I would have assumed the third one to look more like the following:

```
"template" : {
    "request": {
         "query": { "match" : { "field" : "value" } }, 
         "size" : "{{my_size}}"
    },
    "params" : {
        "my_size" : 5
    }
}
```

Changing the format of the last request to look similar to the first three would simplify the PR (less/no changes in "TemplateQueryParser", especially the complicated conditional at line 115 goes away*) but would change the DSL. Not sure if that change would actually make things easier to read for other users or whether that's just my personal preference speaking. @clintongormley @lukas-vlcek what do you think?

.\* I got to the point of having all unit tests including the ones in the PR green, REST tests still pending, but wanted to get your feedback before investing more time.
</comment><comment author="MaineC" created="2015-04-23T09:08:58Z" id="95500448">Mental note: the naming of the parameters above is related to #9995
</comment><comment author="clintongormley" created="2015-04-26T14:13:54Z" id="96388815">@MaineC you raise a good point.... eg for scripts we have:

```
{ "script": {
    "script | script_id | script_file" : "....",
    "params": {...}
}}
```

I know that we've just unified the script parameters, but I'm tempted to have another round, doing something like this:

```
"template": {
    "inline | template_id | template_file": "..."
    "params": {...}
}
```

and:

```
"script": {
    "inline | script_id | script_file": "..."
    "params": {...}
}
```

@javanna what do you think?
</comment><comment author="javanna" created="2015-04-28T08:43:39Z" id="96979305">I like your proposal Clint, it is a bit of a mess api wise though because we just changed it... more bw layers... how about `inline | id | file` in both cases then given that the ancestor says what the element is?

That said we would also need the 4th option for search templates, that supports the whole search request, not just queries?
</comment><comment author="MaineC" created="2015-04-28T08:59:08Z" id="96982041">&gt; That said we would also need the 4th option for search templates, that supports the whole search 
&gt; request, not just queries?

I need to double-check the actual code - in an ideal world I'd actually like to see only the parameter names inline/id/file. 

In my proposal above I used a separate parameter name for template queries and templated search requests because we already had the parameter name "query" and it felt awkward to stuff what is really a search request into something called "query".
</comment><comment author="clintongormley" created="2015-04-28T09:41:17Z" id="96990193">@javanna I also prefer `inline|id|file` instead  of the redundant `script_id|script_file` etc.  Agreed that it is a pain because we've just changed it, but probably worth getting this right for 2.0.

For search request templates, we could do the same thing, eg:

# inline

```
GET /_search/template
{
  "template": {
    "inline": {
      "query": {
        "match": {
          "{{my_field}}": "{{my_value}}"
        }
      },
      "size": "{{my_size}}"
    },
    "params": {
      "my_field": "foo",
      "my_value": "bar",
      "my_size": 5
    }
  }
}
```

# file

```
GET /_search/template
{
  "template": {
    "file": "my_template",
    "params": {
      "my_field": "foo",
      "my_value": "bar",
      "my_size": 5
    }
  }
}
```

Wondering though if need the top level `template` element, as we already have `template` in the URL.  
</comment><comment author="MaineC" created="2015-04-28T09:46:26Z" id="96991891">&gt; Wondering though if need the top level template element, as we already have template in the URL.

IMHO we really only need it here:

http://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-template-query.html

(again w/o having looked at the implications on the implementation but only on the API)
</comment><comment author="javanna" created="2015-04-28T09:53:39Z" id="96995289">I think the get search template should be in sync with what get indexed script does though (it is effectively the same api)? I think we have now the same content but a different object, either called `template` or `script`.
</comment><comment author="MaineC" created="2015-05-04T08:35:19Z" id="98635032">&gt; I think the get search template should be in sync with what get indexed script does though (it is effectively the same api)?

+1

So to summarise - for master I change the current API to the following format:

# inline using the template endpoint:

```
GET /_search/template
{
    "inline": {
      "query": {
        "match": {
          "{{my_field}}": "{{my_value}}"
        }
      },
      "size": "{{my_size}}"
    },
    "params": {
      "my_field": "foo",
      "my_value": "bar",
      "my_size": 5
    }
}
```

# file using the template endpoint:

```
GET /_search/template
{
    "file": "my_template",
    "params": {
      "my_field": "foo",
      "my_value": "bar",
      "my_size": 5
    }
}
```

# inline for query templates (file respectively):

```
GET /_search
{
    "query": {
        "template": {
          "inline": {
            "match": {"{{my_field}}": "{{my_value}}"}},
         "params": {
            "my_field": "foo",
            "my_value": "bar"}
       }
    }
}
```

For 1.x I'd like to keep the current format around but add a deprecation warning to the docs, fix the current behaviour with the original pull request and add the new format proposed above so people can play with it without upgrading.

Does that plan make sense? Did I forget anything?
</comment><comment author="javanna" created="2015-05-04T10:13:56Z" id="98667774">sounds good to me, I think we need to update the indexed scripts endpoint too (to remove the outer `script` object). Also, not sure about the corresponding put apis (for both indexed scripts and templates): do they require to wrap the actual script/templates into a script/template outer object?
</comment><comment author="MaineC" created="2015-06-22T07:50:56Z" id="114031891">@lukas-vlcek can you check your problem is fixed on master with #11512 - if so I believe we can close this PR.
</comment><comment author="clintongormley" created="2015-06-23T17:55:35Z" id="114590309">I've checked and it is fixed by #11512. Closing, thanks @lukas-vlcek, sorry it took so long
</comment><comment author="lukas-vlcek" created="2015-06-24T06:29:55Z" id="114746045">@clintongormley @MaineC thanks, did not have a chance to test it but if some of my former tests are passing now then it should be positive.

You might want to look at https://github.com/elastic/elasticsearch/issues/8928 which I opened while ago as well. The point was to make it clear in the doc which ES versions support search templates correctly and what are the limitations (bugs in this case) in other ES versions.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix GET index API always running all features</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8392</link><project id="" key="" /><description>Previous to this change all features (_alias,_mapping,_settings,_warmer) are run regardless of which features are actually requested. This change fixes the request object to resolve this bug
</description><key id="48102618">8392</key><summary>Fix GET index API always running all features</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Index APIs</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T16:40:17Z</created><updated>2015-06-07T18:42:00Z</updated><resolved>2014-11-13T13:25:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-11T13:01:46Z" id="62543484">@javanna have pushed a new iteration. Let me know what you think
</comment><comment author="javanna" created="2014-11-12T10:28:49Z" id="62698193">LGTM
</comment><comment author="colings86" created="2014-11-13T13:25:16Z" id="62889435">merged into master, for 1.x and 1.4 changes see https://github.com/elasticsearch/elasticsearch/pull/8452/files
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>1.4.0 Children aggregation returns empty buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8391</link><project id="" key="" /><description>I have been trying to test this feature with guide data as follow:

```
PUT /company
{
  "mappings": {
    "branch": {},
    "employee": {
      "_parent": {
        "type": "branch" 
      }
    }
  }
}

POST /company/branch/_bulk
{ "index": { "_id": "london" }}
{ "name": "London Westminster", "city": "London", "country": "UK" }
{ "index": { "_id": "liverpool" }}
{ "name": "Liverpool Central", "city": "Liverpool", "country": "UK" }
{ "index": { "_id": "paris" }}
{ "name": "Champs Élysées", "city": "Paris", "country": "France" }

POST /company/employee/_bulk
{ "index": { "_id": 2, "parent": "london" }}
{ "name": "Mark Thomas", "dob": "1982-05-16", "hobby": "diving" }
{ "index": { "_id": 3, "parent": "liverpool" }}
{ "name": "Barry Smith", "dob": "1979-04-01", "hobby": "hiking" }
{ "index": { "_id": 4, "parent": "paris" }}
{ "name": "Adrien Grand", "dob": "1987-05-11", "hobby": "horses" }
```

and execute the same query as here:
http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/children-agg.html

```
GET /company/branch/_search?search_type=count
{
  "aggs": {
    "country": {
      "terms": { 
        "field": "country"
      },
      "aggs": {
        "employees": {
          "children": { 
            "type": "employee"
          },
          "aggs": {
            "hobby": {
              "terms": { 
                "field": "hobby"
              }
            }
          }
        }
      }
    }
  }
}
```

A response is:

```
{
   "took": 87,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 3,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "country": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "uk",
               "doc_count": 2,
               "employees": {
                  "doc_count": 2,
                  "hobby": {
                     "doc_count_error_upper_bound": 0,
                     "sum_other_doc_count": 0,
                     "buckets": []
                  }
               }
            },
            {
               "key": "france",
               "doc_count": 1,
               "employees": {
                  "doc_count": 1,
                  "hobby": {
                     "doc_count_error_upper_bound": 0,
                     "sum_other_doc_count": 0,
                     "buckets": []
                  }
               }
            }
         ]
      }
   }
}
```

As you can see countries are grouped correctly but buckets for hobbies are empty.
</description><key id="48101713">8391</key><summary>1.4.0 Children aggregation returns empty buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marekmalek</reporter><labels /><created>2014-11-07T16:32:17Z</created><updated>2014-11-07T17:22:11Z</updated><resolved>2014-11-07T17:22:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-07T17:22:11Z" id="62178785">Hi @marekmalek 

This is a bug in field resolution: the `branch` in the URL sets the default type, so with `hobby`, it looks only for `branch.hobby`.  If you change that to `employee.hobby` then it works correctly.

This bug will be fixed by #4081

I've updated the example in the guide to use `employee.hobby`.  Thanks for reporting.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Internal: Don't pass acceptdocs down to parents filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8390</link><project id="" key="" /><description>This may result in something other than FixedBitSet and cause an error.

Its wasteful at least, we already pass it to the scorer. But its possible if acceptDocs are non-null that we could get back a BitsFilteredDocIdSet or similar, which would cause an exception (e.g. during delete by query).

PR moved from #8389

This is because its impossible to add a test for 1.4, it has a dedicated FixedBitSet cache and drops acceptDocs correctly. We should still change it to pass null! But we can also include a test for 1.3

In master everything is type-safe
</description><key id="48099437">8390</key><summary>Internal: Don't pass acceptdocs down to parents filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>v1.3.6</label><label>v1.4.1</label></labels><created>2014-11-07T16:12:22Z</created><updated>2015-03-19T10:18:52Z</updated><resolved>2014-11-07T18:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-07T16:39:06Z" id="62172214">Thanks for fixing this! LGTM
</comment><comment author="mikemccand" created="2014-11-07T17:02:50Z" id="62175974">LGTM.  Sneaky!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't pass acceptdocs down to parents filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8389</link><project id="" key="" /><description>This may result in something other than FixedBitSet and cause an error.

Its wasteful at least, we already pass it to the scorer. But its possible if acceptDocs are non-null that we could get back a BitsFilteredDocIdSet or similar, which would cause an exception (e.g. during delete by query).

I don't have a test yet. Only applies to 1.x and not master, as we fixed the type safety in master/lucene 5
</description><key id="48089224">8389</key><summary>Don't pass acceptdocs down to parents filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-11-07T14:49:41Z</created><updated>2014-11-07T16:13:05Z</updated><resolved>2014-11-07T16:12:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-07T14:53:14Z" id="62154782">LGTM, but maybe add a comment explaining why we pass down null? Also I think we should port this to 1.4 too.
</comment><comment author="rmuir" created="2014-11-07T16:13:05Z" id="62167990">I closed this for #8390, which is against 1.3.x where the bug can strike, and has a test
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log how long IW.rollback took, and when MockFSDir starts its check index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8388</link><project id="" key="" /><description>A trivial PR that logs how long IndexWriter.rollback took.
</description><key id="48088440">8388</key><summary>Log how long IW.rollback took, and when MockFSDir starts its check index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T14:42:14Z</created><updated>2015-06-07T10:49:02Z</updated><resolved>2014-11-07T23:41:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-07T21:28:28Z" id="62215560">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices API: Fixed backward compatibility issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8387</link><project id="" key="" /><description>If a 1.4 node needs to forward a GET index API call to a &lt;1.4 master it cannot use the GET index API endpoints as the master has no knowledge of it. This change detects that the master does not understand the initial request and instead tries it again using the old APIs. If these calls also do not work, an error is returned

Closes #8364
</description><key id="48078290">8387</key><summary>Indices API: Fixed backward compatibility issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Index APIs</label><label>bug</label><label>regression</label><label>v1.4.1</label><label>v1.5.0</label></labels><created>2014-11-07T12:48:30Z</created><updated>2015-03-19T16:42:31Z</updated><resolved>2014-11-07T15:16:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-11-07T13:17:30Z" id="62140652">Left a few comments, nice work @colings86 !
</comment><comment author="javanna" created="2014-11-07T14:33:39Z" id="62152202">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: add max_determinized_states to query_string and regexp query/filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8386</link><project id="" key="" /><description>This PR applies to master, and it includes another Lucene 5.0.0 snapshot upgrade.

I just plumbed the max_determinized_states through to regexp query, filter and query_string, leaving the default at Lucene's default (10000 states).

Closes #8357
</description><key id="48076517">8386</key><summary>Core: add max_determinized_states to query_string and regexp query/filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-11-07T12:24:28Z</created><updated>2015-03-19T10:18:52Z</updated><resolved>2014-11-10T18:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-07T12:55:55Z" id="62138370">looks good. when rebasing you should hit a forbidden failure from #8375 ... just nuke those entries i added and we can remove the FileSystemUtils method and call IOUtils ones safely.
</comment><comment author="mikemccand" created="2014-11-08T21:51:17Z" id="62280266">OK I merged master and then removed the forbidden API + FileSystemUtils.deleteFilesIgnoringExceptions ... I think this is ready.
</comment><comment author="s1monw" created="2014-11-09T07:59:44Z" id="62295383">@mikemccand I don't think master has the latest lucene snapshot yet....
</comment><comment author="mikemccand" created="2014-11-09T11:42:09Z" id="62300470">@s1monw right, in this PR I've upgraded it... (see the pom.xml changes).  I'll push the lucene snapshot bits when I commit ... and when I backport to 1.5 (not 1.4!) I'll also push a new Lucene 4.10.3 snapshot.
</comment><comment author="s1monw" created="2014-11-09T11:42:49Z" id="62300482">aaah nevermind LGTM then
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java</file><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/RegexpFilter.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/RafReference.java</file><file>src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java</file><file>src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Core: add max_determinized_states to query_string and regexp query/filter</comment></comments></commit></commits></item><item><title>Drop pre 0.90 compression BWC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8385</link><project id="" key="" /><description>Pre 0.90 indices need to be upgraded to run with 2.0
we can drop the stored field compression BWC.
</description><key id="48073897">8385</key><summary>Drop pre 0.90 compression BWC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T11:46:31Z</created><updated>2015-06-07T11:56:44Z</updated><resolved>2014-11-07T11:55:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-07T11:50:26Z" id="62132343">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose `max_determinized_states` in regexp query, filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8384</link><project id="" key="" /><description>This PR applies to 1.4.x, and it includes a Lucene snapshot upgrade (so we can't release 1.4.1 until Lucene also releases 4.10.3).

I just plumbed the max_determinized_states through to regexp query, filter and query_string, leaving the default at Lucene's default (10000 states).

Closes #8357
</description><key id="48071861">8384</key><summary>Expose `max_determinized_states` in regexp query, filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T11:18:46Z</created><updated>2015-06-07T16:55:46Z</updated><resolved>2014-11-07T12:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-07T11:21:41Z" id="62129664">I'd rather want this to go into 1.x for now and if we happen to update to `Lucene-4.10.3` on the `1.4` branch we can cherry-pick it? I don't want snapshot builds on releasebranches really since I wanna  be able to release ANY time on those branches
</comment><comment author="mikemccand" created="2014-11-07T11:25:22Z" id="62130011">@s1monw OK I'll switch to master only for now.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use DistributorDirectory only if there are more than one data direcotry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8383</link><project id="" key="" /><description>We don't need the overhead of DistributorDirectory if there is only a single directory
in the distributor.
</description><key id="48071526">8383</key><summary>Use DistributorDirectory only if there are more than one data direcotry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T11:14:04Z</created><updated>2015-06-07T11:56:55Z</updated><resolved>2014-11-18T13:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-07T11:59:02Z" id="62133098">Change looks good. I left one comment for extra simplification.
</comment><comment author="s1monw" created="2014-11-07T19:37:29Z" id="62199985">I pushed some changes @dakrone I didn't change the assert part it's by design like that i added a comment
</comment><comment author="bleskes" created="2014-11-07T20:04:31Z" id="62203714">LGTM
</comment><comment author="s1monw" created="2014-11-09T21:32:13Z" id="62320740">@dakrone @bleskes I updated this PR - sorry I had to rebase since I change so many things on master that had an impact on this.
</comment><comment author="bleskes" created="2014-11-10T10:09:25Z" id="62364116">One more LGTM
</comment><comment author="dakrone" created="2014-11-11T13:18:44Z" id="62545242">LGTM, nice simplification!
</comment><comment author="s1monw" created="2014-11-18T13:04:16Z" id="63467704">@dakrone @bleskes I updated this - can you take a quick look
</comment><comment author="dakrone" created="2014-11-18T13:05:23Z" id="63467829">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Take percentage watermarks into account for reroute listener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8382</link><project id="" key="" /><description>Fixes an issue where only absolute bytes were taken into account when
kicking off an automatic reroute due to disk usage. Also randomized the
tests to use either an absolute value or a percentage so this is tested.

Also adds logging for each node over the high and low watermark every
time a new cluster info usage is gathered (defaults to every 30
seconds).

Related to #8368
Fixes #8367
</description><key id="48063790">8382</key><summary>Take percentage watermarks into account for reroute listener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T09:34:53Z</created><updated>2015-06-06T19:04:37Z</updated><resolved>2014-11-07T12:07:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-07T10:31:39Z" id="62124778">left one comment other than that LGTM
</comment><comment author="bleskes" created="2014-11-07T10:38:59Z" id="62125523">LGTM2 - left two comments regarding logging
</comment><comment author="dakrone" created="2014-11-07T12:07:53Z" id="62133861">Merged to master and 1.x
</comment><comment author="nik9000" created="2014-11-07T12:16:18Z" id="62134621">Thanks!
</comment><comment author="clintongormley" created="2014-11-12T19:36:25Z" id="62779427">@dakrone shouldn't this go into 1.4.1 as well?
</comment><comment author="dakrone" created="2014-11-12T23:01:31Z" id="62811608">@clintongormley no, because the original change to add the listener went into 1.x only (since it is a new feature)
</comment><comment author="s1monw" created="2014-11-21T09:51:28Z" id="63947883">should this be marked as a bug if it's in `1.4.1`? and if it's a bug should we port to `1.3.6`?
</comment><comment author="dakrone" created="2014-11-21T09:52:18Z" id="63947996">@s1monw no, because this was fixing a bug that was merged to the 1.4 branch after 1.4.0 was released, so it affected the not-yet-released 1.4.1, and was fixed here
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ensure fields are overriden and not merged when using arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8381</link><project id="" key="" /><description>In the case you try to put two settings, one being an array and one being
a field, together, the settings were merged instead of being overridden.

First config file
my.value: 1

Second config file
my.value: [ 2, 3 ]

If you execute

settingsBuilder().put(settings1).put(settings2).build()

now only values 2,3 will be in the final settings

Closes #6887
</description><key id="48061748">8381</key><summary>Ensure fields are overriden and not merged when using arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Settings</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T09:06:50Z</created><updated>2015-06-07T18:23:40Z</updated><resolved>2015-01-06T08:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-11-10T11:28:05Z" id="62372012">updated the PR with another commit, which clones the map and adds some more tests (and changes the impl in order to satisfy the newly added tests, good catch)
</comment><comment author="spinscale" created="2015-01-05T15:00:18Z" id="68718764">@jpountz this one has been lingering around for too long. I just rebased and added some other tests, if you can spare some time, could you have another look? Thanks!
</comment><comment author="jpountz" created="2015-01-05T15:14:42Z" id="68720956">LGTM, left two minor style-related comments, but please feel free to push without further review from my end!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/test/java/org/elasticsearch/common/settings/ImmutableSettingsTests.java</file></files><comments><comment>Settings: Ensure fields are overriden and not merged when using arrays</comment></comments></commit></commits></item><item><title>Inverse DocIdSets' heuristic to find out fast DocIdSets.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8380</link><project id="" key="" /><description>DocIdSets.isFast(DocIdSet) has two issues:
- it works on the DocIdSet interface while some doc sets can generate either
  slow or fast iterators depending on their options (eg. whether an OrDocIdSet is
  fast or not depends on the wrapped clauses).
- it only works because the result of this method is only taken into account
  when a DocIdSet has non-null `bits()`.

Here is a proposal to change this method to work on top of a DocIdSetIterator
and to use a black-list rather than a white list: slow iterators should
really be the exception rather than the rule.
</description><key id="48060337">8380</key><summary>Inverse DocIdSets' heuristic to find out fast DocIdSets.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T08:47:41Z</created><updated>2015-06-08T14:15:44Z</updated><resolved>2014-11-10T09:04:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-07T14:20:55Z" id="62150535">@rmuir I renamed the method to `isBroken()`
</comment><comment author="rmuir" created="2014-11-07T18:24:06Z" id="62189259">+1 !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/docset/AndDocIdSet.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/DocIdSets.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/OrDocIdSet.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XDocIdSetIterator.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/test/java/org/elasticsearch/common/lucene/docset/DocIdSetsTests.java</file></files><comments><comment>Internal: Inverse DocIdSets' heuristic to find out fast DocIdSets.</comment></comments></commit></commits></item><item><title>Allow -SNAPSHOT versions to be parsed by Version.fromString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8379</link><project id="" key="" /><description>this helps to run bwc tests against SNAPSHOT versions
</description><key id="48059148">8379</key><summary>Allow -SNAPSHOT versions to be parsed by Version.fromString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T08:29:04Z</created><updated>2015-03-19T09:47:59Z</updated><resolved>2014-11-07T11:26:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-07T10:54:18Z" id="62127041">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix bulk update NPE error when parent is not specified for child doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8378</link><project id="" key="" /><description>Closes #8365
</description><key id="48052142">8378</key><summary>Fix bulk update NPE error when parent is not specified for child doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wwken</reporter><labels><label>:Parent/Child</label><label>bug</label></labels><created>2014-11-07T06:18:31Z</created><updated>2015-03-19T10:18:52Z</updated><resolved>2014-11-17T18:26:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-14T15:54:43Z" id="63084419">@martijnvg I see that you added the `What to do` comment, would you mind reviewing this PR?
</comment><comment author="martijnvg" created="2014-11-14T21:27:28Z" id="63131673">I think instead of throwing in an exception it is better to set the response for that bulk item to an error, something like this:

``` java
BulkItemResponse.Failure failure = new BulkItemResponse.Failure(
        updateRequest.index(), updateRequest.type(), updateRequest.id(), 
        "routing is required for this item", RestStatus.BAD_REQUEST
);
responses.set(i, new BulkItemResponse(i, updateRequest.type(), failure));
```

Throwing an error would fail the entire bulk request, even when other bulk items are correct. 

@wwken Can you upgrade your PR?
</comment><comment author="markharwood" created="2014-11-17T18:26:16Z" id="63350384">Many thanks for your help with this @wwken !
I started work on this issue before I spotted your PR so I rolled your commit in the git history along with my PR here https://github.com/elasticsearch/elasticsearch/pull/8506  and have pushed to master, 1.x and 1.4 branches.
Thanks again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>1.4.0 failed to compile and run native script score function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8377</link><project id="" key="" /><description>Hi y'all, 

After upgrading to 1.4.0, I failed to compile my native(Java) score functions which worked well with 1.3.4.
The first issue I ran into is that my function extended `AbstractDoubleSearchScript` and call `score()` to get document score from Lucene, apparently it won't work now with 1.4.0.

I wonder if any of you could tell me which method should be called to get the original score of one document. An example of my function looks like:

```
public class BasicScorer extends AbstractDoubleSearchScript {

  public static final String NAME = "basic-scorer";

  public static class Factory implements NativeScriptFactory {
    @Override
    public ExecutableScript newScript(@Nullable Map&lt;String, Object&gt; params) {
      return new BasicScorer();
  }

  private BasicScorer() {
  }

  @Override
  public double runAsDouble() {
      return score() * ((ScriptDocValues.Doubles) doc().get("rank")).getValue();
  }
}
```

The second issue is when I just remove the 'score()' method and query ElasticSearch with following parameter:

```
...
     "script_score": {
        "script": "basic-scorer",
        "lang": "native"
      }
...
```

following error was logged:

```
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)
    ... 9 more
Caused by: java.lang.UnsupportedOperationException
    at org.elasticsearch.script.AbstractSearchScript.setScorer(AbstractSearchScript.java:104)
    at org.elasticsearch.common.lucene.search.function.ScriptScoreFunction.&lt;init&gt;(ScriptScoreFunction.java:86)

```

I noticed there were some groovy script function changes, but I don't think that should affect the interface of native score function.

Regards,
Andy
</description><key id="48050617">8377</key><summary>1.4.0 failed to compile and run native script score function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">KaiqiangXu</reporter><labels><label>bug</label></labels><created>2014-11-07T05:48:12Z</created><updated>2014-11-10T15:59:57Z</updated><resolved>2014-11-10T01:23:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dccmx" created="2014-11-07T12:17:13Z" id="62134695">Same problem here, and I can not find a way (efficient way) to get the score lucene provided within a native score script.
</comment><comment author="clintongormley" created="2014-11-07T13:26:50Z" id="62141597">It looks like it is to do with this change: https://github.com/elasticsearch/elasticsearch/pull/7819

@brwe can you shed more light here please?
</comment><comment author="kilnyy" created="2014-11-08T08:33:13Z" id="62250320">I think I have the same issue, I wrote my plugin with the instruction in this repo:
https://github.com/imotov/elasticsearch-native-script-example
and also got `UnsupportedOperationException` in the log.
</comment><comment author="noamt" created="2014-11-09T15:37:00Z" id="62307202">I too suffer from the issue
</comment><comment author="rjernst" created="2014-11-10T01:23:03Z" id="62329962">In 1.3 and before, there was a weird dual API for setting the score (see #6864).  The `score()` function was removed there.

To access the score from a native script, you should `@Override setScorer(Scorer)` and call `scorer.score()` on when you want the score. 
</comment><comment author="rjernst" created="2014-11-10T01:45:01Z" id="62331075">After looking at this a little more, the original intention was the score could be accessed with `doc().score()`, but that was removed with #7819. I've opened #8416 to add back `score()` for `AbstractSearchScript`.
</comment><comment author="imotov" created="2014-11-10T06:59:26Z" id="62348044">@kilnyy, @noamt elasticsearch-native-script-example is updated with  [a temporary work-around](https://github.com/imotov/elasticsearch-native-script-example/commit/1254c4837de635a42e43b2bdbbd5ef70b621b010) until 1.4.1 is released.
</comment><comment author="noamt" created="2014-11-10T07:03:41Z" id="62348277">Cheers!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file></files><comments><comment>Scripting: Add score() back to AbstractSearchScript</comment></comments></commit></commits></item><item><title>Search Template - conditional clauses not rendering correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8376</link><project id="" key="" /><description>- implemented the conditional parsing capabilities
- attached few junit test cases to test it

Closes #8308
</description><key id="48044319">8376</key><summary>Search Template - conditional clauses not rendering correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wwken</reporter><labels><label>:Search Templates</label></labels><created>2014-11-07T03:34:16Z</created><updated>2015-06-23T17:56:06Z</updated><resolved>2015-06-23T17:56:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wwken" created="2014-11-07T03:55:08Z" id="62092607">FYI, this is a fix on this issue: https://github.com/elasticsearch/elasticsearch/issues/8308
</comment><comment author="MaineC" created="2015-06-22T07:51:41Z" id="114032179">@wwken can you check your problem is fixed on master with #11512 - if so I believe we can close this PR.
</comment><comment author="clintongormley" created="2015-06-23T17:56:05Z" id="114590398">This is fixed. thanks @wwken and sorry it took so long
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Temporarily ban buggy IOUtils methods with forbidden</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8375</link><project id="" key="" /><description>Simon's testing found some unreleased bugs in these methods in lucene 5.0, because the bug is very evil, I think we should just ban the methods until we update to a newer snapshot with the fix.

When we upgrade, forbidden APIs will automatically fail as the methods signatures changed to Collection.

In the meantime, I want to prevent anyone from accidentally using them or wasting time.

See https://issues.apache.org/jira/browse/LUCENE-6051
</description><key id="48037810">8375</key><summary>Temporarily ban buggy IOUtils methods with forbidden</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-07T01:34:38Z</created><updated>2015-06-08T14:16:02Z</updated><resolved>2014-11-07T11:33:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-07T01:54:48Z" id="62084508">That bug certainly seems evil.
+1
</comment><comment author="dakrone" created="2014-11-07T09:15:03Z" id="62116736">LGTM
</comment><comment author="s1monw" created="2014-11-07T09:58:20Z" id="62121196">LGTM thanks - I missed that one
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Internal: temporarily ban buggy IOUtils methods with forbidden</comment></comments></commit></commits></item><item><title>Documentation doesn't explain overall structure or composition of document components (queries, filters, etc.)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8374</link><project id="" key="" /><description>https://twitter.com/mausch/status/530504035540160513

https://twitter.com/LeviNotik/status/530505312777023488

https://twitter.com/mwotton/status/442775637648891904

https://twitter.com/melvinmt/status/530474427306479616

https://twitter.com/kosmikko/status/502340971527671808

https://twitter.com/kenperkins/status/494173990152175617 (navigation, I've had this problem and I'm a relatively experienced user of the site and ES itself)

https://twitter.com/icyliquid/status/477500839020752896

https://twitter.com/werg/status/466733972324880384 (another suggestion to bypass official ES documentation)

I wrote https://github.com/bitemyapp/bloodhound partly because I was having a hell of a time generating valid Elasticsearch requests without lots of copy-paste and templating.

The docs have not been improved much in the last couple years other than when you did the big site change.

It's extremely hard for a new user of Elasticsearch to get a bird's eye view.

I brought up Bloodhound when I first released it as a way to possibly start a proper spec of what Elasticsearch accepts as a valid request and what it does not. I'd like to revive the idea of having a spec so that users and client implementors don't have to keep rediscovering how Elasticsearch works through haphazard experimentation.

Back when I was implementing Bloodhound I asked if there was a spec. I was told there wasn't one. I was briefly hopeful when I saw there was a Thrift spec...but it passes off the actual document structure as a JSON blob, lending no useful information.

Please at least consider having a spec for the API.
</description><key id="48030234">8374</key><summary>Documentation doesn't explain overall structure or composition of document components (queries, filters, etc.)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/palecur/following{/other_user}', u'events_url': u'https://api.github.com/users/palecur/events{/privacy}', u'organizations_url': u'https://api.github.com/users/palecur/orgs', u'url': u'https://api.github.com/users/palecur', u'gists_url': u'https://api.github.com/users/palecur/gists{/gist_id}', u'html_url': u'https://github.com/palecur', u'subscriptions_url': u'https://api.github.com/users/palecur/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1779279?v=4', u'repos_url': u'https://api.github.com/users/palecur/repos', u'received_events_url': u'https://api.github.com/users/palecur/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/palecur/starred{/owner}{/repo}', u'site_admin': False, u'login': u'palecur', u'type': u'User', u'id': 1779279, u'followers_url': u'https://api.github.com/users/palecur/followers'}</assignee><reporter username="">bitemyapp</reporter><labels><label>docs</label></labels><created>2014-11-06T23:43:14Z</created><updated>2015-06-04T19:41:10Z</updated><resolved>2015-06-04T19:41:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Fiedzia" created="2014-11-07T01:10:42Z" id="62080718">+1. Especially for horrible navigation. Though I think that the since ES income comes largely from support contracts, this may simply be intentional.
</comment><comment author="Mpdreamz" created="2014-11-07T13:57:26Z" id="62147753">The .NET client NEST [has already mapped a great deal](https://github.com/elasticsearch/elasticsearch-net/tree/develop/src/Nest/Domain)  (guestimate 80%-90%) of the elasticsearch HTTP API so suffice to say we totally can relate to this issue. 

That being said C# is not a great reference, i.e it misses i.e sum types to properly describe different valid states an object can take. NEST always expands to the most elaborate form a construct accepts. i.e NEST will always send:

```
{
    "term" : { "user" : { "value" : "kimchy" } }
}
```

over 

```
{
    "term" : { "user" : "value" }
}
```

Even if the NEST API itself allows you to be terser.

We (.NET team) spent some time internally a couple of weeks ago trying to generate and refine a `json schema draft 4` spec for all the requests and responses and its individual components but sadly found that some constructs simply cannot be expressed strictly enough in json schema i.e:

```
{
    "term" : { "user" : "kimchy" }
}
```

`user` being a variable property name wreaks havoc on trying to lock it down in a schema. Json schema does allow for something like [patternProperties](http://jsonary.com/documentation/json-schema/?section=keywords/Object%20validation#keywords/Object%20validation/02%20-%20patternProperties) but you can not make them mandatory or limit the number of occurrences.

So its back to the drawing board on this for now. The idea i've been floating now is to identify all places that read and accept json and write out all valid objects it can read / write post that up somewhere and go from there (reference implementations of the types in various languages).

Suggestions are much welcomed!
</comment><comment author="ppurang" created="2014-11-07T14:49:52Z" id="62154331">+1 
</comment><comment author="aloiscochard" created="2014-11-07T16:54:23Z" id="62174729">:+1:

That would benefit any client written for statically typed language.
</comment><comment author="mausch" created="2014-11-10T11:45:46Z" id="62373656">You can easily encode sum types in C#, e.g. http://bugsquash.blogspot.com/2012/01/encoding-algebraic-data-types-in-c.html .
They are quite verbose compared to languages with direct support for them, but you still get the benefits of precise typing.
</comment><comment author="clintongormley" created="2015-06-04T19:41:08Z" id="109024861">Closing as a duplicate of https://github.com/elastic/elasticsearch/issues/8965
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't write bad search requests into exception messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8373</link><project id="" key="" /><description>See issue #8370.

If a search requests's source doesn't parse, log the
source instead of putting it into an exception
message and propagating it. This prevents large
strings from being built up.

Ideally the source could be streamed to a log file
so that it never has to be rendered in-memory in
its entirety, but this is a good start.
</description><key id="48008040">8373</key><summary>Don't write bad search requests into exception messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zakmagnus</reporter><labels><label>:Exceptions</label></labels><created>2014-11-06T20:08:30Z</created><updated>2015-03-19T10:18:52Z</updated><resolved>2014-11-07T13:05:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-07T13:05:42Z" id="62139299">Hi @zakmagnus 

Thanks, but this feels like too blunt a change, especially as most clients won't have access to the log files.  Instead of this, we're working on making exceptions better.  Closing in favour of https://github.com/elasticsearch/elasticsearch/pull/7891
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Translog leaks filehandles if it's corrupted or truncated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8372</link><project id="" key="" /><description>If the translog file is corrupted or truncated the stream is never closed
and the filehandle leaks. This commit closes the stream in the case of an
exception.
</description><key id="48005758">8372</key><summary>Translog leaks filehandles if it's corrupted or truncated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-06T19:48:06Z</created><updated>2015-06-07T18:04:34Z</updated><resolved>2014-11-06T20:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-11-06T20:15:31Z" id="62043718">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>1.4.0 tribe node fails to initialize if Marvel is installed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8371</link><project id="" key="" /><description>With Marvel installed, a 1.4.0 tribe node will fail with the following:

[2014-11-06 14:09:34,680][ERROR][bootstrap                ] {1.4.0}: Initialization Failed ...
1) Tried proxying org.elasticsearch.discovery.DiscoveryService to support a circular dependency, but it is not an interface.2) IllegalStateException[This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.]

This can be reproduced by setting up a working 1.4.0 tribe node, then installing Marvel.  On the next restart the node will fail with this error.  If Marvel is removed, the node will start properly.
</description><key id="48002080">8371</key><summary>1.4.0 tribe node fails to initialize if Marvel is installed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">seang-es</reporter><labels /><created>2014-11-06T19:15:23Z</created><updated>2014-12-22T09:05:45Z</updated><resolved>2014-11-13T19:51:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-07T12:23:40Z" id="62135291">This needs to be solved on the Marvel side - ticked opened on Marvel
</comment><comment author="bleskes" created="2014-11-13T19:51:17Z" id="62954706">marvel ticket solved. Will be part of the next marvel release
</comment><comment author="bleskes" created="2014-12-20T08:48:27Z" id="67729520">@seang-es not sure where the original report came from, but this is fixed now.
</comment><comment author="javanna" created="2014-12-22T09:05:45Z" id="67815489">Relates to #8539 and #8415, fixed also upstream in elasticsearch 1.4.1.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Big requests shouldn't be stuffed into exception messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8370</link><project id="" key="" /><description>For example, suppose ES receives a search query exceeding the maximum number of search terms Lucene will accept. This is probably a pretty large query. Eventually the Lucene Query will throw an exception as terms are added to it, and it will be caught by SearchService.parseSource(). In there, the query will be converted to JSON and placed into an exception message. That's a pretty large string.

To make matters worse, all of your shards will create similar exceptions, and their messages will be combined together in SearchPhaseExecutionError.buildMessage(). This means a very large string will be constructed in-memory. Maybe it could even OOM your node, if the original request was large enough.

Instead of placing request contents directly into exception messages, could they instead just be logged?
</description><key id="48000418">8370</key><summary>Big requests shouldn't be stuffed into exception messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zakmagnus</reporter><labels><label>:Exceptions</label><label>discuss</label></labels><created>2014-11-06T19:03:04Z</created><updated>2014-11-07T13:06:14Z</updated><resolved>2014-11-07T13:06:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-07T13:06:14Z" id="62139358">Closed in favour of https://github.com/elasticsearch/elasticsearch/pull/7891
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Distributed update API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8369</link><project id="" key="" /><description>The difference between a primary and a replica shard should be just a flag which indicates the current role that a shard has.  The work load should be the same for all shards in a group, meaning that it shouldn't matter how many primary shards there are on any one node. 

The only API which doesn't follow this principle is the `update` API, which can run a potentially heavy script on the primary shard.  This can result in hotspots in the cluster, where one node happens to host many primary shards (see #8149).

@bleskes suggested a way to fix this: by changing the `update` API to perform the `GET` and `script` phases on any primary or replica in a shard group.  This would change the characteristics of `update` to be more like a normal distributed get-and-reindex.

This increases the window for conflicting changes, so it is probably worth changing `retry_on_conflict` to default to `1`, instead of `0`.

If the user is sure that their updates are light and won't cause hotspots on the primary, then they can opt in to primary-only updates with `?preference=_primary`

/cc @s1monw 
</description><key id="47999811">8369</key><summary>Distributed update API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:CRUD</label><label>adoptme</label><label>enhancement</label></labels><created>2014-11-06T18:57:34Z</created><updated>2016-11-06T07:41:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-06T19:00:59Z" id="62031853">huge +1
</comment><comment author="martijnvg" created="2014-11-06T20:29:38Z" id="62045884">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Raise log level on DiskThresholdDecider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8368</link><project id="" key="" /><description>Having not enough disk space to allocate the shard is worth warning about.

Closes #8367
</description><key id="47976334">8368</key><summary>Raise log level on DiskThresholdDecider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>enhancement</label></labels><created>2014-11-06T15:43:56Z</created><updated>2015-03-19T10:18:51Z</updated><resolved>2014-11-06T16:31:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-06T16:22:34Z" id="62005031">@dakrone please take a look
</comment><comment author="clintongormley" created="2014-11-06T16:31:49Z" id="62006658">Hi @nik9000 

The problem with this is that the allocation deciders can be called hundreds of times per second, which would flood your logs with warnings.  See the discussion here: https://github.com/elasticsearch/elasticsearch/pull/3637#discussion_r6218747

I think a better solution would be to warn once every 30 seconds when the low watermark is breached, as we do for the high watermark already: https://github.com/elasticsearch/elasticsearch/pull/8270/files#diff-1b8dca987fbcfb8d8e452d7e29c4d058R139

@dakrone says he'll work on that.

thanks 
</comment><comment author="nik9000" created="2014-11-06T16:44:12Z" id="62009323">Sounds good.
On Nov 6, 2014 11:31 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; Closed #8368 https://github.com/elasticsearch/elasticsearch/pull/8368.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/8368#event-189409480
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/DiskUsage.java</file><file>src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesTests.java</file></files><comments><comment>Take percentage watermarks into account for reroute listener</comment></comments></commit></commits></item><item><title>Disk free space threshold - at least a Warning message in the log file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8367</link><project id="" key="" /><description>Hi,
Today i had the issue, that all my replica shards were not starting.
After 3 hours i enabled DEBUG in logging.yml and finally spotted:
Less than the required 15.0% free disk threshold (11.348983564561003% free) on node [blahblah], preventing allocation.

So i freed some space and everything is back online.

It would be great if ES emits a (at least)warning in the log file.
</description><key id="47974242">8367</key><summary>Disk free space threshold - at least a Warning message in the log file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">j0r0</reporter><labels /><created>2014-11-06T15:26:24Z</created><updated>2014-11-27T15:53:01Z</updated><resolved>2014-11-07T12:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-06T15:45:19Z" id="61998378">I had to fix this for someone recently as well.  Lots of warning would be annoying (and maybe counter productive if you don't partition your disks sanely) but would still make the problem less mysterious.

You could always turn off the warnings if you don't like them with logging config.
</comment><comment author="synhershko" created="2014-11-27T15:36:59Z" id="64804439">@dakrone I worked with a client for whom this fix may have made things even worse. A log entry every 30 seconds means the log file keeps getting bigger and bigger. If there is no place to move shards to, or if it takes more time for the move to finish (think large indexes, or many re-allocations pending) then this may as well crash the node due to 0 diskspace left after a short while. My 2c.
</comment><comment author="dakrone" created="2014-11-27T15:53:01Z" id="64806240">@synhershko opened #8686 to address this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/DiskUsage.java</file><file>src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/test/java/org/elasticsearch/cluster/DiskUsageTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesTests.java</file></files><comments><comment>Take percentage watermarks into account for reroute listener</comment></comments></commit></commits></item><item><title>Cut over to Path API for file deletion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8366</link><project id="" key="" /><description>Today we use the File API for file deletion as well as recursive
directory deletions. This API returns a boolean if operations
are successful while hiding the actual reason why they failed.
The Path API throws and actual exception that might provide better
insights and debug information.
</description><key id="47968872">8366</key><summary>Cut over to Path API for file deletion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-06T14:42:09Z</created><updated>2015-06-06T16:20:57Z</updated><resolved>2014-11-06T16:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-06T14:55:09Z" id="61990030">@jpountz @rmuir pushed an update
</comment><comment author="jpountz" created="2014-11-06T15:28:55Z" id="61995582">LGTM
</comment><comment author="s1monw" created="2014-11-06T15:32:13Z" id="61996145">pushed another review round
</comment><comment author="s1monw" created="2014-11-06T18:45:59Z" id="62029352">I marked it as breaking since I changed some interfaces internally
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java</file></files><comments><comment>Update S3BlobContainer because BlobContainer changed</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/blobstore/BlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/BlobStore.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobStore.java</file><file>src/main/java/org/elasticsearch/common/blobstore/url/URLBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file><file>src/main/java/org/elasticsearch/gateway/none/NoneGateway.java</file><file>src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/store/fs/FsIndexStore.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/RafReference.java</file><file>src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/FileBasedMappingsTests.java</file><file>src/test/java/org/elasticsearch/index/translog/fs/FsBufferedTranslogTests.java</file><file>src/test/java/org/elasticsearch/index/translog/fs/FsSimpleTranslogTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/BlobContainerWrapper.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/BlobStoreWrapper.java</file><file>src/test/java/org/elasticsearch/snapshots/mockstore/MockRepository.java</file><file>src/test/java/org/elasticsearch/stresstest/fullrestart/FullRestartStressTest.java</file><file>src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/watcher/FileWatcherTest.java</file></files><comments><comment>[CORE] Cut over to Path API for file deletion</comment></comments></commit></commits></item><item><title>Bulk update child doc, NPE error message when parent is not specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8365</link><project id="" key="" /><description>If you do a _bulk that contains an update to a child doc (parent/child) and you don't (or forget to) specify the parent id, you will get an NPE error message in the item response. It would be good to adjust the error message to RoutingMissingException (just like when you do a single update (not _bulk) to the same doc but forget to specify parent id.

Steps to reproduce:

```
curl -XDELETE localhost:9200/test1

curl -XPUT localhost:9200/test1 -d '{
  "mappings": {
    "p": {},
    "c": {
      "_parent": {
        "type": "p"
      }
    }
  }
}'

curl -XPUT localhost:9200/test1/c/1?parent=1 -d '{
}'

curl -XPOST localhost:9200/test1/c/_bulk -d '
{ "update": { "_id": "1" }}
{ "doc": { "foo": "bar" } }
'
```

Response:

```
{"error":"NullPointerException[null]","status":500}
```
</description><key id="47964381">8365</key><summary>Bulk update child doc, NPE error message when parent is not specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>:Parent/Child</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-11-06T13:58:08Z</created><updated>2014-11-21T11:53:52Z</updated><resolved>2014-11-17T17:30:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wwken" created="2014-11-07T06:19:32Z" id="62103681">This is fixed in: https://github.com/elasticsearch/elasticsearch/pull/8378
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file></files><comments><comment>Bulk indexing issue - missing parent routing causes NullPointerException.</comment><comment>Now each error is reported in bulk response rather than causing entire bulk to fail.</comment><comment>Added a Junit test but the use of TransportClient means the error is manifested differently to a REST based request - instead of a NullPointer the whole of the bulk request failed with a RoutingMissingException. Changed TransportBulkAction to catch this exception and treat it the same as the existing logic for a ElasticsearchParseException - the individual bulk request items are flagged and reported individually rather than failing the whole bulk request.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file></files><comments><comment>fix of Bulk update child doc, NPE error message when parent is not specified #8365</comment><comment>  - Throw an RoutingMissingException instead of NPE</comment></comments></commit></commits></item><item><title>Issues during rolling update from 1.3.4 to 1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8364</link><project id="" key="" /><description>Currently doing a rolling update of my 3 node cluster from 1.3.4 to 1.4.0, but after upgrading the first machine (es-node-1), I noticed Kibana stopped working, digging through the logs, I can reproduce the error:

``` bash
curl es-node-1:9200/main-2014.39/_aliases
```

This returns:

``` json
{"error":"RemoteTransportException[[es-node-3][inet[/xxx.xxx.xxx.xxx:9300]][indices:admin/get]]; nested: ActionNotFoundTransportException[No handler for action [indices:admin/get]]; ","status":500}
```

and in the log of es-node-3:

```
[es-node-3] Message not fully read (request) for [128466] and action [indices:admin/get], resetting
```

However the same query on the other two nodes works as expected (tried for all of my indicies), eg:

``` bash
curl es-node-2:9200/main-2014.39/_aliases
```

Indexing seems to be happening as normal, and the _search endpoint also seems to work as expected.

I've now shut down es-node-3 and everything seems to work as expected, continuing with update...
</description><key id="47952667">8364</key><summary>Issues during rolling update from 1.3.4 to 1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jimmyjones2</reporter><labels><label>bug</label></labels><created>2014-11-06T11:38:02Z</created><updated>2014-11-07T15:31:15Z</updated><resolved>2014-11-07T15:31:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimmyjones2" created="2014-11-06T12:42:00Z" id="61973255">Also just noticed in the logs got some further related warnings, which I think was as node-3 came up with 1.4.0 (my rolling update was es-node-1, es-node-3, es-node-2):

```
[es-node-2] Message not fully read (request) for [0] and action [internal:discovery/zen/unicast_gte_1_4], resetting
```
</comment><comment author="colings86" created="2014-11-06T14:10:53Z" id="61983722">Thanks for reporting, we've found the issue. It occurs when the master node has not yet been upgraded and receives a request for GET _aliases from an upgraded node.  You can continue with your upgrade and the problem will disappear when you finish the upgrade. We'll work on a fix for it
</comment><comment author="colings86" created="2014-11-06T14:14:53Z" id="61984270">Also, the messages in your second comment (regarding `internal:discovery/zen/unicast_gte_1_4`) are expected. The new zen discovery attempts to communicate with the node with the new protocol and then falls back to the old protocol if it is not successful. These messages should also disappear when the upgrade is complete
</comment><comment author="jimmyjones2" created="2014-11-06T15:31:39Z" id="61996041">Thanks. Now everything is upgraded no problems.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor shard recovery from anonymous class to ShardRecoveryHandler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8363</link><project id="" key="" /><description>Previously the bulk of our shard recovery code was in a 300-line
anonymous class in `RecoverySource`. This made it difficult to find and
more difficult to read.

This factors out that code into a `ShardRecoveryHandler` class, adding
javadocs for each function and phase of the recovery, as well as
comments explaining some of the more esoteric functions performed during
recovery.

It's hoped that this will help more people understand Elasticsearch's
recovery procedure.

No _major_ functionality has changed, only typo corrections, some minor
allocation improvements and logging clarification changes.
</description><key id="47951708">8363</key><summary>Refactor shard recovery from anonymous class to ShardRecoveryHandler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-11-06T11:26:28Z</created><updated>2015-06-08T14:20:04Z</updated><resolved>2014-11-07T08:38:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-06T20:39:21Z" id="62047315">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elastic should log what's the expected number of nodes to elect master, when it fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8362</link><project id="" key="" /><description>In https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L1037 Elastic logs that it was not possible to elect master from active nodes. Since min_number of nodes is a transient value - its current value should be logged here as well, to understand how many was ussing
</description><key id="47948272">8362</key><summary>Elastic should log what's the expected number of nodes to elect master, when it fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">kretes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-11-06T10:48:59Z</created><updated>2017-04-18T02:18:28Z</updated><resolved>2017-04-18T02:18:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:49:33Z" id="158679973">The relevant code is now here: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L508
</comment><comment author="bleskes" created="2015-11-22T20:17:45Z" id="158794741">and here: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java#L942
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>OOM when indexing large volumes on a single 1.4b1 node </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8361</link><project id="" key="" /><description>We're importing compressed logs (about 7G uncompressed per day and testing with 30 days total) on a single elastic search node with 8G of heap of 16G total on the machine. I'm stress-testing the single ES node (centos 7 + latest openjdk7, ES installed via the RPM) with a maximum of 70 concurrent curl processes feeding ES started from a ruby script, using the bulk API, to insert about 1000 log lines (wildfly client request logs) per curl proces. Data is added to a daily index and I optimize the index after each day's import is completed.

After several hours ES 1.4b1 starts logging excessive gc activity like so:

[2014-11-06 00:00:02,079][INFO ][monitor.jvm              ] [Gorr] [gc][old][32172][4535] duration [8.3s], collections [1]/[8.8s], total [8.3s]/[2.8h], memory [7.5gb]-&gt;[7.3gb]/[7.7gb], all_pools {[young] [360.8mb]-&gt;[160.5mb]/[532.5mb]}{[survivor] [0b]-&gt;[0b]/[66.5mb]}{[old] [7.1gb]-&gt;[7.1gb]/[7.1gb]}
[2014-11-06 00:00:15,211][WARN ][monitor.jvm              ] [Gorr] [gc][old][32173][4536] duration [12.8s], collections [1]/[13.2s], total [12.8s]/[2.8h], memory [7.3gb]-&gt;[7.3gb]/[7.7gb], all_pools {[young] [160.5mb]-&gt;[190.1mb]/[532.5mb]}{[survivor] [0b]-&gt;[0b]/[66.5mb]}{[old] [7.1gb]-&gt;[7.1gb]/[7.1gb]}
[2014-11-06 00:00:28,292][WARN ][monitor.jvm              ] [Gorr] [gc][old][32174][4537] duration [12.5s], collections [1]/[13s], total [12.5s]/[2.8h], memory [7.3gb]-&gt;[7.2gb]/[7.7gb], all_pools {[young] [190.1mb]-&gt;[101.4mb]/[532.5mb]}{[survivor] [0b]-&gt;[0b]/[66.5mb]}{[old] [7.1gb]-&gt;[7.1gb]/[7.1gb]}

During previous runs with 2G heap I would get heap space exhausted exceptions which I initially attributed to query load coming from Kibana. However, I'm still seeing growing memory use with a 8GB heap for ES and no queries during indexing from Kibana.
I would expect the memory use to be roughly constant over time during indexing, but I only notice logged GC activity after importing &gt; 90 days of logs (about 90GB). Last night I had gc lines like the above during more than 10 hours after which I restarted the node to improve the situation. Imports continued at their usual pace after this node restart.

I'm aware that this issue description is vague. Please advice me on what metrics you need to debug this issue.

---

java version "1.7.0_71"
OpenJDK Runtime Environment (rhel-2.5.3.1.el7_0-x86_64 u71-b14)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)
</description><key id="47947533">8361</key><summary>OOM when indexing large volumes on a single 1.4b1 node </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">thomasmarkus</reporter><labels><label>feedback_needed</label></labels><created>2014-11-06T10:42:03Z</created><updated>2014-11-12T17:40:11Z</updated><resolved>2014-11-12T17:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-06T10:45:23Z" id="61961106">Hi @thomasmarkus 

To start, please could you upload the following:

```
curl "localhost:9200/_nodes" &gt; nodes_info.json
curl "localhost:9200/_nodes/stats?fields=*" &gt; nodes_stats.json
```
</comment><comment author="clintongormley" created="2014-11-06T10:49:10Z" id="61961673">Also, it would be useful to know if memory usage goes down after clearing the filter cache:

```
curl -XPOST 'http://localhost:9200/_cache/clear?filter'
```

You may have to wait up to one minute for this to have effect.
</comment><comment author="thomasmarkus" created="2014-11-07T06:51:22Z" id="62105667">nodes_info.json: https://gist.github.com/thomasmarkus/ec50fc18078597ca2eac
nodes_stats_before.json: https://gist.github.com/thomasmarkus/30721db90d791e3fd6c0

```
curl -XPOST 'http://localhost:9200/_cache/clear?filter'
{"_shards":{"total":4112,"successful":2056,"failed":0}}
```

node_stats_after.json: https://gist.github.com/thomasmarkus/9fc62556d25f0547734d
</comment><comment author="clintongormley" created="2014-11-07T13:57:52Z" id="62147795">@thomasmarkus thanks Mark.

The only thing I see here is that you have mlockall disabled and you have swap enabled, which is always going to end up giving you slow GCs.  

Please sort out these settings, try things again, and let us know how things go. See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory
</comment><comment author="thomasmarkus" created="2014-11-12T17:40:11Z" id="62759030">Actually on these machines I'm running without any swap, so I don't think that's the issue.
I've managed to get the performance issues under control by setting "index.merge.scheduler.max_thread_count: 1" , because these nodes are not running on SSDs.

This issue may be closed, because I cannot reliably reproduce this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to compute for the fields of matching documents in Elasticsearch?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8360</link><project id="" key="" /><description>Here is my sample document:

```
{
    "jobID": "ace4c888-1907-4021-a808-4a816e99aa2e",
    "startTime": 1415255164835,
    "endTime": 1415255164898,
    "moduleCode": "STARTING_MODULE"
}
```
- I have thousands of documents.
- I have a pair of documents with the **same jobID** and the module code would be STARTING_MODULE and ENDING_MODULE.
- My formula would be ENDING_MODULE endTime minus STARTING_MODULE startTime equals the elapsed time it took the module to process.

My question is: **How do I get the total of all results with the elapsed time that is less than let's say 28800000?**

Is such results possible with Elasticsearch? I'd like to display my results in Kibana too.

Please let me know if this needs more clarification. Thanks!
</description><key id="47947286">8360</key><summary>How to compute for the fields of matching documents in Elasticsearch?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asumawang</reporter><labels /><created>2014-11-06T10:39:31Z</created><updated>2014-11-06T10:47:26Z</updated><resolved>2014-11-06T10:47:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-06T10:47:26Z" id="61961402">Hi @asumawang 

Please ask questions like these in the mailing list: http://elasticsearch.org/community.  The GitHub issues list is for bug reports and feature requests.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't wait joinThread when stopping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8359</link><project id="" key="" /><description>When a node stops, we cancel any ongoing join process. With #8327, we improved this logic and wait for it to complete before shutting down the node. However, the joining thread is part of a thread pool and will not stop until the thread pool is shutdown.

Another issue raised by the unneeded wait is that when we shutdown, we may ping ourselves - which results in an ugly warn level log. We now log all remote exception during pings at a debug level.
</description><key id="47945487">8359</key><summary>Don't wait joinThread when stopping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.4.2</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-06T10:22:46Z</created><updated>2015-06-06T19:21:33Z</updated><resolved>2014-11-07T09:44:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-06T10:32:56Z" id="61958863">LGTM
</comment><comment author="bleskes" created="2014-11-06T10:55:26Z" id="61962766">pull request took the wrong approach. I reverted it from master and re-opening.
</comment><comment author="martijnvg" created="2014-11-06T11:31:30Z" id="61966444">LGTM
</comment><comment author="s1monw" created="2014-12-09T11:00:11Z" id="66266117">@bleskes should we push this to 1.4 too? We just timed out on a node here: 

http://build-us-00.elasticsearch.org/job/es_core_14_suse/144/
</comment><comment author="bleskes" created="2014-12-11T14:59:23Z" id="66630316">@s1monw done.
</comment><comment author="s1monw" created="2014-12-11T15:05:26Z" id="66631273">thx
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Discovery: don't wait joinThread when stopping</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Discovery: a more lenient wait joinThread when stopping</comment></comments></commit></commits></item><item><title>Keep the last legacy checksums file at the end of restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8358</link><project id="" key="" /><description> This commit fixes the issue caused by restore process deleting all legacy checksum files at the end of restore process. Instead it keeps the latest version of the checksum intact. The issue manifests itself in losing checksum for all legacy files restored into post 1.3.0 cluster, which in turn causes unnecessary snapshotting of files that didn't change.

Fixes #8119
</description><key id="47927055">8358</key><summary>Keep the last legacy checksums file at the end of restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-06T06:12:28Z</created><updated>2015-06-08T00:37:25Z</updated><resolved>2014-11-18T02:29:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-06T08:47:56Z" id="61943514">left some comments - good catch btw.
</comment><comment author="s1monw" created="2014-11-06T08:48:15Z" id="61943543">I guess this should go into `1.3.x` as well?
</comment><comment author="imotov" created="2014-11-06T17:28:43Z" id="62017351">@s1monw, I pushed the updated version. Yes, I think it makes sense to push to 1.3.6 as well if we will release it.
</comment><comment author="s1monw" created="2014-11-06T18:58:37Z" id="62031422">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add option to increase maxDeterminizedStates in Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8357</link><project id="" key="" /><description>With https://issues.apache.org/jira/browse/LUCENE-6046 (to be in 4.10.3, once it's eventually released) we added protection to the various APIs (AutomatonQuery, RegexpQuery, etc.) that attempt to determinize an automaton, to give up when too many states would be created (e.g. the "exponential" worst-cases) instead of trying to consume tons of RAM/CPU.

The default limit is 10,000 states, but there could be real use cases that need a higher limit.

Once 4.10.3 is released and we've upgraded we need to expose control for this new limit in Elasticsearch's rest/client APIs when creating automaton queries.
</description><key id="47892546">8357</key><summary>Add option to increase maxDeterminizedStates in Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-05T21:48:44Z</created><updated>2015-02-04T22:22:26Z</updated><resolved>2015-02-04T22:22:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-11-05T22:09:52Z" id="61890492">+1
</comment><comment author="mikemccand" created="2014-11-07T20:52:09Z" id="62209871">I added 1.5.0 (I misread Simon's comment on my first PR).

When I backport I'll upgrade 1.x branch to Lucene 4.10.3 snapshot.
</comment><comment author="mikemccand" created="2015-02-04T20:24:18Z" id="72931593">Now that 4.10.3 is released, and a user on ES list hit an Automaton bug fixed 4.10.3 (https://issues.apache.org/jira/browse/LUCENE-6046), I think we should port this back for 1.4.3 as well?
</comment><comment author="nik9000" created="2015-02-04T20:30:00Z" id="72932571">If you are backporting automaton issues it'd be easier to backport the
whole thing.  I suppose its a matter of complexity of the backport.

On Wed, Feb 4, 2015 at 3:24 PM, Michael McCandless &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; Now that 4.10.3 is released, and a user on ES list hit an Automaton bug
&gt; fixed 4.10.3 (https://issues.apache.org/jira/browse/LUCENE-6046), I think
&gt; we should port this back for 1.4.3 as well?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8357#issuecomment-72931593
&gt; .
</comment><comment author="mikemccand" created="2015-02-04T20:32:11Z" id="72932946">@nik9000 Yeah my plan is to backport max_determinized_states as well; it's really necessary because Lucene defaults the max to 10K states so users that hit that limit need a way to increase it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java</file><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/RegexpFilter.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/RegexpQueryParser.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/RafReference.java</file><file>src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java</file><file>src/test/java/org/elasticsearch/benchmark/fs/FsAppendBenchmark.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Core: add max_determinized_states to query_string and regexp query/filter</comment></comments></commit></commits></item><item><title>Make `and` filter parsing stricter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8356</link><project id="" key="" /><description>Closes #7311
</description><key id="47884634">8356</key><summary>Make `and` filter parsing stricter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wwken</reporter><labels><label>enhancement</label><label>feedback_needed</label><label>review</label></labels><created>2014-11-05T20:35:14Z</created><updated>2015-08-26T15:11:34Z</updated><resolved>2015-08-26T15:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-14T15:48:31Z" id="63083190">I am concerned that this fix is very specific to the issue described in #7311 and still leaves the door open to lots of corner cases if the filter is malformed. I think a better fix would be to fix `QueryParseContext.innerParse` to fail if the filter is malformed? For instance it has this suspicious logic at the beginning that makes it advance to the next start object, I suspect this is the root cause of this bug.
</comment><comment author="jpountz" created="2015-08-26T15:11:34Z" id="135056530">Closing due to lack of feedback. Additionally `and` is now deprecated.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tab characters in YAML should throw an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8355</link><project id="" key="" /><description>Better handling of tabs vs spaces in elasticsearch.yml
- Throw an exception if there is a 'tab' character in the elasticsearch.yml file

Closes #8259
</description><key id="47884180">8355</key><summary>Tab characters in YAML should throw an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">wwken</reporter><labels><label>:Settings</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-05T20:30:59Z</created><updated>2015-06-08T00:33:13Z</updated><resolved>2014-11-14T15:41:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-14T15:41:12Z" id="63081979">@wwken Merged, thanks for the PR!
</comment><comment author="jpountz" created="2014-11-21T09:53:53Z" id="63948165">I will backport to 1.3.6 as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] fix typo in getting-started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8354</link><project id="" key="" /><description /><key id="47874636">8354</key><summary>[docs] fix typo in getting-started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevinkluge</reporter><labels /><created>2014-11-05T19:06:04Z</created><updated>2014-11-06T09:58:55Z</updated><resolved>2014-11-06T09:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-05T20:15:57Z" id="61873717">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[docs] fix typo in getting-started</comment></comments></commit></commits></item><item><title>Upgrade to 1.4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8353</link><project id="" key="" /><description>After changing /etc/apt/sources.list/

`deb http://packages.elasticsearch.org/elasticsearch/1.3/debian stable main`

to 

`deb http://packages.elasticsearch.org/elasticsearch/1.4/debian stable main`

and running apt-get update &amp;&amp; apt-get upgrade, curl -XGET 'localhost:9200' is reporting version 1.4.0.Beta1.

```
curl -XGET 'localhost:9200'
{
  "status" : 200,
  "name" : "elasticsearch-node",
  "cluster_name" : "elasticsearch-cluster",
  "version" : {
    "number" : "1.4.0.Beta1",
    "build_hash" : "1f25669f3299b0680b266c3acaece43774fb59ae",
    "build_timestamp" : "2014-10-01T14:58:15Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.1"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="47853022">8353</key><summary>Upgrade to 1.4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">FabianKoestring</reporter><labels /><created>2014-11-05T16:06:22Z</created><updated>2014-11-05T19:07:20Z</updated><resolved>2014-11-05T19:07:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-05T16:11:43Z" id="61832129">@FabianKoestring thanks for letting us know.  It's a problem with naming.  We're going to remove the Beta1 release so that it recognises 1.4.0 as the latest.
</comment><comment author="FabianKoestring" created="2014-11-05T16:13:41Z" id="61832541">:+1: 
</comment><comment author="clintongormley" created="2014-11-05T16:15:03Z" id="61832881">@FabianKoestring repo is updated - could you try again please, and let us know if it works?
</comment><comment author="FabianKoestring" created="2014-11-05T16:24:21Z" id="61834759">Sry. For me nothing changes. Maybe it will take a while. Trying it later (1h) again.
</comment><comment author="electrical" created="2014-11-05T16:28:11Z" id="61835446">@FabianKoestring when running `apt-get update` and `apt-cache show elasticsearch  | grep -i version`
do you still see the 1.4.0 Beta release?
</comment><comment author="clintongormley" created="2014-11-05T16:28:25Z" id="61835489">@FabianKoestring I think you may need to remove the installed Beta1 version first,  and remember to run `apt-get update` first
</comment><comment author="FabianKoestring" created="2014-11-05T16:31:09Z" id="61836030">`apt-cache show elasticsearch | grep -i version`

&lt;hr&gt;

```
Version: 1.4.0.Beta1
Version: 1.4.0
```
</comment><comment author="clintongormley" created="2014-11-05T16:34:30Z" id="61836829">@FabianKoestring after doing an update?  We've tested it out locally and it appears to be working correctly. I'm guessing that your `apt` is still using cached info. 
</comment><comment author="FabianKoestring" created="2014-11-05T19:07:20Z" id="61862883">Yea! It was cached. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Minor fixes to the `match` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8352</link><project id="" key="" /><description>Fixed documentation since the default rewrite method for fuzzy queries is to
select top terms, fixed usage of the fuzzy rewrite method, and removed unused
`rewrite` parameter.

Close #6932
</description><key id="47842229">8352</key><summary>Minor fixes to the `match` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-11-05T14:36:27Z</created><updated>2015-07-08T15:03:25Z</updated><resolved>2015-07-08T15:03:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-05T14:37:04Z" id="61815573">This PR mostly applies @clintongormley 's recommandations on #6932 
</comment><comment author="clintongormley" created="2014-11-05T14:39:42Z" id="61816018">@jpountz i don't think the fuzzy rewrite should default to top_terms - it SHOULD be constant_score (unless the score only depends on edit distance, not idf?)
</comment><comment author="jpountz" created="2014-11-05T14:57:57Z" id="61819063">@clintongormley The fuzzy query does indeed take the edit distance into account when scoring.
</comment><comment author="clintongormley" created="2014-11-05T15:21:03Z" id="61822909">@jpountz but it still takes IDF into account, which means that misspellings are considered more relevant than the correct spelling.

I think that fuzzy queries across the board should either:
- take just edit distance into account, or
- be constant score

@rmuir @s1monw thoughts?
</comment><comment author="markharwood" created="2014-11-05T15:30:17Z" id="61824589">We created BlendedTermQuery to resolve some of these issues in multi-match. In that case it was dealing with the side-effects of auto-expanding the set of fields being queried but applies equally when auto-expanding the set of field values considered e.g. when doing fuzzy.
In any form of auto-expansion (field or terms) it is important to counter-act IDF's tendency to reward the most bizarre interpretation of the original input (the wrong field or the typo). BlendedTermQuery does this by taking all auto-expanded forms of a root clause and giving them the same DF for the purpose of IDF calcs.

The reason we want to keep some notion of IDF rather than dropping it completely is so that given a search that has multiple clauses e.g. John OR Patitucci~ we still favour a match on the rarer of these 2 top-level clauses (the variants of Patitucci, all of which are rewarded equally in terms of IDF but not edit distance)
</comment><comment author="rmuir" created="2014-11-05T17:30:07Z" id="61847572">I agree with @markharwood . The BlendedTermQuery should be used whenever two query terms are synonyms of each other and should be treated as 'one thing'. It tries to adjust statistics independently of the scoring function (which may have no concept of IDF) to deal with the problem.

But I think for it to work, it would need per-term boost support? Then we need a rewrite method that can build this instead of BooleanQuery, it would look a lot like the boolean one: https://github.com/apache/lucene-solr/blob/trunk/lucene/core/src/java/org/apache/lucene/search/MultiTermQuery.java#L140
</comment><comment author="clintongormley" created="2014-11-06T09:54:57Z" id="61953308">@rmuir why does it need per-term boost? In order to eg take the edit distance into account?
</comment><comment author="clintongormley" created="2014-11-06T09:56:18Z" id="61953550">Also wondering if we should exposed the BlendedTermQuery in the DSL for expert use cases?
</comment><comment author="rmuir" created="2014-11-06T10:07:07Z" id="61955264">&gt; @rmuir why does it need per-term boost? In order to eg take the edit distance into account?

yes
</comment><comment author="rmuir" created="2014-11-06T12:08:12Z" id="61969993">&gt; Also wondering if we should exposed the BlendedTermQuery in the DSL for expert use cases?

I assumed it already was actually: but it looks like its only exposed as the cross_fields mode for multi match?

I agree with you, it would be nice if it were exposed in some way that can be used for query-time synonyms in the same field too.
</comment><comment author="clintongormley" created="2014-12-30T16:32:32Z" id="68371406">I've opened a new issue for the BlendedTerms query (#9103) so that this PR can be merged.
</comment><comment author="s1monw" created="2015-03-20T21:34:09Z" id="84154632">@jpountz afaik this can be merged, do you wanna go ahead?
</comment><comment author="clintongormley" created="2015-05-29T16:59:24Z" id="106870899">@jpountz pinging in case you've forgotten
</comment><comment author="jpountz" created="2015-07-08T14:24:04Z" id="119595632">Rebased to a recent master. The default rewrite method for fuzzy queries changed in the mean time so I had to change it. I'll merge this PR once #12129 is in too.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file><file>core/src/main/java/org/elasticsearch/index/search/MatchQuery.java</file></files><comments><comment>Merge pull request #8352 from jpountz/fix/match_rewrite</comment></comments></commit></commits></item><item><title>Need to get dynamic fields as facets with out mentioned the field for a index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8351</link><project id="" key="" /><description>Hi All,

   is there any possibility in elastic search to get dynamic fields as facets with out giving any field name for a index.

Thanks

phani.
</description><key id="47839520">8351</key><summary>Need to get dynamic fields as facets with out mentioned the field for a index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phani546</reporter><labels /><created>2014-11-05T14:09:41Z</created><updated>2014-11-05T14:16:49Z</updated><resolved>2014-11-05T14:16:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-05T14:16:49Z" id="61812551">Hi @phani546 

No, but have a look https://github.com/elasticsearch/elasticsearch/issues/5789#issuecomment-59472925 for an alternate approach to this problem.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Observe cluster state on health request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8350</link><project id="" key="" /><description>Today we use busy waiting and sampling when we execute HealthReqeusts
on the master. This is tricky sicne we might sample a not yet fully applied
cluster state and make a decsions base on the partial cluster state. This can
lead to ugly problems since requests might be routed to nodes where shards are
already marked as relocated but on the actual cluster state they are still started.
Yet, this window is very small usually it can lead to ugly test failures.

This commit moves the health request over to a listener pattern that gets the actual
applied cluster state.
</description><key id="47837912">8350</key><summary>Observe cluster state on health request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-05T13:54:00Z</created><updated>2015-06-06T19:21:55Z</updated><resolved>2014-11-07T10:11:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-05T13:54:30Z" id="61809626">@bleskes here you go :)
</comment><comment author="s1monw" created="2014-11-06T15:23:40Z" id="61994606">@bleskes I updated the PR as we discussed
</comment><comment author="s1monw" created="2014-11-06T16:37:22Z" id="62007802">pushed another round
</comment><comment author="s1monw" created="2014-11-06T18:57:02Z" id="62031114">pushed a check for `timeout == 0` I think it's ready
</comment><comment author="bleskes" created="2014-11-06T23:50:35Z" id="62073247">Agreed. LGTM - thanks for cleaning this up.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterStateObserver.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file></files><comments><comment>[STATE] Observe cluster state on health request</comment></comments></commit></commits></item><item><title>Issue with icu collation when upgrading from 0.20.2 to 1.3.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8349</link><project id="" key="" /><description>## 1. ICU version

ElasticSearch 0.20.2 : elasticsearch-analysis-icu-1.7.0
ElasticSearch 1.3.4 : elasticsearch-analysis-icu-2.3.0
## 2. config/elasticsearch.yml

```
index:
  analysis: 
    filter:
      chinese_collator:
          type: icu_collation
          language: zh-cn
    analyzer:      
      keyword_chinese_collator:
          type: custom
          tokenizer: keyword
          filter: [chinese_collator]
```
## 3. Mapping

``` sh
curl -XDELETE localhost:9200/index1?pretty

curl -XPUT localhost:9200/index1?pretty

curl localhost:9200/index1/type1/_mapping?pretty -d '
{
  "type1" : {
    "properties" : {
      "beisen" : {
        "type" : "multi_field",
        "fields" : {
          "beisen" : { "type" : "string", "analyzer" : "standard" },
          "forsort" : { "type" : "string", "analyzer" : "keyword_chinese_collator" }
        }
      }
    }
  }
}'
```
## 4. Index some docs under 0.20.2

``` sh
curl 'localhost:9200/index1/type1/1?routing=beisen&amp;pretty' -d '
{
  "beisen" : "李媛媛"
}'

curl 'localhost:9200/index1/type1/2?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘震宇"
}'

curl 'localhost:9200/index1/type1/3?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘振东"
}'


curl 'localhost:9200/index1/type1/4?routing=beisen&amp;pretty' -d '
{
  "beisen" : "陆燕春"
}'


curl 'localhost:9200/index1/type1/5?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘秀东"
}'

curl 'localhost:9200/index1/type1/6?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘新颖"
}'

curl 'localhost:9200/index1/type1/7?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘小远"
}'

curl 'localhost:9200/index1/type1/8?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘晓航"
}'


curl 'localhost:9200/index1/type1/9?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘晓航"
}'


curl 'localhost:9200/index1/type1/10?routing=beisen&amp;pretty' -d '
{
  "beisen" : "李君"
}'

curl 'localhost:9200/index1/type1/11?routing=beisen&amp;pretty' -d '
{
  "beisen" : "李锋br"
}'

curl 'localhost:9200/index1/type1/12?routing=beisen&amp;pretty' -d '
{
  "beisen" : "李锋"
}'

curl 'localhost:9200/index1/type1/13?routing=beisen&amp;pretty' -d '
{
  "beisen" : "何平"
}'

curl 'localhost:9200/index1/type1/14?routing=beisen&amp;pretty' -d '
{
  "beisen" : "郭鹏程"
}'

curl 'localhost:9200/index1/type1/15?routing=beisen&amp;pretty' -d '
{
  "beisen" : "黄黄"
}'

curl 'localhost:9200/index1/type1/16?routing=beisen&amp;pretty' -d '
{
  "beisen" : "高高"
}'
```
## 5. Optimize

``` sh
curl 'localhost:9200/_optimize?max_num_segments=1&amp;pretty'
```

This step is _critical_, if it is skipped, the issue will not occur.
## 6. Upgrade to 1.3.4 &amp; Index more docs

``` sh
curl 'localhost:9200/index1/type1/17?routing=beisen&amp;pretty' -d '
{
  "beisen" : "刘六"
}'

curl 'localhost:9200/index1/type1/18?routing=beisen&amp;pretty' -d '
{
  "beisen" : "张三"
}'

curl 'localhost:9200/index1/type1/19?routing=beisen&amp;pretty' -d '
{
  "beisen" : "李四"
}'


curl 'localhost:9200/index1/type1/20?routing=beisen&amp;pretty' -d '
{
  "beisen" : "王五"
}'


curl 'localhost:9200/index1/type1/21?routing=beisen&amp;pretty' -d '
{
  "beisen" : "李三"
}'
```
## 7. Search 

``` sh
curl 'localhost:9200/index1/type1/_search?routing=beisen&amp;pretty' -d '
{
  "size" : 100,
  "sort" : [
    { "beisen.forsort" : "asc" }
  ],
  "fields" : ""
}'
```

The result order is:

```
  "_id" : "21",
  "_id" : "19",
  "_id" : "17",
  "_id" : "20",
  "_id" : "16",
  "_id" : "14",
  "_id" : "13",
  "_id" : "15",
  "_id" : "18",
  "_id" : "12",
  "_id" : "11",
  "_id" : "10",
  "_id" : "1",
  "_id" : "7",
  "_id" : "8",
  "_id" : "9",
  "_id" : "6",
  "_id" : "5",
  "_id" : "3",
  "_id" : "2",
  "_id" : "4",
```

But the expected order is:

```
  "_id" : "16",
  "_id" : "14",
  "_id" : "13",
  "_id" : "15",
  "_id" : "12",
  "_id" : "11",
  "_id" : "10",
  "_id" : "21",
  "_id" : "19",
  "_id" : "1",
  "_id" : "17",
  "_id" : "7",
  "_id" : "8",
  "_id" : "9",
  "_id" : "6",
  "_id" : "5",
  "_id" : "3",
  "_id" : "2",
  "_id" : "4",
  "_id" : "20",
  "_id" : "18",
```
</description><key id="47814413">8349</key><summary>Issue with icu collation when upgrading from 0.20.2 to 1.3.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mashudong</reporter><labels /><created>2014-11-05T09:12:18Z</created><updated>2014-11-06T03:23:54Z</updated><resolved>2014-11-05T11:39:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-05T11:32:10Z" id="61794381">Hi @mashudong 

The problem is that ICU doesn't have any backwards compatibility promises.  Fortunately it doesn't change often, and when it does change, usually it doesn't change much. That said, you're moving from an old version of Lucene.  The only way around this issue is to reindex your data with the new ICU plugin.
</comment><comment author="clintongormley" created="2014-11-05T11:39:08Z" id="61795045">In this case, the change is probably due to the fact that ICU's zh-cn used to default to pinyin ordering (which is a massive ruleset sized in megabytes) and now its radical-stroke based instead.
</comment><comment author="mashudong" created="2014-11-06T03:23:54Z" id="61922019">@clintongormley Thank you very much for your reply.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tab characters in YAML should throw an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8348</link><project id="" key="" /><description>Betterr handling of tabs vs spaces in elasticsearch.yml
- Throw an exception if there is a 'tab' character in the elasticsearch.yml file

Closes #8259
</description><key id="47780200">8348</key><summary>Tab characters in YAML should throw an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wwken</reporter><labels><label>enhancement</label></labels><created>2014-11-04T23:24:30Z</created><updated>2015-03-19T10:18:51Z</updated><resolved>2014-11-06T10:00:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-06T09:59:18Z" id="61954074">@wwken why did you close this PR? I assume it was in error so I have reopened it
</comment><comment author="clintongormley" created="2014-11-06T10:00:09Z" id="61954224">Ah I see - you opened a new one.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade master to lucene 5.0 snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8347</link><project id="" key="" /><description>This has a lot of improvements in lucene, particularly around memory usage, merging, safety, compressed bitsets, etc.

On the elasticsearch side, summary of the larger changes:
- API changes: postings API became a "pull" rather than "push", collector API became per-segment, etc.
- packaging changes: add `lucene-backwards-codecs.jar` as a dependency.
- improvements to boolean filtering: especially ensuring it will not be slow for SparseBitSet.
- use generic BitSet api in plumbing so that concrete bitset type is an implementation detail.
- use generic BitDocIdSetFilter api for dedicated bitset cache, so there is type safety.
- changes to support atomic commits
- implement `Accountable.getChildResources` (detailed memory usage API) for fielddata, etc
- change handling of IndexFormatTooOld/New, since they no longer extends CorruptIndexException
</description><key id="47771087">8347</key><summary>Upgrade master to lucene 5.0 snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.0.0-beta1</label></labels><created>2014-11-04T21:53:58Z</created><updated>2015-08-25T13:25:44Z</updated><resolved>2014-11-05T20:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-04T22:49:28Z" id="61729729">changes are too large for github, here is a full patch: http://people.apache.org/~rmuir/8347.patch
</comment><comment author="s1monw" created="2014-11-05T20:42:02Z" id="61877373">I went through the changes one more time and I think it looks awesome. I am 100% sure we missed something but we have to move on here since this is un-maintainable. We have plenty of time to get the TODOs sorted out so here is my LGTM and +1 to squash, merge and push. @rmuir do you wanna go ahead?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/analysis/PrefixAnalyzer.java</file><file>src/main/java/org/apache/lucene/analysis/miscellaneous/UniqueTokenFilter.java</file><file>src/main/java/org/apache/lucene/queries/BlendedTermQuery.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java</file><file>src/main/java/org/apache/lucene/search/XFilteredDocIdSetIterator.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>src/main/java/org/elasticsearch/ElasticsearchCorruptionException.java</file><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorFields.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/common/lucene/MinimumScoreCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/MultiCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/ReaderContextAware.java</file><file>src/main/java/org/elasticsearch/common/lucene/SegmentReaderUtils.java</file><file>src/main/java/org/elasticsearch/common/lucene/all/AllField.java</file><file>src/main/java/org/elasticsearch/common/lucene/all/AllTermQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/AndDocIdSet.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/ContextDocIdSet.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/DocIdSets.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/MatchDocIdSet.java</file><file>src/main/java/org/elasticsearch/common/lucene/docset/OrDocIdSet.java</file><file>src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/AndFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/ApplyAcceptedDocsFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/FilteredCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/LimitFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MatchAllDocsFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MatchNoDocsFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MatchNoDocsQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/MultiPhrasePrefixQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/NoopCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/NotFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/OrFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/RegexpFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XCollector.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/BoostScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FieldValueFactorFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/FunctionScoreQuery.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/RandomScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/ScriptScoreFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/function/WeightFactorFunction.java</file><file>src/main/java/org/elasticsearch/common/lucene/store/OutputStreamIndexOutput.java</file><file>src/main/java/org/elasticsearch/common/lucene/uid/PerThreadIDAndVersionLookup.java</file><file>src/main/java/org/elasticsearch/common/lucene/uid/Versions.java</file><file>src/main/java/org/elasticsearch/common/util/AbstractArray.java</file><file>src/main/java/org/elasticsearch/common/util/BloomFilter.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/CorruptStateException.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>src/main/java/org/elasticsearch/index/analysis/ArabicAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/ArmenianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/BasqueAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/BrazilianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/BrazilianStemTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/BulgarianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/CatalanAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/ChineseAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/CjkAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/ClassicTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/CommonGramsTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/CustomAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/CzechAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/DanishAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/DutchAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/DutchStemTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/ElisionTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EnglishAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/FinnishAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/FrenchAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/FrenchStemTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/GalicianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/GermanAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/GermanStemTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/GreekAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/HindiAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/HungarianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/IndonesianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/IrishAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/ItalianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/KeepTypesFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/KeepWordFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/KeywordMarkerTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/KeywordTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/LatvianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/LengthTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/LetterTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/LowerCaseTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/NGramTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/NorwegianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericDateAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericDateTokenizer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericDoubleAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericDoubleTokenizer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericFloatAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericFloatTokenizer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericIntegerAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericIntegerTokenizer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericLongAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericLongTokenizer.java</file><file>src/main/java/org/elasticsearch/index/analysis/NumericTokenizer.java</file><file>src/main/java/org/elasticsearch/index/analysis/PathHierarchyTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PersianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/PortugueseAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/ReverseTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/RomanianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/RussianAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/SimpleAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/SnowballAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/SoraniAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/SpanishAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/StopAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StopTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/SwedishAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/ThaiAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/ThaiTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/TokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/TrimTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/TurkishAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/UAX29URLEmailTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/UpperCaseTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/WhitespaceAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/WhitespaceTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/WordDelimiterTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/AbstractCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/DictionaryCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/BitsetFilterCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/ShardBitsetFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/bitset/ShardBitsetFilterCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/support/CacheKeyFilter.java</file><file>src/main/java/org/elasticsearch/index/cache/filter/weighted/WeightedFilterCache.java</file><file>src/main/java/org/elasticsearch/index/cache/fixedbitset/FixedBitSetFilter.java</file><file>src/main/java/org/elasticsearch/index/codec/PerFieldMappingPostingFormatCodec.java</file><file>src/main/java/org/elasticsearch/index/codec/docvaluesformat/DocValuesFormats.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/DefaultPostingsFormatProvider.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/Elasticsearch090PostingsFormat.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/PostingFormats.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/SegmentsStats.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/LiveVersionMap.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/VersionValue.java</file><file>src/main/java/org/elasticsearch/index/fielddata/AtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataCache.java</file><file>src/main/java/org/elasticsearch/index/fielddata/NumericDoubleValues.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefFieldComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/FloatValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/LongValuesComparatorSource.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/GlobalOrdinalsIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/InternalGlobalOrdinalsIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/MultiOrdinals.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/OrdinalsBuilder.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ordinals/SinglePackedOrdinals.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractAtomicGeoPointFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractAtomicOrdinalsFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractAtomicParentChildFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractIndexOrdinalsFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AtomicDoubleFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AtomicLongFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVNumericIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BytesBinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BytesBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/IndexIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/NumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIntersectTermsEnum.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVBytesAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVOrdinalsIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/ElasticsearchMergePolicy.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>src/main/java/org/elasticsearch/index/query/ConstantScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndexQueryParserService.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/ScriptFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/support/QueryParsers.java</file><file>src/main/java/org/elasticsearch/index/query/support/XContentStructure.java</file><file>src/main/java/org/elasticsearch/index/search/FieldDataTermsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/NumericRangeFieldDataFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/CustomQueryWrappingFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentIdsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/InMemoryGeoBoundingBoxFilter.java</file></files><comments><comment>Upgrade master to lucene 5.0 snapshot</comment></comments></commit></commits></item><item><title>Tests: queryThenFetch failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8346</link><project id="" key="" /><description>[core es master medium](http://build-us-1.elasticsearch.org/job/es_core_master_medium/505/) fails twice testing TransportTwoNodesSearchTests.

```
MockDirectoryWrapper: cannot close: there are still open files: {_0.cfs=1}
```

and 

```
Unclosed Searchers instance for shards: [[test][0],]
```
</description><key id="47765824">8346</key><summary>Tests: queryThenFetch failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mrsolo/following{/other_user}', u'events_url': u'https://api.github.com/users/mrsolo/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mrsolo/orgs', u'url': u'https://api.github.com/users/mrsolo', u'gists_url': u'https://api.github.com/users/mrsolo/gists{/gist_id}', u'html_url': u'https://github.com/mrsolo', u'subscriptions_url': u'https://api.github.com/users/mrsolo/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/96771?v=4', u'repos_url': u'https://api.github.com/users/mrsolo/repos', u'received_events_url': u'https://api.github.com/users/mrsolo/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mrsolo/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mrsolo', u'type': u'User', u'id': 96771, u'followers_url': u'https://api.github.com/users/mrsolo/followers'}</assignee><reporter username="">c-a-m</reporter><labels><label>test</label></labels><created>2014-11-04T21:12:00Z</created><updated>2014-11-04T22:46:04Z</updated><resolved>2014-11-04T22:46:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="c-a-m" created="2014-11-04T21:12:39Z" id="61715303">@mrsolo is this a test machine issue?
</comment><comment author="c-a-m" created="2014-11-04T21:15:00Z" id="61715665">[Here is another occurrence on the debian machine](http://build-us-1.elasticsearch.org/job/es_core_master_debian/2142/)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Customize advanced shard recovery strategy to alleviate split-brain impact</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8345</link><project id="" key="" /><description>I post the same issue on ES forum a few months back, at: https://groups.google.com/forum/#!msg/elasticsearch/xPpH1HbAGBc/75td8RPkVUgJ.
Thanks to ES team, most of ES split-brain issues have been addressed in 1.4.x. But, during my local tests, I still found an issue after shard recovery.

The problem occurs in 2 nodes cluster with quorum size of 1. I know this is not a recommend setting by ES. But user might have specific requirement of 2 nodes cluster. And I think it is would be better to have high availability in 2 nodes cluster as well. 

Here is the issue with 2 nodes cluster and quorum size of 1 in 1.4.0.Beta1:
- cut off the network between node#1 and node#2, both of nodes become master
- then bring back the network, split-brain occurs, modify two record with different values respectively:
  - 1st record value, modify node#1’s to A first, then node#2’s to B
  - 2nd record value, modify node#2’s to A first, then node#1’s to B
- after that, restart node#2, cluster turn back to normal status:
  - node#1 is the master, records got merged.
  - BUT, 1st record == A, 2nd record == B. That is because all primary shards located in node#1.
- bring down node#1, node#2 become master, then bring back node#1.
  - Now, all primary shards located in node#2. 
  - 1st record == A, 2nd record == B still. That means replica shards got synced with primary.

The above test shows we could have data loss with this kind of setting.
**What if we could customize our advanced shard recovery strategy that, document with the latest timestamp overwrites the old one?** In this case, we could guarantee that latest modification got preserved in ES cluster, right? Is it possible to have this feature in later release? Thanks.

~ Jing Liu
</description><key id="47760027">8345</key><summary>Customize advanced shard recovery strategy to alleviate split-brain impact</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mitkook</reporter><labels /><created>2014-11-04T20:19:25Z</created><updated>2014-11-05T11:34:59Z</updated><resolved>2014-11-05T11:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-05T11:34:58Z" id="61794655">&gt; In this case, we could guarantee that latest modification got preserved in ES cluster, right?

No, you couldn't guarantee anything.  Clocks, even with NTP, diverge. Updates can happen which depend on the current state of the document, those could well be wrong. There are so many potential issues with this setup that we could never make it work correctly in all cases.

We never do shard level merging. Whichever shard is chosen as primary wins. Even if we had atomic clocks, going through two shard copies and deciding which documents to use is just too expensive.

The advice is clear: to avoid a split-brain you need to set `minimum_master_nodes` to a quorum of the master-eligible nodes in the cluster.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: RoutingBackwardCompatibilityUponUpgradeTests: SimpleHashFunction but was:&lt;null&gt;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8344</link><project id="" key="" /><description>Failing on [es_core_master_small](http://build-us-1.elasticsearch.org/job/es_core_master_small/882/testReport/org.elasticsearch.cluster.routing/RoutingBackwardCompatibilityUponUpgradeTests/testCustomRouting/)

```
java.lang.AssertionError: expected:&lt;org.elasticsearch.cluster.routing.operation.hash.simple.SimpleHashFunction&gt; but was:&lt;null&gt;
    __randomizedtesting.SeedInfo.seed([9BB079AAA1CBCBD:6958E48DC1F33431]:0)
    [...org.junit.*]
    org.elasticsearch.cluster.routing.RoutingBackwardCompatibilityUponUpgradeTests.test(RoutingBackwardCompatibilityUponUpgradeTests.java:63)
    org.elasticsearch.cluster.routing.RoutingBackwardCompatibilityUponUpgradeTests.testCustomRouting(RoutingBackwardCompatibilityUponUpgradeTests.java:49)
    [...sun.*, java.lang.reflect.*, com.carrotsearch.randomizedtesting.*]
```

Also, see the ["DjbHashFunction" failing](http://build-us-1.elasticsearch.org/job/es_core_master_small/882/testReport/org.elasticsearch.cluster.routing/RoutingBackwardCompatibilityUponUpgradeTests/testDefaultRouting/) in the same build.

I am not able to reproduce locally.  And this doesn't occur routinely.
</description><key id="47759096">8344</key><summary>Tests: RoutingBackwardCompatibilityUponUpgradeTests: SimpleHashFunction but was:&lt;null&gt;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">c-a-m</reporter><labels><label>test</label></labels><created>2014-11-04T20:10:49Z</created><updated>2014-11-04T22:32:17Z</updated><resolved>2014-11-04T22:32:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="c-a-m" created="2014-11-04T21:27:34Z" id="61717605">It looks like this was disabled here https://github.com/elasticsearch/elasticsearch/commit/181bd6e56a2e6cfa1e0e85a5a7778cae5a8e62e1
by @jpountz 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Prefer recovering the state file that uses the latest format.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8343</link><project id="" key="" /><description>Currently MetaDataStateFormat loads the first available state file that has
the latest version. In case several files are available and some of them use
the new format while other ones use the legacy format, it should also prefer
the new format. This is typically useful when we upgrade the metadata when
recovering from the gateway: we might write the upgraded state with the new
format while the previous state used the legacy format, so we end up with
two files having the same version but using different formats.
</description><key id="47745426">8343</key><summary>Prefer recovering the state file that uses the latest format.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-04T18:07:17Z</created><updated>2015-06-08T00:23:08Z</updated><resolved>2014-11-04T18:59:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-04T18:22:15Z" id="61688384">left a couple of comments
</comment><comment author="s1monw" created="2014-11-04T18:33:37Z" id="61690234">thanks @jpountz I understand now better... LGTM in general feel free to push once you updated the test
</comment><comment author="jpountz" created="2014-11-05T08:18:06Z" id="61773301">Labeled as a non-issue as it was caused by an unreleased change.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/test/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormatTest.java</file></files><comments><comment>Gateway: Prefer recovering the state file that uses the latest format.</comment></comments></commit></commits></item><item><title>Index Creation Timestamp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8342</link><project id="" key="" /><description>I'd like to see an attribute of an index that stores the timestamp of when that index was created. At the moment, the convention is to name indices 'index-YYYY.MM.DD' implying the day it was created (because it's assumed only events from that day necessitated its creation).

However, I see two possible uses for such an attribute:
1. In the event the cluster is in recovery, more often than not I care about the most recent indices recovering before any others (i.e. the past 2 days). I'd like the recovery process to be configurable such that I can define a preference for those indices, based on their creation timestamp.
2. It's not uncommon for "future" and "past" indices to show up due to servers/devices with faulty clocks or other weird phenomena such as mobile apps that bulk send logs (i.e. delayed) when connected to Wifi. Knowing when an index was created may help us identify what is happening. At a minimum, we'll have a more certain way to manage indices than the date implied in their name. For example, `curator` depends heavily on the naming convention described above; I'd like my tools to not be coupled to the names of my indices like this.
</description><key id="47722501">8342</key><summary>Index Creation Timestamp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RyanFrantz</reporter><labels /><created>2014-11-04T15:03:22Z</created><updated>2014-11-04T16:00:25Z</updated><resolved>2014-11-04T15:44:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-04T15:44:00Z" id="61659330">@RyanFrantz Your wish has been granted: https://github.com/elasticsearch/elasticsearch/pull/7218 :)
</comment><comment author="RyanFrantz" created="2014-11-04T16:00:25Z" id="61662340">YES! Thanks for pointing that out.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restore of indices that are only partially available in the cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8341</link><project id="" key="" /><description>Fixes the issue with restoring of an index that had only some of its primary shards allocated before it was closed.

Fixes #8224
</description><key id="47722119">8341</key><summary>Restore of indices that are only partially available in the cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-04T15:00:01Z</created><updated>2015-06-08T00:37:44Z</updated><resolved>2014-11-18T00:38:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-09T20:46:47Z" id="62318828">left a cosmetic comment LGTM otherwise
</comment><comment author="s1monw" created="2014-11-21T09:54:59Z" id="63948288">@imotov should this go into `1.3` as well?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Several recoveries cause IndexShardGatewayRecoveryException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8340</link><project id="" key="" /><description>I have tests environment that restore the index from snapshot before every test.
After a few successful restorings, it fails on:

_[2014-11-03 16:03:54,957][WARN ][indices.cluster ] [Baron Von Blitzschlag] [qs_rm_3][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [qs_rm_3][0] failed recovery
at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [qs_rm_3][0] restore failed
at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:130)
at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)
... 3 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [qs_rm_3][0] failed to restore snapshot [qs_rm_alias]
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:159)
at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)
... 4 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [qs_rm_3][0] Failed to recover index
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:840)
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:156)
... 5 more
Caused by: java.io.FileNotFoundException: C:\TestResults\QuickSearch\data\elasticsearch\nodes\0\indices\qs_rm_3\0\index_8.si (Access is denied)
at java.io.FileOutputStream.open(Native Method)
at java.io.FileOutputStream.(Unknown Source)
at java.io.FileOutputStream.(Unknown Source)
at org.apache.lucene.store.FSDirectory$FSIndexOutput.(FSDirectory.java:389)
at org.apache.lucene.store.FSDirectory.createOutput(FSDirectory.java:282)
at org.apache.lucene.store.RateLimitedFSDirectory.createOutput(RateLimitedFSDirectory.java:40)
at org.elasticsearch.index.store.DistributorDirectory.createOutput(DistributorDirectory.java:118)
at org.apache.lucene.store.FilterDirectory.createOutput(FilterDirectory.java:69)
at org.elasticsearch.index.store.Store.createVerifyingOutput(Store.java:298)
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:887)
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:830)
... 6 more_
</description><key id="47721876">8340</key><summary>Several recoveries cause IndexShardGatewayRecoveryException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">asafc64</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2014-11-04T14:57:54Z</created><updated>2014-11-25T01:55:16Z</updated><resolved>2014-11-25T01:55:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-04T15:40:35Z" id="61658717">It appears you have some permission problem?

&gt; C:\TestResults\QuickSearch\data\elasticsearch\nodes\0\indices\qs_rm_3\0\index_8.si (Access is denied)

@imotov or could this be due to Windows locking the file when being access by another process?

@asafc64 what version are you on?
</comment><comment author="asafc64" created="2014-11-04T15:44:13Z" id="61659368">permission problem - I don't think so, since it succeeds most of the times.
access by another process - I verified that only one instance of ElasticSearch is running.
version - 1.3.4
</comment><comment author="imotov" created="2014-11-04T16:30:39Z" id="61667835">@asafc64 could you tell us how your test is structured? What sort of cleanup are you doing before/after the test? In particular, do you close the index or delete it? How do you perform these operations? Do you wait for their completion? What operations are performed in the test preceding the test that fails?
</comment><comment author="asafc64" created="2014-11-05T07:33:17Z" id="61769639">When the environment is loaded, its creates a snap shot, and before every test it's being restored.
Every 5-9 successful restorings, one fails.
Restore procedure:
1. Close index.
2. Restore index.
3. Get RecoveryStatus until all shards are done.
4. Open index.

By the way, 
some times I get this error although I check that all shards are done:
_[2014-11-05 07:57:21,288][WARN ][snapshots                ] [Elektro] [EsBackups:qs_rm_alias] failed to restore snapshot
org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException: [EsBackups:qs_rm_alias] Restore process is already running in this cluster
    at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:139)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)_
</comment><comment author="imotov" created="2014-11-08T00:28:55Z" id="62235476">@asafc64 on the step 3 when shards are reported done there is still some additional operations that should take place before the restore process is really over. So,  it's not very reliable method of checking restore status in tests. Opening index after restore is not needed, because restore will open automatically once restore is complete. I would recommend the following approach
1. Close Index
2. Running restore with `wait_for_completion=true`
3. Run cluster health for the restored index with `wait_for_status=green`
</comment><comment author="clintongormley" created="2014-11-08T12:28:47Z" id="62256063">@imotov is there some additional status that we should report in recovery status to indicate that the process isn't quite complete?
</comment><comment author="imotov" created="2014-11-17T17:11:53Z" id="63338727">@clintongormley, I am not sure what we can/should do here since the recoveries are reported on the shard level. I think there is a couple of concurrency-related things going on there. First of all there is slight delay in shard status propagation through the cluster between the moment when a shard is done recovering and the moment when a cluster knows that it got started. There is also a global restore clean up stage that is performed once all shards are restored. So, when all shards are restored the restore process is still technically running. I think the recovery API is good to monitor the recovery/restore progress of shards, but it's not a good method to check if restore is done or not globally. 
</comment><comment author="clintongormley" created="2014-11-17T17:16:48Z" id="63339521">@imotov fair enough.  Plus the docs already tell you how to monitor a restore: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html#_monitoring_snapshot_restore_progress
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: restore with wait_for_completion=true should wait for succesfully restored shards to get started</comment></comments></commit></commits></item><item><title>Aggregations: date_histogram aggregation DST bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8339</link><project id="" key="" /><description>Hi,

Since it is the time of year where we adjust our clocks from daylight savings time to normal time again a bug in the date histogram struck us.

When we run a date_histogram in a timezone other than UTC you will see some buckets being wrongfully combined. In the first example you see a `date_histogram` aggregate in UTC followed by the same aggregate in CET;

UTC query:

```
{
    "size": 0,
    "query": {
        "range": {
           "published": {
              "gte": 1414274400000,
              "lt": 1414292400000
           }
        }
    },
    "aggs": {
        "vot": {
            "date_histogram": {
                "field": "published",
                "interval": "hour",
                "min_doc_count": 0,
                "time_zone": "UTC"
            }
        }
    }
}
```

UTC result:

```
{
   "took": 12,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 1130,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "vot": {
         "buckets": [
            {
               "key_as_string": "2014-10-25T22:00:00.000Z",
               "key": 1414274400000,
               "doc_count": 260
            },
            {
               "key_as_string": "2014-10-25T23:00:00.000Z",
               "key": 1414278000000,
               "doc_count": 216
            },
            {
               "key_as_string": "2014-10-26T00:00:00.000Z",
               "key": 1414281600000,
               "doc_count": 222
            },
            {
               "key_as_string": "2014-10-26T01:00:00.000Z",
               "key": 1414285200000,
               "doc_count": 200
            },
            {
               "key_as_string": "2014-10-26T02:00:00.000Z",
               "key": 1414288800000,
               "doc_count": 232
            }
         ]
      }
   }
}
```

CET query:

```
{
    "size": 0,
    "query": {
        "range": {
           "published": {
              "gte": 1414274400000,
              "lt": 1414292400000
           }
        }
    },
    "aggs": {
        "vot": {
            "date_histogram": {
                "field": "published",
                "interval": "hour",
                "min_doc_count": 0,
                "time_zone": "CET"
            }
        }
    }
}
```

UTC result:

```
{
   "took": 9,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 1130,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "vot": {
         "buckets": [
            {
               "key_as_string": "2014-10-25T22:00:00.000Z",
               "key": 1414274400000,
               "doc_count": 260
            },
            {
               "key_as_string": "2014-10-25T23:00:00.000Z",
               "key": 1414278000000,
               "doc_count": 0
            },
            {
               "key_as_string": "2014-10-26T00:00:00.000Z",
               "key": 1414281600000,
               "doc_count": 216
            },
            {
               "key_as_string": "2014-10-26T01:00:00.000Z",
               "key": 1414285200000,
               "doc_count": 422
            },
            {
               "key_as_string": "2014-10-26T02:00:00.000Z",
               "key": 1414288800000,
               "doc_count": 232
            }
         ]
      }
   }
}
```

Points of interest in these queries are the result buckets for key: `1414278000000`. When ran in UTC it give a `doc_count` of `216`. When ran in CET it has a `doc_count` of `0`.

Further more, you will find a `doc_count` of `216` in the CET bucket of `1414281600000` (an hour to late). Last to point out is the next CET bucket, it contains the value of `422` which is the sum of `222` + `200`. As you can see these values can be found in the corresponding bucket and the previous bucket in UTC.

Attached you will find a patch file adding some basic tests to the `org.elasticsearch.common.rounding` package. Here we test key rounding for the `CET` and `America/Chicago` timezones on the days of the DST switch.

I tried diggin into the issue and found that it has to do with the recalculation of `preTz.getOffset` on the following lines [TimeZoneRounding.java:156](https://github.com/elasticsearch/elasticsearch/blob/5797682bd0b87e8efeffb046258287d480435395/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java#L156) and [TimeZoneRounding.java:163](https://github.com/elasticsearch/elasticsearch/blob/5797682bd0b87e8efeffb046258287d480435395/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java#L163).

Running this step by step on my first CET test case results the first time (line 156) in 7200000 (2 hours) and the second time in 3600000 (1 hour). This difference is causeing `2014-10-26T01:01:01 GMT+0200` to resolve to `2014-10-26T02:00:00 GMT+0200`. Explaining why the `1414278000000` (2014-10-26T01:01:01 GMT+0200) bucket to be empty, the next bucket containing the contents of this bucket, and the bucket after that a sum of two buckets.

0001-Add-tests-for-timezone-problems.patch:

```
From 4e022035b25a141027be6aaae78c8ad2454e673f Mon Sep 17 00:00:00 2001
From: Nils Dijk &lt;me@thanod.nl&gt;
Date: Tue, 4 Nov 2014 07:56:05 -0600
Subject: [PATCH 1/1] Add tests for timezone problems.

---
 .../common/rounding/TimeZoneRoundingTests.java     | 39 ++++++++++++++++++++++
 1 file changed, 39 insertions(+)

diff --git a/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java b/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java
index a3d70c7..e79ad1e 100644
--- a/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java
+++ b/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java
@@ -83,6 +83,45 @@ public class TimeZoneRoundingTests extends ElasticsearchTestCase {
         assertThat(tzRounding.round(utc("2009-02-03T01:01:01")), equalTo(time("2009-02-03T01:00:00", DateTimeZone.forOffsetHours(+2))));
         assertThat(tzRounding.nextRoundingValue(time("2009-02-03T01:00:00", DateTimeZone.forOffsetHours(+2))), equalTo(time("2009-02-03T02:00:00", DateTimeZone.forOffsetHours(+2))));
     }
+    
+    @Test
+    public void testTimeTimeZoneRoundingDST() {
+        Rounding tzRounding;
+        // testing savings to non savings switch 
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();
+        assertThat(tzRounding.round(time("2014-10-26T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-10-26T01:00:00", DateTimeZone.forID("CET"))));
+        
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("CET")).build();
+        assertThat(tzRounding.round(time("2014-10-26T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-10-26T01:00:00", DateTimeZone.forID("CET"))));
+        
+        // testing non savings to savings switch
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();
+        assertThat(tzRounding.round(time("2014-03-30T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-03-30T01:00:00", DateTimeZone.forID("CET"))));
+        
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("CET")).build();
+        assertThat(tzRounding.round(time("2014-03-30T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-03-30T01:00:00", DateTimeZone.forID("CET"))));
+        
+        // testing non savings to savings switch (America/Chicago)
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();
+        assertThat(tzRounding.round(time("2014-03-09T03:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-03-09T03:00:00", DateTimeZone.forID("America/Chicago"))));
+        
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("America/Chicago")).build();
+        assertThat(tzRounding.round(time("2014-03-09T03:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-03-09T03:00:00", DateTimeZone.forID("America/Chicago"))));
+        
+        // testing savings to non savings switch 2013 (America/Chicago)
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();
+        assertThat(tzRounding.round(time("2013-11-03T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2013-11-03T06:00:00", DateTimeZone.forID("America/Chicago"))));
+        
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("America/Chicago")).build();
+        assertThat(tzRounding.round(time("2013-11-03T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2013-11-03T06:00:00", DateTimeZone.forID("America/Chicago"))));
+        
+        // testing savings to non savings switch 2014 (America/Chicago)
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();
+        assertThat(tzRounding.round(time("2014-11-02T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-11-02T06:00:00", DateTimeZone.forID("America/Chicago"))));
+        
+        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("America/Chicago")).build();
+        assertThat(tzRounding.round(time("2014-11-02T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-11-02T06:00:00", DateTimeZone.forID("America/Chicago"))));
+    }

     private long utc(String time) {
         return time(time, DateTimeZone.UTC);
-- 
1.9.3 (Apple Git-50)
```
</description><key id="47720901">8339</key><summary>Aggregations: date_histogram aggregation DST bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">thanodnl</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-04T14:49:46Z</created><updated>2014-11-25T19:02:51Z</updated><resolved>2014-11-25T16:07:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="thanodnl" created="2014-11-21T16:20:41Z" id="63994111">I have been fiddling around with this issue and came up with this fix: https://github.com/thanodnl/elasticsearch/commit/1cedf3ed4ff9fb4b8c7f1a1e00efefe8d8330097

The existing tests still pass (I only ran the timezone rounding tests though) and the tests I submitted in the patch above also pass.

If you are happy with the result feel free to take this solution. If you need me to open a pull request let me know!
</comment><comment author="jpountz" created="2014-11-24T19:12:45Z" id="64246545">@thanodnl Your fix and tests look good to me.

&gt; If you need me to open a pull request let me know!

Please do! (and ping me when you open it so that I don't miss it)
</comment><comment author="thanodnl" created="2014-11-25T15:49:17Z" id="64420308">@jpountz I just opened the pull request, also mentioned your handle there but I do not know if you get alerted for that. So: PING! :smile: 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file></files><comments><comment>Aggregations: fix rounding issues on DST switch.</comment></comments></commit></commits></item><item><title>Remove unnecessary code from geo distance builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8338</link><project id="" key="" /><description>The if statements are unneded and also wrong (second
else if can never be reached).
</description><key id="47714465">8338</key><summary>Remove unnecessary code from geo distance builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Geo</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-04T13:46:47Z</created><updated>2015-06-07T18:11:07Z</updated><resolved>2014-11-04T15:27:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-11-04T13:49:54Z" id="61641390">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortBuilder.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file></files><comments><comment>geo sort: remove unneded code from geo distance builder</comment></comments></commit></commits></item><item><title>[TEST] move ClusterDiscoveryConfiguration to org.elasticsearch.test.discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8337</link><project id="" key="" /><description>`ClusterDiscoveryConfiguration` is part of the test infra and should get exported as part of the test jar. This is achieved by moving the class to org.elasticsearch.test.discovery
</description><key id="47708767">8337</key><summary>[TEST] move ClusterDiscoveryConfiguration to org.elasticsearch.test.discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-04T12:42:00Z</created><updated>2014-11-04T12:58:10Z</updated><resolved>2014-11-04T12:58:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-04T12:44:52Z" id="61634188">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptions.java</file><file>src/test/java/org/elasticsearch/discovery/ZenUnicastDiscoveryTests.java</file><file>src/test/java/org/elasticsearch/test/discovery/ClusterDiscoveryConfiguration.java</file></files><comments><comment>[TEST] move ClusterDiscoveryConfiguration to org.elasticsearch.test.discovery</comment></comments></commit></commits></item><item><title>Separate cluster and node level configurations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8336</link><project id="" key="" /><description>Currently the configurations for the cluster and node are mixed together in the same elasticsearch.yml file. This make it hard to sync cluster level configurations as I have to edit elasticsearch.yml on all nodes one by one manually because they have different node level configurations such as paths and publish host etc.
</description><key id="47685727">8336</key><summary>Separate cluster and node level configurations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sunzhu</reporter><labels><label>discuss</label></labels><created>2014-11-04T07:39:08Z</created><updated>2014-11-05T01:41:15Z</updated><resolved>2014-11-05T01:41:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-04T07:47:43Z" id="61604203">Hi @sunzhu 

What we recommend doing is to put the cluster-wide settings into `elasticsearch.yml` and to set the node specific settings on the command line or in the `$ES_JAVA_OPTS` environment variable.
</comment><comment author="sunzhu" created="2014-11-04T08:12:19Z" id="61606163">thanks. It sounds reasonable. I still have 2 questions:
1. Will the configurations in elasticsearch.yml overwrite command line or vice versa?
2. Can all the node level configurations be set on the command line?
</comment><comment author="clintongormley" created="2014-11-04T08:46:34Z" id="61609160">&gt; 1. Will the configurations in elasticsearch.yml overwrite command line or vice versa?

Generally yes, but see #6887 

&gt; 1. Can all the node level configurations be set on the command line?

Yes, you can set any setting on the command line
</comment><comment author="sunzhu" created="2014-11-05T01:41:15Z" id="61746963">OK, I will try the solution.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elastic search date range aggregation not giving complete data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8335</link><project id="" key="" /><description>**I am Querying for getting aggregate data based on date_range, like below**

```
"aggs": {
        "range": {
            "date_range": {
                "field": "sold",
                "ranges": [
                    {  "from": "2014-11-01", "to": "2014-11-30" },
                    {  "from": "2014-08-01", "to": "2014-08-31" } 
                ]
            }
        }
    }
```

**using this I am getting this response**

```
"aggregations": {
    "range": {
      "buckets": [
        {
          "key": "2014-08-01T00:00:00.000Z-2014-08-31T00:00:00.000Z",
          "from": 1406851200000,
          "from_as_string": "2014-08-01T00:00:00.000Z",
          "to": 1409443200000,
          "to_as_string": "2014-08-31T00:00:00.000Z",
          "doc_count": 1
        },
        {
          "key": "2014-11-01T00:00:00.000Z-2014-11-30T00:00:00.000Z",
          "from": 1414800000000,
          "from_as_string": "2014-11-01T00:00:00.000Z",
          "to": 1417305600000,
          "to_as_string": "2014-11-30T00:00:00.000Z",
          "doc_count": 2
        }
      ]
    }
  }
```

**but instead of only doc_count, I have also required complete aggregate data that satisfy this range,
is threre any way to get this..please help**
</description><key id="47677080">8335</key><summary>elastic search date range aggregation not giving complete data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajit-daffodil</reporter><labels /><created>2014-11-04T04:35:15Z</created><updated>2014-11-04T06:57:06Z</updated><resolved>2014-11-04T06:57:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-04T06:57:06Z" id="61600650">@rajit-daffodil The GitHub issues list is for bug reports and feature requests.  Please ask these questions in the mailing list. http://elasticsearch.org/community

You may want to start by reading the aggregations documentation: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html#search-aggregations
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Invalid fields are still being evaluated per validate query api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8334</link><project id="" key="" /><description>Consider the following use case:
https://gist.github.com/ppf2/e2764a37418f4b4e4e6d

The above example shows a query with default operator of AND, and a bogus field in the fields list.  The validate query api shows that it is still trying to do a match of the value "of" against the bogus field (for the other fields, the of has been removed via the stop filter).  As a result, the query returns no documents.  Should the bogus field be ignored completely in the evaluation of the query?
</description><key id="47654786">8334</key><summary>Invalid fields are still being evaluated per validate query api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label></labels><created>2014-11-03T22:42:36Z</created><updated>2015-11-21T20:46:02Z</updated><resolved>2015-11-21T20:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-11-04T03:12:29Z" id="61587658">I think if the bogus fields exists in the fields list, then it is helpful that validate API should give warning.
</comment><comment author="clintongormley" created="2015-11-21T20:46:02Z" id="158679825">&gt; Should the bogus field be ignored completely in the evaluation of the query?

I don't think it should be ignored.  If the field is specified, then we should include it.  We don't know whether it should be there or not.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Possible race condition: Cluster with no master, Indexing call can timeout/fail, but document can still make it into the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8333</link><project id="" key="" /><description>Boaz, this is from the issue we were working on.

In a single master cluster, when an indexing request is in-flight and the master node leaves, it is possible that there is small race condition between the time the index request is timed-out by ES, and a shard started cluster event happens - leading to successful indexing of the document (even though the client was notified of a failed/time-out error).
</description><key id="47642355">8333</key><summary>Possible race condition: Cluster with no master, Indexing call can timeout/fail, but document can still make it into the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>:Cluster</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v1.3.2</label><label>v1.4.0.Beta1</label></labels><created>2014-11-03T20:48:48Z</created><updated>2015-11-22T20:37:02Z</updated><resolved>2015-11-22T20:37:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:41:33Z" id="158679611">@bleskes is this still an issue?
</comment><comment author="bleskes" created="2015-11-22T20:20:25Z" id="158794880">I'm not sure - I don't recall the issue anymore and I don't follow the description. @bly2k do you remember and can you elaborate?
</comment><comment author="bly2k" created="2015-11-22T20:35:55Z" id="158795857">@clintongormley @bleskes Hey guys, I recall this was happening prior to 1.4. I could not reproduce this since 1.4 came out. I assume it has since been fixed already. Regardless, I believe we had tests for this scenario starting 1.4 so it likely is fixed and no longer an issue.
</comment><comment author="bleskes" created="2015-11-22T20:37:02Z" id="158795959">Ok. Thanks @bly2k ! closing...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Grouping on the _parent field influenced by the presence of other parent-child mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8332</link><project id="" key="" /><description>Not without surprise and after spending quite some time clueless in front of Sense, I have isolated yet another bug related to the parent-child relationship. It looks related to #5399.
Fact is that the following setup seems to work just fine (the example is simplified almost to the maximum):

```
PUT idx
{
    mappings:
    {
        Parent:
        {
            properties:
            {
                name: {type: "string", index: "not_analyzed"}
            }
        },
        Child:
        {
            _parent: { type: "Parent"},
            properties:
            {
                name: {type: "string", index: "not_analyzed"}
            }
        }
    }
}

POST idx/_bulk
{create: {_id:"A", _type: "Parent"}}
{name: "A parent"}
{create: {_parent: "A", _type: "Child"}}
{name: "child"}

GET idx/Child/_search?pretty&amp;fields=_parent&amp;_source=*
{
    aggregations:
    {
        per_parent:
        {
            terms: {field: "_parent"},
            aggs:
            {
                each_uid:
                {
                    terms: {field: "_uid"}
                }
            }
        }
    }
}
```

The aggregation on parent field values yields a single group containing a single document of type `Child`. If another child document type is brought into play

```
PUT idx/_mapping/Deeper
{
    _parent: { type: "Child"},
    properties:
    {
        name: {type: "string", index: "not_analyzed"}
    }
}
```

things go suddenly crazy. The above aggregation indicates the `Child` document is now also a child of itself and thus appears in a second resulting group.
But wait, the evil is persistent :exclamation: Even if the last introduced type is deleted via `DELETE idx/Deeper`, the aggregation still claims the same paradox. This is even more concerning than the bug itself since it shows that permanent changes to existing inverted indices are introduced along with the third document type.

Given that there's a way to avoid this bug in _1.4.0_, when is this upcoming version due? :smirk:
</description><key id="47634963">8332</key><summary>Grouping on the _parent field influenced by the presence of other parent-child mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acarstoiu</reporter><labels><label>:Parent/Child</label><label>discuss</label></labels><created>2014-11-03T19:42:10Z</created><updated>2016-05-05T11:31:59Z</updated><resolved>2016-05-05T11:31:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-04T06:50:40Z" id="61600209">Hi @acarstoiu 

I'm unclear as to whether this is a bug or not.  Parent documents store their own ID as their `_parent` ID by design.  So that is all that you are seeing here: the `Child` document is both a child and a parent.

What is it that you are actually trying to achieve?
</comment><comment author="acarstoiu" created="2014-11-04T10:47:23Z" id="61622289">Hi,

in simple words I'm just trying to group some documents (of a certain type) by their parent. The inner aggregation on `_uid` is only meant to show what exactly is classified in a group.
All the necessary calls (in Sense's syntax) are given above. As you may see, I have only two documents in the "database" and one of them of type `Child` has a regular parent of type `Parent`, but _also itself_. This is what I deem a bug.

Do you imply that if a new document type of type `Deeper` is introduced (_without actually inserting documents_), then the existing documents of type `Child` receive an **additional value** for their `_parent` field :open_mouth: ?
On the other hand, how can a document have two parents? The syntax I know for mapping doesn't allow the declaration of multiple parent types. Also, no more than one parent is accepted when a new document is inserted.
</comment><comment author="clintongormley" created="2014-11-04T12:09:17Z" id="61630585">&gt; As you may see, I have only two documents in the "database" and one of them of type Child has a regular parent of type Parent, but also itself. This is what I deem a bug.

I'd say not a bug, but just the internal implementation showing itself unexpectedly.

&gt; Do you imply that if a new document type of type Deeper is introduced (without actually inserting documents), then the existing documents of type Child receive an additional value for their _parent field :open_mouth: ?

No.

&gt; On the other hand, how can a document have two parents? The syntax I know for mapping doesn't allow the declaration of multiple parent types. Also, no more than one parent is accepted when a new document is inserted.

A document can't have two parents.  What happened when you made the `Child` type a parent was that it loaded the parent IDs into memory, so that later it can be mapped to any children of `Child` that you add.
</comment><comment author="acarstoiu" created="2014-11-04T16:45:30Z" id="61671113">I don't get it. Why would parent types alter their own `_parent` field for the sake of their children? What's the connection? I would have understood the need to load into memory the `_parent` field of the children.

Anyway, could you please tell me:
1. how can I avoid this "_unexpectedly showing implementation detail_" ?
2. when is the _1.4.0_ version due?
Thank you.
</comment><comment author="acarstoiu" created="2014-11-04T19:02:19Z" id="61694821">I forgot to mention that this issue reproduces on Elasticsearch _1.4.0.beta1_, as well as on _1.3.2_ and _1.3.4_.
</comment><comment author="acarstoiu" created="2014-11-14T15:17:29Z" id="63078157">Well, I'm sure that this discussion isn't stuck from my fault. But still, can I help to unblock it?
This report is still labelled with **feedback_needed**, but apart from the simple replication recipe provided in the first comment and the clarifications that followed, what else is expected?
</comment><comment author="SushilShrestha" created="2015-07-14T12:06:50Z" id="121216517">Do we have a solution yet or are we stuck ? I am using elasticsearch 1.5.1 and stuck with same problem while aggregating by _ parent field on child type. 
</comment><comment author="clintongormley" created="2015-11-21T20:40:21Z" id="158679559">I believe @martijnvg is planning on removing access to the `_parent` field completely, which would make this point moot.
</comment><comment author="jemc" created="2015-11-28T17:08:38Z" id="160321266">@clintongormley Note that removing access to the `_parent` field in aggregations would be a breaking change - it is clearly documented as a feature in the current guide.

See the second example on this page of the guide:
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-parent-field.html

Also, the feature is clearly spelled out explicitly:

&gt; The value of the `_parent` field is accessible in queries, aggregations, scripts, and when sorting

Given that it is a documented feature, I still don't understand how you can claim that its not a bug when the presence of other parent-child mappings in an otherwise well-formed aggregation as demonstrated in #14955 breaks the documented feature.

I would understand if you had decided that it was a bug that was impractical to fix, so you planned to remove the feature instead, but that doesn't seem to be what you're saying.
</comment><comment author="jemc" created="2015-11-28T17:12:02Z" id="160321420">It's been suggested to me by someone else that the only way for us to be able to use both `has_child` queries and aggregate the children by parent is to use both the `_parent` meta-field and a normal field called `parent_id`, keeping them in sync by hand with every operation.

However, this seems fragile, redundant, and honestly rather pathetic when the `_parent` meta-field is documented as being "accessible in queries, aggregations, scripts, and when sorting".
</comment><comment author="acarstoiu" created="2015-12-07T17:01:41Z" id="162592858">I couldn't agree more, but I suppose @clintongormley already knows it.
</comment><comment author="clintongormley" created="2016-05-05T11:31:59Z" id="217129542">The `_parent` field is no longer available for aggregations in 5.0.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>When corruption strikes, don't create exceptions with circular references</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8331</link><project id="" key="" /><description /><key id="47623741">8331</key><summary>When corruption strikes, don't create exceptions with circular references</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-03T17:57:45Z</created><updated>2015-06-07T18:19:35Z</updated><resolved>2014-11-03T19:18:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-03T18:17:24Z" id="61522203">good catch. Left one comment... 
</comment><comment author="rmuir" created="2014-11-03T18:23:33Z" id="61523134">I tried to give a better explanation. after we compareAndSet current, we are just testing if current was previously unset (null). In that case its either e itself or from the initCause hierarchy of e, so we can't add e as a suppressed exception to it. Otherwise, if current was set, then its a separate exception alltogether, so we can safely addSuppressed.
</comment><comment author="rmuir" created="2014-11-03T18:40:05Z" id="61525694">I tried to simplify the code here so its more intuitive: @bleskes is this easier to grok?
</comment><comment author="s1monw" created="2014-11-03T18:47:05Z" id="61526794">LGTM
</comment><comment author="bleskes" created="2014-11-03T18:48:00Z" id="61526927">@rmuir I missed the removal of current=corruptedEngine.get(). The new version is anyway much better. Thx and sorry for the noise. LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoverySource.java</file></files><comments><comment>Internal: when corruption strikes, don't create exceptions with circular references</comment></comments></commit></commits></item><item><title>Non-uniform score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8330</link><project id="" key="" /><description>I'm doing a multi_match query on multi_fields and I'm expecting a constant scoring because the search string is in all the documents. I'm running ES 1.3.4 with ICU plugin. Here are the steps to reproduce it:

```
# index settings &amp; mappings
PUT /blogs
{
  "settings": {
    "analysis": {
      "filter": {
        "length_filter": {
          "type": "length",
          "min": 2
        }
      },
      "tokenizer": {
        "nGram_tokenizer": {
          "type": "nGram",
          "min_gram": 2,
          "max_gram": 30,
          "token_chars": [ "letter", "digit", "symbol" ]
        }
      },
      "analyzer": {       
        "nGram_analyzer": {
          "type": "custom",
          "filter": ["icu_normalizer", "icu_folding", "length_filter"],
          "tokenizer": "nGram_tokenizer"
        },
        "search_analyzer": {
          "type": "custom",
          "filter": ["icu_normalizer", "icu_folding", "length_filter"],
          "tokenizer": "icu_tokenizer"
        },
        "def_analyzer": {
          "alias": ["icu_analyzer"],
          "type": "custom",
          "filter": ["icu_normalizer", "icu_folding"],
          "tokenizer": "icu_tokenizer"
        }
      }
    }
  },
  "mappings": {
    "_default_": {
      "_source": {
        "enabled": true,
        "compress": true,
        "compress_threshold": "200b"
      },
      "_all": {
        "enabled": true
      },     
      "dynamic_date_formats": ["yyyy-MM-dd", "date_optional_time"]
    },
    "post": {
      "dynamic": "strict",
      "properties": {
        "title": {
          "type": "string",
          "analyzer": "def_analyzer",
          "fields": {
            "ngram": {
              "type": "string",
              "analyzer": "nGram_analyzer",
              "term_vector": "with_positions_offsets"
            }
          }
        },
        "content": {
          "type": "string",
          "analyzer": "def_analyzer",
          "fields": {
            "ngram": {
              "type": "string",
              "analyzer": "nGram_analyzer",
              "term_vector": "with_positions_offsets"
            }
          }
        },
        "visible": {
          "type": "boolean",
          "index": "not_analyzed"
        },
        "post_date": {
          "type": "date",
          "index": "not_analyzed"
        }
      }
    }
  }
}
```

```
# index some documents
PUT blogs/post/1
{
  "title": "title 1",
  "content": "",
  "visible": true,
  "post_date": "2014-11-03T12:50:00"
}

PUT blogs/post/2
{
  "title": "title 2",
  "content": "",
  "visible": true,
  "post_date": "2014-11-03T12:51:00"
}

PUT blogs/post/3
{
  "title": "title 3",
  "content": "",
  "visible": true,
  "post_date": "2014-11-03T12:52:00"
}

PUT blogs/post/4
{
  "title": "title 4",
  "content": "",
  "visible": true,
  "post_date": "2014-11-03T12:53:00"
}

PUT blogs/post/5
{
  "title": "title 5",
  "content": "",
  "visible": true,
  "post_date": "2014-11-03T12:54:00"
}
```
#### run the multi_match query and sort the results by score (desc) and post_date (asc)

```
GET /blogs/post/_search
{
  "query": {
    "filtered": {
      "query": {
        "multi_match": {
          "query": "tit",
          "fields": ["title.ngram^2", "content.ngram"],
          "type": "most_fields",
          "operator": "and",           
          "analyzer": "search_analyzer",
          "minimum_should_match" : 1
        }
      },
      "filter": {
        "term": { "visible": "true" }
      }
    }
  },
  "sort" : [
    { "_score": { "order": "desc" }},
    { "post_date": { "order": "asc" }}
  ]
}

{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 5,
      "max_score": null,
      "hits": [
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "1",
            "_score": 0.025078464,
            "_source": {
               "title": "title 1",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:50:00"
            },
            "sort": [
               0.025078464,
               1415019000000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "2",
            "_score": 0.025078464,
            "_source": {
               "title": "title 2",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:51:00"
            },
            "sort": [
               0.025078464,
               1415019060000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "3",
            "_score": 0.025078464,
            "_source": {
               "title": "title 3",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:52:00"
            },
            "sort": [
               0.025078464,
               1415019120000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "4",
            "_score": 0.025078464,
            "_source": {
               "title": "title 4",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:53:00"
            },
            "sort": [
               0.025078464,
               1415019180000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "5",
            "_score": 0.025078464,
            "_source": {
               "title": "title 5",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:54:00"
            },
            "sort": [
               0.025078464,
               1415019240000
            ]
         }
      ]
   }
}
```

As you see, the score is uniform (equal) as expected. Now if I add one more document, the score changes:
#### index one more document

```
PUT blogs/post/6
{
  "title": "title 6",
  "content": "",
  "visible": true,
  "post_date": "2014-11-03T12:55:00"
}

{
   "took": 4,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 6,
      "max_score": null,
      "hits": [
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "1",
            "_score": 0.053388856,
            "_source": {
               "title": "title 1",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:50:00"
            },
            "sort": [
               0.053388856,
               1415019000000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "6",
            "_score": 0.053388856,
            "_source": {
               "title": "title 6",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:55:00"
            },
            "sort": [
               0.053388856,
               1415019300000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "2",
            "_score": 0.025078464,
            "_source": {
               "title": "title 2",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:51:00"
            },
            "sort": [
               0.025078464,
               1415019060000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "3",
            "_score": 0.025078464,
            "_source": {
               "title": "title 3",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:52:00"
            },
            "sort": [
               0.025078464,
               1415019120000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "4",
            "_score": 0.025078464,
            "_source": {
               "title": "title 4",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:53:00"
            },
            "sort": [
               0.025078464,
               1415019180000
            ]
         },
         {
            "_index": "blogs",
            "_type": "post",
            "_id": "5",
            "_score": 0.025078464,
            "_source": {
               "title": "title 5",
               "content": "",
               "visible": true,
               "post_date": "2014-11-03T12:54:00"
            },
            "sort": [
               0.025078464,
               1415019240000
            ]
         }
      ]
   }
}
```

As you could see the score is not anymore constant.

Is this the expected behavior?
</description><key id="47606134">8330</key><summary>Non-uniform score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sofianito</reporter><labels /><created>2014-11-03T15:31:59Z</created><updated>2014-11-03T18:35:55Z</updated><resolved>2014-11-03T18:35:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-03T18:35:55Z" id="61525068">@sofianito For the third time, please ask these questions on the mailing list, not here.

To find out about expected behaviour, I suggest you start reading the definitive guide: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/index.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Generate dynamic mappings for empty strings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8329</link><project id="" key="" /><description>This will help the exists/missing filters behave as expected in presence of
empty strings, as well as when using a default analyzer that would generate
tokens for an empty string (uncommon).

Close #8198
</description><key id="47605182">8329</key><summary>Generate dynamic mappings for empty strings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-03T15:24:03Z</created><updated>2015-06-07T18:36:34Z</updated><resolved>2014-11-04T16:18:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-04T01:10:36Z" id="61578285">LGTM.
</comment><comment author="clintongormley" created="2014-11-04T15:39:41Z" id="61658560">LGTM
</comment><comment author="yangou" created="2015-01-28T00:44:43Z" id="71760309">Any walk around for filtering out empty string in 1.4.2?
I tried this solution which doesn't seem to work:

"range": {
  "my_field": {}
}
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cannot access elasticsearch from a Mac host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8328</link><project id="" key="" /><description>I am new to elasticsearch and have just installed elasticsearch on mac via brew but I cannot access the server from another computer on the same LAN
</description><key id="47592731">8328</key><summary>Cannot access elasticsearch from a Mac host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Xollie</reporter><labels /><created>2014-11-03T13:23:35Z</created><updated>2014-11-03T13:34:20Z</updated><resolved>2014-11-03T13:34:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-03T13:34:20Z" id="61477342">Hi @Xollie 

Please use the mailing list for questions like these: http://elasticsearch.org/community

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve the lifecycle management of the join control thread in zen discovery.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8327</link><project id="" key="" /><description>This PR also includes:
- Better exception handling in UnicastZenPing#ping
- In the join thread that runs the innerJoinCluster loop, remember the last known exception and throw that when assertions are enabled. We loop until inner join has successfully completed and if continues exceptions are thrown we should fail the test, because the exception shouldn't occur in production (at least not too often).
</description><key id="47585098">8327</key><summary>Improve the lifecycle management of the join control thread in zen discovery.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Discovery</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-03T11:37:41Z</created><updated>2015-06-07T18:07:42Z</updated><resolved>2014-11-04T08:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-03T12:34:59Z" id="61471645">Looking good. left two comments.
</comment><comment author="martijnvg" created="2014-11-03T13:25:52Z" id="61476385">@bleskes I updated the PR.
</comment><comment author="martijnvg" created="2014-11-03T14:12:49Z" id="61482251">@bleskes I applied the feedback, also added better error handling for multicast ping.
</comment><comment author="bleskes" created="2014-11-03T22:01:55Z" id="61557476">@martijnvg looking good. left two last little comments
</comment><comment author="martijnvg" created="2014-11-03T23:04:24Z" id="61566031">@bleskes Thanks, I updated the PR to address your comments.
</comment><comment author="bleskes" created="2014-11-04T08:21:50Z" id="61606944">Left one minor logging comment. LGTM!
</comment><comment author="martijnvg" created="2014-11-04T08:52:41Z" id="61609773">Pushed, thanks @bleskes!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Discovery: don't wait joinThread when stopping</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Discovery: a more lenient wait joinThread when stopping</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ping/unicast/UnicastZenPing.java</file></files><comments><comment>Discovery: Improve the lifecycle management of the join control thread in zen discovery.</comment></comments></commit></commits></item><item><title>Shard UNASSIGNED</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8326</link><project id="" key="" /><description>Hi,

We are using Elasticsearch 1.3.2 on a 2 nodes cluster in a production environment.
Currently only a few indexes contain about 1000 documents.
Mapper-attachments plugin is used in model.
Some shards in one cluster node (SERVER 1) are in the following states:

{
    state: INITIALIZING
    primary: false
    node: KkuMLz0_TKONN77uOoWE7A
    relocating_node: null
    shard: 3
    index: programma_mare-trashcan

}

{
    state: UNASSIGNED
    primary: false
    node: null
    relocating_node: null
    shard: 0
    index: programma_mare_index_initial

}

There are a lot of warning messages like the following:

SERVER 1:

org.elasticsearch.transport.RemoteTransportException: [inl-cdcl-ind2][inet[/10.73.193.51:9300]][index/shard/recovery/startRecovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [context][1] Phase[1] Execution failed
    at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1078)
    at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:636)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:135)
    at org.elasticsearch.indices.recovery.RecoverySource.access$2500(RecoverySource.java:72)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:440)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:426)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [context][1] Failed to transfer [0] files with total size of [0b]
    at org.elasticsearch.indices.recovery.RecoverySource$1.phase1(RecoverySource.java:280)
    at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1074)
    ... 9 more
Caused by: java.lang.OutOfMemoryError: Direct buffer memory
    at java.nio.Bits.reserveMemory(Bits.java:658)
    at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:123)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306)
    at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:174)
    at sun.nio.ch.IOUtil.read(IOUtil.java:195)
    at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:700)
    at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:685)
    at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:176)
    at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
    at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
    at org.apache.lucene.store.BufferedChecksumIndexInput.readByte(BufferedChecksumIndexInput.java:41)
    at org.apache.lucene.store.DataInput.readInt(DataInput.java:96)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:346)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:457)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:907)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:753)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:453)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
    at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:124)
    at org.elasticsearch.index.store.Store.access$300(Store.java:74)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:442)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:433)
    at org.elasticsearch.index.store.Store.getMetadata(Store.java:144)
    at org.elasticsearch.indices.recovery.RecoverySource$1.phase1(RecoverySource.java:145)
    ... 10 more

SERVER 2:

[2014-11-02 00:02:12,250][WARN ][cluster.action.shard     ] [inl-cdcl-ind2] [context][1]
received shard failed for [context][1], node[KkuMLz0_TKONN77uOoWE7A], [R],
s[INITIALIZING], indexUUID [4GkRH6dNR--kmx0FtXwcRA], reason
[Failed to start shard, message [RecoveryFailedException[[context][1]:
 Recovery failed from [inl-cdcl-ind2][2eBj7ijRS82V8md2a78U-A][inl-cdcl-ind2][inet[/10.73.193.51:9300]]
 into [inl-cdcl-ind1][KkuMLz0_TKONN77uOoWE7A][inl-cdcl-ind1][inet[/10.73.193.50:9300]]]; nested:
 RemoteTransportException[[inl-cdcl-ind2][inet[/10.73.193.51:9300]][index/shard/recovery/startRecovery]];
 nested: RecoveryEngineException[[context][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[context][1]
 Failed to transfer [0] files with total size of [0b]]; nested: OutOfMemoryError[Direct buffer memory]; ]]

 ...

[2014-11-01 00:02:28,251][WARN ][cluster.action.shard     ]
[inl-cdcl-ind2] [my-index][3] received shard failed for [my-index][3], node[KkuMLz0_TKONN77uOoWE7A], [R], s[INITIALIZING], indexUUID [Y9nKzEPuRSGPsYhjisxYPQ],
reason [master [inl-cdcl-ind2][2eBj7ijRS82V8md2a78U-A][inl-cdcl-ind2][inet[/10.73.193.51:9300]]
marked shard as initializing, but shard is marked as failed, resend shard failure]   

How can we identify the problem?

Thanks,
Regards

Matteo
</description><key id="47584614">8326</key><summary>Shard UNASSIGNED</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mcremolini</reporter><labels><label>discuss</label></labels><created>2014-11-03T11:30:47Z</created><updated>2015-11-21T20:25:10Z</updated><resolved>2015-11-21T20:25:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-11-03T13:31:44Z" id="61477034">The mapper attachment plugin is tricky as it runs Apache Tika under the hood and it's hard to know what that is doing exactly. Can you disable it and see if the problem occurs?

We recently fixed an issue regarding direct buffers but that only occurs if you have huge documents / large requests - https://github.com/elasticsearch/elasticsearch/pull/7811 . You might be running into this one, in which case upgrading will solve it.
</comment><comment author="mcremolini" created="2014-11-03T14:18:06Z" id="61482947">Hi Boaz,

Thank you for your answer.
I can't disable mapper-attachment plugin because storing the content of the
documents is our primary goal.
I tried both workarounds:
http://www.elasticsearch.org/blog/elasticsearch-1-3-2-released/
that the REST API
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-reroute.html
without success.

Do You think mandatory to upgrade to 1.3.4?

We have replica = 1 but how can I manage similar situation if the
shard/index corrupted are in both nodes?

Many thanks
Matteo

2014-11-03 14:32 GMT+01:00 Boaz Leskes notifications@github.com:

&gt; The mapper attachment plugin is tricky as it runs Apache Tika under the
&gt; hood and it's hard to know what that is doing exactly. Can you disable it
&gt; and see if the problem occurs?
&gt; 
&gt; We recently fixed an issue regarding direct buffers but that only occurs
&gt; if you have huge documents / large requests - #7811
&gt; https://github.com/elasticsearch/elasticsearch/pull/7811 . You might be
&gt; running into this one, in which case upgrading will solve it.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8326#issuecomment-61477034
&gt; .

## 

Matteo Cremolini
</comment><comment author="bleskes" created="2014-11-03T18:15:18Z" id="61521859">the problem is that the node goes OOM when trying to copy the primary over to a replica on another node. Manual reroute won't help there. 

The peculiar thing is that by default ES doesn't limit the direct memory the JVM can allocate. Can you post the output of `curl localhost:9200/_nodes/stats/jvm?human\&amp;pretty` ? 
</comment><comment author="mcremolini" created="2014-11-04T08:22:29Z" id="61607000">Hi,

We have extended the JVM heap to 16gb from (default) 1Gb.
Here is the output:

{
  "cluster_name" : "mycluster",
  "nodes" : {
    "BCpfaofHSYyCzOrK1Oamnw" : {
      "timestamp" : 1415089051098,
      "name" : "inl-cdcl-ind1",
      "transport_address" : "inet[/10.73.193.50:9300]",
      "host" : "inl-cdcl-ind1",
      "ip" : [ "inet[/10.73.193.50:9300]", "NONE" ],
      "jvm" : {
        "timestamp" : 1415089051098,
        "uptime_in_millis" : 65449281,
        "mem" : {
          "heap_used_in_bytes" : 326555288,
          "heap_used_percent" : 1,
          "heap_committed_in_bytes" : 17092640768,
          "heap_max_in_bytes" : 17092640768,
          "non_heap_used_in_bytes" : 62017024,
          "non_heap_committed_in_bytes" : 62324736,
          "pools" : {
            "young" : {
              "used_in_bytes" : 56475464,
              "max_in_bytes" : 697958400,
              "peak_used_in_bytes" : 697958400,
              "peak_max_in_bytes" : 697958400
            },
            "survivor" : {
              "used_in_bytes" : 29049912,
              "max_in_bytes" : 87228416,
              "peak_used_in_bytes" : 81718800,
              "peak_max_in_bytes" : 87228416
            },
            "old" : {
              "used_in_bytes" : 241029912,
              "max_in_bytes" : 16307453952,
              "peak_used_in_bytes" : 241029912,
              "peak_max_in_bytes" : 16307453952
            }
          }
        },
        "threads" : {
          "count" : 158,
          "peak_count" : 164
        },
        "gc" : {
          "collectors" : {
            "young" : {
              "collection_count" : 118,
              "collection_time_in_millis" : 3577
            },
            "old" : {
              "collection_count" : 0,
              "collection_time_in_millis" : 0
            }
          }
        },
        "buffer_pools" : {
          "direct" : {
            "count" : 372,
            "used_in_bytes" : 294141990,
            "total_capacity_in_bytes" : 294141990
          },
          "mapped" : {
            "count" : 146,
            "used_in_bytes" : 1832578,
            "total_capacity_in_bytes" : 1832578
          }
        }
      }
    },
    "5EFieYKQQNG6BAeNIvXofQ" : {
      "timestamp" : 1415089078342,
      "name" : "inl-cdcl-ind2",
      "transport_address" : "inet[/10.73.193.51:9300]",
      "host" : "inl-cdcl-ind2",
      "ip" : [ "inet[/10.73.193.51:9300]", "NONE" ],
      "jvm" : {
        "timestamp" : 1415089078342,
        "uptime_in_millis" : 64366003,
        "mem" : {
          "heap_used_in_bytes" : 867252832,
          "heap_used_percent" : 5,
          "heap_committed_in_bytes" : 17092640768,
          "heap_max_in_bytes" : 17092640768,
          "non_heap_used_in_bytes" : 58676496,
          "non_heap_committed_in_bytes" : 59047936,
          "pools" : {
            "young" : {
              "used_in_bytes" : 640174856,
              "max_in_bytes" : 697958400,
              "peak_used_in_bytes" : 697958400,
              "peak_max_in_bytes" : 697958400
            },
            "survivor" : {
              "used_in_bytes" : 69731272,
              "max_in_bytes" : 87228416,
              "peak_used_in_bytes" : 69731272,
              "peak_max_in_bytes" : 87228416
            },
            "old" : {
              "used_in_bytes" : 157346704,
              "max_in_bytes" : 16307453952,
              "peak_used_in_bytes" : 157346704,
              "peak_max_in_bytes" : 16307453952
            }
          }
        },
        "threads" : {
          "count" : 152,
          "peak_count" : 158
        },
        "gc" : {
          "collectors" : {
            "young" : {
              "collection_count" : 23,
              "collection_time_in_millis" : 937
            },
            "old" : {
              "collection_count" : 0,
              "collection_time_in_millis" : 0
            }
          }
        },
        "buffer_pools" : {
          "direct" : {
            "count" : 307,
            "used_in_bytes" : 145867606,
            "total_capacity_in_bytes" : 145867606
          },
          "mapped" : {
            "count" : 146,
            "used_in_bytes" : 1832615,
            "total_capacity_in_bytes" : 1832615
          }
        }
      }
    }
  }
}

regards
mt

2014-11-03 19:15 GMT+01:00 Boaz Leskes notifications@github.com:

&gt; the problem is that the node goes OOM when trying to copy the primary over
&gt; to a replica on another node. Manual reroute won't help there.
&gt; 
&gt; The peculiar thing is that by default ES doesn't limit the direct memory
&gt; the JVM can allocate. Can you post the output of curl
&gt; localhost:9200/_nodes/stats/jvm?human&amp;pretty ?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8326#issuecomment-61521859
&gt; .

## 

Matteo Cremolini
</comment><comment author="bleskes" created="2014-11-06T13:26:00Z" id="61977936">thx @mcremolini . The stats looks OK - are those taken when the problem occurs  or after a node restart?

Also - which jvm version &amp; type are you using and what is the largest document you have stored?
</comment><comment author="mcremolini" created="2014-11-06T13:48:45Z" id="61980706">The stats output is generated after node restart and the heap is increased
to 16Gb (total 32).
The jvm is:
java version "1.7.0_65"
OpenJDK Runtime Environment

The largest document is about 50M.

thanks
mt

2014-11-06 14:26 GMT+01:00 Boaz Leskes notifications@github.com:

&gt; thx @mcremolini https://github.com/mcremolini . The stats looks OK -
&gt; are those taken when the problem occurs or after a node restart?
&gt; 
&gt; Also - which jvm version &amp; type are you using and what is the largest
&gt; document you have stored?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8326#issuecomment-61977936
&gt; .

## 

Matteo Cremolini
</comment><comment author="bleskes" created="2014-11-06T18:06:26Z" id="62023353">I see. Thx. 

So that seems to confirm the suspicion this is caused by another manifestation of  #7811. 

I suggest you upgrade as soon as possible.  

&gt; On 06 Nov 2014, at 14:48, mcremolini notifications@github.com wrote:
&gt; 
&gt; The stats output is generated after node restart and the heap is increased 
&gt; to 16Gb (total 32). 
&gt; The jvm is: 
&gt; java version "1.7.0_65" 
&gt; OpenJDK Runtime Environment 
&gt; 
&gt; The largest document is about 50M. 
&gt; 
&gt; thanks 
&gt; mt 
&gt; 
&gt; 2014-11-06 14:26 GMT+01:00 Boaz Leskes notifications@github.com: 
&gt; 
&gt; &gt; thx @mcremolini https://github.com/mcremolini . The stats looks OK - 
&gt; &gt; are those taken when the problem occurs or after a node restart? 
&gt; &gt; 
&gt; &gt; Also - which jvm version &amp; type are you using and what is the largest 
&gt; &gt; document you have stored? 
&gt; &gt; 
&gt; &gt; — 
&gt; &gt; Reply to this email directly or view it on GitHub 
&gt; &gt; https://github.com/elasticsearch/elasticsearch/issues/8326#issuecomment-61977936 
&gt; &gt; . 
&gt; 
&gt; ## 
&gt; 
&gt; Matteo Cremolini
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="mcremolini" created="2014-11-07T08:08:22Z" id="62110937">ok, we upgrade as soon as possible.
You suggest the 1.3.4 or 1.3.5 directly?
If one shard will be corrupted/unassigned without a good replication
what do you
suggest to restore the missing data?
thanks

2014-11-06 19:06 GMT+01:00 Boaz Leskes notifications@github.com:

&gt; I see. Thx.
&gt; 
&gt; So that seems to confirm the suspicion this is caused by another
&gt; manifestation of #7811.
&gt; 
&gt; I suggest you upgrade as soon as possible.
&gt; 
&gt; &gt; On 06 Nov 2014, at 14:48, mcremolini notifications@github.com wrote:
&gt; &gt; 
&gt; &gt; The stats output is generated after node restart and the heap is
&gt; &gt; increased
&gt; &gt; to 16Gb (total 32).
&gt; &gt; The jvm is:
&gt; &gt; java version "1.7.0_65"
&gt; &gt; OpenJDK Runtime Environment
&gt; &gt; 
&gt; &gt; The largest document is about 50M.
&gt; &gt; 
&gt; &gt; thanks
&gt; &gt; mt
&gt; &gt; 
&gt; &gt; 2014-11-06 14:26 GMT+01:00 Boaz Leskes notifications@github.com:
&gt; &gt; 
&gt; &gt; &gt; ## thx @mcremolini https://github.com/mcremolini . The stats looks OK
&gt; &gt; &gt; 
&gt; &gt; &gt; are those taken when the problem occurs or after a node restart?
&gt; &gt; &gt; 
&gt; &gt; &gt; Also - which jvm version &amp; type are you using and what is the largest
&gt; &gt; &gt; document you have stored?
&gt; &gt; &gt; 
&gt; &gt; &gt; —
&gt; &gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; &gt; &lt;
&gt; &gt; &gt; https://github.com/elasticsearch/elasticsearch/issues/8326#issuecomment-61977936&gt;
&gt; &gt; &gt; 
&gt; &gt; &gt; .
&gt; &gt; 
&gt; &gt; ## 
&gt; &gt; 
&gt; &gt; Matteo Cremolini
&gt; &gt; —
&gt; &gt; Reply to this email directly or view it on GitHub.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8326#issuecomment-62023353
&gt; .

## 

Matteo Cremolini
</comment><comment author="bleskes" created="2014-11-07T10:56:39Z" id="62127290">&gt; You suggest the 1.3.4 or 1.3.5 directly?

yeah, 1.3.5 directly is better.

&gt; If one shard will be corrupted/unassigned without a good replication
&gt; what do you
&gt; suggest to restore the missing data?

is you don't have replication and your single primary copy is corrupted you have to restore from an external source / snapshot backup. No way around that.
</comment><comment author="mcremolini" created="2014-11-07T11:10:38Z" id="62128672">ok Boaz, many thanks for help me.

2014-11-07 11:57 GMT+01:00 Boaz Leskes notifications@github.com:

&gt; You suggest the 1.3.4 or 1.3.5 directly?
&gt; 
&gt; yeah, 1.3.5 directly is better.
&gt; 
&gt; If one shard will be corrupted/unassigned without a good replication
&gt; what do you
&gt; suggest to restore the missing data?
&gt; 
&gt; is you don't have replication and your single primary copy is corrupted
&gt; you have to restore from an external source / snapshot backup. No way
&gt; around that.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8326#issuecomment-62127290
&gt; .

## 

Matteo Cremolini
</comment><comment author="clintongormley" created="2015-11-21T20:25:09Z" id="158678891">No further info. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix IndexedGeoBoundingBoxFilter to not modify the bits of other filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8325</link><project id="" key="" /><description /><key id="47576091">8325</key><summary>Fix IndexedGeoBoundingBoxFilter to not modify the bits of other filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Geo</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-03T09:37:07Z</created><updated>2015-06-07T18:11:13Z</updated><resolved>2014-11-03T10:09:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-03T10:04:39Z" id="61457808">LGTM, I like the stats!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file></files><comments><comment>Geo: Fix IndexedGeoBoundingBoxFilter to not modify the bits of other filters.</comment></comments></commit></commits></item><item><title>Aggregations: Add an option to use HdrHistogram to compute percentiles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8324</link><project id="" key="" /><description>[HdrHistogram](https://github.com/HdrHistogram/HdrHistogram) like t-digest can be used to estimate percentiles with a fixed amount of memory and has the interesting feature that two instances can be merged (useful for the reduce phase). It is less flexible than t-digest (assumes positive integers) and has a different take on accuracy (based on significant digits on the _values_, while t-digest gives more precision to extreme _percentiles_). That said, this approach makes sense to compute percentiles on response times, which is a common use-case and the specialization allows HdrHistogram to be very fast (I have not performed any benchmark but adding a new value with HdrHistogram is a `O(1)` while with t-digest it is a `O(log(compression))`).
</description><key id="47573297">8324</key><summary>Aggregations: Add an option to use HdrHistogram to compute percentiles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label></labels><created>2014-11-03T08:53:59Z</created><updated>2015-07-24T16:56:40Z</updated><resolved>2015-07-24T16:56:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-28T12:57:15Z" id="64891250">It probably makes sense, as part of this change, to make the algorithm used pluggable in the same way that we have for the significance heuristic.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/AbstractPercentilesParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/InternalPercentile.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanks.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentileRanksParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/Percentiles.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesMethod.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/AbstractHDRPercentilesAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/AbstractInternalHDRPercentiles.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentileRanksAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/HDRPercentilesAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/InternalHDRPercentileRanks.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/hdr/InternalHDRPercentiles.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/AbstractInternalTDigestPercentiles.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/AbstractTDigestPercentilesAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/InternalTDigestPercentileRanks.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/InternalTDigestPercentiles.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentileRanksAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/tdigest/TDigestPercentilesAggregator.java</file><file>core/src/test/java/org/elasticsearch/benchmark/search/aggregations/HDRPercentilesAggregationBenchmark.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentileRanksTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/HDRPercentilesTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentileRanksTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/metrics/TDigestPercentilesTests.java</file></files><comments><comment>Aggregations: Add HDRHistogram as an option in percentiles and percentile_ranks aggregations</comment></comments></commit></commits></item><item><title>typo fixes - https://github.com/vlajos/misspell_fixer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8323</link><project id="" key="" /><description /><key id="47553421">8323</key><summary>typo fixes - https://github.com/vlajos/misspell_fixer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">vlajos</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-02T23:36:47Z</created><updated>2014-11-08T18:11:16Z</updated><resolved>2014-11-08T18:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-03T10:29:15Z" id="61460080">Wow - that's a lot of changes. Thanks @vlajos 

Please could I ask you to sign the CLA so that I can merge them in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="vlajos" created="2014-11-03T13:05:56Z" id="61474498">Hi. I just signed the CLA. Best regards, Lajos
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorFields.java</file><file>src/main/java/org/elasticsearch/common/Base64.java</file><file>src/main/java/org/elasticsearch/common/Strings.java</file><file>src/main/java/org/elasticsearch/common/inject/BindingProcessor.java</file><file>src/main/java/org/elasticsearch/common/inject/assistedinject/FactoryProvider.java</file><file>src/main/java/org/elasticsearch/common/inject/internal/AsynchronousComputationException.java</file><file>src/main/java/org/elasticsearch/common/inject/internal/ComputationException.java</file><file>src/main/java/org/elasticsearch/common/inject/internal/Errors.java</file><file>src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java</file><file>src/main/java/org/elasticsearch/common/inject/spi/ElementVisitor.java</file><file>src/main/java/org/elasticsearch/common/inject/spi/InjectionPoint.java</file><file>src/main/java/org/elasticsearch/common/inject/spi/Message.java</file><file>src/main/java/org/elasticsearch/common/logging/support/LoggerMessageFormat.java</file><file>src/main/java/org/elasticsearch/common/lucene/search/Queries.java</file><file>src/main/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandler.java</file><file>src/main/java/org/elasticsearch/index/indexing/ShardIndexingService.java</file><file>src/main/java/org/elasticsearch/index/query/TypeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/FunctionScoreQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/nested/IncludeNestedDocsQuery.java</file><file>src/main/java/org/elasticsearch/index/similarity/AbstractSimilarityProvider.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/lookup/PositionIterator.java</file><file>src/main/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProvider.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java</file><file>src/test/java/org/elasticsearch/common/BooleansTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingTests.java</file><file>src/test/java/org/elasticsearch/script/IndexLookupTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/AnalyzingCompletionLookupProviderV1.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>typo fixes - https://github.com/vlajos/misspell_fixer</comment></comments></commit></commits></item><item><title>Caching in custom aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8322</link><project id="" key="" /><description>I am developing a custom metric aggregator using ES 1.3.4 similar to the Stats aggregator. The operation is a bit heavy and I want to leverage caching to improve performance when the same inputs are provided to the aggregation again. Obviously the metrics will change if documents in the shard/segment change. I've figured what I want to cache but I am not sure how to determine when to invalidate the cache. Is any API provided in the aggregation module that can be leveraged here? 

I was reading about Shard Query Cache &lt;a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/index-modules-shard-query-cache.html"&gt;here&lt;/a&gt; and found &lt;blockquote&gt;Cached results are invalidated automatically whenever the shard refreshes, but only if the data in the shard has actually changed.&lt;/blockquote&gt;

Can I implement something similar in my aggregation module too? Any pointers would be really helpful.
</description><key id="47550183">8322</key><summary>Caching in custom aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels><label>discuss</label></labels><created>2014-11-02T21:51:59Z</created><updated>2015-08-26T15:22:41Z</updated><resolved>2015-08-26T15:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bittusarkar" created="2014-11-06T19:27:26Z" id="62036229">I downloaded Elasticsearch 1.4.0 today when I read in its release notes that shard query cache was used to return aggregation results instantly on shards that have not changed. I compiled my plugin code with the new libraries and fired a search request with `search_type=count&amp;query_cache=true` along with my custom aggregation. First "unseen" query runs fine. On firing the same query the second time, I get the following error:

&lt;p&gt;
&lt;code&gt;
{
    "error": "ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ElasticsearchParseException[failed to parse a cached query]; nested: NullPointerException; ",
    "status": 503
}
&lt;/code&gt;
&lt;/p&gt;

The stack trace in ES log says

&lt;p&gt;
&lt;code&gt;
[2014-11-07 00:41:53,978][DEBUG][action.search.type       ] [Sif] [dummyindex][0]: Failed to execute [org.elasticsearch.action.search.SearchRequest@75412e2c] while moving to second phase
org.elasticsearch.ElasticsearchParseException: failed to parse a cached query
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$BytesQuerySearchResult.queryResult(IndicesQueryCache.java:466)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:282)
    at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.moveToSecondPhase(TransportSearchCountAction.java:77)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:397)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:198)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:171)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:568)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.search.aggregations.InternalAggregations.readFrom(InternalAggregations.java:190)
    at org.elasticsearch.search.aggregations.InternalAggregations.readAggregations(InternalAggregations.java:172)
    at org.elasticsearch.search.query.QuerySearchResult.readFromWithId(QuerySearchResult.java:175)
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$BytesQuerySearchResult.queryResult(IndicesQueryCache.java:464)
    ... 10 more
&lt;/code&gt;
&lt;/p&gt;

I tried to debug this but could not come to a solid conclusion. My best guess is that this is happening because streams are not registered for my custom aggregation and hence a NPE is thrown by the following line because `stream(type)` returned `null`.
`InternalAggregation aggregation = AggregationStreams.stream(type).readResult(in);`

I noticed that streams are registered for the inbuilt aggregations in `org.elasticsearch.search.aggregations.TransportAggregationModule` class. I could not find a way to do that for my custom aggregation. Do let me know if I am headed in the right direction and what I can do to make my custom aggregation take advantage of shard query cache.
</comment><comment author="bittusarkar" created="2014-11-06T19:38:34Z" id="62038021">This implementation will not solve my problem completely though as the cache key used in shard query cache is the entire JSON request. For my requirements, even if the JSON changes, aggregation values can still remain the same. 

A suggestion here would be to allow users to override the default behaviour by letting him provide the cache key in the search request itself. Don't know if this is already supported or not.
</comment><comment author="jpountz" created="2014-11-07T09:26:41Z" id="62117849">@bittusarkar I think your analysis that the streams have not been registered is good. If you have a custom aggregation, I believe that you needed to create a module for it? If so you would just need to register streams for your aggregation in the configure() method of your module.

Regarding caching, if your aggregation needs to be able to cache across different requests, then it somehow needs to cache on its own (and/or on client side). I am reluctant to allow the query cache to have different requests cached under the same cache key as it might introduce fragility (figuring out whether two json requests are equivalent is quite hard).
</comment><comment author="bittusarkar" created="2014-11-07T11:24:43Z" id="62129954">@jpountz Thank you for the pointer about the `configure()` method. I'll look into it.

Regarding caching, I agree with you. I am also inclined towards implementing caching in my custom aggregation module. But then I would need to detect when to invalidate the cache. Ideally, I would like to do it in the way Shard Query Cache does it (cached results are invalidated automatically whenever the shard refreshes, but only if the data in the shard has actually changed). Can I do something similar in my custom aggregation code too?

I also like the idea of doing this on the client side. What ES API can I use to detect when to invalidate the cache?
</comment><comment author="jpountz" created="2014-11-07T14:48:42Z" id="62154156">@bittusarkar Maybe there is a misunderstanding here: the shard query cache only gets invalidated if a refresh actually updated some data.
</comment><comment author="bittusarkar" created="2014-11-07T15:00:15Z" id="62155831">@jpountz I got that. I think I mixed up shard query cache with the custom cache that I want to implement. Sorry for that.

What I want to know is, is there a way by which I can implement a similar cache invalidation logic inside my custom aggregation code (specifically inside the class extended from `NumericMetricsAggregator.MultiValue`). The cache that I am referring to here is a custom cache that I would implement. 
Idea is to re-use aggregation results stored in a cache but also make sure that the cache is not stale. Aggregation results do not necessarily change if the JSON query changes (changes in `filter` for example will not cause any aggregation results to change).
</comment><comment author="bittusarkar" created="2014-11-13T01:31:22Z" id="62826771">@jpountz Just wanted to drop by asking if you got a chance to work on this?
</comment><comment author="jpountz" created="2015-08-26T15:22:41Z" id="135060756">Sorry that I missed the previous pings. We haven't had the need to use caches for "regular" aggregations and I don't want to make the framework more complicated.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't accept a dynamic update to `min_master_nodes` which is larger then current master node count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8321</link><project id="" key="" /><description>The discovery.zen.minimum_master_nodes setting can be updated dynamically. Settings it to a value higher then the current number of master nodes will cause the current master to step down. This is dangerous because if done by mistake (typo) there is no way to restore the settings (this requires an active master).
</description><key id="47546495">8321</key><summary>Don't accept a dynamic update to `min_master_nodes` which is larger then current master node count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-02T19:48:04Z</created><updated>2015-06-06T19:26:32Z</updated><resolved>2014-11-03T13:55:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-11-03T13:43:04Z" id="61478304">@bleskes This looks good. I left two questions / comments.
</comment><comment author="bleskes" created="2014-11-03T13:50:06Z" id="61479203">@martijnvg thx. responded. 
</comment><comment author="martijnvg" created="2014-11-03T13:50:50Z" id="61479295">@bleskes LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java</file><file>src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file></files><comments><comment>Discovery: don't accept a dynamic update to min_master_nodes which is larger then current master node count</comment></comments></commit></commits></item><item><title>option to use updateRewriteQuery in wildcard query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8320</link><project id="" key="" /><description>Hi,

For my search solution, i've implemented a custom highlighter and a custom aggregator. Since i need to support wildcard searches as well, each of them internally do a query rewrite if its a wildcard query. 
Instead, what i would like is if anyone does a wildcard query, the query should automatically update the searchContext with the rewritten query (updateRewriteQuery) which will then be accessible by both the modules.
Is this already implemented? if not, would this be taken up as a change in the coming updates?

Thanks
Neeraj Makam
</description><key id="47540472">8320</key><summary>option to use updateRewriteQuery in wildcard query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">muzzlebeat</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2014-11-02T16:05:22Z</created><updated>2015-11-23T10:47:47Z</updated><resolved>2015-11-23T10:47:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:24:23Z" id="158678860">@jpountz any thoughts on this?
</comment><comment author="jpountz" created="2015-11-23T10:47:47Z" id="158902707">This is very internal to elasticsearch. I don't think we want to support this kind of functionnality for plugins.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add elastics-rb to the list of community clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8319</link><project id="" key="" /><description /><key id="47534627">8319</key><summary>Add elastics-rb to the list of community clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">printercu</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-11-02T11:51:43Z</created><updated>2014-11-02T14:22:52Z</updated><resolved>2014-11-02T12:55:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-02T12:35:01Z" id="61404966">Hi @printercu 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="printercu" created="2014-11-02T12:48:22Z" id="61405352">Hi @clintongormley !
Signed it.
</comment><comment author="clintongormley" created="2014-11-02T12:55:37Z" id="61405533">thanks @printercu - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add elastics-rb to the list of community clients</comment></comments></commit></commits></item><item><title>ES security using ngnix reverse-proxy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8318</link><project id="" key="" /><description>I am running ES on a server and want to use ngnix in reverse-proxy mode for security. As Kibana requires access from browser to ES directly, ngnix is expected to provide the security. However I see following error in browser:

"Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at http://&lt;MyDomain&gt;:19200/_nodes. This can be fixed by moving the resource to the same domain or enabling CORS."

I tried 'add_header Access-Control-Allow-Origin *;' and several other techniqus to enable CORS without any success. Do you know a way around with problem with ES/ngnix combo?

Is there any other solution to beef up security around ES?

Thanks,
PK
</description><key id="47528158">8318</key><summary>ES security using ngnix reverse-proxy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pkanthi</reporter><labels /><created>2014-11-02T05:46:49Z</created><updated>2014-11-02T12:29:21Z</updated><resolved>2014-11-02T12:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-02T12:29:21Z" id="61404821">Hi @pkanthi 

See https://github.com/elasticsearch/kibana/issues/1361on the Kibana list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Portions of a mapping are silently ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8317</link><project id="" key="" /><description>I'm running Elasticsearch v1.3.4 on Ubuntu. I've geocoded ~800,000 addresses in an index, and I want to make queries against that geodata. Despite creating an explicit mapping of the coordinates as a geo_point field, my queries fail, with Elasticsearch's log complaining that the field is not a geo_point field. The problem turns out to be that the mapping is silently, partially, failing

Example:

```
curl -XPUT localhost:9200/test/_mapping/geodata -d '{                  
    "properties": {
        "location": {
            "type": "object",
            "coordinates": {
                "type": "geo_point"
            } 
        }
    }
}'
```

This returns:

```
{"acknowledged":true}
```

Then when I verify the result of my request:

```
curl -XGET 'http://localhost:9200/business/_mapping/10'
```

I get this:

```
{"business":{"mappings":{"10":{"properties":{"location":{"type":"object"}}}}}}
```

This should create the entire, requested `location` object, including `coordinates` and `"type": "geo_point"`, but instead it only creates `"location":{"type":"object"}`.

At the time that I create the mapping, Elasticsearch's log acknowledges its creation, but doesn't report any error:

```
[2014-11-01 21:33:47,211][INFO ][cluster.metadata         ] [Grey Gargoyle] [business] create_mapping [geodata]
```

To reduce the number of moving parts, I've tried omitting `"type": "object"` (ostensibly an implied value), but that yields an error (`"MapperParsingException[No type specified for property [location]]"`), so I'm not sure that a simpler demonstration case is plausible.

I am at a loss for how to proceed. I imagine that this is a bug, but I'm holding out the hope that I'm doing something foolish. After all, I can't see that such a bug would have gone unnoticed.
</description><key id="47523992">8317</key><summary>Portions of a mapping are silently ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">waldoj</reporter><labels /><created>2014-11-02T01:42:47Z</created><updated>2014-11-03T11:09:24Z</updated><resolved>2014-11-03T11:09:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-11-02T03:21:51Z" id="61392829">You should ask questions like this on the mailing list.

Your mapping is incorrect.
Coordinates is a property of location. So you need to add a properties level before Coordinates.

HTH
</comment><comment author="konklone" created="2014-11-02T03:36:42Z" id="61393056">&gt; You should ask questions like this on the mailing list.

When a user perceives a bug, a user should file a bug in the bug tracker. @waldoj was not asking for help figuring out how to do something. He was pointing out what he believed to be a shortcoming in Elasticsearch, and filed a detailed bug report so that you would have enough information to address it.

If you believe @waldoj was incorrect that it was a bug, don't tell him he should post to the mailing list. Instead, point out the issue he encountered (as you did), and then address why @waldoj was convinced it was a bug.

In this case, the mapping silently drops information. Maybe the command should return some warning text for common errors like the one @waldoj made, instead of the mapping silently dropping information? 

I've seen this exact issue come up before, it's very easy to do for people new to Elasticsearch. People new to Elasticsearch will give the project some of its most valuable feedback as Elasticsearch continues to expand to broader circles of developers.
</comment><comment author="dadoonet" created="2014-11-02T07:24:39Z" id="61396933">&gt; I am at a loss for how to proceed. I imagine that this is a bug, but I'm holding out the hope that I'm doing something foolish. After all, I can't see that such a bug would have gone unnoticed.

This last part made me think that he should ask to the mailing list first before opening an issue if it's actually a bug. So I was first answering to this. 

I'm not saying that users should not open issues if they feel they hit one. 

That said, @konklone perceived my answer as negative somehow. So @waldoj I apologize if you felt my answer a bit rude. 

(Note to self: don't close issues at 4AM :) ).
</comment><comment author="clintongormley" created="2014-11-02T12:33:17Z" id="61404919">@konklone agreed :)
@waldoj  I think this bug should be fixed by #7534, but we should check.
</comment><comment author="waldoj" created="2014-11-02T17:31:31Z" id="61415306">@clintongormley, it certainly looks like that pull request will fix this. I'm sorry that I didn't turn up that particular pull request in my research prior to filing this ticket!

@dadoonet, thank you for your fix for the mistake that I was making that led to this bug (or, really, lack of a feature). It was lost on me that the examples in the documentation were fragments of mappings, instead of entire mappings, so I hadn't realized that I was missing an object. I'm thrilled to be making queries against geodata now.
</comment><comment author="colings86" created="2014-11-03T11:09:24Z" id="61464021">Closing in favour of #7205
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis: Use ES version of PatternAnalyzer for prebuilt version.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8316</link><project id="" key="" /><description>Elasticsearch already has a custom PatternAnalyzerProvider to avoid a lowercasing localization issue.  But the prebuilt version still used lucene's version.  This pulls the ES version out so it can be used in the prebuilt analyzers, and fixes the analysis bwc test to not use text containing characters that could have problems with the old version.
</description><key id="47511463">8316</key><summary>Analysis: Use ES version of PatternAnalyzer for prebuilt version.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2014-11-01T17:29:56Z</created><updated>2015-01-21T23:22:21Z</updated><resolved>2014-11-16T18:00:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-01T18:09:10Z" id="61377312">FYI: this change was already done in the 5.0 upgrade branch, additionally it has unit tests and so on there.
</comment><comment author="rjernst" created="2014-11-16T18:00:38Z" id="63230170">Closing in favor of the changes on 5.0, and moving fixing the tests to a separate issue (#8492).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: Pass through locale and timezone to test runner, and print in repro command line.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8315</link><project id="" key="" /><description>The carrot runner currently randomizes both locale and timezone, but
these are not set in the maven reproduce line.  Since they aren't
even printed, we have no idea what locale/timezone the tests
actually ran with.
</description><key id="47508841">8315</key><summary>Tests: Pass through locale and timezone to test runner, and print in repro command line.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-11-01T15:45:54Z</created><updated>2015-01-21T23:22:21Z</updated><resolved>2014-11-20T06:14:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-16T17:55:10Z" id="63229916">@s1monw Any thoughts on this?
</comment><comment author="s1monw" created="2014-11-19T21:29:38Z" id="63717137">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: Force test locale to en_US, just like start scripts do.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8314</link><project id="" key="" /><description>In May the elasticsearch start scripts were changed to always pass a locale of `en_US`.  However, the tests currently use the default behavior of the carrotsearch runner, which is to randomize the locale.  We should instead always run tests with the same locale as we will in a real system. 
</description><key id="47485097">8314</key><summary>Tests: Force test locale to en_US, just like start scripts do.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2014-11-01T04:02:15Z</created><updated>2015-01-21T23:22:21Z</updated><resolved>2014-11-01T15:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-11-01T11:35:35Z" id="61365455">I think this just shoves "default locale" problems under the rug. The code could be used in ways other than as a server (e.g. client node). I don't think we should make this change.
</comment><comment author="rjernst" created="2014-11-01T15:46:56Z" id="61372299">Alright, I'll handle this in another way.  Closing in favor of #8315
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shards getting reshuffled across nodes during rolling restart.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8313</link><project id="" key="" /><description>There appears to be new behavior (introduced between 1.2.1 and 1.3.4) which causes shards to get juggled around between nodes when they get restarted. This of course makes restart take much longer.

We run some of our rolling restarts where we restart 4 nodes at a time to speed up the rolling restart. The recovery used to favor recovering shards from the local node. In this screenshot of disk space on different nodes (blue is live shards, and brown is other disk space used) the shards are getting randomly assigned across the nodes that were restarting.

![Shard Shuffling](https://i.cloudup.com/xafnWLKeR2.png)

Looking at the disk space on the nodes I see that each node has 75% more shards on disk than it should, which is exactly what you'd expect if ES is re-assigning all the shards randomly.
</description><key id="47443988">8313</key><summary>Shards getting reshuffled across nodes during rolling restart.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2014-10-31T20:59:39Z</created><updated>2015-11-21T20:22:01Z</updated><resolved>2015-11-21T20:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-31T21:35:57Z" id="61334942">which version are you running or upgrading from/to?
</comment><comment author="gibrown" created="2014-10-31T22:25:35Z" id="61339359">We were upgrading from 1.2.1 to 1.3.4.

I think it still happens though. We're probably doing another rolling restart next week so I'll confirm then.
</comment><comment author="clintongormley" created="2015-11-21T20:22:01Z" id="158678764">Closed by the delayed allocation feature added in 1.7 and improved in 2.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Switch doc_values=true to be the default for non-analyzed string fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8312</link><project id="" key="" /><description>We recently started doing a lot of sorting  by dates and integer fields on subsets of our 5 billion documents. We are mostly running queries that filter to a few thousand docs and then sorting that subset. Needing to load 5 billion dates into memory for this seemingly common use case make ES feel broken. :)

Heap memory management can be very painful and I think represents a difficult point for new users. Search engines are all about preprocessing data (indexing) so that querying can be fast. In my opinion doc_values accomplishes this better than the current default even if it can be slower in some cases.

FWIW, doc_values is MUCH faster in our use case using 1.3.4.
</description><key id="47443368">8312</key><summary>Switch doc_values=true to be the default for non-analyzed string fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-31T20:53:04Z</created><updated>2015-08-06T10:00:01Z</updated><resolved>2015-03-27T06:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-12-04T13:30:11Z" id="65631328">@gibrown have you seen increased segment memory usage after switching to `doc_values`? I've seen 2-8x increase (from 32mb to 70-270mb) per index with 120m events.
</comment><comment author="clintongormley" created="2014-12-31T17:18:49Z" id="68455512">The JVM heap size is practically limited to 32GB - above this and the JVM can no longer use compressed pointers (resulting in more memory usage for the same data), and garbage collections become slower.  

By far the biggest user of the heap for most users is in-memory fielddata, used for sorting, aggregations and scripts. In-memory fielddata is slow to load, as it has to read the whole inverted index and uninvert it.  If the fielddata cache fills up, old data is evicted causing heap churn and bad performance (as fielddata is reloaded and evicted again.)

Doc values provide the same function as in-memory fielddata, but the datastructure is written to disk at index time.  This results in more disk usage and somewhat slower indexing and mergging (because there is more I/O).  Aggregations and sorting are about 20% slower than they are with in-memory fielddata. 

The advantages are:
- less heap usage and faster garbage collections
- no longer limited by the amount of fielddata that can fit into 32GB of heap - instead the file system caches can make use of all the available RAM 
- fewer latency spikes caused by reloading a large segment into memory

Unfortunately, there is no way to back-fill these values without reindexing.  The proposal is to make doc values the default for all fields except `analyzed` string fields (which are not supported by doc values anyway).

Some users will end up writing much more data than they need, but to be clear: this is a default setting which can be changed as appropriate.  It is similar to the fact that we index term positions by default on all `analyzed` string fields, so as to make phrase queries work out of the box.

Before making this decision, we need to understand the full impact of doc-values-by-default, by running the following tests:
- ssd vs spinning disk
- merge throttling enabled/disabled
- non-string fields vs all not_analyzed fields
- metric, logging, wikipedia
- aggs
- sorting asc and desc
- multiple aggs and sorting combined

And measuring the following:
- Node stats after indexing (and always starting from zero)
- number of segments before optimize
- index size before/after optimize
- indexing rate (docs/s)
- query rate (docs/s)
- query rate during heavy indexing
- query rate during heavy indexing with eager loading
- query rate during heavy indexing with eager global ordinals
- logs for GC messages and index throttling messages

We need to do this at large scale with a billion documents, on nodes that are properly tuned (eg ES_HEAP, mlockall etc)

Two further proposals:
- Turn off fielddata loading for `analyzed` string fields by default.  It is too easy to blow up memory usage by sorting or aggregating on an analyzed string field, and it is seldom what the user intended.  If the user really does want to run aggregations on an analyzed string field, then it is an easy option to enable on an existing index.
- Default `not_analyzed` string fields to have `ignore_above` set to (eg) 255 characters, as it seldom makes sense to index or write doc values for such large terms
</comment><comment author="bharvidixit" created="2015-03-13T10:20:53Z" id="78903123">@clintongormley what if i enable doc_values=true for the metadata _id field? Will it have some effect?  "_id": {
          "store": true,
          "index": "not_analyzed",
          "doc_values": true,
          "path": "id"
        }
</comment><comment author="jpountz" created="2015-03-16T03:24:27Z" id="81378471">@bharvidixit Actually we are also thinking about having doc values enabled on `_uid` too. This way, random sorting (which mostly merges a seed with a hash of the _uid to be reproducible) would not need to load fielddata on the _uid field (which takes a lot of memory all the time since this field is unique by definition). And it could also help have consistent pagination by tie-breaking on the `_uid` instead of the internal lucene doc ids (since they are not the same on all copies of a shard).
</comment><comment author="gibrown" created="2015-03-27T20:10:54Z" id="87075133">Awesome, Thanks!
</comment><comment author="bobrik" created="2015-03-27T20:15:32Z" id="87075967">:+1: 
</comment><comment author="jknewman3" created="2015-04-01T23:12:46Z" id="88660280">Is it possible to set eager loading, or something like that, for doc_values? 
It would be helpful for us to always have the most recently added data in cache.
And, yeah, warmers could do it for us. Just wondering if eager applies only to JVM.
</comment><comment author="rjernst" created="2015-04-02T19:50:28Z" id="89023989">@jknewman3 See discussions on #8693
</comment><comment author="jknewman3" created="2015-04-02T21:47:29Z" id="89054442">Ah, that helps a lot. Thanks!
</comment><comment author="bolee" created="2015-08-06T09:53:32Z" id="128312060">how to set doc_value=true, via api to set every field?
</comment><comment author="clintongormley" created="2015-08-06T10:00:01Z" id="128313166">@bolee please ask questions like that in the forum: https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractStringFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/BinaryDVFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/SortedSetDVStringFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/AllMapperOnCusterTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/ip/SimpleIpMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/search/FieldDataTermsFilterTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>src/test/java/org/elasticsearch/indices/memory/breaker/RandomExceptionCircuitBreakerTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/timestamp/SimpleTimestampTests.java</file></files><comments><comment>Core: Enable doc values by default, when appropriate</comment></comments></commit></commits></item><item><title>FunctionScore: RandomScoreFunction now accepts long, as well a strings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8311</link><project id="" key="" /><description>closes #8267
</description><key id="47411547">8311</key><summary>FunctionScore: RandomScoreFunction now accepts long, as well a strings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-31T16:07:44Z</created><updated>2015-03-19T16:05:36Z</updated><resolved>2014-11-03T15:57:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-11-01T15:47:27Z" id="61372315">@s1monw anything more here?
</comment><comment author="s1monw" created="2014-11-03T15:04:52Z" id="61490241">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionTests.java</file></files><comments><comment>FunctionScore: RandomScoreFunction now accepts long, as well a strings.</comment></comments></commit></commits></item><item><title>NPE on Snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8310</link><project id="" key="" /><description>I'm seeing NPE's occur on random shards while performing a snapshot. Cluster Health throughout the process is green with very little CPU usage and heap. I usually try and run another snapshot after it finishes and the previous failed shards will complete and a new set will fail. 

```
host                 ip        heap.percent ram.percent load node.role master name
&lt;redacted&gt;    127.0.1.1           55          59 0.38 d         *      Caldari
&lt;redacted&gt; 127.0.1.1           40          56 0.01 d         m      Amarr
&lt;redacted&gt; 127.0.1.1           44          56 0.01 d         m      Minmatar
&lt;redacted&gt; 127.0.1.1           30          66 0.00 d         m      Gallente
```

```
[2014-10-31 15:35:31,771][WARN ][snapshots                ] [Amarr] [[&lt;idx_name&gt;][2]] [s3_backups:1414767797] failed to create snapshot
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [&lt;idx_name&gt;][2] null
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:141)
    at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)
    at org.elasticsearch.snapshots.SnapshotsService$6.run(SnapshotsService.java:829)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:456)
    at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:131)
    ... 5 more
```
</description><key id="47409854">8310</key><summary>NPE on Snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">tankbusta</reporter><labels /><created>2014-10-31T15:55:41Z</created><updated>2014-11-02T18:13:48Z</updated><resolved>2014-11-02T18:13:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-10-31T16:20:25Z" id="61286266">Which version is it?
</comment><comment author="tankbusta" created="2014-10-31T16:21:50Z" id="61286560">Whoops, I should have mentioned that. 1.3.2
</comment><comment author="imotov" created="2014-11-02T18:13:46Z" id="61416937">Duplicate of #7099. Fixed in 1.3.3 by #7376. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add reference to java-8-oracle JDK</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8309</link><project id="" key="" /><description>Just thought we need this reference
</description><key id="47405015">8309</key><summary>Add reference to java-8-oracle JDK</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">envitraux</reporter><labels /><created>2014-10-31T15:15:08Z</created><updated>2015-08-07T10:15:13Z</updated><resolved>2015-07-24T10:28:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T21:35:25Z" id="84155053">@spinscale can you take a look at this - LGTM though
</comment><comment author="spinscale" created="2015-07-24T10:28:46Z" id="124468049">closing, this has been added some while ago
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search Template - conditional clauses not rendering correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8308</link><project id="" key="" /><description>Is the example of filtered query template [1] working? It seems to contain extra `&lt;/6&gt;` string which seems as an issue to me. I am unable to test this example even after I remove this extra string.

[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.3/search-template.html#_conditional_clauses
</description><key id="47391632">8308</key><summary>Search Template - conditional clauses not rendering correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>:Search Templates</label><label>bug</label></labels><created>2014-10-31T13:03:49Z</created><updated>2015-06-24T06:32:42Z</updated><resolved>2015-06-24T06:32:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2014-10-31T13:04:29Z" id="61256849">I was testing it with ES 1.3.0
</comment><comment author="lukas-vlcek" created="2014-10-31T14:15:28Z" id="61265875">@clintongormley thanks for clarifying this. Out of curiosity, does the template example in question work for you?
</comment><comment author="clintongormley" created="2014-10-31T14:34:29Z" id="61268828">@lukas-vlcek did you wrap it in a `query` element and pass it as an escaped string?  

I've just pushed another improvement to the docs to include both of the above elements
</comment><comment author="lukas-vlcek" created="2014-10-31T18:06:38Z" id="61302925">@clintongormley thanks for this, but still I am unable to get search template with section work. Here is trivial recreation https://gist.github.com/lukas-vlcek/56540a7e8206d122d55c
Is there anything I do wrong? Am I missing some escaping?
</comment><comment author="lukas-vlcek" created="2014-10-31T18:12:00Z" id="61303916">Just in case here is excerpt from server log:

```
[2014-10-31 19:03:29,575][DEBUG][action.search.type       ] [Jester] [twitter][4], node[ccMkzywSSfi-4ttibpHh9w], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7de37772] lastShard [true]
org.elasticsearch.ElasticsearchParseException: Failed to parse template
    at org.elasticsearch.search.SearchService.parseTemplate(SearchService.java:612)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:514)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:487)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:256)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.common.jackson.core.JsonParseException: Unexpected character ('{' (code 123)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name
 at [Source: [B@438d7c6b; line: 1, column: 4]
    at org.elasticsearch.common.jackson.core.JsonParser._constructError(JsonParser.java:1419)
    at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508)
    at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:437)
    at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1808)
    at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1496)
    at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:693)
    at org.elasticsearch.common.xcontent.json.JsonXContentParser.nextToken(JsonXContentParser.java:50)
    at org.elasticsearch.index.query.TemplateQueryParser.parse(TemplateQueryParser.java:113)
    at org.elasticsearch.index.query.TemplateQueryParser.parse(TemplateQueryParser.java:103)
    at org.elasticsearch.search.SearchService.parseTemplate(SearchService.java:605)
    ... 9 more
```
</comment><comment author="clintongormley" created="2014-11-01T14:45:00Z" id="61370324">@lukas-vlcek I can't get this working either. No idea what is going on here :(
</comment><comment author="lukas-vlcek" created="2014-11-02T13:22:30Z" id="61406274">@clintongormley thanks for look at this. Shall I change the title of this issue and remove the [DOC] part? It is probably not a DOC issue anymore, is it?
</comment><comment author="wwken" created="2014-11-06T01:09:38Z" id="61911879">After looking deep inside the code, I am wondering has this (i.e. the conditional clause thing) been ever working? The root clause is, inside the JsonXContentParser.java:nextToken() method, the parser.nextToken() throws an exception straight out of the box when it encounters the clause like {{#use_size}}.    I guess such clause has not been implemented to be recognized yet.  Notice that the parser is of type org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser .  

It looks like we have to override the UTF8StreamJsonParser's nextToken() method and do some look ahead for the conditional clauses.   
</comment><comment author="clintongormley" created="2014-11-06T10:05:58Z" id="61955100">@wwken As I understand it, the JSON parser should never see this the `{{#use_size}}`.  The template is passed as a string, not as JSON, so the Mustache should render the template (thus removing the conditionals) to generate JSON which can _then_ be parsed.

This would be much easier to debug if we had #6821
</comment><comment author="wwken" created="2014-11-07T03:40:07Z" id="62091740"> I fixed it.  Please refer to this pull request: https://github.com/elasticsearch/elasticsearch/pull/8376/files
(P.S. plz ignore the above few links as i messed up checking in some incorrect commit files before by mistake)
</comment><comment author="clintongormley" created="2014-11-07T13:18:52Z" id="62140799">thanks @wwken - we'll take a look and get back to you
</comment><comment author="lukas-vlcek" created="2014-11-07T17:21:43Z" id="62178716">@clintongormley #8393 is my try to fix this issue. If you find this relevant and to the point then it might make sense to consider renaming this issue again to more general description (I think the main issue here is broken parser logic of TemplateQueryParser in case the template value is a single string token. In other words I think there is more general issue not directly related to conditional clauses only).
</comment><comment author="wwken" created="2014-11-07T19:25:23Z" id="62198175">@lukas-vlcek  I think for sure your solution is better! I have two suggestions on it though (minor, not any important):

1) In this file, https://github.com/lukas-vlcek/elasticsearch/blob/8308/src/main/java/org/elasticsearch/search/SearchService.java, you can omit the lines from 617-635 since they are not needed

2) in this file, https://github.com/lukas-vlcek/elasticsearch/blob/8308/src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java, you can separate the single if in line 113 to two if statements to make it more elegant :)

thanks 
</comment><comment author="lukas-vlcek" created="2014-11-09T14:28:08Z" id="62305114">@wwken thanks for looking at this. I am going to wait if ES devs give any feedback. May be they will want to fix it in very different approach. Who knows...
</comment><comment author="lukas-vlcek" created="2014-11-12T09:58:31Z" id="62694654">Bump.

Is there any plan to have this fixed in 1.4? Or even backporting it to 1.3? We would like to make use of Search Templates but this issue is somehow limiting us. How can I help with this?
</comment><comment author="clintongormley" created="2015-06-23T17:54:59Z" id="114590198">Closed by #11512
</comment><comment author="lukas-vlcek" created="2015-06-24T06:32:40Z" id="114746552">@clintongormley correct, this issue was created before I opened more general PR. Closing...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update search-template.asciidoc</comment></comments></commit></commits></item><item><title>Dump Event "gpf" (00002000) received</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8307</link><project id="" key="" /><description>i hava a question,today i install es on the linux system ,if i add a new field  ,the es will shutdown , the javacore file said : "Dump Event "gpf" (00002000) received"...

but if i add some values on a field existed,it is ok.

and if the es installed on my windows system ,all of my operations is ok..

help me please....
</description><key id="47389243">8307</key><summary>Dump Event "gpf" (00002000) received</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">balibaba12</reporter><labels /><created>2014-10-31T12:32:31Z</created><updated>2014-11-19T14:34:12Z</updated><resolved>2014-10-31T13:38:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T13:38:05Z" id="61260726">It sounds like you are using IBM's JDK, which isn't supported.  Please install Oracle's JDK or OpenJDK
</comment><comment author="balibaba12" created="2014-11-19T14:34:12Z" id="63648113">thank you very much
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change log level for mpercolate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8306</link><project id="" key="" /><description>When using _mpercolate API we log by default a lot of DEBUG `Percolate shard response`.
They should be in TRACE level instead of DEBUG.
</description><key id="47388550">8306</key><summary>Change log level for mpercolate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-31T12:23:19Z</created><updated>2015-06-07T10:49:12Z</updated><resolved>2014-10-31T12:30:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-31T12:23:58Z" id="61252805">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Add mention of `hyphenation_patterns_path`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8305</link><project id="" key="" /><description>Refs ElasticSearch's HyphenationCompoundWordTokenFilterFactory.java.
</description><key id="47381288">8305</key><summary>Docs: Add mention of `hyphenation_patterns_path`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">akx</reporter><labels><label>docs</label></labels><created>2014-10-31T10:47:18Z</created><updated>2014-11-01T14:48:51Z</updated><resolved>2014-11-01T14:48:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T13:32:11Z" id="61259963">Hi @akx 
Thanks for the PR. Please could you fix the asciidoc - you've used `/` instead of `|` for the table markers.

thanks
</comment><comment author="akx" created="2014-10-31T20:38:38Z" id="61327608">Oops, yeah, my bad @clintongormley -- looks like the online GitHub editor turned the `|` into a weird solidus character of some sort, and not being used to Asciidoc, I figured it was a forward slash. 

Fixed now.
</comment><comment author="clintongormley" created="2014-11-01T14:48:33Z" id="61370430">thanks @akx - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add mention of `hyphenation_patterns_path`</comment></comments></commit></commits></item><item><title>Use a 1024 byte minimum weight for filter cache entries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8304</link><project id="" key="" /><description>This changes the weighing function for the filter cache to use a
configurable minimum weight for each filter cached. This value defaults
to 1kb and can be configured with the
`indices.cache.filter.minimum_entry_weight` setting.

This also fixes an issue with the filter cache where the concurrency
level of the cache was exposed as a setting, but not used in cache
construction.

Relates to #8268
Fixes #8249
</description><key id="47378778">8304</key><summary>Use a 1024 byte minimum weight for filter cache entries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Cache</label><label>enhancement</label><label>resiliency</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-31T10:15:58Z</created><updated>2015-06-07T10:25:10Z</updated><resolved>2014-10-31T12:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-10-31T10:18:46Z" id="61241522">I also manually tested this, where I ran a shell script containing non-existing term filters over the wikipedia corpus, I was able to see:

``` json
        "filter_cache" : {
          "memory_size" : "396.2kb",
          "memory_size_in_bytes" : 405760,
          "evictions" : 55018
        }
```

to show that filter cache entries are being evicted even though the size limit has not been reached due to the minimum size.
</comment><comment author="jpountz" created="2014-10-31T11:52:19Z" id="61249972">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed bitset filter cache leftover in nested filter </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8303</link><project id="" key="" /><description /><key id="47377591">8303</key><summary>Fixed bitset filter cache leftover in nested filter </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cache</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-31T10:01:45Z</created><updated>2015-06-07T18:19:49Z</updated><resolved>2014-10-31T10:38:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-10-31T10:18:05Z" id="61241446">looks great
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add elastic, a client for Google Go.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8302</link><project id="" key="" /><description>I worked on another client for Elasticsearch for Google Go. It is used in production for more than two years now.
</description><key id="47374110">8302</key><summary>Add elastic, a client for Google Go.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">olivere</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-31T09:17:53Z</created><updated>2014-10-31T13:43:05Z</updated><resolved>2014-10-31T13:43:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T13:29:26Z" id="61259631">Hi @olivere 

Please could I ask you to sign the CLA so that I can merge this in? http://www.elasticsearch.org/contributor-agreement/

Also, I think it'd be more polite to add your client to the end of the list, rather than the beginning ;)

thanks
</comment><comment author="olivere" created="2014-10-31T13:39:36Z" id="61260887">Hi @clintongormley,

I signed the CLA just now. And, of course, it wasn't my intent to jump the queue. Sorry about that :-)

Cheers.
</comment><comment author="clintongormley" created="2014-10-31T13:42:54Z" id="61261276">thanks @olivere  - merged :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add elastic client for Google Go.</comment></comments></commit></commits></item><item><title>POST on _mapping endpoint fails with NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8301</link><project id="" key="" /><description>ES 1.3.4

The following command makes not much sense, but it fails with an NPE.

```
POST /alias/type/_mapping
```

Result

```
{
   "error": "RemoteTransportException[[Kierrok][inet[/10.3.2.31:19300]][indices/mapping/put]]; nested: NullPointerException; ",
   "status": 500
}
```

where `alias` is an index alias.

No error in the server logs.
</description><key id="47372302">8301</key><summary>POST on _mapping endpoint fails with NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>bug</label></labels><created>2014-10-31T08:52:53Z</created><updated>2014-10-31T18:58:35Z</updated><resolved>2014-10-31T18:58:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T13:27:58Z" id="61259426">The same thing happens with an index instead of an alias. The NPE seems to come from the missing body.
</comment><comment author="rjernst" created="2014-10-31T18:58:34Z" id="61311913">This was fixed in #7618 and is in 1.4.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mysql migration using jdbc-river</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8300</link><project id="" key="" /><description>curl -XPUT 'localhost:9200/_river/jdbc/_meta' -d '{ 
"type" : "jdbc", "jdbc" : { "driver" : "com.mysql.jdbc.Driver",
 "url" : "jdbc:mysql://1.231.6.213:3306; databaseName=SB", 
"user" : "SIGS", 
"password" : "tdfsdaf38",  
"sql" : "SELECT book_id, contents, reg_dt, upd_dt, upd_id, reg_id, page_id, header FROM epp_editor_book_storage_sample_elasticsearch" } }'

I try to save the mysql data on elasticsearch.
Would not this data is entered into the es?
</description><key id="47361305">8300</key><summary>mysql migration using jdbc-river</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">poswith</reporter><labels /><created>2014-10-31T05:06:44Z</created><updated>2014-10-31T07:24:09Z</updated><resolved>2014-10-31T07:24:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-31T07:24:09Z" id="61227129">Jdbc river is not part of elasticsearch and has its own repository.
Please ask this to the mailing list or open an issue in the jdbc repository or contact the author.

BTW we are using github issues only for issues or feature requests. The best place to ask questions is the mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Netty: Add HTTP pipelining support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8299</link><project id="" key="" /><description>This adds HTTP pipelining support to netty. Previously pipelining was not
supported due to the asynchronous nature of elasticsearch. The first request
that was returned by Elasticsearch, was returned as first response,
regardless of the correct order.

The solution to this problem is to add a handler to the netty pipeline
that maintains an ordered list and thus orders the responses before
returning them to the client. This means, we will always have some state
on the server side and also requires some memory in order to keep the
responses there.

Pipelining is enabled by default, but can be configured by setting the
http.pipelining property to true|false. In addition the maximum size of
the event queue can be configured.

The initial netty handler is copied from this repo
https://github.com/typesafehub/netty-http-pipelining

Closes #2665
</description><key id="47344917">8299</key><summary>Netty: Add HTTP pipelining support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label><label>bug</label><label>release highlight</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-30T23:30:53Z</created><updated>2015-03-19T16:43:04Z</updated><resolved>2014-10-31T15:31:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-10-31T13:16:49Z" id="61258192">few comments, LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test: index dump on test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8298</link><project id="" key="" /><description>This changes adds the ability for a test to create an index dump if a search request fails the test. This can be helpful to be able to analyse the index state after the test has failed
</description><key id="47309300">8298</key><summary>Test: index dump on test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels /><created>2014-10-30T18:28:17Z</created><updated>2014-11-25T11:02:53Z</updated><resolved>2014-11-14T14:47:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-03T11:32:20Z" id="61466024">@rjernst thanks for the review. I left some replies to your comments, let me know if you don't agree
</comment><comment author="colings86" created="2014-11-14T14:47:11Z" id="63073808">Closing this PR as it would be better to keep files around on test failure rather than trying to do a snapshot of the index which would only get the primary index and may not show the problem properly
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cut over MetaDataStateFormat to NIO Path API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8297</link><project id="" key="" /><description>This class already uses Path most of the time since it
uses ATOMIC_MOVE. This commit makes it a bit more consistent.
</description><key id="47298478">8297</key><summary>Cut over MetaDataStateFormat to NIO Path API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-30T17:07:42Z</created><updated>2015-06-07T17:36:19Z</updated><resolved>2014-10-30T19:50:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-10-30T18:29:58Z" id="61145127">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide explanation of 'unassigned' shard state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8296</link><project id="" key="" /><description>As stated in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/states.html shard may be unassigned for variuos reasons. One of them is that it is still pending to be assigned, but another is that index requires more replicas than cluster currently can provide. There are many more, I guess.
It would be good if some explanation would be provided in the response.

Rationale after that is that when cluster starts/node restarts/things happen - it is hard to find out on your own what is the reason behing shard being unassigned
</description><key id="47289960">8296</key><summary>Provide explanation of 'unassigned' shard state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kretes</reporter><labels><label>discuss</label></labels><created>2014-10-30T16:13:37Z</created><updated>2015-11-21T20:21:07Z</updated><resolved>2015-11-21T20:21:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:21:07Z" id="158678734">Closed by https://github.com/elastic/elasticsearch/pull/11653
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Remove redundant call to setTemplateType()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8295</link><project id="" key="" /><description /><key id="47289677">8295</key><summary>[TEST] Remove redundant call to setTemplateType()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-30T16:11:21Z</created><updated>2014-11-06T06:58:31Z</updated><resolved>2014-11-05T11:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-05T10:43:36Z" id="61789099">@lukas-vlcek thanks for the PR. I'll get this merged into the code.
</comment><comment author="colings86" created="2014-11-05T11:38:13Z" id="61794962">@lukas-vlcek this has been merged into the master and 1.x branches. Thanks again for the PR
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Packaging: Confusing configuration override functionality with RPM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8294</link><project id="" key="" /><description>When installing via the RPM we add a property file to `/etc/sysconfig`. The configuration in this file overrides the `elasticsearch.yml` file (namely the data dir, work dir, and log dir). This is not clear in the documentation. We should do one of the following:
- Remove the data dir, log dir and work dir options from this file
- Make the config in elasticsearch.yml override this config
- Make the documentation clearer on this
</description><key id="47284093">8294</key><summary>Packaging: Confusing configuration override functionality with RPM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">colings86</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2014-10-30T15:29:05Z</created><updated>2016-11-26T11:54:14Z</updated><resolved>2016-11-26T11:54:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nellicus" created="2014-10-30T15:51:00Z" id="61115824">using CentOS Linux release 7.0.1406 (Core) the istructions in the docs worked flawlessy.
/etc/sysconfig/elasticsearch.yml is installed but not seem to override /etc/elasticsearch/elasticsearch.yml
</comment><comment author="clintongormley" created="2014-10-31T13:21:59Z" id="61258742">The docs do provide info on the layout for the RPM: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-dir-layout.html#_deb_and_rpm

but i agree it doesn't tell you where those options are set
</comment><comment author="colings86" created="2014-10-31T14:19:21Z" id="61266453">@clintongormley I think we need to be clearer about the fact that the order of precedence for configuration when using the RPM version is:
1. Command line arguments passed into the service start command
2. The settings in `/etc/sysconfig/elasticsearch`
3. the settings in `elasticsearch.yml`
</comment><comment author="clintongormley" created="2016-11-26T11:54:14Z" id="263059400">These docs have been updated</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Debian Daemon Script Question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8293</link><project id="" key="" /><description>I see in the script that the following are required
# Required-Start:    $network $remote_fs $named

I do not have named installed on my machine. Is this a requirement?
</description><key id="47279514">8293</key><summary>Debian Daemon Script Question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">envitraux</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2014-10-30T14:54:43Z</created><updated>2014-11-01T14:46:56Z</updated><resolved>2014-11-01T14:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T12:55:24Z" id="61255857">@electrical any ideas?
</comment><comment author="electrical" created="2014-10-31T20:29:39Z" id="61326323">Did some looking up.

$named for the init scripts stands for the following:

```
daemons which may provide hostname resolution (if present) are running. For example, daemons to query DNS, NIS+, or LDAP.
```

So in this case any daemon that provides that specific service. it doesn't need to be the named process it self.
Since every system comes with a process to do name resolving it should have no impact.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Logging configuration can pick up dpkg-dist files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8292</link><project id="" key="" /><description>There are some cases where dpkg/apt can leave file like logging.yml.dpkg-dist files in /etc/elasticsearch and LogConfigurator will pick them up by default.  This is bad because those files might contain garbage and break logging.  Why not _just_ pick up logging.yml?  Is this something like the [conf.d](http://serverfault.com/a/240182/151116) pattern? 
</description><key id="47270301">8292</key><summary>Logging configuration can pick up dpkg-dist files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-10-30T13:38:22Z</created><updated>2014-10-31T12:51:33Z</updated><resolved>2014-10-31T12:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-31T12:51:33Z" id="61255455">Closing as duplicate of #8040 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Return 0 instead of -1 for unknown/non-exposed ramBytesUsed()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8291</link><project id="" key="" /><description>From @rmuir - "The accountable interface specifies that such values are illegal"

Fixes #8239
</description><key id="47268855">8291</key><summary>Return 0 instead of -1 for unknown/non-exposed ramBytesUsed()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Core</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-30T13:24:35Z</created><updated>2015-03-19T10:18:51Z</updated><resolved>2014-11-03T16:12:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-31T18:00:35Z" id="61301803">What about in `BinaryDVNumericIndexFieldData`, `NumericDVIndexFieldData`? I see anonymous instance of `AtomicDoubleFieldData` that initializes `ramBytesUsed` to -1.

Also `SortedNumericDoubleFieldData`, `SortedNumericFloatFieldData` and `SortedNumericLongFieldData` call `super()` with -1.
</comment><comment author="dakrone" created="2014-11-03T11:06:53Z" id="61463793">I fixed the other invocations, thanks for taking a look @rjernst !
</comment><comment author="rjernst" created="2014-11-03T16:05:06Z" id="61500516">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: document action.replication_type setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8290</link><project id="" key="" /><description>Document action.replication_type setting
</description><key id="47267768">8290</key><summary>Docs: document action.replication_type setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>docs</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-30T13:13:07Z</created><updated>2014-10-31T12:54:33Z</updated><resolved>2014-10-31T12:54:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="astefan" created="2014-10-30T14:19:42Z" id="61098668">@dadoonet I've rephrased, does it look better?
</comment><comment author="dadoonet" created="2014-10-30T14:21:07Z" id="61098944">LGTM. @clintongormley I think it should goes as well in 1.3 branch, right?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Document action.replication_type setting</comment></comments></commit></commits></item><item><title>Immediately remove filter cache entries on cache clear</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8289</link><project id="" key="" /><description>Previously we had a more complex listing where we added each reader key to the list of keys cleaned intermittently by `IndicesFilterCache.ReaderCleaner.run()`, but because every entry is being invalidated due to the cache clear, we should immediately clear them and clean up the cache.

This also moves the periodic cleaning rescheduling into a finally block so an exception does not prevent the cleanup from being rescheduled.

Fixes #8285
</description><key id="47266690">8289</key><summary>Immediately remove filter cache entries on cache clear</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Cache</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-30T13:01:20Z</created><updated>2015-06-07T10:25:20Z</updated><resolved>2014-11-03T13:34:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Support usage of ES_JAVA_OPTS in plugin commands</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8288</link><project id="" key="" /><description>As that environment variable can contain elasticsearch specific configuration
like the path to a configuration, it makes sense to support in both
elasticsearch commands.
</description><key id="47259379">8288</key><summary>Support usage of ES_JAVA_OPTS in plugin commands</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-30T11:34:16Z</created><updated>2015-06-08T00:16:38Z</updated><resolved>2014-10-30T12:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-30T12:02:34Z" id="61081118">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Plugins: Support usage of ES_JAVA_OPTS in plugin commands</comment></comments></commit></commits></item><item><title>_recovery command returns "No handler found" error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8287</link><project id="" key="" /><description>I tried to query a snapshot restore status by querying "GET _recovery" with the Sense.
(See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-recovery.html#indices-recovery)
This query returns "No handler found for uri [/_recovery] and method [GET]".

![screenshot_10](https://cloud.githubusercontent.com/assets/6837849/4841430/a49d6872-601a-11e4-9f4b-976bf842e467.jpg)

Any idea why is it happening?
</description><key id="47250983">8287</key><summary>_recovery command returns "No handler found" error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">asafc64</reporter><labels><label>feedback_needed</label></labels><created>2014-10-30T09:55:33Z</created><updated>2014-11-02T12:34:02Z</updated><resolved>2014-11-02T12:34:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-30T11:05:29Z" id="61075502">The `_recovery` API was added in v1.1.0.  I assume you're using an older version of Elasticsearch.

Please feel free to reopen if this is not the case.
</comment><comment author="asafc64" created="2014-10-30T11:09:48Z" id="61075926">I got the latest version (1.3.4) a few days ago.
</comment><comment author="clintongormley" created="2014-10-30T11:21:17Z" id="61077129">@asafc64 this works for me.  I wonder if you have a space at the end of your request, ie:

```
GET /_recovery[space]
```

In the current version of Sense, available in [Marvel](http://www.elasticsearch.org/guide/en/marvel/current/index.html) it reports this error correctly, but maybe in an older version it didn't.

Try this:

```
curl -XGET "http://localhost:9200/_recovery"
```
</comment><comment author="clintongormley" created="2014-10-30T18:07:04Z" id="61140856">@asafc64 have you tried the above?
</comment><comment author="asafc64" created="2014-11-02T07:27:12Z" id="61396976">Yes.
Apparently, I've had mixed jar files from different versions.
Sorry for the hassle, my bad.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: Fix location information for loggers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8286</link><project id="" key="" /><description>This change corrects the location information gathered by the loggers so that when printing class name, method name, and line numbers in the log pattern, the information from the class calling the logger is used rather than a location within the logger itself.

A reset method has also been added to the LogConfigurator class which allows the logging configuration to be reset. This is needed because if the LoggingConfigurationTests and Log4jESLoggerTests are run in the same JVM the second one to run needs to be able to override the log configuration set by the first

Closes #5130, #8052
</description><key id="47248856">8286</key><summary>Core: Fix location information for loggers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>bug</label></labels><created>2014-10-30T09:32:34Z</created><updated>2014-11-25T11:02:41Z</updated><resolved>2014-10-30T10:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-10-30T09:45:17Z" id="61066617">LGTM, one question though, will resetting the log configuration change the configuration of the logs for the rest of the tests?
</comment><comment author="colings86" created="2014-10-30T09:50:03Z" id="61067191">hmmm I don't think so as while debugging I was happily logging to the tests logger without it being affected. I think once the logger has been created it doesn't look at the configuration again. Also The LoggingConfigurationTests class has been changing that configuration happily before this commit without affecting the rest of the tests so I'm pretty sure its ok
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Immediately clear the filter cache when requested</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8285</link><project id="" key="" /><description>Currently, a `/_cache/clear?filter` request marks entries to be invalidated, but they only get cleared after a maximum delay of `cleanedInterval`.

We should force the cache to be cleaned in these conditions, as was done for the fielddata cache in https://github.com/elasticsearch/elasticsearch/commit/65ce5acfb41b4abd0c527aa0e870c2a1076d76cd
</description><key id="47248731">8285</key><summary>Immediately clear the filter cache when requested</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-10-30T09:31:08Z</created><updated>2015-04-27T09:02:47Z</updated><resolved>2015-04-27T09:02:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-27T09:02:46Z" id="96574463">Won't fix - the filter caching mechanism is changing completely in Lucene 5
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve "directory cannot be listed" message to reflect underlying cause</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8284</link><project id="" key="" /><description>In https://github.com/elasticsearch/elasticsearch/pull/7954#issuecomment-60454227, @drewr saw the following error message:

```
[2014-10-24 21:28:48,949][WARN ][index.engine.internal    ] [ops27-data04-A] [foo][31] failed engine [merge exception]
org.apache.lucene.index.MergePolicy$MergeException: java.io.IOException: directory '/d/es/data/ops27-data04-A/org.elasticsearch.test.ops27.data/nodes/0/indices/foo/31/index' exists and is a directory, but cannot be listed: list() returned null
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:133)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)
```

Apparently, the underlying cause for this exception is not having enough file handles, but there is nothing  in the message to reflect this.

Is too few file handles the only cause of the exception, or are there others?  This is an exception thrown by Java. Is it possible to catch this particular exception and return a better more explanatory message?
</description><key id="47247021">8284</key><summary>Improve "directory cannot be listed" message to reflect underlying cause</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Core</label><label>discuss</label><label>enhancement</label></labels><created>2014-10-30T09:10:41Z</created><updated>2015-11-21T21:03:53Z</updated><resolved>2015-11-21T21:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:18:07Z" id="158678619">@mikemccand not sure if this is still an issue but if so, would it be possible to improve this exception or is the real reason (not enough file handles) just not available?
</comment><comment author="mikemccand" created="2015-11-21T21:03:52Z" id="158680555">@clintongormley I think in ES 2.0 this is already improved, because Lucene switched from `Files.list` in 4.10.x to `Files.newDirectoryStream` in 5.x, and the latter does a better job propagating errors from the OS up to Java, I think.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Marvel is not proxy-friendly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8283</link><project id="" key="" /><description>We have marvel deployed, and it works great.  We'd like to put it behind a proxy, so it's accessible the same way the rest of our monitoring infrastructure is.

However, Marvel isn't playing nice when you access it from a proxy.  It makes relative redirects, it makes HTTP requests to URLs outside its path, etc.

Examples:

Relative redirect:

```
$ curl  $MARVEL_HOST/_plugin/marvel/
  ...
  &lt;meta http-equiv="refresh" content="1;url=kibana/index.html"&gt;
  &lt;script type="text/javascript"&gt;
    window.location.href = "kibana/index.html"
  &lt;/script&gt;
  ...
```

URLs outside the `/_plugin/marvel/` path:

```
GET /_nodes
GET /.marvel-2014.10.30/_aliases?ignore_missing=true
POST /.marvel-2014.10.30/_search
GET /.marvel-2014.10.27,.marvel-2014.10.28,.marvel-2014.10.29,.marvel-2014.10.30/_aliases?ignore_missing=true
GET /.marvel-kibana/appdata/marvelOpts
POST /.marvel-kibana/dashboard/_mget
```

This behavior makes it extremely difficult to put Marvel behind a proxy.  Is this something you'd consider as an improvement to Marvel?

Cheers
</description><key id="47224727">8283</key><summary>Marvel is not proxy-friendly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">simpsora</reporter><labels><label>feedback_needed</label></labels><created>2014-10-30T01:44:22Z</created><updated>2016-05-03T14:59:42Z</updated><resolved>2015-04-16T12:51:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="simpsora" created="2014-10-30T01:45:10Z" id="61034136">This issue was also reported (and closed) here:
https://github.com/elasticsearch/kibana/issues/995
</comment><comment author="bleskes" created="2014-10-30T09:43:30Z" id="61066406">&gt; Relative redirect: ... 

I'm not sure about this one. Why is this a problem for the proxy?

&gt; POST /.marvel-2014.10.30/_search

Those are made based on the `elasticsearch` configuration option stored in the plugins/marvel/_site/kibana/config.js file. We try to be smart about the defaults here (see bellow, which assumes ES is available/proxied from the root of the path) but you can change that to what ever fits your needs:

```
      elasticsearch: window.location.protocol+"//"+window.location.hostname+(window.location.port !== '' ? ':'+window.location.port : '')
```
</comment><comment author="bleskes" created="2015-03-23T13:46:33Z" id="85001588">@simpsora did you try the config test I suggested? 
</comment><comment author="simpsora" created="2015-04-16T03:17:57Z" id="93626827">Hi @bleskes, sorry for the radio silence.  We changed tactics on how we use marvel and are no longer using it with a proxy.  As such, I haven't had a chance to test out your suggestions.  However, I think the config you mention would have solved our problems.
</comment><comment author="bleskes" created="2015-04-16T12:51:37Z" id="93726326">cool. I will close this then. If anything pops up, please re-open.
</comment><comment author="simpsora" created="2015-08-26T01:57:08Z" id="134789690">Hi, just a followup on this, I was able to get Marvel working behind a proxy as you described above, by modifying the `elasticearch` config entry to include a path.

Perhaps when Marvel is updated to use Kibana 4 this will become easier.

Thanks for the tip!
</comment><comment author="my10c" created="2016-05-03T07:07:53Z" id="216455092">can someone share their config ? I'm trying to proxy marvel 1.3 behind nginx and have the same issue
as described by simpsora... the elasticsearch value unchanged, using the port 9200 work, but we are require not to expose any port then 80/443
marvel 1.3 with ES 1.7.3
thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search Template with Count API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8282</link><project id="" key="" /><description>I am using Search Templates for performing queries. How can I get the count with search template?

Example:

```
PUT blogs/post/1
{
  "title": "title1",
  "desc": "desc1"
}

PUT /blogs/post/2
{
  "title": "title2",
  "desc": "desc2"
}

PUT /_search/template/search_posts
{
  "template": {
    "query": {
      "match_all": {}
    }
  }
}

GET /blogs/post/_search/template
{
  "template": {
    "id": "search_posts"
  },
  "params": {}
}

GET blogs/post/_search
{
  "query": {
    "match_all": {}
  }
}

GET blogs/post/_count
{
  "query": {
    "match_all": {}
  }
}
```
</description><key id="47224435">8282</key><summary>Search Template with Count API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sofianito</reporter><labels /><created>2014-10-30T01:38:09Z</created><updated>2014-10-30T10:59:06Z</updated><resolved>2014-10-30T10:49:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-30T10:49:42Z" id="61073818">@sofianito Please use the mailing list for questions like this.  http://elasticsearch.org/community

thanks
</comment><comment author="sofianito" created="2014-10-30T10:59:06Z" id="61074825">Sorry, I'll post the question there.

Gracias :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add the ability to sort aggregations by allowing arithmetic operations between sub aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8281</link><project id="" key="" /><description>I want to get the top X event senders for a time range but sorted not by doc count but rather by percentage increase/decrease to a different time range.
I know I can do this:

```
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "sentTimestamp": {
              "gte": "2014-10-07T00:01:00.0Z",
              "lt": "2014-10-07T00:01:50.0Z"
            }
          }
        }
      ]
    }
  },
  "size": 0,
  "aggs": {
    "topSenders": {
      "terms": {
        "field": "sender",
        "size": 5,
        "order": {
          "t2&gt;tu.value": "desc"
        }
      },
      "aggs": {
        "t1": {
          "filter": {
            "range": {
              "sentTimestamp": {
                "gte": "2014-10-07T00:01:00.0Z",
                "lt": "2014-10-07T00:01:40.0Z"
              }
            }
          },
          "aggs": {
            "tu": {
              "cardinality": {
                "field": "sender"
              }
            }
          }
        },
        "t2": {
          "filter": {
            "range": {
              "sentTimestamp": {
                "gte": "2014-10-07T00:01:40.0Z",
                "lt": "2014-10-07T00:01:50.0Z"
              }
            }
          },
          "aggs": {
            "tu": {
              "cardinality": {
                "field": "sender"
              }
            }
          }
        }
      }
    }
  }
}
```

You can see I am sorting by "t2&gt;tu.value". If I was able to sort by something like "(t2&gt;tu.value-t2&gt;tu.value)/t1&gt;tu.value" I would be able to get that all in one query
</description><key id="47222818">8281</key><summary>Add the ability to sort aggregations by allowing arithmetic operations between sub aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nitzanharel</reporter><labels><label>:Aggregations</label><label>stalled</label></labels><created>2014-10-30T01:05:39Z</created><updated>2017-03-31T10:05:01Z</updated><resolved>2014-10-30T10:47:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-30T10:36:45Z" id="61072411">This looks like a job for reducers, see #8110.

@colings86 a sorting reducer would be a nice addition.
</comment><comment author="clintongormley" created="2014-10-30T10:45:10Z" id="61073349">Actually, @colings86 has pointed out that this would be possible with a scripted sort provided by #6917
</comment><comment author="clintongormley" created="2014-10-30T10:47:10Z" id="61073573">I'm going to close this in favour of #6917
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Read time out errors when recovering a snapshot from S3 repository lets elasticsearch hanging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8280</link><project id="" key="" /><description>During a snapshot restore process for a snapshot which is stored in a S3 repository, chances are that a Read Timeout error occurs when communicating to S3. Don't know whether S3 or Elasticsearch are to blame.

The issue is that that timeout error should be translated in some kind of error response from elasticsearch, but elasticsearch just keeps waiting for an answer that never comes. In consequence, besides dead points in the application code, the cluster remains in a fake recovery process, preventing it from allowing other recoveries.

this is the TimeoutError, as seen in elasticsearch logs

the endpoint is

```
http://elasticsearch.server.com/_snapshot/repository_name/snapshot_name/_restore?wait_for_completion=true
```

```

```

[2014-10-23 16:37:50,005][INFO ][snapshots                ] [Whiplash] snapshot [backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730] is done
[2014-10-23 16:37:50,218][INFO ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632] deleting index
[2014-10-23 16:37:50,236][INFO ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_phonetic_ts20141023163747520269] deleting index
[2014-10-23 16:44:28,557][WARN ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632] re-syncing mappings with cluster state for types [[product]]
[2014-10-23 16:44:28,558][WARN ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_phonetic_ts20141023163747520269] re-syncing mappings with cluster state for types [[product]]
[2014-10-23 16:46:24,886][WARN ][indices.cluster          ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:130)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)
        ... 3 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:158)
        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)
        ... 4 more
Caused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:741)
        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:155)
        ... 5 more
Caused by: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:152)
        at java.net.SocketInputStream.read(SocketInputStream.java:122)
        at sun.security.ssl.InputRecord.readFully(InputRecord.java:442)
        at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:554)
        at sun.security.ssl.InputRecord.read(InputRecord.java:509)
        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:927)
        at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:884)
        at sun.security.ssl.AppInputStream.read(AppInputStream.java:102)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.read(AbstractSessionInputBuffer.java:204)
        at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:182)
        at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)
        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)
        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)
        at java.security.DigestInputStream.read(DigestInputStream.java:161)
        at com.amazonaws.services.s3.internal.DigestValidationInputStream.read(DigestValidationInputStream.java:59)
        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)
        at java.io.FilterInputStream.read(FilterInputStream.java:107)
        at org.elasticsearch.cloud.aws.blobstore.AbstractS3BlobContainer$1.run(AbstractS3BlobContainer.java:99)
        ... 3 more
[2014-10-23 16:46:24,887][WARN ][cluster.action.shard     ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] sending failed shard for [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0], node[4dZyhkKySw-HpxhLjDYp4A], [P], restoring[backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730], s[INITIALIZING], indexUUID [P6kfnE_UTt6Caae5KAIXHg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index]; nested: SocketTimeoutException[Read timed out]; ]]
[2014-10-23 16:46:24,887][WARN ][cluster.action.shard     ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] received shard failed for [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0], node[4dZyhkKySw-HpxhLjDYp4A], [P], restoring[backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730], s[INITIALIZING], indexUUID [P6kfnE_UTt6Caae5KAIXHg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index]; nested: SocketTimeoutException[Read timed out]; ]]

```

```
</description><key id="47216263">8280</key><summary>Read time out errors when recovering a snapshot from S3 repository lets elasticsearch hanging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">JoeZ99</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>discuss</label></labels><created>2014-10-29T23:20:30Z</created><updated>2015-11-21T20:16:37Z</updated><resolved>2015-11-21T20:16:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-10-31T19:25:48Z" id="61316326">When restore operation is interrupted because of an error, the shard that was being restored is most likely in a broken state. So, elasticsearch automatically tries to create this shard on another node in hope that next time it will be successful next time. This process doesn't stop until 1) the shard is successfully restored or 2) the index is [explicitly deleted](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-snapshots.html#_stopping_currently_running_snapshot_and_restore_operations) by the user. So, if there is a communication issues with S3, elasticsearch will retry for as long as needed until S3 recovers. In other words, it's not a "fake" recovery process, it's recovery processes form S3 repository and it follows the same principles that recovery process from a local gateway.
</comment><comment author="JoeZ99" created="2014-10-31T19:57:33Z" id="61321533">So, if I understood you right, when there is an error during a restore process, elasticsearch will retry the restore process using another node as the shard's destination, I guess because elasticsearch thinks that the error during the first restore process was related somehow to the node it tried to restore the shard to first.
- What if you only have one node? (that's precisely our case) does elasticsearch keeps trying on the same node?
- Even if that's the case (elasticsearch keeps trying on the same node, since there is only one), it's not working in our scenario.the "read timeout" s3 error is not a permanent condition. It just happens from time to time when -probably, we haven't nailed it yet- too many requests to the S3 are being made. What this means is that immediately afterwards this error, the S3 is accessible again, and if elasticsearch "retried" a restore, it would be successfull, and that's not what is happening. Once this error occurs, no recovery can be made unless we explicitly delete the index.
</comment><comment author="imotov" created="2014-11-02T22:27:33Z" id="61426866">Yes, in the case of a single node, it might not retry to restore the shard to the same node. That's the common recovery/restore logic and in most cases I think it makes sense. Should we add some sort of detection of retry-able errors that will be processed differently and where it should reside core or s3 plugin - that's a good question. We need to discuss that. 
</comment><comment author="imotov" created="2014-11-03T15:23:59Z" id="61493329">After a brief discussion with @tlrx, we came to the conclusion that it might make sense to address this as part of https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/125. We can add retry check in S3 layer with possible common retry logic on the BlobStore level. Reassigning it to @tlrx.
</comment><comment author="tlrx" created="2014-11-21T09:55:15Z" id="63948332">Commit elasticsearch/elasticsearch-cloud-aws@ea91adf should help to resolve this issue.

When elasticsearch-cloud-aws will be released for 1.4, could you please test this issue again and reopen if needed?

Thanks 
</comment><comment author="cregev" created="2014-12-29T18:05:52Z" id="68281280">When will this issue is going to be fixed ?
</comment><comment author="JoeZ99" created="2015-01-02T14:44:27Z" id="68530877">@tlrx , I've tested with aws plugin 2.4.1 and elasticsearch 1.4.1, and same behavior, so I'll reopen as instructed
</comment><comment author="JoeZ99" created="2015-01-02T14:46:59Z" id="68531053">@tlrx , Don't know how to reopen this issue.
</comment><comment author="dadoonet" created="2015-01-02T14:47:44Z" id="68531106">@JoeZ99 reopened.
</comment><comment author="cregev" created="2015-01-05T09:52:19Z" id="68687044">Is there a workaround ? 
</comment><comment author="JoeZ99" created="2015-01-05T11:11:30Z" id="68695058">The only"workaround" for us has been to make the API call with the
"wait_for_completion" parameter, and oh it takes more than 5 minutes, issue
a delete for the indices being restored to unlock the cluster and assume a
timeout read error
On Jan 5, 2015 10:53 AM, "Costya Regev" notifications@github.com wrote:

&gt; Is there a workaround ?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8280#issuecomment-68687044
&gt; .
</comment><comment author="tlrx" created="2015-04-03T14:47:49Z" id="89309189">Follow up on https://github.com/elastic/elasticsearch-cloud-aws/issues/149#issuecomment-89309092
</comment><comment author="clintongormley" created="2015-11-21T20:16:36Z" id="158678560">Closing in favour of https://github.com/elastic/elasticsearch-cloud-aws/issues/149#issuecomment-89309092
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cloud/aws/AwsS3Service.java</file><file>src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java</file><file>src/main/java/org/elasticsearch/cloud/aws/blobstore/DefaultS3OutputStream.java</file><file>src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobContainer.java</file><file>src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java</file><file>src/main/java/org/elasticsearch/repositories/s3/S3Repository.java</file></files><comments><comment>Add retry logic for S3 connection errors when restoring snapshots</comment></comments></commit></commits></item><item><title>Meta data support with each aggregation request/response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8279</link><project id="" key="" /><description>This commit adds the ability to associate a bit of state with each individual aggregation.

The aggregation response can be hard to stitch back together without having a reference to the aggregation request. In many cases this is not available, many json serializer frameworks cache types globally or have a static deserialisation override mechanism. In these cases making the original request available, if at all possible, would be a hack. 

The old facets returned `_type` which was just enough metadata to know what the originating facet type in the request was. 

This PR takes `_type` one step further by introducing ANY arbitrary meta data. This could be further &lt;strike&gt;ab&lt;/strike&gt;used for instance by generic/automated aggregations that include UI state (color information, thresholds, user input states, etc) per aggregation.

[Here's a gist of a sense session](https://gist.github.com/Mpdreamz/785a3347d4415e1fe6bb#file-aggs-metada) you can use to review this branch

This PR supersedes #6465 which got stale and horribly out of date with all the new aggregations in `1.2-1.4`. 

Tested on a cluster of multiple nodes as well.

Example request:

``` json
"aggs": {
    "name": {
      "terms": {
        "field": "title"
      },
      "meta": { "number_of_clicks" : 2 },
      "aggs": {
        "viewport" : {
          "meta" : { 
              "hasColor" : true 
          },
          "geo_bounds" : {
            "field" : "location"
          }
        },
        "empty_agg" : {
          "meta" : { 
             "string" : "returns"
          },
          "sum": {
            "field": "i_do_not_exist"
          }
        },
```

Example response:

``` json
"aggregations": {
      "name": {
         "meta": {
            "number_of_clicks": 2
         },
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "banner",
               "doc_count": 1,
               "viewport": {
                  "meta": {
                     "hasColor": true
                  },
                  "bounds": {
                     "top_left": {
                        "lat": 28,
                        "lon": 21
                     },
       &lt;redacted /&gt;
```
</description><key id="47203774">8279</key><summary>Meta data support with each aggregation request/response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-29T21:07:14Z</created><updated>2015-06-07T11:57:30Z</updated><resolved>2014-11-03T21:33:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-02T21:05:03Z" id="61423535">Hey @Mpdreamz the change looks good. The comment on the PR seems to suggest that any json document can be used as metadata (eg. a number) but the Java API requires a hash, is it intended?
</comment><comment author="Mpdreamz" created="2014-11-02T21:07:50Z" id="61423641">Thanks for the review @jpountz !

No this is an oversight from copying it over from the previous PR that did allow any Json construct as meta, but now it has to be a hash (based on feedback on that PR).  
</comment><comment author="Mpdreamz" created="2014-11-03T08:40:53Z" id="61450846">@jpountz incorporated your feedback in the PR.
</comment><comment author="jpountz" created="2014-11-03T09:25:19Z" id="61454276">I tried to look at how likely it would be to forget adding support to custom metadata, and I like the fact that you made the metadata a compulsory constructor argument of the aggregators, and that the parsing logic is shared. I think my only concern is that it would be easy to forget to read the metadata in the InternalAggregation objects, so maybe we could try to refactor InternalAggregation so that you could never forget to serialize/deserialize the metadata? I'm thinking about something like:

``` java
class InternalAggregation {

  public final void writeTo(StreamOutput out) { // final so that sub-classes cannot extend
    out.writeGenericValue(metaData);
    doWriteTo(out);
  }

  protected abstract void doWriteTo(StreamOutput out); // this is the method sub-classes should extend instead

}
```

(and similarly for `readFrom`)
</comment><comment author="Mpdreamz" created="2014-11-03T11:49:11Z" id="61467525">++ I briefly discussed this with @spinscale but opted out for now to keep the PR simple, happy to refactor it! Should I also bump writing and reading the `name` to this method?
</comment><comment author="jpountz" created="2014-11-03T11:51:45Z" id="61467761">&gt; Should I also bump writing and reading the name to this method?

That would be great!
</comment><comment author="jpountz" created="2014-11-03T17:35:49Z" id="61515697">I just left two minor comments, but other than that it looks good to me, so feel free to push without further reviews.

Can you update the example request in the description of the PR to replace `2` with a json document since a map is now enforced?
</comment><comment author="javanna" created="2014-11-04T10:10:33Z" id="61618177">I see that this is marked for 1.5 as well but wasn't backported to 1.x. Is there any concern around backwards compatibility? Thoughts @jpountz @Mpdreamz ?
</comment><comment author="jpountz" created="2014-11-04T10:36:23Z" id="61621100">I'd be happy either way: the change has the appropriate version checks to be compatible with 1.x releases. I'm removing the tag until the change is backported (if we backport it).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can't get data when I set the doc_type though it exists.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8278</link><project id="" key="" /><description>``` bash
lnxg33k@ruined-sec /m/D/W/elasticMe (master) $ &gt; curl -XGET "http://localhost:9200/dns_logs/_search?q=*&amp;size=1&amp;pretty"
{
  "took" : 64,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 59897766,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "dns_logs",
      "_type" : "Airways_19Oct2014_raw",
      "_id" : "AUkrekIcDhfVrP07CV-f",
      "_score" : 1.0,
      "_source":{"status": "NOERROR", "indicator": "Snd", "xid": "534c", "raw": "10/14/2014 3:50:31 PM 17E0 PACKET  02D579C0 UDP Snd 172.27.29.33    534c R Q [8081   DR  NOERROR] A     .e7075.x.akamaiedge.net.", "timestamp": "2014-10-14T15:50:31", "proto": "UDP", "ip": "172.27.29.33", "domain": "e7075.x.akamaiedge.net", "thread_id": "17E0", "r_q": "R Q", "flag_hex": "8081", "flag_char_code": "DR", "identifier": "02D579C0", "record": "A"}
    } ]
  }
}
```

As you can see the index called dns_logs that has a doc_type called Airways_19Oct2014_raw

When I try to use the search API with a specific doc_type it returns no hits.

``` bash
lnxg33k@ruined-sec /m/D/W/elasticMe (master) $ &gt; curl -XGET "http://localhost:9200/dns_logs/Airways_19Oct2014_raw/_search?q=*&amp;size=1&amp;pretty"
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

And by providing the doc_type as a request parameter I could get the data

``` bash
lnxg33k@ruined-sec /v/l/elasticsearch $ &gt; curl -XGET "http://localhost:9200/dns_logs/_search?q=_type:Airways_19Oct2014_raw&amp;size=1&amp;pretty"
{
  "took" : 103,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 58378509,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "dns_logs",
      "_type" : "Airways_19Oct2014_raw",
      "_id" : "AUkrekIcDhfVrP07CV-f",
      "_score" : 1.0,
      "_source":{"status": "NOERROR", "indicator": "Snd", "xid": "534c", "raw": "10/14/2014 3:50:31 PM 17E0 PACKET  02D579C0 UDP Snd 172.27.29.33    534c R Q [8081   DR  NOERROR] A     .e7075.x.akamaiedge.net.", "timestamp": "2014-10-14T15:50:31", "proto": "UDP", "ip": "172.27.29.33", "domain": "e7075.x.akamaiedge.net", "thread_id": "17E0", "r_q": "R Q", "flag_hex": "8081", "flag_char_code": "DR", "identifier": "02D579C0", "record": "A"}
    } ]
  }
}
```
</description><key id="47197555">8278</key><summary>Can't get data when I set the doc_type though it exists.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lnxg33k</reporter><labels><label>:Mapping</label><label>bug</label><label>feedback_needed</label></labels><created>2014-10-29T20:12:09Z</created><updated>2015-04-26T19:34:44Z</updated><resolved>2015-04-26T19:34:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-30T10:00:48Z" id="61068393">Please can you provide the output for the following three requests:

```
curl -XGET "http://localhost:9200/dns_logs/_validate/query?q=*&amp;explain"

curl -XGET "http://localhost:9200/dns_logs/_validate/query?q=q=_type:Airways_19Oct2014_raw&amp;explain"

curl -XGET "http://localhost:9200/dns_logs/_mapping?pretty"
```
</comment><comment author="lnxg33k" created="2014-10-30T10:11:19Z" id="61069575">``` bash
lnxg33k@ruined-sec ~/Desktop $ &gt; curl -XGET "http://localhost:9200/dns_logs/_validate/query?q=*&amp;explain"
{"valid":true,"_shards":{"total":1,"successful":1,"failed":0},"explanations":[{"index":"dns_logs","valid":true,"explanation":"ConstantScore(*:*)"}]}⏎ 

lnxg33k@ruined-sec ~/Desktop $ &gt; curl -XGET "http://localhost:9200/dns_logs/_validate/query?q=q=_type:Airways_19Oct2014_raw&amp;explain"
{"valid":true,"_shards":{"total":1,"successful":1,"failed":0},"explanations":[{"index":"dns_logs","valid":true,"explanation":"q=_type:airways_19oct2014_raw"}]}⏎ 
```

Thanks @clintongormley  I think its because it doesn't have a mapping though I did it last day, I think something went wrong when I updated the elasticsearch python module.

``` json
 {
  "dns_logs" : {
    "mappings" : {
      "bbb_raw" : {
        "properties" : {
          "domain" : {
            "type" : "string"
          },
          "flag_char_code" : {
            "type" : "string"
          },
          "flag_hex" : {
            "type" : "string"
          },
          "identifier" : {
            "type" : "string"
          },
          "index_name" : {
            "type" : "string"
          },
          "indicator" : {
            "type" : "string"
          },
          "ip" : {
            "type" : "ip"
          },
          "proto" : {
            "type" : "string"
          },
          "r_q" : {
            "type" : "string"
          },
          "record" : {
            "type" : "string"
          },
          "status" : {
            "type" : "string"
          },
          "thread_id" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "xid" : {
            "type" : "string"
          }
        }
      },
      "lll_raw" : {
        "properties" : {
          "domain" : {
            "type" : "string"
          },
          "flag_char_code" : {
            "type" : "string"
          },
          "flag_hex" : {
            "type" : "string"
          },
          "identifier" : {
            "type" : "string"
          },
          "index_name" : {
            "type" : "string"
          },
          "indicator" : {
            "type" : "string"
          },
          "ip" : {
            "type" : "ip"
          },
          "proto" : {
            "type" : "string"
          },
          "r_q" : {
            "type" : "string"
          },
          "raw" : {
            "type" : "string"
          },
          "record" : {
            "type" : "string"
          },
          "status" : {
            "type" : "string"
          },
          "thread_id" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "xid" : {
            "type" : "string"
          }
        }
      },
      "zzzz_raw" : {
        "properties" : {
          "domain" : {
            "type" : "string"
          },
          "flag_char_code" : {
            "type" : "string"
          },
          "flag_hex" : {
            "type" : "string"
          },
          "identifier" : {
            "type" : "string"
          },
          "index_name" : {
            "type" : "string"
          },
          "indicator" : {
            "type" : "string"
          },
          "ip" : {
            "type" : "ip"
          },
          "proto" : {
            "type" : "string"
          },
          "r_q" : {
            "type" : "string"
          },
          "raw" : {
            "type" : "string"
          },
          "record" : {
            "type" : "string"
          },
          "status" : {
            "type" : "string"
          },
          "thread_id" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "xid" : {
            "type" : "string"
          }
        }
      },
      "xxx_3Sep_raw" : {
        "properties" : {
          "domain" : {
            "type" : "string"
          },
          "flag_char_code" : {
            "type" : "string"
          },
          "flag_hex" : {
            "type" : "string"
          },
          "identifier" : {
            "type" : "string"
          },
          "index_name" : {
            "type" : "string"
          },
          "indicator" : {
            "type" : "string"
          },
          "ip" : {
            "type" : "ip"
          },
          "proto" : {
            "type" : "string"
          },
          "r_q" : {
            "type" : "string"
          },
          "raw" : {
            "type" : "string"
          },
          "record" : {
            "type" : "string"
          },
          "status" : {
            "type" : "string"
          },
          "thread_id" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "xid" : {
            "type" : "string"
          }
        }
      },
      "yyy_24_Sep_raw" : {
        "properties" : {
          "domain" : {
            "type" : "string"
          },
          "flag_char_code" : {
            "type" : "string"
          },
          "flag_hex" : {
            "type" : "string"
          },
          "identifier" : {
            "type" : "string"
          },
          "index_name" : {
            "type" : "string"
          },
          "indicator" : {
            "type" : "string"
          },
          "ip" : {
            "type" : "ip"
          },
          "proto" : {
            "type" : "string"
          },
          "r_q" : {
            "type" : "string"
          },
          "raw" : {
            "type" : "string"
          },
          "record" : {
            "type" : "string"
          },
          "status" : {
            "type" : "string"
          },
          "thread_id" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "xid" : {
            "type" : "string"
          }
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2014-10-30T10:30:10Z" id="61071693">Hmm this is odd - you _have_ a document with that `_type`, but you don't have a mapping for the type.  

Did you delete the mapping at some stage?  Did you experience any node failures, or restart nodes?  It looks like something has managed to get out of sync.

What version of Elasticsearch are you using?
</comment><comment author="lnxg33k" created="2014-10-30T10:39:46Z" id="61072727">Its only a single node and I didn't delete the mapping
Here's my version:

``` bash
lnxg33k@ruined-sec ~/Desktop $ &gt; curl -XGET "http://localhost:9200"
{
  "status" : 200,
  "name" : "Domina",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.4.0.Beta1",
    "build_hash" : "1f25669f3299b0680b266c3acaece43774fb59ae",
    "build_timestamp" : "2014-10-01T14:58:15Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.1"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="clintongormley" created="2014-10-30T11:07:33Z" id="61075711">@lnxg33k Can you provide more background about what you did to get into this state?  I'm concerned that the mapping isn't there, but the document is.
</comment><comment author="lnxg33k" created="2014-10-30T11:11:38Z" id="61076141">I don't exactly know what happened but as far as I remember I used to use an  older version of elasticsearch-py and that happened when I updated it. I am not so sure about that.
</comment><comment author="bleskes" created="2014-10-30T12:54:52Z" id="61086518">@lnxg33k it seems that you have a document in your data which points to a type which is not part of the mapping any more. Did you delete the type by any chance (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-delete-mapping.html#indices-delete-mapping )? That uses delete by query under the hood.
</comment><comment author="bleskes" created="2014-10-30T13:37:55Z" id="61092151">@lnxg33k sorry, I misread you. You already said you didn't delete the mapping. Did you create this type explicitly via the put mapping API/on index creation time or was it created by just indexing into into?
</comment><comment author="lnxg33k" created="2014-10-30T20:28:25Z" id="61163938">@bleskes Yes the type was explicitly created when I pushed the data into my index.  
</comment><comment author="bleskes" created="2014-10-30T20:34:16Z" id="61164784">@lnxg33k thx. double checking - you did not creat the type via the create index API  but rather by indexing into the index. Correct? also, did you restart your node around the time you indexed the very first doc?

This is quite an edge case, so any details you can give may help (custom settings, index settings etc.)
</comment><comment author="clintongormley" created="2015-02-28T04:49:31Z" id="76510514">Hi @lnxg33k 

Any more information about this issue?
</comment><comment author="clintongormley" created="2015-04-26T19:34:43Z" id="96423510">No further info. Closing. Please feel free to reopen if you see this again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch is not correctly applying an updated template to indexes created after the change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8277</link><project id="" key="" /><description>I'm working with an Elasticsearch cluster that handles data from many different sources and am hoping to get it processing Couchbase documents as well. However, once transport-couchbase has been installed on the ES cluster, all indexes in the cluster have this section added to their mappings:

``` json
"couchbaseCheckpoint" : {
  "_source" : {
    "includes" : [
      "meta.*",
      "doc.*"
    ]
  },
  "dynamic_templates" : [
    {
      "store_no_index" : {
        "mapping" : {
          "index" : "no",
          "include_in_all" : false,
          "store" : "no"
        },
        "match" : "*"
      }
    }
  ],
  "properties" : {
    "meta" : {
      "include_in_all" : false,
      "type" : "object"
    }
  }
}
```

To try and fix this, I've changed the first line of the template from:

``` json
"template" : "*",
```

to

``` json
"template" : "couch-*",
```

I edited the template file then used:

```
curl -XPUT http://localhost:9200/_template/couchbase -d @plugins/transport-couchbase/couchbase_template.json
```

I then checked that the template that Elasticsearch had was indeed the edited version using: 

```
curl -XGET localhost:9200/_template/
```

The output showed me the edited version of the template with 

```
"template" : "couch-*",
```

However, this did not prevent the couchbaseCheckpoint section from appearing on the mappings of indexes created after the change. I opened this as an issue for transport-couchbase but was advised to bring it to the attention of the Elasticsearch team.
</description><key id="47177698">8277</key><summary>Elasticsearch is not correctly applying an updated template to indexes created after the change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Elachance</reporter><labels><label>feedback_needed</label></labels><created>2014-10-29T17:25:22Z</created><updated>2014-11-01T14:45:31Z</updated><resolved>2014-11-01T14:45:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-29T17:34:57Z" id="60968605">Hi @Elachance 

I've created a replication of the steps you describe which demonstrates that things work correctly:

```
DELETE /_all
DELETE _template/*

PUT /_template/couchbase
{
  "template": "*",
  "mappings": {
    "couchbaseCheckpoint": {
      "_source": {
        "includes": [
          "meta.*",
          "doc.*"
        ]
      },
      "dynamic_templates": [
        {
          "store_no_index": {
            "mapping": {
              "index": "no",
              "include_in_all": false,
              "store": "no"
            },
            "match": "*"
          }
        }
      ],
      "properties": {
        "meta": {
          "include_in_all": false,
          "type": "object"
        }
      }
    }
  }
}

PUT /foo/bar/1
{}

GET /_mapping

DELETE /foo

PUT /_template/couchbase
{
  "template": "couch-*",
  "mappings": {
    "couchbaseCheckpoint": {
      "_source": {
        "includes": [
          "meta.*",
          "doc.*"
        ]
      },
      "dynamic_templates": [
        {
          "store_no_index": {
            "mapping": {
              "index": "no",
              "include_in_all": false,
              "store": "no"
            },
            "match": "*"
          }
        }
      ],
      "properties": {
        "meta": {
          "include_in_all": false,
          "type": "object"
        }
      }
    }
  }
}

PUT /foo/bar/1
{}

PUT /couch-foo

GET /_mapping
```

I think the likeliest problem is that you missed out on some step above, or that couchbase overwrote your template?  Either way, could you check your steps again and, if still not working, provide a curl recreation which demonstrates the problem?

thanks    
</comment><comment author="Elachance" created="2014-10-29T22:32:40Z" id="61016733">``` json
curl -XDELETE http://localhost:9200/_all?pretty=yes
{
  "acknowledged" : true
}
curl -XDELETE http://localhost:9200/_template/*?pretty=yes
{
  "acknowledged" : true
}
curl -XPUT http://localhost:9200/_template/couchbase?pretty=true -d @couchbase_template.json
{
  "acknowledged" : true
}
curl -XPUT http://localhost:9200/test?pretty=true
{
  "acknowledged" : true
}
curl -XGET http://localhost:9200/_mapping?pretty=true
{
  "test" : {
    "mappings" : {
      "_default_" : {
        "_source" : {
          "includes" : [ "meta.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      },
      "couchbaseCheckpoint" : {
        "dynamic_templates" : [ {
          "store_no_index" : {
            "mapping" : {
              "include_in_all" : false,
              "index" : "no",
              "store" : "no"
            },
            "match" : "*"
          }
        } ],
        "_source" : {
          "includes" : [ "meta.*", "doc.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      }
    }
  }
}
curl -XDELETE http://localhost:9200/test?pretty=yes
{
  "acknowledged" : true
}
vi couchbase_template.json
```

Here is where I changed the template to have "couch-_" instead of "_"

``` json
curl -XPUT http://localhost:9200/_template/couchbase?pretty=true -d @couchbase_template.json
{
  "acknowledged" : true
}
curl -XPUT http://localhost:9200/test?pretty=true
{
  "acknowledged" : true
}
curl -XPUT http://localhost:9200/couch-foo?pretty=true
{
  "acknowledged" : true
}
curl -XGET http://localhost:9200/_mapping?pretty=true
{
  "couch-foo" : {
    "mappings" : {
      "_default_" : {
        "_source" : {
          "includes" : [ "meta.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      },
      "couchbaseCheckpoint" : {
        "dynamic_templates" : [ {
          "store_no_index" : {
            "mapping" : {
              "include_in_all" : false,
              "index" : "no",
              "store" : "no"
            },
            "match" : "*"
          }
        } ],
        "_source" : {
          "includes" : [ "meta.*", "doc.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      }
    }
  },
  "test" : {
    "mappings" : {
      "_default_" : {
        "_source" : {
          "includes" : [ "meta.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      },
      "couchbaseCheckpoint" : {
        "dynamic_templates" : [ {
          "store_no_index" : {
            "mapping" : {
              "include_in_all" : false,
              "index" : "no",
              "store" : "no"
            },
            "match" : "*"
          }
        } ],
        "_source" : {
          "includes" : [ "meta.*", "doc.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      }
    }
  }
}
```

Just to make sure it was using the template I wanted I did this:

``` json
curl -XGET localhost:9200/_template/?pretty=true
{
  "couchbase" : {
    "order" : 10,
    "template" : "couch-*",
    "settings" : { },
    "mappings" : {
      "_default_" : {
        "_source" : {
          "includes" : [ "meta.*" ]
        },
        "properties" : {
          "meta" : {
            "include_in_all" : false,
            "type" : "object"
          }
        }
      },
      "couchbaseCheckpoint" : {
        "_source" : {
          "includes" : [ "doc.*" ]
        },
        "dynamic_templates" : [ {
          "store_no_index" : {
            "mapping" : {
              "include_in_all" : false,
              "index" : "no",
              "store" : "no"
            },
            "match" : "*"
          }
        } ]
      }
    },
    "aliases" : { }
  }
}
```

So the template is right, but the mappings still get messed up. Is there any way the transport-couchbase plugin could be sneaking the original template in somewhere?
</comment><comment author="clintongormley" created="2014-10-30T10:26:07Z" id="61071251">Hmm this is very odd.  What version of Elasticsearch are you using?  I wonder if this is an old bug.

Could you also try deleting all templates before putting your new version:

```
DELETE /_template/*
```
</comment><comment author="Elachance" created="2014-10-30T15:40:18Z" id="61113894">Elasticsearch Version 1.3.0, transport-couchbase version 2.0.0

``` json
curl -XDELETE http://localhost:9200/_template/*?pretty=yes
{
  "acknowledged" : true
}
curl -XGET localhost:9200/_template/?pretty=true
{ }
curl -XGET http://localhost:9200/_mapping?pretty=true
{ }
curl -XPUT http://localhost:9200/_template/couchbase?pretty=true -d @couchbase_template.json
{
  "acknowledged" : true
}
curl -XGET localhost:9200/_template/?pretty=true
{
  "couchbase" : {
    "order" : 10,
    "template" : "couch-*",
    "settings" : { },
    "mappings" : {
      "_default_" : {
        "_source" : {
          "includes" : [ "meta.*" ]
        },
        "properties" : {
          "meta" : {
            "include_in_all" : false,
            "type" : "object"
          }
        }
      },
      "couchbaseCheckpoint" : {
        "_source" : {
          "includes" : [ "doc.*" ]
        },
        "dynamic_templates" : [ {
          "store_no_index" : {
            "mapping" : {
              "include_in_all" : false,
              "index" : "no",
              "store" : "no"
            },
            "match" : "*"
          }
        } ]
      }
    },
    "aliases" : { }
  }
}
curl -XPUT http://localhost:9200/foo?pretty=true
{
  "acknowledged" : true
}
curl -XPUT http://localhost:9200/couch-foo?pretty=true
{
  "acknowledged" : true
}
curl -XGET http://localhost:9200/_mapping?pretty=true
{
  "foo" : {
    "mappings" : {
      "_default_" : {
        "_source" : {
          "includes" : [ "meta.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      },
      "couchbaseCheckpoint" : {
        "dynamic_templates" : [ {
          "store_no_index" : {
            "mapping" : {
              "include_in_all" : false,
              "index" : "no",
              "store" : "no"
            },
            "match" : "*"
          }
        } ],
        "_source" : {
          "includes" : [ "meta.*", "doc.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      }
    }
  },
  "couch-foo" : {
    "mappings" : {
      "_default_" : {
        "_source" : {
          "includes" : [ "meta.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      },
      "couchbaseCheckpoint" : {
        "dynamic_templates" : [ {
          "store_no_index" : {
            "mapping" : {
              "include_in_all" : false,
              "index" : "no",
              "store" : "no"
            },
            "match" : "*"
          }
        } ],
        "_source" : {
          "includes" : [ "meta.*", "doc.*" ]
        },
        "properties" : {
          "meta" : {
            "type" : "object",
            "include_in_all" : false
          }
        }
      }
    }
  }
}
```

No dice :(
</comment><comment author="clintongormley" created="2014-10-31T13:17:00Z" id="61258208">OK  I've tried the steps above with 1.3.0 and I'm not seeing what you're seeing.  Try removing the couchbase plugin and repeating the steps above.  See if it changes things.
</comment><comment author="Elachance" created="2014-10-31T18:48:24Z" id="61310177">Turns out there was an old version of the template in /etc/elasticsearch/templates that was screwing up mappings. All fixed now! Thanks for your help.
</comment><comment author="clintongormley" created="2014-11-01T14:45:31Z" id="61370344">Ah right, thanks for letting us know
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove special file handling from DistributorDirectory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8276</link><project id="" key="" /><description>This commit removes all special file handling from DistributorDirectory
that assigned certain files to the primary directory. This special handling
was added to ensure that files that are written more than once are essentially
overwritten. Yet this implementation is consistent all the time and doesn't need
this special handling for files that are written through this directory. Writes
to the underlying directory not going through the distributor directory are not
and have never been supported.

Note: this commit also fixes the problem of adding directories to the distributor
during restart where the primary can suddenly change and file mappings are by-passed.
</description><key id="47164185">8276</key><summary>Remove special file handling from DistributorDirectory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-29T15:42:51Z</created><updated>2015-06-07T17:37:09Z</updated><resolved>2014-11-04T10:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-29T15:44:34Z" id="60947626">this is a ported version of https://github.com/elasticsearch/elasticsearch/pull/8275 for current master and 1.x. This entire thing came out of the fact that we write now temp files during recovery but the temp filenames will cause them to go into the wrong directories if for instance in lucene 5 a segments_N file is written to a non-primary directory.
</comment><comment author="s1monw" created="2014-10-29T15:44:51Z" id="60947676">@bleskes please take a look at this one
</comment><comment author="bleskes" created="2014-10-30T09:08:16Z" id="61062503">Left some comments
</comment><comment author="s1monw" created="2014-10-30T15:09:42Z" id="61108125">@bleskes I moved away from the actual idea and trashed the usage of the primary in the dist directory entirely except for the lock file. All the mappings from filename to directory are consistent and if you create a temp file you will only be able to rename it in the same directory which is what we want. Additionally it removes the this entire error prone situation where somebody restarts a node and adds a new data path so the primary changes.  I like this much better and in the lucene 5 case it will allow us to use the `Direcotry` interface instead of hardcoding `DistributorDirectory` in `Store`
</comment><comment author="s1monw" created="2014-11-03T14:47:34Z" id="61487645">@bleskes addressed your comments and moved all special casing out of this directory
</comment><comment author="bleskes" created="2014-11-04T08:16:40Z" id="61606529">@s1monw LGTM - left some minor comments but I don't think we need another review cycle. I think we should also change the description in the PR because it has become very different. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/store/DistributorDirectory.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file><file>src/test/java/org/elasticsearch/index/store/DistributorDirectoryTest.java</file><file>src/test/java/org/elasticsearch/index/store/DistributorInTheWildTest.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file><file>src/test/java/org/elasticsearch/indices/recovery/RecoveryStatusTests.java</file></files><comments><comment>[STORE] Remove special file handling from DistributorDirecotry</comment></comments></commit></commits></item><item><title>[STORE] Add dedicated method to write temporary files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8275</link><project id="" key="" /><description>Recovery writes temporary files which might not end up in the
right distributor directories today. This commit adds a dedicated
API that allows specifying the target file name in order to create the
tempoary file in the correct directory.
</description><key id="47161233">8275</key><summary>[STORE] Add dedicated method to write temporary files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-10-29T15:21:32Z</created><updated>2014-10-29T15:46:37Z</updated><resolved>2014-10-29T15:46:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-10-29T15:24:05Z" id="60943480">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"delete_type" is not a valid type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8274</link><project id="" key="" /><description>Greetings

I was writing some tests for a C++ API that I'm working on for my job, and I had created a type that was meant to be deleted, to test the delete functions. So I called this type `delete_type`. Seemed like a natural name. But with any other name the tests of creating some entries in an index, with that type, and then deleting that type from the index fail. Using any other name it works.
</description><key id="47158358">8274</key><summary>"delete_type" is not a valid type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ltworf</reporter><labels><label>feedback_needed</label></labels><created>2014-10-29T14:59:47Z</created><updated>2014-10-30T09:21:23Z</updated><resolved>2014-10-30T09:21:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-29T15:19:46Z" id="60942644">Hi @ltworf 

This works for me:

```
PUT /delete_index/delete_type/1
{}

DELETE /delete_index/delete_type

GET /_search
```

Do you want to provide some details of the actual requests you are sending?
</comment><comment author="ltworf" created="2014-10-30T09:21:23Z" id="61063842">Sorry, there was a mistake from my part that I hadn't found until I sniffed the traffic with wireshark to reproduce the issue.

Sorry again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_msuggest functionality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8273</link><project id="" key="" /><description>I dont know if this is the rigth place for asking such questions. If i am wrong please correct me or move the question elsewhere.
I see functionalities like `bulk` or `_msearch` for combining actions like indexing, deleting, searching and so on.

What about combining more than one `_suggest` request?
</description><key id="47156732">8273</key><summary>_msuggest functionality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">FabianKoestring</reporter><labels><label>:Suggesters</label><label>adoptme</label><label>feature</label></labels><created>2014-10-29T14:47:18Z</created><updated>2016-11-26T11:53:47Z</updated><resolved>2016-11-26T11:53:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-29T15:17:01Z" id="60942062">Hi @FabianKoestring 

You can already do this with suggest as part of an `_msearch` request.  Not sure we need a dedicated `_msuggest` API.  

@martijnvg is the `_suggest` API more efficient than the `_search` API with `search_type=count`?
</comment><comment author="FabianKoestring" created="2014-10-30T07:07:13Z" id="61052999">Thanks @clintongormley for the fast reply.
We tried to get it done over an _msearch request but the results we are getting doesnt make sense.
We just getting the whole index. No suggestions.
</comment><comment author="clintongormley" created="2014-10-30T10:55:39Z" id="61074438">@FabianKoestring It does work:

```
DELETE foo

PUT /foo/bar/1
{
  "text": "one two three"
}

GET /_msearch
{"index": "foo", "search_type": "count"}
{"suggest":{"text":"twos","foo":{"phrase":{"field":"text"}}}}
```
</comment><comment author="FabianKoestring" created="2014-11-03T10:25:51Z" id="61459760">Yea it does! Thx
</comment><comment author="FabianKoestring" created="2014-11-27T15:49:20Z" id="64805832">Should i close this?
</comment><comment author="martijnvg" created="2014-11-28T12:49:08Z" id="64890623">@clintongormley @FabianKoestring Executing suggestions via the _suggest api is more efficient than via the _search api (even when search_type=count). (for example no empty search is executed and no search context is created)

So I think it make sense to add a dedicated _msuggest api at some point.
</comment><comment author="clintongormley" created="2016-11-26T11:53:47Z" id="263059374">The suggest API is deprecated in favour of the search API, so closing this</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Internal: Bind each AllocationDecider as a singleton</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8272</link><project id="" key="" /><description>Previously it's possible to have multiple `AllocationDecider`s created by guice, this ensures only a single instance is created per AllocationDecider.
</description><key id="47139651">8272</key><summary>Internal: Bind each AllocationDecider as a singleton</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>bug</label><label>v1.3.5</label><label>v1.4.0</label></labels><created>2014-10-29T11:58:53Z</created><updated>2014-11-11T12:43:29Z</updated><resolved>2014-10-29T12:24:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-29T12:12:56Z" id="60912759">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change check for finished to a ref count check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8271</link><project id="" key="" /><description>we current check that the recovery is not finished when people access the status local variables. This is wrong and we should check for the refcount being &gt; 0 as it is OK to use the status after it has marked as finished but there are still on going, in-flight reference to it.

This is a follow up on #8092 
</description><key id="47137008">8271</key><summary>Change check for finished to a ref count check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-29T11:27:16Z</created><updated>2015-06-08T00:23:18Z</updated><resolved>2014-10-29T11:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-29T11:28:05Z" id="60907816">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file></files><comments><comment>Recovery: change check for finished to a ref count check</comment></comments></commit></commits></item><item><title>Reroute shards automatically when high disk watermark is exceeded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8270</link><project id="" key="" /><description>This adds a Listener interface to the ClusterInfoService, this is used
by the DiskThresholdDecider, which adds a listener to check for nodes
passing the high watermark. If a node is past the high watermark an
empty reroute is issued so shards can be reallocated if desired.

A reroute will only be issued once every
`cluster.routing.allocation.disk.reroute_interval`, which is "60s" by
default.

Refactors InternalClusterInfoService to delegate the nodes stats and
indices stats gathering into separate methods so they have be overriden
by extending classes. Each stat gathering method returns a
CountDownLatch that can be used to wait until processing for that part
is successful before calling the listeners.

Fixes #8146
</description><key id="47135124">8270</key><summary>Reroute shards automatically when high disk watermark is exceeded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-29T11:07:26Z</created><updated>2015-06-06T19:04:45Z</updated><resolved>2014-10-31T11:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-10-29T12:29:49Z" id="60915501">@s1monw pushed some commits addressing the feedback
</comment><comment author="s1monw" created="2014-10-30T21:08:37Z" id="61169804">left some minor comments - LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inconsistent integer numbers with exponential part</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8269</link><project id="" key="" /><description>```
web245 ~ # curl 'http://web245:9200/statistics-super-fast-201406/_search?pretty' -d '{"size": 0, "aggs": { "filtered": { "filter": { "query": { "query_string": { "query": "_type:events AND @timestamp:2014-06-01" } } }, "aggs": { "sum": { "sum": { "field": "@value" } } } } } }'
{
  "took" : 355,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 38864898,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "filtered" : {
      "doc_count" : 1326965,
      "sum" : {
        "value" : 1.20356041461966131E18
      }
    }
  }
}
```

```
web245 ~ # curl 'http://web245:9200/statistics-super-fast-201406/_search?pretty' -d '{"size": 0, "aggs": { "filtered": { "filter": { "query": { "query_string": { "query": "_type:events AND @timestamp:2014-06-01" } } }, "aggs": { "sum": { "sum": { "field": "@value" } } } } } }'
{
  "took" : 465,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 38864898,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "filtered" : {
      "doc_count" : 1326965,
      "sum" : {
        "value" : 1.20356041461968358E18
      }
    }
  }
}
```

To compare them easily:

`1.20356041461966131E18`
`1.20356041461968358E18`

The index is completely cold, no updates happened between queries. Same happens with facets.

I am doing reindexing with compaction and my checks failed for some indices, this is the reason.

Below is the diff for 180 days (first and last day should be discarded)

``` diff
--- slow.txt    2014-10-29 11:59:19.000000000 +0300
+++ fast.txt    2014-10-29 12:30:19.000000000 +0300
@@ -1,5 +1,4 @@
 "time","_type:events"
-"2014-05-02T03:00:00",148035486204
 "2014-05-03T03:00:00",206274109000
 "2014-05-04T03:00:00",208608806764
 "2014-05-05T03:00:00",217063816673
@@ -26,12 +25,12 @@
 "2014-05-26T03:00:00",226269192968
 "2014-05-27T03:00:00",218775134426
 "2014-05-28T03:00:00",215284624266
-"2014-05-29T03:00:00",1015987720526389800
-"2014-05-30T03:00:00",156505613309490800
+"2014-05-29T03:00:00",1015987720541722500
+"2014-05-30T03:00:00",156505613321714850
 "2014-05-31T03:00:00",12004020735428
-"2014-06-01T03:00:00",1203560414599458800
+"2014-06-01T03:00:00",1203560414619775500
 "2014-06-02T03:00:00",112970958246073
-"2014-06-03T03:00:00",150547467159098050
+"2014-06-03T03:00:00",150547467167122800
 "2014-06-04T03:00:00",201503104830
 "2014-06-05T03:00:00",193058631009
 "2014-06-06T03:00:00",175652820687
@@ -179,4 +178,3 @@
 "2014-10-26T03:00:00",195305183271
 "2014-10-27T03:00:00",205336038145
 "2014-10-28T03:00:00",18642642544668
-"2014-10-29T03:00:00",18489805001076
```

Those days are actually outliers, they have the biggest sums:

```
λ cat slow.txt | awk -F, '{ print $2, $1 }' | sort -n | tail
37077344518816 "2014-09-22T03:00:00"
37079209536035 "2014-10-01T03:00:00"
37082482578643 "2014-10-25T03:00:00"
37086732785585 "2014-10-23T03:00:00"
37087873971618 "2014-09-19T03:00:00"
112970958246073 "2014-06-02T03:00:00"
150547467159098050 "2014-06-03T03:00:00"
156505613309490800 "2014-05-30T03:00:00"
1015987720526389800 "2014-05-29T03:00:00"
1203560414599458800 "2014-06-01T03:00:00"
```

1203560414599458800 log 2 is 60.06201426993902, so there are some bits available :)
</description><key id="47134198">8269</key><summary>Inconsistent integer numbers with exponential part</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bobrik</reporter><labels /><created>2014-10-29T10:57:03Z</created><updated>2014-10-31T18:04:49Z</updated><resolved>2014-10-31T18:04:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-10-29T11:01:38Z" id="60904924">Oh, `preference=_primary` seems to be giving same number all the time. Probably different nodes serialize numbers differently.
</comment><comment author="clintongormley" created="2014-10-29T11:11:32Z" id="60905977">@bobrik did you use dynamic mapping to create these fields?  If so, I think you have inconsistent mappings on different shards.  This is a known problem.
</comment><comment author="bobrik" created="2014-10-29T11:13:41Z" id="60906220">@clintongormley nope, mapping is copied from another index before reindexing documents so mapping for all fields is static.

Can you point me to issue for different mappings on different shards? I'd like to be updated about it too.
</comment><comment author="jpountz" created="2014-10-31T18:04:49Z" id="61302582">We are using doubles for the sum aggregation (so that you don't overflow but just start losing accuracy if your sum is very large).

Doubles have 52 bits of mantissa, so the accuracy loss will start if you need more than 52 bits to store the sum. The accuracy loss on these doubles is going to depend on the order of the operations. And since replicas are allowed to store documents in a different order, doubles will not be added in the same order on all shards. I made a quick Java reproduction to show the issue:

``` java
  private static double sum(List&lt;Double&gt; l) {
    double sum = 0;
    for (double d : l) {
      sum += d;
    }
    return sum;
  }

  public static void main(String[] args) throws IOException {
    Random r = new Random(0);
    List&lt;Double&gt; values = new ArrayList&lt;&gt;();
    for (int i = 0; i &lt; 10; ++i) {
      double d = r.nextDouble() * (1L &lt;&lt; (50 + r.nextInt(12)));
      values.add(d);
    }
    System.out.println(sum(values));
    Collections.shuffle(values);
    System.out.println(sum(values));
  }
```

which prints

```
1.81539321826034048E18
1.81539321826034022E18
```

(see how the last digits differ)

I believe this would explain why you only see this issue on high sums (greater than 2^52) and not when forcing execution on the primary (since iteration order is going to be the same)?

I don't know if there is something open about the issue that Clinton mentioned, but basically dynamic mappings create mappings locally before propagating to the master. So they could end up being different on different nodes based on the documents that they received. And from that point, things can potentially go wild if you have eg. relocations since data can be indexed as longs and interpreted as doubles. We are thinking about making dynamic mappings consult the master before updating the mapping like regular mapping updates and although it would improve safety, it could also make things slower (think about 1000 consecutive documents, each introducing one new field).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: filter cache heap usage should include RAM used by the cache keys (Filter)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8268</link><project id="" key="" /><description>I looked at the heap dump from #8249 and it looks like the majority of the heap is consumed by filter cache keys (TermFilter in that case, where each term was quite large: ~200 bytes long).  There were ~5.5 M such entries, I suspect most of which matching 0 docs.

We don't track cache key RAM usage today in Elasticsearch, because it's tricky.  E.g. Lucene's Filter doesn't implement Accountable, so we can't (easily) ask it how much RAM it's using.  In general there can be sharing between Filters so the RAM used across N filters is less than the sum of each, though I'm not sure Elasticsearch does this. Also, ideally we'd avoid over-counting when the same Filter is cached across N segments.

It's tricky, but I think we need to do something here...

Maybe a separate improvement we could make here is to not bother caching a filter that matched 0 docs?  Such filters are usually (?) fast to re-execute...
</description><key id="47133322">8268</key><summary>Core: filter cache heap usage should include RAM used by the cache keys (Filter)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label></labels><created>2014-10-29T10:47:09Z</created><updated>2015-05-29T17:22:19Z</updated><resolved>2015-05-28T21:23:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-29T11:51:19Z" id="60910336">&gt; Maybe a separate improvement we could make here is to not bother caching a filter that matched 0 docs? Such filters are usually (?) fast to re-execute...

That depends on how much effort it takes to come to that conclusion. Say a geo filter with an arc distance, or cached boolean clause etc.  Maybe we can make it filter dependent like a term filter with 0 hits is not cached but others like Range Terms might still be worth it? I'm not sure we need this complexity... 
</comment><comment author="clintongormley" created="2014-10-29T12:07:08Z" id="60912121">What about just limiting the number of filters that can be added to the cache?
</comment><comment author="jpountz" created="2014-10-29T12:11:46Z" id="60912632">Agreed with Clinton. I think part of the bug is that a filter cache should never reach 5.5M entries: it only makes sense to cache filters if they are going to be reused, otherwise the caching logic is going to add overhead by consuming all documents (as opposed to using skipping in the case of a conjunction) and increasing memory pressure by promoting object to the old gen.

So maybe the fix is to allow to configure an absolute maximum size on the filter cache (with a reasonable value) and to cache filters less aggressively?
</comment><comment author="mikemccand" created="2014-10-29T20:23:13Z" id="60997835">+1 for a simple size limit.  I think Solr (example solrconfig.xml) "defaults" to 512...
</comment><comment author="clintongormley" created="2014-10-30T10:05:01Z" id="61068882">512 seems awful low to me.  Is this number per-segment? So if you have one filter and 20 segments, you'd have 20 entries?  Remember also that we have multiple shards per node.

I think we need to choose a big number, which still allows for a lot of caching, without letting it grow to a ridiculous number like 5 million.  Perhaps 50,000?
</comment><comment author="mikemccand" created="2014-10-30T10:49:04Z" id="61073768">I think for Solr it's per-index, i.e. after 512 cached filters for the index it starts evicting by default.

I agree this may be low, since we now compactly store the sparse cases ... but still caching is "supposed" to be for cases where you expect high re-use of the given filter and the cost to re-generate it is highish.

In general I think Elasticsearch should be less aggressive about filter caching; I suspect in many cases where we are caching, we are not saving that much time vs. letting the OS take that RAM instead and cache the IO pages Lucene will access to recreate that filter.  Running a TermFilter when the IO pages are hot should be quite fast.

Anyway 50K seems OK...
</comment><comment author="clintongormley" created="2014-10-30T11:17:15Z" id="61076745">@mikemccand take this example:

Users are only interested in posts from people that they follow (or even 2 degrees - the people followed by the people that they follow).  eg there are 100,000 user IDs to filter on.

These can all be put into a `terms` query, potentially with a short custom cache key. The first execution takes a bit of time, but the filter remains valid for the rest of the user's session.  This represents a huge saving in execution time.  It is quite feasible that a big website could have 100k user sessions live at the same time.
</comment><comment author="rmuir" created="2014-10-30T12:14:26Z" id="61082288">I think 512 is actually overly large. _way over the top_

IMO: we should only cache filters that:
- are slow (stuff like wildcards and ranges, but not individual terms)
- clear evidence of reuse (e.g. not the first time we have seen the filter)
- being intersected with dense queries (where the risk-reward tradeoff is more clear)

Retrieving every single matching document for a filter to put it into a bitset is extremely slow. Most times, its better to just intersect it on the fly. So this is a huge risk, and we should only do it when the reward is clear: and the reward is tiny unless the criteria above are being met.

I'm not interested in seeing flawed benchmarks around this within current elasticsearch, because it bogusly does this slow way pretty much all the time, even when not caching. So of course mechanisms like caching and bulk bitset intersection will always look like a huge win with the current code. 
</comment><comment author="s1monw" created="2015-05-28T21:06:49Z" id="106599726">@mikemccand what's the status of this issue?
</comment><comment author="rmuir" created="2015-05-28T21:21:49Z" id="106603981">This is bounded to 100k currently in master. I don't much like this number, its ridiculously large, but other changes address the real problems (overcaching). For example only caching on large segments and only when reuse has been noted and so on. 
</comment><comment author="jpountz" created="2015-05-28T21:23:52Z" id="106604388">I don't think this issue is relevant anymore now that we are on the lucene query cache:
- keys are taken into account (the result of ramBytesUsed() if the query implements Accountable and a constant otherwise)
- the cache has a limit on the number of filters that can be added to the cache to limit issues that would be caused by ram usage of keys being underestimated
- also we require that segments have at least 10000 docs for caching so it should be less likely to have cache keys that are much larger than the values than before
</comment><comment author="jpountz" created="2015-05-28T21:29:00Z" id="106605336">Robert, you are right it's 100k currently, nor 10k. I agree we should reduce this number to a more reasonable value.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/filter/weighted/WeightedFilterCache.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file></files><comments><comment>Use a 1024 byte minimum weight for filter cache entries</comment></comments></commit></commits></item><item><title>"random_score", acceptable values for "seed" changed between 1.3 and 1.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8267</link><project id="" key="" /><description>hi there,

we have successfully been using integer representations of javascript Date objects for the "seed" property of the "random_function" scoring function, i.e. output of the following function:

```
function seedval(max_age) 
{
    var ma = max_age || 60000;
    return Math.ceil(+new Date() / ma) * ma;
}
```

which results in numbers like:

&gt; 1414573200000

yesterday a colleague built and installed ES from source using the 1.x branch and stumbled upon the following error (it was definitely referring to the value set for "random_score.seed"!)

```
JsonParseException[Numeric value (1414573200000) out of range of int
```

&gt; NB: I am not looking for a fix as such (we decided to simply divide our generated seed values by 1000 thereby solving our immediate problem) 

I am creating this issue because it just might be something that you would consider to be a regression, I have no idea which of the following might be true ... so by all means close this issue if you consider it to be irrelevant :) 
1. we are running ES as a 32bit process (not the case AFAICT)
2. this is an intentional change/fix
3. this is an unintentional change/bug - possibly related to upgrading (or introducing?) the Jackson lib, from which the parse error seems to originate.

hope this is of some use, thanks for reading!
</description><key id="47125037">8267</key><summary>"random_score", acceptable values for "seed" changed between 1.3 and 1.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">iamjochem</reporter><labels><label>bug</label><label>regression</label><label>v1.4.0</label></labels><created>2014-10-29T09:12:09Z</created><updated>2014-11-03T15:57:49Z</updated><resolved>2014-11-03T15:57:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-29T10:57:50Z" id="60904526">@rjernst could you take a look at this please?  Looks like https://github.com/elasticsearch/elasticsearch/pull/7446/files changed from long to int.
</comment><comment author="jpountz" created="2014-10-31T09:26:36Z" id="61236395">I agree it would be nice to fix it as it is common to provide 64-bits values as seeds (eg. timestamps).

Maybe we could internally translate the 64-bits to 32-bits (since we only use 32-bits in practice) in a `Long.hashCode` fashion? Or maybe we could even allow the seed to be any string and internally use the 32-bits hash of this string as a seed. This could potentially be more user-friendly by allowing users to directly provide things like a session id?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/functionscore/ScoreFunctionBuilders.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/random/RandomScoreFunctionParser.java</file><file>src/test/java/org/elasticsearch/search/functionscore/RandomScoreFunctionTests.java</file></files><comments><comment>FunctionScore: RandomScoreFunction now accepts long, as well a strings.</comment></comments></commit></commits></item><item><title>GetIndexedScript call can deadlock</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8266</link><project id="" key="" /><description>GetIndexedScript can deadlock since they perform blocking operation
on the network thread. This commit moves the blocking to an async operation
</description><key id="47123907">8266</key><summary>GetIndexedScript call can deadlock</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-29T08:56:30Z</created><updated>2015-06-07T18:12:33Z</updated><resolved>2014-10-29T09:09:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-10-29T08:58:14Z" id="60891078">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Decommissioning doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8265</link><project id="" key="" /><description>Hello

I set a up a test cluster, with the following config:
- ES 1.3.4
- One node on Windows (node name Windows)
- One node on Ubuntu - hosted on VMWare player on the same Windows box (node name Ubuntu)

I have 15 indexes.

When I try to decommision a node (either Windows or Ubuntu), nothing happens. I tested with 0 and 1 replicas.

I used the following settings:
curl -XPUT 'localhost:9200/_cluster/settings' -d '{
  "transient" : {
  "cluster.routing.allocation.exclude._name" : "Ubuntu"
}}'

When the command is issued, nothing appears in the logs.

Any idea?
</description><key id="47123285">8265</key><summary>Decommissioning doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geoffroya</reporter><labels /><created>2014-10-29T08:47:23Z</created><updated>2014-10-29T11:04:29Z</updated><resolved>2014-10-29T11:04:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="geoffroya" created="2014-10-29T09:58:26Z" id="60897699">Actually, when there are too many replicas with respect to the number of included nodes, the decommisionning doesn't work.

Is it the expected behaviour?
</comment><comment author="clintongormley" created="2014-10-29T11:04:29Z" id="60905224">Hi @geoffroya 

Yes, this is intentional. It obeys the rules only if it can.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How can I control the shard allocation for the different hardware?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8264</link><project id="" key="" /><description>I have a cluster with 4 \* 24 cores and 2 \* 8 cores, ES allocate to all node evenly,  the 8 cores server will be the bottleneck while stress increase more and more bigger.

How can I keep the 8 cores node allocate shards fewer than others? ie. I have a index with 60 shards, I want others have 12 shards and the 8 cores server just keep 6 shards.
</description><key id="47115276">8264</key><summary>How can I control the shard allocation for the different hardware?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">garbin</reporter><labels /><created>2014-10-29T06:18:39Z</created><updated>2014-10-29T17:20:02Z</updated><resolved>2014-10-29T10:49:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-29T10:49:39Z" id="60903646">Hi @garbin 

Please ask questions like these on the mailing list: http://elasticsearch.org/community
GitHub issues are for bug reports and feature requests.   That said, you can use [shard allocation awareness](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-cluster.html#allocation-awareness) and give your bigger nodes more than one tag value, eg:

```
./bin/elasticsearch --node.rack: rack1, rack2 # big nodes
./bin/elasticsearch --node.rack: rack1  # small nodes
```

then set allocation awareness with:

```
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.awareness.attributes": "rack"
  }
}
```
</comment><comment author="garbin" created="2014-10-29T17:20:02Z" id="60965929">Sorry for my doing...And thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter indices stats for translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8263</link><project id="" key="" /><description>A lot of the indices stats can be filtered (e.g. curl -XGET "http://localhost:9200/example/_stats/suggest"), all of them are documented in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-stats.html. The same does not work for the translog information I would have expected at curl -XGET "http://localhost:9200/example/_stats/translog"

Added the missing call in the RestAction, closes #8262
</description><key id="47113899">8263</key><summary>Filter indices stats for translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels><label>:Stats</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-29T05:47:50Z</created><updated>2015-06-07T18:42:13Z</updated><resolved>2015-03-20T21:43:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-03-20T21:43:29Z" id="84160107">merged thanks!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>translog stats can't be filtered for indices stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8262</link><project id="" key="" /><description>A lot of the indices stats can be filtered (e.g. curl -XGET "http://localhost:9200/example/_stats/suggest"), all of them are documented in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-stats.html. The same does not work for the translog information I would have expected at curl -XGET "http://localhost:9200/example/_stats/translog"

Everything is prepared in the codebase but one piece is missing. I am wondering if this has been removed on purpose?
</description><key id="47113770">8262</key><summary>translog stats can't be filtered for indices stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels /><created>2014-10-29T05:44:33Z</created><updated>2015-03-20T21:43:22Z</updated><resolved>2015-03-20T21:42:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file></files><comments><comment>Filter indices stats for translog</comment></comments></commit></commits></item><item><title>Elasticsearch queries are slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8261</link><project id="" key="" /><description>I have 7 node of cluster. Each having configuration like 16G RAM, 8 Core cpu, centos 6

Heap Memory is - 9000m

1 Master (Non data)
1 Capable master (Non data)
5 Data node

Having 10 indexes, one index is big with 55 million documents of number and 254Gi (508Gi)
size on disk.
Every 1 seconds there are 5-10 new documents indexing.

But problem is search is bit slow. Almost taking average of 2000 to 5000 ms. Some queries are in 1 secs.

Why is that so? 
</description><key id="47074187">8261</key><summary>Elasticsearch queries are slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">appasahebs</reporter><labels /><created>2014-10-28T20:03:12Z</created><updated>2014-10-29T11:02:00Z</updated><resolved>2014-10-29T10:35:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-29T10:35:35Z" id="60902038">Hi @appasahebs 

Please ask questions like these on the mailing list http://elasticsearch.org/community
Github issues are for bug reports and feature requests.

thanks
</comment><comment author="appasahebs" created="2014-10-29T11:02:00Z" id="60904957">ok thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>External multicast discovery does not work for Logstash node clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8260</link><project id="" key="" /><description>To repro:

Set external multicast discovery as described in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#_external_multicast:

discovery:
  zen:
    ping:
      multicast:
        enabled: true
        ping:
          enabled: false
      unicast:
        hosts: ["small1.va.elasticsearch.com", "small2.va.elasticsearch.com"]

Attempt to connect using a Logstash 1.4.2 node client configured for multicast discovery.  This fails.

I have also tested using the perl stub Clint created here: https://github.com/elasticsearch/elasticsearch/issues/1532
Result of the mutlicast query are the same whether discovery.zen.ping.multicast.ping.enabled is true or false.
</description><key id="47060965">8260</key><summary>External multicast discovery does not work for Logstash node clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2014-10-28T18:10:13Z</created><updated>2015-11-21T20:14:36Z</updated><resolved>2015-11-21T20:14:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:14:36Z" id="158678469">Multicast discovery has been removed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better handling of tabs vs spaces in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8259</link><project id="" key="" /><description>Consider the following example, the settings below look fine to the human eye:

![image](https://cloud.githubusercontent.com/assets/7216393/4813647/54a7ee2e-5eca-11e4-94bb-c247a263d72b.png)

But if we show the actual hidden characters, the hosts line is actually using a tab instead of spaces to indent the entry:

```
$
discovery:$
  zen:$
    ping:$
      multicast:$
        enabled: false$
      unicast:$
^Ihosts: ["localhost:9300","localhost:9301"]$
$
```

As a result, ES silently ignores the hosts entry observed in debug logging:

```
[2014-10-28 10:49:01,271][DEBUG][discovery.zen.ping.unicast] [Franz Kafka] using initial hosts [], with concurrent_connects [10]
```

While yaml does not consider tabs valid, it would be nice if there is additional validation implemented in ES to catch this.
</description><key id="47058820">8259</key><summary>Better handling of tabs vs spaces in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-10-28T17:53:02Z</created><updated>2014-11-14T15:40:38Z</updated><resolved>2014-11-14T15:40:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-10-29T09:08:42Z" id="60892212">Now, replace tab with two spaces.
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/settings/loader/YamlSettingsLoader.java#L41-41
</comment><comment author="clintongormley" created="2014-10-29T10:51:54Z" id="60903902">@johtani thanks for spotting that.  that's a bug.  you can't assume two spaces for a tab, because the user might be using 4 spaces etc, which messes up their configuration.

Tabs are illegal in YAML and should throw an error.
</comment><comment author="wwken" created="2014-11-04T22:08:14Z" id="61723832">Yep I agree with @clintongormley . It should throw an exception instead.  
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/settings/loader/YamlSettingsLoader.java</file></files><comments><comment>Configuration: Tab characters in YAML should throw an exception.</comment></comments></commit></commits></item><item><title>Enabled overriding the request headers in the clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8258</link><project id="" key="" /><description>One can set the headers sent with request by the clients by setting the `request.headers` setting. This commit enables overriding any such set headers directly on the requests.
</description><key id="47039472">8258</key><summary>Enabled overriding the request headers in the clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-28T15:24:40Z</created><updated>2015-06-07T10:34:54Z</updated><resolved>2014-10-28T15:39:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-28T15:27:36Z" id="60774222">LGTM.
</comment><comment author="uboness" created="2014-10-28T15:39:57Z" id="60776441">merged in ae1e9edb25be7517b32a21ddd2d5662f99c9ef3f
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix handling of `dangling_timeout` set to 0 and `auto_import_dangled` true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8257</link><project id="" key="" /><description>was set to yes, dangling indices were deleted by mistake,
because a RemoveDanglingIndices runnable was added
to every dangling indices, without considering the auto_import_dangled
setting.
</description><key id="47036067">8257</key><summary>Fix handling of `dangling_timeout` set to 0 and `auto_import_dangled` true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">bogensberger</reporter><labels><label>:Allocation</label><label>bug</label><label>v1.3.6</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-28T14:58:30Z</created><updated>2015-06-07T17:53:33Z</updated><resolved>2014-11-19T15:11:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-28T16:31:52Z" id="60786288">Hi @bogensberger . Thanks for the PR. We'll take a look and get back to you. Meanwhile could I ask you to sign the CLA please: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="bogensberger" created="2014-10-28T17:14:24Z" id="60793866">HI @clintongormley,

Thanks for the reply, i just signed the contributor agreement.
</comment><comment author="bogensberger" created="2014-11-13T09:10:28Z" id="62861782">Hi @clintongormley,

did you have time to take a look at this?
</comment><comment author="colings86" created="2014-11-14T10:27:52Z" id="63038938">@bogensberger I left a small comment, but I think its looking pretty good
</comment><comment author="bogensberger" created="2014-11-18T14:48:42Z" id="63481226">@colings86 i replied to your comment and rebased the current master into the branch, so the branch can be fast forwarded again
</comment><comment author="bogensberger" created="2014-11-18T16:55:01Z" id="63503554">@colings86 good point. I updated the test to use a random timeout, thx
</comment><comment author="colings86" created="2014-11-19T11:51:47Z" id="63628960">@bogensberger thanks for updating, I'll get a final review on this and then we can hopefully get it merged in soon
</comment><comment author="dakrone" created="2014-11-19T14:36:23Z" id="63648446">Left a couple of comments
</comment><comment author="bogensberger" created="2014-11-19T14:51:05Z" id="63650756">@dakrone thx for the comments. I updated the pull request again
</comment><comment author="dakrone" created="2014-11-19T14:53:33Z" id="63651112">LGTM
</comment><comment author="colings86" created="2014-11-19T15:11:58Z" id="63654055">@bogensberger thanks for the PR. I have merged the change in https://github.com/elasticsearch/elasticsearch/commit/69ac8382595deaebb7906d7c9e535896ab9d08e8 in master and also backported the fix to 1.x and 1.4 branches. Not sure why this PR has not autoclosed but will close it now
</comment><comment author="s1monw" created="2014-11-21T09:55:59Z" id="63948441">@colings86 is this also eligible for `1.3.6`? can you backport if so?
</comment><comment author="colings86" created="2014-11-24T09:39:20Z" id="64169977">@s1monw pushed to 1.3 branch in 8473d8dd00b27e73fb105101fcc6b0457203e8f4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Received ping response with no matching id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8256</link><project id="" key="" /><description>I'm seeing a bunch of these on startup:

```
[2014-10-28 14:39:57,956][WARN ][discovery.zen.ping.multicast] [elastic1020] received ping response ping_response{target [[elastic1004][eq_H7EFGQ9SNTJuA6Wj9zw][elastic1004][inet[/10.64.0.111:9300]]{rack=A3, row=A, master=false}], master [[elastic1002][pEwiGcyESvSXFtUUwlh8qw][elastic1002][inet[/10.64.0.109:9300]]{rack=A3, row=A, master=true}], cluster_name[production-search-eqiad]} with no matching id [1]
```

and then everything works just fine.  Is this OK?
</description><key id="47035369">8256</key><summary>Received ping response with no matching id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-10-28T14:53:35Z</created><updated>2014-10-28T19:56:02Z</updated><resolved>2014-10-28T19:56:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-28T19:38:31Z" id="60817886">@nik9000 this can happen when a node takes more then 1.5 seconds to respond to a multicast ping - i.e., after the ping has been timed out locally. Nothing much to worry about if you had nodes under load (and you are indeed using multicast). It can also happen if you have restarted a node in the middle of pinging and it's very quick to come back. The node will receive answers to ping it's predecessor sent but of course, the id is no longer in memory.

Let me know if this explains things so we can close it (or not..).
</comment><comment author="nik9000" created="2014-10-28T19:46:54Z" id="60819177">@bleskes thanks.  That's certainly explains it.  In this case the nodes aren't really under load....  Would a flood of nodes joining cause this?  Like, if I added 10 nodes to my cluster really fast.  Because I did.
</comment><comment author="bleskes" created="2014-10-28T19:50:40Z" id="60819768">@nik9000 if you also took the nodes offline while other nodes were already pinging (for example, you had min master nodes breach) it does - the nodes got responses to the pings they sent before being restarted.
</comment><comment author="nik9000" created="2014-10-28T19:56:02Z" id="60820646">I don't _think_ that is what happened but I've suffered no ill effects from it.  I'll just assume it was a blip of some sort.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make template params take arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8255</link><project id="" key="" /><description>Currently the templateParams in SearchRequest is a `Map&lt;String, String&gt;` - this means that it could not take Arrays as part of the parameters, which is not the case when accessing it through the REST api, and also as documented in the docs for this feature.

This changes it to `Map&lt;String, Object&gt;` so that it can take other data-types.
</description><key id="47032132">8255</key><summary>Make template params take arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">reuben-sutton</reporter><labels><label>:Indexed Scripts/Templates</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-28T14:27:40Z</created><updated>2015-03-20T20:39:33Z</updated><resolved>2014-11-24T14:51:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="reuben-sutton" created="2014-10-28T14:29:03Z" id="60763811">@GaelTadh Can you review please?
</comment><comment author="clintongormley" created="2014-10-28T16:32:42Z" id="60786426">Hi @reuben-sutton. Thanks for the PR.  We'll take a look and get back to you. Meanwhile, could I ask you to sign the CLA please: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="reuben-sutton" created="2014-10-28T16:43:03Z" id="60788329">Thanks @clintongormley - signed.
</comment><comment author="GaelTadh" created="2014-10-28T16:49:27Z" id="60789424">Thanks @reuben-sutton I'll take a look.
</comment><comment author="reuben-sutton" created="2014-11-10T11:31:47Z" id="62372349">Hey @GaelTadh, is there anything I can do to move this forward - would quite like to be able to get rid of the hack in my code for working round it.

Thanks,
Reuben
</comment><comment author="GaelTadh" created="2014-11-10T11:40:51Z" id="62373194">@reuben-sutton I will get to this today or tomorrow at the latest.
</comment><comment author="clintongormley" created="2014-11-10T13:36:39Z" id="62384550">Hi @reuben-sutton - I'm afraid you've signed the CLA with a different email address than the the one you've used on GitHub, so we can't be "sure" that you're the same person :)  Sorry for the ask, but please could you sign again: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="reuben-sutton" created="2014-11-10T13:53:57Z" id="62386472">Hi @clintongormley - Sorry about that, I've signed with my other email too now.

Thanks,
Reuben.
</comment><comment author="clintongormley" created="2014-11-10T14:25:18Z" id="62390446">thanks @reuben-sutton - that worked :)
</comment><comment author="reuben-sutton" created="2014-11-18T14:48:41Z" id="63481222">Hi @GaelTadh, do you mind if I bump this again?
</comment><comment author="GaelTadh" created="2014-11-24T09:50:28Z" id="64171057">I will be looking at this again today
</comment><comment author="GaelTadh" created="2014-11-24T14:33:20Z" id="64201305">This LGTM, @dakrone is going to take a quick look before I merge it in.
</comment><comment author="dakrone" created="2014-11-24T14:42:05Z" id="64202584">LGTM too!
</comment><comment author="GaelTadh" created="2014-11-24T14:51:00Z" id="64203839">This has been merged to master and 1.x
</comment><comment author="lukas-vlcek" created="2014-11-24T14:55:37Z" id="64204511">Great, is this going to be available only from `1.5` or is back-porting to `1.3` or `1.4` planned as well?
</comment><comment author="GaelTadh" created="2014-11-24T15:24:41Z" id="64209099">As it's not a bug fix I haven't ported it to 1.4 or 1.3. @clintongormley what do you think ?
</comment><comment author="s1monw" created="2014-11-24T15:25:04Z" id="64209164">no don't port it
</comment><comment author="reuben-sutton" created="2014-11-24T15:42:02Z" id="64211990">Thanks @GaelTadh!
@s1monw - I think it could be argued that it was a bug that it was possible to do this through the REST API but not the Java API.
</comment><comment author="dadoonet" created="2014-12-02T13:08:01Z" id="65227887">Someone also hit also this issue: https://groups.google.com/d/msgid/elasticsearch/27cafc64-d21b-4fec-8273-3f92ef92ad8f%40googlegroups.com?utm_medium=email&amp;utm_source=footer

I think it's actually a bug in Java API as explained by @reuben-sutton.

I'd be +1 to backport it at least in 1.4
</comment><comment author="prateeka" created="2015-01-16T22:25:03Z" id="70332572">has this been ported to 1.4? else please advise what are my options to use java api for query like below where only query: "XXXX", "YYYY", "ZZZZ" change between different search requests .

GET /cobalt-vehicles/vehicles/_search
{
   "query": {
      "filtered": {
         "query": {
            "multi_match": {
               "query": "XXXX",
               "type": "cross_fields",
               "fields": [
                  "field1",
                  "field2",
                  "field3"
               ]
            }
         },
         "filter": {
            "terms": {
               "owner": [
                  "YYYY",
                  "ZZZZ"
               ]
            }
         }
      }
   }
}
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unicode index names causing DELETE index to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8254</link><project id="" key="" /><description>The test in https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/test/index/10_with_id.yaml uses Unicode characters in the index name: `test-weird-index-中文`.  After running the test, the test runner tries to clear up the created indices with a `DELETE /_all`.

At least on OSX, the name of the index is being changed to `test-weird-index-??`. 

In 1.4, the test succeeds but Elasticsearch logs errors about not being able to delete the index, but DELETE all at least returns.  In 1.x, the DELETE all just hangs.

Both versions leave the `test-weird-index-??` in place.

Relates to #6736 

Logs from 1.x:

```
[2014-10-28 14:07:54,695][INFO ][node                     ] [Turbo] version[1.5.0-SNAPSHOT], pid[60278], build[${build/NA]
[2014-10-28 14:07:54,696][INFO ][node                     ] [Turbo] initializing ...
[2014-10-28 14:07:54,701][INFO ][plugins                  ] [Turbo] loaded [], sites []
[2014-10-28 14:07:57,345][INFO ][node                     ] [Turbo] initialized
[2014-10-28 14:07:57,345][INFO ][node                     ] [Turbo] starting ...
[2014-10-28 14:07:57,457][INFO ][transport                ] [Turbo] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.5.103:9300]}
[2014-10-28 14:07:57,479][INFO ][discovery                ] [Turbo] elasticsearch/aprLOBxNRXKldBazmm8VTg
[2014-10-28 14:08:01,253][INFO ][cluster.service          ] [Turbo] new_master [Turbo][aprLOBxNRXKldBazmm8VTg][Slim.local][inet[/192.168.5.103:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-28 14:08:01,270][INFO ][http                     ] [Turbo] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.5.103:9200]}
[2014-10-28 14:08:01,270][INFO ][node                     ] [Turbo] started
[2014-10-28 14:08:01,283][INFO ][gateway                  ] [Turbo] recovered [0] indices into cluster_state
[2014-10-28 14:10:37,337][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] creating index, cause [auto(index api)], shards [5]/[1], mappings []
[2014-10-28 14:10:37,586][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state
java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-1.st.tmp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)
    at java.nio.channels.FileChannel.open(FileChannel.java:287)
    at java.nio.channels.FileChannel.open(FileChannel.java:334)
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)
    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:10:37,624][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state
java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-1.st.tmp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)
    at java.nio.channels.FileChannel.open(FileChannel.java:287)
    at java.nio.channels.FileChannel.open(FileChannel.java:334)
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)
    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:10:37,633][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state
java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-1.st.tmp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)
    at java.nio.channels.FileChannel.open(FileChannel.java:287)
    at java.nio.channels.FileChannel.open(FileChannel.java:334)
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)
    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:10:37,702][WARN ][index.mapper             ] [Turbo] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name
[2014-10-28 14:10:37,725][WARN ][index.mapper             ] [Turbo] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name
[2014-10-28 14:10:37,727][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] update_mapping [weird.type] (dynamic)
[2014-10-28 14:10:37,732][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state
java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-2.st.tmp
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)
    at java.nio.channels.FileChannel.open(FileChannel.java:287)
    at java.nio.channels.FileChannel.open(FileChannel.java:334)
    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)
    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)
    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:10:37,791][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] deleting index
```

Logs from 1.4:

```
[2014-10-28 14:12:34,362][INFO ][node                     ] [Burstarr] version[1.4.0-SNAPSHOT], pid[60351], build[${build/NA]
[2014-10-28 14:12:34,362][INFO ][node                     ] [Burstarr] initializing ...
[2014-10-28 14:12:34,367][INFO ][plugins                  ] [Burstarr] loaded [], sites []
[2014-10-28 14:12:36,930][INFO ][node                     ] [Burstarr] initialized
[2014-10-28 14:12:36,930][INFO ][node                     ] [Burstarr] starting ...
[2014-10-28 14:12:37,065][INFO ][transport                ] [Burstarr] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.5.103:9300]}
[2014-10-28 14:12:37,080][INFO ][discovery                ] [Burstarr] elasticsearch/M_KcoO3qQCOBp9VuHZ31AA
[2014-10-28 14:12:40,854][INFO ][cluster.service          ] [Burstarr] new_master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-28 14:12:40,869][INFO ][http                     ] [Burstarr] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.5.103:9200]}
[2014-10-28 14:12:40,869][INFO ][node                     ] [Burstarr] started
[2014-10-28 14:12:40,883][INFO ][gateway                  ] [Burstarr] recovered [0] indices into cluster_state
[2014-10-28 14:12:50,301][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] creating index, cause [auto(index api)], shards [5]/[1], mappings []
[2014-10-28 14:12:50,580][INFO ][gateway.local.state.meta ] [Burstarr] [test-weird-index-??] dangling index, exists on local file system, but not in cluster metadata, scheduling to delete in [2h], auto import to cluster state [YES]
[2014-10-28 14:12:50,581][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name
[2014-10-28 14:12:50,611][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name
[2014-10-28 14:12:50,617][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name
[2014-10-28 14:12:50,624][INFO ][gateway.local.state.meta ] [Burstarr] auto importing dangled indices [test-weird-index-??/OPEN] from [[Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]]]
[2014-10-28 14:12:50,682][WARN ][index.mapper             ] [Burstarr] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name
[2014-10-28 14:12:50,709][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][2] shard is locked, releasing lock
[2014-10-28 14:12:50,733][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][3] shard is locked, releasing lock
[2014-10-28 14:12:50,760][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][1] shard is locked, releasing lock
[2014-10-28 14:12:50,782][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][4] shard is locked, releasing lock
[2014-10-28 14:12:50,828][WARN ][index.mapper             ] [Burstarr] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name
[2014-10-28 14:12:50,831][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] update_mapping [weird.type] (dynamic)
[2014-10-28 14:12:50,837][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-??] deleting index
[2014-10-28 14:12:55,730][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][2] Could not lock IndexWriter isLocked [true]
org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock
    at org.apache.lucene.store.Lock.obtain(Lock.java:89)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:753)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,732][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [test-weird-index-??][2] failed to create engine
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:288)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    ... 3 more
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock
    at org.apache.lucene.store.Lock.obtain(Lock.java:89)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:753)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    ... 6 more
[2014-10-28 14:12:55,749][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][3] Could not lock IndexWriter isLocked [true]
org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock
    at org.apache.lucene.store.Lock.obtain(Lock.java:89)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:753)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,750][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [test-weird-index-??][3] failed to create engine
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:288)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    ... 3 more
Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock
    at org.apache.lucene.store.Lock.obtain(Lock.java:89)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:753)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    ... 6 more
[2014-10-28 14:12:55,778][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][1] Could not lock IndexWriter isLocked [true]
org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index/write.lock
    at org.apache.lucene.store.Lock.obtain(Lock.java:89)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:753)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,795][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][4] Could not lock IndexWriter isLocked [true]
org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index/write.lock
    at org.apache.lucene.store.Lock.obtain(Lock.java:89)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:753)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] sending failed shard for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][3] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock]; ]]
[2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] received shard failed for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][3] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock]; ]]
[2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] sending failed shard for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][2] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock]; ]]
[2014-10-28 14:12:55,801][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][2] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock]; ]]
[2014-10-28 14:12:55,806][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] deleting index
[2014-10-28 14:12:55,808][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][0] failed to rollback writer on close
org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index' does not exist
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)
    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)
    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)
    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)
    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)
    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,813][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][2] failed to rollback writer on close
org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index' does not exist
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)
    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)
    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)
    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)
    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)
    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,815][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][1] failed to rollback writer on close
org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index' does not exist
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)
    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)
    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)
    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)
    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)
    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,817][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][4] failed to rollback writer on close
org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index' does not exist
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)
    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)
    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)
    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)
    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)
    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,819][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][3] failed to rollback writer on close
org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index' does not exist
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)
    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)
    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)
    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)
    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)
    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)
    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)
    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2014-10-28 14:12:55,822][INFO ][gateway.local.state.meta ] [Burstarr] auto importing dangled indices [test-weird-index-??/OPEN] from [[Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]]]
[2014-10-28 14:12:55,881][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)
    ... 4 more
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index), type=MERGE, rate=20.0)]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)
    ... 4 more
[2014-10-28 14:12:55,900][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][0] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)
    ... 4 more
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index), type=MERGE, rate=20.0)]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)
    ... 4 more
[2014-10-28 14:12:55,913][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)
    ... 4 more
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index), type=MERGE, rate=20.0)]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)
    ... 4 more
[2014-10-28 14:12:55,929][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][4] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][4] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)
    ... 4 more
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index), type=MERGE, rate=20.0)]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)
    ... 4 more
[2014-10-28 14:12:55,929][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] sending failed shard for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,930][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] received shard failed for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,930][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][0] sending failed shard for [test-weird-index-??][0], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][0] received shard failed for [test-weird-index-??][0], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] sending failed shard for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,932][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] sending failed shard for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,932][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] received shard failed for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,934][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] received shard failed for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]
[2014-10-28 14:12:55,951][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]
[2014-10-28 14:12:55,953][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][1] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)
    ... 4 more
Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index), type=MERGE, rate=20.0)]): files: []
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)
    ... 4 more
[2014-10-28 14:12:55,958][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][1] sending failed shard for [test-weird-index-??][1], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index), type=MERGE, rate=20.0)]): files: []]; ]]
[2014-10-28 14:12:55,958][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][1] received shard failed for [test-weird-index-??][1], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index), type=MERGE, rate=20.0)]): files: []]; ]]
```
</description><key id="47024428">8254</key><summary>Unicode index names causing DELETE index to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2014-10-28T13:18:25Z</created><updated>2014-12-03T07:51:02Z</updated><resolved>2014-12-02T20:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-28T14:16:34Z" id="60761655">This appears to be an artifact of running Elasticsearch in Eclipse.  And the DELETE _all doesn't hang - it times out after 30 sec and returns `{acknowledged: false}`.

Closing
</comment><comment author="uschindler" created="2014-10-28T15:04:52Z" id="60770053">If there would have been an error like this, forbidden-apis should have catched it :-)
</comment><comment author="clintongormley" created="2014-10-28T15:07:57Z" id="60770547">@s1monw has a fix for this in the works
</comment><comment author="s1monw" created="2014-11-12T10:01:14Z" id="62694974">@clintongormley is this still happening?
</comment><comment author="clintongormley" created="2014-11-12T13:14:59Z" id="62715749">@s1monw This still happens in 1.3 and 1.4, in master it is worse.

This request no longer logs an error:

```
curl -XPOST 'http://localhost:9200/test-weird-index-%E4%B8%AD%E6%96%87/weird.type/1?pretty=1' -d '
{
   "foo" : "bar"
}
'
```

However it creates two separate directories, one for the segments and one for the translogs:

```
data/elasticsearch/nodes/0/indices/test-weird-index-??
├── 0
│   ├── index
│   └── translog
│       └── translog-1415797929651
├── 1
│   ├── index
│   └── translog
│       └── translog-1415797929667
├── 2
│   ├── index
│   └── translog
│       └── translog-1415797929681
├── 3
│   ├── index
│   └── translog
│       └── translog-1415797929655
└── 4
    ├── index
    └── translog
        └── translog-1415797929711
data/elasticsearch/nodes/0/indices/test-weird-index-中�\226\207
├── 0
│   ├── _state
│   │   └── state-2.st
│   └── index
│       ├── segments_1
│       └── write.lock
├── 1
│   ├── _state
│   │   └── state-2.st
│   └── index
│       ├── segments_1
│       └── write.lock
├── 2
│   ├── _state
│   │   └── state-2.st
│   └── index
│       ├── segments_1
│       └── write.lock
├── 3
│   ├── _state
│   │   └── state-2.st
│   └── index
│       ├── _0.cfe
│       ├── _0.cfs
│       ├── _0.si
│       ├── segments_1
│       └── write.lock
├── 4
│   ├── _state
│   │   └── state-2.st
│   └── index
│       ├── segments_1
│       └── write.lock
└── _state
    ├── state-1.st
    └── state-2.st
```
</comment><comment author="javanna" created="2014-12-03T07:51:02Z" id="65367229">Hye @s1monw this was closed by a commit that went only to master if I'm not mistaken, but it looks like an actual problem in 1.4 and 1.x too (also 1.3). Is there a way to fix this there as well?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/common/Base64.java</file><file>src/main/java/org/elasticsearch/common/Names.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobStore.java</file><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/common/io/Streams.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file><file>src/main/java/org/elasticsearch/http/HttpServer.java</file><file>src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/store/support/AbstractIndexStore.java</file><file>src/main/java/org/elasticsearch/indices/analysis/HunspellService.java</file><file>src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsService.java</file><file>src/main/java/org/elasticsearch/monitor/fs/JmxFsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/SigarFsProbe.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>src/main/java/org/elasticsearch/repositories/Repository.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/FileChangesListener.java</file><file>src/main/java/org/elasticsearch/watcher/FileWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>src/test/java/org/apache/lucene/util/AbstractRandomizedTest.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java</file><file>src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java</file><file>src/test/java/org/elasticsearch/common/io/FileSystemUtilsTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/store/SimpleDistributorTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>src/test/java/org/elasticsearch/stresstest/fullrestart/FullRestartStressTest.java</file><file>src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java</file><file>src/test/java/org/elasticsearch/watcher/FileWatcherTest.java</file></files><comments><comment>Add File.java to forbidden APIs</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/common/Base64.java</file><file>src/main/java/org/elasticsearch/common/Names.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsBlobStore.java</file><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/common/io/Streams.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/env/Environment.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGateway.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormat.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/LocalGatewayShardsState.java</file><file>src/main/java/org/elasticsearch/http/HttpServer.java</file><file>src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>src/main/java/org/elasticsearch/index/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/index/store/support/AbstractIndexStore.java</file><file>src/main/java/org/elasticsearch/indices/analysis/HunspellService.java</file><file>src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsService.java</file><file>src/main/java/org/elasticsearch/monitor/fs/JmxFsProbe.java</file><file>src/main/java/org/elasticsearch/monitor/fs/SigarFsProbe.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java</file><file>src/main/java/org/elasticsearch/node/service/NodeService.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file><file>src/main/java/org/elasticsearch/repositories/Repository.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/main/java/org/elasticsearch/repositories/fs/FsRepository.java</file><file>src/main/java/org/elasticsearch/script/ScriptService.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/main/java/org/elasticsearch/watcher/AbstractResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/FileChangesListener.java</file><file>src/main/java/org/elasticsearch/watcher/FileWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcher.java</file><file>src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java</file><file>src/test/java/org/apache/lucene/util/AbstractRandomizedTest.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/common/blobstore/BlobStoreTest.java</file><file>src/test/java/org/elasticsearch/common/bytes/PagedBytesReferenceTest.java</file><file>src/test/java/org/elasticsearch/common/io/FileSystemUtilsTests.java</file><file>src/test/java/org/elasticsearch/common/io/StreamsTests.java</file><file>src/test/java/org/elasticsearch/indices/store/IndicesStoreIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/store/SimpleDistributorTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerUnitTests.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file><file>src/test/java/org/elasticsearch/script/ScriptServiceTests.java</file><file>src/test/java/org/elasticsearch/stresstest/fullrestart/FullRestartStressTest.java</file><file>src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java</file><file>src/test/java/org/elasticsearch/watcher/FileWatcherTest.java</file></files><comments><comment>Add File.java to forbidden APIs</comment></comments></commit></commits></item><item><title>Deprecation: MLT Field Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8253</link><project id="" key="" /><description>The MLT field query is simply replaced by a MLT query set to specific field.
To simplify code maintenance we should deprecate it in 1.4 and remove it in
2.0.
</description><key id="47024116">8253</key><summary>Deprecation: MLT Field Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>deprecation</label><label>v1.4.0</label><label>v1.5.0</label></labels><created>2014-10-28T13:15:19Z</created><updated>2015-03-19T16:06:13Z</updated><resolved>2014-10-29T09:49:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-28T19:39:37Z" id="60818065">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make indexQueryParserService available from ParseContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8252</link><project id="" key="" /><description>Closes #8248 

Sorry, new pull request, the previous one had outdated commits in it. Cleaned up my local repo, now its correct.
</description><key id="47015481">8252</key><summary>Make indexQueryParserService available from ParseContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">uschindler</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-28T11:32:21Z</created><updated>2015-06-07T10:25:31Z</updated><resolved>2014-10-28T12:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-10-28T11:34:02Z" id="60742279">LGTM on my end, @s1monw I think its safe to get this to 1.4?
</comment><comment author="s1monw" created="2014-10-28T12:28:08Z" id="60747350">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make ParseContext's IndexQueryParserService accessible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8251</link><project id="" key="" /><description>Closes #8248
</description><key id="47013265">8251</key><summary>Make ParseContext's IndexQueryParserService accessible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels /><created>2014-10-28T11:05:54Z</created><updated>2014-11-03T11:02:49Z</updated><resolved>2014-10-28T11:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-10-28T11:33:35Z" id="60742239">Had to nuke that one because of bullshit commits in it. I HATE GIT, FUCKING GIT... &lt;-- @UweSays
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] initialize SUITE | GLOBAL scope cluster in a private random context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8250</link><project id="" key="" /><description>Today any call to the current randomized context modifies the random
sequence such that cluster initialization is context dependent. If due
to an error for instance a static util method is used like `randomLong`
inside the TestCluster instead of the provided Random instance all
reproducibility guarantees are gone. This commit adds a safe mechanism
to initialize these clusters even if a static helper is used. All none
test scope clusters are now initialized in a private randomized context.
</description><key id="47012842">8250</key><summary>[TEST] initialize SUITE | GLOBAL scope cluster in a private random context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-28T11:00:55Z</created><updated>2014-11-03T11:32:51Z</updated><resolved>2014-10-28T22:54:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-28T13:11:29Z" id="60752244">for reference I opened a PR on randomizedtesting for this https://github.com/carrotsearch/randomizedtesting/pull/177
</comment><comment author="s1monw" created="2014-10-28T13:37:37Z" id="60755678">@javanna @rjernst can you take a look if you have time
</comment><comment author="rjernst" created="2014-10-28T15:41:07Z" id="60776651">LGTM, just one minor comment.
</comment><comment author="s1monw" created="2014-10-28T21:21:32Z" id="60833827">@rjernst I cut over to the randomized runner version and cleaned up stuff a bit more can you take another look... I had to add a sonatype repo for now since it's not yet synced on maven central
</comment><comment author="rjernst" created="2014-10-28T21:26:40Z" id="60834513">Looks good.  The cluster init block for each scope is very parallel now, I like it.
</comment><comment author="rjernst" created="2014-10-28T22:39:12Z" id="60844313">Looks good again!
</comment><comment author="s1monw" created="2014-10-28T23:01:27Z" id="60846877">I didn't push this to 1.3 it's too much of a change
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>1.3.4 heap used growing and crashing ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8249</link><project id="" key="" /><description>I have upgraded from v1.0.0 to 1.3.4 and now see the heap used grow from 60 to 99% over 7 days and crashes ES. When hits 70% heap used see 100% on one CPU at a time. ES is working fine, just appears to be a memory leak or failing to complete GC. Can I safely downgrade to v1.3.1 to see if fixes the problem? Currently have 55M documents, 23GB size.

Thanks, John.
</description><key id="47010643">8249</key><summary>1.3.4 heap used growing and crashing ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johnc10uk</reporter><labels><label>bug</label><label>feedback_needed</label><label>v1.3.5</label><label>v1.4.0</label></labels><created>2014-10-28T10:36:14Z</created><updated>2014-11-03T11:26:33Z</updated><resolved>2014-11-03T11:26:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-28T10:45:38Z" id="60737254">@johnc10uk Few questions:
- what custom settings are you using?
- what is your `ES_HEAP_SIZE`
- do you have swap disabled?

can you send the output of these two requests:

```
curl localhost:9200/_nodes &gt; nodes.json
curl localhost:9200/_nodes/stats?fields=* &gt; nodes_stats.json
```
</comment><comment author="johnc10uk" created="2014-10-28T11:04:32Z" id="60739275">No custom settings, 8.8GB heap and no swap. Again, this was working fine at 1.0.0 .

&lt;script src="https://gist.github.com/johnc10uk/441b58a6181bc07af4c1.js"&gt;&lt;/script&gt;
</comment><comment author="johnc10uk" created="2014-10-28T11:05:36Z" id="60739401">Files https://gist.github.com/johnc10uk/441b58a6181bc07af4c1
</comment><comment author="johnc10uk" created="2014-10-28T11:12:30Z" id="60740092">Tried doubling RAM to 16GB but just lasts longer before filling heap. Upgraded Java from 25 to 55, no difference.

java version "1.7.0_55"
Java(TM) SE Runtime Environment (build 1.7.0_55-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)
</comment><comment author="clintongormley" created="2014-10-28T11:33:19Z" id="60742215">@johnc10uk thanks for the info.  Could I ask you to provide this output as well please:

```
curl localhost:9200/_nodes/hot_threads &gt; hot_threads.txt
```
</comment><comment author="johnc10uk" created="2014-10-28T11:39:21Z" id="60742764">Added to Gist https://gist.github.com/johnc10uk/441b58a6181bc07af4c1
</comment><comment author="clintongormley" created="2014-10-28T11:42:15Z" id="60743050">Thanks. Nothing terribly unusual so far.  Do you have a heap dump left from when one of the nodes crashed?  If not, could you take a heap dump when memory usage is high, and share it with us?

thanks
</comment><comment author="clintongormley" created="2014-10-28T11:43:31Z" id="60743176">Link for heap dump: http://stackoverflow.com/questions/17135721/generating-heap-dumps-java-jre7

Please could you .tar.gz it, and you can share its location with me privately: clinton dot gormley at elasticsearch dot com.

Also, your mappings and config please.
</comment><comment author="johnc10uk" created="2014-10-28T14:26:03Z" id="60763310">Had to add -F to get jmap to work "Unable to open socket file: target process not responding or HotSpot VM not loaded"

jmap has just taken elasticsearch1 out of the cluster and taking forever to dump. Had to kill it, dump only 10Mb. Found out need to run jmap as elasticsearch user.  Emailed link to files on S3.

Thanks, John.
</comment><comment author="johnc10uk" created="2014-10-28T14:39:00Z" id="60765566">Ended up with both elasticsearch1 and 3 server processes being restarted to get all in sync after 1 dropped out. However the heap used on 2 also dropped to about 10% and all now at 40% heap used. Some sort of communication failure not completing jobs?
</comment><comment author="mikemccand" created="2014-10-28T19:27:29Z" id="60816313">I looked at the heap dump here: it's dominated by the filter cache, which for some reason is not clearing itself.  What settings do you have for the filter cache?

It's filled with many (5.5 million) TermFilter instances, and the Term in each of these is typically very large (~200 bytes).
</comment><comment author="clintongormley" created="2014-10-29T09:52:22Z" id="60896985">@johnc10uk when I looked at the stats you sent, filters were only using 0.5GB of memory.  It looks like you are using the default filter cache settings, which defaults to 10% of heap, ie 0.9GB in your case (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-cache.html#filter)

Please could you keep an eye on the output from the following request, and see if the filter cache is growing unbounded:

```
curl -XGET "http://localhost:9200/_nodes/stats/indices/filter_cache?human"
```

In particular, I'd be interested in hearing if the filter cache usage doesn't grow, but memory usage does.
</comment><comment author="johnc10uk" created="2014-10-29T10:09:23Z" id="60898914">Yes, all defaults. The HQ node diagnostics is currently showing 320Mb filter cache and I don't think it ever reached 880Mb to start any evictions. I'll keep a log of filter cache, it is growing.

:{"filter_cache":{"memory_size":"322.3mb","memory_size_in_bytes":337965460,"evictions":0}}}

{"filter_cache":{"memory_size":"325.8mb","memory_size_in_bytes":341677692,"evictions":0}}}  

Thanks, John. 
</comment><comment author="Yakx" created="2014-10-29T12:46:02Z" id="60917477">We are expiriencing this, in production, as well since the upgrade to 1.3.4.
when do you have plans to fix this?
</comment><comment author="johnc10uk" created="2014-10-29T13:13:08Z" id="60920877">We have turned off caching for the TermFilter you identified which should stop the memory creep (it didn't need caching anyway). Restarted each node to clear out heap and will monitor. Thanks for help, very informative. 
</comment><comment author="cregev" created="2014-10-29T14:19:31Z" id="60931255">Hi , 
We have upgraded our Elasticsearch cluster to version 1.3.4, after the upgrade we see that the heap used grow from 60 to 99% in a few hours, when the Heap reaches 99% some of the machines disconnect from the cluster and reconnect again.
In addition , when the Heap used reaches 90% +  it does not return to normal.

I have added your esdiagdump.sh script output , so you could check our cluster stats.
Please advise ?

https://drive.google.com/a/totango.com/file/d/0B17CHQUEl6W8dE15MGVveWlycFk/view?usp=sharing

Thanks,
Costya.
</comment><comment author="clintongormley" created="2014-10-29T15:28:15Z" id="60944299">@CostyaRegev you have high filter eviction rates. I'm guessing that you're also using filters with very long keys which don't match any documents?
</comment><comment author="cregev" created="2014-10-29T16:10:37Z" id="60952682">@clintongormley - as for your question:
It is a test environment of us, and we don't query it much. We usually have less than 1 query per second, and at most 20 filter cache evictions per second. Most of the time, there are 0 evictions per second. And yet, the JVM memory % of some nodes hits the high nineties and never go down. The nodes also crash every here and there. BTW, the CPU usage percentage is not that high, something that might indicate a memory leak of some sort.

Here's our Marvel screenshot - 
![screen shot 2014-10-29 at 6 07 02 pm](https://cloud.githubusercontent.com/assets/7669819/4829258/c62475c0-5f85-11e4-9981-2260e0bc631e.png)
</comment><comment author="clintongormley" created="2014-10-29T16:18:07Z" id="60954302">20 filter evictions per second is a lot - it indicates that you're caching lots of filters that shouldn't be cached.  Also, you didn't answer my question about the length of cache keys.  Are you using filters with, eg, many string IDs in them?
</comment><comment author="cregev" created="2014-10-29T16:32:02Z" id="60956909">Practically, no - not in our test environment. But then again, we have search shard query rate of &lt;0.1 almost always - it's mostly idle. And we have almost no indexing at all all day long (except for a few peaks)... Why is all the memory occupied to the point that nodes just crash?
</comment><comment author="clintongormley" created="2014-10-29T17:25:04Z" id="60966809">@CostyaRegev Please try this to confirm it is the same problem:

```
curl -XPOST 'http://localhost:9200/_cache/clear?filter=true'
```

Within one minute of sending the request, you should see your heap size decrease (although it may take a while longer for a GC to kick in).

If that doesn't work then I suggest taking a heap dump of one of the nodes, and using eg the YourKit profiler to figure out what is taking so much memory.
</comment><comment author="cregev" created="2014-10-29T18:07:59Z" id="60974750">After trying the sent request , indeed i saw our heap size decrease.
It there a workaround for this problem ? When will you fix this problem?
</comment><comment author="s1monw" created="2014-10-29T19:43:17Z" id="60991189">@clintongormley regarding waiting here maybe we should do the same thing we did for FieldData here too in https://github.com/elasticsearch/elasticsearch/commit/65ce5acfb41b4abd0c527aa0e870c2a1076d76cd
</comment><comment author="clintongormley" created="2014-10-30T09:53:02Z" id="61067521">@s1monw I've opened #8285 for that.

@CostyaRegev The problem is as follows:

The filter cache size is the sum of the size of the values in the cache. The size does not take the size of the cache keys into account, because this info is difficult to access.

Previously, cached filters used 1 bit for every document in the segment, so the sum of the size of the values worked as a reasonable heuristic for the total amount of memory consumed.  When the cache filled up with too many values, old filters were evicted.

Now, cached filters which match (almost) all docs or (almost) no docs can be represented in a much more efficient manner, and consume very little memory indeed.  The result is that many more of these filters can be cached before triggering evictions.  

If you have:
- many unique filters
- with long cache keys (eg `terms` filters on thousands of IDs)
- that match almost all or almost no docs

...then you will run into this situation, where the cache keys can consume all of your heap, while the sum of the size of the values in the cache is small.

We are looking at ways to deal with this situation better in #8268.  Either we will figure out a way to include the size of cache keys in the calculation (difficult), or we will just limit the number of entries allowed in the cache (easy).

Workarounds for now:
- Reduce the size of the filter cache so that evictions are triggered earlier (eg set `indices.cache.filter.size` to 2% instead of the default 10%, see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-cache.html#node-filter)
- Use a custom (short) `_cache_key` - see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-terms-filter.html#_terms_lookup_twitter_example
- Disable caching on these filters
- Manually clear the cache once a day or as often as needed (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-clearcache.html#indices-clearcache)
</comment><comment author="johnc10uk" created="2014-10-30T10:01:40Z" id="61068502">@clintongormley as far as getting the size of the cache keys, is that something that could/should be added to Lucene API?
</comment><comment author="clintongormley" created="2014-10-30T11:05:58Z" id="61075560">@johnc10uk no - the caching happens on our side.
</comment><comment author="Yakx" created="2014-11-01T11:45:42Z" id="61365693">ES folks,
 Thanks for the suggestions so far. It helped. However, IMO the problem is much bigger then you think. we have found the following:
1. The ID cache is also having the same problem as Field data and filter cache. we had to stop using all our parent-child queries.  This was not a big loss as this model is practically not usable since it is way too slow.
2. THE MAIN problem is the  cluster management. When a node id crashing after OOM cluster management does not recognize that and then:
3. Calls for the status api return all green
4. Cluster is not using replicas and queries never return 

Our production cluster is crashing every day. The workarounds suggested reduce the number of crashes on our test env and will be implemented in production.
The main problem though, is the cluster recovery.

Let us know what other info/help you need in order to fix.
</comment><comment author="clintongormley" created="2014-11-01T14:56:05Z" id="61370666">@Yakx it sounds like you have a number of configuration issues, which are unrelated to this thread. Please ask these questions in the mailing list instead: http://elasticsearch.org/community
</comment><comment author="clintongormley" created="2014-11-03T11:26:33Z" id="61465527">Closed by #8304
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make ParseContext's IndexQueryParserService accessible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8248</link><project id="" key="" /><description>I have the following problem writing an own query parser from my own package:

My query parser has to internally create another Elasticsaearch query and parse it (which is actually a workaround: I need to create a MoreLikeThis query, but I dont want to replicate all code to generate the native Lucene Query). So I use MoreLikeThisQueryBuilder to create a BytesReference of the newly constructed query, so I can parse it to a Lucene Query.

Of course I cannot do the parseContext.parseInnerQuery, because the newly constructed query is in a separate BytesReference. Instead I do the same trick like WrappedQuery:

```
    final BytesReference querySource = QueryBuilders.moreLikeThisQuery(FIELDNAME_MLT)
      .ids(Integer.toString(id))
      .include(true)
      .percentTermsToMatch(.5f)
      .minTermFreq(0)
      .minDocFreq(5)
      .maxQueryTerms(25)
      .boostTerms(1.0f)
      .buildAsBytes();
    // hack for parsing this as inner query
    try (XContentParser qSourceParser = XContentFactory.xContent(querySource).createParser(querySource)) {
      final QueryParseContext context = cloneParseContext();
      context.reset(qSourceParser);
      return context.parseInnerQuery();
    } catch (IOException ioe) {
      throw new AssertionError("IOException cannot happen here.", ioe);
    }
```

The problem is now: I need to clone the QueryParseContext. WrappedQueryParser just creates a new instance, but it needs the index name (easy to get) and IndexQueryParserService. Unfortunately, the latter is package private.

I would be happy if this could be solved. As this above stuff is used quite often, I think the best solution would be to allow to clone ParseContext or add a method to create a new one from an existion one (copy constructor). The other way would be to make the field accessible.

The current workaround is to use reflection to get the field...
</description><key id="47005239">8248</key><summary>Make ParseContext's IndexQueryParserService accessible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-28T09:34:58Z</created><updated>2014-11-03T11:00:39Z</updated><resolved>2014-10-28T12:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2014-10-28T10:23:03Z" id="60734847">The other "possible" workaround to inject IndexQueryParserService by @Inject into my own query parser does not work (circular dependency).
</comment><comment author="kimchy" created="2014-10-28T10:25:53Z" id="60735178">why not just expose a getter for `indexQueryParser` from the parse context? The fact that its package private is not really needed I think.
</comment><comment author="uschindler" created="2014-10-28T10:37:57Z" id="60736432">@kimchy thats of course the easy thing, as I said :-)

My idea here was to make the common case "parse some other query in context of current ParseContext" easier: The same code is duplicated over and over. Maybe add something like ParseContext#parseQueryInCurrentContext(BytesReference), a setup like parseInnerQuery() - just cloning itsself. By that it could reused in WrapperQueryParser, WrapperFilterParser, and TemplateQueryParser - and for my case.
</comment><comment author="kimchy" created="2014-10-28T10:43:22Z" id="60737032">The repetition is not terrible if you create a new instance of the `QueryParseContext`, but we could have a nice helper method, agreed.

Regardless, I don't see a reason why we can't expose the `indexQueryParser` in the context, which is a very simple change that would help you now (and might get to 1.4), and then look into simplifying the creation of a new instance of QueryParserContext.
</comment><comment author="uschindler" created="2014-10-28T10:44:48Z" id="60737159">I am fine with exposing that! Add a getter and change the field to private final.
</comment><comment author="kimchy" created="2014-10-28T10:45:28Z" id="60737237">@uschindler want to send a pull request?
</comment><comment author="uschindler" created="2014-10-28T10:45:37Z" id="60737252">Can do!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/WrapperQueryParser.java</file></files><comments><comment>Make indexQueryParserService available. Closes #8248</comment></comments></commit></commits></item><item><title>Allow to configure custom thread pools</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8247</link><project id="" key="" /><description /><key id="46929367">8247</key><summary>Allow to configure custom thread pools</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T16:48:42Z</created><updated>2015-06-07T10:25:47Z</updated><resolved>2014-10-31T22:46:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-10-29T09:49:36Z" id="60896638">@martijnvg left a couple of comments
</comment><comment author="martijnvg" created="2014-10-31T13:29:32Z" id="61259649">@colings86 Thanks for looking at this! I updated the PR and applied your feedback.
</comment><comment author="colings86" created="2014-10-31T14:21:35Z" id="61266802">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java</file></files><comments><comment>Core: Allow to configure custom thread pools</comment></comments></commit></commits></item><item><title>or filter issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8246</link><project id="" key="" /><description>I have the following records

``` javascript
{
    "took": 0,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 9,
        "max_score": 1,
        "hits": [
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "V2xbgfWwTzuNyilVGxjElw",
                "_score": 1,
                "_source": {
                    "firstName": "F1",
                    "lastName": "L1",
                    "age": 20
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "fnuqEGNzTHSM8tWsjX2o1w",
                "_score": 1,
                "_source": {
                    "firstName": "F2",
                    "lastName": "L1",
                    "age": 21
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "3TUO53bDQ22rYFcuT-oY3g",
                "_score": 1,
                "_source": {
                    "firstName": "F4",
                    "lastName": "L1",
                    "age": 23
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "h6zZ5WaRQbaSIHanGFpTVg",
                "_score": 1,
                "_source": {
                    "firstName": "F5",
                    "lastName": "L1",
                    "age": 24
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "pOLxQxb_QXSMFG82GocYVQ",
                "_score": 1,
                "_source": {
                    "firstName": "F1",
                    "lastName": "L2",
                    "age": 31
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "4K7kTX32Qui4B3OGOgXahw",
                "_score": 1,
                "_source": {
                    "firstName": "F2",
                    "lastName": "L2",
                    "age": 32
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "HfbbfcBrRQueSTCSCWM1tg",
                "_score": 1,
                "_source": {
                    "firstName": "F3",
                    "lastName": "L2",
                    "age": 33
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "eqQzRvN9SqalZsqZRncb9A",
                "_score": 1,
                "_source": {
                    "firstName": "F3",
                    "lastName": "L1",
                    "age": 22
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "VRuZXRRWSO2PloBY-KUfcA",
                "_score": 1,
                "_source": {
                    "firstName": "F4",
                    "lastName": "L2",
                    "age": 34
                }
            }
        ]
    }
}
```

When i query for all records where the lastName is "L1" and age 20 or lastName is L2 with age 31 using the the query :

``` javascript
{
    "query": {
        "filtered": {
          "filter" : {
                "or": [{
                    "query": {
                        "filtered": {
                            "filter": {
                                "and": [{
                                    "query": {
                                        "match": {
                                            "lastName": "L1"
                                        }
                                    }
                                }, {
                                    "query": {
                                        "match": {
                                            "age": 20
                                        }
                                    }
                                }]
                            }
                        }
                    }
                }, {
                    "query": {
                        "filtered": {
                            "filter": {
                                "and": [{
                                    "query": {
                                        "match": {
                                            "lastName": "L2"
                                        }
                                    }
                                }, {
                                    "query": {
                                        "match": {
                                            "age": 31
                                        }
                                    }
                                }]
                            }
                        }
                    }
                }]
            }
        }
    }
}
```

I get the results correct with the following response :

``` javascript
{
    "took": 1,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 1,
        "hits": [
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "V2xbgfWwTzuNyilVGxjElw",
                "_score": 1,
                "_source": {
                    "firstName": "F1",
                    "lastName": "L1",
                    "age": 20
                }
            },
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "pOLxQxb_QXSMFG82GocYVQ",
                "_score": 1,
                "_source": {
                    "firstName": "F1",
                    "lastName": "L2",
                    "age": 31
                }
            }
        ]
    }
}
```

But the moment i add an "and" filter to the same query where still its applicable for both the records, i get only one record in the response :

``` javascript
{
    "query": {
        "filtered": {
            "filter": {
                "and": [{
                    "query": {
                        "terms": {
                            "firstName": [
                                "f1"
                            ]
                        }
                    }
                }],
                "or": [{
                    "query": {
                        "filtered": {
                            "filter": {
                                "and": [{
                                    "query": {
                                        "match": {
                                            "lastName": "L1"
                                        }
                                    }
                                }, {
                                    "query": {
                                        "match": {
                                            "age": 20
                                        }
                                    }
                                }]
                            }
                        }
                    }
                }, {
                    "query": {
                        "filtered": {
                            "filter": {
                                "and": [{
                                    "query": {
                                        "match": {
                                            "lastName": "L2"
                                        }
                                    }
                                }, {
                                    "query": {
                                        "match": {
                                            "age": 31
                                        }
                                    }
                                }]
                            }
                        }
                    }
                }]
            }
        }
    }
}
```

Response :

``` javascript
{
    "took": 0,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 1,
        "max_score": 1,
        "hits": [
            {
                "_index": "testindex",
                "_type": "collection1",
                "_id": "pOLxQxb_QXSMFG82GocYVQ",
                "_score": 1,
                "_source": {
                    "firstName": "F1",
                    "lastName": "L2",
                    "age": 31
                }
            }
        ]
    }
}
```

It appears to be clearly an issue with the OR filter. I have other cases where OR also fails. But this appears to give a picture about what is going on.

I don't know exactly if there is any issue with how i am using the filter/query. If there is an alternative approach where it would work well, i would appreciate the help.
Let me know if you need any more information as well.
Thanks.
</description><key id="46925364">8246</key><summary>or filter issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">gitfy</reporter><labels /><created>2014-10-27T16:17:01Z</created><updated>2014-10-28T16:08:03Z</updated><resolved>2014-10-28T10:21:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-28T00:22:36Z" id="60692423">I think the issue here is that elasticsearch does not expect having an `and` and an `or` filter on the same level:

``` json
  "filter": {
    "and": [...],
    "or": [...]
  }
```

This is probably confusing the parser, which generates the wrong query (which is a bug as it should raise an error instead)? The query should rather be:

``` json
  "filter": {
    "and": [
      ...,
      {
        "or": [...]
      }
    ]
  }
```
</comment><comment author="clintongormley" created="2014-10-28T10:21:28Z" id="60734679">Sounds like @jpountz nailed it. Closing
</comment><comment author="gitfy" created="2014-10-28T16:08:03Z" id="60781851">Yes, moving the or inside the "and" returns the correct results.

Thanks Adrien.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add current translog ID to commit meta before closing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8245</link><project id="" key="" /><description>Today we simply close the IW when we have to open a new one if ie.
we change merge settings. Yet, the API is deprecated and under the hood
it commits the IW. For safety we should add the current translog ID and
use rollback instead to not wait for merges.
</description><key id="46924299">8245</key><summary>Add current translog ID to commit meta before closing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T16:09:01Z</created><updated>2015-06-07T18:08:05Z</updated><resolved>2014-10-28T20:44:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-27T16:38:15Z" id="60624306">LGTM.
</comment><comment author="s1monw" created="2014-10-28T20:41:17Z" id="60827564">@dakrone I pushed another commit that fixes the map builder too
</comment><comment author="dakrone" created="2014-10-28T20:42:34Z" id="60827784">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce elasticsearch.in.bat (i.e. es.in for Windows)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8244</link><project id="" key="" /><description>Break-out common functionality between elasticsearch.bat and service.bat
Relates #8237
</description><key id="46922476">8244</key><summary>Introduce elasticsearch.in.bat (i.e. es.in for Windows)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T15:55:20Z</created><updated>2015-06-07T16:49:30Z</updated><resolved>2014-10-28T14:40:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-10-27T15:58:09Z" id="60617311">LGTM, can't verify it on windows now, maybe someone else can?
</comment><comment author="dadoonet" created="2014-10-27T16:27:42Z" id="60622502">Wondering if we should also change `plugin.bat` to have a single place where we can define some settings.

For example, if someone want to set path.plugins in elasticsearch.in.bat, it should be taken into account by the `plugin.bat` command as well.
Though that about memory settings, it does not make sense to start the plugin manager with 4gb RAM is the user set ES_HEAP to 4g!
</comment><comment author="gmarz" created="2014-10-27T16:56:33Z" id="60627419">@costin

Just tested this on my machine and found the following issues with `service.bat`:

**(1)** If `service.bat` is ran from outside of the `bin` directory, then it fails to install:

```
PS C:\elasticsearch\elasticsearch-1.4.0.Beta1&gt; .\bin\service.bat install
Installing service      :  "elasticsearch-service-x64"
Using JAVA_HOME (64-bit):  "C:\Program Files\Java\jdk1.7.0_71"
'C:\elasticsearch\elasticsearch-1.4.0.Beta1\elasticsearch.in.bat' is not recognized as an internal or external command,
operable program or batch file.
The service 'elasticsearch-service-x64' has been installed.
```

The cause of this is because `%~dp0` resolves to `C:\elasticsearch\elasticsearch-1.4.0.Beta1` instead of the script directory as you would expect: `C:\elasticsearch\elasticsearch-1.4.0.Beta1\bin`.  I'm confused as to why this is the case though; it works as expected in `elasticsearch.bat`.

**(2)** The service name is installed as `Elasticsearch ${project.version}` instead of `Elasticsearch 1.4.0.Beta`. 
</comment><comment author="s1monw" created="2014-10-28T05:59:16Z" id="60713330">I think this should go into 1.4 as well
</comment><comment author="costin" created="2014-10-28T10:06:38Z" id="60733027">#8243 or some parts of it should go in 1.3 as it provides `ES_USE_IPV4` for Windows files. The common `elasticsearch.in.bat` could be left for 1.4 but the option needs to be added in.
</comment><comment author="clintongormley" created="2014-10-28T10:17:16Z" id="60734209">@Mpdreamz could you review this one too please?
</comment><comment author="Mpdreamz" created="2014-10-28T12:53:10Z" id="60750065">LGTM :+1: @gmarz's issues seem to have all been resolved. 

Passing parameters such as `-Des.cluster.name=x` still work as well

`elasticsearch` `plugin` `service` seem to work properly no matter from where they are called.
</comment><comment author="costin" created="2014-10-28T14:40:18Z" id="60765784">Rebased as is into 1.4, 1.x and master. Added only `ES_USE_IPV4` setting to 1.3 to avoid introducing `elasticsearch.in.bat` (a potentially large change) to the 1.3.x series.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve handling of multicast binding exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8243</link><project id="" key="" /><description>In case the multicast binding fails (due to socket errors), abort zen
pinging code and throw a better error message
Relates #8225
</description><key id="46922350">8243</key><summary>Improve handling of multicast binding exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T15:54:28Z</created><updated>2015-06-06T19:26:42Z</updated><resolved>2014-10-28T15:01:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-10-27T20:43:15Z" id="60665718">LGTM, @s1monw / @clintongormley  I think its worth considering getting this into 1.4 as well, considering #8225.
</comment><comment author="costin" created="2014-10-27T20:50:01Z" id="60666749">Same goes for #8244 - which adds `ES_USE_IPV4` variable in Windows `.bat` files.
</comment><comment author="s1monw" created="2014-10-28T05:58:36Z" id="60713272">+1 to add to 1.4 and even 1.3?
</comment><comment author="costin" created="2014-10-28T15:01:46Z" id="60769537">Pushed to 1.3, 1.4, 1.x and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Received a join request for an existing node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8242</link><project id="" key="" /><description>We had an outage this morning caused by 10/18 of our nodes being unable to clear any of their old generation.  The cause of this isn't yet clear to me so this bug isn't about that.  After restarting the full nodes I believe the cluster handshaking code had some trouble:

```
[2014-10-27 14:50:49,000][WARN ][discovery.zen            ] [elastic1002] received join request from node [[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false}], but found existing node [elastic1017][ua7AQNuDQfu78lHZUz8G6A][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false} with same address, removing existing node
[2014-10-27 14:50:49,000][INFO ][cluster.service          ] [elastic1002] removed {[elastic1017][ua7AQNuDQfu78lHZUz8G6A][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false},}, added {[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false},}, reason: zen-disco-receive(join from node[[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false}])
...
[2014-10-27 14:50:52,047][INFO ][discovery.zen            ] [elastic1002] received a join request for an existing node [[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false}]
```

In the end I had to restart the cluster master to get it to seed control to another node.  The new node worked as expected and added the other nodes back.
This is Elasticsearch 1.3.4.  I know 1.4 contains lots of work on the zen code.  Any chance this is something it'd handle?
</description><key id="46920922">8242</key><summary>Received a join request for an existing node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-10-27T15:42:57Z</created><updated>2015-11-21T20:14:06Z</updated><resolved>2015-11-21T20:14:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-28T08:47:47Z" id="60724766">Hi @nik9000 ,

I need more info to understand what happened:

&gt; clear any of their old generation

What do you mean exactly?

&gt; After restarting the full nodes I believe the cluster handshaking code had some trouble:

Do you mean you restarted the faulty nodes (10 of the 18) or all the nodes? 

&gt; The new node worked as expected and added the other nodes back.

What went wrong before?
</comment><comment author="nik9000" created="2014-11-21T16:34:23Z" id="63996779">Sorry for taking so long to get back to you on this.  I'll reply inline:

&gt; Hi @nik9000 ,
&gt; 
&gt; I need more info to understand what happened:
&gt; 
&gt; ```
&gt; clear any of their old generation
&gt; ```
&gt; 
&gt; What do you mean exactly?

It was https://issues.apache.org/jira/browse/LUCENE-6046 actually.  You can ignore the actual cause of the outage I think, my real question is about the weird "received a join request for an existing node" message.

&gt; ```
&gt; After restarting the full nodes I believe the cluster handshaking code had some trouble:
&gt; ```
&gt; 
&gt; Do you mean you restarted the faulty nodes (10 of the 18) or all the nodes?

Sorry, yeah, the whole cluster.  All 31 nodes.

&gt; ```
&gt; The new node worked as expected and added the other nodes back.
&gt; ```
&gt; 
&gt; What went wrong before?

The new nodes would just sit there claiming not to have a master connection.  Most http requests would time out as they were blocked on getting a master connection and the logs would mention sending requests to the master every once in a while.  The master logged what I quoted in the first post.  Restarting the master allowed the node to join the cluster.
</comment><comment author="clintongormley" created="2015-11-21T20:14:06Z" id="158678443">This reminds me of the old zombie nodes problem, that was solved a long time ago.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MLT Query: use ParseField#withAllDeprecated for percent_terms_to_match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8241</link><project id="" key="" /><description>Also the parameter was deprecated but not removed so we keep it in the doc and
mark it as deprecated ...
</description><key id="46917655">8241</key><summary>MLT Query: use ParseField#withAllDeprecated for percent_terms_to_match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>docs</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T15:16:21Z</created><updated>2014-10-27T17:01:31Z</updated><resolved>2014-10-27T16:52:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-27T15:26:42Z" id="60611607">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file></files><comments><comment>MLT Query: use ParseField#withAllDeprecated for percent_terms_to_match</comment></comments></commit></commits></item><item><title>Throw exception if `index` is null or missing when creating an alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8240</link><project id="" key="" /><description>Fixes a bug where alias creation would allow `null` for index name, which thereby applied the alias to _all_ indices.  This patch makes the validator throw an exception if the index is null.

Java integration tests and YAML REST tests had to be modified, since they were testing this bug as if it were a real feature.

This is a **breaking** change, if someone was accidentally relying on this bug.

Fixes #7863, related to old PR #7976

``` bash
POST /_aliases
{
   "actions": [
      {
         "add": {
            "alias": "empty-alias",
            "index": null
         }
      }
   ]
}
```

``` json
{
   "error": "ActionRequestValidationException[Validation Failed: 1: Alias action [add]: [index] may not be null;]",
   "status": 400
}
```
</description><key id="46917325">8240</key><summary>Throw exception if `index` is null or missing when creating an alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Aliases</label><label>breaking</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T15:14:14Z</created><updated>2015-06-06T15:35:04Z</updated><resolved>2014-10-28T19:48:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-27T16:03:12Z" id="60618229">LGTM.  I left one minor comment.
</comment><comment author="polyfractal" created="2014-10-28T19:48:02Z" id="60819345">This was merged in f5b2dfd052280bc397612bcb35c29333a76c650e
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Updated paths to be inline with #8240</comment></comments></commit></commits></item><item><title>Don't use negative ramBytesUsed() values.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8239</link><project id="" key="" /><description>The accountable interface specifies that such values are illegal. When extending this interface to give more detailed information in https://issues.apache.org/jira/browse/LUCENE-5949, I know I added more checks and so on around this. 

We should fix the places in e.g. fielddata returning -1.
</description><key id="46912425">8239</key><summary>Don't use negative ramBytesUsed() values.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label></labels><created>2014-10-27T14:37:02Z</created><updated>2014-11-03T16:12:30Z</updated><resolved>2014-11-03T16:12:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-10-30T16:19:54Z" id="61120937">These changes look great.

FYI there are more places returning -1 "indirectly". For example, AtomicLongFieldData takes the ram usage in its constructor, and some places (e.g. docvalues impls of it) pass -1 there:

```
AtomicLongFieldData(long ramBytesUsed) {
  this.ramBytesUsed = ramBytesUsed;
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BinaryDVNumericIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/BytesBinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/NumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedNumericDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/SortedSetDVBytesAtomicFieldData.java</file></files><comments><comment>Return 0 instead of -1 for unknown/non-exposed ramBytesUsed()</comment></comments></commit></commits></item><item><title>Remove MLT Field Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8238</link><project id="" key="" /><description>The MLT field query is simply replaced by a MLT query set to specific field.
To simplify code maintenance we should deprecate it in 1.4 and remove it in
2.0.
</description><key id="46910397">8238</key><summary>Remove MLT Field Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T14:20:57Z</created><updated>2015-06-08T14:32:39Z</updated><resolved>2014-10-29T09:30:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-27T17:39:03Z" id="60635615">Is the plan to also remove it in master? (2.0)
</comment><comment author="clintongormley" created="2014-10-28T09:57:13Z" id="60731982">Please also add it to the breaking changes docs for 2.0 in master.
</comment><comment author="alexksikes" created="2014-10-28T10:15:52Z" id="60734050">Should we just deprecate it and not remove it? I don't really have an opinion on it. Thoughts?
</comment><comment author="clintongormley" created="2014-10-28T10:35:34Z" id="60736174">Given that there is a simple workaround (ie use the `mlt` query) I think it is ok to remove it in master directly.
</comment><comment author="s1monw" created="2014-10-28T10:54:08Z" id="60738205">++ to remove
</comment><comment author="martijnvg" created="2014-10-28T19:39:20Z" id="60818009">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisFieldQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryBuilders.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/mlt/MoreLikeThisActionTests.java</file></files><comments><comment>MLT Field Query: remove it from master</comment></comments></commit></commits></item><item><title>Improve </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8237</link><project id="" key="" /><description>Currently `service.bat` and `elasticsearch.bat` duplicate the code required to trigger the variable and classpath initialization on windows. In a similar vein to `elasticsearch.in.sh`, a common file between the two would simplify the maintenance of the two going forward.
</description><key id="46908608">8237</key><summary>Improve </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>:Packaging</label><label>build</label><label>low hanging fruit</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-27T14:07:39Z</created><updated>2014-10-28T14:41:04Z</updated><resolved>2014-10-28T14:41:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-10-28T14:41:04Z" id="60765903">Fixed in 1.3, 1.4, 1.x and master. See #8244 PR for more information.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Introduce elasticsearch.in.bat (i.e. es.in for Windows)</comment></comments></commit></commits></item><item><title>Sorting: Use first non-empty result to detect if sort is required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8236</link><project id="" key="" /><description>Closes #8226
</description><key id="46896469">8236</key><summary>Sorting: Use first non-empty result to detect if sort is required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.3.5</label></labels><created>2014-10-27T11:51:43Z</created><updated>2014-11-03T15:23:34Z</updated><resolved>2014-10-27T12:21:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-27T11:53:24Z" id="60581590">I marked this as `1.3.5` since the test for it doesn't fail on the other branches. Yet, I think the difference here is only in terms of parsing and I wonder if we should add the new method in master too just to make sure we don't depend on shards putting in the right TopDocs instance even if results are emtpy
</comment><comment author="jpountz" created="2014-10-27T12:04:27Z" id="60582675">LGTM
</comment><comment author="s1monw" created="2014-10-27T12:21:58Z" id="60584460">pushed to `1.3`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Compound words filter operator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8235</link><project id="" key="" /><description># Compound words
### Ranking and operator type issue

Given the keyword "shoerack" was misspelled into one word, then I would expect it to be split into:

```
"shoerack", "shoe" and "rack"
```

Using this compound word list:

```
["shoe", "rack"]
```

This all works fine. Testing the analyzer returns the above as expected. When I continue to perform the search after this the keywords appear to be searched this way, when I have the operator set on the multi_match query to be AND:

```
"shoerack" || "shoe" || "rack"
```

I would like these keywords to act as they are searched for this way:

```
"shoerack" || ("shoe" &amp;&amp; "rack")
```

I would like the second argument to be ranked separately

I'm looking for a way to set up an operator in the filter itself to be AND, or maybe it isn't aware at all that the two or more keywords belong together.

I think this might be a feature request or enhancement, however it might be doable already, just that I have not found a solution to it.

Thanks
/Marcus
</description><key id="46888354">8235</key><summary>Compound words filter operator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MarcusSjolin</reporter><labels><label>:Analysis</label><label>discuss</label></labels><created>2014-10-27T10:10:38Z</created><updated>2017-05-26T17:11:12Z</updated><resolved>2016-11-26T11:53:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vineelyalamarthy" created="2015-06-18T20:32:49Z" id="113281415">@clintongormley  How can I assign this bug to myself. This sounds interesting.
</comment><comment author="clintongormley" created="2015-06-19T07:59:45Z" id="113420277">@vineelyalamarthy no need to assign it.  If you see a PR you're interested in working on, you can just go ahead and submit a PR. Although I'd advise discussing the approach in the issue before writing any code.

Not sure this is a good one to start on though, it's more complicated than it looks. I've marked it discuss because I don't think there is an easy way to do this.  Take a look at this example:

```
PUT t
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "compound_words"
          ]
        }
      },
      "filter": {
        "compound_words": {
          "type": "dictionary_decompounder",
          "word_list": [
            "shoe",
            "rack"
          ]
        }
      }
    }
  }
}

GET t/_analyze?analyzer=my_analyzer&amp;text=My shoerack
```

returns

```
{
   "tokens": [
      {
         "token": "my",
         "start_offset": 0,
         "end_offset": 2,
         "type": "&lt;ALPHANUM&gt;",
         "position": 1
      },
      {
         "token": "shoerack",
         "start_offset": 3,
         "end_offset": 11,
         "type": "&lt;ALPHANUM&gt;",
         "position": 2
      },
      {
         "token": "shoe",
         "start_offset": 3,
         "end_offset": 11,
         "type": "&lt;ALPHANUM&gt;",
         "position": 2
      },
      {
         "token": "rack",
         "start_offset": 3,
         "end_offset": 11,
         "type": "&lt;ALPHANUM&gt;",
         "position": 2
      }
   ]
}
```

So `shoerack`, `shoe`, and `rack` are stacked in the same position.  This is the same thing that happens with synonyms (but if you're expanding synonyms at query time, you don't want to enforce matching ALL alternatives).

Queries like the `match` query are constructed by first analyzing the query string to produce a token stream (like the above) then generating low level `term` queries, wrapped in a `bool` query, for each token. There is no way to distinguish between stacked tokens that have been generated by a compound filter and tokens that have been generated by a synonym filter.

To make this change possible, the query generation process would need to understand the analysis chain... that's a big change!
</comment><comment author="clintongormley" created="2016-11-26T11:53:06Z" id="263059341">As of 5.0, this now works correctly:

```
PUT t
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "compound_words"
          ]
        }
      },
      "filter": {
        "compound_words": {
          "type": "dictionary_decompounder",
          "word_list": [
            "shoe",
            "rack"
          ]
        }
      }
    }
  },
  "mappings": {
    "t": {
      "properties": {
        "text": {
          "type": "text",
          "analyzer": "my_analyzer"
        }
      }
    }
  }
}


GET t/_validate/query?explain
{
  "query": {
    "match": {
      "text": {
        "query": "my shoerack",
        "operator": "and"
      }
    }
  }
}
```

The explanation returned by the above is:

    +text:my +Synonym(text:rack text:shoe text:shoerack)</comment><comment author="gbhatia81" created="2017-04-29T10:31:57Z" id="298160923">I am not sure if it is still working as expected. As explained in _validate query, lucene is still trying to search for following
"my" AND ("rack" OR " shoe" OR "shoerack")
What was originally needed and is still a requirement in my case is a way to do following
"my" AND ("shoerack" OR ("shoe" AND "rack"))

This should be a common use case for others as well. When just "Shoerack" is queried , system is going and finding all results that even contain just shoe or just rack, which in my opinion, not good. There should be a way to force AND query for decompounded words.

@clintongormley  - i am not sure if issue should be closed.

</comment><comment author="clintongormley" created="2017-05-26T17:11:12Z" id="304338115">@gbhatia81 the point of decompounding words is to look for some of the sub-words within a compound word.  In a language that uses compound words, you wouldn't have `shoerack` or `shoe rack`, just `shoerack`.  </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>issus with Terms Facet  count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8234</link><project id="" key="" /><description>Hi am trying to build the facet Builder with my multi match query.
in the response am getting facet count was more then the Total matched Records. 
please help me to resolve.
My Query
"query" : "a",
"fields" : [ "Tag1^10.0", "Tag2^5.0", "Tag3^10.0", "Tag4^0.0", "Tag5^0.0" ]
   }
 }, "filters" : [ {
        "and" : {
          "filters" : [ {
            "query" : {
              "match" : {
                "suppInd" : {
                  "query" : "N",
                  "type" : "boolean"
                }
              }
            }
          },
        }]
    }]
"facets" : {
   "tag2" : {
        "terms" : {
        "field" : "tag2",
        "size" : 10
                }
            }
    } 
</description><key id="46886565">8234</key><summary>issus with Terms Facet  count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajaw90</reporter><labels /><created>2014-10-27T09:50:29Z</created><updated>2014-10-27T10:02:02Z</updated><resolved>2014-10-27T10:02:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-27T10:02:02Z" id="60570607">@rajaw90 facets are deprecated. Use aggregations instead.  You have multiple values per field, and each value is being counted.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove usage of Directory#fileExists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8233</link><project id="" key="" /><description>This commit removes the usage of #fileExists which has been
deprecated long ago an can be a source of race conditions.
</description><key id="46853539">8233</key><summary>Remove usage of Directory#fileExists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-26T20:24:44Z</created><updated>2015-06-07T11:57:40Z</updated><resolved>2014-10-27T13:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-27T08:16:24Z" id="60560651">@jpountz actually I don't think we need that one at alll I removed it and unless we access something that has not been written through this directory everything should work just fine. if we do it's a bug IMO
</comment><comment author="jpountz" created="2014-10-27T08:17:36Z" id="60560739">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup non nested filter to not flip the FixedBitSet returned by the wrapped filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8232</link><project id="" key="" /><description>If the filter producing the FBS were to be cached then it would flip bits each time the NonNestedDocsFilter was executed.

In our case we cached the NonNestedDocsFilter itself so flipping bits each time NonNestedDocsFilter was used  didn't happen. However the hashcode of NonNestedDocsFilter and NestedDocsFilter was the same, since NonNestedDocsFilter directly used NestedDocsFilter#hashCode()

PR for #8227
</description><key id="46852837">8232</key><summary>Cleanup non nested filter to not flip the FixedBitSet returned by the wrapped filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cache</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-26T20:02:01Z</created><updated>2015-06-07T18:20:01Z</updated><resolved>2014-10-27T13:19:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-26T21:10:11Z" id="60532445">LGTM I guess this counts as a bugfix and should go on 1.4 and 1.3 as well?
</comment><comment author="jpountz" created="2014-10-26T21:27:28Z" id="60533035">LGTM
</comment><comment author="martijnvg" created="2014-10-27T08:13:40Z" id="60560488">@s1monw Ok, I'll add it to 1.3 and 1.4 too.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/nested/NestedDocsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NonNestedDocsFilter.java</file></files><comments><comment>Core: Cleanup non nested filter to not flip the bits in the FixedBitSet returned by the wrapped filter.</comment></comments></commit></commits></item><item><title>RecoveriesCollection.findRecoveryByShard should call recoveryStatus.tryIncRef before accessing fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8231</link><project id="" key="" /><description>The function iterates over a snapshot of on going recoveries and thus may access RecoveryStatus objects that are already finished.

This issue was introduced with #8092 
</description><key id="46850763">8231</key><summary>RecoveriesCollection.findRecoveryByShard should call recoveryStatus.tryIncRef before accessing fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-26T18:44:58Z</created><updated>2015-06-08T00:23:24Z</updated><resolved>2014-10-26T20:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-26T19:05:44Z" id="60528038">left some comments
</comment><comment author="bleskes" created="2014-10-26T19:11:13Z" id="60528236">@s1monw I pushed another commit.
</comment><comment author="s1monw" created="2014-10-26T20:08:47Z" id="60530206">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file></files><comments><comment>Recovery: RecoveriesCollection.findRecoveryByShard should call recoveryStatus.tryIncRef before accessing fields</comment></comments></commit></commits></item><item><title>NoClassDefFoundError org.elasticsearch.common.netty.channel.socket.nio.NioWorker$RegisterTask</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8230</link><project id="" key="" /><description>Got java.lang.NoClassDefFoundError: org/elasticsearch/common/netty/channel/socket/nio/NioWorker$RegisterTask with elasticsearch 1.3.4
elasticsearch-1.3.4.jar is in classpath.
</description><key id="46841767">8230</key><summary>NoClassDefFoundError org.elasticsearch.common.netty.channel.socket.nio.NioWorker$RegisterTask</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oeztuce</reporter><labels /><created>2014-10-26T12:52:00Z</created><updated>2014-10-27T08:26:36Z</updated><resolved>2014-10-27T08:12:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-27T05:52:22Z" id="60552802">is this on a project where you embed ES? If so can you run a clean build this looks pretty much like a stale dev environment.
</comment><comment author="oeztuce" created="2014-10-27T08:12:40Z" id="60560408">Yes, this is an embedded ES.

I create ES in the before method of my JUnit-Test.
final NodeBuilder nodeBuilder = NodeBuilder.nodeBuilder();
        final Builder settingsBuilder = nodeBuilder.settings();
        settingsBuilder.put("network.publish_host", "localhost");
        settingsBuilder.put("network.bind_host", "localhost");
        final Settings settings = settingsBuilder.build();
        this.node = nodeBuilder.clusterName("mymcluster").local(false).data(true).settings(settings).node();
        this.client = this.node.client();

And in the productive code i create a TransportClient:
final Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", "mycluster").build();
        this.client = new TransportClient(settings).addTransportAddress(new InetSocketTransportAddress(
                elasticSearchHostname, 9300));

Is something wrong with my approach?

Error stacktrace:
2014-10-27 09:19:39,006 [][][][][ WARN]        org.elasticsearch.transport.netty - [Magneto] exception caught on transport layer [[id: 0xc77d65bb, 0.0.0.0/0.0.0.0:56749 :&gt; localhost/127.0.0.1:9300]], closing connection
java.lang.NoClassDefFoundError: org/elasticsearch/common/netty/channel/socket/nio/NioWorker$RegisterTask
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.createRegisterTask(NioWorker.java:118)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.register(AbstractNioSelector.java:104)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.register(NioWorker.java:36)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:155)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.common.netty.channel.socket.nio.NioWorker$RegisterTask
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1720)
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1571)
    ... 13 more
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshots "records" persist after deletion of repo and corresponding snapshots - ES 1.3.2 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8229</link><project id="" key="" /><description>Snapshots in Elasticsearch 1.3.2 seem to have an issue with regard to snapshot deletion and recreation.  If you create a repo and snapshot, then delete these objects, Elasticsearch will somehow retain a record of the snapshot indices and refuse to create the snapshots even after the repo, snapshot and backed up index files are deleted.

Here are the steps to reproduce the problem:
1. Create repo A:
   curl -XPUT 'http://localhost:9200/_snapshot/A' -d '
   {
   "type": "fs",
   "settings": {
       "location": "/media/snaps0",
       "compress": true
   }
   }'
2. Create snapshot B containing indices X and Y:
   curl -XPUT 'http://localhost:9200/_snapshot/A/B' -d '
   {
   "indices": [
       "X", "Y"
   ],
   "ignore_unavailable": "true",
   "include_global_state": false
   }'
3. Verify snapshot created in /media/snaps0, directory structure should look like this:
   /media/snaps0/index
   /media/snaps0/metadata-A  (might not be quite the right name for this file)
   /media/snaps0/snapshot-A
   /media/snaps0/indices/X   (index files in this directory not shown)
   /media/snaps0/indices/Y   (index files in this directory not shown)
4. Delete the repo, snapshot and files in /media/snaps0:
   curl -XDELETE 'http://localhost:9200/_snapshot/A/B'
   curl -XDELETE 'http://localhost:9200/_snapshot/A'
   rm -rf /media/snaps0/*
5. Recreate the same repo and snapshot by repeating steps 1 and 2.
6. Repeating step 3 to verify the contents of /media/snaps0 will show that the /media/snaps0/indices folder is not created.

Somehow Elasticsearch 1.3.2 seems to think that the snapshot of X and Y still exists and refuses to re-snapshot them the second time around.  I thinks this is a bug, because I followed the directions on snapshot and restore from the Elasticsearch.org online documentation.
</description><key id="46824480">8229</key><summary>Snapshots "records" persist after deletion of repo and corresponding snapshots - ES 1.3.2 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">vichargrave</reporter><labels><label>feedback_needed</label></labels><created>2014-10-25T21:44:11Z</created><updated>2014-11-04T17:52:23Z</updated><resolved>2014-11-04T17:52:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-26T08:22:59Z" id="60509981">@imotov can you take a look please
</comment><comment author="imotov" created="2014-10-26T14:07:18Z" id="60518445">@vichargrave thank you for the report. I have a few questions that would help me to understand your issue better. Do you have a single node in your cluster? Did you wait for snapshot for finish in the step 3? If not can you try recreating the issue if you add `?wait_for_completion=true` to the URL on the step 2? On the step 4 the command that you posted `curl -XGET 'http://localhost:9200/_snapshot/A/B'` doesn't delete snapshot, it returns the information about the snapshot back to you. Is it typo in the issue or you indeed executed this statement instead of deleting the snapshot? Did you verify that files are indeed gone after running `rm -rf /media/snaps0/*`
</comment><comment author="vichargrave" created="2014-10-26T16:41:04Z" id="60523185">@imotov Here are the answers to your questions:
1. I have 4 nodes in my cluster.
2. I waited for the completion of the snapshot operations and verified by checking the stats.  
3. Step 4 was a typo.  I've corrected the comment.  Sorry about the confusion on that one. 
4. The  'rm -rf /media/snaps0/*' does indeed remove all the files.

This series of steps is repeatable.
</comment><comment author="imotov" created="2014-10-26T16:48:22Z" id="60523412">@vichargrave is /media/snaps0/ directory shared between nodes? If it is what file system are you using and can you give more information about your operating system and shared storage?
</comment><comment author="vichargrave" created="2014-10-26T16:57:48Z" id="60523710">@imotov I'm using CentoOS 6.5 with ext4 file system.  The /media/snaps0 are mount points available on each node, which is required.  They map to an SMB shared drive on another CentoOS 6.5 system.  Please note, that the snapshot and associated files were successfully created the first time around.  It was only after everything was deleted and the procedure repeated that the snapshots were not created.  When the procedure was repeated, the Elasticsearch status API call revealed that the operation was successful, but that 0 shards were copied.
</comment><comment author="vichargrave" created="2014-10-26T17:02:02Z" id="60523863">@imotov There is another thing I should point out.  My cluster is not in a Green state.  All the primary shards are available, but most replicas are unassigned.  I'm trying to backup several indices so I can wipe the cluster clean and restore the backed up indices.  The cluster uses 5 primay shards per index and 1 replica.
</comment><comment author="imotov" created="2014-10-26T17:05:01Z" id="60523951">@vichargrave which error message are you getting when you are trying to create a snapshot? Do you see any snapshot-related errors in the log file?
</comment><comment author="vichargrave" created="2014-10-26T17:13:30Z" id="60524204">@imotov I don't get errors, just success codes.  The first time around the message indicates how many shards were copied.  The second time around, the status is still success, just 0 shards copied.  I suggest you try this out on your test environment to see the precise behavior.  

I should note that I can use the snapshot mechanism to backup and restore indices.  Again it only works the first time around.  I'm confident I can get and restore snapshots of the indices I need to keep.  So I will proceed to do that so I can get my cluster to a Green state.  When I do I'll repeat the steps I outlined to see if I get the same results with a Green cluster.  I'll keep you posted.
</comment><comment author="imotov" created="2014-10-26T17:35:44Z" id="60524937">@vichargrave what you are describing is pretty much how we test the snapshot/restore feature. In other words we repeat these steps hundreds of times a day on several CI server and they work. But just to be on the safe side I wrote a [script](https://gist.github.com/imotov/20da3c505792f487ebae) that follows the steps that you described and I couldn't reproduce the issue. I am sorry, but I think there is still something important missing in your description. So, I would need your help to figure out this part. It could be the current state of your cluster, so I would need more information on how it got into this state in order to reproduce the issue. Are nodes running out of memory or disk space? Did you disable allocation by any chance? What happened with the cluster before it got there? Could you also show exact syntax and output of the second snapshot command?
</comment><comment author="vichargrave" created="2014-10-26T17:58:28Z" id="60525701">@imotov. You raise good points.  I have disabled shard allocation while I try to backup the indices.  However I believe that is what the recommended procedure is, to disable shard allocation first then do a snapshot.  

My situation must have something to do with my cluster status.  I will go ahead and backup my indices then clear out the cluster.  After I add back the necessary indices, I'll repeat this test to see if I get the same results.

Thanks.

-- vic

Sent from my iPhone

&gt; On Oct 26, 2014, at 10:36 AM, Igor Motov notifications@github.com wrote:
&gt; 
&gt; @vichargrave what you are describing is pretty much how we test the snapshot/restore feature. In other words we repeat these steps hundreds of times a day on several CI server and they work. But just to be on the safe side I wrote a script that follows the steps that you described and I couldn't reproduce the issue. I am sorry, but I think there is still something important missing in your description. So, I would need your help to figure out this part. It could be the current state of your cluster, so I would need more information on how it got into this state in order to reproduce the issue. Are nodes running out of memory or disk space? Did you disable allocation by any chance? What happened with the cluster before it got there? Could you also show exact syntax and output of the second snapshot command?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="vichargrave" created="2014-10-26T18:10:08Z" id="60526126">@imotov.  Thanks for the script.  One more thought.  Your test setup looks like it involves only one node.  Have you tried this on a multinode cluster?  I don't doubt the state of my cluster is contributing to the problem, but your script should work on a multinode cluster in the GREEN state.  Just food for thought.

Thanks again.
</comment><comment author="imotov" created="2014-10-26T18:14:50Z" id="60526335">@vichargrave I tested it with two nodes running on the same machine. So, unless something went seriously wrong with the shared file system it should be equivalent.
</comment><comment author="imotov" created="2014-11-04T17:32:39Z" id="61680487">@vichargrave have you figured out what was going wrong? Any updates?
</comment><comment author="vichargrave" created="2014-11-04T17:41:45Z" id="61681876">Hi Igor.

No I haven't, but I suspect the" problem" was related to the state my
cluster was in - lot's of unassigned shards and reallocation turned off.  I
have since done many snapshots - after snapshoting my indices, removing
them from the cluster and resuming reallocation - so the facility seems to
be working well.

One thing to note, I had to cancel a snapshot in the middle of the
operation - it's a long story, don't ask.  I noticed when I did that not
only the snapshot in Elasticseach was deleted, but also the files that had
been written to my shared backup drive up to that point.  That is great,
but when I reported my "problem" I was deleting the files manually.  I
wonder if that fowled things up as well, although I'm not sure how it would.

At any rate, I convinced that the "problem" was on my end and not related
to any Elasticsearch deficiency.  Sorry to send you on a wild goose chase.

Thanks for everything.

-- vic

On Tue, Nov 4, 2014 at 9:33 AM, Igor Motov notifications@github.com wrote:

&gt; @vichargrave https://github.com/vichargrave have you figured out what
&gt; was going wrong? Any updates?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8229#issuecomment-61680487
&gt; .
</comment><comment author="imotov" created="2014-11-04T17:52:21Z" id="61683526">@vichargrave thanks! I will close it for now then. Please, feel free to reopen if you will find any evidence that snapshot process might be the culprit. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update multi-get.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8228</link><project id="" key="" /><description>Duplicate word
</description><key id="46818543">8228</key><summary>Update multi-get.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kagel</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-25T17:59:46Z</created><updated>2014-10-28T09:59:29Z</updated><resolved>2014-10-28T09:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-27T09:28:47Z" id="60567152">Hi @have-a-look 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="kagel" created="2014-10-27T18:16:51Z" id="60642086">Yes, sure. Done.
</comment><comment author="clintongormley" created="2014-10-28T09:59:12Z" id="60732199">Thanks @have-a-look - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update multi-get.asciidoc</comment></comments></commit></commits></item><item><title>NonNestedDocsFilter.getDocIDSet() looks buggy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8227</link><project id="" key="" /><description>It modifies the docidset (casts to FixedBitSet and flips all the bits).

If this bitset was cached, this will be modifying the cached instance each time flipping all of its bits back and forth.
</description><key id="46810408">8227</key><summary>NonNestedDocsFilter.getDocIDSet() looks buggy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label></labels><created>2014-10-25T12:14:42Z</created><updated>2014-11-03T09:01:25Z</updated><resolved>2014-10-27T13:19:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-31T16:54:18Z" id="61291694">IndexedGeoBoundingBoxFilter has a similar issue: it modifies the bits returned by another filter
</comment><comment author="martijnvg" created="2014-11-03T08:50:34Z" id="61451523">@jpountz Right, it should instead like TermsFilter create a clean bitset and perform the or operation on that.
</comment><comment author="jpountz" created="2014-11-03T08:57:14Z" id="61451997">@martijnvg I tried to understand how it works and it looks like we could fix it and make the code simpler at the same time by just creating an BooleanFilter?
</comment><comment author="martijnvg" created="2014-11-03T09:01:25Z" id="61452353">@jpountz +1 I like it, this should work and we can reuse the BooleanFilter.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/nested/NestedDocsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NonNestedDocsFilter.java</file></files><comments><comment>Core: Cleanup non nested filter to not flip the bits in the FixedBitSet returned by the wrapped filter.</comment></comments></commit></commits></item><item><title>Empty index corrupts sort order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8226</link><project id="" key="" /><description>I've been trying to track down some strange sorting results and have narrowed it down to having an empty index.  I have multiple indexes all aliased to the same value.  Sort order works fine, but once you add an empty index, the order is corrupted.  Reproduction below:

```
#!/bin/bash

# Create index with no docs with alias test
curl -s -XPOST "http://localhost:9200/test-1"     -d '{ "aliases" : { "test" : {} }}' &gt; /dev/null

# Then create a bunch of indices with alias test
curl -s XPOST "http://localhost:9200/test-2"     -d '{ "aliases" : { "test" : {} }}' &gt; /dev/null
curl -s -XPOST "http://localhost:9200/test-2/doc" -d "{ \"entry\": 1 }"

curl -s -XPOST "http://localhost:9200/test-3"     -d '{ "aliases" : { "test" : {} }}' &gt; /dev/null
curl -s -XPOST "http://localhost:9200/test-3/doc" -d "{ \"entry\": 2 }"

curl -s -XPOST "http://localhost:9200/test-4"     -d '{ "aliases" : { "test" : {} }}' &gt; /dev/null
curl -s -XPOST "http://localhost:9200/test-4/doc" -d "{ \"entry\": 3 }"

curl -s -XPOST "http://localhost:9200/test-5"     -d '{ "aliases" : { "test" : {} }}' &gt; /dev/null
curl -s -XPOST "http://localhost:9200/test-5/doc" -d "{ \"entry\": 4 }"

curl -s -XPOST "http://localhost:9200/test-6"     -d '{ "aliases" : { "test" : {} }}' &gt; /dev/null
curl -s -XPOST "http://localhost:9200/test-6/doc" -d "{ \"entry\": 5 }"

curl -s -XPOST "http://localhost:9200/test-7"     -d '{ "aliases" : { "test" : {} }}' &gt; /dev/null
curl -s -XPOST "http://localhost:9200/test-7/doc" -d "{ \"entry\": 6 }"

sleep 2

# Perform a sorted query, descending on field 'entry'
curl -XPOST 'http://localhost:9200/test/_search?pretty' -d '{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "*"
              }
            }
          ]
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "match_all": {}
            }
          ]
        }
      }
    }
  },
  "highlight": {
    "fields": {},
    "fragment_size": 2147483647,
    "pre_tags": [
      "@start-highlight@"
    ],
    "post_tags": [
      "@end-highlight@"
    ]
  },
  "size": 500,
  "sort": [
    {
      "entry": {
        "order": "desc",
        "ignore_unmapped": true
      }
    }
  ]
}' | jq ".hits.hits[] | ._source.entry"
```

Results:

```
5
1
3
2
6
4
```

I can fix sort order in two ways: 1) remove the empty index or 2) change "ignore_unmapped" to false.

But IMO an empty index should no affect sort results.  I'm also confused on why "ignore_unmapped": false, fixes things.  I would think you would want the reverse, but it fails when true.
</description><key id="46799586">8226</key><summary>Empty index corrupts sort order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">matthughes</reporter><labels><label>bug</label><label>v1.3.5</label></labels><created>2014-10-25T02:25:37Z</created><updated>2014-10-27T12:22:12Z</updated><resolved>2014-10-27T12:22:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-27T06:19:15Z" id="60554084">I can reproduce this on `1.3` branch with:

``` Java

public void testIssue8226() {
        int numIndices = between(5, 10);
        for (int i = 0; i &lt; numIndices; i++) {
            assertAcked(prepareCreate("test_" + i).addAlias(new Alias("test")));
            if (i &gt; 0) {
                client().prepareIndex("test_" + i, "foo", "" + i).setSource("{\"entry\": " + i + "}").get();
            }
        }
        ensureYellow();
        refresh();
        SearchResponse searchResponse = client().prepareSearch()
                .addSort(new FieldSortBuilder("entry").order(SortOrder.DESC).ignoreUnmapped(true))
                .setSize(10).get();
        assertSearchResponse(searchResponse);

        for (int j = 1; j &lt; searchResponse.getHits().hits().length; j++) {
            assertThat(searchResponse.toString(), searchResponse.getHits().hits()[j].getId(), lessThan(searchResponse.getHits().hits()[j-1].getId()));
        }
    }
```
</comment><comment author="s1monw" created="2014-10-27T08:25:28Z" id="60561359">It seems this doesn't happen in `master`, `1.x` and neither on `1.4` but the test fails on `1.3` on every run so either we forgot to backport a fix or it's been fixed due to the cut-over to lucene comparators or so?
</comment><comment author="s1monw" created="2014-10-27T08:37:40Z" id="60562307">alright I found the issue - nasty... will open a PR soon
</comment><comment author="jpountz" created="2014-10-27T12:03:16Z" id="60582551">&gt; It seems this doesn't happen in master, 1.x and neither on 1.4 but the test fails on 1.3 on every run so either we forgot to backport a fix or it's been fixed due to the cut-over to lucene comparators or so?

I believe it is because this change https://github.com/elasticsearch/elasticsearch/pull/7039 is only on 1.4+
</comment><comment author="s1monw" created="2014-10-27T12:22:12Z" id="60584481">fixed in `1.3.5`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file></files><comments><comment>[TEST] Add test for #8226</comment></comments></commit></commits></item><item><title>JDK/JRE 7 update 72 breaks multicast in 1.4.0.Beta1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8225</link><project id="" key="" /><description>I've just updated to JDK 7 update 72 and 1.4.0.Beta1 doesn't work any more (win 8.1).

```
# java -version
java version "1.7.0_72"
Java(TM) SE Runtime Environment (build 1.7.0_72-b14)
Java HotSpot(TM) Client VM (build 24.72-b04, mixed mode, sharing)
```

```
[2014-10-25 00:37:58,821][INFO ][node                     ] [Thog] version[1.4.0.Beta1], pid[6224], build[1f25669/2014-10-01T14:58:15Z]
[2014-10-25 00:37:58,822][INFO ][node                     ] [Thog] initializing ...
[2014-10-25 00:37:58,827][INFO ][plugins                  ] [Thog] loaded [], sites []
[2014-10-25 00:38:01,206][INFO ][node                     ] [Thog] initialized
[2014-10-25 00:38:01,206][INFO ][node                     ] [Thog] starting ...
[2014-10-25 00:38:01,308][INFO ][transport                ] [Thog] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.50:9300]}
[2014-10-25 00:38:01,351][INFO ][discovery.zen.ping.multicast] [Thog] multicast failed to start [SocketException[An invalid argument was supplied]], disabling
[2014-10-25 00:38:01,353][INFO ][discovery                ] [Thog] elasticsearch/Sk3GjLbeTduVCehGYnfdRg
[2014-10-25 00:38:01,359][WARN ][discovery.zen.ping.multicast] [Thog] failed to send multicast ping request: NullPointerException[null]
[2014-10-25 00:38:02,860][WARN ][discovery.zen.ping.multicast] [Thog] failed to send multicast ping request: NullPointerException[null]
[2014-10-25 00:38:04,372][INFO ][cluster.service          ] [Thog] new_master [Thog][Sk3GjLbeTduVCehGYnfdRg][cerberus][inet[/192.168.1.50:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-25 00:38:04,415][INFO ][http                     ] [Thog] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.50:9200]}
[2014-10-25 00:38:04,416][INFO ][node                     ] [Thog] started
[2014-10-25 00:38:04,926][INFO ][gateway                  ] [Thog] recovered [2] indices into cluster_state
```

1.4.0.Beta1 works fine with 7u67 and 8u25. It looks like only 7u72 is problematic (potentially 7u71).
Tested 1.3.4 on 7u72 and it seems to be ~~working fine~~.
Verified the java version and the same problem occurs - clearly something has changed in the JDK. 

```
# java -version
java version "1.7.0_72"
Java(TM) SE Runtime Environment (build 1.7.0_72-b14)
Java HotSpot(TM) Client VM (build 24.72-b04, mixed mode, sharing)

# elasticsearch
[2014-10-25 00:50:20,068][INFO ][node                     ] [Wendell Vaughn] version[1.3.4], pid[7220], build[a70f3cc/2014-09-30T09:07:17Z]
[2014-10-25 00:50:20,070][INFO ][node                     ] [Wendell Vaughn] initializing ...
[2014-10-25 00:50:20,074][INFO ][plugins                  ] [Wendell Vaughn] loaded [], sites []
[2014-10-25 00:50:22,367][INFO ][node                     ] [Wendell Vaughn] initialized
[2014-10-25 00:50:22,367][INFO ][node                     ] [Wendell Vaughn] starting ...
[2014-10-25 00:50:22,472][INFO ][transport                ] [Wendell Vaughn] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.50:9300]}
[2014-10-25 00:50:22,570][INFO ][discovery.zen.ping.multicast] [Wendell Vaughn] multicast failed to start [SocketException[An invalid argument was supplied]], disabling
[2014-10-25 00:50:22,571][INFO ][discovery                ] [Wendell Vaughn] elasticsearch/iROZFsnQQb-2OUFP6bA_bA
[2014-10-25 00:50:22,600][WARN ][discovery.zen.ping.multicast] [Wendell Vaughn] failed to send multicast ping request: NullPointerException[null]
[2014-10-25 00:50:24,102][WARN ][discovery.zen.ping.multicast] [Wendell Vaughn] failed to send multicast ping request: NullPointerException[null]
[2014-10-25 00:50:25,605][INFO ][cluster.service          ] [Wendell Vaughn] new_master [Wendell Vaughn][iROZFsnQQb-2OUFP6bA_bA][cerberus][inet[/192.168.1.50:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-25 00:50:25,638][INFO ][gateway                  ] [Wendell Vaughn] recovered [0] indices into cluster_state
[2014-10-25 00:50:25,649][INFO ][http                     ] [Wendell Vaughn] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.50:9200]}
[2014-10-25 00:50:25,650][INFO ][node                     ] [Wendell Vaughn] started
```
</description><key id="46786953">8225</key><summary>JDK/JRE 7 update 72 breaks multicast in 1.4.0.Beta1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>low hanging fruit</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-24T21:43:31Z</created><updated>2014-10-28T15:01:24Z</updated><resolved>2014-10-28T15:01:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-10-24T21:53:10Z" id="60454500">Looking at the JDK [changelog](http://www.oracle.com/technetwork/java/javase/2col/7u71-bugfixes-2298226.html) it might be that update 71 (a critical security update) introduced the breaking behaviour, namely through [8021372](http://bugs.java.com/view_bug.do?bug_id=8021372):
`8021372    core-libs   java.net    NetworkInterface.getNetworkInterfaces() returns duplicate hardware address`
</comment><comment author="nik9000" created="2014-10-24T22:21:42Z" id="60457009">Yay!  We're in the process of upgrading to 7u72 right now.  Happy times.

If it needs an assignee I can look at it on Monday.  I need an excuse to go poking around in this code.  OTOH if someone has time to work on it please don't let me stop you.  Your Monday probably starts before mine anyway.
</comment><comment author="costin" created="2014-10-24T22:24:52Z" id="60457271">@nik9000 Note that I've currently tested it on Windows - there's a slim chance, the bug doesn't occur on *nix machines...
</comment><comment author="nik9000" created="2014-10-24T22:31:51Z" id="60457944">Ah.  Well, I'll at least be able to test that on Monday.  I don't think my wife would appreciate me commandeering her machine to reproduce on Windows.  So if it doesn't reproduce on unix I'd probably be a bad person to work on it.
</comment><comment author="Mpdreamz" created="2014-10-27T10:28:34Z" id="60573333">I'm on Windows 8.1 but I can't reproduce:

```
c:\&gt;"%JAVA_HOME%\bin\java.exe" -version
java version "1.7.0_72"
Java(TM) SE Runtime Environment (build 1.7.0_72-b14)
Java HotSpot(TM) Client VM (build 24.72-b04, mixed mode, sharing)
```

```
c:\Data\elasticsearch-1.4.0.Beta1&gt;bin\elasticsearch
[2014-10-27 11:25:44,236][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2014-10-27 11:25:44,299][INFO ][node                     ] [Pete Wisdom] version[1.4.0.Beta1], pid[4656], build[1f25669/2014-10-01T14:58:15Z]
[2014-10-27 11:25:44,299][INFO ][node                     ] [Pete Wisdom] initializing ...
[2014-10-27 11:25:44,314][INFO ][plugins                  ] [Pete Wisdom] loaded [], sites []
[2014-10-27 11:25:46,317][INFO ][node                     ] [Pete Wisdom] initialized
[2014-10-27 11:25:46,317][INFO ][node                     ] [Pete Wisdom] starting ...
[2014-10-27 11:25:46,458][INFO ][transport                ] [Pete Wisdom] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.194.169:9300]}
[2014-10-27 11:25:46,520][INFO ][discovery                ] [Pete Wisdom] elasticsearch/oVt8wl4-S9usPM0xWva_TA
[2014-10-27 11:25:49,567][INFO ][cluster.service          ] [Pete Wisdom] new_master [Pete Wisdom][oVt8wl4-S9usPM0xWva_TA][WIN-QIFLG7L0DOO][inet[/192.168.194.169:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-27 11:25:49,598][INFO ][gateway                  ] [Pete Wisdom] recovered [0] indices into cluster_state
[2014-10-27 11:25:49,614][INFO ][http                     ] [Pete Wisdom] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.194.169:9200]}
[2014-10-27 11:25:49,614][INFO ][node                     ] [Pete Wisdom] started
```

Am I doing something wrong here @costin ?
</comment><comment author="costin" created="2014-10-27T11:13:08Z" id="60577748">Looks like the issue can be reproduced only on Win 8.1 SP1 with both u71 and u72. Elasticsearch ran fine on Win 7 and Win 8.1 (no SP1). In my opinion, this is not a blocker any more however there is work on the way to provide a fix for it.
</comment><comment author="gmarz" created="2014-10-27T14:56:42Z" id="60606021">@costin I'm also not able to reproduce this running Win 8.1.  

What is considered SP1?  I can't seem to find any updates being referred to as SP1.  The latest update I have installed is KB2995388.

```
java version "1.7.0_71"
Java(TM) SE Runtime Environment (build 1.7.0_71-b14)
Java HotSpot(TM) 64-Bit Server VM (build 24.71-b01, mixed mode)

[2014-10-27 10:42:15,525][INFO ][node                     ] [Persuasion] version[1.4.0.Beta1], pid[4416], build[1f25669/2014-10-01T14:58:15Z]
[2014-10-27 10:42:15,525][INFO ][node                     ] [Persuasion] initializing ...
[2014-10-27 10:42:15,541][INFO ][plugins                  ] [Persuasion] loaded [], sites []
[2014-10-27 10:42:18,119][INFO ][node                     ] [Persuasion] initialized
[2014-10-27 10:42:18,119][INFO ][node                     ] [Persuasion] starting ...
[2014-10-27 10:42:18,259][INFO ][transport                ] [Persuasion] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.211.55.3:9300]}
[2014-10-27 10:42:18,291][INFO ][discovery                ] [Persuasion] elasticsearch/Mwp7dFlMQW6kVg-xsdU87w 
[2014-10-27 10:42:21,338][INFO ][cluster.service          ] [Persuasion] new_master [Persuasion][Mwp7dFlMQW6kVg-xsdU87w][GREGMARZOUK4138][inet[/10.211.55.3:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-27 10:42:21,370][INFO ][http                     ] [Persuasion] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.211.55.3:9200]}
[2014-10-27 10:42:21,370][INFO ][node                     ] [Persuasion] started
```
</comment><comment author="costin" created="2014-10-27T16:05:20Z" id="60618588">Based on various feedback it seems that I am the only one running into this issue which suggests there's some configuration issue with my system. The win 7, win 8.1 (with and without Update 1/ SP1) that we have tested, do not seem to be affected.

@clintongormley Hence why I removed the two labels above (critical and blocker).
</comment><comment author="costin" created="2014-10-27T16:06:08Z" id="60618720">@nik9000 see the comment above - I don't think you're going to be affected and anyway, there's a fix already in the works (see the PR).
</comment><comment author="mrsolo" created="2014-10-27T18:19:34Z" id="60642711">Jenkins seeing similar, if the the same failure on window 2012 r2 server

  1&gt; [2014-10-27 17:59:39,689][DEBUG][discovery.zen.ping.multicast] [node_t0] failed to send multicast ping request
  1&gt; java.lang.NullPointerException
  1&gt;    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.sendPingRequest(MulticastZenPing.java:256)
  1&gt;    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.ping(MulticastZenPing.java:188)
  1&gt;    at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:146)
  1&gt;    at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:124)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:932)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:341)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery.access$5600(ZenDiscovery.java:83)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1299)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2014-10-27 17:59:39,705][DEBUG][discovery.zen.ping.multicast] [node_t1] multicast failed to start [SocketException[Unrecognized Windows Sockets error: 0: Cannot bind]], disabling
  1&gt; java.net.SocketException: Unrecognized Windows Sockets error: 0: Cannot bind
  1&gt;    at java.net.TwoStacksPlainDatagramSocketImpl.bind0(Native Method)
  1&gt;    at java.net.TwoStacksPlainDatagramSocketImpl.bind0(TwoStacksPlainDatagramSocketImpl.java:107)
  1&gt;    at java.net.AbstractPlainDatagramSocketImpl.bind(AbstractPlainDatagramSocketImpl.java:96)
  1&gt;    at java.net.TwoStacksPlainDatagramSocketImpl.bind(TwoStacksPlainDatagramSocketImpl.java:97)
  1&gt;    at java.net.DatagramSocket.bind(DatagramSocket.java:396)
  1&gt;    at java.net.MulticastSocket.&lt;init&gt;(MulticastSocket.java:172)
  1&gt;    at java.net.MulticastSocket.&lt;init&gt;(MulticastSocket.java:137)
  1&gt;    at org.elasticsearch.common.network.MulticastChannel$Plain.buildMulticastSocket(MulticastChannel.java:283)
  1&gt;    at org.elasticsearch.common.network.MulticastChannel$Plain.&lt;init&gt;(MulticastChannel.java:276)
  1&gt;    at org.elasticsearch.common.network.MulticastChannel.getChannel(MulticastChannel.java:50)
  1&gt;    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.doStart(MulticastZenPing.java:127)
  1&gt;    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
  1&gt;    at org.elasticsearch.discovery.zen.ping.ZenPingService.doStart(ZenPingService.java:103)
  1&gt;    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery.doStart(ZenDiscovery.java:215)
  1&gt;    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
  1&gt;    at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)
  1&gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  1&gt;    at java.lang.reflect.Method.invoke(Method.java:606)
  1&gt;    at org.elasticsearch.common.inject.internal.ConstructionContext$DelegatingInvocationHandler.invoke(ConstructionContext.java:110)
  1&gt;    at com.sun.proxy.$Proxy35.start(Unknown Source)
  1&gt;    at org.elasticsearch.discovery.DiscoveryService.doStart(DiscoveryService.java:84)
  1&gt;    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)
  1&gt;    at org.elasticsearch.node.internal.InternalNode.start(InternalNode.java:244)
  1&gt;    at org.elasticsearch.test.InternalTestCluster$5.run(InternalTestCluster.java:1341)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  1&gt;    at java.lang.Thread.run(Thread.java:745)
  1&gt; [2014-10-27 17:59:39,705][INFO ][discovery                ] [node_t1] TEST-ElasticSearchAdTest-CHILD_VM=[0]-CLUSTER_SEED=[-2825708668188973463]-HASH=[12BD0B6F224490]/D3vQTSdQQlWVzREaEwf0AA
  1&gt; [2014-10-27 17:59:39,705][TRACE][discovery.zen.ping.unicast] [node_t0] [1] connecting to [node_t0][1HPXOGEJT46CaUCbPEkDNg][ElasticSearchAdTest][local[54]]{mode=local}
  1&gt; [2014-10-27 17:59:39,705][TRACE][discovery.zen.ping.unicast] [node_t0] [1] connected to [node_t0][1HPXOGEJT46CaUCbPEkDNg][ElasticSearchAdTest][local[54]]{mode=local}
  1&gt; [2014-10-27 17:59:39,705][TRACE][discovery.zen.ping.unicast] [node_t0] [1] sending to [node_t0][1HPXOGEJT46CaUCbPEkDNg][ElasticSearchAdTest][local[54]]{mode=local}
  1&gt; [2014-10-27 17:59:39,705][DEBUG][cluster.service          ] [node_t1] processing [initial_join]: execute
  1&gt; [2014-10-27 17:59:39,720][DEBUG][cluster.service          ] [node_t1] processing [initial_join]: no change in cluster_state
  1&gt; [2014-10-27 17:59:39,720][TRACE][discovery.zen            ] [node_t1] starting to ping
  1&gt; [2014-10-27 17:59:39,720][DEBUG][discovery.zen.ping.multicast] [node_t1] failed to send multicast ping request
  1&gt; java.lang.NullPointerException
  1&gt;    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.sendPingRequest(MulticastZenPing.java:256)
  1&gt;    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.ping(MulticastZenPing.java:188)
  1&gt;    at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:146)
  1&gt;    at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:124)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:932)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:341)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery.access$5600(ZenDiscovery.java:83)
  1&gt;    at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1299)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  1&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</comment><comment author="costin" created="2014-10-27T18:29:41Z" id="60645077">Notice the `NullPointerException` have been fixed in the PR. However the core problem (multicast that suddenly fails) is still present. On my machine the error occurs when calling `java.net.MulticastSocket#setInterface` while here it seems to occur when calling `MulticastSocket(SocketAddress)`
</comment><comment author="costin" created="2014-10-28T15:01:24Z" id="60769475">Pushed the fix in 1.3, 1.4, 1.x and master. In case of a socket error (for whatever reason) a nicer message is displayed to the user and the zen code is skipped (otherwise it causes its own exceptions which only add to the confusion).
This plus the `ES_USE_IPV4` flag seem to counter the error reproducible so far only on my windows machines...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ping/multicast/MulticastZenPing.java</file></files><comments><comment>Improve handling of multicast binding exceptions</comment></comments></commit></commits></item><item><title>Unassigned shards after restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8224</link><project id="" key="" /><description>ES 1.3.3
1. Have two nodes that form a cluster (one shard per index, no replicas).
2. Insert some documents (multiple indices).
3. Put repository, backup the cluster (global state = true, partial = false ).
4. Stop both nodes.
5. Delete the data folder of one of the nodes (path.data).
6. Start the two nodes (the cluster has red status).
7. Close all indices
7. Put repository, restore (global state = true, partial = false).

Observed : Half of the shards remain unassigned.

Note : If deleting data folders of both nodes (on step 5), after restore everything is ok.
</description><key id="46759035">8224</key><summary>Unassigned shards after restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">abaxanean</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2014-10-24T16:50:47Z</created><updated>2014-11-18T00:38:29Z</updated><resolved>2014-11-18T00:38:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: restore of indices that are only partially available in the cluster</comment></comments></commit></commits></item><item><title>FieldSortBuilder.toXContent does not return valid JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8223</link><project id="" key="" /><description>Running

```
SortBuilder sb = SortBuilders.fieldSort("timestamp").order(SortOrder.DESC) ;
System.out.println( sb.toString());
```

prints

```
"timestamp"{
  "order" : "desc"
}
```

which is invalid JSON, since the colon after "timestamp" is missing.

This could be fixed in the toXContent method of the org.elasticsearch.search.sort.FieldSortBuilder class as follows:

```
    @Override
    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {
+       builder.startObject();
        builder.startObject(fieldName);
        if (order != null) {
            builder.field("order", order.toString());
        }
        if (missing != null) {
            builder.field("missing", missing);
        }
        if (ignoreUnmapped != null) {
            builder.field("ignore_unmapped", ignoreUnmapped);
        }
        if (sortMode != null) {
            builder.field("mode", sortMode);
        }
        if (nestedFilter != null) {
            builder.field("nested_filter", nestedFilter, params);
        }
        if (nestedPath != null) {
            builder.field("nested_path", nestedPath);
        }
        builder.endObject();
+       builder.endObject();
        return builder;
    }
```

Could you include this fix in the next version?
</description><key id="46749395">8223</key><summary>FieldSortBuilder.toXContent does not return valid JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">birgitjunker</reporter><labels /><created>2014-10-24T15:17:03Z</created><updated>2014-10-27T09:18:27Z</updated><resolved>2014-10-27T09:18:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-10-24T16:34:50Z" id="60413167">the idea of this level "toXContent" is to be embedded within a higher level call of toXContent, in which case adding start/end object will cause not produce a valid JSON. You can call the start/end object yourself outside of this call. 
</comment><comment author="clintongormley" created="2014-10-27T09:18:27Z" id="60566125">@birgitjunker I'm assuming kimchy's comment resolved this issue. Please reopen if not the case.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[opensuse/Logstash] UDP input: not listening? (no errors)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8222</link><project id="" key="" /><description>OpenSuse 3.11.10-21-default
logstash 1.4.2-modified
Runas user: root
Installation: RPM

I try to setup an UDP input for Logstash but I cannot get it to work. The stdout is not showing any errors.

The following command returns nothing:
`netstat -tapen | grep ":11000 "`

**Conf:**

```
input {
  udp {
      port =&gt; 11000
      type =&gt; "apache-access"
   }
}
filter {
  if [type] == "apache-access" {
      grok {
        match =&gt; { "message" =&gt; "%{COMBINEDAPACHELOG}" }
      }
  }
}
```

**Startup:**

```
test-server:/opt/logstash # /opt/logstash/bin/logstash agent -f /etc/logstash/conf.d/
Using milestone 2 input plugin 'udp'. This plugin should be stable, but if you see strange behavior, please let us know! For more information on plugin milestones, see http://logstash.net/docs/1.4.2-modified/plugin-milestones {:level=&gt;:warn}
```

Could someone please advise how to proceed?
</description><key id="46739534">8222</key><summary>[opensuse/Logstash] UDP input: not listening? (no errors)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simkin</reporter><labels /><created>2014-10-24T13:41:37Z</created><updated>2014-10-24T16:36:54Z</updated><resolved>2014-10-24T14:42:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-24T14:42:28Z" id="60396312">Hi @simkin 

Please open this on https://github.com/elasticsearch/logstash/issues instead

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>how java api specify the script_id just like http request?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8221</link><project id="" key="" /><description>http request can use the 'script_id' like this:
{"explain": true,
"query": {
    "function_score": {
        "functions": [
            {
            "script_score": {
            "lang" : "groovy",
                 "params": { 
                "threshold": 80,
                "discount": 0.1,
                "target": 1000000
                },
          "script_id": "testforchinaesuser"
          }
        }
        ],
        "score_mode": "avg",
        "boost_mode":"replace"
    }
}
}

but the java api haven't the param ------ script_id
</description><key id="46737273">8221</key><summary>how java api specify the script_id just like http request?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LiuGangR</reporter><labels /><created>2014-10-24T13:18:28Z</created><updated>2014-10-24T13:19:31Z</updated><resolved>2014-10-24T13:19:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-24T13:19:31Z" id="60385364">please use the mailing list for questions like this  - the issue tracker is for bugs / features only
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] `Scope.SUITE` is not reproducible due to late cluster initialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8220</link><project id="" key="" /><description>The cluster for `Scope.SUITE` tests must be initialize in a static manner
before the first test runs otherwise the random context used to initialize
the cluster is taken from tests randomness rather than the suites randomness.
This means test clusters will have different setups if only a single test is
executed or even the test might have a entirely different random sequence.
</description><key id="46736982">8220</key><summary>[TEST] `Scope.SUITE` is not reproducible due to late cluster initialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-24T13:16:15Z</created><updated>2014-10-26T08:45:17Z</updated><resolved>2014-10-26T08:45:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-24T13:16:51Z" id="60385044">thanks to @brwe for finding the inconsistency in bwc tests and pinging me about it!!
</comment><comment author="brwe" created="2014-10-24T13:21:09Z" id="60385543">thanks for fixing so quickly! 
I hate to ask but...should we not have a test for it? Can we do something like InternalTestClusterTests?
</comment><comment author="s1monw" created="2014-10-24T13:28:58Z" id="60386480">we have assertions for this already now we can't add tests for this if you can come up with one that doesn't fail as soon as we change something in the base class I am willing to add it but it's hard I am sorry.
</comment><comment author="rjernst" created="2014-10-24T16:49:00Z" id="60415136">LGTM.
</comment><comment author="s1monw" created="2014-10-25T07:09:23Z" id="60474206">@brwe had a great idea yesterday about how to test this I added tests that actually failed for the `Scope.SUITE` case and are now passing. I think it's ready!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Production problem on Solaris 10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8219</link><project id="" key="" /><description>Hi,

We've setup an elastic search (V1.1.2) cluster on the following environment:
- java 1.6.0_35
- Solaris 10 (x86-64)
- 7,2 millions of document (document size between 300 byte and 10 kbyte)
- Xms = Xmx = 14000m on 64 Go servers
- 2 nodes
- Filesystem on ZFS (localdisk, no disk array)

Usually everything is fine on Linux, but on the Solaris server, we are facing a lot of problems:
- low performance on indexing
- hprof dumped on the bin directory
- very long full GC

```
[2014-10-21 19:48:00,111][WARN ][monitor.jvm              ] [spdmc2p] [gc][young][620023][49751] duration [1.7s], collections [1]/[2s], total [1.7s]/[48.7m], memory [9.6gb]-&gt;[9.6gb]/[13.6gb], all_pools {[young] [1mb]-&gt;[75.2kb]/[382.7mb]}{[survivor] [47.8mb]-&gt;[47.8mb]/[47.8mb]}{[old] [9.5gb]-&gt;[9.6gb]/[13.2gb]}
[2014-10-21 19:48:07,634][WARN ][monitor.jvm              ] [spdmc2p] [gc][young][620024][49753] duration [7.2s], collections [2]/[7.5s], total [7.2s]/[48.8m], memory [9.6gb]-&gt;[9.9gb]/[13.6gb], all_pools {[young] [75.2kb]-&gt;[765.1kb]/[382.7mb]}{[survivor] [47.8mb]-&gt;[47.8mb]/[47.8mb]}{[old] [9.6gb]-&gt;[9.9gb]/[13.2gb]}
```
- with impact on the other node

```
[2014-10-21 19:46:08,151][WARN ][transport                ] [spdmc1p] Received response for a request that has timed out, sent [40080ms] ago, timed out [10072ms] ago, action [discovery/zen/fd/masterPing], node [[spdmc2p][msHrhbPBSmacoCbW
R4yaXQ][spdmc2p.mts.am][inet[/192.168.111.10:9300]]], id [3431689]
```

Any idea if there is something special to check.do on Solaris to make it work ?
(we've several production in Linux with much more documents which are working perfectly)

Thanks and Regards,
Eric
</description><key id="46734211">8219</key><summary>Production problem on Solaris 10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">evidal</reporter><labels><label>feedback_needed</label></labels><created>2014-10-24T12:48:58Z</created><updated>2014-10-24T14:40:27Z</updated><resolved>2014-10-24T14:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-24T12:50:22Z" id="60382171">@evidal do you have swap disabled?
</comment><comment author="evidal" created="2014-10-24T12:54:53Z" id="60382637">I think swap is enabled (I'm not an expert with Solaris)

&gt; swap -l
&gt; swapfile             dev  swaplo blocks   free
&gt; /dev/zvol/dsk/rpool/swap 181,1       8 33554424 30815984
&gt; swap -s
&gt; total: 39299272k bytes allocated + 6688812k reserved = 45988084k used, 14023916k available

~~and thanks for your fast support~~
thanks for your help
</comment><comment author="clintongormley" created="2014-10-24T14:40:27Z" id="60396031">@evidal looks like it is.  swap is the bane of garbage collection. even a little usage of swap kills GC.  You really want to disable it, or to use mlockall (which i believe solaris supports)

See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html#setup-configuration-memory  for more details.

I'm going to close this for now, but feel free to reopen if you find that things are still going wrong, even with swap taken out of the picture.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: rolling upgrade process seems incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8218</link><project id="" key="" /><description>When reading the [rolling upgrade process](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#rolling-upgrades), you can see that we wrote:
- disable allocation
- upgrade node1
- upgrade node2
- upgrade node3
- ...
- enable allocation

That won't work as after a node has been removed and restarted, no shard will be allocated anymore.
So closing node2 and remaining nodes, won't help to serve index and search request anymore.

We should write:
- disable allocation
- upgrade node1
- enable allocation
- wait for shards being recovered on node1
- disable allocation
- upgrade node2
- enable allocation
- wait for shards being recovered on node2
- disable allocation
- upgrade node3
- enable allocation
- wait for shards being recovered on node3
- disable allocation
- ...
- enable allocation

I think this documentation update should go in 1.3, 1.4, 1.x and master branches.

Closes #7973.
</description><key id="46720829">8218</key><summary>Docs: rolling upgrade process seems incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-10-24T09:43:12Z</created><updated>2015-03-19T10:18:51Z</updated><resolved>2014-10-24T14:46:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: rolling upgrade process seems incorrect</comment></comments></commit></commits></item><item><title>Reject DELETE requests with a body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8217</link><project id="" key="" /><description>To add some level of protection to DELETE requests, reject requests that come with a body, when no body is expected  (this excludes delete-by-query and delete-scroll)

See https://github.com/elasticsearch/elasticsearch/issues/5960
</description><key id="46717099">8217</key><summary>Reject DELETE requests with a body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-10-24T08:55:06Z</created><updated>2017-03-17T20:21:26Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>collect() method of a custom aggregator invoked with docs in descending order for should queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8216</link><project id="" key="" /><description>I am developing a custom aggregator using ES 1.3.4. It extends from `NumericMetricsAggregator.MultiValue` class. Its code structure closely resembles that of the `Stats` aggregator. For my requirements, I need the doc Ids to be received in ascending order in the overridden `collect()` method. For most queries, I do get the doc Ids in ascending order. Interestingly for `bool` `should` queries with multiple clauses, I get doc Ids in descending order. How can I fix this? Is this a bug?
</description><key id="46705920">8216</key><summary>collect() method of a custom aggregator invoked with docs in descending order for should queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bittusarkar</reporter><labels /><created>2014-10-24T05:32:57Z</created><updated>2014-10-24T14:36:57Z</updated><resolved>2014-10-24T14:14:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-24T14:14:28Z" id="60392311">You can call `aggregationContext.ensureScoreDocsInOrder();` to make sure that docs are going to come in order, have a look for instance at `ReverseNestedAggregator` which uses this method.

Queries are indeed allowed to emit documents out-of-order if allowed to do so and if it makes things faster. I believe the only case when it happens today is when you get Lucene's BooleanScorer which is used for top-level disjunctions, so your observation makes sense.
</comment><comment author="bittusarkar" created="2014-10-24T14:36:57Z" id="60395481">Awesome! This is exactly what I wanted. It is working correctly now :+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Put mapping property update for _default_ type performs a complete replacement instead of a merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8215</link><project id="" key="" /><description>Consider the following example.    We are trying to add eager loading of field data to an existing property across all document types in an index:

https://gist.github.com/ppf2/2f49184d1c34a1c9b9b0

This works as expected for custom types.  If you look at the mapping defined for the custom_type type, there are 2 hostname fields (hostname and hostname2), if you update just hostname, it will successfully apply the change and do a merge such that hostname2 is retained and unchanged but hostname is updated.

```
PUT /test_mapping/custom_type/_mapping
{
    "properties": {
      "hostname": {
        "type": "string",
        "index": "not_analyzed",
        "doc_values": true,
        "fielddata": {
          "loading": "eager_global_ordinals"
        }
      }
    }
}
```

But if you execute the same update against `_default_` type, it performs a complete replace of the type. As a result, all other properties in the `_default_` type are deleted and it ends up with just the hostname property.  It doesn't just override the existing values within the property definition with the requested update but essentially create a new `_default_` type definition based on what is passed in via the put mapping API.  The doc description is a bit ambiguous "Any mapping specified in the create-index or put-mapping request _override values_ set in the `_default_` mapping.".   Is there a reason why we are not performing a merge?  The concern here is that this can cause unexpected loss in the default properties since it behaves differently from all the custom types.  So if the end users are going through all the types one by one and updating just 1 property, when they get to `_default_`, they can inadvertently cause the default type to drop everything else from its definition.
</description><key id="46694243">8215</key><summary>Put mapping property update for _default_ type performs a complete replacement instead of a merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2014-10-24T00:50:50Z</created><updated>2016-11-26T11:46:06Z</updated><resolved>2016-11-26T11:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-24T14:06:35Z" id="60391157">Hmmm, I think I agree - I think it makes more sense to merge.
</comment><comment author="spinscale" created="2014-10-24T15:38:29Z" id="60404876">seems as if  there is a specific behaviour for `_default_`, see https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/MapperService.java#L292 - not sure, why it is there
</comment><comment author="bleskes" created="2014-10-24T20:09:20Z" id="60442172">I agree it's confusing, but the `_default_` type is unique in the sense that you can actually delete fields and/or change settings in an incompatible manner. With merging you'll need a way to indicate that a field should be removed. Maybe we should go with something ala json patches that better represents mutations. 
</comment><comment author="jbrook" created="2015-01-13T13:30:17Z" id="69743813">While you make up your minds on this, do you think it would be possible to update the documentation for the PUT mapping API to add a warning that you can blow up your '_default_' mapping in this way? It seems like quite an enormous omission from the documentation. This behaviour must be unexpected for a lot of people and must lead to a lot of time wasted reindexing. Someone just replaced our default mapping with _nothing_...
</comment><comment author="clintongormley" created="2015-01-15T19:10:07Z" id="70142603">@bleskes in the interim, (and given that most mapping changes are permanent) I'd opt for merging and being done with it.
</comment><comment author="clintongormley" created="2015-01-15T19:10:40Z" id="70142702">@jbrook would you be up for sending a docs PR?
</comment><comment author="clintongormley" created="2016-11-26T11:45:41Z" id="263059061">Revisiting this after a long time...  I don't think it makes sense to add complicated JSON-patch syntax for this.  If you want to merge with an existing `_default_` mapping, then retrieve it, make changes client side, and PUT it.

I'll document that default mappings are overwritten with the PUT mapping API, and close this issue.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Document that the PUT mapping API with the _default_ type overwrites instead of merging</comment></comments></commit></commits></item><item><title>Testing: Added utility class to find free port</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8214</link><project id="" key="" /><description>The current netty tests heavily rely on finding a free port to bind to.

In order to simplify this, the first step is to bind only on so-called
ephemeral ports, which are considered to be high and short lived. This also
prevents binding on the heavily used 9xxx port range on testing systems.

The second step is to add a SocketUtil class, which returns a currently
unused port by trying to bind and unbind to it. This class check randomly
one of the ephemeral ports and also allows to return several ports at once.
</description><key id="46691096">8214</key><summary>Testing: Added utility class to find free port</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label></labels><created>2014-10-23T23:51:42Z</created><updated>2015-03-19T10:18:51Z</updated><resolved>2015-02-02T12:24:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-10-24T04:22:53Z" id="60343226">I think this is problematic, since you might not be able to open a port on a just closed server socket if it's quick enough. Can we maybe somehow improve the test to retry if it can't bind?
</comment><comment author="spinscale" created="2014-10-24T15:41:52Z" id="60405424">would `serverSocket.setReuseAddress(true);` improve the situation you are refering to here?
</comment><comment author="spinscale" created="2014-10-24T16:05:28Z" id="60408903">thinking about this, your suggestion still makes sense even with reuseAddress... will add that
</comment><comment author="spinscale" created="2014-10-24T21:25:34Z" id="60451547">revised and decoupled the code a bit to retry each test in case a binding error occurs...
</comment><comment author="spinscale" created="2015-02-02T12:24:45Z" id="72449477">I am going to close this (there seems not to be too much need for the SocketUtil at the moment), but open up another PR that makes sure to repeat a netty test in case of a bind exception, as this pops up in our test infra regurlary and we simply could rerun the test
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Return the sum of the doc counts of other buckets in terms aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8213</link><project id="" key="" /><description>This commit adds a new field to the response of the terms aggregation called
`sum_other_doc_count` which is equal to the sum of the doc counts of the buckets
that did not make it to the list of top buckets. It is typically useful to have
a sector called eg. `other` when using terms aggregations to build pie charts.

Example query and response:

``` json
GET test/_search?search_type=count
{
  "aggs": {
    "colors": {
      "terms": {
        "field": "color",
        "size": 3
      }
    }
  }
}
```

``` json
{
   [...],
   "aggregations": {
      "colors": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 4,
         "buckets": [
            {
               "key": "blue",
               "doc_count": 65
            },
            {
               "key": "red",
               "doc_count": 14
            },
            {
               "key": "brown",
               "doc_count": 3
            }
         ]
      }
   }
}
```
</description><key id="46652819">8213</key><summary>Return the sum of the doc counts of other buckets in terms aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-23T17:39:37Z</created><updated>2015-08-13T15:29:43Z</updated><resolved>2014-10-27T11:25:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2014-10-23T19:26:03Z" id="60294780">If we go with sum of counts rather than a bucket approach. Could we configure this field to sum not just counts but values in another field. So if we build a chart of sales $ that other segment in your example  will show other sales $ total rather than count
</comment><comment author="jpountz" created="2014-10-23T19:39:01Z" id="60296593">@roytmana We want to see how we can make this feature more flexible in 2.0 (eg. with sub aggregations) but it is challenging but for now (1.4), we only plan to return document counts.
</comment><comment author="roytmana" created="2014-10-23T22:07:37Z" id="60317170">@jpountz thanks makes sense I should not be too greedy :-) count is helpful. Will missing bucket be implemented in upcomimg 1.x versions?
</comment><comment author="jpountz" created="2014-10-24T06:56:19Z" id="60351089">@roytmana Missing is also something we are thinking about. The issue is a bit more complicated than "other" because there are more options: should we add a bucket for missing, allow to replace the missing value with arbitrary values (like sorting does) or both? And also, how to implement it in a unified way across all aggregations since this is something that would make sense on nearly all aggregations.
</comment><comment author="colings86" created="2014-10-24T08:16:27Z" id="60357467">@jpountz Code looks good, left some comments on the tests
</comment><comment author="roytmana" created="2014-10-24T13:43:16Z" id="60388176">Ok, thank you @jpountz. I was hoping to get some sense on when these features may become available. I need to start moving my BI framework off facets but without buckets for _other and _missing and agg filtering on array of terms including numeric it would be very challenging and pure waste if missing/other is finally supported
</comment><comment author="jpountz" created="2014-10-24T17:06:04Z" id="60418132">@colings86 I replied/addressed your comments
</comment><comment author="colings86" created="2014-10-27T08:34:14Z" id="60562035">@jpountz Looks good
</comment><comment author="niemyjski" created="2014-11-07T21:05:36Z" id="62211537">Will you be pushing a 1.4.2 with this fix?
</comment><comment author="jpountz" created="2014-11-07T21:10:23Z" id="62212107">@niemyjski I am confused, what bug are you talking about?
</comment><comment author="niemyjski" created="2014-11-07T21:23:26Z" id="62214176">We were getting a null reference exception when we upgraded ( https://github.com/elasticsearch/elasticsearch-net/issues/1042)
</comment><comment author="gmarz" created="2014-11-07T21:35:48Z" id="62217557">Hey @niemyjski this is an issue with NEST, not elasticsearch.  As mentioned in https://github.com/elasticsearch/elasticsearch-net/issues/1041, we'll be releasing a new version next week.  In the meantime you can pick up the CI package [here](https://www.myget.org/gallery/elasticsearch-net).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/AbstractStringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/UnmappedTerms.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/AbstractTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MinDocCountTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsSignificanceScoreTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file></files><comments><comment>Aggregations: Return the sum of the doc counts of other buckets.</comment></comments></commit></commits></item><item><title>Include warmer queries in slowlog?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8212</link><project id="" key="" /><description>Currently, the way to determine if a warmer query actually ran is to turn up TRACE logging on the index.warmer:

[2014-10-23 10:04:44,386][TRACE][index.warmer             ] [Franz Kafka] [index_name][0] warmed [warmer_1], took [195.5ms]
[2014-10-23 10:04:44,386][TRACE][index.warmer             ] [Franz Kafka] [index_name][0] warming took [200.2ms]

Would be nice to also capture warmer queries in the slowlog.
</description><key id="46649910">8212</key><summary>Include warmer queries in slowlog?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label></labels><created>2014-10-23T17:10:37Z</created><updated>2015-11-21T20:09:54Z</updated><resolved>2015-11-21T20:09:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-23T17:15:37Z" id="60273855">I'm afraid it could potentially be noise? I mean these queries are expected to be slow since the index is cold, so having them in the slowlog could raise concern while it's not actually an issue (since they did not slow down any user).
</comment><comment author="ppf2" created="2014-10-23T17:20:15Z" id="60274531">What if we somehow tag them with the warmer name when writing to slowlog so the user can easily tell between warmer vs regular queries, perhaps set a stats group name for these queries to the name of the warmer so that they get written out with a reference to the warmer name? :)
</comment><comment author="clintongormley" created="2015-11-21T20:09:53Z" id="158678239">I think we should remove warmers instead.  Closing in favour of https://github.com/elastic/elasticsearch/issues/9331
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>using doc_count from one aggregation into another aggregation to calculate additional percentage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8211</link><project id="" key="" /><description>Lets say I have doc like this

&lt;pre&gt;

      {
       user_id : "12312321321"
       location_shared_with { [ ]},
       location {location_name:.... 
                      location_verification[{} ]
                } 
       }

&lt;/pre&gt;

In here location_verification is an array of all the friends who have verified the location and it will always be equal or less than location_shared_with array.

mapping

&lt;pre&gt;

    { 
        "user" : 
            {"properties" : 
                 { 
                    "locations":
                    {
                        "type" : "nested",
                        "properties" : 
                        {
                            "location_name": { "type": "string", "index": "not_analyzed"},
                            "location_verification":
                            {
                                "type" : "nested",
                                "properties" : 
                                {
                                    "rating": { "type": "short", "store":"true" }
                                }
                            }
                        }
                    },
                    "location_shared_with":
                    {
                        "type" : "nested",
                        "properties" : 
                        {
                            "status": { "type": "string", "store":"true", "index": "analyzed"},
                            "friend_id": { "type": "string", "store" : "true"}
                        }
                    }
                 }
            }
    }'

&lt;/pre&gt;


So I need to calculate the % verified per location array . I have created right "nested" mapping between main doc (user) and location , location and location_verification.  I have terms aggregation that tells me 

when I execute

```
&lt;pre&gt;
GET _search?search_type=count
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "user_id": "1"
          }
        }
      ]
    }
  },  
  "aggs": {
    "location_shared_with_count":{
      "nested": {
        "path": "location_shared_with"
      }
     },
    "locations_count": {
      "nested": {
        "path": "locations"
      },
      "aggs": {
        "locations_aggs": {
          "terms": {
            "field": "locations.location_name"
          },
          "aggs": {
              "location_verification_count": {
                "nested": {
                  "path": "locations.location_verification"
                }

              }
            }          
        }
      }
    }
  },   
    "size": 10
} 
```

&lt;/pre&gt;

i do get following

&lt;pre&gt;

    {
       "took": 2,
       "timed_out": false,
       "_shards": {
          "total": 5,
          "successful": 5,
          "failed": 0
       },
       "hits": {
          "total": 1,
          "max_score": 0,
          "hits": []
       },
       "aggregations": {
          "location_shared_with_count": {
             "doc_count": 6
          },
          "locations_count": {
             "doc_count": 2,
             "locations_aggs": {
                "buckets": [
                   {
                      "key": "Boston",
                      "doc_count": 1,
                      "location_verification_count": {
                         "doc_count": 2
                      }
                   },
                   {
                      "key": "Miami",
                      "doc_count": 1,
                      "location_verification_count": {
                         "doc_count": 0
                      }
                   }
                ]
             }
          }
       }
    }

&lt;/pre&gt;

- Location verification count (doc_count) per location - perfect . So I do get e.g. "Boston" location  was verified by 2 friends (doc_count) .

What I want to do is calculate % verified per location . 
Formula = [ (location_verification_count -&gt; doc_count / location_shared_with_count-&gt;doc_count) x 100 ].

Question is 
- How can I get or use existing doc_count in another sub-aggregation to calculate above % 
- What kind of aggregation type I would use to perform above ?
</description><key id="46640823">8211</key><summary>using doc_count from one aggregation into another aggregation to calculate additional percentage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajeshetty</reporter><labels /><created>2014-10-23T15:49:10Z</created><updated>2014-10-27T09:24:11Z</updated><resolved>2014-10-24T13:55:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-24T13:55:12Z" id="60389691">Hi @rajeshetty 

Please ask questions like these in the mailing list, rather than on the github issues.  That said, you would have to do this calculation application side today.  However when #8110 is merged, you should be able to do things like this.
</comment><comment author="rajeshetty" created="2014-10-24T17:02:50Z" id="60417369">when is 8110 going into main release ?. what release would that be ?
</comment><comment author="jpountz" created="2014-10-24T17:10:26Z" id="60418769">@rajeshetty It is scheduled for elaticsearch 2.0 (see tags of the issue), which I don't see released before **at least** several months.
</comment><comment author="rajeshetty" created="2014-10-24T18:33:46Z" id="60430032">Appreciate your quick response on this. Which mailing list you recommend asking these kind of questions.? - Stackoverflow or somewhere else?. 

You see your answer about new reducers was very helpful even though i can't use it anywhere in coming months till 2.0 is out, but still knowing that ES is working on it is useful , So which forum would give me such a quick turn around on my queries around questions I might have ?
</comment><comment author="clintongormley" created="2014-10-27T09:24:11Z" id="60566678">@rajeshetty http://www.elasticsearch.org/community
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce a RefCounted interface and basic impl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8210</link><project id="" key="" /><description>We already have two places duplicating this rather hairy logic, this
commit intorduces a new RefCoutned interace and an abstract implementation
that can be used for delegation. It factors out all the reference counting
and adds single and multithreaded test for it.
</description><key id="46639318">8210</key><summary>Introduce a RefCounted interface and basic impl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-23T15:36:59Z</created><updated>2015-06-06T19:22:03Z</updated><resolved>2014-10-24T08:44:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-23T15:37:42Z" id="60258495">@bleskes all yours... ;)
</comment><comment author="bleskes" created="2014-10-24T07:23:21Z" id="60353027">LGTM. I like the abstract class + interface approach. Left two little comments.
</comment><comment author="s1monw" created="2014-10-24T08:31:13Z" id="60358736">@bleskes pushed a new commit
</comment><comment author="bleskes" created="2014-10-24T08:33:05Z" id="60358920">@s1monw looks great.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/concurrent/AbstractRefCounted.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/RefCounted.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file><file>src/test/java/org/elasticsearch/common/util/concurrent/RefCountedTest.java</file></files><comments><comment>[UTILITIES] Introduce a RefCounted interface and basic impl</comment></comments></commit></commits></item><item><title>Extended bounds create misaligned empty buckets in date histogram aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8209</link><project id="" key="" /><description>Using a timezone in combination with 'pre_zone_adjust_large_interval' set to true leads to the creation of buckets which are not correctly aligned. 

Example:
Timezone set to CET, interval set to month (march and april are off here):

```
"aggregations" : {
    "histo" : {
      "buckets" : [ {
        "key_as_string" : "2013-12-31T23:00:00.000Z",
        "key" : 1388530800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2014-01-31T23:00:00.000Z",
        "key" : 1391209200000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-02-28T23:00:00.000Z",
        "key" : 1393628400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-03-28T23:00:00.000Z",
        "key" : 1396047600000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-04-28T23:00:00.000Z",
        "key" : 1398726000000,
        "doc_count" : 0
      } ]
    }
  }
```

Reproduction:
https://gist.github.com/miccon/4eecfeafc3a66a9d8b24
</description><key id="46632236">8209</key><summary>Extended bounds create misaligned empty buckets in date histogram aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">miccon</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.5</label><label>v1.5.0</label></labels><created>2014-10-23T14:39:30Z</created><updated>2015-02-23T18:16:43Z</updated><resolved>2015-02-23T18:16:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="miccon" created="2014-10-23T15:00:37Z" id="60252175">This also leads to 'double' buckets during daylight savings time change:

Example with interval set to day:

```
"aggregations" : {
    "histo" : {
      "buckets" : [ {
        "key_as_string" : "2014-10-25T22:00:00.000Z",       // Still daylight saving (midnight 26th)
        "key" : 1414274400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-10-26T22:00:00.000Z",       // Should be 23h in UTC (wrong bucket)
        "key" : 1414360800000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-10-26T23:00:00.000Z",       // Winter time (midnight 27th)
        "key" : 1414364400000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2014-10-27T23:00:00.000Z",
        "key" : 1414450800000,
        "doc_count" : 0
      } ]
    }
  }
```

https://gist.github.com/miccon/de967d67cc8fc80fb3d8
</comment><comment author="miccon" created="2014-10-23T15:04:26Z" id="60252809">In the code the following assertion is triggered (using 1.4, seems to be also present in master)

```
Caused by: java.lang.AssertionError
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.reduce(InternalHistogram.java:368)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:140)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:374)
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction$1.doRun(TransportSearchQueryAndFetchAction.java:83)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    ... 3 more
```
</comment><comment author="wojcikstefan" created="2015-01-13T22:45:16Z" id="69836229">+1

Although the title is misleading - the issue has nothing to do with extended bounds.

Running the commands below:

```
curl -sXDELETE 'localhost:9200/test'
curl -sXPOST 'localhost:9200/test/test/?pretty=true&amp;refresh=true' -d '{"date": "2014-01-01T0:00:00Z"}'
curl -sXPOST 'localhost:9200/test/test/?pretty=true&amp;refresh=true' -d '{"date": "2014-04-01T0:00:00Z"}'
curl -sXPOST 'localhost:9200/test/test/?pretty=true&amp;refresh=true' -d '{"date": "2014-04-30T0:00:00Z"}'
curl -sXGET 'localhost:9200/_search?pretty=true' -d '                                                                                                                                                                                                                                                                                            
{
  "size": 0,
  "aggs": {
    "histo": {
      "date_histogram": {
        "field": "date",
        "interval": "month",
        "pre_zone": "+01:00",
        "pre_zone_adjust_large_interval": true,
        "min_doc_count": 0
      }
    }
  }
}'
```

Results in:

```
{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 127,
    "successful" : 127,
    "failed" : 0
  },
  "hits" : {
    "total" : 30155,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "histo" : {
      "buckets" : [ {
        "key_as_string" : "2013-12-31T23:00:00.000Z",
        "key" : 1388530800000,
        "doc_count" : 1
      }, {
        "key_as_string" : "2014-01-31T23:00:00.000Z",
        "key" : 1391209200000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-02-28T23:00:00.000Z",
        "key" : 1393628400000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-03-28T23:00:00.000Z",
        "key" : 1396047600000,
        "doc_count" : 0
      }, {
        "key_as_string" : "2014-03-31T23:00:00.000Z",
        "key" : 1396306800000,
        "doc_count" : 2
      } ]
    }
  }
}
```

The bucket keys I'd expect are: `2013-12-31T23:00:00.000Z`, `2014-01-31T23:00:00.000Z`, `2014-02-28T23:00:00.000Z`, `2014-03-31T23:00:00.000Z`. Note that this is only an issue with positive `pre_zone` values and `pre_zone_adjust_large_interval` equal to `true`.
</comment><comment author="cbuescher" created="2015-02-20T15:30:01Z" id="75256327">This looks very similar to #9491 and #7673 to me. Probably also fixed the same way. Will see if this behaviour is already fixed with the latest clean up of the `date_histogram` on master.
</comment><comment author="cbuescher" created="2015-02-20T16:12:21Z" id="75264111">I tried the script of @wojcikstefan on current master (7c20a8), works there. However, bug is reproducable on 1.x. Will dig into this further.
</comment><comment author="cbuescher" created="2015-02-23T16:56:17Z" id="75582472">@jpountz Had a look at these cases and why they still don't work on 1.x an 1.4 even after fix from https://github.com/elasticsearch/elasticsearch/pull/9790. The reason is that using `pre_zone_adjust_large_interval = true` for month and bigger intervals forces the TimeZoneRounding implementation to be TimeTimeZoneRoundingFloor. When calculating next buckets keys when inserting empty buckets we should do the adding of the time duration in local time (where also the rounding takes place).
Adding the back and forth conversion for preTz in TimeTimeZoneRoundingFloor.nextRoundingValue solved the issue for me. Will issue a PR if you want to have a look.
</comment><comment author="jpountz" created="2015-02-23T17:40:39Z" id="75591852">@cbuescher +1 on a PR, your description of the fix looks good to me. Thanks for taking care of this!
</comment><comment author="cbuescher" created="2015-02-23T18:16:42Z" id="75598992">Fix on 1.4 branch: e869e90
Fix on 1.x branch: f829670
Only tests on master: 4ef430d
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file></files><comments><comment>[Test] Add `date_histogram` test for time zone corner case</comment></comments></commit></commits></item><item><title>Update pathhierarchy-tokenizer.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8208</link><project id="" key="" /><description>The docs seem to contain a bug regarding the type of the path hierarchy tokenizer.
</description><key id="46621527">8208</key><summary>Update pathhierarchy-tokenizer.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">abraxxa</reporter><labels /><created>2014-10-23T12:59:18Z</created><updated>2016-03-31T08:51:27Z</updated><resolved>2015-06-19T14:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-23T13:55:13Z" id="60241858">Hi @abraxxa 

The documentation is correct as it stands.  We prefer the version with underscores, but we support camel casing as well.

thanks anyway
</comment><comment author="clintongormley" created="2014-10-23T13:56:53Z" id="60242098">@abraxxa Ah I see why you say that, further down the page the docs are incorrect...
</comment><comment author="clintongormley" created="2015-06-19T14:44:50Z" id="113535406">Hmmm I don't see what I meant in https://github.com/elastic/elasticsearch/pull/8208#issuecomment-60242098. All looks good to me.  Closing
</comment><comment author="abraxxa" created="2016-03-31T08:51:27Z" id="203826869">2.3.0 finally fixed this in https://github.com/elastic/elasticsearch/pull/15785.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't catch FNF/NSF exception when reading metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8207</link><project id="" key="" /><description>When reading metadata we do catch FileNotFound and NoSuchFileExceptions
today, log the even and return an empty metadata object. Yet, in some cases
this might be the wrong thing todo ie. if a commit point is provided these
situations are actually an error and should be rethrown. This commit
pushes the responsiblity to the caller to handle this exception.
</description><key id="46620863">8207</key><summary>Don't catch FNF/NSF exception when reading metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>resiliency</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-23T12:52:17Z</created><updated>2015-06-07T18:04:52Z</updated><resolved>2014-10-24T10:25:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-23T14:44:05Z" id="60249490">@bleskes can you take a look at this please?
</comment><comment author="bleskes" created="2014-10-24T08:25:33Z" id="60358242">LGTM. Left two minor comments.
</comment><comment author="s1monw" created="2014-10-24T10:06:36Z" id="60368165">@bleskes pushed a new commit
</comment><comment author="bleskes" created="2014-10-24T10:08:22Z" id="60368320">looks great. Thx @s1monw 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/test/java/org/elasticsearch/index/store/StoreTest.java</file></files><comments><comment>[STORE] Don't catch FNF/NSF exception when reading metadata</comment></comments></commit></commits></item><item><title>Enable ClusterInfoService by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8206</link><project id="" key="" /><description>Since we enabled the disk threshold decider by default, we need to
enable the cluster info service so that disk usages and shard sizes can
be gathered also.

Adds a test that checks that we are gathering information by default.

I also talked to @bleskes about how to test both the
`InternalClusterInfoService` and `DiskThresholdDecider` at an integration
level, and he suggested mocking the responses at the transport layer. I
will work on that for a subsequent pull request after this one.
</description><key id="46615424">8206</key><summary>Enable ClusterInfoService by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-23T11:41:59Z</created><updated>2015-06-07T17:53:44Z</updated><resolved>2014-10-23T11:56:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-23T11:46:21Z" id="60227222">LGTM good catch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fixed typo in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8205</link><project id="" key="" /><description /><key id="46613141">8205</key><summary>fixed typo in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">mikosik</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-23T11:12:09Z</created><updated>2014-10-24T13:36:53Z</updated><resolved>2014-10-24T13:36:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-23T11:38:12Z" id="60226252">Hi @mikosik 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="mikosik" created="2014-10-23T14:42:06Z" id="60249192">Hi @clintongormley 
I've just signed CLA.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: fixed typo in documentation</comment></comments></commit></commits></item><item><title>Missing results when searching with the same snowball analyzer used at index time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8204</link><project id="" key="" /><description>Hi,

if I have a document with `name.en: "Western Europe antiquities"` (analyzed with snowball) and I made a query with the snowball analyzer on "Europe", the document is not found which is very weird.

If I do the same query, but with the standard analyzer, the document is found. If I put the field on the query (`name.en:Europe`) with the snowball analyzer, the document is also found.

I think that the document has a token with the value `europ` at index time, and that the query will use this same token `europ`, so the document should be returned.

I've made a gist to exhibit this issue: https://gist.github.com/nono/40bc5a2f6fd5c0fd86f2
</description><key id="46597522">8204</key><summary>Missing results when searching with the same snowball analyzer used at index time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nono</reporter><labels /><created>2014-10-23T07:49:40Z</created><updated>2014-10-23T09:02:15Z</updated><resolved>2014-10-23T09:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-23T09:00:02Z" id="60210731">Hi @nono 

You are querying the `_all` field, which uses the `standard` analyzer by default.  So `Europe` is indexed as the term `europe`.  Then at search time you are applying the Snowball analyzer to produce `europ`, but that term doesn't exist in the `_all` field.
</comment><comment author="nono" created="2014-10-23T09:02:15Z" id="60210938">Thanks for the answer!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Passing `fielddata_fields` as a non array causes OOM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8203</link><project id="" key="" /><description>If `fielddata_fields` are passed as a simple value instead of an array
we end up in an infinite loop createing parsed elements with null
values.
This commit validates the incoming token
</description><key id="46563512">8203</key><summary>Passing `fielddata_fields` as a non array causes OOM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-22T21:32:58Z</created><updated>2015-06-08T00:21:12Z</updated><resolved>2014-10-23T12:05:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-22T21:34:34Z" id="60159569">thanks to @astefan for finding this
</comment><comment author="jpountz" created="2014-10-22T21:34:56Z" id="60159616">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsParseElement.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>[SEARCH] Passing fieddata_fields as a non array causes OOM</comment></comments></commit></commits></item><item><title>Adds ability to specify 'named' threadpool to searches.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8202</link><project id="" key="" /><description>Named threadpools needs to be configured in yml explicitly .

For eg:

threadpool:
     fred:
        type: fixed
        size: 30

[ Here fred , named threadpool is configured with default size of 30]

SearchRequestBuilder and ScrollRequestBuilder takes threadpool name ,
but defaults to SEARCH. REST API accepts new http parameter 'threadpool'
which can take name of threadpool to run queries.

curl -XPUT "http://localhost:9200/movies/movie/1" -d'

&gt; {
&gt;     "title": "The Godfather",
&gt;     "director": "Francis Ford Coppola",
&gt;     "year": 1972,
&gt;     "genres": ["Crime", "Drama"]
&gt; }'

Call with 'custom' threadpool
++

curl 'localhost:9200/_search?q=*&amp;threadpool=fred'
{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"movies","_type":"movie","_id":"1","_score":1.0,"_source":
{
    "title": "The Godfather",
    "director": "Francis Ford Coppola",
    "year": 1972,
    "genres": ["Crime", "Drama"]
}

Invalid threadpool name
++

 curl 'localhost:9200/_search?q=*&amp;threadpool=pollois
'
{"error":"SearchPhaseExecutionException[Failed to execute phase [query],
all shards failed; shardFailures {[J8vv81u1SKK5BUizD5Yppw][movies][0]:
ElasticsearchIllegalArgumentException[No executor found for
[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][1]:
ElasticsearchIllegalArgumentException[No executor found for
[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][2]:
ElasticsearchIllegalArgumentException[No executor found for
[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][3]:
ElasticsearchIllegalArgumentException[No executor found for
[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][4]:
ElasticsearchIllegalArgumentException[No executor found for
[pollois]]}]","status":400}

default call
++

curl 'localhost:9200/_search?q=*'
{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"movies","_type":"movie","_id":"1","_score":1.0,"_source":
{
    "title": "The Godfather",
    "director": "Francis Ford Coppola",
    "year": 1972,
    "genres": ["Crime", "Drama"]

}

Thanks to way original code is done - _stats shows new threadpool stats
</description><key id="46560516">8202</key><summary>Adds ability to specify 'named' threadpool to searches.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nirmalc</reporter><labels><label>enhancement</label></labels><created>2014-10-22T21:04:32Z</created><updated>2015-03-19T10:19:05Z</updated><resolved>2014-10-24T08:50:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nirmalc" created="2014-10-22T21:06:14Z" id="60155374">SHA: a2be78437f0a3881e6f711f25c9116b86ca12e0c
</comment><comment author="nirmalc" created="2014-10-22T21:08:19Z" id="60155709">@rjernst  - updated pr . Sorry about bad update
</comment><comment author="clintongormley" created="2014-10-24T08:50:19Z" id="60360657">Hi @nirmalc 

Thanks for the PR.  We've had a talk about this today and, while we like the idea in general, this doesn't feel like the right approach.  While it would work fine with just a few users, it wouldn't scale to many users, because the minimum "unit" is one thread.

What you would end up with is thousands of threads, essentially a thread explosion.

And it is not just about threads, it is about resources: IO, CPU, disk space, network etc.  I think this is a much bigger project to manage resources as a whole.  Also, you'd want to be able to change quotas etc on a live cluster, rather than just in config.

We're going to pass on this one, but thanks for submitting the PR
</comment><comment author="nirmalc" created="2014-10-24T14:44:54Z" id="60396677">Thanks @clintongormley  , @rjernst  for reviews.

I agree with your comments - it may not work for everyone. I had to force the config setting to avoid 'thread explosion' you mentioned .

Probably it may be nice to have some feature ( if at all possible)  which throttles query resources by 'profile' of caller app . I have a system which gets around ~ 1.5K queries per second from the web app, it also gets queries from 'writer' app which also does heavy queries - intention was to make sure 'reader' ( in this case web app) are not blocked by 'writer' because of threadpool. 

Thanks again for looking into this.
</comment><comment author="clintongormley" created="2014-10-24T14:48:40Z" id="60397249">@nirmalc agreed, and thanks for submitting the PR - sorry it didn't work out this time :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable indy (invokedynamic) compile flag for Groovy scripts by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8201</link><project id="" key="" /><description>With Java 7 and the `groovy-all-x.y.z-indy` jar, Groovy scripts can be compiled with `invokedynamic` instruction support. Even with the "indy" jar, Groovy does not enable this by default so that Groovy code compiled with these features are compatible with JVMs prior to Java 7.

With this commit, it is turned on by default because Elasticsearch requires Java 7. However, Java 7's invokedynamic support was buggy until Java 7u60 (and later), which means that users of this should update their JVMs to at least that version.

This does provide an unadvertised property (`"script.groovy.indy"`) to disable the feature in the unlikely event that user's do not want it enabled (e.g., future, non-existent JVM bug). Set it to `false` to disable this feature. If disabled, the assumption is that users _must_ replace the jar file with the non-indy variant as well, which is why this is not advertised or dynamically settable.

Closes #8184
</description><key id="46556949">8201</key><summary>Enable indy (invokedynamic) compile flag for Groovy scripts by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Internal</label><label>enhancement</label><label>low hanging fruit</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2014-10-22T20:32:50Z</created><updated>2016-12-08T14:05:18Z</updated><resolved>2015-09-17T20:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-10-22T21:40:01Z" id="60160298">I updated the benchmark script to run a few times in order to collect some more variations. These were run from my laptop (dual core Haswell Core i5 2.8 GHz with HT enabled) with no special JVM settings.

The resulting averages table of just Groovy's milliseconds per request:

![Averages](https://cloud.githubusercontent.com/assets/1501235/4744788/88e2600c-5a33-11e4-8b75-80c2ca04ca9b.png)

The relative improvements with each block show a nice picture for enabling `invokedynamic` support for Groovy (minimum of 15% improvement with no code change), with a pretty glowing view of the Groovy sandbox not seriously impacting performance.

![Relative Improvements](https://cloud.githubusercontent.com/assets/1501235/4744831/cbdec418-5a33-11e4-97f8-58c45722b709.png)

This was built from this table, which used the source data:

&gt; ## Run 1 of 3 (1000000 docs and 1000 queries)
&gt; 
&gt; ## Node Settings: Sandboxed Groovy without indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 10 seconds and 751 milliseconds (10 msec per req)
&gt; native: 26 seconds and 575 milliseconds (26 msec per req)
&gt; groovy: 1 minute, 29 seconds and 376 milliseconds (89 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 22 seconds and 152 milliseconds (22 msec per req)
&gt; native: 44 seconds and 294 milliseconds (44 msec per req)
&gt; groovy: 2 minutes, 46 seconds and 121 milliseconds (166 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 22 seconds and 487 milliseconds (22 msec per req)
&gt; native: 43 seconds and 200 milliseconds (43 msec per req)
&gt; groovy: 2 minutes, 50 seconds and 858 milliseconds (170 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 41 seconds and 253 milliseconds (41 msec per req)
&gt; native: 1 minute, 12 seconds and 672 milliseconds (72 msec per req)
&gt; groovy: 4 minutes, 22 seconds and 717 milliseconds (262 msec per req)
&gt; 
&gt; ## Node Settings: Unsandboxed Groovy without indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 14 seconds and 936 milliseconds (14 msec per req)
&gt; native: 32 seconds and 169 milliseconds (32 msec per req)
&gt; groovy: 1 minute, 13 seconds and 281 milliseconds (73 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 22 seconds and 34 milliseconds (22 msec per req)
&gt; native: 44 seconds and 258 milliseconds (44 msec per req)
&gt; groovy: 2 minutes, 15 seconds and 107 milliseconds (135 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 22 seconds and 846 milliseconds (22 msec per req)
&gt; native: 43 seconds and 54 milliseconds (43 msec per req)
&gt; groovy: 2 minutes, 21 seconds and 616 milliseconds (141 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 41 seconds and 164 milliseconds (41 msec per req)
&gt; native: 1 minute, 10 seconds and 743 milliseconds (70 msec per req)
&gt; groovy: 3 minutes, 39 seconds and 563 milliseconds (219 msec per req)
&gt; 
&gt; ## Node Settings: Sandboxed Groovy with indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 15 seconds and 264 milliseconds (15 msec per req)
&gt; native: 29 seconds and 793 milliseconds (29 msec per req)
&gt; groovy: 1 minute, 9 seconds and 503 milliseconds (69 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 22 seconds and 216 milliseconds (22 msec per req)
&gt; native: 45 seconds and 118 milliseconds (45 msec per req)
&gt; groovy: 2 minutes, 2 seconds and 837 milliseconds (122 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 22 seconds and 943 milliseconds (22 msec per req)
&gt; native: 44 seconds and 314 milliseconds (44 msec per req)
&gt; groovy: 2 minutes, 1 second and 793 milliseconds (121 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 42 seconds and 71 milliseconds (42 msec per req)
&gt; native: 1 minute, 12 seconds and 527 milliseconds (72 msec per req)
&gt; groovy: 3 minutes, 26 seconds and 398 milliseconds (206 msec per req)
&gt; 
&gt; ## Node Settings: Unsandboxed Groovy with indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 15 seconds and 731 milliseconds (15 msec per req)
&gt; native: 31 seconds and 633 milliseconds (31 msec per req)
&gt; groovy: 1 minute, 12 seconds and 80 milliseconds (72 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 23 seconds and 490 milliseconds (23 msec per req)
&gt; native: 47 seconds and 746 milliseconds (47 msec per req)
&gt; groovy: 2 minutes, 7 seconds and 558 milliseconds (127 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 23 seconds and 917 milliseconds (23 msec per req)
&gt; native: 45 seconds and 803 milliseconds (45 msec per req)
&gt; groovy: 2 minutes, 8 seconds and 733 milliseconds (128 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 53 seconds and 248 milliseconds (53 msec per req)
&gt; native: 1 minute, 21 seconds and 260 milliseconds (81 msec per req)
&gt; groovy: 3 minutes, 15 seconds and 172 milliseconds (195 msec per req)
&gt; 
&gt; ## Run 2 of 3 (1000000 docs and 1000 queries)
&gt; 
&gt; ## Node Settings: Sandboxed Groovy without indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 15 seconds and 529 milliseconds (15 msec per req)
&gt; native: 34 seconds and 297 milliseconds (34 msec per req)
&gt; groovy: 1 minute, 17 seconds and 85 milliseconds (77 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 22 seconds and 515 milliseconds (22 msec per req)
&gt; native: 46 seconds and 519 milliseconds (46 msec per req)
&gt; groovy: 2 minutes, 17 seconds and 579 milliseconds (137 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 23 seconds and 95 milliseconds (23 msec per req)
&gt; native: 46 seconds and 990 milliseconds (46 msec per req)
&gt; groovy: 2 minutes, 22 seconds and 289 milliseconds (142 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 41 seconds and 803 milliseconds (41 msec per req)
&gt; native: 1 minute, 13 seconds and 916 milliseconds (73 msec per req)
&gt; groovy: 3 minutes, 37 seconds and 559 milliseconds (217 msec per req)
&gt; 
&gt; ## Node Settings: Unsandboxed Groovy without indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 16 seconds and 74 milliseconds (16 msec per req)
&gt; native: 33 seconds and 230 milliseconds (33 msec per req)
&gt; groovy: 1 minute, 13 seconds and 971 milliseconds (73 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 20 seconds and 203 milliseconds (20 msec per req)
&gt; native: 42 seconds and 123 milliseconds (42 msec per req)
&gt; groovy: 2 minutes, 11 seconds and 736 milliseconds (131 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 20 seconds and 742 milliseconds (20 msec per req)
&gt; native: 42 seconds and 550 milliseconds (42 msec per req)
&gt; groovy: 2 minutes, 19 seconds and 255 milliseconds (139 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 33 seconds and 605 milliseconds (33 msec per req)
&gt; native: 1 minute, 3 seconds and 84 milliseconds (63 msec per req)
&gt; groovy: 3 minutes, 31 seconds and 267 milliseconds (211 msec per req)
&gt; 
&gt; ## Node Settings: Sandboxed Groovy with indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 17 seconds and 502 milliseconds (17 msec per req)
&gt; native: 33 seconds and 224 milliseconds (33 msec per req)
&gt; groovy: 1 minute, 11 seconds and 869 milliseconds (71 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 19 seconds and 918 milliseconds (19 msec per req)
&gt; native: 42 seconds and 244 milliseconds (42 msec per req)
&gt; groovy: 2 minutes, 2 seconds and 580 milliseconds (122 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 20 seconds and 473 milliseconds (20 msec per req)
&gt; native: 42 seconds and 399 milliseconds (42 msec per req)
&gt; groovy: 2 minutes, 6 seconds and 716 milliseconds (126 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 32 seconds and 756 milliseconds (32 msec per req)
&gt; native: 1 minute, 5 seconds and 636 milliseconds (65 msec per req)
&gt; groovy: 3 minutes, 3 seconds and 92 milliseconds (183 msec per req)
&gt; 
&gt; ## Node Settings: Unsandboxed Groovy with indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 15 seconds and 702 milliseconds (15 msec per req)
&gt; native: 35 seconds and 112 milliseconds (35 msec per req)
&gt; groovy: 1 minute, 17 seconds and 381 milliseconds (77 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 23 seconds and 3 milliseconds (23 msec per req)
&gt; native: 51 seconds and 240 milliseconds (51 msec per req)
&gt; groovy: 2 minutes, 7 seconds and 706 milliseconds (127 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 25 seconds and 145 milliseconds (25 msec per req)
&gt; native: 50 seconds and 103 milliseconds (50 msec per req)
&gt; groovy: 2 minutes, 5 seconds and 988 milliseconds (125 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 42 seconds and 114 milliseconds (42 msec per req)
&gt; native: 1 minute, 15 seconds and 353 milliseconds (75 msec per req)
&gt; groovy: 3 minutes, 12 seconds and 353 milliseconds (192 msec per req)
&gt; 
&gt; ## Run 3 of 3 (1000000 docs and 1000 queries)
&gt; 
&gt; ## Node Settings: Sandboxed Groovy without indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 16 seconds and 321 milliseconds (16 msec per req)
&gt; native: 34 seconds and 888 milliseconds (34 msec per req)
&gt; groovy: 1 minute, 19 seconds and 137 milliseconds (79 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 23 seconds and 379 milliseconds (23 msec per req)
&gt; native: 46 seconds and 573 milliseconds (46 msec per req)
&gt; groovy: 2 minutes, 20 seconds and 561 milliseconds (140 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 24 seconds and 99 milliseconds (24 msec per req)
&gt; native: 47 seconds and 93 milliseconds (47 msec per req)
&gt; groovy: 2 minutes, 23 seconds and 963 milliseconds (143 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 42 seconds and 178 milliseconds (42 msec per req)
&gt; native: 1 minute, 15 seconds and 24 milliseconds (75 msec per req)
&gt; groovy: 3 minutes, 42 seconds and 19 milliseconds (222 msec per req)
&gt; 
&gt; ## Node Settings: Unsandboxed Groovy without indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 15 seconds and 655 milliseconds (15 msec per req)
&gt; native: 33 seconds and 170 milliseconds (33 msec per req)
&gt; groovy: 1 minute, 13 seconds and 793 milliseconds (73 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 23 seconds and 125 milliseconds (23 msec per req)
&gt; native: 46 seconds and 851 milliseconds (46 msec per req)
&gt; groovy: 2 minutes, 17 seconds and 772 milliseconds (137 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 23 seconds and 778 milliseconds (23 msec per req)
&gt; native: 47 seconds and 686 milliseconds (47 msec per req)
&gt; groovy: 2 minutes, 29 seconds and 45 milliseconds (149 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 45 seconds and 263 milliseconds (45 msec per req)
&gt; native: 1 minute, 16 seconds and 78 milliseconds (76 msec per req)
&gt; groovy: 3 minutes, 50 seconds and 923 milliseconds (230 msec per req)
&gt; 
&gt; ## Node Settings: Sandboxed Groovy with indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 16 seconds and 572 milliseconds (16 msec per req)
&gt; native: 33 seconds and 833 milliseconds (33 msec per req)
&gt; groovy: 1 minute, 12 seconds and 761 milliseconds (72 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 23 seconds and 72 milliseconds (23 msec per req)
&gt; native: 48 seconds and 509 milliseconds (48 msec per req)
&gt; groovy: 2 minutes, 6 seconds and 71 milliseconds (126 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 23 seconds and 901 milliseconds (23 msec per req)
&gt; native: 47 seconds and 190 milliseconds (47 msec per req)
&gt; groovy: 2 minutes, 6 seconds and 72 milliseconds (126 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 42 seconds and 36 milliseconds (42 msec per req)
&gt; native: 1 minute, 15 seconds and 400 milliseconds (75 msec per req)
&gt; groovy: 3 minutes, 11 seconds and 953 milliseconds (191 msec per req)
&gt; 
&gt; ## Node Settings: Unsandboxed Groovy with indy
&gt; 
&gt; ---
&gt; 
&gt; ## Script: x
&gt; 
&gt; expression: 16 seconds and 602 milliseconds (16 msec per req)
&gt; native: 35 seconds and 181 milliseconds (35 msec per req)
&gt; groovy: 1 minute, 15 seconds and 367 milliseconds (75 msec per req)
&gt; 
&gt; ## Script: x + y
&gt; 
&gt; expression: 22 seconds and 331 milliseconds (22 msec per req)
&gt; native: 46 seconds and 418 milliseconds (46 msec per req)
&gt; groovy: 2 minutes, 7 seconds and 320 milliseconds (127 msec per req)
&gt; 
&gt; ## Script: 1.2 \* x / y
&gt; 
&gt; expression: 22 seconds and 130 milliseconds (22 msec per req)
&gt; native: 45 seconds and 465 milliseconds (45 msec per req)
&gt; groovy: 2 minutes, 6 seconds and 104 milliseconds (126 msec per req)
&gt; 
&gt; ## Script: sqrt(abs(z)) + ln(abs(x \* y))
&gt; 
&gt; expression: 39 seconds and 82 milliseconds (39 msec per req)
&gt; native: 1 minute, 11 seconds and 214 milliseconds (71 msec per req)
&gt; groovy: 3 minutes, 11 seconds and 931 milliseconds (191 msec per req)
</comment><comment author="pickypg" created="2015-02-03T04:40:50Z" id="72591771">Since #9520 was merged, I should probably update this (now simplified PR without the dependency portion) and add it to 1.5.
</comment><comment author="clintongormley" created="2015-06-04T19:25:20Z" id="109019180">@pickypg any chance we can get this merged for 2.0?
</comment><comment author="pickypg" created="2015-06-04T19:26:22Z" id="109019425">@clintongormley Yeah, I'll get this updated and merged by the weekend!
</comment><comment author="rmuir" created="2015-09-16T15:57:02Z" id="140786109">looks good to me. 
</comment><comment author="rmuir" created="2015-09-16T15:59:08Z" id="140787119">maybe get someone else to review security changes (they are pretty simple). But otherwise this is a good one to stop using those crazy caches, we should go into master soon at least, I will be looking at heavy work on these scripting integrations soon to deal with some of their security issues.
</comment><comment author="jaymode" created="2015-09-16T16:36:37Z" id="140796806">LGTM
</comment><comment author="karmi" created="2016-12-08T13:58:35Z" id="265745692">Hi @pickypg, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git [commit](https://github.com/elastic/elasticsearch/pull/8201.patch). Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?</comment><comment author="karmi" created="2016-12-08T14:05:18Z" id="265747199">@pickypg, apologies for the notification above, this is a fault from some manual testing... please ignore it.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/bootstrap/ESPolicy.java</file><file>core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java</file></files><comments><comment>Merge pull request #8201 from pickypg/feature/groovy-compile-indy-8184</comment></comments></commit></commits></item><item><title>Date histogram aggregation Milliseconds problem using minutes in date_range</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8200</link><project id="" key="" /><description>When using minutes in the date range the returned keys are wrong (minutes and milliseconds).

Using the query according to:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-daterange-aggregation.html

I'm using ElasticSearch Version 1.3.2.

Example query:
"date_range":{
"field":"doc_published","
format":"dateOptionalTime",
"ranges":[
{"from":"20140101T00:10:00","to":"20140101T00:15:00"},{"from":"20140101T00:15:00","to":"20140101T00:20:00"}]
}

Example response:
{"key":"20140101-01-01T00:10:00.064Z-20140101-01-01T00:14:59.968Z",
"from":6.3549803335740006E17,
"from_as_string":"20140101-01-01T00:10:00.064Z",
"to":6.3549803335769997E17,
"to_as_string":"20140101-01-01T00:14:59.968Z"},

{"key":"20140101-01-01T00:14:59.968Z-20140101-01-01T00:20:00.000Z",
"from":6.3549803335769997E17,
"from_as_string":"20140101-01-01T00:14:59.968Z",
"to":6.35498033358E17,
"to_as_string":"20140101-01-01T00:20:00.000Z"}

The returning keys are invalid:
RECEIVED: 20140101-01-01T00:10:00.064Z-20140101-01-01T00:14:59.968Z
Should be: 
20140101-01-01T00:10:00.000Z-20140101-01-01T00:14:59.968Z 

RECEIVED: 20140101-01-01T00:14:59.968Z-20140101-01-01T00:20:00.000Z
Should be: 
20140101-01-01T00:15:00.000Z-20140101-01-01T00:20:00.000Z  
</description><key id="46554757">8200</key><summary>Date histogram aggregation Milliseconds problem using minutes in date_range</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">deniseoboschetti</reporter><labels /><created>2014-10-22T20:13:14Z</created><updated>2015-11-21T20:06:18Z</updated><resolved>2015-11-21T20:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T20:06:18Z" id="158678072">I'm unable to reproduce this in 2.1.0 or in 1.3.2.  I'm going to close this, but please feel free to reopen if you can replicate.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: Improve range tests to check inclusive/exclusive on ends.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8199</link><project id="" key="" /><description /><key id="46552185">8199</key><summary>Tests: Improve range tests to check inclusive/exclusive on ends.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2014-10-22T19:48:58Z</created><updated>2015-01-21T23:22:21Z</updated><resolved>2014-10-22T22:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-22T20:13:42Z" id="60147431">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java</file></files><comments><comment>Tests: Improve range tests to check inclusive/exclusive on ends.</comment></comments></commit></commits></item><item><title>Non-deterministic inclusion of empty string by exists filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8198</link><project id="" key="" /><description>This happens with 1.3.4. Repeating these steps enough times I sometimes get both documents returned by the filter and sometimes only the one with the non-empty string.

Delete the index

```
$ curl -XDELETE localhost:9200/test
```

Insert 2 documents using bulk API (one has empty string and one does not)

```
$ cat requests
    { "index" : { "_index" : "test", "_type" : "test", "_id" : "1" } }
    { "title": "" }
    { "index" : { "_index" : "test", "_type" : "test", "_id" : "2" } }
    { "title": "Test document" }
$ curl -XPOST localhost:9200/_bulk --data-binary @requests
```

This filter sometimes returns only document 2 and sometimes both

```
$ curl -XPOST localhost:9200/test/_search?pretty -d '
{
    "filter": {
        "exists": {
            "field": "title"
        }
    }
}'
```

I suspect it may have to do with the order in which the documents are indexed by the bulk API. But in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_batch_processing.html it is mentioned that "The bulk API executes all the actions sequentially and in order" so I am wondering if this is a bug.
</description><key id="46540800">8198</key><summary>Non-deterministic inclusion of empty string by exists filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">takism1</reporter><labels><label>discuss</label></labels><created>2014-10-22T18:04:39Z</created><updated>2014-11-04T16:16:21Z</updated><resolved>2014-11-04T16:16:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-22T22:07:55Z" id="60163789">I think this is due to the fact that elasticsearch does not generate dynamic mappings for the empty string. Additionally, your documents likely go to different shards, so they are processed in parallel, the order is not defined.

The mapping for `title` is created by the document that has a non empty title. And sometimes it occurs before the document with an empty title is indexed (when you have 2 matches), sometimes after (when you have 1 match).

I believe this would be fixed by creating dynamic mappings for empty strings but I'm unsure about side effects it could have.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/dynamic/DynamicMappingTests.java</file></files><comments><comment>Mappings: Generate dynamic mappings for empty strings.</comment></comments></commit></commits></item><item><title>Fix CompletionFieldMapper to correctly parse weight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8197</link><project id="" key="" /><description>Allows weight to be defined as a string representation of a positive integer

closes #8090
</description><key id="46540054">8197</key><summary>Fix CompletionFieldMapper to correctly parse weight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-22T17:57:25Z</created><updated>2015-06-08T00:42:04Z</updated><resolved>2014-10-28T22:50:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-22T18:14:12Z" id="60129669">I left some comments.  The main thing is I would simplify by separating the string/number case, since they really are different other than range checking of the value.
</comment><comment author="areek" created="2014-10-22T21:09:12Z" id="60155827">@rjernst Thanks for the feedback, I have updated the PR accordingly.
</comment><comment author="rjernst" created="2014-10-22T22:30:01Z" id="60166293">LGTM, just one more minor comment.
</comment><comment author="areek" created="2014-10-23T19:59:42Z" id="60299526">@rjernst Thanks!
I think this is ready, I will merge the fix to all the appropriate branches after a couple of hours, if there is no more objections.
</comment><comment author="clintongormley" created="2014-10-28T11:04:59Z" id="60739333">@areek you want to get this merged in?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>logstash multiline filter: last part of message flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8196</link><project id="" key="" /><description /><key id="46532493">8196</key><summary>logstash multiline filter: last part of message flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tudit</reporter><labels /><created>2014-10-22T16:47:02Z</created><updated>2014-10-22T16:56:13Z</updated><resolved>2014-10-22T16:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-22T16:54:21Z" id="60117351">Hi @tudit 

You should open this issue on https://github.com/elasticsearch/logstash/issues
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8195</link><project id="" key="" /><description /><key id="46531552">8195</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">cmpich</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-22T16:38:23Z</created><updated>2014-10-29T14:04:40Z</updated><resolved>2014-10-29T14:04:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update getting-started.asciidoc</comment></comments></commit></commits></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8194</link><project id="" key="" /><description /><key id="46531231">8194</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">cmpich</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-22T16:35:27Z</created><updated>2014-10-29T14:01:50Z</updated><resolved>2014-10-29T14:01:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-22T16:39:40Z" id="60115023">Hi @cmpich 

Thanks for the fix. Please can I ask you to sign the CLA so that I can get this merged in?
http://www.elasticsearch.org/contributor-agreement/
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update getting-started.asciidoc</comment></comments></commit></commits></item><item><title>Cluster Update Settings timeout shouldn't be a 503</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8193</link><project id="" key="" /><description>Our standard procedure for doing a full cluster restart is to disable shard allocation, restart, wait for all nodes to come up, and then reenable allocation.

Unfortunately with many shards and many nodes (3 masters, 42 data nodes, and 8800 primaries and replicas) the cluster update to enable allocation can timeout.

```
curl --silent -XPUT  "http://esm1.blah.wordpress.com:9200/_cluster/settings" -d '{ "persistent" : { "cluster.routing.allocation.enable" : "all" } }'
```

```
{"error":"ProcessClusterEventTimeoutException[failed to process cluster event (cluster_update_settings) within 30s]","status":503}
```

When I got this error I assumed it meant the setting was not applied. However, it looks like the master just keeps retrying to set it because eventually the nodes did start allocating shards. The 503 error makes it feel like something is broken when it is not actually.

This is running ES 1.3.4.
</description><key id="46528965">8193</key><summary>Cluster Update Settings timeout shouldn't be a 503</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels><label>:REST</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-10-22T16:15:00Z</created><updated>2015-04-04T12:59:51Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-22T16:33:49Z" id="60114203">@glbrown when you make the change the master notifies all the nodes about it and waits for, by default, 30s for them to process it. If that doesn't happen you see the response you get. However, the nodes were notified and will typically get to it and some point. This is a super light operation, so it should be very quick to complete. Any idea why it's slow? Are the nodes under load or share the machine with something? (it's enough to have one node not respond in a timely fashion for this to happen). 

Also, if you have anything non-default in your settings it would be great if you shared it.
</comment><comment author="gibrown" created="2014-10-22T17:03:35Z" id="60118828">There's only a couple of odd things in my config that could be related. The cluster is spread across multiple data centers so we have some increased timeouts:

```
discovery:
  zen:
    minimum_master_nodes: 2
    ping:
      timeout: 10s
      multicast.enabled: false
      unicast.hosts: [...]
    fd:
      ping_interval: 15s
      ping_timeout: 60s
      ping_retries: 5
```

When we do the cluster update all of the nodes should have been up for at least 30 seconds. Also subsequent calls to reapply the setting receive the same error. My guess is that the nodes start to get busy recovering all of the shards and data and that causes them to not respond to the master very promptly because they are busy initializing. When the cluster is fully recovered I don't see any problems changing this setting.

It feels like if all the master eligible nodes have acknowledged getting the new setting then the response should be a 200. The masters can then deal with retrying to make sure that all data nodes have been informed. I'm sure I'm missing some details though.
</comment><comment author="pickypg" created="2015-02-04T17:40:34Z" id="72901012">I have observed this on a cluster that was unresponsive due to a long pending tasks list during a large recovery and the setting never appears to "take" later.
</comment><comment author="s1monw" created="2015-02-20T09:33:24Z" id="75211196">@gibrown going back to the original issue would you expect a `408` instead a `503`?
</comment><comment author="gibrown" created="2015-03-06T20:27:31Z" id="77629584">@s1monw sorry for the slow response, I broke my GH notifications. :)

408 maybe makes sense if I can resend the request and will eventually get a 200.

Could also be a 202 Accepted response. Ideally I shouldn't have to do anything more for the request to complete processing. I guess there are cases where you want to know that it has completed though. Maybe if it has actually completed there should be a 200 response, and if it doesn't complete in a few seconds, then return 202 and the client can choose to send again and wait for a 200 if it cares?

For this particular use case I don't care. I go into a polling loop after enabling allocation and wait for the cluster to be green.
</comment><comment author="bleskes" created="2015-03-07T09:17:06Z" id="77680850">The 4xx region indicates client side errors, so I don't feel 408 describes the situation correctly, despite of the timeout name. +1 to 202 (accepted for processing but not completed)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search: Expose Lucene's searchAfter in the search API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8192</link><project id="" key="" /><description>We already integrated IndexSearcher.searchAfter in https://github.com/elasticsearch/elasticsearch/issues/4940 in order to make deep pagination more efficient with the scroll API. However, in quite a number of cases using the scroll API is not possible as it is heavy and requires to associate a scroll context to each search request and to clear this context when it is not needed anymore.

So if you have a user-facing application that needs to perform deep pagination, performance is terrible because of the pagination, and you cannot really use the scroll API since scroll contexts are costly and users typically don't explicitely tell the application when they don't need the context anymore.

A middle ground could be to allow configuring an array of sort values, and we would only search after these sort values. Compared to the scroll API, it would have the downside of not always requesting the same point-in-time view of a shard, so you can miss documents because of deletes or see documents twice because of insertions, but you already have this issue when paginating using from/size. On the other hand, performance could be much better since it would allow to manage smaller priority queues on each shard.

NOTE: in order for this feature to work well with pagination, the _uid should be used as a last element of the sort specification. Otherwise the sort order for documents that have the same sort values would be undefined (it is defined in the case of Lucene using doc ids, but this doesn't work with elasticsearch because we do not always query the same shard, and if a merge happens between 2 requests, doc ids could be reordered)
</description><key id="46524306">8192</key><summary>Search: Expose Lucene's searchAfter in the search API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>feature</label><label>v5.0.0-alpha1</label></labels><created>2014-10-22T15:35:25Z</created><updated>2016-01-27T10:10:13Z</updated><resolved>2016-01-27T10:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-10-22T15:54:00Z" id="60107784">Neat!
</comment><comment author="clintongormley" created="2014-10-22T16:28:31Z" id="60113419">Related to #7881 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>core/src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>core/src/main/java/org/elasticsearch/search/searchafter/SearchAfterBuilder.java</file><file>core/src/test/java/org/elasticsearch/search/basic/TransportSearchFailuresIT.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/searchafter/SearchAfterBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/searchafter/SearchAfterIT.java</file><file>test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Add search_after parameter in the Search API.</comment><comment>The search_after parameter provides a way to efficiently paginate from one page to the next. This parameter accepts an array of sort values, those values are then used by the searcher to sort the top hits from the first document that is greater to the sort values.</comment><comment>This parameter must be used in conjunction with the sort parameter, it must contain exactly the same number of values than the number of fields to sort on.</comment></comments></commit></commits></item><item><title>Aggregations: new “Sampler” provides a filter for top-scoring docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8191</link><project id="" key="" /><description>Used to limit processing in child aggregations.

The existing Aggregator base class support for deferring computation of child aggs is modified (DeferringBucketCollector is refactored to be abstract and the logic it had for “best bucket” trimming is pushed down into a new BestBucketsDeferringCollector while the new logic for trimming based on doc score quality is in subclass BestDocsDeferringCollector).
Closes #8108
</description><key id="46509257">8191</key><summary>Aggregations: new “Sampler” provides a filter for top-scoring docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2014-10-22T13:29:06Z</created><updated>2015-05-29T15:08:06Z</updated><resolved>2015-05-26T08:14:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-10-22T13:30:05Z" id="60084304">@jpountz Would appreciate you taking a look if you have time
</comment><comment author="jpountz" created="2014-10-26T21:44:22Z" id="60533618">I'm wondering how we could allow for other modes of sampling in the future. For instance this mode is focused on quality and replays the top documents to the sub aggregators but we could also imagine having a sampling aggregation that would only forward every N-th document to the sub aggs (for speed reasons as it would certainly not help quality). So maybe we could rename it to something like `top_docs_sampler` or make the configuration allow for different modes of sampling in the future?

The `relative_to_max_score` option embarrasses me a bit since it is generally considered a bad practice to compare absolute values of scores?
</comment><comment author="markharwood" created="2014-11-03T14:36:34Z" id="61486020">&gt; we could also imagine having a sampling aggregation that would only forward every N-th document to the sub aggs (for speed reasons as it would certainly not help quality)

If the sampling is being done for reasons of speed rather than quality then you'll want to cap the number of docs sampled with some accuracy. Taking every N-th doc is probably not the best way to achieve this as we typically don't know how many docs will match a query. It's probably better to take a random sample of a fixed size - this can be achieved by using the existing priority queue impl with a fixed size and populating it with matches that have random scores.

&gt; The relative_to_max_score option embarrasses me a bit since it is generally considered a bad practice to compare absolute values of scores?

The only alternative means of applying a quality-based filter is to tighten up the query using min_should_match or similar and this is not always easy to control or for users to understand. 
Imagine a product-recommendation scenario where the search criteria is a list of movies you like and you are summarising matches on "user-profile" docs, each of which contain a user's viewed movies.
You would hit a lot of docs (potentially every one!) but would like to focus on the users with similar interests and see which of their movie choices are "uncommonly common" using a significant terms agg. 
If we filter based purely on score we get the following useful ranking factors:
1) _IDF_ means that we will focus on the users who share your interests in obscure movies rather than the movies everyone in the world tends to like (e.g. Star Wars or Shawshank redemption)
2) _coord_ means we will focus on users who match more of the movies you like
3) _norms_ means we will avoid users who list a huge number of movies

The large deviation in scores produced by these factors could provide a useful way to separate the strongly and weakly matched users.
If we don't filter based on score then we would have to find a way to "tighten up" the query to have a hard cut-off which trims the long-tail of weak matches. Criteria settings like `min_should_match` can be used to do this but they don't address all 3 of the ranking concerns above and setting the wrong value could mean you get no results at all.
There isn't a great answer here but I think the score-based filter is the least-worst option.
</comment><comment author="markharwood" created="2014-11-04T14:00:49Z" id="61642878">For the record, scoring (as in use of IDF, norms and coord) seems impossible if your content is numerics as in my movie-lists recommendation example above. The fact that the field type is an integer means `match` and `terms` queries always assume a ConstantScoreQuery is to be used. MLT doesn't work with numerics. To work around this I had to re-index my numeric values as strings.
</comment><comment author="markharwood" created="2014-11-05T17:16:52Z" id="61845339">Rebased on latest master. Removed option of "relative_to_max_score", added suggested option of random sampling.
</comment><comment author="jpountz" created="2014-11-09T22:50:21Z" id="62323785">@markharwood Thanks for your explanations, this aggregation is making more and more sense to me. I left some comments about the configuration of this new agg.
</comment><comment author="markharwood" created="2014-11-20T16:29:21Z" id="63835739">The "diversity" feature I mentioned in comments above is dependent on this proposed Lucene feature: https://issues.apache.org/jira/browse/LUCENE-6066
</comment><comment author="markharwood" created="2014-12-10T17:35:49Z" id="66490468">Some use cases that are coming up: https://groups.google.com/forum/#!topic/elasticsearch/iNA75M3xkZ4
https://groups.google.com/forum/#!topic/elasticsearch/F6iS3kpTWUA
</comment><comment author="markharwood" created="2015-01-06T17:36:31Z" id="68900378">Added thought - as a result of poking around the time-checking features involved in https://github.com/elasticsearch/elasticsearch/issues/9156 it looks like it should also be possible to constrain a sample by max processing time rather than volume of documents as suggested here. SearchContext includes a timer that can be used to check elapsed time efficiently.
</comment><comment author="clintongormley" created="2015-05-25T18:35:08Z" id="105286283">@markharwood Now that #10221 is in, we can close this PR, no?
</comment><comment author="markharwood" created="2015-05-26T08:14:44Z" id="105437732">Pushed to master https://github.com/elastic/elasticsearch/commit/63db34f649d1de038c30d61f3c5e17e059b19b69
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add rebalance enabled allocation decider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8190</link><project id="" key="" /><description>This commit adds the ability to enable / disable relocations
on an entire cluster or on individual indices for either:
- `primaries` - only primaries can rebalance
- `replica` - only replicas can rebalance
- `all` - everything can rebalance (default)
- `none` - all rebalances are disabled

similar to the allocation enable / disable functionality.

Relates to #7288
</description><key id="46497119">8190</key><summary>Add rebalance enabled allocation decider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>:Recovery</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-22T10:59:51Z</created><updated>2015-10-19T06:06:25Z</updated><resolved>2014-10-23T12:12:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-10-22T12:49:50Z" id="60079118">This LGTM, I left some minor comments, how do you feel about adding an integration test in addition to the unit test?
</comment><comment author="s1monw" created="2014-10-22T13:06:48Z" id="60081179">I will add an integration test, thanks
</comment><comment author="s1monw" created="2014-10-22T14:38:37Z" id="60094858">@dakrone addressed all your comments
</comment><comment author="dakrone" created="2014-10-23T09:54:27Z" id="60216589">Left a minor grammatical comment, this LGTM
</comment><comment author="s1monw" created="2014-10-23T11:47:37Z" id="60227318">addressed all your comments @bleskes @dakrone can you take another look
</comment><comment author="dakrone" created="2014-10-23T11:56:06Z" id="60228124">Whoops, I commented on the "second review round" (806d78d) commit instead of the PR, sorry :-/
</comment><comment author="s1monw" created="2014-10-23T12:01:36Z" id="60228594">fixed @dakrone thanks!
</comment><comment author="dakrone" created="2014-10-23T12:01:52Z" id="60228623">LGTM!
</comment><comment author="cfeio" created="2015-02-17T21:43:04Z" id="74760354">What is the status of the support of this feature? Is this available in any versions of ElasticSearch? 
</comment><comment author="geekpete" created="2015-10-19T06:06:25Z" id="149111000">Hi Team,

Would this new relocation control directive compliment or replace setting cluster.routing.allocation.cluster_concurrent_rebalance to 0 as in existing ES versions to prevent unwanted relocation activity?

Or will it just give more fine grained control over relocation activity than only at cluster level?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add tests with forcibly corrupted non-fsynced translog contents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8189</link><project id="" key="" /><description>Currently the translog is fsynced every 5 seconds, in our tests in order to detect places where we do not handle corruption correctly we should corrupt the non-fsynced portion.
</description><key id="46488648">8189</key><summary>Add tests with forcibly corrupted non-fsynced translog contents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>adoptme</label><label>test</label></labels><created>2014-10-22T09:18:51Z</created><updated>2015-11-21T19:57:20Z</updated><resolved>2015-11-21T19:57:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T19:57:20Z" id="158677659">No longer relevant. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>highlighting the char array of  boundaryChars should be the array of Character?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8188</link><project id="" key="" /><description> "highlight" : {
    "fields" : {
      "content1" : {
        "pre_tags" : [ "&lt;span&gt;" ],
        "post_tags" : [ "&lt;/span&gt;" ],
        "fragment_size" : 150,
        "number_of_fragments" : 1,
        "boundary_max_scan" : 50,
        "boundary_chars" : "[C@18442484",
        "type" : "fvh"
      }
    }
  }
</description><key id="46478988">8188</key><summary>highlighting the char array of  boundaryChars should be the array of Character?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">linlihuiyang</reporter><labels /><created>2014-10-22T07:08:33Z</created><updated>2014-10-22T15:50:29Z</updated><resolved>2014-10-22T15:50:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-22T15:50:29Z" id="60107202">According to the docs, it should be a single string containing all of the characters.
See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#boundary-characters
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> nested: ElasticsearchParseException[Expected field name but got START_OBJECT \"range\"]; }]",</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8187</link><project id="" key="" /><description>```
GET guba/post/_search
{
  "query":{
    "filtered": {
      "filter": {
        "term":{
          "sentiment": 1,
          "has_sentiment": 1,
          "ad": 1
        }
      }
    },
    "range": {
      "releaseTime": {
        "gte": "2014-10-22 12:00:00",
        "lte": "2014-10-22 12:30:00"
      }
    }
  }
}
```

{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[a1Os1jrGRgGclG4LuASCyA][guba][1]: RemoteTransportException[[ubuntu4][inet[/219.224.135.48:9300]][search/phase/query]]; nested: SearchParseException[[guba][1]: query[ConstantScore(cache(ad:[1 TO 1]))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\":{\n    \"filtered\": {\n      \"filter\": {\n        \"term\":{\n          \"sentiment\": 1,\n          \"has_sentiment\": 1,\n          \"ad\": 1\n        }\n      }\n    },\n    \"range\": {\n      \"releaseTime\": {\n        \"gte\": \"2014-10-22 12:00:00\",\n        \"lte\": \"2014-10-22 12:30:00\"\n      }\n    }\n  }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got START_OBJECT \"range\"]; }{[z2hlGPmnS6O7yu-d7zUYzw][guba][2]: SearchParseException[[guba][2]: query[ConstantScore(cache(ad:[1 TO 1]))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\":{\n    \"filtered\": {\n      \"filter\": {\n        \"term\":{\n          \"sentiment\": 1,\n          \"has_sentiment\": 1,\n          \"ad\": 1\n        }\n      }\n    },\n    \"range\": {\n      \"releaseTime\": {\n        \"gte\": \"2014-10-22 12:00:00\",\n        \"lte\": \"2014-10-22 12:30:00\"\n      }\n    }\n  }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got START_OBJECT \"range\"]; }{[a1Os1jrGRgGclG4LuASCyA][guba][0]: RemoteTransportException[[ubuntu4][inet[/219.224.135.48:9300]][search/phase/query]]; nested: SearchParseException[[guba][0]: query[ConstantScore(cache(ad:[1 TO 1]))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\":{\n    \"filtered\": {\n      \"filter\": {\n        \"term\":{\n          \"sentiment\": 1,\n          \"has_sentiment\": 1,\n          \"ad\": 1\n        }\n      }\n    },\n    \"range\": {\n      \"releaseTime\": {\n        \"gte\": \"2014-10-22 12:00:00\",\n        \"lte\": \"2014-10-22 12:30:00\"\n      }\n    }\n  }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got START_OBJECT \"range\"]; }]",
   "status": 400
}
</description><key id="46472931">8187</key><summary> nested: ElasticsearchParseException[Expected field name but got START_OBJECT \"range\"]; }]",</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">linhaobuaa</reporter><labels /><created>2014-10-22T05:03:00Z</created><updated>2014-10-22T06:29:19Z</updated><resolved>2014-10-22T06:29:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-22T06:29:19Z" id="60042131">Please use the mailing list for questions. We will help you there.
This place is only for bugs and feature requests.

That said, your query is incorrect. Have a look at the QueryDSL in docs. If you want to have multiple filters, you could use a BoolFilter to wrap all clauses.

Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Validates bool values in yaml for node settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8186</link><project id="" key="" /><description>- Added parseBooleanExact in booleans which throws exception in case of
  parse failure
- Used ParseExact in static's to make it consistent

Closes #8097 
</description><key id="46465704">8186</key><summary>Validates bool values in yaml for node settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">nirmalc</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-22T02:09:19Z</created><updated>2015-06-07T17:23:30Z</updated><resolved>2014-10-27T03:07:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-22T15:46:03Z" id="60106475">@johtani please could you review this one
</comment><comment author="nirmalc" created="2014-10-23T16:19:04Z" id="60265441">@johtani  - changed exception type to IllegalArgs and added test.
- rebased with upstream/master
</comment><comment author="johtani" created="2014-10-24T14:57:32Z" id="60398637">LGTM
</comment><comment author="johtani" created="2014-10-27T03:07:41Z" id="60546016">@nirmalc thanks for submitting the PR. Merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Spelling error in MissingFilterBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8185</link><project id="" key="" /><description>In org.elasticsearch.index.query.MissingFilterBuilder, the javadoc on method existence(boolean) has a spelling error. Replace 'hte' with 'the'.
</description><key id="46445610">8185</key><summary>Spelling error in MissingFilterBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darkoc</reporter><labels /><created>2014-10-21T21:14:28Z</created><updated>2014-10-22T15:35:32Z</updated><resolved>2014-10-22T15:35:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-22T15:35:20Z" id="60104601">Thanks @darkoc - fixed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MissingFilterBuilder.java</file></files><comments><comment>Docs: Fixed typo in MissingFilterBuilder</comment></comments></commit></commits></item><item><title>enable indy compilation in GroovyScriptEngineService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8184</link><project id="" key="" /><description>This should just be a one liner like:
    compilerConfiguration.setOptimizationOptions(Collections.singletonMap("indy"), Boolean.TRUE);
in the ctor 

Thanks @uschindler for tracking this down
</description><key id="46429946">8184</key><summary>enable indy compilation in GroovyScriptEngineService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>enhancement</label></labels><created>2014-10-21T18:50:00Z</created><updated>2015-09-18T17:14:36Z</updated><resolved>2015-09-17T20:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-21T18:58:34Z" id="59979475">+1
</comment><comment author="uschindler" created="2014-10-21T19:13:58Z" id="59981826">We should at least check with JavaP that the classes have target 1.7 and use invokedynamic.

otherwise +1
</comment><comment author="pickypg" created="2014-10-22T21:41:39Z" id="60160516">Great find @uschindler! It's a free 15% speed improvement to Groovy.
</comment><comment author="uschindler" created="2014-10-22T22:42:26Z" id="60167587">Making this configureable makes no sense: If you use the "-indy" JAR file, you alreyd broke pre-JDK 7u60 people!
</comment><comment author="pickypg" created="2014-10-22T23:27:38Z" id="60172001">The -indy jar and this change are only on our master branch (2.0) and not
targeted for any 1.x release at this time for that exact reason.

By then we should be able to much more confidently tell people to not use
anything less than 7u60.

On Wednesday, October 22, 2014, Uwe Schindler notifications@github.com
wrote:

&gt; Making this configureable makes no sense: If you use the "-indy" JAR file,
&gt; you alreyd broke pre-JDK 7u60 people!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8184#issuecomment-60167587
&gt; .
</comment><comment author="uschindler" created="2014-10-23T11:42:31Z" id="60226863">Hi,
for documentation purposes: I did some investigation about invokedynamic bugs. There are no real severe Hotspot/JVM bugs like SIGSEGV with it, it is more that the class libraryaround java.lang.invoke had some bugs when creating the CallSite. In fact, Groovy got something like an IllegalArgumentException when trying to setup the call site: https://bugs.openjdk.java.net/browse/JDK-8033669 and https://bugs.openjdk.java.net/browse/JDK-8019184 (this is the main bug fixed in the final 7u60 version). There were some other fixes in hotspot itsself (fixed by Vladimir Kozlov, as usual), but those were not severe.
So users with pre-7u60 JVMs may get an IllegalArgumentException when invoking skripts.
</comment><comment author="uschindler" created="2014-10-23T11:49:30Z" id="60227507">In addition, this bug does only happen for call sites with more than 8 arguments. So it may not fail at all for Elasticsearch. We can maybe do some quick tests with pre-7u60.
</comment><comment author="pickypg" created="2014-10-24T00:45:10Z" id="60330721">I appreciate you looking into this @uschindler! Very cool. I should have some time to downgrade this tomorrow and I'll report back.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use groovy-x.y.z-indy jar for better scripting performance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8183</link><project id="" key="" /><description>Using the Groovy jar with the indy (short for `invokedynamic`) classifier enables usage of the `invokedynamic` instruction available in Java 7+. Due to buggy JVMs, it should only be used with Java 7u60 or later.

Closes #8182 
</description><key id="46419020">8183</key><summary>Use groovy-x.y.z-indy jar for better scripting performance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Scripting</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2014-10-21T17:10:26Z</created><updated>2015-06-08T14:31:19Z</updated><resolved>2014-10-21T18:14:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-10-21T17:45:58Z" id="59967726">Looks good. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #8183 from pickypg/feature/groovy-jdk7-8182</comment></comments></commit></commits></item><item><title>Groovy jar - Use Java 7 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8182</link><project id="" key="" /><description>Now that we require Java 7, we can get `invokedynamic` benefits by using the [Java 7 version of Groovy 2.x](http://groovy.codehaus.org/InvokeDynamic+support).

The change requires using a different Groovy jar geared specifically for Java 7. It's important to note their warning (taken verbatim from the above link):

&gt; All JDK 7 versions ranging from 7u21 to 7u55 are buggy with regards to invokedynamic. If you plan to use invokedynamic support, make sure you either use 7u60 or JDK 8.

This should help to speed up scripting support without any effort.

Thanks to Robert and Uwe for pointing it out.
</description><key id="46418756">8182</key><summary>Groovy jar - Use Java 7 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels /><created>2014-10-21T17:07:37Z</created><updated>2014-10-21T18:39:26Z</updated><resolved>2014-10-21T18:14:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-10-21T17:52:15Z" id="59968813">This would scratch 7u25 from the list of viable Java versions for Elasticsearch I think.  Probably worth thinking about.
</comment><comment author="rmuir" created="2014-10-21T18:05:30Z" id="59970900">7u25 is pretty ancient, the jre technically "expired" over a year ago. The change should go in 2.0 for sure.
</comment><comment author="s1monw" created="2014-10-21T18:39:26Z" id="59976394">+1 for going for a newer JVM here I think the benefits are worth it!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>New to ElasticSearch, need help with memory allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8181</link><project id="" key="" /><description>To start we are running ElasticSeach 1.1.2 in a SuSE Enterprise Server environment.

We have a new application that is designed to use ElasticSearch and installed it based on the requirements we were given. Based on the number records stored, we were told to use 16GB ram with an 8GB JVM. Well it blew through that quickly. 

We ended up having to set it to 48GB of ram with an 8GB JVM just to get it loaded. 48GB with 24GB of ram failed as well because of the sheer amount of memory the native processes were using outside the JVM. 

After some testing, we discovered that there was almost a 1 to 1 ratio of data stored in the index and the amount of data ElasticSearch was holding into memory. After we loaded our data, the memory used settled at 23GB with it reserving 3GB for the JVM. Stopping the ElasticSearch process only freed up the memory used for the JVM (which was expected) but not the rest. However when we deleted the index, the amount of memory used went down by 20GB, which just so happens is the same about of drive space the index was using.

Is this normal? Is there a way to limit the amount of non-JVM memory the process uses? I ask because it is a much bigger footprint than we were lead to believe. Thanks in advance for any help.
</description><key id="46401903">8181</key><summary>New to ElasticSearch, need help with memory allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rolltide0</reporter><labels /><created>2014-10-21T14:45:48Z</created><updated>2014-10-28T14:21:59Z</updated><resolved>2014-10-28T14:21:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="j0r0" created="2014-10-28T14:20:02Z" id="60762246">This has nothing to do with ES.
Linux will never free memory unless it needs to. 
When you stop ES all memory from JVM will be freed, but not the memory cached by the OS.
In your case , the index data was cached by OS. That's why it was freed after you delete the index.
This is not a problem - this a nice linux feature (LRU cache).

Btw.. i don't think this is a support forum.
</comment><comment author="clintongormley" created="2014-10-28T14:21:59Z" id="60762569">Agreed with @j0r0. And for questions like these please use the mailing list. 
http://elasticsearch.org/community/

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The `children` agg didn't take deleted document into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8180</link><project id="" key="" /><description>The live docs that is passed down was ignored by the filter impl. Now the children filter gets wrapped with ApplyAcceptedDocsFilter, so live docs are actually applied.

This issue was reported via the user list: https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/sybogZsMFso/f_UW4Zv01ZIJ
</description><key id="46400247">8180</key><summary>The `children` agg didn't take deleted document into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-21T14:32:52Z</created><updated>2015-06-07T17:49:56Z</updated><resolved>2014-10-22T08:27:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-21T21:08:58Z" id="59999135">LGTM

That said I am concerned that the live docs could just get ignored! This is something we should fix (in a different issue).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ParentToChildrenAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java</file></files><comments><comment>Aggregations: the `children` agg didn't take deleted document into account.</comment></comments></commit></commits></item><item><title>Make "noop" request breaker a non-dynamic setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8179</link><project id="" key="" /><description>The issue with making it dynamic is that in the event a cluster is
switched from a noop to a concrete implementation, there may be
in-flight requests, once these requests complete we adjust the breaker
with a negative number and trip an assertion.
</description><key id="46396632">8179</key><summary>Make "noop" request breaker a non-dynamic setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>bug</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-21T14:01:25Z</created><updated>2015-06-07T18:42:55Z</updated><resolved>2014-10-22T09:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-21T18:37:45Z" id="59976114">left one comment other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>rack awareness allocation and allocation filtering lead to unassigned shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8178</link><project id="" key="" /><description>Using shard awareness and allocation filtering together may lead to unassigned shard.

To reproduce:
- 3 nodes cluster (2 with `node.flavor: type1`, the other one with `node.flavor: type2`)
- shard are required to have 2 copies
- use `cluster.routing.allocation.awareness.attributes: flavor`
- use `routing.allocation.require: type1` on index template

On index creation, shard primary will be set on a `type1` node, its copy will stay unassigned.

The full description of the scenario leading to this issue is described on #8104 and https://groups.google.com/forum/#!topic/elasticsearch/wbVIsDzYLDM

Investigation in the code show that 
- shard allocation on `type2` node is ruled out because of allocation filter
- shard allocation on `type1` nodes are ruled out because of rack awareness

Those rules should cooperate and I expect the shard replica to go to the second `type1` node.
</description><key id="46391720">8178</key><summary>rack awareness allocation and allocation filtering lead to unassigned shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kamaradclimber</reporter><labels><label>:Allocation</label><label>adoptme</label><label>bug</label><label>high hanging fruit</label></labels><created>2014-10-21T13:15:12Z</created><updated>2015-11-21T19:56:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Check if there is a search context, otherwise throw a query parse exception.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8177</link><project id="" key="" /><description>Also added a bwc test that runs a delete by query with a has_child query and verifies that only that operation is ignored when recovering from disk during a upgrade.

PR for #8031
</description><key id="46386584">8177</key><summary>Check if there is a search context, otherwise throw a query parse exception.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-21T12:19:32Z</created><updated>2015-06-07T18:43:06Z</updated><resolved>2014-10-22T07:51:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-21T18:59:49Z" id="59979658">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryParserUtils.java</file><file>src/test/java/org/elasticsearch/bwcompat/ParentChildDeleteByQueryBackwardsCompatibilityTest.java</file></files><comments><comment>Parent/child: Check if there is a search context, otherwise throw a query parse exception.</comment></comments></commit></commits></item><item><title>missing quote</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8176</link><project id="" key="" /><description>fix missing quote
</description><key id="46373118">8176</key><summary>missing quote</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kyeyeon</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-21T09:33:50Z</created><updated>2014-10-21T10:52:43Z</updated><resolved>2014-10-21T10:52:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-21T09:34:41Z" id="59902169">Hi @double73 

Thanks for the fix! Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-10-21T10:52:30Z" id="59910295">thanks @double73 - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: missing quote</comment></comments></commit></commits></item><item><title>Disable Unsafe usage from Groovy dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8175</link><project id="" key="" /><description>We should set the system property `groovy.json.faststringutils.disable=true` to ensure Groovy does not use the Unsafe APIs.
</description><key id="46369582">8175</key><summary>Disable Unsafe usage from Groovy dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>low hanging fruit</label></labels><created>2014-10-21T08:57:29Z</created><updated>2016-06-20T13:40:41Z</updated><resolved>2016-06-20T13:40:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-21T11:25:10Z" id="59913412">++
</comment><comment author="uschindler" created="2014-10-21T13:34:42Z" id="59927641">LOL. Fast classes are so funny. In most cases they are slower than the original.
</comment><comment author="javanna" created="2015-10-16T14:16:00Z" id="148729339">I wonder it this is still relevant at this point or we need to take some action. @dakrone ?
</comment><comment author="dakrone" created="2015-10-16T16:56:58Z" id="148770065">@javanna it's still a "nice-to-do" even with Groovy as a plugin.
</comment><comment author="rmuir" created="2015-10-16T17:54:35Z" id="148785185">Strong +1 to fix this: security manager will not allow it to do this (https://github.com/groovy/groovy-core/blob/master/subprojects/groovy-json/src/main/java/groovy/json/internal/FastStringUtils.java#L43-L57), but its a big reliability risk if someone disables security manager.

This particular optimization uses Unsafe to change _final fields_ of String, which is pretty scary. It has JVM-specific implementations (https://github.com/groovy/groovy-core/blob/master/subprojects/groovy-json/src/main/java/groovy/json/internal/FastStringUtils.java#L155), and although it tries to be a little defensive, who knows if its really correct in all cases.

Just think about stuff like this going forward too: https://bugs.openjdk.java.net/browse/JDK-8134758 and https://bugs.openjdk.java.net/browse/JDK-8054307

IMO: disable it unconditionally!
</comment><comment author="dakrone" created="2015-10-16T18:19:17Z" id="148790755">&gt; IMO: disable it unconditionally!

+1
</comment><comment author="uschindler" created="2015-10-16T18:26:03Z" id="148793865">Nuke that. I don't want to know how many buggy code tries to deoptimize strings and crush! :(
</comment><comment author="s1monw" created="2015-10-16T18:36:18Z" id="148799139">++ I think we are sold! @dakrone feel free to open a PR please
</comment><comment author="clintongormley" created="2015-11-21T19:56:25Z" id="158677622">Has this already been implemented by another change?
</comment><comment author="tlrx" created="2016-06-20T13:40:34Z" id="227144742">@clintongormley We no longer use the `groovy-all` dependency but only the core `groovy`, I think we can close this issue: the system property `groovy.json.faststringutils.disable` was used by the class `FastStringUtils`which is not shipped anymore.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[feature] actionGet method return null with NullPointerException when doc id does not exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8174</link><project id="" key="" /><description>Is there an exist or check method instead of  actionGet method without NullPointerException when doc id does not exist ? 

Hope this little new feature can be supported :) 
</description><key id="46364163">8174</key><summary>[feature] actionGet method return null with NullPointerException when doc id does not exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">node</reporter><labels /><created>2014-10-21T07:47:12Z</created><updated>2014-10-21T07:54:55Z</updated><resolved>2014-10-21T07:54:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-21T07:52:10Z" id="59891335">@node I presume you mean the `TransportGetAction`. The return object `GetResponse` has a isExists method that should give you that boolean. NPE should never be thrown. Which version of ES do you use? Can describe a little more where and how you get the exception?
</comment><comment author="node" created="2014-10-21T07:54:55Z" id="59891606">Got it. Found isExists and isSourceEmpty method in GetResponse . Many thanks. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>This adds ability to specify 'named' threadpool to searches.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8173</link><project id="" key="" /><description>Currently all searches uses SEARCH thread pool to run queries. However if we have programs using ES with different 'interests' and 'SLA's' -they can block each others queries.

Example:
- 'fred' indexes documents, but does some denormalization and needs to query ES for that purpose.
- 'barney' queries same ES cluster to serve website traffic.

fred's queries get queued in thread pool due to large amount of queries from barney at times ; fred is okay with 800 ms while barney needs query response by 200 ms.

Named threadpools need to be configured in yml explicitly, eg:

```
threadpool:
     fred:
        type: fixed
        size: 30
```

[ Here fred , named threadpool is configured with default size of 30]

SearchRequestBuilder and ScrollRequestBuilder takes threadpool name ,
but defaults to SEARCH. REST API accepts new http parameter 'threadpool'
which can take name of threadpool to run queries.

```
curl -XPUT "http://localhost:9200/movies/movie/1" -d'
&gt; {
&gt;     "title": "The Godfather",
&gt;     "director": "Francis Ford Coppola",
&gt;     "year": 1972,
&gt;     "genres": ["Crime", "Drama"]
&gt; }'
```
## Call with 'custom' threadpool

```
curl 'localhost:9200/_search?q=*&amp;threadpool=fred'
```
## Invalid threadpool name

```
curl 'localhost:9200/_search?q=*&amp;threadpool=barney

{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[J8vv81u1SKK5BUizD5Yppw][movies][0]: ElasticsearchIllegalArgumentException[No executor found for [pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][1]: ElasticsearchIllegalArgumentException[No executor found for [pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][2]: ElasticsearchIllegalArgumentException[No executor found for [pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][3]: ElasticsearchIllegalArgumentException[No executor found for [pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][4]: ElasticsearchIllegalArgumentException[No executor found for [pollois]]}]","status":400}
```

Thanks to way original code is done - _stats shows new threadpool stats

Closes #8124
</description><key id="46348838">8173</key><summary>This adds ability to specify 'named' threadpool to searches.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirmalc</reporter><labels><label>enhancement</label></labels><created>2014-10-21T02:41:09Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2014-10-22T21:04:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-22T20:58:00Z" id="60154108">This PR appears to be for a different issue altogether?  The changes I see look like they are for #8097?
</comment><comment author="rjernst" created="2014-10-22T21:00:45Z" id="60154514">This seems like some weird github issue...the original email notification I see lists files that look related to this issue, but when I go to this actual PR in github, it shows an entirely different set of changes...perhaps try opening a new PR?
</comment><comment author="nirmalc" created="2014-10-22T21:01:11Z" id="60154574">my bad , i updated branch with another patch by mistake - will resubmit 
</comment><comment author="rjernst" created="2014-10-22T21:02:16Z" id="60154748">I ok, I see now, you forced your change over the branch.  Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide example of deleting a repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8172</link><project id="" key="" /><description>Example of deleting a repository with explanation that snapshots themselves are left untouched.
</description><key id="46344101">8172</key><summary>Provide example of deleting a repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels /><created>2014-10-21T01:04:57Z</created><updated>2014-10-21T08:10:52Z</updated><resolved>2014-10-21T08:10:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-21T08:10:37Z" id="59893078">thanks @geekpete - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Provide example of deleting a repository</comment></comments></commit></commits></item><item><title>Move the child filter over to the fixed bitset cache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8171</link><project id="" key="" /><description /><key id="46323705">8171</key><summary>Move the child filter over to the fixed bitset cache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cache</label><label>bug</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-20T20:48:21Z</created><updated>2015-06-07T18:43:17Z</updated><resolved>2014-10-22T08:27:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-21T21:17:02Z" id="60000261">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file></files><comments><comment>Core: Forget to move the child filter over to fixed bitset filter.</comment></comments></commit></commits></item><item><title>Make allocation deciders more debuggable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8170</link><project id="" key="" /><description>Currently, the excellent `_cluster/reroute?explain` API gives very detailed information about why a particular shard could or couldn't be moved, but we have little insight into why a cluster **doesn't** rebalance shards automatically.

eg if you change a setting like `cluster.routing.allocation.balance.primary`, and nothing changes, it is difficult to know why nothing has changed.

It would be very useful to have more insight into the decisions made by each allocation decider for each shard. 
</description><key id="46300844">8170</key><summary>Make allocation deciders more debuggable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label></labels><created>2014-10-20T17:12:06Z</created><updated>2015-07-23T12:21:32Z</updated><resolved>2015-07-23T12:21:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-10-20T17:13:30Z" id="59803240">Sometimes I find that the allocation decider has to be "kicked" by manually forcing a move.  I'm not sure what the deal is there but something like this could help debug that.
</comment><comment author="clintongormley" created="2014-10-20T17:14:41Z" id="59803762">@nik9000 the allocation deciders aren't checked periodically, but only when something changes. We've opened https://github.com/elasticsearch/elasticsearch/issues/8146 for that one.
</comment><comment author="nik9000" created="2014-10-20T17:18:39Z" id="59804372">@clintongormley thanks!  It probably is that, yeah.
</comment><comment author="clintongormley" created="2015-07-23T12:21:32Z" id="124079291">Closing in favour of #12412
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GET don't have a body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8169</link><project id="" key="" /><description /><key id="46287367">8169</key><summary>GET don't have a body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2014-10-20T15:27:28Z</created><updated>2014-10-20T18:26:10Z</updated><resolved>2014-10-20T18:26:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-20T16:23:03Z" id="59792389">GET verb supports a body.
Is there anything wrong with this API?
</comment><comment author="bleskes" created="2014-10-20T17:19:56Z" id="59804566">@synhershko those are searches which are doable with `GET`s + body just as any other _search call. I don't want to start the entire `GET` with body discussion again, but we should be at least consistent with the rest of the docs where settled on `GET _search` 
</comment><comment author="synhershko" created="2014-10-20T17:30:24Z" id="59806224">@bleskes wasn't aware there was a discussion. The Sense version I'm using (ehem ehem) has an error mark when GET is used with a body, hence this PR. Feel free to close if a decision was made against that.
</comment><comment author="bleskes" created="2014-10-20T18:17:11Z" id="59814407">Yeah, latest Sense allows you to use gets and just converts it behind the scene to POST due to the browser limitations. I think we can close this.

—
Sent from Mailbox

On Mon, Oct 20, 2014 at 7:30 PM, Itamar Syn-Hershko
notifications@github.com wrote:

&gt; ## @bleskes wasn't aware there was a discussion. The Sense version I'm using (ehem ehem) has an error mark when GET is used with a body, hence this PR. Feel free to close if a decision was made against that.
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/8169#issuecomment-59806224
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>In fixed bitset service fix order where the warmer listener is added.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8168</link><project id="" key="" /><description>Add warmer listener only when index service is set, in order to prevent possible NPE.

The IndicesWarmer gets set before the InternalIndexService gets set, which can lead to a small time window were InternalIndexService isn't set

Closes #8140
</description><key id="46282971">8168</key><summary>In fixed bitset service fix order where the warmer listener is added.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-20T14:57:45Z</created><updated>2015-06-07T18:43:33Z</updated><resolved>2014-10-22T08:27:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-21T21:19:25Z" id="60000591">The change looks good but can you please leave a comment about it when you push since this is a bit subtle?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/fixedbitset/FixedBitSetFilterCache.java</file></files><comments><comment>Core: Add warmer listener only when index service is set, in order to prevent possible NPE.</comment></comments></commit></commits></item><item><title>Update filtered-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8167</link><project id="" key="" /><description>Fix mistyping
</description><key id="46267064">8167</key><summary>Update filtered-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">andrei-kolosok</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-20T12:41:46Z</created><updated>2014-10-21T07:45:45Z</updated><resolved>2014-10-21T07:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T12:45:26Z" id="59744270">Hi @andrei-kolosok 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="andrei-kolosok" created="2014-10-20T12:49:58Z" id="59744750">https://elasticsearch.echosign.com/public/viewAgreement?aid=XFFKJ3W3Y4P323Q&amp;eid=XFFLQP245XY6B6Z&amp;
</comment><comment author="clintongormley" created="2014-10-21T07:45:37Z" id="59890742">thanks @andrei-kolosok - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update filtered-query.asciidoc</comment></comments></commit></commits></item><item><title>Transport/ Java API: Fix result transport and add api methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8166</link><project id="" key="" /><description /><key id="46262441">8166</key><summary>Transport/ Java API: Fix result transport and add api methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-10-20T11:50:18Z</created><updated>2014-10-21T07:03:37Z</updated><resolved>2014-10-21T07:03:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Reduce memory usage in top children query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8165</link><project id="" key="" /><description>Closes https://github.com/elasticsearch/elasticsearch/issues/8160
</description><key id="46261697">8165</key><summary>Reduce memory usage in top children query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">kire321</reporter><labels><label>:Parent/Child</label><label>enhancement</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-20T11:40:18Z</created><updated>2015-06-07T16:50:46Z</updated><resolved>2014-10-20T17:42:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-20T11:54:51Z" id="59735758">@kire321 Thanks for addressing this unnecessary usage of memory! The change looks good. Can you sign the CLA: http://www.elasticsearch.org/contributor-agreement/ ? Then this change can get merged in.
</comment><comment author="kire321" created="2014-10-20T12:16:56Z" id="59741340">I did sign the CLA... Do I need to associate the email I used to sign it with my github account?
</comment><comment author="martijnvg" created="2014-10-20T13:43:56Z" id="59755517">@kire321 As far as I know there is no need to for the email to be in sync with your GH account.
</comment><comment author="kire321" created="2014-10-20T15:42:30Z" id="59782027">@martijnvg Is there anything I need to do to help the CLA get through?
</comment><comment author="martijnvg" created="2014-10-20T17:04:47Z" id="59798680">@kire321 The CLA is fine, I'll pull your change in.
</comment><comment author="martijnvg" created="2014-10-20T17:44:03Z" id="59808381">@kire321 I pushed your change and thanks for brining this up. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file></files><comments><comment>Parent/child: Reduce memory usage in top children query.</comment></comments></commit></commits></item><item><title>Add time_zone setting for query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8164</link><project id="" key="" /><description>Query String query now supports a new `time_zone` option based on JODA time zones.
When using a range on date field, the time zone is applied.

``` json
{
"query": {
  "query_string": {
    "text": "date:[2012 TO 2014]",
    "timezone": "Europe/Paris"
  }
 }
}
```

Closes #7880.
</description><key id="46260150">8164</key><summary>Add time_zone setting for query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Query DSL</label><label>feature</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-20T11:20:17Z</created><updated>2015-04-08T05:53:47Z</updated><resolved>2014-10-20T17:37:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-20T11:55:34Z" id="59735829">This looks good, should we add the same feature to the `simple_query_string` query for consistency?
</comment><comment author="dadoonet" created="2014-10-20T13:31:54Z" id="59753773">@jpountz Indeed! That was my plan at the beginning and it sounds like I forgot it :)
</comment><comment author="dadoonet" created="2014-10-20T13:59:13Z" id="59757845">@jpountz Actually, `simple_query_string` does not support yet `range` queries. So for now, this PR can be only applied onto `query_string`.
</comment><comment author="jpountz" created="2014-10-20T14:06:40Z" id="59758943">LGTM
</comment><comment author="dadoonet" created="2014-10-20T17:38:09Z" id="59807468">Merged in master and 1.x with commits: 0ff61e1d6fda93462ed0442a00de7931eba51d7b and 9fd06b1d03c0424c39a3da97f545f047b7d4eb47
</comment><comment author="im-denisenko" created="2015-04-07T20:56:43Z" id="90730355">@dadoonet Could you have a look please? Either this option not working or I don't understand how it should work.

I have following code: 

``` bash
curl -XPUT 'http://localhost:9200/foo/?pretty' -d '{"mapping": {"tweets": {"properties": {"tweet_date": {"type": "date"}}}}}'
curl -XPOST 'http://localhost:9200/foo/tweets/1/' -d '{"tweet_date": "2015-04-05T23:00:00+0000"}'
curl -XPOST 'http://localhost:9200/foo/tweets/2/' -d '{"tweet_date": "2015-04-06T00:00:00+0000"}'
curl -XPOST 'http://localhost:9200/foo/_refresh?pretty'

curl -XGET 'http://localhost:9200/foo/tweets/_search?pretty' -d '{
    "query": {
        "query_string": {
            "query": "tweet_date:[2015-04-06T00:00:00+0200 TO 2015-04-06T23:00:00+0200]"
        }
    }
}'

curl -XGET 'http://localhost:9200/foo/tweets/_search?pretty' -d '{
    "query": {
        "query_string": {
            "query": "tweet_date:[2015-04-06T00:00:00 TO 2015-04-06T23:00:00]",
            "time_zone": "+0200"
        }
    }
}'

```

First query result:

```
{
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "foo",
      "_type" : "tweets",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"tweet_date": "2015-04-06T00:00:00+0000"}
    }, {
      "_index" : "foo",
      "_type" : "tweets",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"tweet_date": "2015-04-05T23:00:00+0000"}
    } ]
  }
}
```

Second query result:

```
{
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "foo",
      "_type" : "tweets",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"tweet_date": "2015-04-06T00:00:00+0000"}
    } ]
  }
}
```

Shouldn't these results be the same?
</comment><comment author="dadoonet" created="2015-04-07T21:04:45Z" id="90731993">@im-denisenko I think they should. If you are using Elasticsearch 1.5.0, I'd open an issue with this great full reproduction script.
</comment><comment author="im-denisenko" created="2015-04-07T21:22:01Z" id="90736267">@dadoonet Yes, this is 1.5.0
Thanks for answer. It resolves all my "wtf" for last few hours :)
</comment><comment author="dadoonet" created="2015-04-08T05:53:47Z" id="90809498">@im-denisenko Could you open an issue so we won't forget to add some tests and fix it?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Handle failed request when auto create index is disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8163</link><project id="" key="" /><description>If a bulk request contains a mix of indexing requests for an existing index and one that needs to be auto-created but a cluster configuration prevents the auto-create of the new index, the ingest process hangs. The exception for the failure to create an index was not caught or reported back properly. Added a Junit test to recreate the issue and the associated fix is in TransportBulkAction.

Closes #8125
</description><key id="46258788">8163</key><summary>Handle failed request when auto create index is disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-20T11:09:15Z</created><updated>2015-06-07T17:54:32Z</updated><resolved>2014-10-28T10:31:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-20T13:22:58Z" id="59752568">I think adding an explicit handling of IndexMissingException which will not cause the entire request to fail. However, the source of the problem is deeper and lies in the fact that we don't deal with failure correctly in the `executeBulk` method while returning from the create index command. IMHO this is where the proper fix belongs.
</comment><comment author="markharwood" created="2014-10-20T15:51:34Z" id="59783693">@bleskes can you review the last change please? thx
</comment><comment author="bleskes" created="2014-10-21T19:16:26Z" id="59982197">@markharwood I left one more comment, once that's done I think it's good.
</comment><comment author="markharwood" created="2014-10-22T14:32:03Z" id="60093805">Rebased and consolidated exception handling
</comment><comment author="bleskes" created="2014-10-23T13:25:45Z" id="60237700">LGTM. Thx @markharwood 
</comment><comment author="markharwood" created="2014-10-24T15:21:49Z" id="60402196">@bleskes Pushed to master, 1.x and 1.4 but TransportBulkAction in 1.3 looks different and is missing addFailureIfIndexIsClosed() method.
Are we back-porting this fix and the related checks for closed indexes into 1.3?
</comment><comment author="bleskes" created="2014-10-27T08:16:52Z" id="60560686">@markharwood thx. I think this an important bug fix. It can go into 1.3 as well.
</comment><comment author="markharwood" created="2014-10-27T09:21:10Z" id="60566373">@bleskes @GaelTadh 1.3 is missing https://github.com/elasticsearch/elasticsearch/commit/61c21f9a which addresses https://github.com/elasticsearch/elasticsearch/issues/6410
My patch here extends the above's logic for testing for closed indices - any reason that didn't go into 1.3, Brian?
</comment><comment author="clintongormley" created="2014-10-27T09:34:53Z" id="60567746">@markharwood @GaelTadh I think #6410 should go into 1.3 as well
</comment><comment author="GaelTadh" created="2014-10-27T11:35:36Z" id="60579862">Looks like both 61c21f9 and this depend on 5d987ad5.
</comment><comment author="GaelTadh" created="2014-10-27T14:05:58Z" id="60596911"> 5d987ad addresses #7223, this is a fairly big change that is only in 1.4B1 and up.
</comment><comment author="clintongormley" created="2014-10-27T14:22:41Z" id="60600051">OK - then let's merge this only into 1.4. thanks @GaelTadh 
</comment><comment author="s1monw" created="2014-10-27T16:13:29Z" id="60620032">from the history here this seems to be committed to 1.4 and up, can we close it?
</comment><comment author="markharwood" created="2014-10-28T10:31:32Z" id="60735739">Committed to 1.4 and up, see https://github.com/elasticsearch/elasticsearch/commit/d12ae196afd555ce942bf9476b6935267117ade5
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make simple_query_string leniency more fine-grained</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8162</link><project id="" key="" /><description>Previously, the leniency was on a per-query basis, with each query being
parsed into multiple queries, one for each field. If any one of these
queries failed, the entire query was discarded in the name of being
lenient.

Now query parts will only be discarded if they fail for a particular
field, the entire query is not discarded. This helps when performing a
query over a numeric and string field, as only the sub-queries that are
invalid due to format exceptions will be discarded.

Also moves the `simple_query_string` queries out of SimpleQueryTests and
into a dedicated SimpleQueryStringTests class.

Fixes #7967
</description><key id="46256238">8162</key><summary>Make simple_query_string leniency more fine-grained</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-20T10:38:05Z</created><updated>2015-06-07T18:43:44Z</updated><resolved>2014-10-22T08:37:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-21T19:03:11Z" id="59980203">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transport/ Java API: Fix result transport and add api methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8161</link><project id="" key="" /><description>Also this commit changes request structure. "reducers" shoudl be given as object
and not as array to be consistent with aggregations.

```
"reducers": [
   "reducer1": {...
...
]
```

becomes

```
"reducers": {
   "reducer1": {...
...
}
```
</description><key id="46255350">8161</key><summary>Transport/ Java API: Fix result transport and add api methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-10-20T10:27:28Z</created><updated>2014-10-20T11:13:45Z</updated><resolved>2014-10-20T11:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T10:39:08Z" id="59725202">@brwe we specifically spec'ed reducers to be an array, because reducers can act on the results of other reducers.  So order is important.
</comment><comment author="brwe" created="2014-10-20T11:13:45Z" id="59731996">I see, I totally forgot. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>TopChildren Queries Waste Memory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8160</link><project id="" key="" /><description>The TopChildrenQuery allocates an array to hold the results of the query. Currently, the size of this array is based on the number of documents in the index and can be many megabytes in size. Instead, it should be based on the number of parent documents that were asked for.

Pull request coming soon.
Details: Look for `readerParentDocs = new IntObjectOpenHashMap&lt;&gt;(indexReader.maxDoc());` in `src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java`
</description><key id="46254837">8160</key><summary>TopChildren Queries Waste Memory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kire321</reporter><labels /><created>2014-10-20T10:21:06Z</created><updated>2014-10-20T17:42:25Z</updated><resolved>2014-10-20T17:42:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T10:26:04Z" id="59723841">@kire321 Out of interest, why do you use top children instead of `has_children`?
</comment><comment author="kire321" created="2014-10-20T10:33:17Z" id="59724616">For our most heavily used cluster, `has_children` has a latency of 30 seconds or so.
</comment><comment author="clintongormley" created="2014-10-20T10:37:05Z" id="59724989">@kire321 what version of Elasticsearch are you using, and are you using eager global ordinals on the `_parent` field?  (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/fielddata-formats.html#_fielddata_loading)

Also, how many parents and how many children?  And in a typical query, how many children match, and how many parents does that equate to?
</comment><comment author="kire321" created="2014-10-20T11:42:00Z" id="59734593">version 1.3.1
We've never tinkered with field data settings. If I understand the documentation correctly, the default is lazy loading, so no.
38544074 parents, 46344271 children. I'll get back to you about the number of matches in a typical query. Stuff is compiling... :)
</comment><comment author="kire321" created="2014-10-20T13:13:03Z" id="59751164">A typical query has 2125 parents and 2114 children.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file></files><comments><comment>Parent/child: Reduce memory usage in top children query.</comment></comments></commit></commits></item><item><title>org/elasticsearch/node/NodeBuilder : Unsupported major.minor version 51.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8159</link><project id="" key="" /><description>hi everybody ,
i want to use ES from java App : 

my java vesion 1.7.0_71
i use Windwos 7 

my POM.xml : 

![unbenannt](https://cloud.githubusercontent.com/assets/7645496/4699069/97426852-583c-11e4-8cb9-dfde5a07020e.PNG)

My Java Code : 

``` java
package testListJava;

import java.util.LinkedList;
import java.util.List;
import java.util.ListIterator;
import java.util.Enumeration;
import java.util.Hashtable;

import static org.elasticsearch.node.NodeBuilder.*;
import org.elasticsearch.action.get.*;
import org.elasticsearch.client.*;
import org.elasticsearch.node.*;

public class Test {

  public static void main(String[] args) {

        System.out.println("------------------------ElasticSearch--------------------------");  
        Node node = nodeBuilder().clusterName("testCluster").node();
        Client client = node.client();
        GetResponse response = client.prepareGet("twitter", "tweet", "1")
                .execute()
                .actionGet();

        System.out.println(response.toString());  
        System.out.println("-----------------------------------");  

  }
}
```

console Error : 

```
-------------------------ElasticSearch--------------------------
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/node/NodeBuilder : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:616)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
    at testListJava.Test.main(Test.java:19)
```

Please ask questions if anything is not clear.
Thanks in advance,
Sebi
</description><key id="46250234">8159</key><summary>org/elasticsearch/node/NodeBuilder : Unsupported major.minor version 51.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sehli</reporter><labels /><created>2014-10-20T09:34:33Z</created><updated>2015-03-20T08:10:38Z</updated><resolved>2014-10-20T09:51:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-20T09:51:39Z" id="59716918">Mailing list is a better place to ask such questions. We keep this space for issues and feature requests.

Though you are using an old JVM version.

Closing.
</comment><comment author="ankitagrawal" created="2015-03-19T18:05:51Z" id="83696338">Sehil,

Pls help me how did you resolve this issue. I am getting the same.
</comment><comment author="sehli" created="2015-03-20T08:10:38Z" id="83946400">Hi ,
i  had an old Java version , for this i need also an old ES version .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set maximum index name length to 255 bytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8158</link><project id="" key="" /><description>Fixes #8079
</description><key id="46249880">8158</key><summary>Set maximum index name length to 255 bytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-20T09:30:28Z</created><updated>2015-06-07T11:58:06Z</updated><resolved>2014-10-20T13:49:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-20T12:03:55Z" id="59736576">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update minimum-should-match.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8157</link><project id="" key="" /><description>Add %-sign to examle in the last section
</description><key id="46241503">8157</key><summary>Update minimum-should-match.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">andrei-kolosok</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-20T08:01:52Z</created><updated>2014-10-21T07:45:45Z</updated><resolved>2014-10-21T07:44:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T09:57:37Z" id="59717534">Hi @andrei-kolosok 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge your PR in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="andrei-kolosok" created="2014-10-20T12:49:17Z" id="59744680">https://elasticsearch.echosign.com/public/viewAgreement?aid=XFFKJ3W3Y4P323Q&amp;eid=XFFLQP245XY6B6Z&amp;
</comment><comment author="clintongormley" created="2014-10-21T07:45:45Z" id="59890759">thanks @andrei-kolosok - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update minimum-should-match.asciidoc</comment></comments></commit></commits></item><item><title> error: elasticsearch java : java.lang.NumberFormatException: For input string: "value"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8156</link><project id="" key="" /><description>i'm trying to index an xml document after convert it to  json but i have this error.
do you have any idea please?
Thanks in advance.
0    [main] INFO  org.elasticsearch.plugins  - [Jared Corbo] loaded [], sites []
Exception in thread "main" org.elasticsearch.index.mapper.MapperParsingException: failed to parse [documents.document.body.lead.p.TC]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:417)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:648)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:501)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:549)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:491)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:534)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:483)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:397)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:191)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:527)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "value"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Long.parseLong(Long.java:441)
    at java.lang.Long.parseLong(Long.java:483)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:145)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.innerParseCreateField(LongFieldMapper.java:296)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:223)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:407)
    ... 21 more
</description><key id="46229960">8156</key><summary> error: elasticsearch java : java.lang.NumberFormatException: For input string: "value"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">elastic10</reporter><labels /><created>2014-10-20T04:05:54Z</created><updated>2014-10-20T05:49:22Z</updated><resolved>2014-10-20T05:49:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-20T05:49:22Z" id="59686292">Please use the mailing list. We can better help you there. This space is for issues.

Also GIST your JSON document. Hard to answer without any example.

Closing. Feel free to reopen if you think it's an issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested: Change structure of the nested query and filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8155</link><project id="" key="" /><description>Currently the structure of the `nested` query is:

``` json
{
   "nested" : {
      "path" : "a-nested-object-path",
      "query ": { &lt;query&gt; } 
   }
}
```

If the structure were to be changed to:

``` json
{
   "nested" : {
      "a-nested-object-path" : {
         "query" : { &lt;query&gt; } 
      }  
   }
}
```

Then the structure is streaming parsing friendly and the nested object path is always known when the `query` is parsed. Also the `NestedQueryParser.LateBindingParentFilter` workaround can then be removed.

Like in #8154 nested path validation can then also be added, so that when a `nested` query is wrapped in another `nested` query a parser error can be thrown if the supplied paths don't match with each other.
</description><key id="46220341">8155</key><summary>Nested: Change structure of the nested query and filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Query DSL</label><label>breaking</label><label>enhancement</label></labels><created>2014-10-19T22:50:58Z</created><updated>2015-10-26T05:35:29Z</updated><resolved>2015-10-26T05:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-20T09:35:55Z" id="75211463">LateBindingParentFilter was removed on https://github.com/elasticsearch/elasticsearch/pull/9692 so this issue is only about making parsing more friendly I guess?
</comment><comment author="javanna" created="2015-10-15T17:21:12Z" id="148464041">I wonder if this becomes obsolete given that we now parse queries on the coordinating node. Is it still something that we want to consider doing @martijnvg ?
</comment><comment author="martijnvg" created="2015-10-26T05:34:31Z" id="151028678">@javanna yes, the issue can be closed. Looking up field mappings and other things that are type specific  happens when we build the lucene query and at that time the type is already known, because the query has already been parsed on the coordination node.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parent/child: Change structure of has_child, has_parent queries and filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8154</link><project id="" key="" /><description>Currently the structure of the `has_child` query is:

``` json
{
   "has_child" : {
      "type" : "a-child-type",
      "query ": { &lt;query&gt; } 
   }
}
```

If the structure were to be changed to:

``` json
{
   "has_child" : {
      "a-child-type" : {
         "query" : { &lt;query&gt; } 
      }  
   }
}
```

Then the structure is streaming parsing friendly and the child type is always known when the `query` is parsed. 

At the moment if the `query` field is encountered before the `type` field by the parser then the query json object is read into memory, but not parsed. It is parsed after the `type` field has been read. This work around can then also be removed.

Also validation should be added whether the used type is valid to use. This becomes useful in cases where a query contains multiple hierarchal `has_child` or `has_parent` queries. For example if the following types are defined grand_parent -&gt; parent -&gt; child. Then the only valid type when wrapping a has_child query in a has_child query that has type `parent` is `child`. This validation is already possible, but it is a nice opportunity to add this as well with this change.
</description><key id="46220329">8154</key><summary>Parent/child: Change structure of has_child, has_parent queries and filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Query DSL</label><label>breaking</label><label>enhancement</label></labels><created>2014-10-19T22:50:38Z</created><updated>2015-10-26T05:35:19Z</updated><resolved>2015-10-26T05:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T08:26:23Z" id="59704538">I like this - can we provide a bwc layer as well?
</comment><comment author="martijnvg" created="2014-10-20T08:47:38Z" id="59706710">I think the tricky bit is that the `query` is also json object directly under `has_child` and others. Which would mean we can't know the difference between a type `query` and the actual query.

If we decide to break bwc, then we need to think about how to deal with alias filters containing a has_child or has_parent filter.
</comment><comment author="s1monw" created="2015-02-20T09:38:30Z" id="75211771">I think we should do this and also make sure we fix #8155 for consistency. @martijnvg I guess we have to provide bwc for this unfortunately
</comment><comment author="martijnvg" created="2015-02-20T14:21:15Z" id="75244673">Yes, bwc for this is unfortunately, but it seems the only way forward.

The bwc logic is going to be tricky. (there may be a type with the name
`query`) We can only make the decision if the old or new syntax is used the
two fields (`query` and `type`) either do or do not exist under the
`has_child` or `has_parent`. I think this bwc logic prevent streaming
parsing for the time being...

On 20 February 2015 at 10:38, Simon Willnauer notifications@github.com
wrote:

&gt; I think we should do this and also make sure we fix #8155
&gt; https://github.com/elasticsearch/elasticsearch/issues/8155 for
&gt; consistency. @martijnvg https://github.com/martijnvg I guess we have to
&gt; provide bwc for this unfortunately
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/8154#issuecomment-75211771
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="martijnvg" created="2015-10-26T05:35:19Z" id="151028992">Can be closed as it is no longer needed for the same reason as in: https://github.com/elastic/elasticsearch/issues/8155#issuecomment-151028678
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add inner hits to nested and parent/child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8153</link><project id="" key="" /><description>Inner hits allows to embed nested inner objects, children documents or the parent document that contributed to the matching of the returned search hit as inner hits, which would otherwise be hidden.

Example search request w/ nested query:

``` bash
curl -XGET "http://localhost:9200/stack/question/_search" -d'
{
  "query": {
    "nested": {
      "path": "comments",
      "query": {
        "match": {
          "comments.text": "elasticsearch"
        }
      }
    }
  },
  "_source": {
    "include": "title"
  },
  "inner_hits": {
    "comments": {
      "path": {
        "comments": {
          "_source": {
            "include": "text"
          },
          "query": {
            "match": {
              "comments.text": "elasticsearch"
            }
          }
        }
      }
    }
  }
}'
```

Example response:

``` json
{
   "took": 16,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 3,
      "max_score": 2.1920953,
      "hits": [
         {
            "_index": "stack",
            "_type": "question",
            "_id": "485316",
            "_score": 2.1920953,
            "_source": {
               "title": "timestamp +5 hours logstash"
            },
            "inner_hits": {
               "comments": {
                  "hits": {
                     "total": 1,
                     "max_score": 2.1920953,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "485316",
                           "_nested": {
                              "field": "comments",
                              "offset": 2
                           },
                           "_score": 2.1920953,
                           "_source": {
                              "text": "So my timezone is EST which shows in the log, but the agent stamps 2013-03-06T17:03:56.934Z before it sends to elasticsearch."
                           }
                        }
                     ]
                  }
               }
            }
         },
         {
            "_index": "stack",
            "_type": "question",
            "_id": "107518",
            "_score": 1.8563976,
            "_source": {
               "title": "Web based file search in the lan?"
            },
            "inner_hits": {
               "comments": {
                  "hits": {
                     "total": 1,
                     "max_score": 1.8563976,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "107518",
                           "_nested": {
                              "field": "comments",
                              "offset": 2
                           },
                           "_score": 1.8563976,
                           "_source": {
                              "text": "Did you ever settle on something? I am looking for something similar. Does Solr work? that seems to be the most common suggestion. I tried elasticsearch but didn't know how to add a network path as an index to it."
                           }
                        }
                     ]
                  }
               }
            }
         },
         {
            "_index": "stack",
            "_type": "question",
            "_id": "419532",
            "_score": 1.2994784,
            "_source": {
               "title": "How to silently buffer tunnel traffic on network split, and automatically renew the connection when possible?"
            },
            "inner_hits": {
               "comments": {
                  "hits": {
                     "total": 1,
                     "max_score": 1.2994784,
                     "hits": [
                        {
                           "_index": "stack",
                           "_type": "question",
                           "_id": "419532",
                           "_nested": {
                              "field": "comments",
                              "offset": 4
                           },
                           "_score": 1.2994784,
                           "_source": {
                              "text": "Let me elaborate. I'm using ElasticSearch replication, when network disconnects, it puts all \"cluster\" into \"yellow\" state, means writes are postponed. I want to use \"localhost\" as replica nodes' host setting in config, both in China and in Germany. Under this there will be daemons acting as tcp proxy buffer and handling disconnections nicely. That's the outline of the idea, however maybe it would better to just leave it to ElasticSearch's protocol and handle yellow state in my app."
                           }
                        }
                     ]
                  }
               }
            }
         }
      ]
   }
}
```

For documentation and more examples check the: `inner-hits.asciidoc` file.

Closes #3022, #3152
</description><key id="46220303">8153</key><summary>Add inner hits to nested and parent/child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>feature</label><label>release highlight</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-19T22:50:00Z</created><updated>2015-08-30T11:26:44Z</updated><resolved>2014-12-02T11:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mathieu007" created="2014-11-05T06:11:23Z" id="61764207">This is a nice feature, but i was asking myself how is the inner documents returned. Is it returned by stream reading the source or a desieralization process or is there some kind of index behind the inner docs. 

The main reason i am asking this is because i have some very large nested documents and i don't know if returning inner docs his a wise solution in term of performance.

Thank you
</comment><comment author="martijnvg" created="2014-11-05T12:12:11Z" id="61798326">@mathieu007 It deserialises the _source and retrieves the relevant inner json objects from the _source. The deserialisation is done only once per top level hit, so that shouldn't affect performance too much. 

Also if you are concerned about this you can enable stored fields inside nested inner objects in the mapping, that way the _source isn't touched at all and the field values for each returned inner hit can simply be fetched from disk. If you go down this path you may not need to `_source` at all in ES and you can then disable it.
</comment><comment author="martijnvg" created="2014-11-05T12:12:15Z" id="61798331">@jpountz @dadoonet @clintongormley I updated the PR with the docs feedback.
</comment><comment author="clintongormley" created="2014-11-05T13:03:14Z" id="61803497">@martijnvg are we sold on the idea of specifying inner hits within the `has_child` and `nested` queries/filters themselves? If so, is there ever a use case for accepting `inner_hits` at the top level of the search request?

If we go for specifying within the query/filter, then I'd probably add a section on inner hits to the `has_child` docs, and another section to the `nested` docs.
</comment><comment author="martijnvg" created="2014-11-05T14:27:00Z" id="61814087">@clintongormley Yes, specifying the `inner_hits` on the query/filter level is a good idea and I'm sold on that. I think we should keep the flexibility of defining the inner hits on the top level. This allows an entire different query to be specified that runs in the nested scope. Also defining multiple levels of inner_hits definitions can be defined in a top level inner hits.
</comment><comment author="noter" created="2014-11-27T12:07:20Z" id="64782507">Is it possible to do global sorting based on inner hits doc filed ? 
</comment><comment author="martijnvg" created="2014-11-27T17:16:27Z" id="64814790">@noter That is possible via nested sorting: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_sorting_within_nested_objects
</comment><comment author="noter" created="2014-11-27T17:21:38Z" id="64815249">Yes thats clear but i was thinking about parent/child docs.   
</comment><comment author="martijnvg" created="2014-11-27T17:52:27Z" id="64817777">Ok, sorting by fields in a child / parent document still needs to be implemented. 

The best workaround is to transform the value you'd like to sort by to a number based value and wrap a `function_score` query that refers to your field in a `has_child` or `has_parent`.
</comment><comment author="martijnvg" created="2014-11-27T21:32:33Z" id="64831614">I updated the PR to be in sync with the recent changes in master.
</comment><comment author="mathieu007" created="2014-11-28T17:49:42Z" id="64917179">@martijnvg Just an idea, it would be great if the returned data could be the inner hits + all it's parents, 

something like that: 

```
{
    id: "2",
    title: "My root node",
    nestedKey: {
        id: "3",
        title: "Nested data level 1",
        nestedKey2: {
            id: "4",
            title: "Nested data level 2",
            "nestedKey3": {
                id: "5",
                title: "My matching nested document"
            }
        }
    }
}
```
</comment><comment author="jpountz" created="2014-11-28T23:37:26Z" id="64934759">@martijnvg I left some comments but it looks good overall. If you perform some changes, please do them as separate commits so that I don't need to review everything again, this change is BIG! :)
</comment><comment author="martijnvg" created="2014-12-01T11:18:00Z" id="65050931">@jpountz I answered your remarks and updated the PR. 
</comment><comment author="jpountz" created="2014-12-01T23:22:59Z" id="65156114">@martijnvg Left one comment, otherwise it looks good. Thanks for having added a unit test.
</comment><comment author="martijnvg" created="2014-12-02T09:48:50Z" id="65205649">@jpountz Great. I added the null check.
</comment><comment author="jpountz" created="2014-12-02T10:47:23Z" id="65212517">LGTM
</comment><comment author="ChrisRM" created="2014-12-03T09:14:09Z" id="65376099">Trying this thing out, the first level of nesting works perfectly, but how would I go about the second level ? E.g:

``` JS
{
    "inner_hits": {
        // Works
        "projects": {
            "path": {
                "projects": {
                    "query": { "....." }
                }
            }
        }
        // Not working
        "tasks": {
            "path": {
                "projects.tasks": {
                    "query": { "....." }
                }
            }
        }
    }
}
```
</comment><comment author="martijnvg" created="2014-12-03T09:57:13Z" id="65382027">@ChrisRM It isn't working because the project nested layer also needs to be defined as a inner_hits layer. The following should work:

``` json
{
    "inner_hits": {
        "projects": {
            "path": {
                "projects": {
                    "query": { "....." },
                    "inner_hits" : {
                        "tasks" : {
                              "path" : {
                                   "projects.tasks" : {
                                       "query": { "....." }
                                   }
                              }
                         }
                    }
                }
            }
        }
    }
}
```
</comment><comment author="ChrisRM" created="2014-12-03T10:13:13Z" id="65384202">Thanks @martijnvg, that kinda works, but fails when the match only happens in the "projects.tasks" layer, as "projects" wouldn't return anything, thus not returning any inner_hits.
</comment><comment author="martijnvg" created="2014-12-03T10:30:29Z" id="65386621">Right, I see that isn't very useful in your case. Maybe inner_hits should then also support definitions like the one you initially specified, so that also indirect nested levels can be mapped to the root level. This change shouldn't be too drastic in the inner hits code.
</comment><comment author="ChrisRM" created="2014-12-03T10:36:16Z" id="65387439">That would be awesome :+1: Let me know when or if you add this feature, @martijnvg :-)
</comment><comment author="portante" created="2014-12-03T22:57:07Z" id="65507396">Hello, can I bump our use case against this to see if I am understanding this feature correctly?

We are working on indexing sar data into elasticsearch with the following mapping: https://gist.github.com/portante/87d27b6e67669279bb7c

Our queries today return all values for nested document fields, such that for the following query:

```
{
    "size": 65536,
    "fields": ["_timestamp", "cpu-load.cpu", "cpu-load.user", "cpu-load.nice", "cpu-load.system", "cpu-load.iowait", "cpu-load.steal", "cpu-load.idle"],
    "query": {
        "filtered": {
            "filter": {
                "and": {
                    "filters": [
                        {
                            "range": {
                                "_cache": "false",
                                "_timestamp": {
                                    "lt": "2014-06-06T00:00:00",
                                    "gt": "2014-05-30T00:00:00"
                                },
                                "execution": "fielddata"
                            }
                        },
                        {
                            "term": {
                                "_metadata.nodename": "foo.example.com",
                                "_cache": "false"
                            }
                        },
                        {
                            "nested": {
                                "query": {
                                    "filtered": {
                                        "filter": {
                                            "term": {
                                                "cpu-load.cpu": "5"
                                            }
                                        }
                                    }
                                },
                                "path": "cpu-load"
                            }
                        }
                    ],
                    "_cache": "false"
                }
            }
        }
    }
}
```

Would result in a response like (one hit of many):

```
{
    "_index": "sar-20140530",
    "fields": {
        "cpu-load.steal": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        "cpu-load.cpu": ["all", 0, 1, 2, 3, 4, 5, 6, 7],
        "cpu-load.iowait": [0.72, 0.02, 2.22, 1.21, 1.92, 0.02, 0.14, 0.27, 0.0],
        "_timestamp": 1401408001000,
        "cpu-load.idle": [91.77, 98.21, 83.75, 84.64, 86.94, 95.9, 94.88, 95.78, 94.09],
        "cpu-load.user": [6.82, 1.67, 12.37, 12.82, 10.09, 3.74, 4.52, 3.68, 5.68],
        "cpu-load.system": [0.68, 0.1, 1.67, 1.33, 1.05, 0.35, 0.46, 0.27, 0.23],
        "cpu-load.nice": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    },
    "_type": "sar",
    "_id": "9bb4fecd22734c41c032ba39b1213ecd",
    "_score": 0.0
}
```

I could then use the inner_hits feature to only return fields for nested documents with a given field value like so (find all the CPU idle and system usage values for a given host):

```
{
    "size": 65536,
    "fields": ["_timestamp"],
    "query": {
        "filtered": {
            "filter": {
                "and": {
                    "filters": [
                        {
                            "range": {
                                "_cache": "false",
                                "_timestamp": {
                                    "lt": "2014-06-06T00:00:00",
                                    "gt": "2014-05-30T00:00:00"
                                },
                                "execution": "fielddata"
                            }
                        },
                        {
                            "term": {
                                "_metadata.nodename": "foo.example.com",
                                "_cache": "false"
                            }
                        },
                        {
                            "nested": {
                                "query": {
                                    "filtered": {
                                        "filter": {
                                            "term": {
                                                "cpu-load.cpu": "5"
                                            }
                                        }
                                    }
                                },
                                "path": "cpu-load"
                            }
                        }
                    ],
                    "_cache": "false"
                }
            }
        }
    },
    "inner_hits": {
        "cpu-load": {
            "path": {
                "cpu-load": {
                    "_source": {
                        "include": ["cpu-load.cpu", "cpu-load.user", "cpu-load.nice", "cpu-load.system", "cpu-load.iowait", "cpu-load.steal", "cpu-load.idle"]
                    }
                }
            },
            "query": {
                "filtered": {
                    "filter": {
                        "term": {
                            "cpu-load.cpu": "5"
                        }
                    }
                }
            }
        }
    }
}
```

Would result in a response like (one hit of many):

```
{
    "_index": "sar-20140530",
    "fields": {
        "_timestamp": 1401408001000
    },
    "_type": "sar",
    "_id": "9bb4fecd22734c41c032ba39b1213ecd",
    "_score": 0.0,
    "inner_hits": {
        "cpu-load": {
            "hits": {
                "total": 1,
                "max_score": 1.2994784,
                "hits": [
                    {
                        "_index": "sar-20140530",
                        "_type": "sar",
                        "_id": "419532",
                        "_nested": {
                            "field": "cpu-load",
                            "offset": 4
                        },
                        "_score": 1.2994784,
                        "_source": {
                            "cpu-load.cpu": 5,
                            "cpu-load.user": 1.0,
                            "cpu-load.nice": 0.0,
                            "cpu-load.system": 1.9,
                            "cpu-load.iowait": 2.9,
                            "cpu-load.steal": 0.0,
                            "cpu-load.idle": 94.0
                        }
                    }
                ]
            }
        }
    }
}
```

Does this match what is being implemented?  Or am I not understanding something (or all, as the case may be)?
</comment><comment author="martijnvg" created="2014-12-04T13:27:58Z" id="65631062">@portante Yes, this does match what is being implemented. The syntax for your usage is going to be more simplified: #8770
</comment><comment author="portante" created="2014-12-04T14:49:46Z" id="65642102">@martijnvg, thanks. Has this work be slated for a particular release yet?
</comment><comment author="adrianocrestani" created="2014-12-04T14:55:07Z" id="65642930">@portante , it has v1.5 label on it :)
</comment><comment author="pierrre" created="2014-12-08T10:44:13Z" id="66094691">Can you access inner hits from scripts? (sort or script fields)
</comment><comment author="aminakhan85" created="2015-01-12T06:18:25Z" id="69533027">Hi, has this feature of inner hits been implemented in ES Java API? 
</comment><comment author="martijnvg" created="2015-01-12T09:46:44Z" id="69546970">@aminakhan85 Yes, from version 1.5 on the nested, has_child and has_parent query/filter there is a new method with the name `innerHit(...)` that allows one to define inner hits. You can just pass a `QueryInnerHitBuilder` instance with no options set.
</comment><comment author="aminakhan85" created="2015-01-13T02:48:58Z" id="69687485">@Martijnvg. Thanks for the info. Right now im using version 1.4. Ill upgrade to 1.5 and will check it out. Thanks
</comment><comment author="aminakhan85" created="2015-01-13T06:16:19Z" id="69700319">Hello, looks like 1.5 official release is not available . When is it expected to be available? the latest I could find is 1.4.2
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/NestedQueryParser.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/SearchHit.java</file><file>src/main/java/org/elasticsearch/search/SearchModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/tophits/TopHitsParser.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsBuilder.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsContext.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/innerhits/InnerHitsParseElement.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHits.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SubSearchContext.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/elasticsearch/search/fetch/innerhits/NestedChildrenFilterTest.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file><file>src/test/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Added `inner_hits` feature that allows to include nested hits.</comment></comments></commit></commits></item><item><title>Geo distance filter always returns 0 results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8152</link><project id="" key="" /><description>Hi all. I want to be able to search with geo distance in ES 1.3.4.
I did geo mapping:
POST /geo/
{ "mappings":{
"pin" : {
"properties" : {
"location" : {
"type" : "geo_point"
}
}
}
}
}
Added some data in this format:
PUT /geo/location/1
{
"pin" : {
"location" : {
"lat" : 42.4398181,
"lon" : 21.466074
},
"text" : "Geolocation distance search"
}
}
Tried to search with geo_distance filter:
POST geo/location/_search
{ 
"query": {"filtered": {
"query": {
"match_all": {}},
"filter": {
"geo_distance": {
"distance": "500",
"distance_unit": "km", 
"pin.location": {
"lat" : 42.1,
"lon" : 21.1
}
}
}
}}
}
And always getting 0 results even that this distance is 48.32 km.
I tried many options and ES always returns 0 results to me.
( I used sense extension above)
</description><key id="46206139">8152</key><summary>Geo distance filter always returns 0 results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DritonA</reporter><labels /><created>2014-10-19T13:47:07Z</created><updated>2014-10-19T20:10:39Z</updated><resolved>2014-10-19T19:21:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-19T19:21:57Z" id="59661393">Hi @DritonA 

Please ask these questions in the mailing list, rather than here.

You've mixed up a number of things, here is a working example:

```
DELETE _all 

POST /geo/
{
  "mappings": {
    "location": {
      "properties": {
        "pin": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT /geo/location/1
{
  "pin": {
    "lat": 42.4398181,
    "lon": 21.466074
  },
  "text": "Geolocation distance search"
}

POST geo/location/_search
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "geo_distance": {
          "distance": "500km",
          "pin": {
            "lat": 42.1,
            "lon": 21.1
          }
        }
      }
    }
  }
}
```
</comment><comment author="DritonA" created="2014-10-19T20:10:38Z" id="59663232">Hi @clintongormley. I'm sorry that I mixed such questions place. Important is that you helped me so much and I hope that the others can learn from this. Your example is working perfect. Thanks a lot.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added 'd' to the list of supported units.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8151</link><project id="" key="" /><description>Day was missing from the list of supported units in the date math section.
</description><key id="46182204">8151</key><summary>Added 'd' to the list of supported units.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ryangrimm</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-18T17:35:06Z</created><updated>2014-10-19T19:25:06Z</updated><resolved>2014-10-19T19:25:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-19T11:35:32Z" id="59647050">Thanks for the fix @ryangrimm. Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="ryangrimm" created="2014-10-19T15:37:17Z" id="59653550">Sure can, just signed it.

--Ryan

&gt; On Oct 19, 2014, at 4:36 AM, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; Thanks for the fix @ryangrimm. Please could I ask you to sign the CLA so that I can merge this in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; thanks
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2014-10-19T19:24:44Z" id="59661474">Thanks @ryangrimm - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Added 'd' to the list of supported units.</comment></comments></commit></commits></item><item><title>Should we parse search requests on the coordinating node?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8150</link><project id="" key="" /><description>At the moment, the coordinating node uses the index name from URL and routing values from the query string to forward search requests directly to the shards where they should be executed.

This is very efficient, but does impose some limitations on the choices we can make at execution time.  For instance:
- #4272 asks for per-index routing values, which would need to be passed in the body
- #7298 raises the problem of limited space for index names in the URL
- #7570 complains about parsing errors being returned from all shards, rather than just one
- #6719 would allow terms-lookup filters, geo-shape lookup filters, indexed scripts or indexed templates to be fetched just once, instead of by every shard

All of the above (and possibly more?) could be fixed if we parsed the search request in the body. Should we consider doing this? What are the downsides? Would it be worth making it optional?
</description><key id="46169277">8150</key><summary>Should we parse search requests on the coordinating node?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>discuss</label><label>enhancement</label><label>high hanging fruit</label><label>stalled</label></labels><created>2014-10-18T09:49:49Z</created><updated>2015-11-21T15:50:33Z</updated><resolved>2015-11-21T15:50:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-19T22:52:39Z" id="59669263">Also the way percolating an existing document works can then be changed, so that the index, type and id can then be defined in the request body instead of the url.
</comment><comment author="rjernst" created="2014-10-22T00:33:36Z" id="60020962">@clintongormley What do you mean by "making it optional"?

This sounds like a good improvement to me.  We should preprocess anything we can on the coordinating node. Especially the issue with getting lots of shard errors instead of one clear error is a personal annoyance.
</comment><comment author="clintongormley" created="2014-10-22T15:42:47Z" id="60105904">@rjernst i think the reason that we don't do it today is that parsing the JSON creates a bunch of objects which, usually, are not needed.  Hence my suggestion of making this optional.
</comment><comment author="s1monw" created="2015-02-20T09:46:20Z" id="75212673">we discussed this today and we all agreed that this would be awesome to have.

&gt; Would it be worth making it optional?

I don't think we should make this optional but it's hard to do everything on the coordinating node. For instance we don't have the mappings instantiated for all indices or maybe not at all. But I think we can take a lot of advantage of this if we do a pre-parsing / rewrite stage. I think this would be awesome to have for lots of stuff like error reporting, prefetching, reducers and cacheing. maybe we should decouple parsing of the query and processing of it! I am +1 on that for sure
</comment><comment author="javanna" created="2015-03-31T16:44:40Z" id="88166977">I removed the discuss label, I think we have consensus here. We are working on making this possible in #10217 , more specifically on https://github.com/elastic/elasticsearch/tree/feature/query-parse-refactoring branch
</comment><comment author="javanna" created="2015-05-01T08:56:41Z" id="98081697">I think it's important to draw a line and define what we want to move to the coordinating node. It's clear that we want to parse json on the coordinating node, and throw any parsing error once from there. That said we don't have mappings on all nodes, thus any field related validation (e.g. unmapped field) will still be thrown on each data node, where we are sure we have the mappings.

As for terms/geo-shape lookup filters, it sounds good to reduce the roundtrips and have a single get to retrieve the pre-indexed terms/shapes, but then we would have to serialize them all on the wire to the data nodes, making the intermediate (`Streamable`) filter representation potentially much heavier. What do people think about this?
</comment><comment author="martijnvg" created="2015-05-01T09:11:46Z" id="98083676">@javanna What about having the `IndexService` (with all  index level related services) for all indices available on all nodes, but the `IndexShard` instances should only exist on the nodes that actually hold the shards? I think this is possible without serious downsides? (for example all caches we have are per shard and not per index)
</comment><comment author="javanna" created="2015-05-01T09:16:22Z" id="98084032">@martijnvg that means that even client nodes would have an `IndexService` per index, think of many indices potentially. I am not sure what the effect is going to be on non data nodes (think of master nodes too), sounds scary at first glance, but maybe it's going to be just fine.

I am marking this issue back for discussion, I think we should collect thoughts around mappings on coordinating nodes and how to split the work when it comes to pre-indexed terms/shapes/what have you.
</comment><comment author="martijnvg" created="2015-05-01T09:23:53Z" id="98084587">@javanna true, in case of many indices this would increase memory usage, but on the other hand in case of many indices in general this is already quite heavy (for example the size of the cluster state). 

For the dedicated (elected) master node, I think we shouldn't even use it is a coordinating node to begin with, but it should simple redirect any incoming request to another data node.

For the client node, I think it ok to do this? It think it is ok to increase the maximum heap space a bit for this purpose. Maybe this behaviour should be controllable via a setting? Maybe this behaviour isn't preferable if a client node is running inside another jvm process.
</comment><comment author="rjernst" created="2015-05-01T15:03:36Z" id="98150078">I agree with @martijnvg. Don't we already (or will soon?) have the cluster state on every node? It seems silly when we have the mappings sitting around, but can't use them to do the validation.
</comment><comment author="kimchy" created="2015-05-01T15:10:58Z" id="98153010">The ability to do validation means we need to materialize the mappings for any potential index that will exist in the cluster, compared to relaying on the natural distribution of indices across the data nodes. 

There is a big difference between storing the mappings in compressed fashion in the cluster state (which is present on each node), and having all the index level data structures around on each node. 

This will be a problem with many indices. Historically, btw, ES used to store the index level data structure on each node (not for any specific reason like validation, just because the optimization was not there), and it ended up being significant with 1000s of indices, which is why only materializing the index level data structures only where shards are allocated ended up happening.
</comment><comment author="martijnvg" created="2015-05-03T22:20:19Z" id="98550760">Being able to validate a search request in the coordination node is very neat, it is just unfortunate that the many IndexService instances is problematic. I think for most of the many indices cases the mappings between indices are likely to be the same or similar. Maybe it would be less of problem if for example the MapperService was able to share DocumentMapper instances between indices on the same node that are equal.
</comment><comment author="javanna" created="2015-05-06T15:05:49Z" id="99505253">We seem to be on the fence on this, I think that we might want to go ahead with parsing on the coordinating node without moving mappings checks there just yet. We can always do it once we have the infra for it though. 

What do people think about terms lookups etc.? If we do lookups on the coordinating node it's going to be a single get call, but then we would need to serialize all of the terms to all of the data nodes which kinda defeats the purpose of terms lookups somehow? 
</comment><comment author="kimchy" created="2015-05-06T15:20:17Z" id="99511161">@javanna I think we should make it in steps, and keep the existing behavior as is today, and focus on parsing on the coordinating node, and keep the lookups and such on the shard?
</comment><comment author="javanna" created="2015-05-08T10:13:38Z" id="100183329">sounds good @kimchy I agree
</comment><comment author="clintongormley" created="2015-11-21T15:50:33Z" id="158657347">Closing in favour of #10217
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Updates causing hotspots in cluster when multiple primary shards for an index exist on a single node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8149</link><project id="" key="" /><description>Some of our end users are seeing "hotspots" in the cluster when multiple primary shards for an index are allocated to a single node (unbalanced primary shards).  This becomes an issue when there are a lot of updates where the documents are loaded onto the primary shards first.    The http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-update-settings.html#_balanced_shards weight settings do not always help and require re-tuning when new nodes or new shards are added.  It will be nice to provide a setting similar to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-allocation.html#_total_shards_per_node , but only applies to primary shards, like total_primary_shards_per_node which allows the end user to define the maximum # of primary shards an index can have on a single node.  This will ensure that multiple primary shards are not allocated to the same node.
</description><key id="46149099">8149</key><summary>Updates causing hotspots in cluster when multiple primary shards for an index exist on a single node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2014-10-17T22:03:36Z</created><updated>2015-11-21T19:55:16Z</updated><resolved>2015-11-21T19:55:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-18T09:55:36Z" id="59605604">@ppf2 The `total_shards..` options scare me a bit because they are a hard limit, and can make it impossible to assign shards.

It seems like the `cluster.routing.allocation.balance.primary` setting _should_  do what you are after, but perhaps the value needs some tuning. Could you give more detail about why this particular option doesn't work well enough?

I'd prefer to keep the number of options small, as the more you add, the more difficult it is to reason about the outcome.  
</comment><comment author="clintongormley" created="2014-10-20T18:18:32Z" id="59814621">@ppf2 so it looks like `balance.primary` tries to balance ALL primary shards, rather than just the primary shards within each index.  I've also seen problems where I add a new node, and it receives mostly replicas.

So there is no doubt that this heuristic needs improvement.  But the `total*` setting is a blunt weapon, and, like most weapons, can harm the person who wields it.

Part of the problem is that the rebalancing algorithm is a bit of a black box.  I've opened https://github.com/elasticsearch/elasticsearch/issues/8170 to see if we can improve that situation.
</comment><comment author="ppf2" created="2014-10-20T23:11:20Z" id="59855090">Thanks @clintongormley.  Agree that total\* setting if not used properly can cause primary shards not to be fully allocated.  Would be nice to provide some way (not necessarily the total\* setting) in the future to balance primary shards per index per node, something that's more automatic without having to manually reroute/move the shards around.
</comment><comment author="s1monw" created="2014-10-21T11:21:21Z" id="59913040">There are several reasons why I think we need to solve this problem differently. The problem here is really not the balance function or anything in that code it's the fact that updates put extra pressure on the primary shard and therefore don't scale very well. I am against a setting like this because it can bring your cluster in a very bad state where it can potentially not allocate a primary at all, or we need to move a shard from one node to another to make space for a primary which is the wrong thing to do. 

Being a primary is only a boolean flag really and flipping it in order to make another healthy replica a primary should be simple and IMO is the way to go. It might be part of the balancing algorithm but not via an allocation decider or anything along those lines. The problem here is really the update feature and not the balancing algorithm.
</comment><comment author="clintongormley" created="2014-11-06T18:58:13Z" id="62031343">See #8369
</comment><comment author="s1monw" created="2015-02-20T09:43:59Z" id="75212409">removing the discuss tag since we are all on the same page here and we have #8369
</comment><comment author="clintongormley" created="2015-11-21T19:55:16Z" id="158677566">Closing in favour of #8369
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Startup: Prevent potential loop in bin/elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8148</link><project id="" key="" /><description>If bin/elasticsearch is started with a single --long argument, it could
potentially be caught up in a loop on bash (dash handles this correctly).

This commit adds an additional check that prints out an error message

Closes #7104
</description><key id="46147737">8148</key><summary>Startup: Prevent potential loop in bin/elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>bug</label></labels><created>2014-10-17T21:45:22Z</created><updated>2014-12-02T11:21:30Z</updated><resolved>2014-12-02T11:21:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-18T00:21:16Z" id="59591868">LGTM.
</comment><comment author="t-lo" created="2014-12-02T09:59:28Z" id="65206840">This pull request has been obsoleted by #8729. 
</comment><comment author="spinscale" created="2014-12-02T11:21:30Z" id="65216205">fixed by #8729
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices: Change IndexPrimaryShardNotAllocatedException from 409 to 500</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8147</link><project id="" key="" /><description>Closes #7632
</description><key id="46134165">8147</key><summary>Indices: Change IndexPrimaryShardNotAllocatedException from 409 to 500</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels /><created>2014-10-17T19:17:36Z</created><updated>2014-10-17T19:19:13Z</updated><resolved>2014-10-17T19:19:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2014-10-17T19:19:05Z" id="59561929">Ooops, a pull request already exists, closing this one
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disk allocation not periodically checking for whether thresholds/watermarks are exceeded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8146</link><project id="" key="" /><description>Repo video below.

In short:

You will see that there is an index with shards allocated to 2 nodes.  Cluster setting has watermark.low set to 80% and high set to 90%.  Disk usage for node_local is at 76%, node_vm is at 83%.

You can skip the video from 0:30 to 2:38, that is when I incrementally add large files to the disk on node_vm so that it uses &gt; 90% of disk.  After 2:38, You will see that the disk usage has increased to 90%+ but no relocation of shards occurred from node_vm to node_local.  But if I wait 30+s after the disk hits 90%+ usage, and then set the _same_ cluster.routing.allocation.disk.\* settings again, the relocation immediately happens.  This suggests that we are not currently periodically checking for whether the disk is filling up beyond the high mark without manual intervention by either:
- Setting the same cluster.routing.allocation.disk.\* settings again, or
- Using the reroute command with no actions, eg. `POST /_cluster/reroute`

https://drive.google.com/file/d/0BzqaicoBfqMfdXdSWnI2OFp0eXM/view?usp=sharing

Workaround is to periodically call reroute command (or reset the same settings).
</description><key id="46131200">8146</key><summary>Disk allocation not periodically checking for whether thresholds/watermarks are exceeded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label></labels><created>2014-10-17T18:47:11Z</created><updated>2014-10-31T11:10:32Z</updated><resolved>2014-10-31T11:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-21T11:55:06Z" id="59916167">I agree this is not how it should be... I think it would make sense to issue reroute commands from the `ClusterInfoService` if needed. I guess we can take the actual settings that we are using into account ie. we should issue reroutes at least once we crossed the high watermark on a any node... @dakrone WDYT
</comment><comment author="dakrone" created="2014-10-21T14:12:43Z" id="59933421">@s1monw I think it should be implemented differently. I think the `ClusterInfoService` should be entirely separated from the internals of shard allocation, its job should only be to gather information and make it available as needed. This makes it a bit easier to use for other purposes in the future where we may need to gather information (and not operation on it at all) at specific intervals.

I think a better way to implement it would be to allow the `ClusterInfoService` to register listeners that are called when new information is gathered, then we could register a standalone handler that was responsible for knowing about the high watermark and kicking off the reroute when necessary.

What do you think about this?
</comment><comment author="s1monw" created="2014-10-21T18:35:45Z" id="59975764">yeah I think that is the right way to do it abstraction wise...
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/LatchedActionListener.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>src/main/java/org/elasticsearch/cluster/DiskUsage.java</file><file>src/main/java/org/elasticsearch/cluster/EmptyClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsStats.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/MockDiskUsagesTests.java</file></files><comments><comment>Reroute shards automatically when high disk watermark is exceeded</comment></comments></commit></commits></item><item><title>Use 1 instead of 0 as filler version value for nested docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8145</link><project id="" key="" /><description>This is consistent with the default value of version=1, which means if a user isnt using versioning, the lucene codec can potentially completely optimize all store for the version field away.
</description><key id="46129980">8145</key><summary>Use 1 instead of 0 as filler version value for nested docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T18:34:38Z</created><updated>2015-03-19T09:34:49Z</updated><resolved>2014-10-17T19:23:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-17T18:36:36Z" id="59556244">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for distributed frequencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8144</link><project id="" key="" /><description>Adds distributed frequencies support for the Term Vectors API. A new parameter
called `dfs` is introduced which defaults to `false`.
</description><key id="46121098">8144</key><summary>Add support for distributed frequencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T17:00:39Z</created><updated>2015-06-07T10:36:40Z</updated><resolved>2014-10-23T12:01:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-21T21:34:39Z" id="60002790">This looks good to me overall, however I have no experience at all with the transport layer so it would be good if someone else could review this PR before we get it in
</comment><comment author="martijnvg" created="2014-10-22T10:18:41Z" id="60063925">@alexksikes This looks good. I left a couple of comments.
</comment><comment author="martijnvg" created="2014-10-23T11:45:13Z" id="60227104">@alexksikes this look great! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorWriter.java</file><file>src/main/java/org/elasticsearch/action/termvector/dfs/DfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/dfs/DfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/dfs/ShardDfsOnlyRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/dfs/ShardDfsOnlyResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/dfs/TransportDfsOnlyAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/dfs/package-info.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorService.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestTermVectorAction.java</file><file>src/test/java/org/elasticsearch/action/termvector/GetTermVectorTests.java</file><file>src/test/java/org/elasticsearch/transport/ActionNamesTests.java</file></files><comments><comment>Term Vectors: support for distributed frequencies</comment></comments></commit></commits></item><item><title>Mappings: disallow exotic options on meta fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8143</link><project id="" key="" /><description>We have some mapping options that sound interesting but are actually almost useless or dangerous. I propose to remove them:
### `_type: { index: no }`

Not indexing the `_type` sounds appealing since documentation mentions everything will keep on working, so that should just save space. Except that elasticsearch will internally run a prefix query on the `_uid` field instead, which is going to be super slow. I think we should remove this option.
### `_type: { store: yes }` and `_id: { store: yes }`

Storing the type and _id is useless since we already enforce the _uid to be stored, and the _uid contains these informations.
### `_id: { index: not_analyzed }`

The `_id` field is the same for all documents so we should not need to index, store or doc-value it. (Can be done now thanks to https://github.com/elasticsearch/elasticsearch/pull/6073 and https://github.com/elasticsearch/elasticsearch/pull/7965)

In general I'm wondering if we shouldn't go further and completely lock down how data is indexed/stored/docvalued for meta fields. There would just remain high-level configuration options such as `enabled` on the `_timestamp` mapper or `type` on `_parent`.

Relates to #8870
</description><key id="46108390">8143</key><summary>Mappings: disallow exotic options on meta fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T14:56:29Z</created><updated>2015-09-10T20:33:44Z</updated><resolved>2015-02-27T21:25:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T16:56:05Z" id="59542511">+1  I get the feeling a number of these options were added just in case the current settings didn't work out so well, but I think we can safely declare them battle tested now
</comment><comment author="rjernst" created="2014-10-18T00:08:50Z" id="59591236">&gt; In general I'm wondering if we shouldn't go further and completely lock down how data is indexed/stored/docvalued for meta fields.

+1
</comment><comment author="jpountz" created="2015-02-20T09:46:23Z" id="75212681">Removed the `discuss` label, let's do it!
</comment><comment author="rjernst" created="2015-02-23T22:38:14Z" id="75653206">I started work on this, but realized the branch was becoming massive with all the test fixes for different meta fields.  I'm now attempting to have a separate PR for each meta field.  First one is simple, just _uid: #9836
</comment><comment author="rjernst" created="2015-02-27T21:25:53Z" id="76474569">`_timestamp` and `_all` are the only meta fields left that can have their field type modifed.  I'm going to close this issue as `_timestamp` has its own discussion going in #9679 and `_all` I'm not sure what we can do there to limit it down (more discussion is probably needed), but we probably want to lock out some settings or improve error messages (right now, if you try to set it to not indexed, it just silently indexes it anyways...)
</comment><comment author="jpountz" created="2015-02-27T21:38:36Z" id="76476633">Thanks @rjernst these changes are already awesome!
</comment><comment author="djschny" created="2015-09-09T13:55:03Z" id="138916379">In regards to the __type_ disabling, I've used this historically when I already had a field on my document that was the same value as the __type_ and therefore indexing it was not necessary since would craft the queries to filter on that field instead. For folks that have billions of very small documents, it was my understanding that not indexing __type_ was helpful as the ratio of that data compared to overall document size was larger.
</comment><comment author="rjernst" created="2015-09-09T16:17:27Z" id="138960838">@djschny if there is a field that duplicates a meta field, why can't the user not send that field? _type is essentially a virtual field on _uid, so it is actually not using any more memory than would be used nornally. 
</comment><comment author="djschny" created="2015-09-10T15:08:28Z" id="139274715">@rjernst Sure a user could not send that field, but that may not be ideal for them as they want their document to contain that field explicitly for when pulling data back out of ES. I understand that _type is essentially a virtual field on _uid, but it is my understanding that unless it is indexed separately then queries to find all docs of a particular type use a prefix query on _id which is not as ideal as a filter.

Why I bring this up is there are valid use cases for the configuration of _type to not be indexed and it's not an "exotic" use case. I think its very important to draw a distinction between a configuration that is invalid or not necessary (like the "store" on _type) vs. ones that have a practical use case.
</comment><comment author="rjernst" created="2015-09-10T20:33:43Z" id="139371186">I said "essentially" because it wasn't quite true. `_type` is indexed (the virtual field over _uid is for returning type as a stored field), but this is the cost of having the type system. Having types not indexed is not a valid use case, it is necessary for the basic operation of elasticsearch (doing a search like `GET /myindex/type1,type2,type3/_search`).

If a user doesn't want to pay that cost, they can not use types. By that I mean: send all their documents with the same ES type (the type will still be indexed, but the posting list will be highly compressed because all docs will have the same value). Then they can make their `type` field (notice no underscore) and do with it as they please.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>core/src/test/java/org/elasticsearch/document/BulkTests.java</file><file>core/src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file><file>core/src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>core/src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>core/src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>core/src/test/java/org/elasticsearch/timestamp/SimpleTimestampTests.java</file><file>core/src/test/java/org/elasticsearch/ttl/SimpleTTLTests.java</file><file>core/src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Mappings: Lockdown _timestamp</comment></comments></commit><commit><files /><comments><comment>Docs: Cleanup meta field docs</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/ttl/TTLMappingTests.java</file><file>src/test/java/org/elasticsearch/ttl/SimpleTTLTests.java</file><file>src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Mappings: Lock down _ttl field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/size/SizeMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>Mappings: Lock down _size field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/internal/FieldNamesFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/search/query/ExistsMissingTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Mappings: Lock down _field_names field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/routing/RoutingTypeMapperTests.java</file></files><comments><comment>Mappings: Lock down _routing field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingTests.java</file></files><comments><comment>Mappings: Lock down _index field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/test/java/org/elasticsearch/count/query/CountQueryTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/ContextSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Mappings: Lock down _type field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/percolator/QueriesLoaderCollector.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/QueryCollector.java</file><file>src/test/java/org/elasticsearch/count/query/CountQueryTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/id/IdMappingTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java</file><file>src/test/java/org/elasticsearch/search/query/SearchQueryTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Mappings: Lock down _id field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file></files><comments><comment>Mappings: Lock down _uid field</comment></comments></commit></commits></item><item><title>Mappings: Ensure that reindexing is always possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8142</link><project id="" key="" /><description>Our mappings are very permissive in terms of what can be enabled or disabled, and in particular it is possible to disable the `_source`. This can be problematic if at some point you need to perform a change that requires reindexing (eg. an analyzer change) because this means that the data needs to be pulled again from another data source.

Instead, we could disable some mappings options in order to make sure that we always have all the information needed to reindex (eg. with [client helpers](http://elasticsearch-py.readthedocs.org/en/latest/helpers.html#elasticsearch.helpers.reindex)). Here are the settings that we would need to enforce:
- `_source` is enabled and stored
- `_timestamp`, `_ttl`, `_routing`, `_parent` are stored

Relates to #8870
</description><key id="46105054">8142</key><summary>Mappings: Ensure that reindexing is always possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label></labels><created>2014-10-17T14:26:19Z</created><updated>2015-08-13T14:29:25Z</updated><resolved>2015-05-06T05:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T14:30:49Z" id="59521043">+1
</comment><comment author="rmuir" created="2014-10-17T14:43:51Z" id="59523053">+1
</comment><comment author="dadoonet" created="2014-10-17T21:44:44Z" id="59580697">I'm aware of some corner cases where people would definitely not store the source and want to get back only _id.
And as they send a massive amount of data, they don't want to store _source neither fields.
Same for users who only want to compute. 

Not saying that we should not do it but that we need to be aware of the consequences for some users.
As I said, extreme corner cases though.
</comment><comment author="rjernst" created="2014-10-18T00:18:17Z" id="59591728">+1
</comment><comment author="Vineeth-Mohan" created="2014-10-24T06:28:53Z" id="60349470">+1
</comment><comment author="clintongormley" created="2015-02-12T18:04:15Z" id="74120058">I can imagine there are certain expert cases where users actually do want to disable storing `_source`, so perhaps we should still maintain the ability to do so, but make it much harder. eg some non-dynamic cluster setting, the documentation for which also lists all the things you will lose if you disable `_source`.
</comment><comment author="bleskes" created="2015-02-12T18:10:05Z" id="74121071">+1 to that latest suggestion.

On Thu, Feb 12, 2015 at 7:04 PM, Clinton Gormley notifications@github.com
wrote:

&gt; ## I can imagine there are certain expert cases where users actually do want to disable storing `_source`, so perhaps we should still maintain the ability to do so, but make it much harder. eg some non-dynamic cluster setting, the documentation for which also lists all the things you will lose if you disable `_source`.
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/8142#issuecomment-74120058
</comment><comment author="jpountz" created="2015-02-12T19:00:52Z" id="74130313">Can we get more information about these expert use-cases? Having the `_source` stored is so valuable that I would like to make sure that we don't allow users to disable this without good reasons. And if we still decide to allow disabling the source, can we consider making backward compatibility best-effort only on such indices?
</comment><comment author="clintongormley" created="2015-02-12T19:10:08Z" id="74132199">eg from an email I received:

&gt; &gt; A binary search engine? How are you indexing the binary fields to make them searchable?
&gt; 
&gt; The client uses Lucene to search over his binary data. In summary they insert and query data sending chunks in a binary format. Internally this binary format is broken into terms (base64 small strings) and offsets for each term. So, during the search they verify if the term and the offsets matches, returning the documents with the most matches of terms respecting the offsets. They have created new queries, scorers, collectors, refiners, etc.

This custom plugin wasn't able to keep up with indexing and query speed, but disabling the source allowed them to do that.

I realise that this is not typical, hence "expert case", but not storing the _source could be a tradeoff that some people would be willing to make. I just want to make it hard enough that the ordinary user won't do it without realising the consequences.

&gt; And if we still decide to allow disabling the source, can we consider making backward compatibility best-effort only on such indices?

Yes absolutely.  That is one of the tradeoffs.
</comment><comment author="colings86" created="2015-02-20T09:52:48Z" id="75213414">Maybe we should have a cluster level setting to allow indices to disable the _source field? this way we can put a big notice in the documentation warning about how this is an expert feature and what affect it has on backwards compatibility. Also with a cluster-wide setting, we can detect the setting on startup and put a warning in the logs stating that backwards compatibility is not guaranteed with this setting on. This way we will have made it very clear to the user that there are serious implications to turning on this setting
</comment><comment author="brwe" created="2015-02-24T13:20:50Z" id="75754710">Disabling `_source`  does reduce the size on disk:
https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/cZ7tkx8HZ70/AOFAt42mNt0J
I think when people store the raw data outside es in some secondary store then we should not force them to store the source in es again or even make it hard to disable it.
</comment><comment author="rmuir" created="2015-02-24T13:50:15Z" id="75758843">&gt; This custom plugin wasn't able to keep up with indexing and query speed, but disabling the source allowed them to do that.

They are indexing base64 binary? Come on man, you cant get much more esoteric than that.

These excuses are pathetic.
</comment><comment author="rmuir" created="2015-02-24T13:51:35Z" id="75759052">Also we optimized the stored fields in lucene a lot for 5.0, cpu costs around merging and so on are just not comparable to previous versions in most cases.

So any old complaints, benchmarks, etc around this ---&gt; obselete.
</comment><comment author="s1monw" created="2015-02-24T14:11:33Z" id="75762200">&gt; They are indexing base64 binary? Come on man, you cant get much more esoteric than that.

I really agree here with robert. It's requirements like those that prevent progress and make 1 user happy for the cost of 99% having a harder life!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayTests.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/CompressSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/typelevels/ParseMappingTypeLevelTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/compress/SearchSourceCompressTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Mappings: Remove ability to disable _source field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/source/DefaultSourceMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file><file>src/test/java/org/elasticsearch/search/innerhits/InnerHitsTests.java</file></files><comments><comment>Mappings: Remove includes and excludes from _source</comment></comments></commit></commits></item><item><title>PluginManager: Add an --update command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8141</link><project id="" key="" /><description>Adds a simple --update command that combines a remove + install commands.

Closes #5064
</description><key id="46103649">8141</key><summary>PluginManager: Add an --update command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">tlrx</reporter><labels><label>enhancement</label><label>stalled</label></labels><created>2014-10-17T14:13:05Z</created><updated>2015-11-03T09:39:51Z</updated><resolved>2015-11-03T09:39:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-18T16:49:27Z" id="59621250">I'd prefer to see `UPDATE` as an actual COMMAND not as a shortcut to 2 commands.
If we think about it as a shortcut, it could also be implemented in shell/bat scripts as:

``` sh
bin/plugin --remove repo/plugin/version
bin/plugin --install repo/plugin/version
```

And not in the plugin manager itself.

It means we should have a new action:

``` java
case ACTION.UPDATE:
  // Do Remove and then Install
```

Instead of a list of actions. Though I know that it will mean duplicating code.

It makes more sense with the upcoming change on plugin manager. See #7339 for info. 

Also, a side note about this. To install a plugin, you need to provide a given structure:

``` sh
bin/plugin --install org/artifact/version
```

For example:

``` sh
bin/plugin --install elasticsearch/elasticsearch-transport-thrift/2.4.0
```

But the plugin will be installed and known by elasticsearch as `transport-thrift`.
If you run `bin/plugin --list`, the output will refer to `transport-thrift`.

When a user wants to remove a plugin, he can do either:

``` sh
bin/plugin --remove elasticsearch/elasticsearch-transport-thrift/2.4.0
# Or
bin/plugin --remove transport-thrift
```

This could be confusing for users. They could think that running `bin/plugin --update transport-thrift` will actually try to install the latest available version.
I think we should check and reject it.

Users will need to provide `org/artifact/version` as they did when installing:

``` sh
bin/plugin --update elasticsearch/elasticsearch-transport-thrift/2.4.1
```
</comment><comment author="tlrx" created="2014-10-23T10:23:06Z" id="60219317">Thanks @dadoonet for the review!

&gt; If we think about it as a shortcut, it could also be implemented in shell/bat scripts as:
&gt; 
&gt; bin/plugin --remove repo/plugin/version
&gt; bin/plugin --install repo/plugin/version
&gt; And not in the plugin manager itself.

Doing this in scripts as you suggest imposes to implement the update logic in at least two places (shell + bat) and controls return/exit codes correctly. Also, scripts are more complex to unit test.

&gt; I'd prefer to see UPDATE as an actual COMMAND not as a shortcut to 2 commands.
&gt; Instead of a list of actions. Though I know that it will mean duplicating code.

In the context of the issue #5064, we agreed on defining `update` as a composion of a remove + install command. I found it simplier to not define another command since no extra logic is needed here. This avoid us to duplicate code and tests as you suggested. But that's just a point of view :)

BTW, @clintongormley assigned me the issue. Clinton, how do you want me to move forward on this issue? Closing this PR and wait for #7339 to be merged?

Thanks :)
</comment><comment author="clintongormley" created="2014-10-23T10:48:02Z" id="60221604">Hi @tlrx 

I agree with @dadoonet that update should be an actual command rather than a convenience shortcut. The command itself can call the remove &amp; install commands in the plugin manager itself, but it should be a real command, in case we want to add any other logic to it.

And yes, looks like it will have to wait for #7339 to be merged first.  
</comment><comment author="tlrx" created="2014-10-23T10:49:13Z" id="60221692">Ok, thanks!
</comment><comment author="tlrx" created="2015-11-03T09:39:51Z" id="153299002">Obsolete pull request
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>non fatal npe in warmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8140</link><project id="" key="" /><description>I've seen the stacktrace below in our integration tests a couple of time. We're starting elasticsearch as an embedded node. The error appears to be non fatal since our integration tests pass anyway. I've seen this stacktrace twice in the past week but can't reproduce it reliably.

We are running our maven tests concurrently and in randomized order, so there are a lot of integration tests hitting our elasticsearch node all at once right after it starts and reports a green status.

Using elasticsearch 1.4.0 Beta1

17-10-2014T15:56:40+0200 W warmer - [test-node-gstJI] [inbot_users_v27][2] failed to load random access for [_type:usercontact]
org.elasticsearch.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.getAndLoadIfNotPresent(FixedBitSetFilterCache.java:132) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.access$100(FixedBitSetFilterCache.java:75) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer$1.run(FixedBitSetFilterCache.java:284) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0]
    at java.lang.Thread.run(Thread.java:744) [na:1.8.0]
Caused by: java.lang.NullPointerException: null
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:157) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:132) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[elasticsearch-1.4.0.Beta1.jar:na]
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[elasticsearch-1.4.0.Beta1.jar:na]
    ... 8 common frames omitted
</description><key id="46102651">8140</key><summary>non fatal npe in warmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">jillesvangurp</reporter><labels><label>bug</label><label>v1.4.0</label></labels><created>2014-10-17T14:03:24Z</created><updated>2014-10-22T08:27:44Z</updated><resolved>2014-10-22T08:27:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T14:17:48Z" id="59519143">thanks for reporting @jillesvangurp - we'll take a look
</comment><comment author="martijnvg" created="2014-10-17T15:24:26Z" id="59529312">@jillesvangurp I think I found a small issue where there is a small window of time that a field is unset, but the warmer that needs is running.  In order to confirm this can you share how you start the embedded node and run your test? (for example you wait for green status before running?) Sharing code snippets how the node is brought up before the tests run would even be more helpful.
</comment><comment author="jillesvangurp" created="2014-10-17T15:28:37Z" id="59529957">Sure no problem:

```
            String defaultIndexDirectory = "target/data-"+UUID.randomUUID().toString();
            String indexDir = config.getString("estestserver.indexdir",defaultIndexDirectory);
            String logDir = config.getString("estestserver.logdir",defaultIndexDirectory + "/logs");
            String esPort = config.getString("estestserver.port","9299");
            File file = new File(defaultIndexDirectory);
            file.mkdirs();
            LOG.info("using " + file.getAbsolutePath() + " for es data and logs");
            Settings settings = ImmutableSettings.settingsBuilder()
                    .put("name", "test-node-"+RandomStringUtils.randomAlphabetic(5))
                    .put("cluster.name", "linko-dev-cluster-"+RandomStringUtils.randomAlphabetic(5))
                    .put("index.gateway.type", "none")
                    .put("gateway.type", "none")
                    .put("discovery.zen.ping.multicast.ping.enabled", "false")
                    .put("discovery.zen.ping.multicast.enabled", "false")
                    .put("path.data", indexDir)
                    .put("path.logs", logDir)
                    .put("foreground", "true")
                    .put("http.port", esPort)
                    .put("http.cors.enabled", "true")
                    .put("http.cors.allow-origin","/https?:\\/\\/(localhost|kibana.*\\.linko\\.io)(:[0-9]+)?/")
                    .build();

            LOG.info(settings.toDelimitedString(';'));

            NodeBuilder nodeBuilder = NodeBuilder.nodeBuilder()
                    .settings(settings)
                    .loadConfigSettings(false);
            node = nodeBuilder
                    .build();


            // register a shutdown hook
            Runtime.getRuntime().addShutdownHook(new Thread() {
                @Override
                public void run() {
                    node.close();
                }
            });
            node.start();

            // wait until the shards are ready
            node.client().admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();
```
</comment><comment author="jillesvangurp" created="2014-10-18T10:39:42Z" id="59606767">One additional bit of information that I just realized may be relevant here is that we have a parent child relation between user and usercontact. So, the exception is happening when it is doing something with the child type. For reference, here's a gist with the full mapping for the index: https://gist.github.com/jillesvangurp/d0cd29573b876f9cc4d3
</comment><comment author="jillesvangurp" created="2014-10-18T10:50:34Z" id="59607049">Also, we're using testNg and surefire. The elasticsearch node is started in a @BeforeSuite. We use a very large threadcount and randomized order to surface any issues related to inter test dependencies and stability of our system. This pretty much means all our integration test classes are starting at the same time. We generally use randomized test data and there are a lot of calls to /_refresh to ensure indices are committed in each tests. 

```
&lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;parallel&gt;classes&lt;/parallel&gt;
                    &lt;threadCount&gt;50&lt;/threadCount&gt;
                    &lt;runOrder&gt;random&lt;/runOrder&gt;
                    &lt;argLine&gt;-Xms1024m -Xmx2048m&lt;/argLine&gt;
                    &lt;properties&gt;
                        &lt;property&gt;
                            &lt;name&gt;listener&lt;/name&gt;
                            &lt;value&gt;io.linko.ng.testutil.TestProgressLogger&lt;/value&gt;
                        &lt;/property&gt;
                    &lt;/properties&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
```
</comment><comment author="martijnvg" created="2014-10-20T14:59:15Z" id="59770996">Thanks for the provided information @jillesvangurp 

I opened: #8168 for this issue. Are you able to verify if the non fatal NPE doesn't occur any more with this fix in your test infrastructure?
</comment><comment author="jillesvangurp" created="2014-10-20T15:11:16Z" id="59774652">No problem. Unfortunately, most of our builds don't trigger this exception; so it is a bit hard for me to confirm. I've only spotted it twice out of dozens of test runs over the past week. If you issue another beta, I'll be able to depend on that at least.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/fixedbitset/FixedBitSetFilterCache.java</file></files><comments><comment>Core: Add warmer listener only when index service is set, in order to prevent possible NPE.</comment></comments></commit></commits></item><item><title>Store `_timestamp` by default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8139</link><project id="" key="" /><description>Storing `_timestamp` by default means that under the default configuration, you
would have all the information you need in order to reindex into a different
index.
</description><key id="46102208">8139</key><summary>Store `_timestamp` by default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T13:59:05Z</created><updated>2015-06-07T11:58:37Z</updated><resolved>2014-10-20T10:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T14:00:54Z" id="59516712">You might need `_ttl` too? Or is it already stored?
</comment><comment author="jpountz" created="2014-10-17T14:06:09Z" id="59517466">I initially thought it was not stored, but it appears to be. So `_timestamp` was the only missing piece: after this change we would store by default everything that we need to be able to reindex without losing information.
</comment><comment author="rjernst" created="2014-10-18T00:01:52Z" id="59590874">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/update/UpdateMappingOnClusterTests.java</file></files><comments><comment>Mappings: Store _timestamp by default.</comment></comments></commit></commits></item><item><title>Reduce memory usage during fetch source sub phase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8138</link><project id="" key="" /><description>If includes or excludes are set
XContentFactory.xcontentBuilder() allocates a new
BytesStreamOutput using the default page size which is 16kb.

Can be optimized to use the length of the sourceRef because
that is the maximum possible size that the streamOutput will
use.

This redcues the amount of memory allocated for a request
that is fetching 200.000 small documents (~150 bytes each)
by about 300 MB
</description><key id="46097688">8138</key><summary>Reduce memory usage during fetch source sub phase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mfussenegger</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T13:14:33Z</created><updated>2015-06-07T17:14:44Z</updated><resolved>2014-10-31T18:00:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-20T11:14:15Z" id="59732035">@mfussenegger This change looks good to me. I just left a minor comment, please let me know what you think.
</comment><comment author="mfussenegger" created="2014-10-20T13:23:08Z" id="59752588">Sounds good to me. I've added a fixup commit with the change. Let me know if I should squash them.
</comment><comment author="jpountz" created="2014-10-20T13:29:06Z" id="59753410">Unsquashed commits make the review easier, I will squash when pushing. Thanks!
</comment><comment author="clintongormley" created="2014-10-28T11:03:37Z" id="60739191">@jpountz please can you review?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/source/FetchSourceSubPhase.java</file></files><comments><comment>Search: Reduce memory usage during fetch source sub phase.</comment></comments></commit></commits></item><item><title>Add support for `http.publish_port` </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8137</link><project id="" key="" /><description>Will help with Docker setups. 
See https://github.com/elasticsearch/elasticsearch/issues/5302#issuecomment-59504440 for more.
</description><key id="46096732">8137</key><summary>Add support for `http.publish_port` </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-10-17T13:04:12Z</created><updated>2014-12-11T15:10:47Z</updated><resolved>2014-12-11T15:10:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/http/netty/NettyHttpServerTransport.java</file><file>src/test/java/org/elasticsearch/http/netty/HttpPublishPortTests.java</file></files><comments><comment>HTTP: Add 'http.publish_port' setting to the HTTP module</comment></comments></commit></commits></item><item><title>Dynamic changes to `max_merge_count` are now picked up by index throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8136</link><project id="" key="" /><description>Today, index throttling won't notice any dynamic/live changes to max_merge_count.

So, I just fixed the throttle code to ask the MergeSchedulerProvider for its maxMergeCount every time a merge starts/finishes.

This means after a dynamic change, it will be the next merge that starts/finishes until the throttling notices the change.  We could also install an UpdateSettingsListener to force throttling to notice the change immediately, but that's more complex and I think this simple solution is sufficient.

Closes #8132
</description><key id="46095781">8136</key><summary>Dynamic changes to `max_merge_count` are now picked up by index throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T12:53:24Z</created><updated>2015-06-07T18:20:16Z</updated><resolved>2014-10-18T08:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-17T22:41:40Z" id="59585778">LGTM.
</comment><comment author="kimchy" created="2014-10-17T22:56:33Z" id="59586940">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file></files><comments><comment>Core: fix index throttling to notice a change to max_merge_count on the next merge start/finish</comment></comments></commit></commits></item><item><title>Allow setting individual breakers to "noop" breakers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8135</link><project id="" key="" /><description>This adds a NoopCircuitBreaker, and then adds the dynamic settings
`indices.breaker.fielddata.type` and `indices.breaker.request.type`,
which can be set to "noop" in order to use a breaker that will never
break, and incurs no overhead during computation.

This also refactors the tests for the CircuitBreakerService to use
@Before and @After functions as well as adding settings in
ElasticsearchIntegrationTest to occasionally use NOOP breakers for all
tests.
</description><key id="46095067">8135</key><summary>Allow setting individual breakers to "noop" breakers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>feature</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T12:44:29Z</created><updated>2015-06-06T18:19:33Z</updated><resolved>2014-10-20T08:54:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-10-17T12:53:59Z" id="59508017">few minor comments (they apply to other parts of the code).

Can we also change `indices.breaker.breaker_impl` to `indices.breaker.type`?
</comment><comment author="dakrone" created="2014-10-17T12:57:34Z" id="59508422">@kimchy addressed your feedback and marked this as breaking.
</comment><comment author="dakrone" created="2014-10-17T12:59:28Z" id="59508640">Actually, since there was no separate CircuitBreakerService implementations in 1.3, this isn't breaking! :)
</comment><comment author="kimchy" created="2014-10-17T13:00:15Z" id="59508739">LGTM, would love someone to have another look at the tests
</comment><comment author="rjernst" created="2014-10-17T23:34:01Z" id="59589256">I left some comments on the tests, all minor.  LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor parent/child to not be Lucene queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8134</link><project id="" key="" /><description>Parent/child queries have a non-desirable property: given a parent/child query Q, updating a document in segment A might change the set of matching document in another segment B.

This is an issue because it means that parent/child queries and filters cannot be cached per segment, so we had to add [logic](https://github.com/elasticsearch/elasticsearch/pull/4774) to make sure these queries don't get cached, either directly or as part of a cached parent filter (eg. under a cached `bool` filter). The propagation logic can be a bit [fragile](https://github.com/elasticsearch/elasticsearch/pull/7685) so I think we should work on a better fix.

One idea could be to change the abstraction we have to match document from a single Lucene query to something that could perform several Lucene queries. For instance in the case of `has_child`, we could have a first query that would collect parent ids and then build a new query based on these ids. This is the same execution logic, but each query on its own would solely depend on data that is stored in the current segment, so they would be cacheable (even though it might not a good idea to cache them).
</description><key id="46093445">8134</key><summary>Refactor parent/child to not be Lucene queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>high hanging fruit</label><label>PITA</label></labels><created>2014-10-17T12:24:00Z</created><updated>2015-05-29T19:57:43Z</updated><resolved>2015-05-29T19:57:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-20T21:08:17Z" id="59840006">+1 to make the parent/child logic less fragile.

Just wondering how these two separate searches are executed:
1) We could run the first search in the query parser and pass the matching parent ordinals (optionally with scores) as argument to the query/filter that is returned by the parser.
2) We could back to how has_child was executed around version `0.19` where there was scoped queries infrastructure. If a query implemented `ScopePhase` interface it ran twice as separate queries. First just the inner query ran and it collected parent ids, then the actual has_child query ran and only emitted parent docs that had a parent id that was collected before. Running with two distinct queries would mean that we wouldn't run into this filter cache issue. I believe this mechanism was removed because it added additional complexity in the query phase that was unwanted at that time. Maybe was can bring it back to live and simplify it.
</comment><comment author="clintongormley" created="2014-10-21T08:08:16Z" id="59892872">Wondering if it would be possible to take parent-level filters into account within the child queries. For instance, imagine you have a filter on the parent which matches only 1% of your parents, but the `has_child` query/filter matches 90% of your docs.  Currently, the `has_child` clause can't take advantage of the parent-level filter and so has to match all 90% of parents regardless.
</comment><comment author="gmenegatti" created="2014-10-22T21:02:36Z" id="60154807">+1 one million times @martijnvg @clintongormley 
For a company like ours that currently has more than 500 million parents and in few months around 60 billion children, and where the goal is to count and filter the number of parent based on some data that is stored on the child, this is definitely important.
</comment><comment author="iddoav" created="2014-10-25T07:35:00Z" id="60474697">The optimization @clintongormley suggests would be very beneficial for our use-cases in Totango. Probably also @martijnvg's. Currently, the latency for such queries is not acceptable product-wise, and we'll have to work around with ugly denormalization. What we really want, is to have the parent-child queries to work faster. So, I am happy that you're pushing for it.
</comment><comment author="RossLieberman" created="2015-01-28T19:03:13Z" id="71894256">I too ran into the issue described in #5116 using rescore with has_child. Performing the query X times for each set of rescore works and is the difference between 60ms (each rescore) versus 10,000ms for my data, but it would be more ideal to just use the parent's data set during rescore or even better if function_score in the original query supported summing child document values to generate the score on each node before combining on the cluster.
</comment><comment author="jpountz" created="2015-02-13T08:51:47Z" id="74222649">I tried to think more about it and essentially:
1. the current design has the issue that queries are not cacheable per segment because whether a document matches a document depends on what happens in other segments as well
2. as a work-around, we could run the first phase (collecting ids) in the query parser and then return a query for the second phase (which would essentially be a fielddata terms filter with scores for each term). However this query would be specific to a given index searcher and it would not work for things that want to keep queries alive for a long time like filtered aliases or the percolator
3. using a different abstraction is tempting, but then you could only use parent/child queries as top-level queries (ie. no more nesting in boolean queries)
</comment><comment author="martijnvg" created="2015-02-13T09:17:15Z" id="74225371">@jpountz I think that option 2 is fair. The terms collected in phase1 are always tied to a given IndexSearch/DirectoryReader, so if that changes the collected terms are invalid. I think for the percolator we should forbid the usage of parent/child queries.

What I also wanted to tackle with this issue is making parent/child queries smarter in terms of execution. Currently most of the times parent/child queries need either evaluate all parent or children documents (depending on whether `has_child` or `has_parent` is used) if there is a match with the inner query, which is unneeded because usually parent/child queries are part of a bigger query and therefor not all parents/children will ever match. 

The best thing I can come up with is to make the parent/child queries execute lazily. So for example if a parent document matches with all other queries we push it to has_child and check it has a child document and if it matches with the inner query. Maybe we need to buffer all matching parent docs in order to make this efficient. The easiest place this approach could be applied is if has_child was defined as a post filter.  If has_child was defined in a bool query in the main query applying this approach is trickier, because of how the query execution works at the moment.
</comment><comment author="jpountz" created="2015-02-20T10:06:44Z" id="75215043">&gt; The best thing I can come up with is to make the parent/child queries execute lazily. So for example if a parent document matches with all other queries we push it to has_child and check it has a child document and if it matches with the inner query.

I believe this could be done with two-phase iteration that has been introduced in Lucene 5.1: https://issues.apache.org/jira/browse/LUCENE-6198

We just discussed this issue a bit more and agreed on the following points:
- the limitations of option 2 are fair (no use in filtered aliases or the percolator)
- we need to move the infrastructure to Lucene
</comment><comment author="martijnvg" created="2015-02-22T21:54:37Z" id="75464977">&gt; I believe this could be done with two-phase iteration that has been introduced in Lucene 5.1

Cool, with that the second phase matching logic in `has_child` and `has_parent` then only needs to be executed for docs we know that are going to match. This should speed things up significantly.

&gt; we need to move the infrastructure to Lucene

+1 I think we should rewrite the JoinUtil in the join module to do what `has_child` and `has_parent` do. 
</comment><comment author="rmuir" created="2015-02-25T16:27:20Z" id="75992456">+1 this sounds great.
</comment><comment author="pauleil" created="2015-05-06T18:02:31Z" id="99554133">What are your plans on this task? Asking because it blocks #2917, which (at least for us) would save an enormous amount of effort and maintenance involved in roundabout solutions.
</comment><comment author="pauleil" created="2015-05-28T14:01:24Z" id="106325506">Does your commit mean that you've solved this? If so, that's really great.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/index/AbstractIndexComponent.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/children/ChildrenParser.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/AbstractChildTests.java</file><file>src/test/java/org/elasticsearch/search/child/ChildQuerySearchBwcTests.java</file><file>src/test/java/org/elasticsearch/search/child/ChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/search/child/ParentFieldLoadingBwcTest.java</file><file>src/test/java/org/elasticsearch/search/child/ParentFieldLoadingTest.java</file><file>src/test/java/org/elasticsearch/update/UpdateTests.java</file></files><comments><comment>Parent/child: refactored _parent field mapper and parent/child queries</comment></comments></commit></commits></item><item><title>Remove partial fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8133</link><project id="" key="" /><description>Partial fields have been deprecated since 1.0.0Beta1 in favor of _source
filtering. They will be removed in 2.0.
</description><key id="46089375">8133</key><summary>Remove partial fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T11:26:39Z</created><updated>2015-06-06T16:21:36Z</updated><resolved>2014-10-20T10:31:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T13:06:50Z" id="59509554">++
</comment><comment author="rjernst" created="2014-10-17T23:37:24Z" id="59589450">LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: dynamic updates to max_merge_count is ignored by index throttling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8132</link><project id="" key="" /><description>With #6066 we added index throttling when merges cannot keep up, which is important since this ensures index remains healthy (does not develop ridiculous number of segments).

It works by watching the number of merges that need to run, and if this exceeds max_merge_count, it starts throttling.

However, max_merge_count is dynamically updatable, but when you update it dynamically, the index throttling doesn't notice and keeps throttling at the original max_merge_count (on ES startup).
</description><key id="46088983">8132</key><summary>Core: dynamic updates to max_merge_count is ignored by index throttling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T11:21:37Z</created><updated>2014-10-18T08:32:47Z</updated><resolved>2014-10-18T08:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file></files><comments><comment>Core: fix index throttling to notice a change to max_merge_count on the next merge start/finish</comment></comments></commit></commits></item><item><title>Add all meta fields to the top level json document in search response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8131</link><project id="" key="" /><description>Some of our meta fields (such as _id, _version, ...) are returned as top-level
properties of the json document, while other properties (_timestamp, _routing,
...) are returned under `fields`. This commit makes all meta fields returned
as top-level properties.

So eg. `GET test/test/1?fields=_timestamp,foo` would now return

``` json
{
   "_index": "test",
   "_type": "test",
   "_id": "1",
   "_version": 1,
   "_timestamp": 10000000,
   "found": true,
   "fields": {
     "foo": [ "bar" ]
   }
}
```

while it used to return

``` json
{
   "_index": "test",
   "_type": "test",
   "_id": "1",
   "_version": 1,
   "found": true,
   "fields": {
     "_timestamp": 10000000,
     "foo": [ "bar" ]
   }
}
```
</description><key id="46087993">8131</key><summary>Add all meta fields to the top level json document in search response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:REST</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T11:07:25Z</created><updated>2015-06-06T16:22:56Z</updated><resolved>2015-06-05T07:15:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T12:27:52Z" id="59505281">Should we return these metadata fields by default?  And, for bwc, include them under `fields` only if they are asked for?
</comment><comment author="jpountz" created="2014-10-17T12:47:26Z" id="59507277">@clintongormley  I am working on separate changes in order to
- store `_timestamp` by default (it's not the case today)
- and add the "important" (definition still needs to be defined) meta fields to the response by default

I set the fix version to 2.0 so that we can make it break backward compatibility, is that ok?
</comment><comment author="clintongormley" created="2014-10-17T13:10:20Z" id="59509960">Sure :)
</comment><comment author="s1monw" created="2015-03-23T13:28:02Z" id="84996891">@jpountz are you going to revisit this one?
</comment><comment author="clintongormley" created="2015-06-04T19:24:32Z" id="109019038">@jpountz can we get this in?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/get/GetResult.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file></files><comments><comment>Merge pull request #8131 from jpountz/enhancement/top-level-meta-fields</comment></comments></commit></commits></item><item><title>doc_count for key in bucket has another value than match query returns for this key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8130</link><project id="" key="" /><description>Hello, folks.

My problem in more detail.

/var/logs/log.tsv - original log file. 

Command 'grep' shows us that there are 87 entries for ip 'x.x.x.x' in this log.
grep 'x.x.x.x' /var/logs/log.tsv | wc -l
87

I insert this log into elasticsearch.

Then I do follow query for ip 'x.x.x.x'.
curl -XPOST 'localhost:9200/test/log/_search?pretty' -d '
{
    "size":100,
    "query": { 
                "match" : {
                            "remote_addr" : "x.x.x.x"
                }
    }
}' &gt; /tmp/query

Let's check output:
grep 'x.x.x.x' /tmp/query | wc -l
87

It's right!

Then I do very simple aggs query:
curl -XPOST 'localhost:9200/test/log/_search?pretty' -d '
 {
   "size": 0,
   "aggs": {
     "group_by_remote_addr": {
       "terms": {
         "size": 20,
         "field": "remote_addr"
       }
     }
   }
 }'
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 20000,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_remote_addr" : {
      "buckets" : [ 
      ...
      , {
        "key" : "x.x.x.x",
        "doc_count" : 46
      } ]
    }
  }
}

And this query returns wrong result: "doc_count" : 46.

Do you have any thoughts about reasons of this mismatch?
</description><key id="46085368">8130</key><summary>doc_count for key in bucket has another value than match query returns for this key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fervid</reporter><labels /><created>2014-10-17T10:30:13Z</created><updated>2014-10-17T16:17:50Z</updated><resolved>2014-10-17T11:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T11:57:07Z" id="59502437">The terms aggregation is approximate.  You can read more about this in the docs for 1.4: http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-terms-aggregation.html#_document_counts_are_approximate  (Note, the section titled "calculating document count error" only applies to 1.4 onwards).
</comment><comment author="clintongormley" created="2014-10-17T16:17:50Z" id="59537330">&gt; As there wrote: "document counts (and the results of any sub aggregations)
&gt; in the terms aggregation are not always accurate because each shard
&gt; provides its own view of what the ordered list of terms should be".
&gt; 
&gt; But there is one shard in my case and doc_count is not correct. Why?

Not according to your output:

"_shards" : {
  "total" : 5,
  "successful" : 5,
  "failed" : 0
},

</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion Suggest: Omitting results even output is different</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8129</link><project id="" key="" /><description>I have a case where the completion suggester is omitting results, below are two example documents. For a search like "Samsung", only Document 1 will be returned. Actually its the only document returned, so the `size=10` is not reached. The only major difference i can is see is the weight, will the completion suggester omitt result if there's a huge difference in weight?

Document 1:
`{
      "_index" : "suggester",
      "_type" : "suggestions",
      "_id" : "b089f7c6a85f39fb882398b6c8ba34c1",
      "_score" : 1.0,
      "_source":{"suggest":{"input":["Sm","Samsung Mobiles Tablets","Mobiles \u0026 Tablets","(Export) Galaxy Note 4 Sm","Galaxy Note 4 Sm","Note 4 Sm","4 Sm"], "output":62020, "payload":{"br":"Samsung", "ca":"Mobiles \u0026 Tablets", "cl":"N910C LTE 32GB White","co":"product","li":"http://one", "pr":"(Export) Galaxy Note 4 Sm","th":"thumb1.jpg"},"weight":63}}
    }`

Document 2:
`{
      "_index" : "suggester",
      "_type" : "suggestions",
      "_id" : "22480c43af984689f7254bda8d16f99a",
      "_score" : 1.0,
      "_source":{"suggest":{"input":["Samsung Mobiles Tablets","Mobiles \u0026 Tablets","Galaxy Note 4 Sm","Note 4 Sm","4 Sm","Sm"],"output":21970, "payload":{"br":"Samsung","ca":"Mobiles \u0026 Tablets","cl":"N910S LTE Black (Local warranty)","co":"product","li":"http://two", "pr":"Galaxy Note 4 Sm", "th":"thumb2.jpg"}, "weight":27}}
    }`
</description><key id="46079978">8129</key><summary>Completion Suggest: Omitting results even output is different</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">petard</reporter><labels /><created>2014-10-17T09:23:15Z</created><updated>2014-12-11T20:37:34Z</updated><resolved>2014-12-11T20:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T11:32:16Z" id="59500267">Hi @petard 

So it looks like the duplicate inputs ("Samsung Mobiles Tablets") result in a single suggestion only.  This too will have to wait for #7353. 
</comment><comment author="petard" created="2014-10-17T11:39:01Z" id="59500837">Thanks @clintongormley, so it will also deduplicate if two inputs from two or more documents are same?
</comment><comment author="clintongormley" created="2014-10-17T11:40:49Z" id="59501007">@petard I don't know exactly what plans @areek has, but I've assigned this to him so that he can see what people are asking for.
</comment><comment author="petard" created="2014-10-17T11:42:40Z" id="59501156">Ok will wait for @areek then. It's just strange, while the inputs of the two document are very similar they are not the same, e.g. (Export) being present in one but not the other.
</comment><comment author="areek" created="2014-12-11T20:37:34Z" id="66684431">closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8909
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cat API: Add node name to _cat/recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8128</link><project id="" key="" /><description>Add source_node and target_node fields to the recovery cat API. Also fixed and updated the documentation which was not complete concerning fields names.

Closes #8041
</description><key id="46079025">8128</key><summary>Cat API: Add node name to _cat/recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels /><created>2014-10-17T09:12:28Z</created><updated>2014-10-27T08:58:26Z</updated><resolved>2014-10-27T08:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-17T11:03:23Z" id="59497885">LGTM
</comment><comment author="pickypg" created="2014-10-19T06:06:08Z" id="59640560">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[TESTS Fix wrong assertion in test introduced by #8128</comment></comments></commit></commits></item><item><title>Add `first` and `last` sort modes to nested sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8127</link><project id="" key="" /><description /><key id="46075317">8127</key><summary>Add `first` and `last` sort modes to nested sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-10-17T08:23:51Z</created><updated>2015-11-21T19:54:46Z</updated><resolved>2015-11-21T19:54:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T19:54:46Z" id="158677529">Can't think of a use case for this anymore. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve rolling upgrade doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8126</link><project id="" key="" /><description>fixes #7973 
</description><key id="46074688">8126</key><summary>Improve rolling upgrade doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kamaradclimber</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-17T08:16:26Z</created><updated>2015-06-19T14:31:01Z</updated><resolved>2015-06-19T14:31:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T10:56:49Z" id="59497322">That's great, thanks @kamaradclimber - i'll merge this in once you get the CLA sorted.
</comment><comment author="clintongormley" created="2015-06-19T14:31:00Z" id="113531886">PR no longer valid. Upgrade docs rewritten in dd680669f57156687825a5b2eb053e5b7ac86a7c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk request hangs when one index can be auto created an another cannot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8125</link><project id="" key="" /><description>To reproduce, download latest 1.3.x or 1.4 beta and update config/elasticsearch.yml to include:
action.auto_create_index: +willwork*

Then create a requests file that contains:

```
{ "index" : { "_index" : "willwork", "_type" : "type1", "_id" : "1" } }
{ "field1" : "value1" }
{ "index" : { "_index" : "noway", "_type" : "type1", "_id" : "1" } }
{ "field1" : "value1" }
```

Run the command to bulk insert:

```
curl -s -XPOST localhost:9200/_bulk --data-binary @requests; echo
```

The command hangs and doesn't return. 
</description><key id="46062832">8125</key><summary>Bulk request hangs when one index can be auto created an another cannot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">ppearcy</reporter><labels><label>adoptme</label><label>blocker</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-17T04:48:00Z</created><updated>2014-10-27T14:54:28Z</updated><resolved>2014-10-24T13:04:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T05:28:55Z" id="59468064">thanks for the succinct report @ppearcy - will look into it
</comment><comment author="markharwood" created="2014-10-17T17:23:03Z" id="59546086">Reproduced and I have a fix - just need to write the automated test for our test suite as it obviously failed to be a case we covered.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkProcessorClusterSettingsTests.java</file></files><comments><comment>Bulk indexing: Fix 8125 hanged request when auto create index is off.</comment><comment>If a bulk request contains a mix of indexing requests for an existing index and one that needs to be auto-created but a cluster configuration prevents the auto-create of the new index the ingest process hangs. The exception for the failure to create an index was not caught or reported back properly. Added a Junit test to recreate the issue and the associated fix is in TransportBulkAction.</comment></comments></commit></commits></item><item><title>'Named' thread pools to run searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8124</link><project id="" key="" /><description>Currently all searches uses SEARCH thread pool to run queries. However if  we have programs using ES with different 'interests' and 'SLA's'  -they can block each others queries.

Example:
- 'fred' indexes documents, but does some de normalization and needs to query ES for that purpose. 
- 'barney' queries same ES cluster to serve website traffic.

fred'ss queries get q'd in thread pool due to large amount of queries from barney at times ; fred is okay with 800 ms while barney needs query response by 200 ms. 
</description><key id="46052129">8124</key><summary>'Named' thread pools to run searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirmalc</reporter><labels><label>discuss</label></labels><created>2014-10-17T00:44:21Z</created><updated>2014-10-29T10:23:39Z</updated><resolved>2014-10-29T10:23:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nirmalc" created="2014-10-17T00:58:51Z" id="59453244">This implementation seems to do it , if this feature sounds reasonable/useful - will clean up code + add tests and submit PR 

SHA: fbf5035e02f6c13e627b3d1b9629ba87523d6c2e
</comment><comment author="clintongormley" created="2014-10-29T10:23:38Z" id="60900638">Closing - see discussion in #8173
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding setters or making them public in ActionRequests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8123</link><project id="" key="" /><description>MoreLikeThisRequest, AnalyzeRequest, and DeleteIndexTemplateRequest (and associated builders) all prevented some fields from being set outside their constructor. This makes those fields _publically_ settable, which helps to simplify usage in other languages like Groovy.

Closes #8122
</description><key id="46031054">8123</key><summary>Adding setters or making them public in ActionRequests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-16T20:22:55Z</created><updated>2015-06-22T14:31:18Z</updated><resolved>2015-05-27T09:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-11-05T10:59:13Z" id="61790822">Although helps to simplify things in languages like groovy I am concerned about the impact it has on the Java API. I would assume that the reason these fields are explicitly prevented from being set outside the constructor is that they are required parameters (e.g. you must specify the name of the template you want to delete). By making them optional in the Java API we increase the risk that someone will accidentally not set it causing a headache for the user. Furthermore, since this fields have always been guaranteed to be set I would be wary of unknown side-effects if the request did get made with those fields set to null. The code may not guard against these fields being null, at best this would result in a NPE, at worst the null could be interpreted as _all_ and have serious side-effects.
</comment><comment author="pickypg" created="2015-02-03T04:39:17Z" id="72591662">@colings86 I think that they were overlooked because it was already added to the Validation API. Honestly, it should just fail at the constructor rather than being a surprise later if we _don't_ change it.

Except `MoreLikeThisRequest`, which apparently does not use the Validation API and I suspect that it should.
</comment><comment author="javanna" created="2015-02-03T09:31:08Z" id="72619471">I think all of the requests should ideally do validation using the `validate` method. If `MoreLikeThisRequest` doesn't we should fix it. That said some of them can enforce required arguments by making them required in their constructor. But I am not against adding setters too for them, as long as validate method works properly, for consistency reasons. I tend to be against adding empty constructors though to those requests, especially if using the `@Nullable` annotation for required fields. Does this make sense?
</comment><comment author="s1monw" created="2015-03-20T21:54:36Z" id="84164433">@javanna can you take care of this and work with @pickypg towards closing / merging, thanks!
</comment><comment author="javanna" created="2015-03-21T08:48:18Z" id="84283084">Do you have time to address my comments @pickypg ? Let me know if you have any question.
</comment><comment author="javanna" created="2015-05-27T09:27:24Z" id="105837370">I pushed a fix for this: note that the more like this api is gone in master, thus no need to modify anything there. `AnalyzeRequest` already had text setter after #3023 was resolved. I added the name setter to `DeleteIndexTemplateRequest`. Also made public default constructors for `AnalyzeRequest` and `DeleteIndexTemplateRequest`, given that through their corresponding builder it is already possible to create a request without required parameters, the validate method will throw error though.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequestBuilder.java</file></files><comments><comment>Java api: add name setter to delete index template request and make default constructor public for AnalyzeRequest and DeleteIndexTemplateRequest</comment></comments></commit></commits></item><item><title>Some ActionRequests require constructor args</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8122</link><project id="" key="" /><description>The vast majority of `ActionRequest`s provide both getters and setters for every parameter, which  behave eerily similar to their `*Builder` versions.

However, some implementations do not provide no-arg constructors and, then, they require those arguments to be known at construction time. In each case, these seem in contrast with the majority of other requests.

Of note: `MoreLikeThisRequest` (index), `AnalyzeRequest` (text), and `DeleteIndexTemplateRequest` (name) lack setters and require those parameters in the constructor.
</description><key id="46030320">8122</key><summary>Some ActionRequests require constructor args</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Java API</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2014-10-16T20:15:59Z</created><updated>2015-05-27T09:24:57Z</updated><resolved>2015-05-27T09:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequestBuilder.java</file></files><comments><comment>Java api: add name setter to delete index template request and make default constructor public for AnalyzeRequest and DeleteIndexTemplateRequest</comment></comments></commit></commits></item><item><title>Fixing SearchRequestBuilder aggregations call to facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8121</link><project id="" key="" /><description>This is already fixed in master thanks to the removal of facets.

Closes #8120
</description><key id="46022478">8121</key><summary>Fixing SearchRequestBuilder aggregations call to facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Java API</label><label>bug</label><label>v1.5.0</label></labels><created>2014-10-16T19:01:37Z</created><updated>2015-03-19T09:35:33Z</updated><resolved>2014-10-16T20:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-16T20:27:23Z" id="59424880">LGTM

Since this issue has not been noticed before, I'm wondering if we should just remove it in 2.0.
</comment><comment author="pickypg" created="2014-10-16T20:29:29Z" id="59425188">It does seem like an oddly complex version of the method. I would be pretty stunned if people were submitting their aggregations in bytes to the builder at all.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SearchRequestBuilder passes aggregation bytes as facet bytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8120</link><project id="" key="" /><description>This issue does not exist on master because facets were removed, but it persists on 1.x where they're only deprecated. Chances are that this method is rarely used in this form.

``` java
sourceBuilder().facets(aggregations, aggregationsOffset, aggregationsLength);
```

`facets` should be `aggregations`.
</description><key id="46022356">8120</key><summary>SearchRequestBuilder passes aggregation bytes as facet bytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Java API</label><label>bug</label><label>low hanging fruit</label><label>v1.5.0</label></labels><created>2014-10-16T19:00:29Z</created><updated>2015-06-19T22:14:14Z</updated><resolved>2015-06-19T22:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-06-19T22:14:11Z" id="113659382">Fixed by #8121
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshots are taking more place even if no changes happened</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8119</link><project id="" key="" /><description>I took 2 snapshots for read-only indices with curator and some indices were snapshotted again even though they didn't have any changes.

Look at the first backup (50 oldest indices):

```
51G      s3://backups-es-statistics/indices/statistics-20131004/
27G      s3://backups-es-statistics/indices/statistics-20131005/
24G      s3://backups-es-statistics/indices/statistics-20131006/
39G      s3://backups-es-statistics/indices/statistics-20131007/
25G      s3://backups-es-statistics/indices/statistics-20131008/
30G      s3://backups-es-statistics/indices/statistics-20131009/
37G      s3://backups-es-statistics/indices/statistics-20131010/
28G      s3://backups-es-statistics/indices/statistics-20131011/
27G      s3://backups-es-statistics/indices/statistics-20131012/
28G      s3://backups-es-statistics/indices/statistics-20131013/
32G      s3://backups-es-statistics/indices/statistics-20131014/
41G      s3://backups-es-statistics/indices/statistics-20131015/
42G      s3://backups-es-statistics/indices/statistics-20131016/
33G      s3://backups-es-statistics/indices/statistics-20131017/
29G      s3://backups-es-statistics/indices/statistics-20131018/
29G      s3://backups-es-statistics/indices/statistics-20131019/
30G      s3://backups-es-statistics/indices/statistics-20131020/
32G      s3://backups-es-statistics/indices/statistics-20131021/
33G      s3://backups-es-statistics/indices/statistics-20131022/
29G      s3://backups-es-statistics/indices/statistics-20131023/
36G      s3://backups-es-statistics/indices/statistics-20131024/
32G      s3://backups-es-statistics/indices/statistics-20131025/
32G      s3://backups-es-statistics/indices/statistics-20131026/
34G      s3://backups-es-statistics/indices/statistics-20131027/
31G      s3://backups-es-statistics/indices/statistics-20131028/
40G      s3://backups-es-statistics/indices/statistics-20131029/
29G      s3://backups-es-statistics/indices/statistics-20131030/
35G      s3://backups-es-statistics/indices/statistics-20131031/
7G       s3://backups-es-statistics/indices/statistics-20131101/
6G       s3://backups-es-statistics/indices/statistics-20131102/
7G       s3://backups-es-statistics/indices/statistics-20131103/
7G       s3://backups-es-statistics/indices/statistics-20131104/
7G       s3://backups-es-statistics/indices/statistics-20131105/
7G       s3://backups-es-statistics/indices/statistics-20131106/
7G       s3://backups-es-statistics/indices/statistics-20131107/
7G       s3://backups-es-statistics/indices/statistics-20131108/
7G       s3://backups-es-statistics/indices/statistics-20131109/
7G       s3://backups-es-statistics/indices/statistics-20131110/
7G       s3://backups-es-statistics/indices/statistics-20131111/
7G       s3://backups-es-statistics/indices/statistics-20131112/
```

And the subsequent backup, same indices:

```
57G      s3://backups-es-statistics/indices/statistics-20131004/
30G      s3://backups-es-statistics/indices/statistics-20131005/
27G      s3://backups-es-statistics/indices/statistics-20131006/
44G      s3://backups-es-statistics/indices/statistics-20131007/
28G      s3://backups-es-statistics/indices/statistics-20131008/
33G      s3://backups-es-statistics/indices/statistics-20131009/
41G      s3://backups-es-statistics/indices/statistics-20131010/
31G      s3://backups-es-statistics/indices/statistics-20131011/
30G      s3://backups-es-statistics/indices/statistics-20131012/
31G      s3://backups-es-statistics/indices/statistics-20131013/
35G      s3://backups-es-statistics/indices/statistics-20131014/
46G      s3://backups-es-statistics/indices/statistics-20131015/
47G      s3://backups-es-statistics/indices/statistics-20131016/
37G      s3://backups-es-statistics/indices/statistics-20131017/
33G      s3://backups-es-statistics/indices/statistics-20131018/
33G      s3://backups-es-statistics/indices/statistics-20131019/
34G      s3://backups-es-statistics/indices/statistics-20131020/
36G      s3://backups-es-statistics/indices/statistics-20131021/
37G      s3://backups-es-statistics/indices/statistics-20131022/
32G      s3://backups-es-statistics/indices/statistics-20131023/
40G      s3://backups-es-statistics/indices/statistics-20131024/
36G      s3://backups-es-statistics/indices/statistics-20131025/
36G      s3://backups-es-statistics/indices/statistics-20131026/
38G      s3://backups-es-statistics/indices/statistics-20131027/
34G      s3://backups-es-statistics/indices/statistics-20131028/
45G      s3://backups-es-statistics/indices/statistics-20131029/
33G      s3://backups-es-statistics/indices/statistics-20131030/
39G      s3://backups-es-statistics/indices/statistics-20131031/
7G       s3://backups-es-statistics/indices/statistics-20131101/
6G       s3://backups-es-statistics/indices/statistics-20131102/
7G       s3://backups-es-statistics/indices/statistics-20131103/
7G       s3://backups-es-statistics/indices/statistics-20131104/
7G       s3://backups-es-statistics/indices/statistics-20131105/
7G       s3://backups-es-statistics/indices/statistics-20131106/
7G       s3://backups-es-statistics/indices/statistics-20131107/
7G       s3://backups-es-statistics/indices/statistics-20131108/
7G       s3://backups-es-statistics/indices/statistics-20131109/
7G       s3://backups-es-statistics/indices/statistics-20131110/
7G       s3://backups-es-statistics/indices/statistics-20131111/
7G       s3://backups-es-statistics/indices/statistics-20131112/
```

Segments are here:

```
statistics-20131004 0 p 4.6
statistics-20131004 0 r 4.6
statistics-20131005 0 p 4.4
statistics-20131005 0 p 4.9
statistics-20131005 0 r 4.4
statistics-20131005 0 r 4.9
statistics-20131006 0 p 4.4
statistics-20131006 0 p 4.9
statistics-20131006 0 r 4.4
statistics-20131006 0 r 4.9
statistics-20131007 0 p 4.4
statistics-20131007 0 p 4.9
statistics-20131007 0 r 4.4
statistics-20131007 0 r 4.9
statistics-20131008 0 p 4.4
statistics-20131008 0 p 4.9
statistics-20131008 0 r 4.4
statistics-20131008 0 r 4.9
statistics-20131009 0 p 4.4
statistics-20131009 0 p 4.9
statistics-20131009 0 r 4.4
statistics-20131009 0 r 4.9
statistics-20131010 0 p 4.4
statistics-20131010 0 p 4.9
statistics-20131010 0 r 4.4
statistics-20131010 0 r 4.9
statistics-20131011 0 p 4.4
statistics-20131011 0 p 4.9
statistics-20131011 0 r 4.4
statistics-20131011 0 r 4.9
statistics-20131012 0 p 4.4
statistics-20131012 0 p 4.9
statistics-20131012 0 r 4.4
statistics-20131012 0 r 4.9
statistics-20131013 0 p 4.4
statistics-20131013 0 p 4.9
statistics-20131013 0 r 4.4
statistics-20131013 0 r 4.9
statistics-20131014 0 p 4.4
statistics-20131014 0 p 4.9
statistics-20131014 0 r 4.4
statistics-20131014 0 r 4.9
statistics-20131015 0 p 4.4
statistics-20131015 0 p 4.9
statistics-20131015 0 r 4.4
statistics-20131015 0 r 4.9
statistics-20131016 0 p 4.4
statistics-20131016 0 p 4.9
statistics-20131016 0 r 4.4
statistics-20131016 0 r 4.9
statistics-20131017 0 p 4.4
statistics-20131017 0 p 4.9
statistics-20131017 0 r 4.4
statistics-20131017 0 r 4.9
statistics-20131018 0 p 4.4
statistics-20131018 0 p 4.9
statistics-20131018 0 r 4.4
statistics-20131018 0 r 4.9
statistics-20131019 0 p 4.4
statistics-20131019 0 p 4.9
statistics-20131019 0 r 4.4
statistics-20131019 0 r 4.9
statistics-20131020 0 p 4.4
statistics-20131020 0 p 4.9
statistics-20131020 0 r 4.4
statistics-20131020 0 r 4.9
statistics-20131021 0 p 4.4
statistics-20131021 0 p 4.9
statistics-20131021 0 r 4.4
statistics-20131021 0 r 4.9
statistics-20131022 0 p 4.4
statistics-20131022 0 p 4.9
statistics-20131022 0 r 4.4
statistics-20131022 0 r 4.9
statistics-20131023 0 p 4.4
statistics-20131023 0 p 4.9
statistics-20131023 0 r 4.4
statistics-20131023 0 r 4.9
statistics-20131024 0 p 4.4
statistics-20131024 0 p 4.9
statistics-20131024 0 r 4.4
statistics-20131024 0 r 4.9
statistics-20131025 0 p 4.4
statistics-20131025 0 p 4.9
statistics-20131025 0 r 4.4
statistics-20131025 0 r 4.9
statistics-20131026 0 p 4.4
statistics-20131026 0 p 4.9
statistics-20131026 0 r 4.4
statistics-20131026 0 r 4.9
statistics-20131027 0 p 4.4
statistics-20131027 0 p 4.9
statistics-20131027 0 r 4.4
statistics-20131027 0 r 4.9
statistics-20131028 0 p 4.4
statistics-20131028 0 p 4.9
statistics-20131028 0 r 4.4
statistics-20131028 0 r 4.9
statistics-20131029 0 p 4.4
statistics-20131029 0 p 4.9
statistics-20131029 0 r 4.4
statistics-20131029 0 r 4.9
statistics-20131030 0 p 4.4
statistics-20131030 0 p 4.9
statistics-20131030 0 r 4.4
statistics-20131030 0 r 4.9
statistics-20131031 0 p 4.4
statistics-20131031 0 p 4.9
statistics-20131031 0 r 4.4
statistics-20131031 0 r 4.9
statistics-20131101 0 p 4.4
statistics-20131101 0 p 4.9
statistics-20131101 0 r 4.4
statistics-20131101 0 r 4.9
statistics-20131101 1 p 4.4
statistics-20131101 1 p 4.9
statistics-20131101 1 r 4.4
statistics-20131101 1 r 4.9
statistics-20131101 2 p 4.4
statistics-20131101 2 p 4.9
statistics-20131101 2 r 4.4
statistics-20131101 2 r 4.9
statistics-20131101 3 p 4.4
statistics-20131101 3 p 4.9
statistics-20131101 3 r 4.4
statistics-20131101 3 r 4.9
statistics-20131101 4 p 4.4
statistics-20131101 4 p 4.9
statistics-20131101 4 r 4.4
statistics-20131101 4 r 4.9
statistics-20131102 0 p 4.4
statistics-20131102 0 p 4.9
statistics-20131102 0 r 4.4
statistics-20131102 0 r 4.9
statistics-20131102 1 p 4.4
statistics-20131102 1 p 4.9
statistics-20131102 1 r 4.4
statistics-20131102 1 r 4.9
statistics-20131102 2 p 4.4
statistics-20131102 2 p 4.9
statistics-20131102 2 r 4.4
statistics-20131102 2 r 4.9
statistics-20131102 3 p 4.4
statistics-20131102 3 p 4.9
statistics-20131102 3 r 4.4
statistics-20131102 3 r 4.9
statistics-20131102 4 p 4.4
statistics-20131102 4 p 4.9
statistics-20131102 4 r 4.4
statistics-20131102 4 r 4.9
statistics-20131103 0 p 4.4
statistics-20131103 0 p 4.9
statistics-20131103 0 r 4.4
statistics-20131103 0 r 4.9
statistics-20131103 1 p 4.4
statistics-20131103 1 p 4.9
statistics-20131103 1 r 4.4
statistics-20131103 1 r 4.9
statistics-20131103 2 p 4.4
statistics-20131103 2 p 4.9
statistics-20131103 2 r 4.4
statistics-20131103 2 r 4.9
statistics-20131103 3 p 4.4
statistics-20131103 3 p 4.9
statistics-20131103 3 r 4.4
statistics-20131103 3 r 4.9
statistics-20131103 4 p 4.4
statistics-20131103 4 p 4.9
statistics-20131103 4 r 4.4
statistics-20131103 4 r 4.9
statistics-20131104 0 p 4.4
statistics-20131104 0 p 4.9
statistics-20131104 0 r 4.4
statistics-20131104 0 r 4.9
statistics-20131104 1 p 4.4
statistics-20131104 1 p 4.9
statistics-20131104 1 r 4.4
statistics-20131104 1 r 4.9
statistics-20131104 2 p 4.4
statistics-20131104 2 p 4.9
statistics-20131104 2 r 4.4
statistics-20131104 2 r 4.9
statistics-20131104 3 p 4.4
statistics-20131104 3 p 4.9
statistics-20131104 3 r 4.4
statistics-20131104 3 r 4.9
statistics-20131104 4 p 4.4
statistics-20131104 4 p 4.9
statistics-20131104 4 r 4.4
statistics-20131104 4 r 4.9
statistics-20131105 0 p 4.4
statistics-20131105 0 p 4.9
statistics-20131105 0 r 4.4
statistics-20131105 0 r 4.9
statistics-20131105 1 p 4.4
statistics-20131105 1 p 4.9
statistics-20131105 1 r 4.4
statistics-20131105 1 r 4.9
statistics-20131105 2 p 4.4
statistics-20131105 2 p 4.9
statistics-20131105 2 r 4.4
statistics-20131105 2 r 4.9
statistics-20131105 3 p 4.4
statistics-20131105 3 p 4.9
statistics-20131105 3 r 4.4
statistics-20131105 3 r 4.9
statistics-20131105 4 p 4.4
statistics-20131105 4 p 4.9
statistics-20131105 4 r 4.4
statistics-20131105 4 r 4.9
statistics-20131106 0 p 4.4
statistics-20131106 0 p 4.9
statistics-20131106 0 r 4.4
statistics-20131106 0 r 4.9
statistics-20131106 1 p 4.4
statistics-20131106 1 p 4.9
statistics-20131106 1 r 4.4
statistics-20131106 1 r 4.9
statistics-20131106 2 p 4.4
statistics-20131106 2 p 4.9
statistics-20131106 2 r 4.4
statistics-20131106 2 r 4.9
statistics-20131106 3 p 4.4
statistics-20131106 3 p 4.9
statistics-20131106 3 r 4.4
statistics-20131106 3 r 4.9
statistics-20131106 4 p 4.4
statistics-20131106 4 p 4.9
statistics-20131106 4 r 4.4
statistics-20131106 4 r 4.9
statistics-20131107 0 p 4.4
statistics-20131107 0 p 4.9
statistics-20131107 0 r 4.4
statistics-20131107 0 r 4.9
statistics-20131107 1 p 4.4
statistics-20131107 1 p 4.9
statistics-20131107 1 r 4.4
statistics-20131107 1 r 4.9
statistics-20131107 2 p 4.4
statistics-20131107 2 p 4.9
statistics-20131107 2 r 4.4
statistics-20131107 2 r 4.9
statistics-20131107 3 p 4.4
statistics-20131107 3 p 4.9
statistics-20131107 3 r 4.4
statistics-20131107 3 r 4.9
statistics-20131107 4 p 4.4
statistics-20131107 4 p 4.9
statistics-20131107 4 r 4.4
statistics-20131107 4 r 4.9
statistics-20131108 0 p 4.4
statistics-20131108 0 p 4.9
statistics-20131108 0 r 4.4
statistics-20131108 0 r 4.9
statistics-20131108 1 p 4.4
statistics-20131108 1 p 4.9
statistics-20131108 1 r 4.4
statistics-20131108 1 r 4.9
statistics-20131108 2 p 4.4
statistics-20131108 2 p 4.9
statistics-20131108 2 r 4.4
statistics-20131108 2 r 4.9
statistics-20131108 3 p 4.4
statistics-20131108 3 p 4.9
statistics-20131108 3 r 4.4
statistics-20131108 3 r 4.9
statistics-20131108 4 p 4.4
statistics-20131108 4 p 4.9
statistics-20131108 4 r 4.4
statistics-20131108 4 r 4.9
statistics-20131109 0 p 4.4
statistics-20131109 0 p 4.9
statistics-20131109 0 r 4.4
statistics-20131109 0 r 4.9
statistics-20131109 1 p 4.4
statistics-20131109 1 p 4.9
statistics-20131109 1 r 4.4
statistics-20131109 1 r 4.9
statistics-20131109 2 p 4.4
statistics-20131109 2 p 4.9
statistics-20131109 2 r 4.4
statistics-20131109 2 r 4.9
statistics-20131109 3 p 4.4
statistics-20131109 3 p 4.9
statistics-20131109 3 r 4.4
statistics-20131109 3 r 4.9
statistics-20131109 4 p 4.4
statistics-20131109 4 p 4.9
statistics-20131109 4 r 4.4
statistics-20131109 4 r 4.9
statistics-20131110 0 p 4.4
statistics-20131110 0 p 4.9
statistics-20131110 0 r 4.4
statistics-20131110 0 r 4.9
statistics-20131110 1 p 4.4
statistics-20131110 1 p 4.9
statistics-20131110 1 r 4.4
statistics-20131110 1 r 4.9
statistics-20131110 2 p 4.4
statistics-20131110 2 p 4.9
statistics-20131110 2 r 4.4
statistics-20131110 2 r 4.9
statistics-20131110 3 p 4.4
statistics-20131110 3 p 4.9
statistics-20131110 3 r 4.4
statistics-20131110 3 r 4.9
statistics-20131110 4 p 4.4
statistics-20131110 4 p 4.9
statistics-20131110 4 r 4.4
statistics-20131110 4 r 4.9
statistics-20131111 0 p 4.4
statistics-20131111 0 p 4.9
statistics-20131111 0 r 4.4
statistics-20131111 0 r 4.9
statistics-20131111 1 p 4.4
statistics-20131111 1 p 4.9
statistics-20131111 1 r 4.4
statistics-20131111 1 r 4.9
statistics-20131111 2 p 4.4
statistics-20131111 2 p 4.9
statistics-20131111 2 r 4.4
statistics-20131111 2 r 4.9
statistics-20131111 3 p 4.4
statistics-20131111 3 p 4.9
statistics-20131111 3 r 4.4
statistics-20131111 3 r 4.9
statistics-20131111 4 p 4.4
statistics-20131111 4 p 4.9
statistics-20131111 4 r 4.4
statistics-20131111 4 r 4.9
statistics-20131112 0 p 4.4
statistics-20131112 0 p 4.9
statistics-20131112 0 r 4.4
statistics-20131112 0 r 4.9
```

Those indices should be roughly the same size in snapshot.

I also took dir diff from subsequent backups:

``` diff
--- before.3.txt    2014-10-16 21:39:05.559338129 +0400
+++ after.3.txt 2014-10-16 21:56:46.272597922 +0400
@@ -152,6 +152,26 @@
 2014-10-16 17:25       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3m.part0
 2014-10-16 17:25       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3m.part1
 2014-10-16 17:25        21M  s3://backups-es-statistics/indices/statistics-20140108/3/__3m.part2
+2014-10-16 17:48       436   s3://backups-es-statistics/indices/statistics-20140108/3/__3n
+2014-10-16 17:48       179   s3://backups-es-statistics/indices/statistics-20140108/3/__3o
+2014-10-16 17:48       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part0
+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part1
+2014-10-16 17:48       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part2
+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part3
+2014-10-16 17:48        18M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part4
+2014-10-16 17:48       459k  s3://backups-es-statistics/indices/statistics-20140108/3/__3q
+2014-10-16 17:48         7M  s3://backups-es-statistics/indices/statistics-20140108/3/__3r
+2014-10-16 17:48        34   s3://backups-es-statistics/indices/statistics-20140108/3/__3s
+2014-10-16 17:48         2k  s3://backups-es-statistics/indices/statistics-20140108/3/__3t
+2014-10-16 17:48        16M  s3://backups-es-statistics/indices/statistics-20140108/3/__3u
+2014-10-16 17:49        81M  s3://backups-es-statistics/indices/statistics-20140108/3/__3v
+2014-10-16 17:48        57   s3://backups-es-statistics/indices/statistics-20140108/3/__3w
+2014-10-16 17:48         2M  s3://backups-es-statistics/indices/statistics-20140108/3/__3x
+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3y.part0
+2014-10-16 17:49        87M  s3://backups-es-statistics/indices/statistics-20140108/3/__3y.part1
+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3z.part0
+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3z.part1
+2014-10-16 17:49        21M  s3://backups-es-statistics/indices/statistics-20140108/3/__3z.part2
 2014-09-30 19:39       959k  s3://backups-es-statistics/indices/statistics-20140108/3/__4
 2014-09-30 19:39        53M  s3://backups-es-statistics/indices/statistics-20140108/3/__5
 2014-09-30 19:39       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__6.part0
@@ -205,3 +225,4 @@
 2014-10-15 08:46         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-14
 2014-10-16 08:41         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-15
 2014-10-16 17:25         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-16
+2014-10-16 17:49         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-16-again
```

Cluster consists of 5 nodes on 1.3.2.

cc @imotov 
</description><key id="46016276">8119</key><summary>Snapshots are taking more place even if no changes happened</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>:Snapshot/Restore</label></labels><created>2014-10-16T18:00:58Z</created><updated>2014-11-18T02:29:07Z</updated><resolved>2014-11-18T02:29:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-10-16T18:10:51Z" id="59404705">Is it possible that some primary shards for this index switched between snapshots?
</comment><comment author="bobrik" created="2014-10-16T18:16:24Z" id="59405572">Cluster is healthy, so I think no. Look at `statistics-20131004`, it is 57gb instead of 7gb. This is far from normal.

Do I understand this correctly? If shard is snapshotted, then replica becomes primary, then snapshot is taken again, then procedure is repeated 10 times, then snapshot is going to be 10 times bigger? What if replica is rebuild from primary (previous replica is totally lost).

Let me know if I could add more info to help you with this issue.
</comment><comment author="imotov" created="2014-10-16T18:23:12Z" id="59406578">We copy only files that changed since the last snapshot. If you have a replica that was never synched with primary and primary went down, it's possible to have another copy but it will stop there. So, it can explain 2x difference not 10x difference in size. Could you send us these two files:

```
s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-16
s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-16-again
```

Do you know which version of elasticsearch you had when the index statistics-20140108 was created?
</comment><comment author="bobrik" created="2014-10-16T18:46:50Z" id="59410204">2x is better than 10x, but can we do better? Is there an api to sync replicas with primaries on byte level so even if replica becomes primary snapshot is noop?

https://gist.github.com/bobrik/c7ab1e0df88f0585f274 here are the files you requested. Elasticsearch version was from 0.90.x line, but those specific indices were probably restored from snapshot on 1.3.2.

Actually it looks like all problematic indices were restored from snapshot with renaming and good indices weren't restored at all (they were here since 0.90.x). That should help.
</comment><comment author="imotov" created="2014-10-16T18:48:41Z" id="59410481">Which version of elasticsearch did you use to restore these indices from snapshot?
</comment><comment author="bobrik" created="2014-10-16T18:51:51Z" id="59410961">1.3.2 was used for snapshot and restore. Cluster is 1.3.4 since the day of release, in all comments above you should assume 1.3.4 instead of 1.3.2.

Current snapshots are made on 1.3.4.
</comment><comment author="imotov" created="2014-10-16T19:01:53Z" id="59412529">So, index was created with 0.90, upgraded to 1.3.2, then you created snapshot with 1.3.2, restored this index while renaming in 1.3.2, now you are creating snapshots with 1.3.4 and they are duplicated. Is this correct description? Could you also send us files `snapshot-statistics-2014-10-16` and `snapshot-statistics-2014-10-16-again` from the root directory of your repository?
</comment><comment author="bobrik" created="2014-10-16T19:06:22Z" id="59413309">Yes, this looks correct, but index was upgraded from 0.90 to 1.3.2 with many intermediate versions (1.0, 1.1, 1.2 hold those indices too).

Here are the files from the root of repository: https://gist.github.com/bobrik/d1deb9239c59db998f24
</comment><comment author="imotov" created="2014-10-16T19:15:06Z" id="59414590">I see. One last piece of information (hopefully). Could you also post these two files:

```
s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-16
s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-16-again
```
</comment><comment author="bobrik" created="2014-10-16T19:40:39Z" id="59418196">They are the same:

``` json
{"statistics-20140108":{"version":8,"state":"open","settings":{"index.number_of_replicas":"1","index.version.created":"900999","index.number_of_shards":"5","index.uuid":"7OLwrzjOSFemAoXS1XB2qg","index.codec.bloom.load":"false"},"mappings":[{"markers":{"_all":{"enabled":false},"properties":{"@message":{"type":"string"},"@timestamp":{"type":"date","format":"dateOptionalTime"}}}},{"precise":{"_all":{"enabled":false},"_routing":{"required":true,"path":"@key"},"properties":{"@key":{"type":"string","index":"not_analyzed"},"@precise":{"type":"double"},"@timestamp":{"type":"date","format":"dateOptionalTime"}}}},{"events":{"_all":{"enabled":false},"_routing":{"required":true,"path":"@key"},"properties":{"---":{"type":"long"},"@key":{"type":"string","index":"not_analyzed"},"@timestamp":{"type":"date","format":"dateOptionalTime"},"@value":{"type":"long"},"ad":{"type":"string"},"age":{"type":"long"},"app":{"type":"string","index":"not_analyzed"},"cit":{"type":"string","index":"not_analyzed"},"cnt":{"type":"string","index":"not_analyzed"},"con":{"type":"string","index":"not_analyzed"},"cor":{"type":"long"},"cvn":{"type":"string","index":"not_analyzed"},"lng":{"type":"string","index":"not_analyzed"},"mob":{"type":"long"},"mtd":{"type":"string","index":"not_analyzed"},"nic":{"type":"long"},"nov":{"type":"long"},"plc":{"type":"string","index":"not_analyzed"},"plt":{"type":"string","index":"not_analyzed"},"pwr":{"type":"string","index":"not_analyzed"},"ref":{"type":"string","index":"not_analyzed"},"sbs":{"type":"long"},"sex":{"type":"long"},"spc":{"type":"long"},"spl":{"type":"string","index":"not_analyzed"},"tag":{"type":"string","index":"not_analyzed"},"tgt":{"type":"string","index":"not_analyzed"},"trs":{"type":"string","index":"not_analyzed"},"val":{"type":"string","index":"not_analyzed"},"wsh":{"type":"string"}}}}],"aliases":{}}}
```

All of them:

```
$ s3cmd ls --list-md5 s3://backups-es-statistics/indices/statistics-20140108/
                       DIR                                     s3://backups-es-statistics/indices/statistics-20140108/0/
                       DIR                                     s3://backups-es-statistics/indices/statistics-20140108/1/
                       DIR                                     s3://backups-es-statistics/indices/statistics-20140108/2/
                       DIR                                     s3://backups-es-statistics/indices/statistics-20140108/3/
                       DIR                                     s3://backups-es-statistics/indices/statistics-20140108/4/
2014-09-30 16:59      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-09-29
2014-10-02 08:30      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-01
2014-10-09 08:29      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-08
2014-10-10 08:34      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-09
2014-10-11 08:34      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-10
2014-10-14 13:43      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-13
2014-10-15 08:36      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-14
2014-10-16 08:31      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-15
2014-10-16 17:15      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-16
2014-10-16 17:39      1849   f6e41305195be8a005a489e36c022dd4  s3://backups-es-statistics/indices/statistics-20140108/snapshot-statistics-2014-10-16-again
```
</comment><comment author="imotov" created="2014-11-06T06:24:36Z" id="61932517">@bobrik I was able to reproduce the issue. It turned out that cleanup process in v1.3.0+ at the end of restore mistakenly deletes information about legacy checksums (checksums for segments created with old version of elasticsearch). As a result, consecutive snapshots don't store the checksum in snapshot metadata and have to fallback to creating copies of these old segments again and again. 

To reproduce this issue:
- create an index using elasticsearch v0.90.10
- upgrade cluster to 1.0.1 and create a snapshot
- restore the index from the created snapshot into cluster v1.3.4
- create a several snapshots of the restored index in the v1.3.4 cluster
- observe that each snapshot creates a new copy of all old segment files of the index
</comment><comment author="bobrik" created="2014-11-06T07:32:05Z" id="61937222">Great! Any thoughts about release where the fix will land?

What would happen with fix? Just final snapshot with checksums or something else?
</comment><comment author="imotov" created="2014-11-06T19:28:36Z" id="62036415">It's going to land in 1.3.6 and 1.4.1. The fix is not going to restore checksums for old segments restored with elasticsearch v.1.3.0-1.3.5 though. You will need to restore indices with such segments again in v1.3.6+ or upgrade them to the new version using [upgrade api](https://github.com/elasticsearch/elasticsearch/issues/7884).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file></files><comments><comment>Snapshot/Restore: keep the last legacy checksums file at the end of restore</comment></comments></commit></commits></item><item><title>Fixing copy/paste mistake in SearchRequest.extraSource's exception message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8118</link><project id="" key="" /><description>Uses the `extraSource` parameter for debug instead of the `source` field.

Closes #8117
</description><key id="46015368">8118</key><summary>Fixing copy/paste mistake in SearchRequest.extraSource's exception message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Exceptions</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-16T17:52:10Z</created><updated>2015-06-07T18:24:10Z</updated><resolved>2014-10-16T20:40:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-16T20:28:41Z" id="59425065">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SearchRequest - Map extraSource exception uses source field accidently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8117</link><project id="" key="" /><description>This is incredibly minor, and it really only helps with client debug that is unlikely to be needed. The method with the signature:

``` java
public SearchRequest extraSource(Map extraSource)
```

Includes the re-thrown exception:

``` java
throw new ElasticsearchGenerationException("Failed to generate [" + source + "]", e);
```

where `source` is the internal field due to copy/paste.
</description><key id="46015192">8117</key><summary>SearchRequest - Map extraSource exception uses source field accidently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>bug</label><label>low hanging fruit</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-16T17:50:26Z</created><updated>2014-10-16T20:40:10Z</updated><resolved>2014-10-16T20:40:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolate reference - a typo and a misused word</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8116</link><project id="" key="" /><description /><key id="46010448">8116</key><summary>Percolate reference - a typo and a misused word</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">golubev</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-16T17:02:12Z</created><updated>2014-10-17T15:16:36Z</updated><resolved>2014-10-17T13:27:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T13:26:51Z" id="59511986">Thanks @golubev - merged
</comment><comment author="golubev" created="2014-10-17T15:16:36Z" id="59528092">Thanks, @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Percolate reference - a typo and a misused word</comment></comments></commit></commits></item><item><title>Percolate `_score` reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8115</link><project id="" key="" /><description>Added missing `_score` word, made the sentence less ambiguous.
</description><key id="46008807">8115</key><summary>Percolate `_score` reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">golubev</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-16T16:46:45Z</created><updated>2014-10-17T15:17:30Z</updated><resolved>2014-10-17T13:25:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-17T13:25:22Z" id="59511775">Thanks @golubev - merged!
</comment><comment author="golubev" created="2014-10-17T15:17:30Z" id="59528225">Thanks, @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Percolate `_score` reference</comment></comments></commit></commits></item><item><title>Typo: s/by/be/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8114</link><project id="" key="" /><description /><key id="46008801">8114</key><summary>Typo: s/by/be/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndrewO</reporter><labels /><created>2014-10-16T16:46:43Z</created><updated>2014-10-16T18:52:38Z</updated><resolved>2014-10-16T18:52:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T18:52:18Z" id="59411022">thanks @AndrewO - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Typo: s/by/be/</comment></comments></commit></commits></item><item><title>Buckets can now be serialized outside of an Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8113</link><project id="" key="" /><description>This change means that buckets can now be serialised to JSON and serialized and deserialized to the transport API outside of the aggregation that contains them.  This is a required change for #8110 (Reducers framework) but should make sense on it's own since object should really take care of their own serialization rather than relying on their parent object.
</description><key id="45994525">8113</key><summary>Buckets can now be serialized outside of an Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-16T14:48:51Z</created><updated>2015-06-07T11:58:48Z</updated><resolved>2014-10-17T15:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-16T21:52:40Z" id="59436793">This looks good to me overall but I have some questions:
- do we actually need to register streams for bucket in #8110 (I see ithe BucketStreams.stream method is not used in this PR)?
- if yes would it make sense to make the aggregation streams and bucket streams (with different method names for aggs and buckets obviously)
</comment><comment author="jpountz" created="2014-10-17T14:59:13Z" id="59525425">Just discussed it with @colings86 and his current impl actually makes more sense. :) So LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose concurrency_level setting on all caches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8112</link><project id="" key="" /><description>The concurrency level allows to configure the cache internal segments
used to cache data. This can have direct impact on evicition rates since
memory bound caches are equally divided into segments which can cause
early evictions if cache entries are not well balanced.

Relates to #7836
</description><key id="45993785">8112</key><summary>Expose concurrency_level setting on all caches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-16T14:43:02Z</created><updated>2015-06-07T10:26:06Z</updated><resolved>2014-10-24T09:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-21T12:56:33Z" id="59922718">LGTM
</comment><comment author="craigwi" created="2014-10-23T17:53:33Z" id="60279555">Can this be back ported to 1.3.x?  Or in some release in the next week or two?  At least the field data cache part?   Thanks.
</comment><comment author="s1monw" created="2014-10-24T09:52:23Z" id="60366790">&gt; Can this be back ported to 1.3.x? Or in some release in the next week or two? At least the field data cache part? Thanks.

I am afraid 1.3 is bugfix only so this won't make it into a  1.3.x release
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8111</link><project id="" key="" /><description /><key id="45987242">8111</key><summary>fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels /><created>2014-10-16T13:45:53Z</created><updated>2014-10-16T18:49:35Z</updated><resolved>2014-10-16T18:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T18:49:18Z" id="59410576">thanks @peschlowp - merged.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: fix typo</comment></comments></commit></commits></item><item><title>Reducers - Post processing of aggregation results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8110</link><project id="" key="" /><description># Overview

This feature would introduce a new 'reducers' module, allowing for the processing and transformation of aggregation results. The current aggregation module is very powerful and can compute varied and complex analytics but is limited when calculating analytics which depend on numerous independently calculated aggregated metrics. Currently, the only way to calculate these types of complex metrics is to write client side logic.  The reducer module will add a new phase in the search request where the coordinating node can pass the reduced aggregations for further processing.
# Key Concepts &amp; Terminology
- _Reducer_ - A reducer is the computation unit which receives a `bucket aggregation` and uses it's set of buckets to perform calculations and generate a new `aggregation` which represents either a single answer (a new `metric aggregation`) or a new set of buckets (a new `bucket aggregation`). Reducers run after aggregations and act on the result tree returned by aggregations or the result tree of previous reducers. There are 2 types of `reducer`:
  - _Bucket reducer_ - Take all specified buckets and divide them into groups (`selections`). A `bucket reducer` may create new buckets to add to the `selections` but is not able to modify existing buckets.
  - _Metric reducer_ - Performs calculations on the input aggregation and outputs a single result (i.e. cannot create buckets)
## Reducer Structure

The following snippet captures the basic structure of aggregations:

``` javascript
"reducers" : [
    {
        "&lt;reducer_name&gt;" : {
            "&lt;reducer_type&gt;" : {
                &lt;reducer_body&gt;
            },
            "reducers" : [ 
                {
                    &lt;sub_reducer_1&gt;
                },
                {
                    &lt;sub_reducer_2&gt;
                },
                ...
           ] 
        }
    },
    {
        "&lt;reducer_name_2&gt;" : { ... }
    },    ...
]
```

Some rules for reducers requests:
- reducers and aggregations are separate in the request structure
- reducers are specified in order because they can process the output of other reducers
- reducers can be nested, in which case they only see the data for the bucket they're in, although they can refer to any value in the tree with `_root.path.to.value`
# Use Cases

Some possible implementations of reducers are detailed below:
## Bucket Reducers
- Sliding window - Can be used with the results of a histogram aggregation to produce selections representing a sliding time window of buckets. The reducer would create a selection for each bucket in the histogram aggregation which contains the bucket and the N subsequent buckets (where N is a configurable window size). 
## Metric Reducers
- Avg/Min/Max/Sum/Stats - Same as their aggregation equivalent except they calculate the result for a particular field in their input aggregation's buckets.
- Delta - Calculates the range of the values of a field between the input buckets
- Gradient - Calculates the range of values of two fields between the input buckets and returns the result of the first divided by the second (e.g. change in price divided by change in time)
## Example

As an example, imagine that you have sales data in an index and you would like to calculate the derivative of the price field over time. This is useful for determining trends in the data, such as whether sales are increasing linearly over time.

You might start with the following aggregation:

``` javascript
{
  "aggs": {
    "months": {
      "date_histogram": {
        "field": "date",
        "interval": "month"
      },
      "aggs": {
        "avg_price": {
          "avg": {
            "field": "price"
          }
        }
      }
    }
  }
}

```

The results of these aggregations might look something like the following:

``` javascript
"aggregations": {
  "months": {
     "buckets": [
        {
           "key_as_string": "2014/02/01 00:00:00",
           "key": 1391212800000,
           "doc_count": 6,
           "avg_price": {
              "value": 11
           }
        },
        {
           "key_as_string": "2014/03/01 00:00:00",
           "key": 1391212800000,
           "doc_count": 4,
           "avg_price": {
              "value": 19
           }
        },
        {
           "key_as_string": "2014/04/01 00:00:00",
           "key": 1391212800000,
           "doc_count": 8,
           "avg_price": {
              "value": 31
           }
        }
     ]
  }
}
```

The sliding window reducer could then be used to request time windows of two months. A nested delta reducer inside the sliding window reducer could be used to calculate the rate of change of price for each two month selection.

``` javascript
"reducers": [
  {
    "two_months": {
      "sliding_window": {
        "buckets": "months",
        "window": 2
      },
      "reducers": [
        {
          "price_derivative": {
            "gradient": {
              "field": "avg_price.value"
            }
          }
        }
      ]
    }
  }
]

```

Which would produce the following response (derivative value is shown in price per month):

``` javascript
{
  "reducers": {
    "two_months": {
      "selections": [
        {
          "buckets": [
            {
              "key_as_string": "2014/02/01 00:00:00",
              "key": 1391212800000,
              "doc_count": 6,
              "avg_price": {
                "value": 11
              }
            },
            {
              "key_as_string": "2014/03/01 00:00:00",
              "key": 1391212800000,
              "doc_count": 4,
              "avg_price": {
                "value": 19
              }
            }
          ],
          "price_derivative": {
            "value": 8
          }
        },
        {
          "buckets": [
            {
              "key_as_string": "2014/03/01 00:00:00",
              "key": 1391212800000,
               "doc_count": 4,
               "avg_price": {
                "value": 19
               }
            },
            {
              "key_as_string": "2014/04/01 00:00:00",
              "key": 1391212800000,
              "doc_count": 8,
              "avg_price": {
                "value": 31
              }
            }
          ],
          "price_derivative": {
            "value": 12
          }
        }
      ]
    }
  }
}
```
</description><key id="45986185">8110</key><summary>Reducers - Post processing of aggregation results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>feature</label><label>v2.0.0-beta1</label></labels><created>2014-10-16T13:36:39Z</created><updated>2016-10-31T14:29:49Z</updated><resolved>2015-01-14T09:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-16T13:38:12Z" id="59361790">+1
</comment><comment author="roytmana" created="2014-10-17T00:54:35Z" id="59452965">Hi @colings86 

Will it allow for expressions against aggregated values? Say I need to calculate a metric: 
number of docs[where field1=value1] divided by number of docs[where field2=value2]

would transformation merging (adding new and replacing existing) buckets from one agg into another be possible. My use cases for it are:
1. Need to have terms agg where several terms MUST be present even if they would be cut off based on agg size/ordering. I do it by aggregating twice once normally and with filter and adding filtered results (at which point I can chose to add to top/bottom or sort properly handling dups to )
2. Need to sub-aggregate only choosen parent agg buckets not all of them. Achieve it by doing two aggs one for all terms without sub-agg and one for selected terms with sub aggs and merging second into the first

Thanks
alex
</comment><comment author="clintongormley" created="2014-10-17T06:36:39Z" id="59472004">Also see #4983 for other use cases.
</comment><comment author="martijnvg" created="2014-10-17T11:39:14Z" id="59500857">+1 :)
</comment><comment author="colings86" created="2014-10-17T11:46:15Z" id="59501475">@roytmana currently the implementations of the reducers are still being discussed but the design does allow for creating new buckets (which could be a merge of existing buckets) but not modifying existing buckets
</comment><comment author="abhijitiitr" created="2014-10-17T17:27:35Z" id="59546723">+1
</comment><comment author="roytmana" created="2014-10-17T17:35:04Z" id="59547753">Thank you @colings86. Is replacement of an existing bucket considered to be modification? I would not want to modify an existing bucket but rather replace it all together

I think **combining two aggregation results is a hugely useful use case** (I outline just couple of cases in my previous post which I and other who build generic interactive solutions based on ES would be interested in)
</comment><comment author="colings86" created="2014-11-17T11:17:50Z" id="63290674">I am trying to implement the Union Reducer and have hit a dilemma which I want some opinions on. Basically reducers need to be able to access and use multiple aggregations from anywhere in the aggregation tree. Thinking for the moment only about top-level reducers, I think it would be good if the logic to extract the aggregations was done in the Reducer class so that the implementations don't have to worry about doing it. That would mean the doReduce method would have a list of aggregations passed to it rather than a single aggregation.

The problem is that this makes reducers which only care about 1 aggregation a bit clunky since they each have to check that the list only contains one aggregation and then get it. its not a lot of code but will get repeated in a lot of places. I think the options are:
1) Deal with the list in each implementation and just leave the repetition
2) Create a SingleAggregationBucketReducer and a SingleAggregationMetricReducer but now have 4 types of reducer instead of two
3) suggestion from @polyfractal: Have the implementations receive a list of aggregations but have a helper method to check there is only 1 aggregation in the list and return it (throwing an exception if there is more than 1 aggregation) which can be used by the reducers which only want a single aggregation as input

Interested to hear peoples thought on this
</comment><comment author="brwe" created="2014-11-17T11:30:38Z" id="63292039">I am actually not sure if we need to care for reducers that only take one aggregation. Instead we might also just implement all the "single" reducers so that they can also operate on an arbitrary number of aggregations and then return the result for any of the curves. For example, a reducer running on  the following aggregation could return 1 or several results depending on if there are one or more terms in the terms aggregation:

```
{
  "aggs": {
    "terms": {
      "terms": {
        "field": "label",
        "size": 10
      },
      "aggs": {
        "histo": {
          "histogram": {
            "field": "number",
            "interval": 1
          }
        }
      }
    }
  }
}
```
</comment><comment author="Grauen" created="2014-11-18T12:06:02Z" id="63460475">+1
</comment><comment author="mewwts" created="2014-11-24T11:26:02Z" id="64180540">+1. Would love the ability to do a simple metric aggregation on bucket fields. For example, give me the average "doc_count" etc.
</comment><comment author="NicolasTr" created="2014-11-25T14:29:56Z" id="64407016">:+1: 
</comment><comment author="mrtheb" created="2014-11-25T16:59:21Z" id="64433109">:+1: 

My use case is the same as #6419 
</comment><comment author="rbnacharya" created="2014-11-28T05:21:38Z" id="64856038">:+1: 
</comment><comment author="tigra" created="2014-12-28T01:04:04Z" id="68195134">It would be great to be able to "search" within aggregation bucket names in some way, e.g. run a term aggregation over some query, and filter bucket names matching to a given string (e.g. by query string query rules, with given fuzziness). 

E.g.

"reducers": {
   "matchingBucketz": {
       "query_string": {
           "query": "cell phones",
           "fuzziness": 1
       }
   }
}
</comment><comment author="zhoul-HS" created="2015-01-08T13:32:26Z" id="69178981">yes. yes yes yes.
</comment><comment author="colings86" created="2015-01-14T09:46:05Z" id="69891827">Having started to implement bucket reducers using the style of API above we have found the API to be overly complex and very verbose to perform even straight forward tasks (such as a simple derivative of a metric). A lot of this is to do with a user having to carefully build their aggregations and then have to define multiple reducers and link them to the aggregations. We found that the only way to build reducer requests with any confidence is to first build the aggregations and then inspect the response to work out what the reducers request should look like. This is not what we want. We would like an API where reducers can be defined at the same time as the aggregations with a structure which is inuitive from the aggregation request structure (i.e. you shouldn't need to get the aggregations response to be able to write your reducers request)

So, we have rethought the API, going back to a more native design with the existing aggregations framework so that reducers look and feel like just another aggregation. 

Now there is a new type of aggregation which takes the output of an aggregation tree and computes a new aggregation tree based on these results. The original sub-aggregation tree is destroyed in the process. This can be used to create functionality such as derivatives and moving average where the original data is not needed in the final output. With this new aggregation functionality we can have even more aggregation implementations. The first implementation of this will be the derivative (https://github.com/elasticsearch/elasticsearch/issues/9293).
</comment><comment author="jayhilden" created="2015-01-22T06:35:18Z" id="70977745">:+1: 
</comment><comment author="jasalmeron" created="2015-01-22T16:26:57Z" id="71048635">So there is no option to directly get the division between two aggregations? I have this search query, and I wonder if it's possible to obtain the division between the balanceIncrement after the nested aggregations are done? ideally i want to calculate the payout:

sum balanceIncrement where 'type' = "prizes" / sum  balanceIncrement where 'type' in ("play","ext) grouped by room and then grouped by day. What I have now is that summation is done but I don't find the way I can then divide between them and then get the payout. This is my query:

/fb/balance_trasnfer/_search?pretty

``` json
{   
    "query" : {
        "filtered": {
            "filter": {
               "range" : {
                    "transferDate" : {
                        "from" : "${BAL_TRANSFER_DATE_INIT}",
                        "to" : "${BAL_TRANSFER_DATE_PLUS_7_DAYS}"
                    }
                }
            }
        }
    },
    "size": 0,
    "aggs" : {
        "transfers_over_day" : {
            "date_histogram" : {
                "field" : "transferDate",
                "interval" : "day"
            },
            "aggs": {
                "group_by_room": {
                    "terms": {
                        "field": "roomName"
                    },                
                    "aggs" : {
                        "wins-and-bets" : {
                             "filters" : {
                                "filters" : {
                                  "win" :   { "term" : { "type" : "prize"}},
                                  "bet" : { "term" : { "type" : ["play","ext"]}}
                                }
                              },
                             "aggs" : {
                                 "sum_balance": {
                                   "sum" : {"field":"balanceIncrement"}
                                 }
                            }                    
                        }               
                    }
                }
            }
        }        
    }
}'
```
</comment><comment author="tcucchietti" created="2015-01-26T12:27:31Z" id="71453450">:+1: 
</comment><comment author="d-chabrovsky" created="2015-01-28T15:49:43Z" id="71857279">+1

```
{
    "aggregations": {
        "currentMessageID": {
            "buckets": [
                {
                    "key": "5fbdfcb6-e6e9-446f-be59-e9527c006656",
                    "doc_count": 6,
                    "status": {
                        "buckets": [ {
                            "key": "FAILURE",
                            "doc_count": 6
                        } ]
                    },
                    "operationDateTm": { "value": 1422343776347 }
                },
                {
                    "key": "25a561c3-d45f-4f17-9ea9-e8b7a7cc14e8",
                    "doc_count": 6,
                    "status": {
                        "buckets": [ {
                            "key": "SUCCESS",
                            "doc_count": 1
                        } ]
                    },
                    "operationDateTm": { "value": 1422343964242 }
                }
            ]
        }
    }
}
```

Would be great to be able to filter out top buckets having the **"SUCCESS"** in the sub-aggregation's **success** field.
</comment><comment author="neilvarnas" created="2015-02-20T08:14:22Z" id="75203420">Why has this issue been closed ?
</comment><comment author="colings86" created="2015-02-20T08:34:53Z" id="75205209">As described in the comment above (https://github.com/elasticsearch/elasticsearch/issues/8110#issuecomment-69891827) the approach and API proposed in this issue turned out to be too complex and not user friendly. Work is still continuing to provide this kind of functionality but with a better more intuitive API. The start of this is the implementation of derivatives in #9293 
</comment><comment author="neilvarnas" created="2015-02-20T08:39:50Z" id="75205659">Thank You, managed to miss that comment.
</comment><comment author="brupm" created="2015-03-16T18:32:16Z" id="81856809">:+1: 
</comment><comment author="lakshmi-guruparan" created="2015-03-27T21:52:05Z" id="87102449">Hi All,

I have a requirement to aggregate on 3 fields.I need to perform range operation on the doc_count value of last aggregation. Is there a way I can accomplish this ? Let us say my query looks like the below -

 "aggregations" : {
    "g1" : {
      "terms" : {
        "field" : "TECHNOLOGY",
        "size" : 100000
      },
      "aggregations" : {
        "g2" : {
          "terms" : {
            "field" : "FEATURE_TITLE",
            "size" : 100000
          },
          "aggregations" : {
            "g3" : {
              "terms" : {
                "field" : "SOFTWARE_TYPE",
                "size" : 100000,  
              }
            }
          }
        }
      }
    }
  }

I want to explore on the possibilities of filtering the results based in doc_count value of last aggregation(g3).  Below is the result - 
g1: {
buckets: [
{
key: LAN Switching
doc_count: 271
g2: {
buckets: [
{
key: EtherChannel
doc_count: 8
g3: {
buckets: [
{
key: IOS
doc_count: 8
}
]
}
} 

Thanks.
</comment><comment author="OPtoss" created="2015-05-12T19:49:02Z" id="101399221">While I see why reducers can create an overly complex API, I don't think the specialized route is any better. There are many actions I may want to perform on the resulting buckets from an aggregation besides the derivative. The derivative approach here (#9293) is simple and easy, but very specific. How do you plan to cover all the potential use cases for this? 

For instance, I may want the entire filter API to filter the buckets returned from an aggregation. In my particular case, I want to only find the buckets with "doc_count" equal to 1. But I can see many more complex use cases for these "meta-filters".

Edit: I just found this #9876, which might answer how you're planning for other use cases. But I don't see an answer for my particular use case. Those seem to be aggregating aggregation results, not filtering them. Any idea how I can resolve my use case?
</comment><comment author="clintongormley" created="2015-05-15T17:32:56Z" id="102464849">&gt; For instance, I may want the entire filter API to filter the buckets returned from an aggregation. In my particular case, I want to only find the buckets with "doc_count" equal to 1. 

We plan to add a `having` agg which allows you to do exactly what you describe.

&gt; But I can see many more complex use cases for these "meta-filters".

Anything that these post-processing aggs can do, can also be done on the client.  So the major benefit of adding these is convenience for the user.  We'll keep adding aggs that are commonly useful, but if something is really specific, then it can still be done client side.
</comment><comment author="acarstoiu" created="2015-06-18T18:14:41Z" id="113243777">&gt; Anything that these post-processing aggs can do, can also be done on the client. So the major benefit of adding these is convenience for the user. We'll keep adding aggs that are commonly useful, but if something is really specific, then it can still be done client side.

Actually no, not really.
Of course one can theoretically crunch the data on the client side following any deterministic recipe, **but** that's not the idea! Going down that line, you'll end up saying that databases are optional, as you can always read all data in a huge (in-memory) structure and perform on them endless transformations :-1:
I'm using Elasticsearch _precisely_ because I do not want to bother with the feasibility of such a medieval approach :v:

So, no, I don't want to transfer tens of megabytes of aggregated data from Elasticsearch into my application just to be able to return to my consumer _a few figures_ obtainable by further aggregating the aggregations. No, I would very much like the aggregations of aggregations to take place on the Elasticsearch coordinating node, as the application delivers **high-level statistics**.

Some `post-aggregations` applicable on the aggregations would cover more than 90% of the needs, I reckon. There's no need for fancier aggregations, the existing ones are enough; it's just that they cannot be used on their own results. And they should :wink: 
</comment><comment author="clintongormley" created="2015-06-18T18:25:27Z" id="113249603">@acarstoiu thank you for your essay.  If, instead of venting your spleen, you'd followed some of the links above, you would have discovered the wonder that is pipeline aggregations: https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-pipeline.html

Please do remember that there are real humans here, with real feelings,  writing software which you are using.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreamContext.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BucketStreams.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/MultiBucketsAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/Filters.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/FiltersAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filters/InternalFilters.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGrid.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/Range.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRange.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistance.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4Range.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/GlobalOrdinalsStringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificanceHeuristicTests.java</file></files><comments><comment>Aggregations: Buckets can now be serialized outside of an Aggregation</comment></comments></commit></commits></item><item><title>Docs: adds note about using null_value with dynamic templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8109</link><project id="" key="" /><description>Closes #7874
</description><key id="45979316">8109</key><summary>Docs: adds note about using null_value with dynamic templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-10-16T12:23:25Z</created><updated>2014-10-21T21:40:30Z</updated><resolved>2014-10-16T13:05:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T12:33:49Z" id="59354002">Small edit, then LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sampling aggregation to filter down to top-scoring docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8108</link><project id="" key="" /><description>#### Requirement

As part of making https://github.com/elasticsearch/elasticsearch/pull/6796 more modular, there is a general need to limit analysis of documents to a subset of the top-matching results based on 
1. Quantity - some aggregations can be expensive to run on many documents so we need to cap the numbers
2. Quality - the result set of a "fuzzy" search has a long-tail of low-quality results which we want to ignore

An example problem that requires capping is in a [recommendation problem like this](http://www.elasticsearch.org/blog/significant-terms-aggregation/#recommend) where the query can yield a large number of results and we want to cap the processing time of looking up background stats from disk by fixing the volumes of results we consider.
#### Solution

The aggregation would be placed as a parent aggregation to the child aggs that need filtering. 

```
{
    "query": {
        "terms": {
            "movie_id": [
                46970,
                3726
            ]
        },
        "aggs": {
            "qualityFilter": {
                "top_docs_filter": {
                    "shard_size": 1000
                },
                "aggregations": {
                    "recommendations": {
                        "significant_terms": {
                            "field": "movie_id"
                        }
                    }
                }
            }
        }
    }
}
```

The assumption is we would always rank the top N selections on a shard based on the query score but there are various parameters we can use to control "N":
1. An absolute number e.g. "shard_size" : 100
2. An absolute score e.g.   "min_score" : 3.5
3. A relative score i.e expressed as a percentage of max score:  "relative_to_max_score" : "50%"

All of these options could be used simultaneously as an OR rule for capping results.
Option 3 would need to use a priority queue whose size is capped, either by a large sensible default or the setting chosen in 1.
#### Concerns

There is a general concern over whether this feature is a new top-level agg which is a documented part of the Query DSL (as proposed here) or an implementation detail for existing aggs best hidden from the DSL and end users. The concerns with this proposal of use as a top-level agg are:
##### Return value formatting

While a potentially useful tool during execution of a query it is questionable that we would want to see this aggregation as a nested container in the search results. If we do choose to keep it as a container in results we could return some stats e.g number of rejected docs.
##### Dangerous if forgotten

If you have a computationally expensive aggregation (e.g. a significant terms agg that needs to hit disk randomly) then you could argue it is a mistake to rely on end users configuring a separate parent agg to filter the volume of hits it processes. In the PR https://github.com/elasticsearch/elasticsearch/pull/6796 I made a conscious decision to couple the quality filter settings directly with the settings that enable use of the expensive disk-access mode. That way there's less chance of running a query-from-hell.
##### Clash with top_hits agg

If we go down the route of making this filter a stand-alone agg then for consistency's sake the [existing top_hits agg](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-top-hits-aggregation.html#search-aggregations-metrics-top-hits-aggregation) should be changed to be nested under this. In fact, we can refactor its sort and pagination features into the functionality proposed by this filtering agg.
### Conclusion

We need to decide if this functionality needs breaking out as a new agg feature in the DSL.
If we choose not to do this we need to at least have some DSL consistency and reuse of sampling logic that is defined on things like top_hits agg and signifcant_terms.
</description><key id="45966507">8108</key><summary>Sampling aggregation to filter down to top-scoring docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>feature</label></labels><created>2014-10-16T09:40:52Z</created><updated>2015-04-21T09:30:58Z</updated><resolved>2015-04-21T09:30:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-16T20:25:06Z" id="59424545">What concerns me most here is that if we have something that **needs** sampling in order to have a reasonable runtime, then I am wondering if it should be an aggregation. I can see value in sampling, but rather as a way to trade precision for speed.

I think I would prefer the sampling agg to be explicit (both in the requests and responses) because this way we can have the options that are specific to sampling in the sampling aggregation and only there. And in responses, it would help know how many documents were sampled in a given bucket?
</comment><comment author="markharwood" created="2014-10-17T08:07:51Z" id="59479857">&gt; What concerns me most here is that if we have something that needs sampling in order to have a reasonable runtime, then I am wondering if it should be an aggregation

The only existing agg I'd put in that category is "top_hits" 

The significant_terms agg as it stands today works for a large number of use cases _without_ sampling but has the potential to be slow if the query, number of docs and field cardinality combine to produce a high number of term frequency look-ups. In the movie recommendation example above, finding what movies people who watched a lesser-known movie such as "Brazil" liked may be quick but slow if you are querying for people who have watched a more popular movie like "Star Wars". 
As it happens the MovieLens dataset of user behaviours is sufficiently small for it not to be a problem but I encountered some performance issues doing a similar analysis based on Wikipedia's link structure due to the volume of docs. A quality-based sampling helps avoid the over-linked docs that cause problems due to length-norms ranking (avoids the person who has watched ALL movies or the Wikipedia article that lists all living actors). Note this is nothing to do with "free-text" analytics which is a distinction we previously drew as possible justification for removing significant_terms from the aggs tree. There are structured applications of this background-frequency checking logic.

If an agg is generally fast enough but has the potential to overrun in the wrong conditions the improved timeout controls (e.g. https://github.com/elasticsearch/elasticsearch/pull/4586 ) will help steer people towards redefining the query appropriately 

I'm happy to start work on a sampling agg but uncertain about what your comments might mean for the future of top_hits.
</comment><comment author="markharwood" created="2015-02-20T10:26:17Z" id="75217244">Waiting on recent Lucene 5.1 snapshot that adds DiversifiedTopDocsCollector from https://issues.apache.org/jira/browse/LUCENE-6066. I think this is the PR that will bring this in: https://github.com/elasticsearch/elasticsearch/pull/9746
This means our samples can optionally have a choice of key on which to diversify e.g. max 5 tweets from the same author in our sample of 1000 top-scoring tweets. Diversity is important if you choose to sample.
</comment><comment author="rmuir" created="2015-02-20T13:35:52Z" id="75238471">I just pushed the upgrade, with the new topdocscollector. Sorry for the delay.
</comment><comment author="markharwood" created="2015-02-20T13:52:25Z" id="75240567">Thanks! Wasn't it expecting to happen so soon. Excited to get this in.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BestBucketsDeferringCollector.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BestDocsDeferringCollector.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/DeferringBucketCollector.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedBytesHashSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedMapSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedNumericSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/DiversifiedOrdinalsSamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/InternalSampler.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/Sampler.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregationBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/SamplerParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/sampler/UnmappedSampler.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/SamplerTests.java</file></files><comments><comment>New feature - Sampler aggregation used to limit any nested aggregations' processing to a sample of the top-scoring documents.</comment><comment>Optionally, a “diversify” setting can limit the number of collected matches that share a common value such as an "author".</comment></comments></commit></commits></item><item><title>Documents in deleted mapping are preserved if an index is closed and re-opened</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8107</link><project id="" key="" /><description>Hello everyone!

Here's the test case, which is pretty self-describing:

``` shell
#create an index and some mapping
curl -XPUT localhost:9200/mk

curl -XPUT localhost:9200/mk/loc/_mapping -d '{
  "loc": {
    "dynamic": "strict",
    "properties": {
      "isLeaf": {
        "type": "boolean"
      }
    }
  }
}'

#put a document to this mapping

curl -XPUT localhost:9200/mk/loc/1 -d '{
    "isLeaf" : true
}'

#now delete previously created mapping 

curl -XDELETE localhost:9200/mk/loc/_mapping

#close the index and reopen int

curl -XPOST localhost:9200/mk/_close
curl -XPOST localhost:9200/mk/_open

#re-create the mapping

curl -XPUT localhost:9200/mk/loc/_mapping -d '{
  "loc": {
    "dynamic": "strict",
    "properties": {
      "isLeaf": {
        "type": "boolean"
      }
    }
  }
}'

#now the bug reveals itself — newly created mapping is not empty, but contains documents that were put there before mapping deletion

curl -XGET localhost:9200/mk/loc/_search -d '{
    "query": {"match_all": {}}
}'
```

And there's more: I re-created the exact same mapping in the test case, but you actually can put completely different mapping, and the bug will still appear: you'll get the exact same result after executing query.

Thanks in advance for fixing this (or pointing out where I've done smth wrong =)
</description><key id="45966157">8107</key><summary>Documents in deleted mapping are preserved if an index is closed and re-opened</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radiosterne</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-10-16T09:36:48Z</created><updated>2015-11-21T21:38:09Z</updated><resolved>2015-11-21T19:54:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T18:29:45Z" id="59407607">@radiosterne thanks for reporting. I can reproduce this.

It seems the transaction log is replayed when reopening the index, as flushing before closing resolves this issue.
</comment><comment author="clintongormley" created="2015-11-21T19:54:03Z" id="158677500">Mappings can no longer be deleted. Closing
</comment><comment author="radiosterne" created="2015-11-21T21:38:09Z" id="158683994">Great job, thanks Clinton!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The parameter "now" seems doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8106</link><project id="" key="" /><description>Hi everyone,

We have this properties in the index "grafana-ann" in 1.3.4 Elasticsearch:

``` sh
curl http://localhost:9200/grafana-ann/_search
```

``` json
{"took":6,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"grafana-ann","_type":"ann","_id":"R5OJt-L1TGqw8bJpnwNT0g","_score":1.0,"_source":{
    "timestamp" : "2014/10/16 10:48:12",
    "text" : "desplegament al minut 10:48",
    "desc" : "això es un desplegament al minut 10:48",
    "time" : "10:48",
    "tags" : "deploy"
}},{"_index":"grafana-ann","_type":"ann","_id":"1pTIAzqeSuySf0uAAX0bEw","_score":1.0,"_source":{
    "timestamp" : "2014/10/16 10:48:04",
    "text" : "canvi al minut 10:48",
    "desc" : "això es un canvi al minut 10:48",
    "time" : "10:48",
    "tags" : "canvi"
}}]}}
```

We would like to find the second one id searching wiht the timestamp and the tag, like the following:

``` sh
curl -XPOST 'http://localhost:9200/grafana-ann/_search' -d '{
  "fields": [
    "timestamp",
    "_source"
  ],
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "tags:deploy"
              }
            }
          ]
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "timestamp": {
                  "from": "2014/10/16 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  },
  "size": 100
}'
```

But doesn't work. 

{"took":6,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}

Later we have tried the same command with the complet date format, without the "now": 

``` json
 "timestamp": {
                  "from": "2014/10/16 08:00:00",
                  "to": "2014/10/16 11:00:00"
                 }
```

And in this case, we obtain the correct id:

``` json
{"took":5,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"grafana-ann","_type":"ann","_id":"R5OJt-L1TGqw8bJpnwNT0g","_score":0.30685282,"_source":{
    "timestamp" : "2014/10/16 10:48:12",
    "text" : "desplegament al minut 10:48",
    "desc" : "això es un desplegament al minut 10:48",
    "time" : "10:48",
    "tags" : "deploy"
},"fields":{"timestamp":["2014/10/16 10:48:12"]}}]}}
```

Do you know why "now" doesn't work?

Thank you very much!
</description><key id="45963567">8106</key><summary>The parameter "now" seems doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ricardmestre</reporter><labels /><created>2014-10-16T09:06:01Z</created><updated>2014-10-22T12:23:13Z</updated><resolved>2014-10-22T12:23:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-16T09:09:42Z" id="59334022">Do you get any error message?
</comment><comment author="ricardmestre" created="2014-10-16T09:12:51Z" id="59334407">No. Sorry I'm going to update the comment with the output of the first command.
</comment><comment author="dadoonet" created="2014-10-16T09:33:57Z" id="59336729">Please also provide your mapping. I think your timestamp is a String and not a date.
</comment><comment author="ricardmestre" created="2014-10-16T10:05:14Z" id="59340043">I check it too, but it seems that is a date:

"grafana-ann" : {
    "mappings" : {
      "ann" : {
        "properties" : {
          "desc" : {
            "type" : "string"
          },
          "tags" : {
            "type" : "string"
          },
          "text" : {
            "type" : "string"
          },
          "time" : {
            "type" : "string"
          },
          "timestamp" : {
            "type" : "date",
            "format" : "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
          }
        }
      }
    }
  },
</comment><comment author="clintongormley" created="2014-10-16T10:14:36Z" id="59340962">Perhaps your timestamps are using a non-UTC timezone?  The `now` function uses UTC, so may be earlier than your `from` value.

Try this to see the real values that are being used:

```
curl -XPOST "http://localhost:9200/grafana-ann/_validate/query?explain&amp;pretty" -d'
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "should": [
            {
              "query_string": {
                "query": "tags:deploy"
              }
            }
          ]
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "timestamp": {
                  "from": "2014/10/16 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}'
```
</comment><comment author="ricardmestre" created="2014-10-16T10:18:59Z" id="59341402">I think this is OK:

{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "grafana-ann",
    "valid" : true,
    "explanation" : "filtered(tags:deploy)-&gt;BooleanFilter(+no_cache(timestamp:[1413446400000 TO 1413454693332]))"
  } ]
}

date +%s

1413454694
</comment><comment author="dadoonet" created="2014-10-16T13:15:20Z" id="59358749">I just tried this:

``` json
DELETE test
PUT test
{
  "mappings": {
    "doc": {
      "properties": {
        "date": {
          "type": "date",
          "format": "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
        }
      }
    }
  }
}
PUT test/doc/1
{
  "date": "2014/10/16 10:10:00"
}
GET test/doc/_search
{
  "fields": [
    "timestamp",
    "_source"
  ],
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "date": {
                  "from": "2014/10/16 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}
```

This gives correct results:

``` json
{
   "took": 74,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "doc",
            "_id": "1",
            "_score": 1,
            "_source": {
               "date": "2014/10/16 10:10:00"
            }
         }
      ]
   }
}
```

I don't think there is an issue. Any chance you could create a full reproduction as I just wrote?
</comment><comment author="clintongormley" created="2014-10-16T18:43:21Z" id="59409695">@ricardmestre when you ran the validate-query request mentioned above, did you check whether the equivalent query was working at the same time?  obviously a few hours passed from the time you provided the original query to the time that you ran validate query...
</comment><comment author="ricardmestre" created="2014-10-20T09:39:16Z" id="59715617">Hi @dadoonet,

As you suggestted , I have done the following steps:

1.- Delete the index:

```
curl -XDELETE http://localhost:9200/test/

{"acknowledged":true}
```

2.- Create the new index with the mapping:

```
curl -XPUT http://localhost:9200/test/ -d '{
  "mappings": {
    "doc": {
      "properties": {
        "date": {
          "type": "date",
          "format": "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
        }
      }
    }
  }
}'

{"acknowledged":true}
```

3.- insert the tested data:

```
curl -XPOST http://localhost:9200/test/doc/ -d '{"date": "2014/10/20 10:10:00"}'

{"_index":"test","_type":"doc","_id":"z4LgoPHaTlW3pSZ1PjNbQQ","_version":1,"created":true}
```

4.- Look if is created:

```

curl -XGET http://localhost:9200/grafana-ann/ann/_search

{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "doc",
      "_id" : "z4LgoPHaTlW3pSZ1PjNbQQ",
      "_score" : 1.0,
      "_source":{"date": "2014/10/20 10:10:00"}
    } ]
  }
}
```

5.- Query with "now" keyword:

```
curl -XGET http://localhost:9200/test/doc/_search?pretty -d '{
  "fields": [
    "timestamp",
    "_source"
  ],
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "date": {
                  "from": "2014/10/20 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}'


{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

This is the current time that we are using:

```
date +"%Y/%m/%d %H:%M:%S

2014/10/20 11:17:42
```

As you can see something is not working since the second query doesn't return the data that we have inserted. Can you give me any clue?
</comment><comment author="clintongormley" created="2014-10-20T10:10:34Z" id="59720391">@ricardmestre Repeat the above experiment, but then (at the same time) run the validate-query request, to see the actual value that `now` is generating. When you use `date` from the console, it is giving you a local time, but Elasticsearch uses UTC.
</comment><comment author="ricardmestre" created="2014-10-20T11:47:32Z" id="59735095">@clintongormley I have tried the same process with the change that you suggested:

```
curl -XGET "http://localhost:9200/test/_validate/query?explain&amp;pretty" -d '{
  "fields": [
    "timestamp",
    "_source"
  ],
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "date": {
                  "from": "2014/10/20 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}'
```

And this is the result, seems that there is an error:

```
{
  "valid" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "test",
    "valid" : false,
    "error" : "org.elasticsearch.index.query.QueryParsingException: [test] request does not support [fields]"
  } ]
}
```

Do you know what is the problem? 
Thank you!
</comment><comment author="clintongormley" created="2014-10-20T12:43:13Z" id="59744056">@ricardmestre Pass just the `query` to the validate-query request.
</comment><comment author="clintongormley" created="2014-10-20T12:43:47Z" id="59744108">And remember to add `?explain` in the query string
</comment><comment author="ricardmestre" created="2014-10-21T10:35:22Z" id="59908629">Sorry @clintongormley I can't understand why are you requesting again that since I did before 

this one

```
curl -XGET "http://localhost:9200/test/_validate/query?explain&amp;pretty" 
```

. Isn't it?

If you are requesting for a different query , can you give me more feedback on how to do it ?
</comment><comment author="clintongormley" created="2014-10-21T10:56:45Z" id="59910700">@ricardmestre OK I didn't see the `?explain` in what you pasted.  All I'm saying is:
- follow the process you explained in https://github.com/elasticsearch/elasticsearch/issues/8106#issuecomment-59715617
- then run a `/_validate/query?explain` request, passing in just the `query` parameter, not the `fields` parameter
</comment><comment author="ricardmestre" created="2014-10-21T11:05:53Z" id="59911566">@clintongormley, I have done what you say in this is the output:

```
{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "test",
    "valid" : true,
    "explanation" : "ConstantScore(BooleanFilter(+no_cache(date:[1413878400000 TO 1413889453343])))"
  } ]
}
```
</comment><comment author="clintongormley" created="2014-10-21T11:39:56Z" id="59914732">thanks @ricardmestre 

OK, please could you run the following requests and paste the output of each request:

```
DELETE test

PUT /test/
{
  "mappings": {
    "doc": {
      "properties": {
        "date": {
          "type": "date",
          "format": "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
        }
      }
    }
  }
}

POST /test/doc/1
{"date": "2014/10/20 10:10:00"}

GET /test/_mapping?pretty

GET /test/_search
{
  "fielddata_fields": ["date"]
}

GET /test/doc/_search?pretty
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "date": {
                  "from": "2014/10/30 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}    

GET /test/doc/1/_explain?pretty
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "date": {
                  "from": "2014/10/30 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}
```
</comment><comment author="ricardmestre" created="2014-10-21T11:56:17Z" id="59916279">Ok @clintongormley, here goes:

```
curl -XDELETE http://localhost:9200/test/

{"acknowledged":true}
```

```
curl -XPUT http://localhost:9200/test/ -d '{
  "mappings": {
    "doc": {
      "properties": {
        "date": {
          "type": "date",
          "format": "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
        }
      }
    }
  }
}'

{"acknowledged":true}
```

```
curl -XPOST http://localhost:9200/test/doc/1 -d '{"date": "2014/10/21 10:10:00"}'

{"_index":"test","_type":"doc","_id":"1","_version":1,"created":true}
```

```
curl -XGET http://localhost:9200/test/_mapping?pretty

{
  "test" : {
    "mappings" : {
      "doc" : {
        "properties" : {
          "date" : {
            "type" : "date",
            "format" : "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
          }
        }
      }
    }
  }
}
```

```
curl -XGET http://localhost:9200/test/_search?pretty -d '{"fielddata_fields": ["date"]}'

{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"date": "2014/10/21 10:10:00"},
      "fields" : {
        "date" : [ 1413886200000 ]
      }
    } ]
  }
}
```

```
curl -XGET http://localhost:9200/test/doc/_search?pretty -d '{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "date": {
                  "from": "2014/10/21 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}'

{
  "took" : 11,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"date": "2014/10/21 10:10:00"}
    } ]
  }
}
```

```
curl -XGET http://localhost:9200/test/doc/1/_explain?pretty -d '{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "date": {
                  "from": "2014/10/21 08:00:00",
                  "to": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}'

{
  "_index" : "test",
  "_type" : "doc",
  "_id" : "1",
  "matched" : true,
  "explanation" : {
    "value" : 1.0,
    "description" : "ConstantScore(BooleanFilter(+no_cache(date:[1413878400000 TO 1413892461329]))), product of:",
    "details" : [ {
      "value" : 1.0,
      "description" : "boost"
    }, {
      "value" : 1.0,
      "description" : "queryNorm"
    } ]
  }
}
```

I can observe that in this way the query is done correctly, but it is important for me to do it with the fields parameter. Can you help me?

Thank you!!
</comment><comment author="clintongormley" created="2014-10-21T12:56:21Z" id="59922693">@ricardmestre the `fields` parameter should work just fine (although in your previous test example you were asking for `timestamp` but the field was called `date`).

So, you have different ways of requesting individual fields:
- `_source` allows you to include/exclude the parts of the `_source` field. The values return correspond to the original format as you indexed the field. See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-source-filtering.html
- `fields` tries to load a "stored" field, and falls back to extracting from `_source` if the field is not stored. It returns a field as an array of values, because we can't be sure if the original document contained a single value or an array of values
- `fielddata_fields` loads the values of the field into fielddata, and returns them in the format that they are stored in fielddata, which is why you get back ms-since-the-epoch for the `date` field, instead of the original date.

You should be able to use any of these parameters with the query that I showed above.  (And it works for me when I test it on 1.3.4).  So I think you're doing something wrong, but I'm not really sure what it is.  You need to play around with adding and removing parameters until you find out what is causing the query not to work, and figure it out from there.
</comment><comment author="ricardmestre" created="2014-10-22T11:08:36Z" id="60068820">@clintongormley, the problem is solved! The issue were with the Time Zone. I query with the Zero time zone and I am at the +2 time zone.
Sorry for the nuisances, and thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Indices exists returns false during recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8105</link><project id="" key="" /><description>I have a problem with the embedded Java client (for testing) after moving our [our Java integration library](https://github.com/molindo/esi4j) from 1.0 to 1.4.0.Beta. We use the Java client embedded. 

During application restart, we check if an index exists and create or update it. Now we have the problem, that [indices exists](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-exists.html) returns false, causing [create index](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/indices-create-index.html) to fail.

I tried IRC for a solution but @karmi thought it's supposedly a bug.

(test case to follow)
</description><key id="45959856">8105</key><summary>Indices exists returns false during recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sfussenegger</reporter><labels><label>:Index APIs</label><label>discuss</label></labels><created>2014-10-16T08:20:16Z</created><updated>2016-06-23T15:52:09Z</updated><resolved>2016-06-23T15:52:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sfussenegger" created="2014-10-16T11:35:01Z" id="59348441">Ok, so here's [a test case](https://github.com/molindo/elasticsearch8105/blob/master/src/main/java/at/molindo/elasticsearch8105/Main.java) although further discussion with @karmi suggested it's working as designed. 

Nevertheless, running the linked code twice results in `org.elasticsearch.indices.IndexAlreadyExistsException: [foobar] already exists` (Note: it's reliably reproducible for me, but since it's a race condition, results may differ)

I've tested several version of ES and it turned out that 1.3.0 was the version introducing the change. 

A viable workaround is waiting for cluster status before checking for existence (yellow is sufficient).

Not sure if this is a bug or by design, so feel free to proceed as you see fit.
</comment><comment author="clintongormley" created="2014-10-16T18:39:39Z" id="59409134">I'd be interested in seeing what change in 1.3 is causing this too...
</comment><comment author="zakmagnus" created="2015-04-17T23:18:21Z" id="94095307">I also run into this issue. I'm trying to see if I can repro it even when not in local mode. If I can, then I think it would definitely be a big problem. I don't think "wait for yellow state" is an acceptable solution when using ES as a remote system, because it can always just go down and then come back up, or change in any other arbitrary way, in between the "wait for yellow" request and the actual work request that I want it to do.

But maybe it's only a local mode quirk. That is still pretty annoying, because it's hard to test with local mode when it introduces its own strange behaviors. Whenever a test fails, I have to try to figure out if it's exposing a real buggy behavior or if it's just local mode being weird.
</comment><comment author="zakmagnus" created="2015-04-19T00:53:22Z" id="94217224">A few more interesting notes:

Index creation is asynchronous (right?) so I actually think we can't expect Stefan's test case to pass. There doesn't seem to be any waiting after making the index creation call, and that call returning does not mean that an index has actually been created. For this reason alone, the test case may fail.

I tried waiting for yellow state and for an active shard after creating an index, but it didn't work. It would still often say that the index did not exist, even after those waits returned successfully, if asked quickly enough. This seems like a local mode problem, at the very least.

I should note that I was also shutting down the local node in between every operation, to stress it more. For example: start local node, create index, wait for health state, shut down node, start up node, check if index exists, shut down node. Note that I inserted a health wait after creation, but it didn't help. However, if I insert a health wait right before the check, then I could not get it to say that the index doesn't exist. I can't tell if this is because yellow state is somehow necessary, or if just putting that call in delayed the timing enough to change the race that I seemed to be observing.

I could not reproduce this issue with an actual external cluster as opposed to local mode. As long as I waited for a healthy state right after each creation call, then each index existence check would return accurately, regardless of what waits I did beforehand. There's a lot more time in between each event when I have to poke an external program, though, so again, I can't tell if this is just a case of slowing things down enough to hide any race conditions. But I'm pleased that at least it's not trivial to get an actual ES cluster to tell me wrong information.
</comment><comment author="clintongormley" created="2015-04-25T15:12:00Z" id="96220593">Also see #9126
</comment><comment author="Schaka" created="2016-06-22T12:23:34Z" id="227726694">I ran into this same issue with 5.0.0-alpha3. 

During index recovery, which is asynchronous, we cannot check whether the index exists. So we need to wait for the cluster to start fully (or an event for recovery - which currently doesn't seem possible) until we check whether the index exists and THEN create it.

I thought this would be possile by registering a ClusterStateChangeListener, but no event seems to be fired for what I required.

Any workaround?
</comment><comment author="ywelsch" created="2016-06-23T15:46:07Z" id="228093332">I've opened  #19047 that fixes this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Question: shard awareness allocation and shard allocation filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8104</link><project id="" key="" /><description>We have two kinds of nodes: those with ssds (used for indexing and search recent data), those with large spinning disks (used for archiving old indices).

I'd like to setup a mechanism to move old indices from ssds to spinning disks.

The first solution uses reroute command in cluster api. However it feels unnatural since you have to do it shard by shard and decide the target node.

What I want to achieve is the following:
1. stick recent indices (the current one being written) to ssds. They have 2 copies.
2. at some point (disk on ssds is above 65%), one copy is moved to larger boxes (1 copy is still on ssd to help search, 1 copy on large box)
3. when disk is scarce on ssd boxes (90%), we simply drop the copy present on ssd. Since we don't care that much of old data having only one copy is not an issue.

I have tried to implement this with shard awareness allocation and allocation filtering but it does not seem to work as expected.

Nodes have `flavor` attribute depending on their hardware (`ssd` or `iodisk`).
Cluster is using shard awareness based on `flavor` attribute (`cluster.routing.allocation.awareness.attributes: flavor`).
1. My index template has `routing.allocation.require: ssd` to impose two have all copies on ssds first. 
2. At some point, I drop the requirement (effectively `routing.allocation.require: *``). I expect flavor awareness to move one copy to large (iodisk) boxes.
3. At a later point, I'll set `number_of_replicas` to 0 and change `routing.allocation.require` to `iodisk` to drop the shard copy on ssds

Sadly allocation filtering and shard awareness do not seem to cooperate well :
when an new index is created, one copy goes to ssds and the other is not allocated anywhere (index stays in yellow state).

Using `curl -XPUT localhost:9200/_cluster/settings -d '{"transient":{"logger.cluster.routing.allocation":"trace"}}'`,
I have observed what happen when a new index is created.

```
[2014-10-16 06:53:19,462][TRACE][cluster.routing.allocation.decider] [bungeearchive01-par.storage.criteo.preprod] Can not allocate [[2014-10-16.01][3], node[null], [R], s[UNASSIGNED]] on node [qK34VLdhTferCQs2oNJOyg] due to [SameShardAllocationDecider]
[2014-10-16 06:53:19,463][TRACE][cluster.routing.allocation.decider] [bungeearchive01-par.storage.criteo.preprod] Can not allocate [[2014-10-16.01][3], node[null], [R], s[UNASSIGNED]] on node [gE7OTgevSUuoj44RozxK0Q] due to [AwarenessAllocationDecider]
[2014-10-16 06:53:19,463][TRACE][cluster.routing.allocation.decider] [bungeearchive01-par.storage.criteo.preprod] Can not allocate [[2014-10-16.01][3], node[null], [R], s[UNASSIGNED]] on node [Y2k9qXfsTx6X2iQTxg9RBQ] due to [AwarenessAllocationDecider]
[2014-10-16 06:53:19,463][TRACE][cluster.routing.allocation.decider] [bungeearchive01-par.storage.criteo.preprod] Can not allocate [[2014-10-16.01][3], node[null], [R], s[UNASSIGNED]] on node [FwWc2XPPRWuje2KH6AlDEQ] due to [FilterAllocationDecider]
[2014-10-16 06:53:19,492][TRACE][cluster.routing.allocation.allocator] [bungeearchive01-par.storage.criteo.preprod] No Node found to assign shard [[2014-10-16.01][3], node[null], [R], s[UNASSIGNED]]
```

This transcript shows that 
- shard 3 primary replica is on node qK34VLdhTferCQs2oNJOyg (flavor:ssd) which prevent its copy to placed there
- it cannot be placed on gE7OTgevSUuoj44RozxK0Q (ssd as well) because it tries to maximizes dispersion accross flavors
- it cannot be placed on Y2k9qXfsTx6X2iQTxg9RBQ for the same reason
- it cannot be placed on FwWc2XPPRWuje2KH6AlDEQ (flavor: iodisk) because of the filter

Questions:
- am I doing it wrong?
- should I stick with a set of reroute command?
- are awareness and filtering supposed to cooperate?
</description><key id="45956626">8104</key><summary>Question: shard awareness allocation and shard allocation filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kamaradclimber</reporter><labels /><created>2014-10-16T07:34:05Z</created><updated>2014-10-16T09:42:37Z</updated><resolved>2014-10-16T09:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-16T09:27:37Z" id="59336006">can you please ask question on the mailing list this issue tracker is only for bugs / features etc.
</comment><comment author="kamaradclimber" created="2014-10-16T09:42:37Z" id="59337657">will do

too bad google group does not support markdown formatting :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Some Data Cant't be obtained.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8103</link><project id="" key="" /><description>In previous case, the first 100 data can't be obtained if it has.
</description><key id="45945015">8103</key><summary>Some Data Cant't be obtained.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">wmx3ng</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-16T03:36:41Z</created><updated>2014-10-17T13:21:15Z</updated><resolved>2014-10-17T13:21:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-16T09:26:43Z" id="59335891">thanks for this PR - can you please sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) so I can pull this in
</comment><comment author="clintongormley" created="2014-10-17T13:20:52Z" id="59511231">thanks @wmx3ng  - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Some Data Cant't be obtained.</comment></comments></commit></commits></item><item><title>snapshot should work when cluster is in read_only mode.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8102</link><project id="" key="" /><description>I was trying to make a full, consistent backup before an upgrade.  Snapshots are at a moment of time, which doesn't work if clients are still updating your indexes.

I tried putting the cluster into read_only mode by setting cluster.blocks.read_only: true, but running a snapshot returned this error:

```
{"error":"ClusterBlockException[blocked by: [FORBIDDEN/6/cluster read-only (api)];]","status":403}
```

Please consider allowing snapshots to provide a consistent backup by running when in read-only mode.
</description><key id="45917528">8102</key><summary>snapshot should work when cluster is in read_only mode.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">webmstr</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-10-15T20:48:17Z</created><updated>2015-07-16T11:57:23Z</updated><resolved>2015-06-11T19:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T18:19:02Z" id="59405923">@webmstr Snapshots are still moment in time while updates are happening.  You don't need to lock anything.  A snapshot will only backup the state of the index at the point that the backup starts, it won't take any later changes into account.
</comment><comment author="webmstr" created="2014-10-16T20:09:17Z" id="59422228">As I mentioned, snapshots - as currently implemented - are an unreasonable method of performing a consistent backup prior to an upgrade.  This enhancement would have allowed that option.

Without the enhancement, snapshots should not be used before an upgrade, because the indexes may have been changed while the snapshot was running.  As such, the upgrade documentation should be changed to not propose the use of snapshots as backups, and a "full" backup procedure should be documented in its place.
</comment><comment author="clintongormley" created="2014-10-17T05:04:47Z" id="59466808">Out of interest, why don't you just stop writing to your cluster?  Reopening for discussion.
</comment><comment author="clintongormley" created="2014-10-17T05:05:03Z" id="59466831">@imotov what are your thoughts?
</comment><comment author="webmstr" created="2014-10-17T06:18:42Z" id="59470869">I could turn off logstash, but that's just one potential client.  Someone could be curl'ing, or using an ES plugin (like head), etc.  If you need a consistent backup, you have to disconnect and lock out the clients from the server side.
</comment><comment author="imotov" created="2014-10-17T13:33:58Z" id="59512983">@clintongormley see https://github.com/elasticsearch/elasticsearch/pull/5876 I think this one is similar. 
</comment><comment author="clintongormley" created="2014-10-17T13:39:15Z" id="59513667">@imotov thanks, so setting `index.blocks.write` to `true` on all indices would be a reasonable workaround, at least until #5855 is resolved.
</comment><comment author="saahn" created="2014-11-12T23:26:35Z" id="62814670">@clintongormley Actually, I discovered that the `index.blocks.write` attribute only prevents writes to **existing** indices. If a client tries to create a new index, that request succeeds, which brings us back to the same problem. My workaround was to shutdown the proxy node though which our clients access our ES cluster.
I am running into the same issue as @webmstr , but for different reason: I cannot create a consistent backup for a restore to a secondary datacenter because each snapshot takes ~1 hour to complete and we cannot afford to block writes from our clients for such a long period of time. 
I am still trying to root cause why snapshots are taking so long; the time required for snapshot completion increases with each snapshot. However, when i restore the same data to a new cluster, snapshotting that data to a new S3 bucket takes less than a minute. 

EDIT: I may have a theory on why the snapshots were taking so long... i was taking a snapshot every two hours, and the s3 bucket has a LOT of snapshots now (49). I'm thinking that the calls the ES aws plugin makes to the S3 endpoint slow down over time as the number of snapshots increase. 

Or may be it's just the number of snapshots that's causing the slowness...i.e. regardless of whether the backend repository is S3 or fs? I guess I should have an additional cron job that deletes older snaphots. Is there a good rule of thumb on the number of snapshots to retain?
</comment><comment author="colings86" created="2015-02-20T10:37:12Z" id="75218561">@imotov we discussed this issue but were unclear on what the differences are between the index.blocks.\* options are and why the snapshot fails with read_only set to false?
</comment><comment author="imotov" created="2015-02-20T15:54:26Z" id="75260795">@colings86 there is an ongoing effort to resolve this issue in #9203
</comment><comment author="imotov" created="2015-05-19T16:35:32Z" id="103582661">After discussing this with @tlrx it looks like the best way to address this issue is by moving snapshot and restore cluster state elements from cluster metadata to a custom cluster element where it seems to belong (since information about currently running snapshot and restore hardly qualifies as metadata).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>core/src/main/java/org/elasticsearch/cluster/RestoreInProgress.java</file><file>core/src/main/java/org/elasticsearch/cluster/SnapshotsInProgress.java</file><file>core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java</file><file>core/src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>core/src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>core/src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>core/src/test/java/org/elasticsearch/action/admin/cluster/snapshots/SnapshotBlocksTests.java</file><file>core/src/test/java/org/elasticsearch/cluster/ClusterStateDiffTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/AbstractSnapshotTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>core/src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: Move in-progress snapshot and restore information from custom metadata to custom cluster state part</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/verify/TransportVerifyRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/get/TransportGetIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/TransportGetIndexTemplatesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlock.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlockLevel.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoverySettings.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/repositories/RepositoryBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/snapshots/SnapshotBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/tasks/PendingTasksBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/create/CreateIndexTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/flush/FlushBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/get/GetIndexTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/optimize/OptimizeBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/refresh/RefreshBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsBlocksTests.java</file><file>src/test/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsBlocksTests.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/blocks/SimpleBlocksTests.java</file><file>src/test/java/org/elasticsearch/cluster/BlockClusterStatsTests.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/cluster/block/ClusterBlockTests.java</file><file>src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsTests.java</file><file>src/test/java/org/elasticsearch/cluster/shards/ClusterSearchShardsTests.java</file><file>src/test/java/org/elasticsearch/gateway/RecoverAfterNodesTests.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/exists/indices/IndicesExistsTests.java</file><file>src/test/java/org/elasticsearch/indices/exists/types/TypesExistsTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleGetMappingsTests.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/GetSettingsBlocksTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/UpdateSettingsTests.java</file><file>src/test/java/org/elasticsearch/indices/state/OpenCloseIndexTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateBlocksTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/IndicesWarmerBlocksTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Internal: Add METADATA_READ and METADATA_WRITE blocks</comment></comments></commit></commits></item><item><title>Not all stop words are being removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8101</link><project id="" key="" /><description>It seems not all stop words are being removed if they appear in a specific order.

Here is the gist for this:
https://gist.github.com/meganeinu7/9591cefb4a56c5ca581b
</description><key id="45911516">8101</key><summary>Not all stop words are being removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meganeinu7</reporter><labels /><created>2014-10-15T19:58:36Z</created><updated>2014-10-16T19:07:37Z</updated><resolved>2014-10-16T09:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-16T09:31:01Z" id="59336389">try to remove the quotes `"` around the queries. I think you are producing phrase queries which will likely not match
</comment><comment author="s1monw" created="2014-10-16T09:31:18Z" id="59336417">and please use the mailing list for questions like this....
</comment><comment author="meganeinu7" created="2014-10-16T19:07:37Z" id="59413500">I found the solution and it was not related to stop words.
The issue was that I was using multi_match query with phrase type which internally uses match_phrase query. That's why the stop words in the middle of phrases are not being removed. Changing the type to cross_fields with AND operator was the solution I was looking for.
Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Children aggregation for recursive parent-child relation leads to incorrect results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8100</link><project id="" key="" /><description>Hi,

I tried to create recursive parent-child relation:

family-mapping.json:

``` javascript
{
  "mappings": {
    "member": {
      "_parent": {
        "type": "member" 
      },
      "properties" : { 
        "name" : {
          "type" : "string",
          "index" : "not_analyzed"
        }
      }
    }
  }
}
```

family.json:

``` javascript
{ "index" : { "_id" : "1", "parent" : "0" } }
{ "name" : "Abraham Simpson" }
{ "index" : { "_id" : "2", "parent" : "1" } }
{ "name" : "Homer J Simpson" }
{ "index" : { "_id" : "3", "parent" : "2" } }
{ "name" : "Bart Simpson" }
```

aggregation.json:

``` javascript
{
  "aggs" : {
    "parent" : {
       "terms" : { "field" : "name" },
       "aggs" : {
          "child-members" : {
             "children" : { "type" : "member" },
             "aggs" : {
               "child-names" : {
                 "terms" : { "field" : "name" }
               }
             }
          }
       }
    }
  }
}
```

When I'm trying to aggregate this data I'm getting child buckets inside parent ones with same name:

``` javascript
.....
  "aggregations" : {
    "parent" : {
      "doc_count_error_upper_bound" : 0,
      "buckets" : [ {
        "key" : "Abraham Simpson",
        "doc_count" : 1,
        "child-members" : {
          "doc_count" : 1,
          "child-names" : {
            "doc_count_error_upper_bound" : 0,
            "buckets" : [ {
              "key" : "Abraham Simpson",
              "doc_count" : 1
            } ]
          }
        }
      }, {
        "key" : "Bart Simpson",
        "doc_count" : 1,
        "child-members" : {
          "doc_count" : 1,
          "child-names" : {
            "doc_count_error_upper_bound" : 0,
            "buckets" : [ {
              "key" : "Bart Simpson",
              "doc_count" : 1
            } ]
          }
        }
      }, {
        "key" : "Homer J Simpson",
        "doc_count" : 1,
        "child-members" : {
          "doc_count" : 1,
          "child-names" : {
            "doc_count_error_upper_bound" : 0,
            "buckets" : [ {
              "key" : "Homer J Simpson",
              "doc_count" : 1
            } ]
          }
        }
      } ]
    }
  }
}
```

I'm using 1.4.0-beta1 version (downloaded from elasticsearch.org). 
</description><key id="45901510">8100</key><summary>Children aggregation for recursive parent-child relation leads to incorrect results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">whiter4bbit</reporter><labels /><created>2014-10-15T18:27:47Z</created><updated>2015-05-30T08:41:18Z</updated><resolved>2015-05-30T08:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T18:13:18Z" id="59405104">@martijnvg I've assigned this to you. I didn't think we supported recursive p/c mappings?
</comment><comment author="martijnvg" created="2014-10-17T07:13:58Z" id="59474613">@clintongormley In general in parent/child we never explicitly supported it, but I think we should. This kind of usage works for the `has_child` query, but not for the `has_parent` query (#7357) and `children` agg.
</comment><comment author="gmenegatti" created="2014-10-22T21:16:54Z" id="60156978">+1
</comment><comment author="rieder91" created="2014-11-13T11:29:16Z" id="62877492">is there any known workaround?
</comment><comment author="brendangibat" created="2015-01-14T20:05:06Z" id="69981817">+1
</comment><comment author="ArkeologeN" created="2015-04-17T10:03:02Z" id="93957182">+1 - I also need parent / child mapping for nested `categories` pattern which I'm importing through river from MySQL.
</comment><comment author="martijnvg" created="2015-05-30T08:41:16Z" id="107010425">Closing in favour for #11432.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>field type mapping is wrong besides previous template definition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8099</link><project id="" key="" /><description>Situation:
- Template with two mappings definition
- Index already exists with one type mapping filled

Problems:
1) When inserting for the first time an entry for the second type mapping, the field is defined as String instead of the template matching mapping (in this case anything different than String)
2) Also happens when the two mappings exists in the same index, and a new field is added to the mapping, regardless where I choose double, long, the first insert defines the field data type as String.

Workaround:
I have to delete the index and insert again

Versions where I found this issue:
- 1.1 to 1.4 beta
</description><key id="45894052">8099</key><summary>field type mapping is wrong besides previous template definition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">farracha</reporter><labels><label>feedback_needed</label></labels><created>2014-10-15T17:15:42Z</created><updated>2014-10-17T08:28:46Z</updated><resolved>2014-10-17T05:20:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T18:11:55Z" id="59404886">Hi @farracha 

Please can you provide a simple but complete replication, with curl statements that we can execute locally.  It's much easier to follow code.

thnkas
</comment><comment author="farracha" created="2014-10-16T23:03:08Z" id="59444452">I couldn't reproduced it with curl on the first scenario:
Tried standalone indexing and with bulk api ( both types at the same time and one at a time), but the datatype was correct in index metadata.

I have to think on an easy way to replicate my issue, because I'm using the java api (transport class) with bulkprocessor where it receives events straight from the source. And I get the same erroneous behaviour I show next on the second scenario .

Regarding the second scenario I was able to reproduce it.
1) Create Template with the following definition
curl -XPUT http://localhost:9200/_template/template_def
{
  "template": "*",
  "settings": {
    "index.refresh_interval": "60s",
    "index.number_of_replicas": "0",
    "index.number_of_shards": "3"
  },
  "mappings": {
    "_default_": {
      "_source": {
        "compress": true,
        "enabled": true
      },
      "dynamic_templates": [
        {
          "string_fields": {
            "mapping": {
              "index": "not_analyzed",
              "omit_norms": true,
              "norms": {
                "enabled": false
              },
              "type": "string"
            },
            "match_mapping_type": "string",
            "match": "*"
          }
        }
      ],
      "numeric_detection": false,
      "date_detection": false,
      "_ttl": {
        "enabled": true,
        "default": "25w"
      },
      "properties": {
        "@version": {
          "index": "not_analyzed",
          "norms": {
            "enabled": false
          },
          "type": "string"
        }
      },
      "_all": {
        "enabled": false
      }
    },
    “firsttype”: {
      "properties": {
        “col1”: {
          "norms": {
            "enabled": false
          },
          "type": "double"
        }
      }
    },
    “secondtype: {
      "properties": {
        “col2”: {
          "norms": {
            "enabled": false
          },
          "type": "double"
        }
      }
    }
  }
}

2) Then insert data on both types

curl http://localhost:9200/scenario/firsttype/ -d '{"colA":"random column", "col1":"123"}'

curl http://localhost:9200/scenario/secondtype/ -d '{"colB":"other random column","colC":"another", "col2":"456"}'

Everything at this point is fine, both col1 and col2 are of type double in index metadata.

3) Then update the template:
Add a new column "col3" just like col2 on secondtype

4)
curl http://localhost:9200/scenario/secondtype/ -d '{"colB":"buggy type","colC":"just check col3 type", "col2":"789", "col3":"101"}'

col3 will be of type String instead of double. 
</comment><comment author="clintongormley" created="2014-10-17T05:20:24Z" id="59467580">@farracha Templates are only taken into account at index creation time.  If you want to add `col3` to an existing mapping, then you need to use the update-mapping API instead.  Updating the template at this point won't help.
</comment><comment author="farracha" created="2014-10-17T08:28:46Z" id="59482663">Thanks. I didn't know that.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support binding on multiple host/port pairs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8098</link><project id="" key="" /><description>This PR allows to bind to more than host and port using the `bind_host` and `port` properties. You can basically create differrent profiles, which can also have different tcp and buffer settings.

TODO: Documentation, explain possible options
</description><key id="45881872">8098</key><summary>Support binding on multiple host/port pairs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-15T15:29:12Z</created><updated>2015-09-22T11:30:49Z</updated><resolved>2014-10-17T21:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-10-16T17:41:43Z" id="59400462">incorporated all your review comments, except the one I didnt understand, maybe have another view..

docs still need to be added
</comment><comment author="kimchy" created="2014-10-17T18:39:06Z" id="59556583">LGTM
</comment><comment author="edagarli" created="2015-09-22T11:13:43Z" id="142254610">which version?
</comment><comment author="spinscale" created="2015-09-22T11:30:49Z" id="142259976">@edagarli the issue labels at the top show versions associated with this PR
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/concurrent/EsExecutors.java</file><file>src/main/java/org/elasticsearch/transport/netty/NettyTransport.java</file><file>src/test/java/org/elasticsearch/test/transport/NettyTransportMultiPortIntegrationTests.java</file><file>src/test/java/org/elasticsearch/test/transport/NettyTransportMultiPortTests.java</file><file>src/test/java/org/elasticsearch/test/transport/NettyTransportTests.java</file></files><comments><comment>Netty: Support to bind on multiple host/port pairs</comment></comments></commit></commits></item><item><title>Throw exceptions with invalid boolean settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8097</link><project id="" key="" /><description>If you do something like specify a value of True for node.master/node.data, it will silently set these values to false. Logs indicate a value of 'True', however this is actually false because it's not literally 'true'.

In the logs I see:

```
[elastic1][yydqFncaS1qVaASPrOtMlg][brn-elastic1][inet[/10.211.3.101:9300]]{data=True, master=True}, local
```

But the node never becomes either data or master.
</description><key id="45880197">8097</key><summary>Throw exceptions with invalid boolean settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-10-15T15:16:24Z</created><updated>2014-10-27T02:29:26Z</updated><resolved>2014-10-27T02:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nirmalc" created="2014-10-22T02:09:39Z" id="60027194">SHA: 507b4df1404274c7fe528378a0e15eb833932b5d
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>src/main/java/org/elasticsearch/common/Booleans.java</file><file>src/test/java/org/elasticsearch/common/BooleansTests.java</file></files><comments><comment>Core: Validates bool values in yaml for node settings</comment></comments></commit></commits></item><item><title>Completion suggester ignores filters set on index alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8096</link><project id="" key="" /><description>To reproduce:
1) Create an index
2) Create an alias with a filter on the index that restricts the set of document returned
3) Use the completion suggester on a field of your choosing targeting the alias created
4) Notice that results are returned that are filtered out on a normal search query on the alias

I think this problem may generalize to all suggesters, not just the completion suggester.
</description><key id="45875729">8096</key><summary>Completion suggester ignores filters set on index alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">almer-fuel</reporter><labels /><created>2014-10-15T14:45:36Z</created><updated>2014-12-11T20:39:03Z</updated><resolved>2014-12-11T20:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-15T18:47:07Z" id="59255519">the suggester don't support filters at this point.
</comment><comment author="almer-fuel" created="2014-10-16T12:00:06Z" id="59350628">@s1monw Is this planned at some point? 
</comment><comment author="s1monw" created="2014-10-16T13:43:44Z" id="59362668">yes but it's still in dev phase we have a branch for this @areek should be able to give some more info here
</comment><comment author="areek" created="2014-12-11T20:39:03Z" id="66684644">closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8909
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>New feature - add lookup by Lucene doc ID to ShardTermVectorService.getTermVector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8095</link><project id="" key="" /><description>As part of breaking up https://github.com/elasticsearch/elasticsearch/pull/6796 into more digestible pieces we need a service to fetch the tokens in a field.
The existing ShardTermVectorService.getTermVector(..) handles much of the complexity of reading stored TermVectors or using Analyzers on source fields to provide terms but is predicated on the caller providing an es ID (primary key) to identify the required document. In the context of analyzing query results, the caller has a collection of Lucene document IDs and not primary keys and so it would be useful/faster if the ShardTermvectorService could cope with looking up TermVectors given a Lucene document ID.
</description><key id="45864180">8095</key><summary>New feature - add lookup by Lucene doc ID to ShardTermVectorService.getTermVector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Term Vectors</label><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2014-10-15T13:14:32Z</created><updated>2016-03-03T19:20:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T19:49:55Z" id="158677311">@markharwood is this still something you're interested in adding?
</comment><comment author="markharwood" created="2015-11-22T19:58:15Z" id="158793712">I think this could still be useful for any free-text analysis of search results e.g significant_terms.
Not a high priority for me currently but certainly a longer-term nice-to-have
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Give a weight for documents in aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8094</link><project id="" key="" /><description>Hi all,

I'm wondering something about aggregations, say "Percentiles" (although it could be fine to get it with other aggregations).
When a percentile aggregation is processed, it uses a specific field as a reference. If the 50th percentile for field 'f' is 10, it means there are 50% of documents with 'f' under 10. 
=&gt; Each document has the same weight in the aggregation ( =&gt; 1)

I'm wondering if it could be possible to give a different weight for each document using another field in the document.
The following Gist give an example of what I'd like to do : https://gist.github.com/rnonnon/093c111014bd14a46efe

I'd like to compute some percentiles on the "age" field. But for each document, there is a "count" field associated.
For example, there are 5 persons who are 10 years old ; 1 who is 20 years old... 
If the percentile agg runs, it won't use my factor(number of person) to compute the percentile, it will count the number of documents...
I don't think that feature is natively supported, but, do you guess it could be easily supported? Do you think it makes sense to implement that? 

Why am I asking this?
I'm using percentile (and other) aggregation over around 70 000 000 documents and I use only 1 node. ES uses my 8 cores at 100% for a while :s... Then I try to reduce the number of documents by grouping them, but I can't use aggregation in the same way...

Thanks.
</description><key id="45863150">8094</key><summary>Give a weight for documents in aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rnonnon</reporter><labels><label>discuss</label></labels><created>2014-10-15T13:02:40Z</created><updated>2015-11-21T19:48:45Z</updated><resolved>2015-11-21T19:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-20T10:53:34Z" id="75220496">Your suggestion is achievable with a script that would create an array that contains `count` occurrences of the `age` value but it would not make things faster I'm afraid.

The algorithm that we use for percentiles (t-digest) is not so fast because it tries to work on all kinds of data.

We have another issue open in order to add support to HdrHistogram: https://github.com/elasticsearch/elasticsearch/issues/8324. It is faster but has relative accuracy: percentiles would be more accurate when values are close to 0 and vice-versa. This typically works very well when working with eg. response times (since you care about microsecond precision for millisecond response times, but usually only about second precision for hour response times). Would it work in your case too?
</comment><comment author="clintongormley" created="2015-11-21T19:48:45Z" id="158677251">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8093</link><project id="" key="" /><description /><key id="45862020">8093</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">XerMajor</reporter><labels /><created>2014-10-15T12:49:47Z</created><updated>2014-10-16T17:20:34Z</updated><resolved>2014-10-16T17:20:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T17:20:33Z" id="59397497">Hi @XerMajor 

Thanks for the PR. I must say, I've never seen that syntax before and after googling for it, it seems to be SQL Server specific?  I'd rather keep things generic here, to represent the idea without necessarily being 100% correct.

I think I'm going to close this PR, but thanks for submitting it anyway. Hopefully there will be more to come.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor RecoveryTarget state management</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8092</link><project id="" key="" /><description>The PR rewrites the state controls in the RecoveryTarget family classes to make it easier to guarantee that:
- recovery resources are only cleared once there are no ongoing requests
- recovery is automatically canceled when the target shard is closed/removed
- canceled recoveries do not leave temp files behind when canceled. 

Highlights of the change:
1) All temporary files are cleared upon failure/cancel (see #7315 )
2) All newly created files are always temporary 
3) Doesn't list local files on the cluster state update thread (which throw unwanted exception)
4) Recoveries are canceled by a listener to IndicesLifecycle.beforeIndexShardClosed, so we don't need to explicitly call it.
5) Simplifies RecoveryListener to only notify when a recovery is done or failed. Removed subtleties like ignore and retry (they are dealt with internally)

Relates to #7893
</description><key id="45859909">8092</key><summary>Refactor RecoveryTarget state management</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-15T12:23:25Z</created><updated>2015-06-07T17:05:08Z</updated><resolved>2014-10-23T13:13:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-16T09:10:57Z" id="59334172">I left a bunch of comments - I really like that change btw :) :+1: 
</comment><comment author="bleskes" created="2014-10-19T18:20:51Z" id="59659122">@s1monw I pushed some commits to address your comments. Can we do another round?
</comment><comment author="s1monw" created="2014-10-21T19:09:10Z" id="59981115">LGTM
</comment><comment author="bleskes" created="2014-10-23T07:50:24Z" id="60203774">@s1monw I pushed another commit based on the latest feedback. I'm giving CI an hour or two to chew this and will then merge.
</comment><comment author="bleskes" created="2014-10-23T13:15:02Z" id="60236306">merged. Thx @s1monw !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file></files><comments><comment>Recovery: change check for finished to a ref count check</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveriesCollection.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryFailedException.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/test/java/org/elasticsearch/recovery/RelocationTests.java</file></files><comments><comment>Recovery: refactor RecoveryTarget state management</comment></comments></commit></commits></item><item><title>fix typo in docs/reference/modules/cluster.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8091</link><project id="" key="" /><description /><key id="45856741">8091</key><summary>fix typo in docs/reference/modules/cluster.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kamaradclimber</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-15T11:41:31Z</created><updated>2014-10-29T14:00:04Z</updated><resolved>2014-10-29T14:00:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T17:09:12Z" id="59395965">Hi @kamaradclimber 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="kamaradclimber" created="2014-10-17T06:25:06Z" id="59471280">Hello @clintongormley,

it will take some time, I have to ask my employer (even for such a trivial contribution)
will come back to you
</comment><comment author="clintongormley" created="2014-10-29T13:51:48Z" id="60926574">CLA not signed - treating as bug report.
</comment><comment author="clintongormley" created="2014-10-29T14:00:04Z" id="60927849">Already fixed by #8111
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion Suggester: Fix CompletionFieldMapper to correctly parse weight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8090</link><project id="" key="" /><description>Followup for #3977:

if #3977 a check was added, that indexing completion suggester fields are rejected, if the weight is not an integer. My problem is that this check introduced there only works if the weight is a number. Unfortunately the parser has a bit strange logic: The new code is not executed if the client (creating the JSON) is passing the weight as "string", e.g. { "weight" : "10.5" }

In fact the weight is then ignored completely and not even an error is given (this is an additional bug in the parser logic). This caused me headaches yesterday, because the weight was given as JSON string in the indexing document. For other fields this makes no difference while indexing.

The parser for completion fields should be improved to have the outer check on the JSON key first and later check the types, not vice versa. This would also be consistent with indexing other fields, where the type of JSON value does not matter.
</description><key id="45852330">8090</key><summary>Completion Suggester: Fix CompletionFieldMapper to correctly parse weight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">uschindler</reporter><labels><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-15T10:41:13Z</created><updated>2014-10-29T08:42:36Z</updated><resolved>2014-10-28T22:50:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-29T08:42:36Z" id="60889486">@areek FYI I cherry-picked this commit into `1.4` as well I thing you missed it :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file></files><comments><comment>Completion Suggester: Fix CompletionFieldMapper to correctly parse weight</comment><comment> - Allows weight to be defined as a string representation of a positive integer</comment></comments></commit></commits></item><item><title>Completion Suggester Highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8089</link><project id="" key="" /><description>## Highlighting

Many search-as-you-type implementations highlight the portion of the term which has already been entered. We plan to add this to the response as we well.
[You complete me (August 22, 2013)](http://www.elasticsearch.org/blog/you-complete-me/)

Are there any plans when these feature will be implemented?
</description><key id="45841945">8089</key><summary>Completion Suggester Highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">FabianKoestring</reporter><labels /><created>2014-10-15T08:36:38Z</created><updated>2015-11-21T19:47:40Z</updated><resolved>2015-11-21T19:47:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-15T08:37:41Z" id="59174195">@mikemccand @areek can you comment :)
</comment><comment author="vgross" created="2014-12-08T04:07:56Z" id="65973970">+1 
</comment><comment author="telendt" created="2014-12-08T13:54:51Z" id="66118266">+1
</comment><comment author="mikemccand" created="2014-12-08T15:26:40Z" id="66131187">I agree this is an important feature but it is unfortunately surprisingly challenging for FST-based suggesters.

We explored it a bit in https://issues.apache.org/jira/browse/LUCENE-4518 but the initial results were poor.  The challenge is in how the FST "eagerly" matches outputs to inputs, at least in that initial patch.

Other suggesters e.g. the infix suggester (AnalyzingInfixSuggester) highlight very well since it's just a Lucene index under-the-hood.
</comment><comment author="abrahamduran" created="2015-09-02T12:50:09Z" id="137063271">+1
</comment><comment author="clintongormley" created="2015-11-21T19:47:40Z" id="158677201">Looks unlikely that we'll be able to implement this.  If it is added to Lucene (see https://issues.apache.org/jira/browse/LUCENE-4518) then we'll get access to it.  No need to keep this issue open.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Error while using plugins in Elasticsearch 1.4.0beta</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8088</link><project id="" key="" /><description>I upgraded my ElasticSearch from 1.3.1 to 1.4.0beta without taking the backup of config files. Now to get the index replic from other server, I need to use cloud-aws plugin. After installing cloud-aws plugin and restarting es, the status says ElasticSearch is not running. Even though, curl localhost:9200 gives 

{
  "status" : 200,
  "name" : "Mammomax",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.4.0.Beta1",
    "build_hash" : "1f25669f3299b0680b266c3acaece43774fb59ae",
    "build_timestamp" : "2014-10-01T14:58:15Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.1"
  },
  "tagline" : "You Know, for Search"
}

Same case with head plugin! 
</description><key id="45841806">8088</key><summary>Error while using plugins in Elasticsearch 1.4.0beta</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geetanjaligg</reporter><labels /><created>2014-10-15T08:34:54Z</created><updated>2014-10-15T14:13:38Z</updated><resolved>2014-10-15T08:35:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-15T08:35:58Z" id="59173992">AWS plugin needs to be upgraded. See https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/117
</comment><comment author="geetanjaligg" created="2014-10-15T14:11:09Z" id="59210901">/usr/share/elasticsearch/bin/plugin --install cloud-aws --url file:target/releases/elasticsearch-cloud-aws-2.4.0-SNAPSHOT.zip
-&gt; Installing cloud-aws...
Trying file:target/releases/elasticsearch-cloud-aws-2.4.0-SNAPSHOT.zip...
Failed: FileNotFoundException[target/releases/elasticsearch-cloud-aws-2.4.0-SNAPSHOT.zip (No such file or directory)]
Trying https://github.com/null/cloud-aws/archive/master.zip...
Failed to install cloud-aws, reason: failed to download out of all possible locations..., use --verbose to get detailed information
</comment><comment author="dadoonet" created="2014-10-15T14:13:38Z" id="59211285">Indeed. We did not fix yet the issue I mentioned.
The 2.4.0 has not been released yet as written in docs here: https://github.com/elasticsearch/elasticsearch-cloud-aws
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Only schedule another refresh if `refresh_interval` is positive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8087</link><project id="" key="" /><description>If a EngingRefresher.run was already running when the refresh_interval is
dynamically updated down to a non-positive value (0, -1, etc.), then
it's was possible for the refresh thread to go into while (true)
refresh() loop.

Closes #8085
</description><key id="45841727">8087</key><summary>Only schedule another refresh if `refresh_interval` is positive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-15T08:33:43Z</created><updated>2015-06-07T18:20:35Z</updated><resolved>2014-10-15T08:48:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-15T08:35:09Z" id="59173904">LGTM
</comment><comment author="s1monw" created="2014-10-15T08:35:28Z" id="59173934">@mikemccand maybe put a comment on the future.cancle why we don't pass true :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Core: don't spin 100% CPU when disabling refresh_interval</comment></comments></commit></commits></item><item><title>Don't handle FNF exceptions when reading snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8086</link><project id="" key="" /><description>We used to handle FNF exceptions in the store when reading a snapshot.
For instance if we can't open a segments file for a given commit point
we just return an empty metadata object and tracelog the even. This can
cause shards to be false marked as corrupted if a shard is forcefully
removed while a recovery started at the same time. We should in general
bubble up these exceptions and let the caller decided how to handle the
IOExceptions.
</description><key id="45840298">8086</key><summary>Don't handle FNF exceptions when reading snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-15T08:14:17Z</created><updated>2015-06-07T18:05:00Z</updated><resolved>2014-10-22T12:07:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-15T12:06:51Z" id="59195439">The change looks good, but I'm worried about the implication it has for recovery as we use the same api to list the files already available on the local directory. I'm not sure if FNF can be thrown in that cause, but I think we should be safe. 

I'm working on a change to the recovery code that will give us better protection in that case, after which we can pull this in? (PR Coming)
</comment><comment author="bleskes" created="2014-10-15T12:24:14Z" id="59197083">Here is the PR mentioned: https://github.com/elasticsearch/elasticsearch/pull/8092
</comment><comment author="bleskes" created="2014-10-22T09:48:34Z" id="60060591">&gt;  I'm not sure if FNF can be thrown in that cause, but I think we should be safe.

It turns out this is not a concern because when listing local files on a recovery target we do not supply a commit point but rather pass a null to `Store.readSegmentsInfo(commit, directory)`. In this case it try to find a commit point but not throw a FNF if none was found.

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Core: changing refresh_interval to non-positive (0, -1, etc.) value might cause 100% CPU spin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8085</link><project id="" key="" /><description>As reported on the user's list:

```
https://groups.google.com/d/msg/elasticsearch/IQWvod8hq_Q/H6358j_24B0J
```

It looks like there is a concurrency bug when you dynamically update refresh_interval down to a value &lt;= 0.  We cancel the scheduled future when this happens, but if the future was already executing (which we don't try to cancel because we pass false to the cancel call), EngineRefresher.run will then forever continue rescheduling itself for the immediate future.
</description><key id="45840014">8085</key><summary>Core: changing refresh_interval to non-positive (0, -1, etc.) value might cause 100% CPU spin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-15T08:10:10Z</created><updated>2014-10-15T08:48:08Z</updated><resolved>2014-10-15T08:48:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Core: don't spin 100% CPU when disabling refresh_interval</comment></comments></commit></commits></item><item><title>SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[MHxs9PqFTq6ctF_g15Tneg][guba][2]: </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8084</link><project id="" key="" /><description>1、GET guba/_search

{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 3,
      "successful": 2,
      "failed": 0
   },
   "hits": {
      "total": 135592,
      "max_score": 1,
      "hits": [
         {
            "_index": "guba",
            "_type": "post",
            "_id": "125570200",
            "_score": 1,
            "_source": {
               "stockholder": false,
               "stock_id": "600010",
               "user_url": "http://iguba.eastmoney.com/9926113891298328",
               "post_id": 125570200,
               "url": "http://guba.eastmoney.com/news,600010,125570200,d.html",
               "lastReplyTime": null,
               "content": "买入 738381 买入价格99.00 买入股数 2",
               "user_name": "赚够一千万123",
               "title": "应",
               "em_info": null,
               "_id": 125570200,
               "replies": 0,
               "clicks": 342,
               "first_in": 1413097216.340654,
               "user_id": 9926113891298328,
               "releaseTime": "2014-10-12 11:14:00",
               "last_modify": 1413097216.340654,
               "stock_name": "包钢股份"
            }
         }
    ]
}

2、
GET guba/_search
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {}
    }
  },
  "aggs": {
    "range": {
      "date_range": {
        "field": "releaseTime",
        "format": "yyyy-MM-dd H:M:s",
        "ranges": [
          {
            "to": "2014-10-14 10:30:00"
          },
          {
            "from": "2014-10-14 10:00:00"
          }
        ]
      }
    }
  }
}

{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[MHxs9PqFTq6ctF_g15Tneg][guba][2]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}{[MHxs9PqFTq6ctF_g15Tneg][guba][0]: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData]}]",
   "status": 500
}
</description><key id="45823060">8084</key><summary>SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[MHxs9PqFTq6ctF_g15Tneg][guba][2]: </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">linhaobuaa</reporter><labels /><created>2014-10-15T02:23:37Z</created><updated>2016-05-30T13:16:32Z</updated><resolved>2014-10-16T17:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T17:05:38Z" id="59395474">Hi @linhaobuaa 

It looks like you have two fields with the same name `date_range` in two types in the same index, which have different mappings: one is a date, and one is a string.  This will not be allowed in the future (see #4081) for exactly the reasons that you reported.

Please reopen if the issue is not as I have described it.
</comment><comment author="keertirastogi" created="2015-01-30T10:35:47Z" id="72182479">Hi,

Am new to elastic search and getting same error while creating a complex filter query. It would be great if somebody could look into this and help.

1) GET /devices/app_89/_search?pretty=true
{
  "took" : 313,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "devices",
      "_type" : "app_89",
      "_id" : "dev_5794712",
      "_score" : 1.0, "_source" : {"id":5794712,"app_id":89,"platform":{"id":2,"name":"iOS"},"UDID":null,"token":"APA91bEQ2XCyCwZxq3MU_IgQ7gNqpDyzjxqqPqHgaRk-8_jqUzOYweBMwIRW8L04xsuHS2oaMjr313oCW6Tgd61uSDHZavCqJ4W4GuFVFt2xnCxdqoFc1qTvhuGW1R9JoRNrVwKG0PhvsHMeJhJsUDnvWAs5A","disabled":0,"active":1,"push_disabled":0,"time_zone":2,"username":null,"dev_name":"GT-I9300","language":"English","country":"Spain","model":"GT-I9300","version":"Android 4.3(API level: 18)","sdk_version":null,"bundle_version":null,"badge":0,"created":"2015-01-29T14:19:43.000Z","sandbox":0,"disabled_date":null,"locations":{"last":{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},"work":{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},"home":{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},"list":[{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"}]},"pushes":{"total_push":8,"openedCount":5,"last_push":null,"sent_list":[2352345,234,234234,2341,546,4534,464,657],"open_list":[2352345,234,234234,2341,546]},"sessions":{"morning_opens":1,"afternoon_opens":1,"evening_opens":3,"last_open":"2015-01-20T14:19:43.000Z","time_in_app":87,"total_sessions":2,"list":[{"datetime":"2015-01-20T14:19:43.000Z","length":256,"pushID":546},{"datetime":"2015-01-20T14:19:43.000Z","length":256,"pushID":546}]}}
    }, {
      "_index" : "devices",
      "_type" : "app_89",
      "_id" : "dev_5794711",
      "_score" : 1.0, "_source" : {"id":5794711,"app_id":89,"platform":{"id":1,"name":"android"},"UDID":null,"token":"APA91bEQ2XCyCwZxq3MU_IgQ7gNqpDyzjxqqPqHHmqJngaRk-8_jqUzOYweBMwIRW8L04xsuHS2oaMjr313oCW6Tgd61uSDHZavCqJ4W4GuFVFt2xnCxdqoFc1qTvhuGW1R9JoRNrVwKG0PhvsHMeJhJsUDnvWAs5A","disabled":0,"active":0,"push_disabled":0,"time_zone":2,"username":null,"dev_name":"GT-I9300","language":"Spanish","country":"Spain","model":"GT-I9300","version":"Android 4.3(API level: 18)","sdk_version":null,"bundle_version":null,"badge":0,"created":"2015-01-20T14:19:43.000Z","sandbox":0,"disabled_date":null,"locations":{"last":{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},"work":{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},"home":{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},"list":[{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"},{"gpt":{"lat":12.345346,"lon":12.345346},"dt":"2015-01-20T14:19:43.000Z"}]},"pushes":{"total_push":8,"openedCount":5,"last_push":null,"sent_list":[2352345,234,234234,2341,546,4534,464,657],"open_list":[2352345,234,234234,2341,546]},"sessions":{"morning_opens":1,"afternoon_opens":1,"evening_opens":3,"last_open":"2015-01-20T14:19:43.000Z","time_in_app":87,"total_sessions":2,"list":[{"datetime":"2015-01-20T14:19:43.000Z","length":256,"pushID":546},{"datetime":"2015-01-20T14:19:43.000Z","length":256,"pushID":546}]}}
    } ]
  }
}

Now my query url is :  /devices/app_89/_search?pretty=true&amp;search_type=count
And JSON query array is : 
{
                  "aggs": {
                    "device_counts": {
                      "query" : {
                        "filtered" : { 
                           "filter" : {
                              "bool" : {  
                                "must_not" : [
                                   { "term" : {"token" : null }}
                                ],
                                "must": [
                                    {"term" : {"push_disabled" : "0" }},
                                    {"term" : {"disabled" : "0" }},
                                    {"term" : {"app_id" : &lt;any_value&gt; }}
                                ],
                                "should" : [
                                  {"term" : {"sandbox" : &lt;any_value&gt;}}, 
                                  {"bool" : { 
                                    "must" : [
                                      {"term" : {"platform_id" : 2}},
                                      {"term" : {"sandbox" : 0}}
                                    ]
                                  }}
                                ]
                             }
                           }
                        }
                      },
                      "aggs": {
                        "platform_counts": {
                          "terms": {"field":"platform.name"}
                        }
                      }
                   }
                 }
               }

Basically am trying to get document counts based on all the platforms found, including filters like - ( TOKEN!=null AND DISABLED=0 AND PUSH_DISABLED=0 AND APP_ID="&lt;any_value&gt;" AND (SANDBOX=&lt;any_value&gt; OR (PLATFORM_ID=2 AND SANDBOX=0)) ) 

Without including all the query filters, I am able to get the document counts correctly by this query -
{
                    "aggs": {
                      "device_counts": {
                        "filter": {
                          "bool": {
                            "must": [
                              {"term":{"active":"1"}},
                              {"term":{"disabled":"0"}}
                            ]
                          }
                        },
                        "aggs": {
                          "platform_counts": {
                            "terms": {"field":"platform.name"}
                          }
                        }
                      }
                   }
 }

Thanks in advance !
</comment><comment author="keertirastogi" created="2015-01-31T05:48:49Z" id="72305983">Got that fixed yesterday, by just replacing null with this - '', in must_not filter. Like this -
{
.
.
.
"must_not" : [
{ "term" : {"token" : '' }}
],  
.
.
.
}
</comment><comment author="axper" created="2015-05-21T09:29:08Z" id="104195597">For me this issue was caused by not having enough disk space.
So if anyone is having this issue, check that the partition elastic is running on has enough free space and also check that you have enough RAM.
</comment><comment author="rohitkothari" created="2015-07-06T22:05:13Z" id="119009763">@axper I also faced the same problem and it was caused by not having enough disk space. I have free'd enough disk space, but don't know how to change the health status from 'red' to 'green'. Do you have any idea?
</comment><comment author="IvRRimum" created="2016-05-30T13:16:32Z" id="222490629">@rohitkothari You remember what was the problem ? I am stuck on here right now too ( using aws )
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix order for put mapping doc v2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8083</link><project id="" key="" /><description>@clintongormley please review this new cherry-picked PR.
</description><key id="45792557">8083</key><summary>Fix order for put mapping doc v2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phungleson</reporter><labels /><created>2014-10-14T19:53:20Z</created><updated>2014-10-16T22:45:15Z</updated><resolved>2014-10-16T16:55:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T16:55:14Z" id="59393899">Thanks for the work @phungleson - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fix order for PUT _mapping docs</comment></comments></commit></commits></item><item><title>Histogram bucketing negative values incorrectly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8082</link><project id="" key="" /><description>I've been trying to make sense of Elasticssearch histogram aggregations the last couple of days. And I've found that they don't work as expected, or even advertised.

Lets say i want to aggregate like so:

```
"aggregations": {
  "sea_water_temperature": {
    "histogram": {
      "field": "sea_water_temperature",
      "interval": 3
    }
  }
}
```

Response buckets looks fine at first glance, but when trying to query for documents within the bounds of a bucket I don't get the same document count as the bucket suggested. E.g.

```
"filter": {
  "range": {
    "sea_water_temperature": {
      "lt": 0,
      "gte": -3
    }
  }
}
```

This could give x results while the bucket "-3" had a doc_count of y. This seems to only be an issue for negative bucket keys.

In the docs for histogram it states that the bucket key for a given value is:

```
rem = value % interval
if (rem &lt; 0) {
  rem += interval
}
bucket_key = value - rem
```

However I tried a term aggregation with that as a value script:

```
"aggregations": {
  "sea_water_temperature": {
    "terms": {
      "field": "sea_water_temperature",
      "script": "rem = _value % interval; rem = rem &lt; 0 ? rem + interval : rem; _value - rem",
      "params": {
        "interval": 3
      }
    }
  }
}
```

That gives me the same kind of bucketing as histogram does but now my filter queries actually match the doc_counts of the buckets(!). Why isn't histogram working as described? or am I missing something?
</description><key id="45766348">8082</key><summary>Histogram bucketing negative values incorrectly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">baelter</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label></labels><created>2014-10-14T15:48:21Z</created><updated>2016-08-03T06:43:36Z</updated><resolved>2016-08-03T06:43:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T14:43:05Z" id="59372121">I can replicate this:

```
DELETE /_all 

POST /t/t/_bulk
{ "create": {}}
{  "t": -3.9}
{ "create": {}}
{  "t": -2.6}
{ "create": {}}
{  "t": -1.4}
{ "create": {}}
{  "t": -0.2}
{ "create": {}}
{  "t": 0.2}
{ "create": {}}
{  "t": 2.4}
{ "create": {}}
{  "t": 2.8}
{ "create": {}}
{  "t": 3.9}

GET /_search?search_type=count
{
  "aggs": {
    "NAME": {
      "histogram": {
        "field": "t",
        "interval": 3
      }
    }
  }
}
```

Returns:

```
"aggregations": {
  "NAME": {
     "buckets": [
        {
           "key": -3,
           "doc_count": 3
        },
        {
           "key": 0,
           "doc_count": 4
        },
        {
           "key": 3,
           "doc_count": 1
        }
     ]
  }
}
```
</comment><comment author="baelter" created="2014-10-20T09:47:55Z" id="59716505">Can I contribute in any way?
</comment><comment author="clintongormley" created="2014-10-20T10:11:59Z" id="59721371">@baelter The problem here is that the histogram casts floats to integers, which is why some negative numbers are being added to the wrong bucket.  @jpountz has a solution here, he just needs to implement it.
</comment><comment author="jpountz" created="2014-10-20T10:51:43Z" id="59726308">It is a general limitation of histograms today that they do neither work on decimal values (because of negative values, as you noticed) nor on decimal intervals (intervals are required to be an integer). I agree that it would be nice to fix both issues.
</comment><comment author="baelter" created="2014-10-20T11:56:04Z" id="59735869">It would, in the mean time, a term aggregation with histogram key function as a script will do as a temp fix.
</comment><comment author="jpountz" created="2014-10-20T11:56:39Z" id="59735917">For reference here is the related (though not duplicate) issue https://github.com/elasticsearch/elasticsearch/issues/4847
</comment><comment author="sylvinus" created="2015-08-01T17:21:48Z" id="126938564">I would strongly recommend mentioning this in the docs until it is fixed. It's a subtly broken behavior and it would prevent more users losing time (or their minds! :)
</comment><comment author="clintongormley" created="2015-08-05T11:28:54Z" id="127962374">@sylvinus agreed - feel like sending a docs PR?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/Histogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBoundsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java</file><file>core/src/test/java/org/elasticsearch/search/profile/aggregation/AggregationProfilerIT.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java</file></files><comments><comment>Split regular histograms from date histograms. #19551</comment></comments></commit></commits></item><item><title>Prevent too many agg buckets from causing OOM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8081</link><project id="" key="" /><description>Once of the last places we see OOM conditions is in aggregations, when combinatorial explosion results in too many buckets being assigned.

We need a way to catch these conditions and abort the request rather than running into OOM.
</description><key id="45751812">8081</key><summary>Prevent too many agg buckets from causing OOM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label><label>resiliency</label></labels><created>2014-10-14T13:48:33Z</created><updated>2016-09-20T09:48:15Z</updated><resolved>2016-09-20T09:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Fizzadar" created="2015-06-24T13:14:59Z" id="114863843">We're not capturing OOM exceptions with a filters aggregations, would this count as a combinatorial aggregation that this issue would fix?
</comment><comment author="clintongormley" created="2015-11-21T19:45:36Z" id="158677108">Related to https://github.com/elastic/elasticsearch/issues/9825
</comment><comment author="blavoie" created="2016-03-04T17:31:44Z" id="192371924">+1
</comment><comment author="dakrone" created="2016-09-19T15:10:04Z" id="248021194">@clintongormley do you think this can be considered fixed by https://github.com/elastic/elasticsearch/pull/19394 ?
</comment><comment author="clintongormley" created="2016-09-20T09:48:15Z" id="248255061">@dakrone yes!  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Mark combinatorial explosion in aggs as 'done'</comment></comments></commit></commits></item><item><title>Include the from/size priority queues in the circuit breakers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8080</link><project id="" key="" /><description>Users often specify enormous (eg `MAXINT`) `size` or `from` parameters in search requests, which can result in OOM.  The priority queue sizes should be taken into account by the circuit breakers.
</description><key id="45751017">8080</key><summary>Include the from/size priority queues in the circuit breakers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>enhancement</label><label>resiliency</label></labels><created>2014-10-14T13:41:14Z</created><updated>2015-02-03T14:23:39Z</updated><resolved>2015-02-03T14:23:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-03T14:23:39Z" id="72658125">Closing in favour of #9311
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change max index name length to 255 bytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8079</link><project id="" key="" /><description>As per discussion in #7252, the current max index name length of 100 is too short for some users.

Worth setting it to a safe (on all? most? file systems) max of 255.
</description><key id="45745289">8079</key><summary>Change max index name length to 255 bytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Index APIs</label><label>breaking</label><label>enhancement</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-14T12:42:59Z</created><updated>2015-06-06T16:41:18Z</updated><resolved>2014-10-20T13:49:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-14T12:52:21Z" id="59037493">`WhatdoyoumeanIthinkthisiswayenoughforanindexnameotherwisesomethingiswrongwithyoursystemey?` &lt;== you have 10 chars left!!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file></files><comments><comment>Raise maximum index name length to 255 bytes</comment></comments></commit></commits></item><item><title>Still use of unsafe methods in 1.3.4 - causing crashes on SPARC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8078</link><project id="" key="" /><description>The changes for Issue 6962 is present in 1.3.2, but there are still uses of Unsafe methods in other classes, apart from UnsafeUtils.

jprante signalled one occurrence on the site below:
http://www.snip2code.com/Snippet/140415/Solaris-SPARC-JVM-64bit-crash-with-Java-

but I could not find a reference to it here at ElasticSearch.

These classes are involved.

UnsafeChunkDecoder.class
UnsafeChunkEncoder.class
UnsafeChunkEncoderBE.class
UnsafeChunkEncoderLE.class
UnsafeChunkEncoders.class
UnsafeDynamicChannelBuffer.class
</description><key id="45737013">8078</key><summary>Still use of unsafe methods in 1.3.4 - causing crashes on SPARC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">decuypeb</reporter><labels><label>bug</label><label>v1.3.5</label></labels><created>2014-10-14T10:59:17Z</created><updated>2014-10-16T13:39:14Z</updated><resolved>2014-10-16T13:39:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-14T23:30:24Z" id="59134891">This was fixed in #7468 but was not backported to 1.3.
</comment><comment author="s1monw" created="2014-10-15T07:46:08Z" id="59169175">@rjernst should we backport to 1.3 - it's a bugfix from a sparc user perspective?
</comment><comment author="clintongormley" created="2014-10-16T12:39:06Z" id="59354568">Agreed 
</comment><comment author="s1monw" created="2014-10-16T13:39:14Z" id="59361935">I backported this to `1.3.5`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix serialization of PendingClusterTask.timeInQueue.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8077</link><project id="" key="" /><description>This parameter is serialized as a vLong while it could sometimes be negative.
</description><key id="45733719">8077</key><summary>Fix serialization of PendingClusterTask.timeInQueue.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-14T10:17:20Z</created><updated>2015-06-07T18:20:45Z</updated><resolved>2014-10-14T11:18:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-14T10:21:33Z" id="59018965">LGTM
</comment><comment author="s1monw" created="2015-02-11T12:25:00Z" id="73873170">we just hit this in our BWC test here http://build-us-00.elasticsearch.org/job/es_bwc_1x/7637/CHECK_BRANCH=tags%2Fv1.1.2,jdk=JDK7,label=bwc/testReport/junit/org.elasticsearch.snapshots/SnapshotBackwardsCompatibilityTest/testSnapshotAndRestore/

maybe we should make sure the value to vlong is positive?
</comment><comment author="s1monw" created="2015-02-11T12:25:13Z" id="73873193">@jpountz ping
</comment><comment author="bleskes" created="2015-02-11T12:29:46Z" id="73873657">@s1monw we use -1 to signal stuff in the queue which are not updateTasks (as far as I can tell those are timeout notifications). See https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L306

I think this is what is broken. We should have a common queue item base class that gives as the time inserted + a source and only have these in queue (UpdateTask should inherit from it).
</comment><comment author="s1monw" created="2015-02-11T12:30:48Z" id="73873753">I just wanna fix the assertion here. We arlready have new code that transfers it correctly. WE can't fix this differently sorry.
</comment><comment author="s1monw" created="2015-02-11T12:31:16Z" id="73873816">^^ by that I mean for nodes &lt; 1.4.0
</comment><comment author="bleskes" created="2015-02-11T12:35:11Z" id="73874274">My suggestion implies we never write negative values. if we never have to write a negative value, I think the problem is solved for &lt;1.4 as well. 

&gt; maybe we should make sure the value to vlong is positive?

maybe you meant writing a 0 where we see -1 (which is a valid value now). This is also a workaround but is not the "right" thing. Just wanted to share the idea of a proper fix.
</comment><comment author="s1monw" created="2015-02-11T12:41:11Z" id="73874944">again the issue is fixed we use `writeLong` if you wanna communicate this differently that's all fine with me but it won't fix anything.
</comment><comment author="jpountz" created="2015-02-11T13:02:24Z" id="73877295">@s1monw are you suggesting we should fix it by removing the assertion?
</comment><comment author="s1monw" created="2015-02-11T14:54:43Z" id="73893590">&gt; @s1monw are you suggesting we should fix it by removing the assertion?

no I am suggesting to fix the BWC code to send `out.writeVLong(Math.max(0, timeInQueue));` since the serialization will be broken if you pass a neg value to `writeVLong` no?
</comment><comment author="jpountz" created="2015-02-11T15:51:24Z" id="73904278">Makes sense to me. +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/service/PendingClusterTask.java</file></files><comments><comment>Internal: Introduce TimedPrioritizedRunnable base class to all commands that go into InternalClusterService.updateTasksExecutor</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/service/PendingClusterTask.java</file></files><comments><comment>Internal: Fix serialization of PendingClusterTask.timeInQueue.</comment></comments></commit></commits></item><item><title>Internal: Make version parsing more lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8076</link><project id="" key="" /><description>Today we fall back to a beta/alpha version parsing if the first attemp
failed. Yet, if the second attemp failed we throw the exception rather
than returning the default value provided.

Note: this PR is only against `1.3` newer branches don't have this problem
</description><key id="45728427">8076</key><summary>Internal: Make version parsing more lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.3.5</label></labels><created>2014-10-14T09:16:13Z</created><updated>2014-11-03T15:24:46Z</updated><resolved>2014-10-14T13:22:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-14T11:48:43Z" id="59030084">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow partial update to replace json fields too, not always merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8075</link><project id="" key="" /><description>Using the _update (partial) update request, currently, modifying a field in an existing document will merge the json objects. It would be great if in a future version, we add a REST parameter to allow the caller to specify how he wants to handle subdocuments: "merge" or "replace" (with the current "merge" behaviour still as default)

This means that if you do this:

`curl -XPUT http://localhost:9200/upd/person/1 --data-binary '{name: "John", gender: "male", flags: {clever: true, length: 196}}'
{"_index":"upd","_type":"person","_id":"1","_version":1,"created":true}`

`curl -XPOST http://localhost:9200/upd/person/1/_update --data-binary '{doc:{ flags: {clever: false, addiction: "coffee"}}}'      
{"_index":"upd","_type":"person","_id":"1","_version":2}`                                                           

Then, if you get the doc: 

`curl -XGET http://localhost:9200/upd/person/1
{"_index":"upd","_type":"person","_id":"1","_version":2,"found":true,"_source":{"name":"John","gender":"male","flags":{"clever":false,"length":196,"addiction":"coffee"}}}`

The .length field is still there! This can be an action that the user wants, but if you do not want it (e.g. if your document is actually a merge of two docs from different sources and you want to simply replace a full subdocument), you have to use scripting, or download-and-merge yourself.

This is actually done in org.elasticsearch.action.update.UpdateHelper, which calls:

`boolean noop = !XContentHelper.update(updatedSourceAsMap, indexRequest.sourceAsMap(), request.detectNoop());`

and that code does a recursive merge for json objects always:

`if (old instanceof Map &amp;&amp; changesEntry.getValue() instanceof Map) {
                // recursive merge maps
                modified |= update((Map&lt;String, Object&gt;) source.get(changesEntry.getKey()),
                        (Map&lt;String, Object&gt;) changesEntry.getValue(), checkUpdatesAreUnequal &amp;&amp; !modified);
                continue;
            }`

For non-map fields, the values are simply replaced, even for arrays.

Propose to add a REST parameter called "update-mode" with default value "merge" (no change), but if you specify "replace", then all json subdocs are simply replaced.

And while we're at it, allowing that to influence array update modes too, would be handy?
</description><key id="45724732">8075</key><summary>Allow partial update to replace json fields too, not always merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anneveling</reporter><labels /><created>2014-10-14T08:33:00Z</created><updated>2014-10-16T13:42:07Z</updated><resolved>2014-10-16T12:36:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T12:36:54Z" id="59354333">@anneveling I think it makes sense to do this in a more generic way, as in #7030, rather than to add flags which govern the whole request, no?

Closing in favour of #7030.
</comment><comment author="anneveling" created="2014-10-16T13:42:07Z" id="59362444">agree, better :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES crashes with OutOfMemoryError every alternate day</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8074</link><project id="" key="" /><description>We are using ES 1.3.2 on an Amazon EC2 server.
Our configuration is - 2 nodes, 32 shards and 1 replica. We have allocated 2GB as max heap size in ES settings.
Attached is the status (doc counts and size) of our cluster from ES head plugin.
We index about 50K documents daily and execute queries and aggregations (nested too) on this data.

The issue is that every alternate day ES crashes with an OutOfMemoryError.

Also attached are the thread and heap dumps when the OutOfMemoryError occurred recently.
Could you please advise what is causing this?
![es_head](https://cloud.githubusercontent.com/assets/6479566/4625416/0300eeea-5376-11e4-9269-81a905ed8684.jpg)
</description><key id="45721017">8074</key><summary>ES crashes with OutOfMemoryError every alternate day</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">praj2</reporter><labels><label>feedback_needed</label></labels><created>2014-10-14T07:45:13Z</created><updated>2014-10-17T05:26:18Z</updated><resolved>2014-10-17T05:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="praj2" created="2014-10-14T07:46:36Z" id="59001970">How do I attach .rar / .txt files?
</comment><comment author="praj2" created="2014-10-14T07:58:03Z" id="59003147">thread dump: https://gist.github.com/praj2/6603ed38eddee8157708 
heap dump: https://drive.google.com/file/d/0Bwji_gzAroRKWHFpTDZWNV9ZVjg/view?usp=sharing 
</comment><comment author="clintongormley" created="2014-10-14T13:47:05Z" id="59046396">Hi @praj2 

I'm guessing that your aggregations are too deeply nested for the tiny amount of heap space that you have.  You may want to try using ["breadth_first" mode](http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_preventing_combinatorial_explosions.html#_depth_first_vs_breadth_first) , or you may just have to add more memory.
</comment><comment author="praj2" created="2014-10-17T04:12:12Z" id="59464217">Thanks for your reply Clinton.
What would be the ideal heap size recommended for the size of data we have, given the need to frequently execute 3-level nested aggregations.
</comment><comment author="clintongormley" created="2014-10-17T05:26:18Z" id="59467897">@praj2 that depends on the types of aggregations you are running, and the cardinality of each field.  Imagine you had three nested `terms` aggregations, and each field had 1,000 unique values.  That's 1,000,000,000 buckets with the default execution mode.

Breadth-first would create much smaller numbers of buckets.  You'll just need to experiment to find the right approach.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>OOM when updating docs with groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8073</link><project id="" key="" /><description>To reproduce
- start elasticsearch (tested 1.3.4, 1.4.0Beta1) 
- run this script: https://gist.github.com/brwe/5ba123604c4cc0af9c3a

Results in:

```
...
[2014-10-14 09:26:28,859][DEBUG][index.engine.internal    ] [Chan Luichow] [testidx][3] updating index_buffer_size from [64mb] to [60.7mb]
[2014-10-14 09:26:28,860][DEBUG][index.engine.internal    ] [Chan Luichow] [testidx][4] updating index_buffer_size from [64mb] to [60.7mb]
java.lang.OutOfMemoryError: PermGen space
Dumping heap to java_pid3947.hprof ...
Heap dump file created [82885613 bytes in 1.006 secs]
```

This works fine with mvel.
</description><key id="45720177">8073</key><summary>OOM when updating docs with groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>critical</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-14T07:32:28Z</created><updated>2014-10-14T07:35:14Z</updated><resolved>2014-10-14T07:35:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-10-14T07:33:16Z" id="59000708">@brwe see #7658 and #8062
</comment><comment author="brwe" created="2014-10-14T07:35:14Z" id="59000903">@dakrone thanks, I did not see
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add params properly when using wildcard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8072</link><project id="" key="" /><description>fix #8071
</description><key id="45717685">8072</key><summary>add params properly when using wildcard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wy96f</reporter><labels><label>:REST</label><label>bug</label><label>review</label></labels><created>2014-10-14T06:51:13Z</created><updated>2016-12-08T14:03:09Z</updated><resolved>2016-03-08T18:58:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T11:59:52Z" id="59350610">Hi @wy96f 

Thanks for the PR.  While we're taking a look at it, can I ask you to sign the CLA please.
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="wy96f" created="2014-10-17T11:56:34Z" id="59502381">Hi @clintongormley 
I have just signed the CLA.
</comment><comment author="colings86" created="2014-10-17T13:02:12Z" id="59508964">@wy96f Thanks for the PR and for signing the CLA. I left a comment on the changes
</comment><comment author="wy96f" created="2014-10-18T11:05:23Z" id="59607705">Hi, colings86. Thanks for the review. Do you mean this?
</comment><comment author="colings86" created="2014-10-23T09:00:59Z" id="60210820">Not quite, I have left a comment in the code explaining what I mean. Let me know if it doesn't make sense
</comment><comment author="colings86" created="2015-01-28T11:17:34Z" id="71817899">@wy96f Sorry it's taken so long to reply on this. We are keen to get this merged in, would you mind [signing the Contributor Licence Agreement](http://www.elasticsearch.org/contributor-agreement/) so we can do a final review and merge
</comment><comment author="colings86" created="2015-01-28T11:24:07Z" id="71818668">@wy96f sorry, I just noticed your comment above saying that you had signed it already.
</comment><comment author="clintongormley" created="2015-05-29T16:57:17Z" id="106870538">CLA is signed.  @wy96f apologies for the delay
</comment><comment author="clintongormley" created="2015-12-18T09:54:30Z" id="165733870">This has been hanging around for so long because the PathTrie code is so hairy that nobody wants to touch it...  This particular change doesn't seem to solve any bugs, but might it prevent future bugs?

@colings86 @javanna @jasontedor any thoughts about what we should do here?
</comment><comment author="jasontedor" created="2015-12-19T16:50:13Z" id="166002532">&gt; any thoughts about what we should do here?

@clintongormley I've left a note on linked issue to see if we can get a better understanding of what this is suppose to be addressing in Elasticsearch. At a minimum, this code will require a rebase on master (Guava is gone, for example), and the CLA check is failing but I'd like to understand if there is an actual bug in Elasticsearch that the contributor is trying to address before merging.
</comment><comment author="clintongormley" created="2016-03-08T18:58:47Z" id="193915329">No further feedback.  Closing
</comment><comment author="karmi" created="2016-12-08T13:58:36Z" id="265745698">Hi @wy96f, we have found your signature in our records, but it seems like you have signed with a different e-mail than the one used in yout Git [commit](https://github.com/elastic/elasticsearch/pull/8072.patch). Can you please add both of these e-mails into your Github profile (they can be hidden), so we can match your e-mails to your Github profile?</comment><comment author="karmi" created="2016-12-08T14:03:09Z" id="265746719">@wy96f Apologies for the comment above, this was a manual fault from some local testing...</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PathTrie wrongly adds params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8071</link><project id="" key="" /><description>For instance, PathTrie builds two tries: "/a/{x}/b", "/{y}/c/d".  While retrieving "/a/c/d", it breaks for the params are {x=c, y=a} instead of {y=a}. We should push params before retrieving next token and pop them if we have to use wildcard to retrieve again.
</description><key id="45707484">8071</key><summary>PathTrie wrongly adds params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wy96f</reporter><labels><label>:REST</label><label>bug</label></labels><created>2014-10-14T03:00:40Z</created><updated>2016-03-16T19:39:52Z</updated><resolved>2016-03-08T18:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-12-08T12:22:28Z" id="66109083">Hi @wy96f I would like to understand more about the actual usecase here. I would be curious to know what made you find this problem for instance, is this something that happens with some of the  standard REST endpoints, or was this triggered by a plugin that registers custom ones? 
</comment><comment author="wy96f" created="2014-12-09T13:27:42Z" id="66281952">Hi @javanna Found this while reading the code:)
</comment><comment author="jasontedor" created="2015-12-19T16:47:31Z" id="166002414">Is this causing any actual bugs in Elasticsearch today? That is, are there any situations today where this issue causes Elasticsearch to do something unexpected or wrong? If there isn't a bug today, are there any situations where this issue _could_ lead to a bug in Elasticsearch?
</comment><comment author="clintongormley" created="2016-03-08T18:58:40Z" id="193915296">No further feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create PDF copies of the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8070</link><project id="" key="" /><description>This will allow for download for offline viewing, on a kindle while travelling etc.

Asciidoc can convert to PDF among other formats so it would be nice if this was supplied in PDF format as an option on elasticsearch.org.
</description><key id="45691803">8070</key><summary>Create PDF copies of the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HariSekhon</reporter><labels /><created>2014-10-13T22:12:37Z</created><updated>2014-10-16T11:53:23Z</updated><resolved>2014-10-16T11:46:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T11:46:27Z" id="59349450">Hi @harisekhon 

Our docs are updated several times per day, so we don't want to create static, out of date, PDFs from them as it will muddy the waters when we talk about "the docs from version X". That said, you could use asciidoc to create a PDF yourself.
</comment><comment author="HariSekhon" created="2014-10-16T11:53:14Z" id="59349984">@clintongormley I know, it's just a pain to do, it would be better if you auto-generated PDF for each asciidoc change for convenience of providing a PDF download link.

The docs and PDF should be timestamped if you're updating the docs several times a day, neutralizing the out of date thing with the "latest version of docs for version X".
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unknown field error when function score filter filters out all records</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8069</link><project id="" key="" /><description>I've created an example that demonstrates this issue here https://gist.github.com/lukebergen/1ddd897a1635f46c732b.

I've tried to pare this down to the bare essentials of the issue that I'm seeing but it's a little convoluted of a situation.

Basically, if I have a function score that I only want to apply if my field is present, so I add an exists filter to the function. This works great except in the use case where _all_ records are filtered out by this existence filter. In which case an error comes back "Unknown field [&lt;field name&gt;]".

I'm on ElasticSearch 1.3.4.
</description><key id="45688341">8069</key><summary>Unknown field error when function score filter filters out all records</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukebergen</reporter><labels /><created>2014-10-13T21:32:18Z</created><updated>2016-04-22T12:04:06Z</updated><resolved>2014-10-16T11:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T11:42:44Z" id="59349131">Hi @lukebergen 

The problem is that you are relying on dynamic mapping.  Your first two indexing requests don't create the field `age` (as we have no way of determining what datatype `age` will be from the value `null`).

If, instead, you map the `age` field before indexing, then your query works correctly:

```
DELETE /test 

PUT /test/
{
  "mappings": {
    "test": {
      "properties": {
        "age": {
          "type": "integer"
        }
      }
    }
  }
}

PUT /test/test/1
{
  "name": "Finn",
  "age": null
}

PUT /test/test/2
{
  "name": "Jake",
  "age": null
}


POST /test/_search
{
  "query": {
    "function_score": {
      "query": {
        "match_all": {}
      },
      "functions": [
        {
          "filter": {
            "exists": {
              "field": "age"
            }
          },
          "linear": {
            "age": {
              "origin": 10,
              "scale": 10,
              "decay": 0.9
            }
          }
        }
      ],
      "score_mode": "multiply"
    }
  }
}
```
</comment><comment author="sandstrom" created="2016-04-22T12:04:06Z" id="213396566">@clintongormley Wouldn't it make more sense to not even attempt to run the function score if the filter removes the document? We've finally tracked down an issue caused by this, and it's very un-intuitive.

Or am I missing something obvious?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Return _parent value by default for child documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8068</link><project id="" key="" /><description>When you search or GET to retrieve documents you don't get back the value for `_parent` field (or `_routing` for that matter) which is part of the essential metadata. Meaning the document cannot be retrieved or updated without the value. This is breaking some functionality, example use cases:
- reindex (results of `scan` being fed into `bulk`) cannot work
- persistence layers on top of elasticsearch cannot update a document they retrieved from ES without additional info/queries
- you cannot GET a document you searched for (for example to retrieve fields you excluded)

I believe it would be worth the cost to retrieve the value of `_parent` field when retrieving a child document.
</description><key id="45648811">8068</key><summary>Return _parent value by default for child documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">HonzaKral</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-13T14:35:40Z</created><updated>2015-06-25T13:36:04Z</updated><resolved>2015-06-25T13:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T11:34:56Z" id="59348430">This should probably be expanded to cover other metadata fields:
- _parent
- _routing
- _timestamp  (only retrievable if stored)
- _ttl (?)
</comment><comment author="tcucchietti" created="2014-11-21T23:07:55Z" id="64052394">:+1: 

I've stumbled across the following case for one of our projects : we needed to retrieve the generated timestamp for indexed document. However, it currently forces us to make another request to the cluster only to retrieve this `_timestamp` value.

I'm not sure if return it _by default_ is worthy of the extra cost. Maybe using a flag (as for [version parameter](http://www.elasticsearch.com/guide/en/elasticsearch/reference/current/search-request-version.html#search-request-version)) to tell ES to include it in the response could do the trick.

However, it would be nice to have it included by default (if stored) when indexing a document for the first time.
</comment><comment author="clintongormley" created="2014-11-24T19:23:12Z" id="64248218">@tcucchietti  you don't need a separate request for this value.  Just add `?fields=_timestamp,_source` to your search request, and  it'll return it to you
</comment><comment author="tcucchietti" created="2014-11-24T19:35:27Z" id="64250193">@clintongormley Sorry, my comment was maybe misleading.

I was talking about adding it in the indexing request's answer.

For example, this request :
`POST index/type
{
  "name":"test"
}`

currently returns something like :

```
{
   "_index": "index",
   "_type": "type",
   "_id": "eo3sI8NdS32NE2JUR3HrYA",
   "_version": 1,
   "created": true
}
```

It would be nice to have this :

```
{ 
   "_index": "index",
   "_type": "type",
   "_id": "eo3sI8NdS32NE2JUR3HrYA",
   "_timestamp": 1416857203356,
   "_version": 1,
   "created": true
}
```
</comment><comment author="clintongormley" created="2014-11-25T16:52:25Z" id="64431882">@tcucchietti Ah ok.  Why do you need that?  If you are wanting to set the timestamp to `now()` and you need it on the indexing side, why don't you just pass it as part of the indexing request?
</comment><comment author="tcucchietti" created="2014-11-25T20:17:10Z" id="64463884">@clintongormley In our case, this is what was finally done and it works perfectly.

As a user, it feeled a bit _awkward_ to have to do this on the indexing side. It's just a personal feeling, that's why I proposed some flag or configuration to achieve this.

Besides this, there could be cases where you need the exact timestamp of the document indexing in the cluster. For example, if you prepare a _big_ bulk indexing request, setting the timestamp value while preparing it could lead to some (minor) difference between this preset value and the exact value of the indexing time.
</comment><comment author="colings86" created="2015-02-20T10:52:45Z" id="75220394">+1 for returning any stored fields that are not derived from the _source field
</comment><comment author="clintongormley" created="2015-06-25T13:36:03Z" id="115260239">Fixed by #11816
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support for when all fields are deprecated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8067</link><project id="" key="" /><description /><key id="45647961">8067</key><summary>Support for when all fields are deprecated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-13T14:27:32Z</created><updated>2015-06-07T10:57:53Z</updated><resolved>2014-10-24T12:47:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-10-16T21:31:39Z" id="59434075">Left some notes. LGTM.
</comment><comment author="s1monw" created="2014-10-24T12:10:01Z" id="60378472">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/ParseField.java</file><file>src/test/java/org/elasticsearch/common/ParseFieldTests.java</file></files><comments><comment>ParseField: Support for when all fields are deprecated</comment></comments></commit></commits></item><item><title>Update source-field.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8066</link><project id="" key="" /><description>very minor typofix
</description><key id="45643020">8066</key><summary>Update source-field.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">bartvanhalder</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-13T13:40:24Z</created><updated>2014-10-29T13:51:32Z</updated><resolved>2014-10-29T13:51:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T11:29:22Z" id="59347952">Hi @bartvanhalder 

Thanks for the fix. Please could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-10-29T13:50:48Z" id="60926396">CLA not signed. Treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update source-field.asciidoc</comment></comments></commit></commits></item><item><title>[TEST] Add simple BWC tests that start from pre-build indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8065</link><project id="" key="" /><description>Today we only have integrated bwc tests for the current  major version. Yet there is no java-based bwc test that starts an old pre-build index. We lately added a test that starts from a prebuild `0.20` index to test if our upgrade API works. We should have simple things like tests for get by id and simple sorts
</description><key id="45632768">8065</key><summary>[TEST] Add simple BWC tests that start from pre-build indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-13T11:34:17Z</created><updated>2014-11-21T17:39:18Z</updated><resolved>2014-11-21T17:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-14T08:19:25Z" id="59005811">What I have in mind here is a two stage process. I think we should add a basic test class that can:
- start &amp; recover from a zipped index
- run basic searches on that index on a field 
- run basic sort on a field (utilizes fielddata/docvalues)
- uses realtime get to fetch document (check if hashing works ok)
- expand / reduce number or replicas to check if relocation / recovery works just fine

each of these can be a test method in that base test. The test itself then picks one or more bwc indices to run and iterates over them. `@Nightly` would build all of them I guess?

to create the zipped version of the index I think we can modify the upgrade python script we have and build a `backwards_index.py` from it that creates those indices. That way we can directly create a BWC index when we do a release and push it directly to the relevant branches.

Once we have the infra for this I guess we can go through all previous releases and build those bwc indices. This would be essentially the same as our upgrade python test script but would run with the other unittests which I'd prefer since the upgrade script runs against releases not against dev / release branches and it would be more integrated.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nested doc copy_to mapping broken?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8064</link><project id="" key="" /><description>It would appear that copy_to on nested documents has changed behaviour between 1.3.2 and master and is currently broken

Given this mapping:

```
curl -XPUT "http://localhost:9200/test/nesttest/_mapping" -d'
{
   "nesttest": {
      "properties": {
         "parent_name": {
            "type": "string"
         },
         "child": {
            "type": "nested",
            "properties": {
               "child_firstname": {
                  "type": "string",
                  "copy_to": "child_all"
               },
               "child_lastname": {
                  "type": "string",
                  "copy_to": "child_all"
               },
               "child_all": {
                  "type": "string"
               }
            }
         }
      }
   }
}'
```

And this document: 

```
curl -XPUT "http://localhost:9200/test/nesttest/1" -d'
{
   "parent_name": "bob",
   "child": [
      {
         "child_firstname": "pixie",
         "child_lastname": "geldof"
      },
      {
         "child_firstname": "peaches",
         "child_lastname": "geldof"
      }
   ]
}'
```

This query matched in 1.3.2 but sadly no longer:

```
curl -XGET "http://localhost:9200/test/nesttest/_search" -d'
{
   "query": {
      "nested": {
         "path": "child",
         "score_mode": "max",
         "query": {
            "match": {
               "child_all": "peaches"
            }
         }
      }
   }
}'
```

whereas this (rightly) matched nothing in 1.3.2 but mistakenly does now:

```
curl -XGET "http://localhost:9200/test/nesttest/_search" -d'
{
   "query": {
      "match": {
         "child_all": "peaches"
      }
   }
}'
```
</description><key id="45628049">8064</key><summary>Nested doc copy_to mapping broken?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>bug</label></labels><created>2014-10-13T10:21:48Z</created><updated>2015-01-21T07:59:08Z</updated><resolved>2014-10-13T12:14:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-10-13T12:14:44Z" id="58883632">It looks like the change relates to this issue: https://github.com/elasticsearch/elasticsearch/issues/6701

The solution is to make the copy_to declaration use the dot syntax to refer to the nested target field i.e. child.child_all
</comment><comment author="apatrida" created="2015-01-21T07:59:08Z" id="70798165">This should be documented for the copy_to setting within mappings, currently it is not clearly specified anywhere.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add NoopCircuitBreaker used in NoneCircuitBreakerService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8063</link><project id="" key="" /><description>This adds the NoopCircuitBreaker, which is used in the
NoneCircuitBreakerService when the overhead of a circuit breaker is
undesired.

In order to use it, the setting `indices.breaker.breaker_impl` can be
set to `org.elasticsearch.indices.breaker.NoneCircuitBreakerService`.

This also adds a benchmark for CircuitBreakerService implementations, so
that the overhead of the circuit breaker can be measured.
</description><key id="45627202">8063</key><summary>Add NoopCircuitBreaker used in NoneCircuitBreakerService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>feature</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-13T10:09:52Z</created><updated>2015-06-06T17:59:19Z</updated><resolved>2014-10-20T09:07:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-13T10:27:47Z" id="58873457">LGTM
</comment><comment author="kimchy" created="2014-10-13T10:28:33Z" id="58873516">I wonder if the benchmark is the correct way to do so? double checking it won't end up running it as part of our test infra?
</comment><comment author="dakrone" created="2014-10-13T10:30:36Z" id="58873671">@kimchy I will check to see if it runs during a regular test run, if it does, I'll change it to have a `main` method and not run.
</comment><comment author="dakrone" created="2014-10-20T09:07:17Z" id="59708716">Closing, this was added in #8135
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Clear the GroovyClassLoader cache before compiling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8062</link><project id="" key="" /><description>Since we don't use the cache, it's okay to clear it entirely if needed,
Elasticsearch maintains its own cache for compiled scripts.

Fixes #7658
Fixes #8073
</description><key id="45623691">8062</key><summary>Clear the GroovyClassLoader cache before compiling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-13T09:26:43Z</created><updated>2015-06-08T00:25:17Z</updated><resolved>2014-10-14T08:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-13T10:29:54Z" id="58873611">++ I think this is a a bug really no? should it go to 1.3.5 and 1.4 too? LGTM
</comment><comment author="dakrone" created="2014-10-13T14:38:11Z" id="58901022">Refactored this after talking with @kimchy, this is now implemented as a listener where the classloader is only cleared when scripts are removed from the cache.

I tested this with the script from #7658 and I was able to send/compile 10,000 unique scripts on both Java 7 and 8 without running into PermGen issues.
</comment><comment author="s1monw" created="2014-10-14T08:11:12Z" id="59004726">left minor comments - LGTM
</comment><comment author="dadoonet" created="2014-10-15T15:26:35Z" id="59223973">This one breaks script plugins for elasticsearch 1.3.5, 1.4.0 and above.

A new method in `ScriptEngineService` broke the contract:

``` java
void scriptRemoved(@Nullable CompiledScript script);
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update cjk-bigram-tokenfilter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8061</link><project id="" key="" /><description>Insert missing comma and correct typo in the source code.
</description><key id="45603835">8061</key><summary>Update cjk-bigram-tokenfilter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">sp836490</reporter><labels><label>docs</label></labels><created>2014-10-13T03:11:15Z</created><updated>2014-10-15T03:00:02Z</updated><resolved>2014-10-15T03:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-10-14T07:42:10Z" id="59001560">Hi @sp836490 

Looks good. Thanks for the PR. Could I ask you to sign the CLA so that I can merge this in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="sp836490" created="2014-10-14T08:35:30Z" id="59007577">Hi Jun,

Sure! I have signed the CLA.

Best Regards,

Tien-Hsu

---------- Original Message -----------
From: Jun Ohtani notifications@github.com
To: elasticsearch/elasticsearch elasticsearch@noreply.github.com
Cc: sp836490 thlee@vaplab.ce.ncu.edu.tw
Sent: Tue, 14 Oct 2014 00:42:39 -0700
Subject: Re: [elasticsearch] Update cjk-bigram-tokenfilter.asciidoc (#8061)

&gt; Hi @sp836490
&gt; 
&gt; Looks good. Thanks for the PR. Could I ask you to sign the CLA so 
&gt; that I can merge this in? http://www.elasticsearch.org/contributor-
&gt; agreement/
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/8061#issuecomment-
&gt; 59001560
&gt; ------- End of Original Message -------
</comment><comment author="johtani" created="2014-10-15T03:00:02Z" id="59151432">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sort on multi-value fields should work for range when specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8060</link><project id="" key="" /><description>Sorting on multi-value fields works on every value of a field. However, when the sort specifies a nested filter defining a range, the sorting algorithm should sort taken into accont those values included in the range specified.

document 1:

{

```
_index: calendar_events
_type: calendar_event
_id: 1
_version: 1
_score: 1
_source: {
    name: smalltalk
    description: fun with objects
    localized_time_expressions: [
        {
            occurrences: [
                10
                12
                14
                31
                33
                35
            ]
        }
    ]
}
```

}

document 2

{

```
_index: calendar_events
_type: calendar_event
_id: 2
_version: 1
_score: 1
_source: {
    name: spinning
    description: fun with bikes
    localized_time_expressions: [
        {
            occurrences: [
                11
                13
                15
                30
                32
                34
            ]
        }
    ]
}
```

}

If the sort clause is:

{

"sort": {
    "localized_time_expressions.occurrences": {
      "order": "asc",
      "mode": "min",
      "nested_filter": {
            "range": {
              "occurrences": {
                "gte": 29,
                "lte": 40
              }
            }
      }
    }
  }
}

You can see that in the filter I specify GTE 29 and LTE 40 so I would expect to see first DOCUMENT2 since it starts with 30 but I see first DOCUMENT1 despite it starts with 31.
The reason is that based on the Docs it is the lowest number (min) is taken from the multi value field. 
However, when a filterra nge  is specified it would be great to SORT by that subset of values that belong to the multi value field.
</description><key id="45602197">8060</key><summary>Sort on multi-value fields should work for range when specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">soliveri</reporter><labels /><created>2014-10-13T02:25:10Z</created><updated>2014-11-02T00:57:27Z</updated><resolved>2014-10-16T11:16:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T11:16:34Z" id="59346833">@soliveri In order to do this, `occurences` needs to be a nested object.  The filter can only identify _which_ documents match, then all of the values in the field are considered.  If you make `occurences` nested, then each internal document would only have one value - the value matching the filter.
</comment><comment author="soliveri" created="2014-10-18T03:35:09Z" id="59597653">@clintongormley I couldn't.

When sorting by a multi field and I specify a range gte and lte I want the sort to be done for picking the MIN or MAX value of THAT range, but it doesn't. Otherwise it takes the MIN or MAX of the multi field (the whole array and not the subset). 
</comment><comment author="clintongormley" created="2014-10-18T10:27:35Z" id="59606499">@soliveri Indeed you can :)

```
DELETE /t

PUT /t
{
  "mappings": {
    "t": {
      "properties": {
        "localized_time_expressions": {
          "type": "nested",
          "properties": {
            "occurrences": {
              "type": "nested",
              "properties": {
                "value": {
                  "type": "byte"
                }
              }
            }
          }
        }
      }
    }
  }
}

PUT /t/t/1
{
  "name": "smalltalk",
  "description": "fun with objects",
  "localized_time_expressions": [
    {
      "occurrences": [
        {
          "value": 10
        },
        {
          "value": 12
        },
        {
          "value": 14
        },
        {
          "value": 31
        },
        {
          "value": 33
        },
        {
          "value": 35
        }
      ]
    }
  ]
}

PUT /t/t/2
{
  "name": "spinning",
  "description": "fun with bikes",
  "localized_time_expressions": [
    {
      "occurrences": [
        {
          "value": 11
        },
        {
          "value": 13
        },
        {
          "value": 15
        },
        {
          "value": 30
        },
        {
          "value": 32
        },
        {
          "value": 34
        }
      ]
    }
  ]
}

GET t/t/_search
{
  "sort": {
    "localized_time_expressions.occurrences.value": {
      "order": "asc",
      "mode": "min",
      "nested_filter": {
        "range": {
          "localized_time_expressions.occurrences.value": {
            "gte": 29,
            "lte": 40
          }
        }
      }
    }
  }
}
```

Check the `sort` values in the output:

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": null,
      "hits": [
         {
            "_index": "t",
            "_type": "t",
            "_id": "2",
            "_score": null,
            "_source": {
               "name": "spinning",
               "description": "fun with bikes",
               "localized_time_expressions": [
                  {
                     "occurrences": [
                        {
                           "value": 11
                        },
                        {
                           "value": 13
                        },
                        {
                           "value": 15
                        },
                        {
                           "value": 30
                        },
                        {
                           "value": 32
                        },
                        {
                           "value": 34
                        }
                     ]
                  }
               ]
            },
            "sort": [
               30
            ]
         },
         {
            "_index": "t",
            "_type": "t",
            "_id": "1",
            "_score": null,
            "_source": {
               "name": "smalltalk",
               "description": "fun with objects",
               "localized_time_expressions": [
                  {
                     "occurrences": [
                        {
                           "value": 10
                        },
                        {
                           "value": 12
                        },
                        {
                           "value": 14
                        },
                        {
                           "value": 31
                        },
                        {
                           "value": 33
                        },
                        {
                           "value": 35
                        }
                     ]
                  }
               ]
            },
            "sort": [
               31
            ]
         }
      ]
   }
}
```
</comment><comment author="soliveri" created="2014-10-18T17:57:43Z" id="59623584">@clintongormley thanks a lot my friend, it worked like a charm and you saved me a lot of time :)
I am sorry about posting it like an issue but I could not find any other fast wat to get it answered.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dangling indices import ignores aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8059</link><project id="" key="" /><description>Dangling indices are indexes found on disk which are not part of the cluster state. By default, we don't delete them but rather import them into the cluster state in order to not accidentally delete data and also allow for the ease of copying index data folders from one cluster to another. Currently, the import logic doesn't check for existing aliases of the same name as the imported dangling index, causing both an index and an alias with the same name.

 This commit add a protection against this. Note that the index is still kept as dangling and is only deleted from disk after `gateway.local.dangling_timeout` has passed (2 hours).

 We also change the log message indicating deletion of dangling indices to a `WARN` level.
</description><key id="45556229">8059</key><summary>Dangling indices import ignores aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-11T16:50:47Z</created><updated>2015-06-07T18:20:53Z</updated><resolved>2014-10-15T11:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-13T13:50:49Z" id="58894540">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalAllocateDangledIndices.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/test/java/org/elasticsearch/gateway/local/LocalGatewayIndexStateTests.java</file></files><comments><comment>Internal: dangling indices import ignores aliases</comment></comments></commit></commits></item><item><title>download links / versions should be consistent across elk stack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8058</link><project id="" key="" /><description>inconsistent punctuation ( dot vs dash preceding 'beta' ) and capitalization ( for the word beta ) between the different products download links causes usability issues for people writing chef/puppet/etc to automate installs.

https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.0.Beta1.tar.gz
https://download.elasticsearch.org/kibana/kibana/kibana-4.0.0-BETA1.1.tar.gz
https://download.elasticsearch.org/logstash/logstash/logstash-1.4.0.beta2.tar.gz
</description><key id="45555250">8058</key><summary>download links / versions should be consistent across elk stack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">paulczar</reporter><labels><label>:Packaging</label></labels><created>2014-10-11T16:11:58Z</created><updated>2014-11-07T13:16:54Z</updated><resolved>2014-11-07T13:16:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-11-07T02:58:40Z" id="62089237">Thanks for reporting this, and we completely agree.  
We have decided to standardise package names across all of our products, 
and we will use the following format: x.x.x-Beta1, x.x.x-Beta2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Thread leak with autodiscover after adding / removing a master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8057</link><project id="" key="" /><description>I've had an epic bug leading to a thread leak (and ES dying because it had no more RAM to create more threads). [Whole story here](https://t37.net/my-epic-elasticsearch-bug-and-how-i-fucked-up-my-investigation-not-focusing-wide-enough.html).
## Bug description

4 nodes ES cluster on EC2. 1 routing node, 3 data nodes. The routing node acts as master (configuration provided below)

After I added and removed a master node, one of the data nodes kept sending auto discovery requests to a the gone node. This led to the remaining routing node to create about 1 thread / second (sending auto discovery requests) and never closing them.
## Configuration
- ES 1.0.1 (old, I know)
- ES Transport Thrift: elasticsearch-transport-thrift-2.0.0
- AWS cloud plugin: cloud-aws-2.0.0
### Routing node

``` yml
bootstrap:
  mlockall: true
cloud:
  aws:
    access_key: something
    region: us-east-1
    secret_key: something
cluster:
  name: robots
discovery:
  ec2:
    ping_timeout: 360
    tag:
      Cluster: production
  type: ec2
  zen:
    minimum_master_nodes: 1
gateway:
  expected_nodes: 4
  recover_after_nodes: 4
  recover_after_time: 5m
http:
  max_content_length: 100mb
index:
  query:
    bool:
      max_clause_count: 1000000
  refresh_interval: 300
  store:
    type: mmapfs
indices:
  fielddata:
    cache:
      expire: 10m
      size: 30%
  memory:
    index_buffer_size: 10%
network:
  host: 0.0.0.0
node:
  data: false
  master: true
  name: something
path:
  data: /mnt/elasticsearch
  logs: /var/log/elasticsearch
```
### Data nodes

``` yaml
bootstrap:
  mlockall: true
cloud:
  aws:
    access_key: something
    region: us-east-1
    secret_key: something
cluster:
  name: robots
discovery:
  ec2:
    ping_timeout: 360
    tag:
      Cluster: production
  type: ec2
  zen:
    minimum_master_nodes: 1
gateway:
  expected_nodes: 4
  recover_after_nodes: 4
  recover_after_time: 5m
http:
  max_content_length: 100mb
index:
  query:
    bool:
      max_clause_count: 1000000
  refresh_interval: 300
  store:
    type: mmapfs
indices:
  fielddata:
    cache:
      expire: 10m
      size: 30%
  memory:
    index_buffer_size: 10%
network:
  host: 0.0.0.0
node:
  data: true
  master: false
  name: something
path:
  data: /mnt/elasticsearch
  logs: /var/log/elasticsearch
```
### JVM
- xmx and xms to 4G
- no fancy GC tuning

Tell me if I you need anything else.
</description><key id="45549227">8057</key><summary>Thread leak with autodiscover after adding / removing a master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fdv</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2014-10-11T11:49:58Z</created><updated>2015-11-21T19:44:01Z</updated><resolved>2015-11-21T19:44:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-13T08:30:27Z" id="58861270">Hi Frederic,

Would it be possible to run some other tests on your side?
- Without AWS plugin (using unicast list)
- Without Thrift plugin
- Upgrading to elasticsearch 1.3.4 and AWS 2.3.0 

I would like to understand if this leak is caused by AWS plugin or one of its dependencies. In AWS plugin, we updated AWS SDK to a more recent version (`1.7.13` - was `1.7.3` in AWS 2.0.0).
</comment><comment author="imotov" created="2014-10-14T15:42:59Z" id="59067130">@fdv if you can reproduce the leak, could you also run the following command on the node that is leaking threads and post there the output here: `curl "localhost:9200/_nodes/_local/hot_threads?threads=100000"`
</comment><comment author="clintongormley" created="2014-12-30T20:26:47Z" id="68394583">No further info provided. Closing
</comment><comment author="jh12z" created="2015-10-02T10:13:57Z" id="144982961">&lt;img width="1552" alt="screenshot 2015-10-02 10 44 37" src="https://cloud.githubusercontent.com/assets/567081/10244402/e0a7ab8a-68f6-11e5-9ffc-8f5a36275fa2.png"&gt;
I'm getting the same thread leak caused by the `MulticastZenPing` class which ends up causing an OOM (`OutOfMemoryError: unable to create new native thread`) being thrown **every second** when the limit of **2031 threads** is reached! (Mac OS X native thread limit)

It seems threads are created for the auto discovery but never disposed.

This is the stack trace for the thread creation:

```
java.lang.OutOfMemoryError: unable to create new native thread
    at java.lang.Thread.start0(Native Method)
    at java.lang.Thread.start(Thread.java:714)
    at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:950)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1368)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:79)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver.handleNodePingRequest(MulticastZenPing.java:539)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver.onMessage(MulticastZenPing.java:429)
    at org.elasticsearch.common.network.MulticastChannel$MultiListener.onMessage(MulticastChannel.java:139)
    at org.elasticsearch.common.network.MulticastChannel$Plain$Receiver.run(MulticastChannel.java:381)
    at java.lang.Thread.run(Thread.java:745)
```

The result of requesting `http://localhost:9200/_nodes/_local/hot_threads?threads=100000` brings back around 2000 threads with the following stack trace:

```
 0.0% (0s out of 500ms) cpu usage by thread 'elasticsearch[Jazz][generic][T#131]'
     10/10 snapshots sharing following 15 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
       java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
       java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)
       java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)
       org.elasticsearch.common.util.concurrent.KeyedLock.acquire(KeyedLock.java:79)
       org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:743)
       org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:731)
       org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:216)
       org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver$1.run(MulticastZenPing.java:544)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="jh12z" created="2015-10-02T10:18:18Z" id="144984319">Forgot to comment this is happening with Elasticsearch 1.7.1
</comment><comment author="clintongormley" created="2015-10-02T16:46:43Z" id="145083609">@bleskes any ideas?
</comment><comment author="bleskes" created="2015-10-20T13:07:51Z" id="149562408">@jh12z when ever a node receives a multi cast ping, it tries to connect the source of the ping and respond. This is done on a background thread. Something seems to be blocking connections (but not rejecting them) causing these thread to accumulate. Any idea what it can be? Do you have only one node in your cluster (you mention mac os x)? can you check your host and publish addresses ?  Es logs them when it starts:

```
[2015-10-20 11:42:07,426][INFO ][transport                ] [Meteorite] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[fe80::1]:9300}, {[::1]:9300}
```
</comment><comment author="clintongormley" created="2015-11-21T19:44:00Z" id="158677035">No further feedback for a year. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Indicate that the Children Aggregation is coming in 1.4.0 on 404_Parent_Child Children Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8056</link><project id="" key="" /><description>I spent a couple of hours trying out the children aggregation only to find out that Child Aggregation will be comping out in 1.4.0

Target Document: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/children-agg.html

Related: #6936 #7755
</description><key id="45544990">8056</key><summary>[DOCS] Indicate that the Children Aggregation is coming in 1.4.0 on 404_Parent_Child Children Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">juneym</reporter><labels /><created>2014-10-11T07:52:49Z</created><updated>2014-10-13T10:04:21Z</updated><resolved>2014-10-13T10:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-13T09:45:38Z" id="58868952">It is properly tagger in the 1.x and 1.4 documentation: http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/search-aggregations-bucket-children-aggregation.html#search-aggregations-bucket-children-aggregation

But the search results return the documentation from the master branch. The master branch doesn't contain any version tags.
</comment><comment author="clintongormley" created="2014-10-13T10:04:21Z" id="58871421">Actually the search results come from the "current" branch, which, for the reference docs, is 1.3.  If you switch to the 1.4 docs, then you see results from 1.4.

The definitive guide, however, targets version 1.4.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify discovery node initialization if version is unknown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8055</link><project id="" key="" /><description>Today it's easy to use the wrong version when initializing DiscoveryNode
instances. This commit adds javadocs and a utility constant to
initialize DiscoveryNode instances if the the remotes node version is
unknown.

Closes #8051
</description><key id="45544734">8055</key><summary>Simplify discovery node initialization if version is unknown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-11T07:38:58Z</created><updated>2015-06-06T19:26:49Z</updated><resolved>2014-10-13T10:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-11T08:03:12Z" id="58741954">LGTM
</comment><comment author="clintongormley" created="2014-10-16T11:22:21Z" id="59347350">Is this something that could bite existing users who just try to upgrade to 1.4?  Should we be highlighting this in the migrating-to-1.4 docs?
</comment><comment author="s1monw" created="2014-10-16T13:47:26Z" id="59363245">to be honest I think this is very expert and a bit over the top for a migration guide?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Dropped shard after delete by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8054</link><project id="" key="" /><description>After performing a delete by query on an index, a few hours later it drops one of the shards with an a MergePolicyException ArrayIndexOutOfBoundsException.  Elasticsearch version is 0.90.13

Exception in thread "elasticsearch[servername][[index][0]: Lucene Merge Thread #5228]" org.apache.lucene.index.MergePolicy$MergeException: java.lang.ArrayIndexOutOfBoundsException: 79001
        at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:545)
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:110)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 79001
        at org.apache.lucene.codecs.lucene40.BitVector.get(BitVector.java:147)
        at org.apache.lucene.index.MergeState$DocMap$1.get(MergeState.java:87)
        at org.apache.lucene.codecs.MappingMultiDocsAndPositionsEnum.nextDoc(MappingMultiDocsAndPositionsEnum.java:107)
        at org.apache.lucene.codecs.PostingsConsumer.merge(PostingsConsumer.java:109)
        at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:164)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:383)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:106)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4071)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3668)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)

[2014-10-10 18:30:30,802][WARN ][index.merge.scheduler    ] [servername] [index][0] failed to merge
java.lang.ArrayIndexOutOfBoundsException: 79001
        at org.apache.lucene.codecs.lucene40.BitVector.get(BitVector.java:147)
        at org.apache.lucene.index.MergeState$DocMap$1.get(MergeState.java:87)
        at org.apache.lucene.codecs.MappingMultiDocsAndPositionsEnum.nextDoc(MappingMultiDocsAndPositionsEnum.java:107)
        at org.apache.lucene.codecs.PostingsConsumer.merge(PostingsConsumer.java:109)
        at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:164)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:383)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:106)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4071)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3668)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
[2014-10-10 18:30:30,896][WARN ][index.engine.robin       ] [servername] [index][0] failed engine
org.apache.lucene.index.MergePolicy$MergeException: java.lang.ArrayIndexOutOfBoundsException: 79001
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:109)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 79001
        at org.apache.lucene.codecs.lucene40.BitVector.get(BitVector.java:147)
        at org.apache.lucene.index.MergeState$DocMap$1.get(MergeState.java:87)
        at org.apache.lucene.codecs.MappingMultiDocsAndPositionsEnum.nextDoc(MappingMultiDocsAndPositionsEnum.java:107)
        at org.apache.lucene.codecs.PostingsConsumer.merge(PostingsConsumer.java:109)
        at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:164)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:383)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:106)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4071)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3668)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
[2014-10-10 18:30:30,962][DEBUG][index.shard.service      ] [servername] [index][0] state: [STARTED]-&gt;[CLOSED], reason [engine failure [MergeException[java.lang.ArrayIndexOutOfBoundsException: 79001]; nested: ArrayIndexOutOfBoundsException[79001]; ]]
[2014-10-10 18:30:30,994][WARN ][cluster.action.shard     ] [servername] [index][0] sending failed shard for [index][0], node[3cPPL9a5RTOL8UMpWLJBbQ], [P], s[STARTED], indexUUID [_na_], reason [engine failure, message [MergeException[java.lang.ArrayIndexOutOfBoundsException: 79001]; nested: ArrayIndexOutOfBoundsException[79001]; ]]
</description><key id="45537172">8054</key><summary>Dropped shard after delete by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clly</reporter><labels /><created>2014-10-11T01:12:01Z</created><updated>2014-10-11T07:45:16Z</updated><resolved>2014-10-11T07:45:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-11T07:45:16Z" id="58741514">I don't think this is caused by the delete by query but it might be triggered by it. This is very likely a shard corruption which was undiscovered until the merge happened which was likely triggered by the deletes you caused with delete by query. The only way to detect this earlier is to upgrade to the the latest ES version `1.3.4` (at this point). We have checksum support in elasticsearch now which can detect these problems way earlier and makes sure corruption's are not replicated on recovery. I highly recommend you to upgrade. With ES 1.4 there will also be an upgrade API that brings your segments up to the latest version which might be required in your case since your segments don't have checksums yet. (this was introduced in lucene 4.8 which corresponds to ES `1.2.0` or greater.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parent/child: Eager global ordinal loading should be the default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8053</link><project id="" key="" /><description /><key id="45488242">8053</key><summary>Parent/child: Eager global ordinal loading should be the default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>discuss</label><label>enhancement</label></labels><created>2014-10-10T14:47:21Z</created><updated>2017-03-22T08:56:08Z</updated><resolved>2016-08-24T15:30:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-10T14:52:06Z" id="58666404">+1
</comment><comment author="clintongormley" created="2015-11-21T19:43:07Z" id="158676992">@martijnvg With the new implementation of parent/child, should we still default to eager global ordinal loading for p/c?
</comment><comment author="martijnvg" created="2015-11-23T08:01:49Z" id="158872983">@clintongormley I'm actually unsure if we should do this. In heavy read use cases it makes perfect sense, but heavy write and irregular searches (logging use case) the eager loading that happens during each refresh is useless and wasteful, because most of the times the global ordinal data structure is not being used.
</comment><comment author="jpountz" created="2015-11-23T10:13:07Z" id="158895893">This "should we perform operations eagerly on refresh" discussion happens all the time for the timeseries use-case. Since the goal of the refresh operation is to make changes searchable, maybe what we need is a different way to refresh for the timeseries use-case? For instance instead of running asynchronously every X seconds, it could run when calling the `_search` API if the last refresh occurred more than X seconds ago? This way users might achieve better write performance and on our end it would be easier to decide to perform costly operations on refresh since we would know that the things that we build are likely to be used.
</comment><comment author="jpountz" created="2016-08-24T15:30:48Z" id="242107403">Closing: the _parent field loads ordinals eagerly in recent versions of Elasticsearch.
</comment><comment author="nezda" created="2017-03-20T16:34:07Z" id="287817376">docs don't reflect this https://www.elastic.co/guide/en/elasticsearch/reference/5.2/mapping-parent-field.html#_global_ordinals</comment><comment author="clintongormley" created="2017-03-21T19:50:37Z" id="288197657">@martijnvg @jpountz does the _parent field load ordinals eagerly in 5.x?  If so we should update the docs</comment><comment author="martijnvg" created="2017-03-22T08:56:08Z" id="288335881">@clintongormley Yes, the _parent field does load global ordinals eagerly. I think since 5.0, it was changed as part of #17148. I'll update the docs.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Update the docs about the fact that global ordinals for _parent field are loaded eagerly instead of lazily by default.</comment></comments></commit></commits></item><item><title>Fix location information for loggers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8052</link><project id="" key="" /><description>This change corrects the location information gathered by the loggers so that when printing class name, method name, and line numbers in the log pattern, the information from the class calling the logger is used rather than a location within the logger itself.

Closes #5130
</description><key id="45475573">8052</key><summary>Fix location information for loggers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Logging</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-10T12:31:53Z</created><updated>2015-06-07T18:25:03Z</updated><resolved>2014-10-29T10:33:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T09:52:15Z" id="59338710">@spinscale could you review this one please?
</comment><comment author="dakrone" created="2014-10-29T09:40:48Z" id="60895691">Left a couple of minor comments, could you add some javadocs to the `ESLogRecord` class to explain at a glance why this is needed for anyone stumbling upon it in the future?

Other than that it looks good to me.
</comment><comment author="dakrone" created="2014-10-29T10:21:58Z" id="60900433">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/logging/jdk/ESLogRecord.java</file><file>src/main/java/org/elasticsearch/common/logging/jdk/JdkESLogger.java</file><file>src/main/java/org/elasticsearch/common/logging/jdk/JdkESLoggerFactory.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/Log4jESLogger.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java</file><file>src/main/java/org/elasticsearch/common/logging/slf4j/Slf4jESLogger.java</file><file>src/test/java/org/elasticsearch/common/logging/LoggingConfigurationTests.java</file><file>src/test/java/org/elasticsearch/common/logging/jdk/JDKESLoggerTests.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java</file></files><comments><comment>Core: Fix location information for loggers</comment></comments></commit></commits></item><item><title>Error during unicast discovery between 1.3.4 and 1.4.0.Beta1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8051</link><project id="" key="" /><description>I ran into this issue where a freshly 1.4.0.Beta.1 instance would not be able to ping (or receive pings?) from a 1.3.4-instance, and found the following in the 1.4 instance log:

```
[2014-10-09 09:43:15,759][TRACE][discovery.zen.ping.unicast] [foo-instance] failed to ping [#zfd0#][ec2-host-1.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.0.3.4:9300]], falling back to &lt;=1.4.0 ping action
[2014-10-09 09:43:15,760][TRACE][discovery.zen.ping.unicast] [foo-instance] [1] sending to [#zen_unicast_1_#zfd0##][ec2-host-1.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.0.3.4:9300]]
[2014-10-09 09:43:15,767][WARN ][discovery.zen.ping.unicast] [foo-instance] failed to send ping to [[#zfd0#][ec2-host-1.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.0.3.4:9300]]]
org.elasticsearch.transport.RemoteTransportException: [bar-instance][inet[/10.0.3.4:9300]][internal:discovery/zen/unicast]
Caused by: org.elasticsearch.transport.ActionNotFoundTransportException: No handler for action [internal:discovery/zen/unicast]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:210)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

Is there something obvious I have missed/done wrong? The action name seems pretty odd.
</description><key id="45466925">8051</key><summary>Error during unicast discovery between 1.3.4 and 1.4.0.Beta1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2014-10-10T10:25:32Z</created><updated>2014-10-13T10:32:14Z</updated><resolved>2014-10-13T10:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nkvoll" created="2014-10-10T10:30:48Z" id="58638518">I should add that the 1.4 instance discovers the 1.3.4-node using a separate `UnicastHostsProvider`, which creates a discovery node similar this:

```
new DiscoveryNode("#zfd"+discoveryNodeCounter.getAndIncrement()+"#", address, Version.CURRENT);
```

The part where it uses `Version.CURRENT` might not be technically correct in this case, since it's actually connecting to an older version, but it's not obvious (to me, at least) what other way there is if the version of the destination node isn't known at that point (i.e before connecting to it).
</comment><comment author="nkvoll" created="2014-10-10T10:35:45Z" id="58638958">I'm also unsure of whether I'm reading this commit message correctly: https://github.com/elasticsearch/elasticsearch/commit/d2fea5378ad8beff6e2eb566c9d573f93771e2ef

.. should I take it to mean that I have to specify the lower bound during the `DiscoveryNode` instantiation, or should it really be handled in a backwards-compatible way by Elasticsearch itself? I initially understood it as the latter, but this doesn't seem to be the case.
</comment><comment author="s1monw" created="2014-10-10T10:43:46Z" id="58639687">are you actual having problems with your cluster of is it only that you see this errors showing up? I think this error comes from the backwards layer we have in 1.4 that can't tell what version it's pinging on the first contact so it goes and tries the new (1.4 and above) and the old endpoint (1.3.4 and below). If the new endpoint is causing an error it falls back to the old for backwards compatibility. It might be just this initial probe you are seeing?
</comment><comment author="nkvoll" created="2014-10-10T10:53:18Z" id="58640471">The end result here was that the 1.4-instance did not join the cluster, but instead elected itself as master after what looked like three retries:

```
[2014-10-09 09:43:18,718][TRACE][discovery.zen.ping.unicast] [foo-instance] [1] disconnecting from [#zen_unicast_1_#zen_found_discovery_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.0.3.4:9300]]
[2014-10-09 09:43:18,722][TRACE][discovery.zen.ping.unicast] [foo-instance] [1] disconnecting from [#zen_unicast_3_#zen_found_discovery_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.0.3.4:9300]]
[2014-10-09 09:43:18,723][TRACE][discovery.zen.ping.unicast] [foo-instance] [1] disconnecting from [#zen_unicast_2_#zen_found_discovery_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.0.3.4:9300]]
[2014-10-09 09:43:18,725][TRACE][discovery.zen            ] [foo-instance] full ping responses: {none}
[2014-10-09 09:43:18,725][DEBUG][discovery.zen            ] [foo-instance] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2014-10-09 09:43:18,732][INFO ][cluster.service          ] [foo-instance] new_master [foo-instance][nacDp91kRDCmsh04NoJBWA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.0.4.4:9300]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}, reason: zen-disco-join (elected_as_master)
[2014-10-09 09:43:18,734][INFO ][plugin.found.zookeeper.updaters] [foo-instance] Master node changed. Current masterNode: [[foo-instance][nacDp91kRDCmsh04NoJBWA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.0.4.4:9300]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}]
[2014-10-09 09:43:18,740][TRACE][discovery.zen            ] [foo-instance] cluster joins counter set to [1] (elected as master)
[2014-10-09 09:43:18,755][INFO ][http                     ] [foo-instance] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[ec2-host-2.compute.amazonaws.com/10.0.4.4:9200]}
```
</comment><comment author="bleskes" created="2014-10-10T11:16:05Z" id="58642319">@nkvoll this is not intended. The backwards compatibility check that @s1monw mentioned does exist but it doesn't manifest itself in warn level logs.

Can you tell me some more about the cluster layout? I see aws related logs. I also see the zoo keeper plugin?  
</comment><comment author="nkvoll" created="2014-10-10T12:17:00Z" id="58647562">@bleskes what I'm attempting is a simple cluster upgrade, adding a 1.4.0.Beta1 instance to a cluster consisting of a single instance running 1.3.4. They discover each-other via a custom `UnicastHostProvider`, which provides the host/port of the other instances in the cluster.

The 1.3.4 instance only has the following to say (discovery is logged down to the trace level):

```
[2014-10-10 12:03:14,671][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [0] and action [internal:discovery/zen/unicast_gte_1_4], resetting
[2014-10-10 12:03:14,703][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [2] and action [internal:discovery/zen/unicast], resetting
[2014-10-10 12:03:16,124][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [4] and action [internal:discovery/zen/unicast_gte_1_4], resetting
[2014-10-10 12:03:16,132][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [5] and action [internal:discovery/zen/unicast], resetting
[2014-10-10 12:03:17,619][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [7] and action [internal:discovery/zen/unicast_gte_1_4], resetting
[2014-10-10 12:03:17,626][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [8] and action [internal:discovery/zen/unicast], resetting
```

While the 1.4.0.Beta1-instance logs the following three times before forming its own cluster (which is permitted since minimum_master_nodes: 1 for both instances -- but shouldn't really be happening)

```
[2014-10-10 12:03:14,601][TRACE][discovery.zen            ] [instance-0000000001] starting to ping
[2014-10-10 12:03:14,604][TRACE][discovery.zen.ping.unicast] [instance-0000000001] replacing [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]] with temp node [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]]
[2014-10-10 12:03:14,606][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connecting (light) to [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]]
[2014-10-10 12:03:14,606][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connecting to [instance-0000000001][lEeCD6d4QjyETwJLV3ao5Q][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.82.2.158:19182]]{region=eu-west-1, availability_zone=eu-west-1a, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:03:14,624][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connected to [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]]
[2014-10-10 12:03:14,625][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]], using &gt;=1.4.0 serialization
[2014-10-10 12:03:14,648][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connected to [instance-0000000001][lEeCD6d4QjyETwJLV3ao5Q][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.82.2.158:19182]]{region=eu-west-1, availability_zone=eu-west-1a, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:03:14,649][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [instance-0000000001][lEeCD6d4QjyETwJLV3ao5Q][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.82.2.158:19182]]{region=eu-west-1, availability_zone=eu-west-1a, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:03:14,658][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] received response from [instance-0000000001][lEeCD6d4QjyETwJLV3ao5Q][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.82.2.158:19182]]{region=eu-west-1, availability_zone=eu-west-1a, max_local_storage_nodes=1, logical_availability_zone=zone-0}: [ping_response{node [[instance-0000000001][lEeCD6d4QjyETwJLV3ao5Q][ec2-host-2.compute.amazonaws.com][inet[/10.82.2.158:19182]]{region=eu-west-1, availability_zone=eu-west-1a, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[1], master [null], hasJoinedOnce [false], cluster_name[538289781bfa9cb13184890f9f23584b]}, ping_response{node [[instance-0000000001][lEeCD6d4QjyETwJLV3ao5Q][ec2-host-2.compute.amazonaws.com][inet[/10.82.2.158:19182]]{region=eu-west-1, availability_zone=eu-west-1a, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[2], master [null], hasJoinedOnce [false], cluster_name[538289781bfa9cb13184890f9f23584b]}]
[2014-10-10 12:03:14,700][TRACE][discovery.zen.ping.unicast] [instance-0000000001] failed to ping [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]], falling back to &lt;=1.4.0 ping action
[2014-10-10 12:03:14,700][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]]
[2014-10-10 12:03:14,707][WARN ][discovery.zen.ping.unicast] [instance-0000000001] failed to send ping to [[#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.74.20.146:19775]]]
org.elasticsearch.transport.RemoteTransportException: [instance-0000000000][inet[/10.74.20.146:19775]][internal:discovery/zen/unicast]
Caused by: org.elasticsearch.transport.ActionNotFoundTransportException: No handler for action [internal:discovery/zen/unicast]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:210)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```

The zookeeper plugin handles the `UnicastHostProvider`-part, but does not currently include any versioning information, which is why it uses `Version.CURRENT`, which I immediately thought could be the culprit -- but if I understand it correctly, it shouldn't actually matter.
</comment><comment author="nkvoll" created="2014-10-10T12:32:11Z" id="58648992">If I simply change the `DiscoveryNode` instantiation in our code from using `Version.CURRENT` to using `Version.V_1_3_0` it works, with the following in the 1.3.4 instance logs:

```
[2014-10-10 12:23:35,247][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [0] and action [internal:discovery/zen/unicast_gte_1_4], resetting
[2014-10-10 12:23:36,679][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [4] and action [internal:discovery/zen/unicast_gte_1_4], resetting
[2014-10-10 12:23:38,186][WARN ][transport.netty          ] [instance-0000000000] Message not fully read (request) for [7] and action [internal:discovery/zen/unicast_gte_1_4], resetting
[2014-10-10 12:23:38,243][INFO ][cluster.service          ] [instance-0000000000] added {[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0},}, reason: zen-disco-receive(join from node[[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}])
```

and the following from the 1.4.0.Beta1-instance:

```
[2014-10-10 12:23:35,152][TRACE][discovery.zen            ] [instance-0000000001] starting to ping
[2014-10-10 12:23:35,155][TRACE][discovery.zen.ping.unicast] [instance-0000000001] replacing [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]] with temp node [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:35,157][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connecting (light) to [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:35,168][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connecting to [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:23:35,199][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connected to [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:35,200][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]], using &gt;=1.4.0 serialization
[2014-10-10 12:23:35,220][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connected to [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:23:35,221][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:23:35,232][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] received response from [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}: [ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[1], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[2], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}]
[2014-10-10 12:23:35,272][TRACE][discovery.zen.ping.unicast] [instance-0000000001] failed to ping [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]], falling back to &lt;=1.4.0 ping action
[2014-10-10 12:23:35,274][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:35,281][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] received response from [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]: [ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [null], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}]
[2014-10-10 12:23:36,669][TRACE][discovery.zen.ping.unicast] [instance-0000000001] replacing [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]] with temp node [#zen_unicast_2_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:36,670][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connecting (light) to [#zen_unicast_2_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:36,670][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:23:36,673][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connected to [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:36,674][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_2_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]], using &gt;=1.4.0 serialization
[2014-10-10 12:23:36,676][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] received response from [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}: [ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[1], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[3], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[4], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}]
[2014-10-10 12:23:36,681][TRACE][discovery.zen.ping.unicast] [instance-0000000001] failed to ping [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]], falling back to &lt;=1.4.0 ping action
[2014-10-10 12:23:36,682][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_2_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:36,685][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] received response from [#zen_unicast_2_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]: [ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [null], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [null], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}]
[2014-10-10 12:23:38,174][TRACE][discovery.zen.ping.unicast] [instance-0000000001] replacing [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]] with temp node [#zen_unicast_3_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:38,176][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connecting (light) to [#zen_unicast_3_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:38,176][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:23:38,179][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] connected to [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:38,180][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_3_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]], using &gt;=1.4.0 serialization
[2014-10-10 12:23:38,183][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] received response from [instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[ec2-host-2.compute.amazonaws.com/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}: [ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[1], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[3], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[5], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[6], master [null], hasJoinedOnce [false], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}]
[2014-10-10 12:23:38,187][TRACE][discovery.zen.ping.unicast] [instance-0000000001] failed to ping [#zfd_0#][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]], falling back to &lt;=1.4.0 ping action
[2014-10-10 12:23:38,188][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] sending to [#zen_unicast_3_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:38,192][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] received response from [#zen_unicast_3_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]: [ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [null], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [null], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000001][ifebecHgTh6lPu8IPgDMGA][ec2-host-2.compute.amazonaws.com][inet[/10.74.20.146:19861]]{region=eu-west-1, availability_zone=eu-west-1b, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [null], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}, ping_response{node [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}]
[2014-10-10 12:23:38,193][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] disconnecting from [#zen_unicast_1_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:38,196][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] disconnecting from [#zen_unicast_3_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:38,197][TRACE][discovery.zen.ping.unicast] [instance-0000000001] [1] disconnecting from [#zen_unicast_2_#zfd_0##][ec2-host-2.compute.amazonaws.com][inet[ec2-host-1.compute.amazonaws.com/10.106.7.243:19224]]
[2014-10-10 12:23:38,199][TRACE][discovery.zen            ] [instance-0000000001] full ping responses:
    --&gt; ping_response{node [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}
[2014-10-10 12:23:38,200][DEBUG][discovery.zen            ] [instance-0000000001] filtered ping responses: (filter_client[true], filter_data[false])
    --&gt; ping_response{node [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], id[-1], master [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], hasJoinedOnce [null], cluster_name[414938d3d78690f02ac6a9df4356b6c6]}
[2014-10-10 12:23:38,201][TRACE][discovery.zen            ] [instance-0000000001] ignoring joined once flags in ping responses, minimum ping version [1.3.4]
[2014-10-10 12:23:38,208][TRACE][discovery.zen            ] [instance-0000000001] joining master [instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}
[2014-10-10 12:23:38,248][DEBUG][discovery.zen.publish    ] [instance-0000000001] received cluster state version 10
[2014-10-10 12:23:38,250][DEBUG][discovery.zen.fd         ] [instance-0000000001] [master] restarting fault detection against master [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}], reason [new cluster state received and we are monitoring the wrong master [null]]
[2014-10-10 12:23:38,251][DEBUG][discovery.zen            ] [instance-0000000001] got first state from fresh master [o5ulTkS4TV-BDs_NbkReqQ]
[2014-10-10 12:23:38,252][TRACE][discovery.zen            ] [instance-0000000001] updated cluster join cluster to [1]
[2014-10-10 12:23:38,254][INFO ][cluster.service          ] [instance-0000000001] detected_master [instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}, added {[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0},}, reason: zen-disco-receive(from master [[instance-0000000000][o5ulTkS4TV-BDs_NbkReqQ][ec2-host-1.compute.amazonaws.com][inet[/10.106.7.243:19224]]{region=eu-west-1, availability_zone=eu-west-1c, max_local_storage_nodes=1, logical_availability_zone=zone-0}])
```

The notable difference being that there were no exceptions logged when instantiating the `DiscoveryNode` using a lower-bound `Version`, and that the 1.4 instance was able to join the 1.3 cluster.

Does this possibly mean that there might be an issue with providing `DiscoveryNode`s using a `UnicastHostsProvider` when the target instance version is actually unknown?
</comment><comment author="s1monw" created="2014-10-10T19:44:54Z" id="58706591">oh I see - sorry for the confusion about the BackwardsCompatibility endpoint. @nkvoll for the case when you don't know the discovery nodes version you can just use `Version. CURRENT.minimumCompatibilityVersion()` which returns the minimal compatible version. This should work just fine in that case. 
</comment><comment author="nkvoll" created="2014-10-10T23:03:50Z" id="58726781">Ok @s1monw, I'll verify that it works for us either this weekend, or at latest, on Monday. Since we have a workaround already, I'm fine with closing this issue now if you wish to do so, and I leave it entirely up to you whether this behaviour needs to be noted somewhere else as a help for other developers that have latched onto the `UnicastHostsProvider`-interface and are instantiating `DiscoveryNode`s the same way we did. 

Thanks for the feedback and for letting me know about the `minimumCompatibilityVersion` method :)
</comment><comment author="s1monw" created="2014-10-11T06:48:32Z" id="58740372">I will add some javadocs to the ctors to clarify! Thanks for raising this issue
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file></files><comments><comment>[CORE] Simplify discovery node initialization if version is unknown</comment></comments></commit></commits></item><item><title>Circuit Breakers: Log if CircuitBreaker is tripping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8050</link><project id="" key="" /><description>today we only throw an exception which might not be logged at all. this PR adds debug logging if we tripping a CB. It's debug for now but I might even vote for info?
</description><key id="45464861">8050</key><summary>Circuit Breakers: Log if CircuitBreaker is tripping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Circuit Breakers</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-10T09:57:35Z</created><updated>2015-03-19T16:23:16Z</updated><resolved>2014-10-10T11:09:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-10T09:59:43Z" id="58635705">LGTM
</comment><comment author="dakrone" created="2014-10-10T10:00:33Z" id="58635792">LGTM, I vote for INFO level :)
</comment><comment author="s1monw" created="2014-10-10T10:03:42Z" id="58636089">updated the PR
</comment><comment author="dakrone" created="2014-10-10T10:06:18Z" id="58636336">LGTM
</comment><comment author="kimchy" created="2014-10-10T10:07:17Z" id="58636428">since this is a message that we already return to the user, and potentially can be repeated a lot, typically in ES we do those as debug messages. If we decide to change those logging levels, I prefer to do it across the board then to be inconsistent.

Btw, what I saw is that most of the times where "info" logging was requested, actually what was missing is adding it to our stats API.
</comment><comment author="s1monw" created="2014-10-10T10:13:24Z" id="58636983">I moved back to debug and removed the guard - I don't want this to be controversial 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/breaker/ChildMemoryCircuitBreaker.java</file><file>src/main/java/org/elasticsearch/common/breaker/MemoryCircuitBreaker.java</file></files><comments><comment>[CORE] Log if CircuitBreaker is tripping</comment></comments></commit></commits></item><item><title>Parse synonyms with the same analysis chain</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8049</link><project id="" key="" /><description>Change behavior synonym filter factory.
Now, synonyms are tokenized with the whitespace tokenizer or a tokenizer specified "tokenizer" parameter.
This PR is to tokenize synonyms with whatever tokenizer and token filters appear before it in the chain.
- change parsing synonyms from constructor to building custom analyzer
- clone SynonymTokenFilterFactory with the analysis chain in CustomAnalyzerProvider

Close #7199
</description><key id="45464546">8049</key><summary>Parse synonyms with the same analysis chain</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>enhancement</label><label>feature</label><label>review</label><label>v6.0.0</label></labels><created>2014-10-10T09:53:58Z</created><updated>2017-06-20T12:50:34Z</updated><resolved>2017-06-20T12:50:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-16T09:48:27Z" id="59338309">@rmuir could you review this please?
</comment><comment author="rjernst" created="2014-10-16T21:44:04Z" id="59435713">I like the idea here! But why does ES have its own synonym filter factory?  Seems almost the same as what is in Lucene.  It also seems like this would be something that should be there?  Right now the SynonymFilterFactory takes an optional "analyzer" setting, which is the name of an analyzer to load.  I think it could be modified to also have a setAnalyzer (along with the cloning like done here?).  The only real difference I see between the ES factory and lucene one is lucene actually loads the synonyms within the inform (so that it can have the classloader to find the named analyzer).
</comment><comment author="johtani" created="2014-10-17T15:18:34Z" id="59528405">Thanks for your comment.
I'm not sure exact reason, but I think that the reason is Inject and Settings classes. 

And I'm not sure when call inform method in Lucene.
Before create? Only once?
</comment><comment author="rmuir" created="2014-10-17T16:12:36Z" id="59536593">I think ryan's analysis is a bigger issue, that should not block the work here :)

Currently, the factories are duplication of sorts. But for now we at least have a test ensuring that we know things are in sync (https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/index/analysis/AnalysisFactoryTests.java)

As far as review of this specific patch, i am not the expert on the ES factory mechanism, but here are my two questions:
- can we try harder to make the 'analyzer for synonym parsing' a final member and initialized in ctor?
- is the back compat logic correct? if the user sets tokenizer, it seems to still use WhitespaceTokenizer.
</comment><comment author="johtani" created="2014-10-18T14:05:52Z" id="59614582">- a final member and initialized in ctr
  I think it is difficult. Factory is initialized before providing analysis chain.  http://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/analysis/AnalysisService.java#L152-152
  However, I will consider other option a little.
- the back compat logic
  It is completely my mistake... I will fix soon.
</comment><comment author="johtani" created="2014-10-20T06:02:59Z" id="59687072">Fix back compat.
</comment><comment author="clintongormley" created="2014-12-19T09:18:06Z" id="67614923">@johtani we should get this in for 2.0
</comment><comment author="markharwood" created="2015-03-24T10:16:47Z" id="85436016">Just downloaded to take a look @johtani and this has a compile error against master due to changes in Lucene's Analyzer in LUCENE-5388
</comment><comment author="johtani" created="2015-03-24T10:53:06Z" id="85448857">Oh... I will rebase and fix it.
</comment><comment author="johtani" created="2015-03-24T15:11:38Z" id="85544987">@markharwood Rebase and fix LUCENE-5388. please review again.
</comment><comment author="clintongormley" created="2015-06-04T19:23:20Z" id="109018823">@markharwood please could you review
</comment><comment author="markharwood" created="2015-06-08T16:55:59Z" id="110072552">Should the `ignore_case` setting be removed/deprecated too? I can see it was a requirement when we relied on WhitespaceTokenizer but if we shift to using the configured token stream it just becomes extra baggage?
</comment><comment author="johtani" created="2015-06-09T02:09:03Z" id="110198222">Thanks for your comment, @markharwood .
Good point.
I think we should `ignore_case` and `tokenizer` setting be deprecated.
I will add the description to document.
</comment><comment author="markharwood" created="2015-06-09T10:33:21Z" id="110308612">In testing I came across this - see https://gist.github.com/markharwood/a69d055e8027f2a8a865

Note the offsets and positions of the output synonyms - things start to go wrong when we inject overlapping and parallel streams of tokens as synonyms. 
There is no sensible way of representing multiple output synonyms of variable numbers of words and retaining a sensible notion of offset and position.
Each output synonym effectively creates an alternative branch which has a token numbering system that is incompatible with other branches.
Any sense of order (e.g. X follows Y) can no longer be reasoned about when we fork these alternative streams in a system designed to record only one sequence - it's a bit like trying to keep track of the plot of Back To The Future movies.
@clintongormley  Maybe we should assume that output synonyms always have to be single tokens?

Also, note in my Gist the change in behaviour of synonym entry "Cosmos" now being output as "cosmos". I guess a more common scenario would be an output token like `mp3_player` would typically now be output as `mp3` and `player`. This is potentially a breaking change in behaviour - do we 
a) document this breaking change in our migration guide or
b) consider what options are needed to maintain backwards compatibility?

I haven't seen enough synonym files in the wild to comment but I think these 2 points may be connected - people may reasonably expect something like output synonym `mp3_player` to be untokenized because  it avoids the danger of false matches on `player` and doesn't introduce alternative streams of ordering like the confusion time-travel adds to any movie plot.
</comment><comment author="markharwood" created="2015-06-09T13:05:48Z" id="110351373">Further on this - the wise old owl that is @mikemccand had this:

```
"you should ensure all of your injected synonyms are single tokens! " [1]
```

Maybe it is a mistake for us to introduce this change?

[1] http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html
</comment><comment author="rmuir" created="2015-06-09T13:08:39Z" id="110351999">I don't think its a mistake. Instead of the user worrying about two things:
1) putting synonymfilter in the correct place in the chain
2) configuring synonymfilter's own analysis process to be correct

Users just need to put synonymfilter in the correct place in their analysis chain. We can't fix the first here. But it removes the 2nd and makes it easier to use.
</comment><comment author="markharwood" created="2015-06-09T13:15:46Z" id="110353490">There's 3 token streams to consider:
1) The doc's terms
2) The synonym rule input condition terms
3) The synonym rule output terms

Agreed, it is convenient if 1) and 2) are made to talk the same language.
Disagree about the appropriateness of tokenization on 3) for the reasons Mike outlined in his blog.
</comment><comment author="rmuir" created="2015-06-09T13:21:36Z" id="110355217">I dont think the user should have to consider the third either. Again its just a matter of inserting the synonymfilter in the correct place in the analysis chain, so that both input and output processing makes sense.
</comment><comment author="rmuir" created="2015-06-09T13:25:37Z" id="110356270">Just think about it: if the synonyms don't go through the same output processing, then they are useless always (they will not match with expected terms because processing is different).

Mike's blog isn't really relevant here. Either you have synonyms in the analysis chain and they emit terms consistent with the way the analysis chain processes all text, or don't use synonymfilter.
</comment><comment author="markharwood" created="2015-06-09T14:05:55Z" id="110366004">```
Just think about it: if the synonyms don't go through the same output processing,
then they are useless 
```

Not always. In a normalization rule the output term doesn't have to be anything other than a single indexed token that represents a common concept mapped to various forms of end-user expressions e.g.

```
i pod, i-pod, mp3 player =&gt; SKU_3672
```

Here, end users will not search for SKU_3672 directly - tokenization in this case would be irrelevant and unhelpful.

The alternative use case is an expansion rule where we have 2 use cases - 
1) Index-time expansion of document terms (anticipating future user searches)
2) Query-time expansion of user search criteria

In the case of 1) Mike's blog is relevant and talks about introducing false positives in searches. I note the offsets in my GIST look screwed too so highlighting accuracy might be a concern [1] we need to fix here.
Given these issues and the general inflexibility of the index-time approach an end user might just as well consider using a normalization rule here?

In the case of 2) we can benefit from increased matching capabilities on previously untreated content but I wonder if these outputs are better expressed as actual query expressions if that is their intention? That brings the possibility of adding NOTs etc to the definition.

[1] Check out the reported offset of the word "device" in both master and this PR.
</comment><comment author="markharwood" created="2015-06-09T17:04:22Z" id="110434924">More detailed acknowledgement of the broken-ness to injecting multi-word synonyms: https://www.elastic.co/guide/en/elasticsearch/guide/current/multi-word-synonyms.html

```
"The way to avoid this mess is to use simple contraction to inject a single term that represents
all synonyms" 
```

With this in mind I'm not sure why we are choosing to switch to a policy that is likely to increase multi-word outputs and could break configurations where users have followed our advice and created single output tokens like 'MP3_player'. By moving away from whitespace tokenizer these tokens would now be split into multiple by the typical tokenizer used for document content.  
</comment><comment author="rmuir" created="2015-06-09T20:36:32Z" id="110497016">&gt; Here, end users will not search for SKU_3672 directly - tokenization in this case would be irrelevant and unhelpful.

this is esoteric usage.
</comment><comment author="markharwood" created="2015-06-09T20:54:07Z" id="110501271">```
this is esoteric usage.
```

Setting aside the issue of if the output token is likely to be in the end-user's vocabulary or not - single-token output was the recommended solution in both of the articles I referenced. Mike's given example output token was `dns` and Zach/Clint's was `usa`. My pessimistic assumption was not all concepts we would want to represent would be lucky enough to have a single-token representation that users would stand a chance of typing which is why I opted for something that resembled an internal primary key. 

The choice of symbol doesn't change the reasoning for why single-token output is being recommended here.
</comment><comment author="johtani" created="2015-06-19T00:24:50Z" id="113327068">I think multi-word synonym is related LUCENE-6582 https://issues.apache.org/jira/plugins/servlet/mobile#issue/LUCENE-6582
</comment><comment author="johtani" created="2015-06-22T05:34:46Z" id="114010181">@markharwood @rmuir 
Thanks for your discussion.
I think multi-word synonym is relate LUCENE-6582.

And I think "Cosmos" and "mp3_player" is related `tokenizer` option.
If user needs backwards compatibility, then user set `tokenizer: "whitespace"`.
I should to add this to migration guide, right?
</comment><comment author="markharwood" created="2015-06-22T15:11:37Z" id="114146805">Good to see there's some upstream activity in Lucene to tackle the multi-word concerns. 

+1 for documenting `tokenizer: "whitespace"` in the migration guide.
</comment><comment author="johtani" created="2015-06-23T03:19:09Z" id="114339522">Added to the migration guide
</comment><comment author="johtani" created="2015-07-29T10:09:39Z" id="125906433">Related another Lucene ticket.
https://issues.apache.org/jira/browse/LUCENE-6664
</comment><comment author="clintongormley" created="2015-11-21T16:46:40Z" id="158661433">@johtani Where are we with this PR?
</comment><comment author="clintongormley" created="2016-03-08T18:57:57Z" id="193914970">@johtani can we get this in for 5.0 please?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/AnalysisRegistry.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/CustomAnalyzerProvider.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SynonymGraphTokenFilterFactory.java</file><file>core/src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>core/src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTests.java</file><file>core/src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionIT.java</file></files><comments><comment>Parse synonyms with the same analysis chain (#8049)</comment></comments></commit></commits></item><item><title>Request feature: BSON support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8048</link><project id="" key="" /><description>Please add BSON format with Elasticsearch API because is faster than JSON 
</description><key id="45461822">8048</key><summary>Request feature: BSON support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thangman22</reporter><labels><label>discuss</label></labels><created>2014-10-10T09:20:45Z</created><updated>2015-11-21T19:40:58Z</updated><resolved>2015-11-21T19:40:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-20T10:56:52Z" id="75220840">@thangman22 Do you need BSON support specifically? The reason why I am asking is that we already support some binary json formats such as Smile.
</comment><comment author="kimchy" created="2015-02-20T11:45:51Z" id="75226227">we also support CBOR
</comment><comment author="marcustconnolly" created="2015-05-12T16:10:05Z" id="101332756">Hi there,I just found this page as I am wondering about the same thing. I am looking to save our MongoDB audit logs to BSON as it is supposed to be much for performant than JSON. We are looking to use the ELK Stack for all our logging and monitoring of logs, and not sure it can parse BSON at this time?
</comment><comment author="clintongormley" created="2015-05-15T17:19:40Z" id="102462001">Hi @marcustconnolly 

As long as BSON is not supported by Jackson core, it is unlikely that we will add support for it.
</comment><comment author="clintongormley" created="2015-11-21T19:40:58Z" id="158676901">We have no plans to support BSON. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix snapshotting of a single closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8047</link><project id="" key="" /><description>Snapshot of a closed index can leave snapshot hanging in initializing state.

Fixes #8046
</description><key id="45436717">8047</key><summary>Fix snapshotting of a single closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-10T01:19:45Z</created><updated>2015-06-08T00:37:51Z</updated><resolved>2014-10-15T23:28:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-13T10:33:37Z" id="58873920">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot of a closed index can leave snapshot hanging in initializing state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8046</link><project id="" key="" /><description>To reproduce close and index and start snapshot by specifying the index name explicitly in the list of indices. 
</description><key id="45433693">8046</key><summary>Snapshot of a closed index can leave snapshot hanging in initializing state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label></labels><created>2014-10-10T00:26:10Z</created><updated>2014-10-15T23:28:45Z</updated><resolved>2014-10-15T23:28:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: fix snapshot of a single closed index</comment></comments></commit></commits></item><item><title>Updates to threadpool docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8045</link><project id="" key="" /><description>Someone questioned the existing doc on IRC as they didn't find it clear.

I've done some minor updates to clarify and elaborate what the queues are in relation to, as well as pad out the explanation of the threadpool calculations.
</description><key id="45428224">8045</key><summary>Updates to threadpool docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2014-10-09T23:00:15Z</created><updated>2014-10-16T18:19:39Z</updated><resolved>2014-10-16T18:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T20:05:37Z" id="59267162">Hiya @markwalkom 

Did you close this on purpose?  I've reopened, assuming it was a mistake. Let me know :)
</comment><comment author="markwalkom" created="2014-10-15T21:08:57Z" id="59276687">Yeah I did as it (re)pushed my other directory changes from a few months back.

I just haven't gotten round to resubmitting it.
</comment><comment author="clintongormley" created="2014-10-16T18:19:39Z" id="59405995">OK Closing again :) thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exception should be thrown if trying to enable ordinals on numeric/date field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8044</link><project id="" key="" /><description>Only string fields can generate eager global ordinals, but the mapper parser will happily accept the setting for numerics and dates.  This only becomes apparent in the logs later, when ES spams a bunch of warnings about being unable to convert the numeric into an ordinal.

It would be better if the mapper parser threw an exception up front so the user knew it was not a valid setting.

E.g.

``` bash
PUT /my_index/_mapping/my_type
{
   "my_type":{
      "properties":{
         "my_double_field":{
            "type":"double",
            "fielddata":{
               "loading":"eager_global_ordinals"
            }
         }
      }
   }
}
```

```
{
   "acknowledged": true
}
```

And later in the logs:

```
[WARN ][index.warmer ] [prod-box] [my_index][5] failed to warm-up global ordinals for [my_double_field] 
java.lang.ClassCastException: org.elasticsearch.index.fielddata.plain.DoubleArrayIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexFieldData$WithOrdinals 
at org.elasticsearch.search.SearchService$FieldDataWarmer$3.run(SearchService.java:869) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
at java.lang.Thread.run(Thread.java:745)
```
</description><key id="45427059">8044</key><summary>Exception should be thrown if trying to enable ordinals on numeric/date field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-10-09T22:45:29Z</created><updated>2016-08-25T10:08:22Z</updated><resolved>2016-08-24T15:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="govindm" created="2014-11-09T06:05:48Z" id="62292936">Is this a bug or a limitation?
As per the document it is possible to enable gloabl ordinals on integer fields.

http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/preload-fielddata.html
</comment><comment author="missinglink" created="2015-07-16T16:44:06Z" id="122016396">I'm also seeing `[2015-07-16 15:41:25,997][WARN ][index.warmer             ] [Omerta] [pelias][0] failed to warm-up global ordinals for [center_point]` for a `geo_point` type.

ref: https://github.com/pelias/schema/blob/master/mappings/partial/centroid.js

We're also seeing a very CPU happy cluster with not much indexing going on, is this possibly related?
</comment><comment author="jpountz" created="2016-08-24T15:32:07Z" id="242107987">Fixed in master: the mapping is checked when parsed and global ordinals on numerics are rejected.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch 1.4.0.Beta1 fails to start if Marvel plugin is installed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8043</link><project id="" key="" /><description>I just upgraded from 1.3.2 to 1.4.0.Beta1 on one of our test servers and noticed that ES would fail to start if the Marvel plugin was installed with the following error:

[2014-10-09 20:32:52,235][INFO ][node                     ] [indx] version[1.4.0.Beta1], pid[14476], build[1f25669/2014-10-01T14:58:15Z]
[2014-10-09 20:32:52,236][INFO ][node                     ] [indx] initializing ...
[2014-10-09 20:32:52,252][INFO ][plugins                  ] [indx] loaded [marvel], sites [marvel]
{1.4.0.Beta1}: Initialization Failed ...
1) Tried proxying org.elasticsearch.discovery.DiscoveryService to support a circular dependency, but it is not an interface.2) IllegalStateException[This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.]

Remove the plugin, restart ES, and ES runs just fine.
Marvel was running fine on this system with ES 1.3.2.

Linux indx 3.12.13-gentoo_radar_2 #1 SMP Thu Mar 13 05:00:54 UTC 2014 x86_64 AMD Opteron(tm) Processor 6378 AuthenticAMD GNU/Linux
java version "1.7.0_45"
OpenJDK Runtime Environment (IcedTea 2.4.3) (Gentoo build 1.7.0_45-b31)
OpenJDK 64-Bit Server VM (build 24.45-b08, mixed mode)
Latest Marvel
</description><key id="45415043">8043</key><summary>Elasticsearch 1.4.0.Beta1 fails to start if Marvel plugin is installed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bbailey1024</reporter><labels /><created>2014-10-09T20:42:36Z</created><updated>2014-10-13T15:14:33Z</updated><resolved>2014-10-13T15:14:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-10T06:40:14Z" id="58617932">Hi @bbailey1024 , which version of marvel do you have installed? you can look at the plugin/marvel/ folder and check the jar file for a version.
</comment><comment author="bbailey1024" created="2014-10-10T12:15:27Z" id="58647408">Running marvel-1.2.1.jar
</comment><comment author="bleskes" created="2014-10-13T07:51:55Z" id="58857823">@bbailey1024 thx for checking. I currently have no problem with 1.4.0.Beta1 and 1.2.1, which doesn't mean they don't exist but it does mean it's subtle.

How did you install marvel? Can you post your elasticsearch.yml file as well?
</comment><comment author="bbailey1024" created="2014-10-13T15:14:26Z" id="58906625">Marvel was installed using the $ES_HOME/bin/plugin -i marvel -u file:///path/to/marvel-latest.zip

elasticsearch.yml:
cluster.name: name1
node.name: name
node.master: true
node.data: true
node.local: true
index.number_of_shards: 1
index.number_of_replicas: 0
path.conf: /etc/elasticsearch
path.data: /path/to/elasticsearch/data
path.work: /path/to/elasticsearch/work
path.logs: /path/to/elasticsearch/logs
path.plugins: /usr/share/elasticsearch/plugins
bootstrap.mlockall: true
network.host: x.x.x.x
transport.tcp.port: 9300
http.port: 9200
marvel.agent.exporter.es.hosts: ["x.x.x.x:9200"]
action.destructive_requires_name: true

However, I believe I understand what the problem might relate to. This dev server was running 1.3.2. The server was moved to 1.4.0.Beta1, then moved back to 1.3.2. This generated errors due to a missing Lucene jar file (obviously). Then the developer upgraded again to 1.4.0.Beta1. It is at this point that we started experiencing this issue.

I would imagine that this flawed process resulted in the anomalous behavior. I had thought this was a fresh install at the time when I opened this ticket. We've removed the old indexes and started fresh and all works well. As it was a dev server it was easier to just remove everything and start over. Since then we cannot recreate the issue or have a test case to work with. Chalk it up to user error perhaps.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations order : Escaping the user defined AGG_NAME</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8042</link><project id="" key="" /><description>The order by path has the form defined as  -
AGG_SEPARATOR       :=  '&gt;'
METRIC_SEPARATOR    :=  '.'
AGG_NAME            :=  &lt;the name of the aggregation&gt;
METRIC              :=  &lt;the name of the metric (in case of multi-value metrics aggregation)&gt;
PATH                :=  &lt;AGG_NAME&gt;[&lt;AGG_SEPARATOR&gt;&lt;AGG_NAME&gt;]*[&lt;METRIC_SEPARATOR&gt;&lt;METRIC&gt;]

Since AGG_NAME can contain dots (.) which is the METRIC_SEPARATOR, is it possible to somehow quote the AGG_NAME ?

A similar problem would arise if the multi-value metric aggregation name is a part of AGG_NAME
</description><key id="45413139">8042</key><summary>Aggregations order : Escaping the user defined AGG_NAME</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aaneja</reporter><labels><label>discuss</label></labels><created>2014-10-09T20:27:45Z</created><updated>2015-11-21T19:38:59Z</updated><resolved>2015-11-21T19:38:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aaneja" created="2014-10-09T20:43:14Z" id="58574407">I took a quick look at src/main/java/org/elasticsearch/search/aggregations/support/OrderPath.java and it doesn't look like there's any code to escape/quote parts of the order path.
</comment><comment author="clintongormley" created="2014-10-15T20:03:38Z" id="59266887">Related to #6736 
</comment><comment author="colings86" created="2015-02-20T11:01:22Z" id="75221369">closing in favour of #9059 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Node Name to _cat/recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8041</link><project id="" key="" /><description>We have multiple data nodes, on a single server and when using _cat/recovery it could be useful to not only display the Host Name, but also the Node Name, so we can identify which node is in recovery process.
</description><key id="45410371">8041</key><summary>Add Node Name to _cat/recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">ghoumard</reporter><labels><label>:CAT API</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-10-09T20:01:29Z</created><updated>2016-05-06T15:20:27Z</updated><resolved>2016-05-06T15:20:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file></files><comments><comment>Cat Recovery API: Reverting changes introduced with commit e1c75bae87300f5fd59c9a455f9a89298688df49</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file></files><comments><comment>Cat API: Add node name to _cat/recovery</comment></comments></commit><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file></files><comments><comment>Add node name to Cat Recovery</comment></comments></commit></commits></item><item><title>logging config in a svn working copy fails to load with elasticsearch 1.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8040</link><project id="" key="" /><description>elasticsearch 1.3 will try to import .svn/text-base/logging.yml.text-base as json and fail to set up logging as a whole.
I think it's not reproducible with subversion 1.7+ since a new .svn-format with hash-based filenames is used. But Debian wheezy/stable is still using subversion 1.6 and is what we use.
I found https://github.com/elasticsearch/elasticsearch/blob/1.3/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java#L107 which is the problematic part.
imo it should at least skip "hidden" directories, if not be limited to some more strict rules (file extension whitelist without the "catch-all" default, ...).

I thought of only ignoring the failed file, but that could lead to strange errors too. 
What if you have a logging.json. Then you would have a .svn/text-base/logging.json.text-base, which could import and override the original logging.json (local modified logging.json). =&gt; quite troublesome to find out why the changes don't work until you commit them :)
##### Known workarounds (also see https://groups.google.com/forum/#!topic/elasticsearch/8zlwEj12DjI):
1. Add "copy/move to logging.yml" command before startup (so that the file in .svn doesn't start with "logging.")
2. mount an empty directory over .svn (tmpfs, ...). Umount/mount if needed to work with svn &lt;= that's the solution we use, since we mount it readonly over nfs and cp/mv doesn't work

I think both workarounds are quite dirty hacks and you shouldn't be forced to use those, just because you use a vcs.
</description><key id="45405669">8040</key><summary>logging config in a svn working copy fails to load with elasticsearch 1.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thetuxkeeper</reporter><labels><label>bug</label></labels><created>2014-10-09T19:15:36Z</created><updated>2014-11-12T16:06:27Z</updated><resolved>2014-11-12T16:05:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lifeforms" created="2014-10-14T20:31:17Z" id="59111260">We are seeing the same problem.

In our case, it turned out that `logging.yml.sample` is now being interpreted as a JSON file. (The `.sample` file is dropped by the FreeBSD package, as per convention)

This leads to the following startup error, which was initially tricky to diagnose... as logging didn't work :)

`org.elasticsearch.common.settings.SettingsException: Failed to load settings from [file:/usr/local/etc/elasticsearch/logging.yml.sample] [...] Caused by: org.elasticsearch.common.jackson.core.JsonParseException: Unexpected character ('#' (code 35)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')`

I feel that walking the tree and using _all_ `logging.*` files will invite a lot of unexpected problems for others in the future too. For instance, think of someone editing the file and keeping the original in `logging.yml.bak`, which now also is parsed as JSON and causes the same startup failure.
</comment><comment author="javanna" created="2014-11-12T16:05:46Z" id="62742409">This is being addressed in #7457. The plan would be to load only `logging.*.yml`, `logging.*.yaml`, `logging.*.json` and `logging.*.properties`, but keep throwing exception if the parsing of one of the files is problematic. How does that sound? 
Closing in favour of #7457.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add versatile 'like' parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8039</link><project id="" key="" /><description>The MLT query has a lot of parameters. For example, a set of documents is
specified with either `like_text`, `ids` or `docs`, with at least one
parameter required. This commit groups all the document specification
parameters under one called `like`. The syntax is described below and could
easily be extended to allow for new means of specifying document input. The
`like_text`, `ids` and `docs` parameters are deprecated.

As a single piece text:

``` json
{
  "query": {
    "more_like_this": {
      "like": "some text here"
    }
  }
}
```

As a single item:

``` json
{
  "query": {
    "more_like_this": {
      "like": {
        "_index": "imdb",
        "_type": "movies",
        "_id": "88247"
      }
    }
  }
}
```

Or as a mixture of all:

``` json
{
  "query": {
    "more_like_this": {
      "like": [
        "Some random text ...",
        {
          "_index": "imdb",
          "_type": "movies",
          "_id": "88247"
        },
        {
          "_index": "imdb",
          "_type": "movies",
          "doc": {
            "title": "Document with an artificial title!"
          }
        }
      ]
    }
  }
}
```
</description><key id="45393729">8039</key><summary>Add versatile 'like' parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-09T17:26:41Z</created><updated>2015-06-07T11:59:01Z</updated><resolved>2014-10-25T09:21:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-09T19:50:42Z" id="58566887">I like this thsi looks much better! Can we maybe have some comments next to the parse fields which of them are deprecated?
</comment><comment author="s1monw" created="2014-10-13T10:57:38Z" id="58875900">LGTM - I wonder if we should wait with this a bit and refactor `ParseField` to support field names where all of them are deprecated. I'd love to be able to pass something like this:

``` Java
ParseField LIKE_TEXT = new ParseField("like_text", true /* deprecated */, "like" /* the actual field to use instead */);
```

such that we can barf with the strict parse option? (note my example might not be the best way to implement it)
</comment><comment author="alexksikes" created="2014-10-13T14:28:08Z" id="58899551">OK I've just opened https://github.com/elasticsearch/elasticsearch/pull/8067
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/MoreLikeThisQueryParser.java</file></files><comments><comment>MLT Query: versatile 'like' parameter</comment></comments></commit></commits></item><item><title>Query logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8038</link><project id="" key="" /><description>When debugging some GC issues, it was really useful to correlate what queries were being run at the time of the GC's, by abusing the slowlog and some ELK magic.

It would be great to have the ability to log all queries out of the box in a more obvious way (eg. index.search.log_all: true in elasticsearch.yml). Maybe even in the future Marvel could allow viewing of queries alongside cluster stats as generally it's a query/index event that caused some kind of effect.
</description><key id="45377220">8038</key><summary>Query logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jimmyjones2</reporter><labels><label>discuss</label></labels><created>2014-10-09T15:05:38Z</created><updated>2014-10-13T07:57:22Z</updated><resolved>2014-10-13T07:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-10T10:02:51Z" id="58636008">&gt; When debugging some GC issues, it was really useful to correlate what queries were being run at the time of the GC's, by abusing the slowlog and some ELK magic.

IMHO this is not abusing but part of the slowlog valid usages. During GC queries will be slow and will be logged.

I don't think it's realistic to log all queries all the time waiting for a GC to happen. You are better off configure some threshold and see what goes above it.

FYI - GC can come for other things, not necessarily query related, like norms, swapping , big term dictionaries etc.
</comment><comment author="jimmyjones2" created="2014-10-10T10:25:50Z" id="58638082">Oddly enough my queries weren't running too slowly, but caused a GC a short period later (as far as I could make out). GC is a bit of a beast I don't really understand to be honest, but thanks for the tips.

However, in general, I think it would be nice to be able to flick a simple switch to log all queries, then compare with IO/CPU/latency spikes, evictions etc to help find the culprit. But that's just my 2-cents!
</comment><comment author="bleskes" created="2014-10-13T07:57:22Z" id="58858402">&gt;  But that's just my 2-cents!

For sure - thank you!

The feeling is that as logging all queries has unexpected performance and disk space implications, it is best kept as an expert setting (i.e., set slow log to 0) rather than a very easy switch.

I'm closing this issue now, but if you rather keep it open (or someone else does) please feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes scripted metrics aggregation when used as a sub aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8037</link><project id="" key="" /><description>The scripted metric aggregation is now a PER_BUCKET aggregation so that parent buckets are evaluated independently. Also the params and reduceParams are copied for each instance of the aggregator (each parent bucket) so modifications to the values are kept only within the scope of its parent bucket

Closes #8036
</description><key id="45359402">8037</key><summary>Fixes scripted metrics aggregation when used as a sub aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-09T12:19:41Z</created><updated>2015-06-07T17:50:07Z</updated><resolved>2014-10-10T08:18:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-10T07:49:56Z" id="58623139">LGTM
</comment><comment author="diegoasth" created="2014-10-10T18:23:04Z" id="58696389">I am already using it. Thank you guys very much.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: scripted metric agg does not separate parent buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8036</link><project id="" key="" /><description>The script metric aggregation does not separate script scopes based on the bucketOrd so when using it as a sub aggregation the results are given for all parent buckets combined rather than each parent bucket separately

See the following comment for more details:
https://github.com/elasticsearch/elasticsearch/pull/7075#issuecomment-58403364
</description><key id="45359332">8036</key><summary>Aggregations: scripted metric agg does not separate parent buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-09T12:18:48Z</created><updated>2014-10-10T08:18:24Z</updated><resolved>2014-10-10T08:18:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/ScriptedMetricTests.java</file></files><comments><comment>Aggregations: Fixes scripted metrics aggregation when used as a sub aggregation</comment></comments></commit></commits></item><item><title>Aggregations: Memory-bound terms.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8035</link><project id="" key="" /><description>This pull request is only a first building block for #6697. It doesn't change
anything and is not ready for review yet.

Utility classes to compute the top terms sorted either by term or count in a
streaming fashion. In both cases, we rely on priority queues.

Sorting by term produces accurate results. Sorting by count (only desc is
supported, not asc) is based on the space-saving algorithm:
https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf
and is not fully accurate.
</description><key id="45358471">8035</key><summary>Aggregations: Memory-bound terms.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>WIP</label></labels><created>2014-10-09T12:08:34Z</created><updated>2015-09-11T09:26:11Z</updated><resolved>2015-09-11T09:26:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Script support to convert alias to real index name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8034</link><project id="" key="" /><description>Currently when a new user comes in , i need to run an alias telling map &lt;Person&gt;-configIndex to configIndex with the filter action

```
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "index" : "userInfo",
                 "alias" : "alex-userinfo",
                 "filter" : { "term" : { "user" : "alex" } }
            }
        }
    ]
}'
```

Instead , i would like to give

```
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "indexNameScript" : "_value.split('-')[1]",
                 "alias" : "alex-userinfo",
                 "filter" : { "script" : { "script" : ""_value.split('-')[0]" } }
            }
        }
    ]
}'
```
</description><key id="45351983">8034</key><summary>Script support to convert alias to real index name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>discuss</label></labels><created>2014-10-09T10:50:04Z</created><updated>2015-11-21T19:36:14Z</updated><resolved>2015-11-21T19:36:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T19:36:14Z" id="158676673">This wouldn't work without a big change to aliases, as aliases are stored in the index config.  Currently, we have no plans to implement something like this, so I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tests: dump all thread stacks on failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8033</link><project id="" key="" /><description>Sometimes when a test fails, its logs are not necessarily sufficient to determine what threads were doing at the time ... this change will dump all threads whenever there is a failure either in the main test case itself, or in the cleanup we do after the test succeeds.
</description><key id="45346426">8033</key><summary>Tests: dump all thread stacks on failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>test</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-09T09:43:55Z</created><updated>2015-03-19T09:37:50Z</updated><resolved>2014-10-09T14:25:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-09T12:04:35Z" id="58499253">that's going to be pretty verbose... but lets see how it turns out. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>EL 1.4-BETA - fatal error on the 'save visualisation'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8032</link><project id="" key="" /><description>Bombs when trying to create a new visualization. Code seems to be saying the visualization was unnamed, however I had entered a name. Retried the process a few times under different condition. All resulted in the same error

Cannot read property 'byName' of undefined
TypeError: Cannot read property 'byName' of undefined
    at BaseAggParam.FieldAggParamFactory.FieldAggParam.deserialize (http://localhost:5601//index.js:103023:47)
    at http://localhost:5601//index.js:104513:28
    at Array.forEach (native)
    at AggConfigFactory.AggConfig.fillDefaults (http://localhost:5601//index.js:104502:27)
    at new AggConfig (http://localhost:5601//index.js:104487:12)
    at http://localhost:5601//index.js:104620:18
    at Array.map (native)
    at Registry.AggConfigs (http://localhost:5601//index.js:104618:42)
    at VisFactory.Vis.setState (http://localhost:5601//index.js:104757:19)
    at new Vis (http://localhost:5601//index.js:104706:12)
</description><key id="45344574">8032</key><summary>EL 1.4-BETA - fatal error on the 'save visualisation'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Tomlox</reporter><labels /><created>2014-10-09T09:23:35Z</created><updated>2014-10-09T09:31:31Z</updated><resolved>2014-10-09T09:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-09T09:29:18Z" id="58484313">I think you should report this in [Kibana repo](https://github.com/elasticsearch/kibana).

It's not really an elasticsearch error, right?

Closing. Feel free to reopen if you think it's caused by elasticsearch 1.4.0.Beta1
</comment><comment author="Tomlox" created="2014-10-09T09:31:31Z" id="58484571">Yeah sorry I'll move it
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE due to delete-by-query with parent/child when upgrading from 1.1.1 to 1.3.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8031</link><project id="" key="" /><description>An NPE was encountered when upgrading from 1.1.1 to 1.3.4.  During the rolling upgrade, a background cron tried to execute a delete-by-query which included a parent/child query.  This was allowed in 1.1.1, but [disabled in later versions](https://github.com/elasticsearch/elasticsearch/pull/5916).

This caused a delete-by-query to queue up in the translog of a 1.1.1 node.  Before the translog was cleared, the shard tried to move to a 1.3.4 node, which caused an NPE.  The shards repeatedly failed recovery and kept bouncing around the cluster.  Because allocation filtering was being used to migrate data from old -&gt; new, the cluster tried to recover the shards on only 1.3.4 nodes...leading to a continuous failure.

The situation eventually resolved itself, likely because a background flush cleared out the translog and allowed the recovery to finally proceed normally.

Stack trace (sanitized to remove sensitive names/ips):

```

[2014-10-08 21:43:26,881][WARN ][indices.cluster          ] [prod-1.3.4] [my_index][6] failed to start shard
org.elasticsearch.indices.recovery.RecoveryFailedException: [my_index][6]: Recovery failed from [prod-1.1.1][YhcqkTzLTGSF8dyKAQPRBQ][prod-1.1.1.localdomain][inet[...]]{aws_availability_zone=us-east-1e, max_local_storage_nodes=1} into [prod-1.3.4][0cRcLbzTTAm15PMu_R_U2w][prod-1.3.4.localdomain][inet[prod-1.3.4.localdomain/...]]{aws_availability_zone=us-east-1e, max_local_storage_nodes=1}
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:306)
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$200(RecoveryTarget.java:65)
    at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:175)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [prod-1.1.1][inet[/...]][index/shard/recovery/startRecovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [my_index][6] Phase[2] Execution failed
    at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1109)
    at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:627)
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
    at org.elasticsearch.indices.recovery.RecoverySource.access$1600(RecoverySource.java:61)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:337)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:323)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [prod-1.3.4][inet[/...]][index/shard/recovery/translogOps]
Caused by: org.elasticsearch.index.query.QueryParsingException: [my_index] Failed to parse
    at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:330)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:449)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:780)
    at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:431)
    at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:410)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.query.QueryParserUtils.ensureNotDeleteByQuery(QueryParserUtils.java:36)
    at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:52)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:302)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:283)
    at org.elasticsearch.index.query.NotFilterParser.parse(NotFilterParser.java:63)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:302)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:283)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:239)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:342)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:268)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:263)
    at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:314)
    ... 8 more
```
</description><key id="45311864">8031</key><summary>NPE due to delete-by-query with parent/child when upgrading from 1.1.1 to 1.3.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>bug</label></labels><created>2014-10-08T23:51:59Z</created><updated>2014-10-29T08:18:12Z</updated><resolved>2014-10-22T07:51:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-09T07:28:21Z" id="58472651">This is bad. First of all a the actual exception should be a `QueryParsingException` with the message the p/c queries are unsupported in the delete by query api and second I think the translog should just skip a operation if it fails with a QueryParsingException.
</comment><comment author="s1monw" created="2014-10-09T16:06:23Z" id="58533006">@martijnvg can we somehow reproduce this with bwc test? just curious.... I think we should work on something with @dakrone to be able to skip individual operations in the translog... might be even a standalone tool? @dakrone any ideas?
</comment><comment author="martijnvg" created="2014-10-09T16:07:08Z" id="58533100">@s1monw I'm sure that this can be reproduced in a bwc test :)
</comment><comment author="clintongormley" created="2014-10-15T19:55:47Z" id="59265689">@martijnvg assigned this to you, but perhaps @dakrone is the person best placed to look at this?
</comment><comment author="martijnvg" created="2014-10-21T12:15:02Z" id="59918092">This issue is less severe as I initially thought. What it boils down to is that any delete by query translog operation with a p/c query is just ignored, but the rest of all translog operations are successfully executed and the shard gets assigned.

The NPE is annoying (which I will fix) but that gets wrapped by a QueryParsingException (in IndexQueryParserService#parseQuery(...) line 370) and because of this in LocalIndexShardGateway#recover(...) at line 276 we ignore the delete by query operation. A QueryParsingException exception status is seen as bad request, so the idea here is to ignore it.
</comment><comment author="martijnvg" created="2014-10-21T12:21:09Z" id="59918754">I opened this PR for the NPE during recovery: #8177
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryParserUtils.java</file><file>src/test/java/org/elasticsearch/bwcompat/ParentChildDeleteByQueryBackwardsCompatibilityTest.java</file></files><comments><comment>Parent/child: Check if there is a search context, otherwise throw a query parse exception.</comment></comments></commit></commits></item><item><title>Inconsistent query results when an index type matches a document property object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8030</link><project id="" key="" /><description>Rather than try to explain the entire issue in English, please see the small test case below.
### Test Case

**Setup**

``` bash
~ elasticsearch -v
Version: 1.3.4, Build: a70f3cc/2014-09-30T09:07:17Z, JVM: 1.8.0_20
~ curl -sX POST 127.0.0.1:9200/test | jq .
{
  "acknowledged": true
}
~ curl -sX POST 127.0.0.1:9200/test/genre -d '{"name":"foo"}' | jq .
{
  "_index": "test",
  "_type": "genre",
  "_id": "69IFvhCNQpme5yq0zJH9gA",
  "_version": 1,
  "created": true
}
~ curl -sX POST 127.0.0.1:9200/test/movie -d '{"genre":{"name":"foo"}}' | jq .
{
  "_index": "test",
  "_type": "movie",
  "_id": "ZVANN7bHSY-d8GEqnPtvKA",
  "_version": 1,
  "created": true
}
```

**Unexpected Results A**

I would expect the `movie` document to be returned here as well because it has a property `genre` with the name `foo`, but the `genre` document is the only thing returned.

``` bash
~ curl -s 127.0.0.1:9200/test/_search?q=name:foo+OR+genre.name:foo | jq .
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1.4142135,
    "hits": [
      {
        "_index": "test",
        "_type": "genre",
        "_id": "69IFvhCNQpme5yq0zJH9gA",
        "_score": 1.4142135,
        "_source": {
          "name": "foo"
        }
      }
    ]
  }
}
```

**Unexpected Results B**

I tried adding the types in the URL to be explicit and now the `movie` result shows up. Unfortunately, now the previous `genre` result is not being returned.

``` bash
~ curl -s 127.0.0.1:9200/test/movie,genre/_search?q=name:foo+OR+genre.name:foo | jq .
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1.4142135,
    "hits": [
      {
        "_index": "test",
        "_type": "movie",
        "_id": "ZVANN7bHSY-d8GEqnPtvKA",
        "_score": 1.4142135,
        "_source": {
          "genre": {
            "name": "foo"
          }
        }
      }
    ]
  }
}
```

**Expected Results**

This is where it gets fun. By simply switching the order of `genre` and `movie` in the types list in the URL, the expected results are returned. But this seems crazy...right?

``` bash
~ curl -s 127.0.0.1:9200/test/genre,movie/_search?q=name:foo+OR+genre.name:foo | jq .
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 0.35355338,
    "hits": [
      {
        "_index": "test",
        "_type": "genre",
        "_id": "69IFvhCNQpme5yq0zJH9gA",
        "_score": 0.35355338,
        "_source": {
          "name": "foo"
        }
      },
      {
        "_index": "test",
        "_type": "movie",
        "_id": "ZVANN7bHSY-d8GEqnPtvKA",
        "_score": 0.35355338,
        "_source": {
          "genre": {
            "name": "foo"
          }
        }
      }
    ]
  }
}
```
</description><key id="45308892">8030</key><summary>Inconsistent query results when an index type matches a document property object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">caseywebdev</reporter><labels /><created>2014-10-08T23:10:06Z</created><updated>2014-10-15T19:58:17Z</updated><resolved>2014-10-15T19:58:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hadronzoo" created="2014-10-08T23:42:12Z" id="58445209">+1 I'm having the same issue
</comment><comment author="rjernst" created="2014-10-09T07:45:07Z" id="58474058">I debugged this for a while and I think the issue is that in the MapperService, when finding the mapping to use for a field that was parsed from the query the order the types are iterated over matters here.  So when we get to parsing the second clause of the query, the `movie` mapping is first checked, and it finds there is a mapping for `genre.name`, which has a simple name of just `name`, and that matches the field of the second clause.  The query actually ends up being `+genre.name:foo +genre.name:foo`.
</comment><comment author="caseywebdev" created="2014-10-09T14:13:13Z" id="58515071">@rjernst can you think of any workarounds other than renaming the type or property name? FWIW this problem exists on 1.1 and 1.4beta1 as well =\
</comment><comment author="caseywebdev" created="2014-10-09T19:03:09Z" id="58560495">For anyone else encountering this, I've found a stopgap with `copy_to`.

``` bash
curl -sX PUT 127.0.0.1:9200/test/movie/_mapping -d '{
  "movie": {
    "properties": {
      "genre": {
        "properties": {
          "name": {"type": "string", "copy_to": "genre_name"}
        }
      }
    }
  }
}'
```

And then querying over `genre_name` instead of `genre.name`. Definitely not ideal, but better than nothing.
</comment><comment author="clintongormley" created="2014-10-15T19:58:17Z" id="59266079">Bah - I'm looking forward to the day that we implement #4081 to resolve all of this.  I'm going to close this in favour of #4081 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Removes documentation escape character</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8029</link><project id="" key="" /><description>The escape character `\` in the `curl` example doesn't work and should be removed from the documentation.
</description><key id="45297834">8029</key><summary>Removes documentation escape character</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikringsmuth</reporter><labels /><created>2014-10-08T21:13:56Z</created><updated>2014-10-15T19:42:57Z</updated><resolved>2014-10-15T19:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T19:42:37Z" id="59263738">thanks @erikringsmuth - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Removing escape character</comment></comments></commit></commits></item><item><title>Fixes many misbehaving user parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8028</link><project id="" key="" /><description>Previously, the MLT API would create one MLT query per field and per value.
This would make the parameters related to the term selection and query
formation such as `max_query_terms`, `min_term_freq`, `minimum_should_match`
(previously `percent_terms_to_match`) or `boost_terms` behave in an unexpected
manner. Let's take the common example of looking up similar documents with
respect to a list of tag names. Suppose these tags are modeled by a multi-
value field with a keyword analyzer. Performing a MLT request would therefore
result in one MLT query per tag, regardless of the value of `max_query_terms`
or `minimum_should_match`. This would result in a query made of all the tag
names, if `min_term_freq` = 1 (no actual selection of terms is taking place),
or zero tag whatsoever, if `min_term_freq` &gt; 1 (note the default is 2). The
`boost_terms` parameter would also have unexpected effects as it would depend
on the frequency of the term within field value and, again, not within the
whole field.

This commit fixes these issues by calling upon the term vector API and by
directly passing the response (the terms) to the MoreLikeThisQueryParser. Now
both the API and the query yield exactly the same results under any given set
of parameters, but while keeping the added benefit for the API of calling upon
the TV API only once.

Closes #2914
</description><key id="45297098">8028</key><summary>Fixes many misbehaving user parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-10-08T21:07:25Z</created><updated>2015-06-07T18:37:28Z</updated><resolved>2015-04-15T18:40:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-09T20:02:18Z" id="58568523">left some comments but I like the idea simplifies the thing a bit :)
</comment><comment author="alexksikes" created="2014-10-11T15:29:43Z" id="58753374">I have switched to using dummy Fields. However, there is an easier way which would consist of making a TermVectorResponse object from the parsed json. Maybe that would a cleaner implementation?
</comment><comment author="s1monw" created="2014-10-13T11:06:03Z" id="58876644">@alexksikes I am not sure I understand what you mean can you clarify?
</comment><comment author="alexksikes" created="2014-10-13T11:28:40Z" id="58879700">@s1monw Well the idea was to simply overload TermVectorWriter#setFields to take parser.map() and so not even have a TermVectorResponseParser class with a ParsedTermVectorResponse, but just a TermVectorResponse. But now that I re-think of it, it seems fine the way it is.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inconsistency between `_percolate` and `_search`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8027</link><project id="" key="" /><description># Summary

There seems to be an unexplained inconsistency between the results of `_percolate` and `_search` when using the same document and query set.
# Description

I have an index defined with a custom analyzer set as my default and a few string fields. The analyzer tokenizes on commas, and nothing else. Curling the mapping returns the following output:

```
{
    "news_documents_20140729": {
        "mappings": {
            "0000000022-nid": {
                "_all": {
                    "enabled": false
                },
                "_timestamp": {
                    "enabled": true,
                    "store": true
                },
                "_ttl": {
                    "enabled": true
                },
                "dynamic": "false",
                "analyzer": "csv_lowercase_commas",
                "properties": {
                     ...
                     "subjects": {
                        "type": "string"
                     },
                     "product": {
                        "type": "string",
                        "null_value": "DJGM"
                     },
                     ...
```

The analyzer referenced above is defined as follows:

```
"csv_lowercase_commas" : {
  "type" : "custom",
  "filter" : [ "lowercase" ],
  "tokenizer" : "commas"
}
```

And the `commas` tokenizer is defined as follows:

```
"commas" : {
    "pattern" : ",+",
    "type" : "pattern"
}
```

I've indexed a very simple document to the above index, as the shown type..

```
{ "subjects":"N/DJMT", "product":"DJGM"}
```

I search for the above document using the following three queries, and the document is returned each time, as expected.
## Match All Query

```
{"query": { "match_all":{}}}
```
## Term Query

```
{
    "query": {
        "filtered": {
            "filter": {
                "and": [
                    {
                        "term": {
                            "product": "djgm"
                        }
                    },
                    {
                        "in": {
                            "subjects": [
                                "n/djmt"
                            ]
                        }
                    }
                ]
            }
        }
    }
}
```
## Phrase Query

```
{
    "query": {
        "filtered": {
            "filter": {
                "term": {
                    "product": "djgm"
                }
            },
            "query": {
                "match_phrase" : {
                    "subjects" : "n/djmt"
                }
            }
        }
    }
}
```
## Here's the problem

I add each of the above queries to the .percolator type in the above described index, and then percolate `{"doc":{ "subjects":"N/DJMT", "product":"DJGM"}}` -- i.e., the exact same document content I indexed and searched on previously. _However, the result contains only the `Match All Query` and the `Phrase Query`_. 

To be more concise...

**Searching for the document using the above queries returns the document independent of which query is used to perform the search. Percolating the same document, with the above three queries indexed in the same index's .percolator type, returns _only the first two queries_.**

**The phrase query is inexplicably missing from the percolate results.**
# More Information

This was verified on a 1.1.0 instance, within a clean index. It was originally discovered in a production index through a slightly more complicated, albeit technically synonymous (by my assessment), example.

~~Sorry, I don't have access to a 1.3 cluster at the moment, so I haven't verified it there.~~
Also verified on a 1.3.4 test cluster I just spooled up for testing.

A quick grep through the issue tracker using `percolate is:open label:bug` didn't show anything similar.
# (Naive) Partially-substantiated Insights

Running the same experiment, except removing the '/' in the string 'n/djmt' in both all of the queries and the document, produces the expected, correct results. In other words, `_percolate` returns all three queries, and `_search` returns the document in response to all three queries. _(Did this on a 1.3.4 cluster with the same mappings as described above.)_

The combination of (1) the punctuation being a variable in the problem and (2) the fact that analysis is applied to phrase queries and not to term queries, has me believing the issue has to do with analysis, specifically it not being applied properly when percolating.

_Lastly, if I'm missing something trivial, then I apologize in advance for wasting time. It's very likely I'm ignorant of some crucial fact here._
</description><key id="45294071">8027</key><summary>Inconsistency between `_percolate` and `_search`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">georgi0u</reporter><labels><label>feedback_needed</label></labels><created>2014-10-08T20:41:11Z</created><updated>2014-10-22T15:14:11Z</updated><resolved>2014-10-22T15:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T16:47:01Z" id="59236938">Hi @georgi0u 

Thanks for all the description.  It's quite difficult to follow the detail though. Could you provide a simple but complete recreation of the problem (ie curl commands that I can copy and paste).  It will make it much easier to figure out where the problem lies.

thanks
</comment><comment author="georgi0u" created="2014-10-21T20:48:12Z" id="59996176"># Simple Concrete Recreation

**Executing the following code blocks, in order, as written -- with the exception of the hostname and port being replaced with relevant values -- should illustrate the issue on an otherwise untouched instance of elasticsearch 1.3.4.**

To summarize the potential issue again: 

Running a set of specific queries through the `_search` utility, with one document indexed, returns said document each time (i.e., each of the unique queries used matches the document).

Indexing the same set of queries into the `.percolator` type, followed by percolating the same document used in the searching example returns a smaller subset of the queries indexed, whereas it should be the same exact set, as far as I understand. 

This seems inconsistent and thus I think it's a bug.

Here are the steps:

## Step 1: Create index

_My original example had similar analyzers/tokenizers so I recreated them here_

```
curl -XPUT 'unixdeva10:13312/a_test_index' -d '
{
    "settings" : {
        "analysis" : {
            "analyzer" : {
                "my_analyzer" : {
                    "tokenizer" : "my_tokenizer",
                    "filter" : ["lowercase"]
                }
            },
            "tokenizer" : {
                "my_tokenizer" : {
                    "pattern" : ",+",
                    "type" : "pattern"
                }
            }
        },
        "index" : {"number_of_shards" : 5,"number_of_replicas" : 0}
    }
}'
```

## Step 2: Set up a mapping

_Note: I don't think the `another_field` is necessary, but I'm trying to replicate my environment. It's probable that this example could be whittled down a bit._

```
curl -XPUT 'unixdeva10:13312/a_test_index/_mapping/a_type' -d '
{
    "a_type": {
        "analyzer": "my_analyzer",
        "properties": {
            "a_field": {
                "type": "string"
            },
            "another_field": {
                "type":"string",
                "null_value":"STATIC"
            }
        }
    }
}
' 
```

## Step 3: Index a document

```
curl -XPUT 'unixdeva10:13312/a_test_index/a_type/a_document' -d '{"a_field":"A/VALUE","another_field":"static"}'
```

## Step 4: Attempt to `_search` for the document

_control group to make sure queries match_

### `match_all` query returns document, as expected.

```
curl -XGET 'unixdeva10:13312/a_test_index/a_type/_search?pretty' -d '{"query": { "match_all":{}}}'
```

### `term` query returns document, as expected.

```
curl -XGET 'unixdeva10:13312/a_test_index/a_type/_search?pretty' -d '{
    "query": {
        "filtered": {
            "filter": {
                "and": [
                    {
                        "term": {
                            "another_field": "static"
                        }
                    },
                    {
                        "in": {
                            "a_field": [
                                "a/value"
                            ]
                        }
                    }
                ]
            }
        }
    }
}'
```

### `phrase` query returns document, as expected

```
curl -XGET 'unixdeva10:13312/a_test_index/a_type/_search?pretty' -d '{
    "query": {
        "filtered": {
            "filter": {
                "term": {
                    "another_field": "static"
                }
            },
            "query": {
                "match_phrase" : {
                    "a_field" : "a/value"
                }
            }
        }
    }
}
'
```

## Step 5: Run the same set of queries against the same document, via percolation.

_Note: These are the same queries as used in the searching example_

### Index the `phrase` query into `.percolator`

```
curl -XPUT 'unixdeva10:13312/a_test_index/.percolator/PHRASE?pretty' -d '{
    "query": {
        "filtered": {
            "filter": {
                "term": {
                    "another_field": "static"
                }
            },
            "query": {
                "match_phrase" : {
                    "a_field" : "a/value"
                }
            }
        }
    }
}'

```

### Index the `term` query into `.percolator`

```
curl -XPUT 'unixdeva10:13312/a_test_index/.percolator/TERM?pretty' -d '{
    "query": {
        "filtered": {
            "filter": {
                "and": [
                    {
                        "term": {
                            "another_field": "static"
                        }
                    },
                    {
                        "in": {
                            "a_field": [
                                "a/value"
                            ]
                        }
                    }
                ]
            }
        }
    }
}'

```

### Index the `match_all` query into `.percolator`

```
curl -XPUT 'unixdeva10:13312/a_test_index/.percolator/ALL' -d '{"query": { "match_all":{}}}'
```

## Step 6: Percolate the same example document

```
curl -XGET 'unixdeva10:13312/a_test_index/a_type/_percolate?percolate_format=ids' -d '{"doc" : {"a_field":"A/VALUE","another_field":"static"}}' | pprint
```

### The above **incorrectly** returns:

_missing the query id `PHRASE` from its returned matches_

```
{
    "matches": [
        "ALL",
        "TERM"
    ],
    "total": 2,
    "took": 2,
    "_shards": {
        "successful": 5,
        "failed": 0,
        "total": 5
    }
}
```

### ...when _I think_ it should return the following:

_Note: the inclusion of the query id `PHRASE` in the matches section_

```
{
    "matches": [
        "ALL",
        "TERM",
        "PHRASE"
    ],
    "total": 2,
    "took": 2,
    "_shards": {
        "successful": 5,
        "failed": 0,
        "total": 5
    }
}
```
</comment><comment author="clintongormley" created="2014-10-22T15:14:11Z" id="60100977">Hi @georgi0u 

I've found the issue.  The problem is to do with the type-level `analyzer`.  In fact, if you run the phrase query search without using the type name in the URL, it doesn't work either.

However, if you change the phrase query to use `a_type.a_field` then it works correctly, both in search and in percolate.

In fact, this is a duplicate of #3102, so I'll close this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>wildcard query doesn't work with the wildcard in a phrase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8026</link><project id="" key="" /><description>Hi,

I have a document contains a phrase "interest entities".  If I use wildcard query to search "entit_", that document is returned.  However, if I use wildcard query to do a phrase search "interest_entit_", that document is not found.  I also tried with prefix query as well as wildcard query with "interest entit_", no document is found either.  

I would expect follow query to work according to ElasticSearch document:  http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_literal_wildcard_literal_and_literal_regexp_literal_queries.html

  {"wildcard":{"document":"interest_entit_"}}

The document was indexed using standard tokenizer and standard filter, which should be not stemmed. 

We are using ElaticSearch 1.1.0.

Does anyone has any advices on this?  Thanks!
</description><key id="45269424">8026</key><summary>wildcard query doesn't work with the wildcard in a phrase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joycew</reporter><labels /><created>2014-10-08T17:03:45Z</created><updated>2014-10-15T16:43:35Z</updated><resolved>2014-10-15T16:43:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T16:43:35Z" id="59236409">Hi @joycew 

This issues list is for feature requests and bug reports. Please ask questions like these in the mailing list.  You may also want to read this chapter: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/partial-matching.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix serialization of short[] arays.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8025</link><project id="" key="" /><description>short[] were mistakenly encoded as a float[]. This is not an issue for the
text-based xcontents that we have (yaml, json) since floats can represent any
short value and are serialized as strings. However, this will make the binary
xcontents serialize shorts as int instead of floats.

Close #7845
</description><key id="45259543">8025</key><summary>Fix serialization of short[] arays.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-08T15:39:03Z</created><updated>2015-03-19T09:58:58Z</updated><resolved>2014-10-09T12:31:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-08T15:40:18Z" id="58378050">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update query-string-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8024</link><project id="" key="" /><description>Update sample as it was using the wrong text.
</description><key id="45249015">8024</key><summary>Update query-string-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jmluy</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-08T14:17:59Z</created><updated>2014-12-02T13:38:00Z</updated><resolved>2014-10-16T18:21:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T16:31:51Z" id="59234655">Hi @jmluy 

Good spot. Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="jmluy" created="2014-10-15T23:43:21Z" id="59294344">Hi @clintongormley 

Contributor agreement signed.
</comment><comment author="clintongormley" created="2014-10-16T18:21:37Z" id="59406299">Many thanks @jmluy - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update query-string-query.asciidoc</comment></comments></commit></commits></item><item><title>remove objects from array in elastic search , conditional basis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8023</link><project id="" key="" /><description>my elastic search document contains list array as follow:-

PUT twitter/twit/1
    {"list": 
         [
            {
                "tweet_id": "1",
                "a": "b"
            },
            {
                "tweet_id": "123",
                "a": "f"
            }
        ]
    }

for remove i am doing this

POST /twitter/twit/1/_update
    { "script": "foreach (item : ctx._source.list) {
                        if item['tweet_id'] == tweet_id) {
                              ctx._source.list.remove(item); 
                        }
                }",
      "params": { tweet_id": "123" }
    }

but this is not working and giving this error

ElasticsearchIllegalArgumentException[failed to execute script]; nested: ConcurrentModificationException; 

I am able to update the object of array on the basis of condition , in the same manner, as follow

POST /twitter/twit/1/_update
    {"script":"foreach (item :ctx._source.list) {
                    if item['tweet_id'] == tweet_id) {
                          item['new_field'] = 'ghi';
                    }
               }",
     "params": {tweet_id": 123"}
    }

**I am able to remove whole array or whole field using** 

```
"script": "ctx._source.remove('list')"
```

**I am also able to remove object from array by specifying all the keys of an object using**

```
"script":"ctx._source.list.remove(tag)",
     "params" : {
        "tag" : {"tweet_id": "123","a": "f"}
```

**my node module elastic search version is 2.4.2 elastic search server is 1.3.2**

**please help........**
</description><key id="45245444">8023</key><summary>remove objects from array in elastic search , conditional basis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajit-daffodil</reporter><labels /><created>2014-10-08T13:48:19Z</created><updated>2014-10-08T14:49:07Z</updated><resolved>2014-10-08T14:49:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-08T14:49:07Z" id="58369511">You already asked that multiple times in the mailing list which is the right place to ask questions.

We only deal here with issues or feature requests.
Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>dynamic parser tries to handle empty string as date or time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8022</link><project id="" key="" /><description>I have no mapping and data is 

``` json
{
internal_fields: {
  date-of-last-physical: [""]
}
}
```

Sometimes I get an error

``` json
{"error"=&gt;
  "MapperParsingException[failed to parse [internal_fields.date-of-last-physical]]; nested: MapperParsingException[failed to parse date field [], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"\"]; ",
 "status"=&gt;400}
```

I think it shouldn't fail on empty string as it is empty string and cannot be any date :)
ES 1.3.2
</description><key id="45239238">8022</key><summary>dynamic parser tries to handle empty string as date or time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kritik</reporter><labels><label>discuss</label></labels><created>2014-10-08T12:48:30Z</created><updated>2015-02-20T13:27:03Z</updated><resolved>2015-02-20T13:27:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-02-20T11:13:06Z" id="75222632">@kritik I cannot reproduce this. Can you give more details on what you do?
</comment><comment author="kritik" created="2015-02-20T12:49:11Z" id="75232975">Sorry, but not any more.  We converted this stuff form ES to PostgreSQL JSON, as we had really big models and after some data inserted next ES were very slow. Mapping was switched off.

Closing this issue? May be it was connected  to specific version and not existing in repo any more?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cross_type query needs to be cross_fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8021</link><project id="" key="" /><description /><key id="45236165">8021</key><summary>cross_type query needs to be cross_fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels><label>docs</label></labels><created>2014-10-08T12:13:20Z</created><updated>2014-10-15T16:28:37Z</updated><resolved>2014-10-15T16:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T16:28:03Z" id="59234054">thanks @peschlowp - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: cross_type query needs to be cross_fields</comment></comments></commit></commits></item><item><title>`has_parent` filter must take parent filter into account when executing the inner query/filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8020</link><project id="" key="" /><description>A bug introduced via: #7362

The has_parent query does take the parent filter into account when executing the inner query.
</description><key id="45235275">8020</key><summary>`has_parent` filter must take parent filter into account when executing the inner query/filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-08T12:03:13Z</created><updated>2015-06-08T00:14:54Z</updated><resolved>2014-10-08T14:52:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-08T13:57:55Z" id="58361042">Good catch! LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>[TEST] fix test after pr #8020</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Parent/child: has_parent filter must take parent filter into account when executing the inner query/filter.</comment></comments></commit></commits></item><item><title>Allow add Access-Control-Allow-Origin header manually or switch on this option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8019</link><project id="" key="" /><description>Will be nice for backward compatitbilty
</description><key id="45228206">8019</key><summary>Allow add Access-Control-Allow-Origin header manually or switch on this option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ozlevka</reporter><labels /><created>2014-10-08T10:32:12Z</created><updated>2014-12-19T15:30:36Z</updated><resolved>2014-10-15T16:24:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T16:24:10Z" id="59233438">@ozlevka this is already supported in 1.3 (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-http.html) and expanded in 1.4 (http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.4/modules-http.html)
</comment><comment author="ozlevka" created="2014-10-15T16:42:27Z" id="59236222">1.3 it's working 1.4 not.
</comment><comment author="clintongormley" created="2014-10-16T18:07:08Z" id="59404159">CORS is disabled by default in 1.4  You can enable it with `http.cors.enabled: true`

If this isn't what you are talking about please can you provide more information.
</comment><comment author="adaman79" created="2014-12-19T15:30:36Z" id="67652436">It is like @ozlevka said.

For me it is the same I have elasticsearch on one machine and kibana on another machine. With elasticsearch 1.3.4 with the following elasticsearch.yml everything works fine. The same config with elasticsearch 1.4.2 doesn't work any more and gives me again an Access-Control-Allow-Origin header problem. Therefor I introduced the last 2 lines already in elasticsearch 1.1.1

XMLHttpRequest cannot load http://172.17.0.73:9200/_nodes. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://172.17.0.74' is therefore not allowed access.

The config file:

path:
  data: /data
  logs: /var/lib/elasticsearch/log
  plugins: /var/lib/elasticsearch/plugins
  work: /var/lib/elasticsearch/work
cluster:
  name: felmas

# Set up CORS for read-only access

http.cors.enabled: true
http.cors.allow-origin: *
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Resiliency: Be more conservative if index.version.created is not set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8018</link><project id="" key="" /><description>Today we use the current version which might enable features that are
not supported. We should use the minimal known version rather than
defaulting to the current.
</description><key id="45216525">8018</key><summary>Resiliency: Be more conservative if index.version.created is not set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>resiliency</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-08T08:20:13Z</created><updated>2015-03-19T16:23:29Z</updated><resolved>2014-10-14T18:56:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-08T09:13:57Z" id="58330523">LGTM, the only concern in have is that we do similar checks in other place is the code , searching for usage of `SETTING_VERSION_CREATED` shows them. I think we need to replace them for this to have proper effect. 
</comment><comment author="jpountz" created="2014-10-08T10:01:03Z" id="58335716">What about throwing an exception if it is not set? Things would very likely go wrong anyway if it is not set?
</comment><comment author="bleskes" created="2014-10-08T10:11:43Z" id="58336857">@jpountz we started setting it on indices as of 0.19. This is an index meta data so there is no upgrade path. Not sure what to do with indices that were created with 0.18 and long since upgraded. Officially we are still supposed to be able to read them? Maybe it should be part of the upgrade API that once we are guaranteed that all segments of an index are on the current lucene version, we can update this setting. Not sure what can of worms that will open though.  We can consider dropping this with 2.0 but we should let people know and offer them a way out.
</comment><comment author="s1monw" created="2014-10-08T12:44:14Z" id="58351367">&gt; LGTM, the only concern in have is that we do similar checks in other place is the code , searching for usage of SETTING_VERSION_CREATED shows them. I think we need to replace them for this to have proper effect.

I agree there should be only one way to get this maybe we can just use the Version utils everywhere?

&gt; What about throwing an exception if it is not set? Things would very likely go wrong anyway if it is not set?

I kind of agree - I wonder what would happen if we missed something here? I mean there is no real upgradepath if shit hits the fan?

&gt; @jpountz we started setting it on indices as of 0.19. This is an index meta data so there is no upgrade path. Not sure what to do with indices that were created with 0.18 and long since upgraded. Officially we are still supposed to be able to read them? Maybe it should be part of the upgrade API that once we are guaranteed that all segments of an index are on the current lucene version, we can update this setting. Not sure what can of worms that will open though. We can consider dropping this with 2.0 but we should let people know and offer them a way out.

that is actually not true - there is an upgrade path in `LocalGatewayMetaState:625`
</comment><comment author="bleskes" created="2014-10-08T13:15:19Z" id="58355108">&gt; I agree there should be only one way to get this maybe we can just use the Version utils everywhere?
&gt; +1
&gt; 
&gt; that is actually not true - there is an upgrade path in LocalGatewayMetaState:625

Fair enough :)

&gt; I kind of agree - I wonder what would happen if we missed something here? I mean there is no real upgradepath if shit hits the fan?

I wonder how bad it is to run with the wrong version (i.e., too old in this case). Wouldn't that cause way more subtle exceptions?  I see things around field data and analysis.  

If we are concerned about this maybe we should have a work around cluster level settings which assigns a version instead of throwing an exception. We can potentially only do it in 1.x.
</comment><comment author="s1monw" created="2014-10-08T13:52:24Z" id="58360201">I agree though we should really throw an exception here :)
</comment><comment author="jpountz" created="2014-10-08T13:54:19Z" id="58360458">&gt; I wonder how bad it is to run with the wrong version

In case doc values are enabled, this can be pretty bad as eg. we recently switched from binary doc values to sorted numerics for numeric fields and lucene would refuse to index a document that has a doc value type that is different from what is currently indexed.

&gt; I agree though we should really throw an exception here :)

+1 on an exception. It would be much better than digging an issue for hours before noticing that the cause was that the version was not set.
</comment><comment author="s1monw" created="2014-10-14T15:41:15Z" id="59066812">ok guys I changed this to barf if the setting is not present and I am surprised how many failures I got.... Yet I think this is the safest option to be honest!
</comment><comment author="jpountz" created="2014-10-14T15:47:57Z" id="59067936">LGTM

I agree this is safer, I like this option better!
</comment><comment author="bleskes" created="2014-10-14T17:24:33Z" id="59083142">+1
</comment><comment author="dadoonet" created="2014-10-15T09:51:13Z" id="59182278">This patch breaks some plugins because of a signature change for `AnalysisService(Index)` which now requires Settings: `AnalysisService(Index, Settings)

It means that we will need to release analysis plugins for elasticsearch 1.4.0 in addition to 1.4.0.Beta1.
</comment><comment author="s1monw" created="2014-10-15T09:55:59Z" id="59182848">wait, the `AnalysisService(Index, Settings)` ctor is test-only why do they depend on this?
</comment><comment author="s1monw" created="2014-10-15T13:56:00Z" id="59208598">ok it seems like not a breaking change for the plugins
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/index/analysis/Analysis.java</file><file>src/main/java/org/elasticsearch/index/analysis/AnalysisService.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/EdgeNGramTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/NGramTokenizerFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PatternAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltCharFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenizerFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/SynonymTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/test/java/org/elasticsearch/VersionTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisModuleTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalysisTestsHelper.java</file><file>src/test/java/org/elasticsearch/index/analysis/CharFilterTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/CompoundAnalysisTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PatternCaptureTokenFilterTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltCharFilterFactoryFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltTokenFilterFactoryFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltTokenizerFactoryFactoryTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/StopAnalyzerTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPlugin2Tests.java</file><file>src/test/java/org/elasticsearch/index/query/plugin/IndexQueryParserPluginTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTokenStreamTestCase.java</file></files><comments><comment>[CORE] Be more explicit if index.version.created is not set</comment></comments></commit></commits></item><item><title>Use Case for Aggregation Metrics Calculated based on Other Metrics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8017</link><project id="" key="" /><description>Is I mentioned in #5608 it would be great to be able to define metrics as an expression against other metrics (ideally but not necessarily including other calculated metrics) on the same bucket level and if possible able to traverse to parent/child buckets. While this calculation could be done outside of ES, **sorting** of buckets (such as terms) on these calculated values can only be done as part of ES query

Here is one example:

For health care providers I want to find top 10  providers with highest % of overcharge relative to allowed charges (another variation would be % of overcharge relative to average charge) given charge amount and allowed amount fields in my document.   

I would like to be able to define aggregation `overchargePct`  as `(totalCharges - totalAllowes) * 100 / totalAllowes` and do descending sort on this calculated metric
</description><key id="45199799">8017</key><summary>Use Case for Aggregation Metrics Calculated based on Other Metrics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">roytmana</reporter><labels /><created>2014-10-08T03:35:50Z</created><updated>2014-10-16T13:42:26Z</updated><resolved>2014-10-16T13:42:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-10-16T13:42:26Z" id="59362491">Closing in favour of #8110
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added RethinkDB River to list of community plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8016</link><project id="" key="" /><description /><key id="45189286">8016</key><summary>Added RethinkDB River to list of community plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">deontologician</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-08T00:27:53Z</created><updated>2014-10-16T18:16:12Z</updated><resolved>2014-10-16T18:16:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T16:18:22Z" id="59232543">Hi @deontologician 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="deontologician" created="2014-10-15T19:58:53Z" id="59266157">@clintongormley /signed
</comment><comment author="clintongormley" created="2014-10-16T18:16:02Z" id="59405528">Thanks @deontologician - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Added RethinkDB River to list of community plugins</comment></comments></commit></commits></item><item><title>Custom _all fields do not support not_analyzed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8015</link><project id="" key="" /><description>Is that by design? is there a reason that a custom _all field should always be analyzed? For example, the following mapping doesnt work...

```
                "LocationName": {
                    "type": "string",
                    "copy_to": "Location",
                    "include_in_all": true,
                    "index": "not_analyzed",
                    "store": false
                },
                "LocationParent": {
                    "type": "string",
                    "copy_to": "Location",
                    "include_in_all": true,
                    "index": "not_analyzed",
                    "store": false
                },
                "Location": {
                    "type": "string",
                    "index": "not_analyzed"
                }
```
</description><key id="45186158">8015</key><summary>Custom _all fields do not support not_analyzed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">bobbyhubbard</reporter><labels><label>feedback_needed</label></labels><created>2014-10-07T23:39:16Z</created><updated>2014-10-29T14:37:36Z</updated><resolved>2014-10-29T14:37:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T16:15:13Z" id="59232084">What makes you think it doesn't work?

```
DELETE /t

PUT /t
{
  "mappings": {
    "t": {
      "properties": {
        "one": {
          "type": "string",
          "index": "not_analyzed",
          "copy_to": "three"
        },
        "two": {
          "type": "string",
          "index": "not_analyzed",
          "copy_to": "three"
        },
        "three": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}

PUT /t/t/1
{
  "one": "One ONE",
  "two": "Two TWO"
}

GET /_search
{
  "size": 0, 
  "aggs": {
    "three": {
      "terms": {
        "field": "three"
      }
    }
  }
}
```

Returns:

```
{
   "took": 32,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "three": {
         "buckets": [
            {
               "key": "One ONE",
               "doc_count": 1
            },
            {
               "key": "Two TWO",
               "doc_count": 1
            }
         ]
      }
   }
}
```
</comment><comment author="clintongormley" created="2014-10-29T14:37:36Z" id="60934522">No more feedback, so closing. Feel free to reopen if you can demonstrate this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster inconsistency after master node is restarted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8014</link><project id="" key="" /><description># Version

We can reproduce this consistently both in 1.2.x, 1.3.2, 1.3.4 and 1.4.0Beta1 with different documents and different data sets on different machines.
# Setup
- OS: Linux SLES
- Java:

```
java version "1.7.0_60"
Java(TM) SE Runtime Environment (build 1.7.0_60-b19)
Java HotSpot(TM) 64-Bit Server VM (build 24.60-b09, mixed mode)
```
- Three node cluster with five shards and two replicas
- Reproduced on three separate machines as well as on a single machine.
- Plugins installed:
  - Head or HQ
  - CouchDB river plugin (Reproduced without river plugin as well)
- Head plugin reports:
  - **index_name_real**
  - **size:** 72.7Mi (216Mi)
  - **docs:** 131,686 (131,686)
- Query against alias **index_name** and **index_name_real**
# Steps to reproduce issue
1. Start up three new nodes in any order with no indexes.
2. Set up index, mappings and alias
3. Set up a CouchDB river to populate data
4. Wait for data to be populated.
5. Optionally remove river definitions to ensure it is not a problem with the river (_reproduced behavior with and without rivers_).
6. Query ElasticSearch - consistently get a number of results eg. 27
7. Shut down master node
   - through the head plugin, 
   - startup script or 
   - killing the process
8. Give a few minutes to ensure cluster stabilize (say 5 minutes)
9. Query ElasticSearch - consistently get the number of results eg. 27
10. Start up old master node
11. Give a few minutes to ensure cluster and shards stabilize (say 5 minutes)
12. Query ElasticSearch - got the following sequence of results:
    - **27, 22, 18, 27, 9, 5, 27, 22, 18, 27, 9, 5**
# Additional information
- No errors or stack traces in any of the log files.
- Old master node log statements when starting up:

```
[2014-10-08 09:10:57,398][INFO ][node                     ] [nodePD02] version[1.3.4], pid[15545], build[a70f3cc/2014-09-30T09:07:17Z]
[2014-10-08 09:10:57,399][INFO ][node                     ] [nodePD02] initializing ...
[2014-10-08 09:10:57,423][INFO ][plugins                  ] [nodePD02] loaded [river-couchdb], sites [head]
[2014-10-08 09:11:01,583][INFO ][node                     ] [nodePD02] initialized
[2014-10-08 09:11:01,583][INFO ][node                     ] [nodePD02] starting ...
[2014-10-08 09:11:01,747][INFO ][transport                ] [nodePD02] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/..........:9300]}
[2014-10-08 09:11:01,752][INFO ][discovery                ] [nodePD02] elasticsearch_PROD/ridS6JGyQOuNql_m5mMoMQ
[2014-10-08 09:11:11,919][INFO ][cluster.service          ] [nodePD02] detected_master [nodePD03][2XpaT8CfTO2m-FhoyOaIgg][es03][inet[/..........:9300]]{river=_none_}, added {[nodePD03][2XpaT8CfTO2m-FhoyOaIgg][es03][inet[/..........:9300]]{river=_none_},[nodePD01][OBQXEeP6R32NCus3guXUdQ][es01][inet[/..........:9300]],}, reason: zen-disco-receive(from master [[nodePD03][2XpaT8CfTO2m-FhoyOaIgg][es03][inet[/..........:9300]]{river=_none_}])
[2014-10-08 09:11:12,146][INFO ][http                     ] [nodePD02] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/..........:9200]}
[2014-10-08 09:11:12,147][INFO ][node                     ] [nodePD02] started
```
## Query

```
{
  "query": {
    "filtered": {
      "query": {
        "bool": {
          "must": [
            {
              "multi_match": {
                "query": "10011",
                "fields": [
                  "payload.id",
                  "payload.code"
                ]
              }
            }
          ]
        }
      },
      "filter": {
        "and": [
          {
            "term": {
              "fortress": "ct"
            }
          },
          {
            "term": {
              "docType": "plannedtrain"
            }
          }
        ]
      }
    }
  },
  "sort": [
    {
      "eventDate": {
        "order": "asc"
      }
    }
  ],
  "size": 100,
  "from": 0
}
```
</description><key id="45166858">8014</key><summary>Cluster inconsistency after master node is restarted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeanvanwyk</reporter><labels /><created>2014-10-07T20:59:00Z</created><updated>2014-10-17T05:10:12Z</updated><resolved>2014-10-16T02:59:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeanvanwyk" created="2014-10-14T04:11:28Z" id="58988056">Reproduced on ElasticSearch 1.4.0Beta1 with bulk uploaded documents and no rivers defined.  Loaded 4888 documents...
</comment><comment author="bleskes" created="2014-10-14T07:31:32Z" id="59000580">@jeanvanwyk thx for the detailed report. Next to the query you already shared, any chance you can upload your mapping, index setting and a data sample (which reproduces this) somewhere? This is something we test continuously so I suspect it will take quite some time to chase it without a concrete reproduction.  
</comment><comment author="jeanvanwyk" created="2014-10-16T02:59:04Z" id="59307790">The problem seemed to be a field in one of our mappings that was declared as _"type": "string"_ while in the rest of the document mappings the field was defined as _"type" : "long"_

To be more specific:
- We had mappings A, B, C, D, E, F, G, H, J, K and L
- Mapping K defined the field as _"string"_ while the rest did it as _"long"_
- We were populating and querying documents of type G and saw the specified behavior.

After your data sample and mapping comments we investigated some more and diagnosed the offending mapping. Some more analysis diagnosed the problem as listed above.

A possible feature request: Warn when trying to load conflicting definitions for the same field in multiple mappings.

Thank you for your help.
</comment><comment author="jeanvanwyk" created="2014-10-16T03:00:57Z" id="59307883">Fixed mapping tested in both 1.3.4 and 1.4.0Beta1 and succeeded.
</comment><comment author="bleskes" created="2014-10-16T06:26:50Z" id="59319337">@jeanvanwyk thx for the explanation. I can see how mapping inconsistencies would explain unexpected results but I don't see how this is effected by a master restart. Did you figure that one out too?
</comment><comment author="jeanvanwyk" created="2014-10-16T22:27:45Z" id="59441014">Only a guess: we think that the multiple mappings causes the different shards to do searches subtly different.  When all the nodes are up they agree in what it should be.  When the master node goes away and comes back and the shards are reallocated(?) it is not done to the exact same state.  As to why this only happens with the master node is a good question. I can only assume that is the different code path for master nodes that causes the issue.
</comment><comment author="jeanvanwyk" created="2014-10-16T22:31:06Z" id="59441357">For interest sake:  When we took the offending mapping and only used it with our test/main mapping we got no results back from our search - even if none of the nodes has been restarted. It only "works" if we add more mappings - that is when there is a "majority"
</comment><comment author="clintongormley" created="2014-10-17T05:10:12Z" id="59467075">The master restart might just result in different hash randomization, resulting in different field resolution order.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix upgrade logic to check for major version bump.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8013</link><project id="" key="" /><description>I had not noticed this since I was only testing using bwc tests, which only go back to ES 1.1 (which is still lucene 4.x).
</description><key id="45140854">8013</key><summary>Fix upgrade logic to check for major version bump.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Upgrade API</label><label>bug</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-07T18:13:49Z</created><updated>2015-06-07T18:43:54Z</updated><resolved>2014-10-13T02:07:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-08T04:59:31Z" id="58309744">That LGTM can we maybe have a test that has a zipped shard from a 0.20 version that get's upgraded? no need to do this here
</comment><comment author="rjernst" created="2014-10-10T23:35:21Z" id="58728783">Ok, added a test.  Thanks for the help with the integration.  I also cleaned up the UpgradeTest a bit so the checks could be reused.  I still want to add a final validation using the segments api that t he index is completely upgraded.
</comment><comment author="s1monw" created="2014-10-11T06:47:34Z" id="58740353">added minor comments - LGTM otherwise
</comment><comment author="rjernst" created="2014-10-11T23:15:37Z" id="58767899">@javanna I think I addressed all your comments.
</comment><comment author="rjernst" created="2014-10-12T00:42:14Z" id="58769641">@s1monw I get random failures sometimes, where flush failed so nothing to upgrade, or an extra flush seems to have happened so that there are no segments to be recovered.  Can double check how I'm doing the flush in UpgradeTest?
</comment><comment author="rjernst" created="2014-10-12T18:42:01Z" id="58815662">Ok, test is passing consistently now.
</comment><comment author="s1monw" created="2014-10-12T19:59:20Z" id="58820045">LGTM left two minor comments but you can squash and push once fixed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/merge/policy/ElasticsearchMergePolicy.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/upgrade/RestUpgradeAction.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeReallyOldIndexTest.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Admin: Fix upgrade logic to work on lucene major version differences.</comment></comments></commit></commits></item><item><title>Async support has already been added with Search::Elasticsearch::Async</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8012</link><project id="" key="" /><description /><key id="45128845">8012</key><summary>Async support has already been added with Search::Elasticsearch::Async</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">abraxxa</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2014-10-07T16:28:35Z</created><updated>2014-10-29T13:49:22Z</updated><resolved>2014-10-29T13:49:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T11:28:16Z" id="59028120">Hi @abraxxa 

Thanks for spotting that.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="abraxxa" created="2014-10-15T19:20:33Z" id="59260550">I've read through the CLA and find it confusing that at the very top the CLA is with Elasticsearch BV, a European company, but under point 8 it states that Californian laws apply.
</comment><comment author="clintongormley" created="2014-10-15T19:40:10Z" id="59263372">@abraxxa it is a European company, but we also have headquarters in SF, so we have chosen Californian law.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Updated Perl client page to mention async client</comment></comments></commit></commits></item><item><title>Aggregations ignore "filter" word in request body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8011</link><project id="" key="" /><description>I am using elasticsearch 1.3.2

Aggregations ignore "filter" word in request body.
Aggregations accept only "query" word.

Very easy to reproduce:
Search any aggregation with **"filter"** and with **"query{filtered{filter}}"**

Obviously, you should get equal results.
BUT, you will recieve different result, aggregation will ignore "filter" word but not ignore "query{filtered{filter}}" word.

This issue is closely related with issue #7971, please take a look at thatone too.
</description><key id="45098472">8011</key><summary>Aggregations ignore "filter" word in request body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maximpashuk</reporter><labels /><created>2014-10-07T11:55:12Z</created><updated>2014-10-16T12:16:03Z</updated><resolved>2014-10-15T15:46:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T15:46:57Z" id="59227443">Hi @maximpashuk 

I'm afraid I don't understand what you have written at all.  Please can you provide a simple but complete curl recreation of the problem.  Closing for now - feel free to reopen when you have a replication
</comment><comment author="golubev" created="2014-10-16T08:58:21Z" id="59332816">Hello @clintongormley 

It seems to me that @maximpashuk describes the same issue about the top-level `filter` as in the #7971.

He sais that a filter applied in the `filtered` `query` affects the aggregation results, while filter applied in the top-level `filter` - does not. But he expected that the results will be the same as I did too.
</comment><comment author="clintongormley" created="2014-10-16T09:22:41Z" id="59335453">ok @golubev - see my answer to #7971. You will see on the search docs that `filter` isn't even documented: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-body.html
</comment><comment author="golubev" created="2014-10-16T09:27:57Z" id="59336033">Thanks, @clintongormley !
</comment><comment author="maximpashuk" created="2014-10-16T12:16:03Z" id="59352196">@clintongormley Thanks for explanations abount 'post_filter', now I understand that this is expected behaviour.
You right I can always use query-&gt;filtered-&gt;filter
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add cluster and index state checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8010</link><project id="" key="" /><description>This commit adds checksumming for cluster and index states. It moves
from a plain XContent based on-disk format to a more structured binary
format including a header and footer as well as a CRC32 checksum for
each of these files. Since previous versions didn't write any format
identifier etc. this commit adds a file extension `.st` for states
that have header/footer and checksum.
This commit also moves over to write temporary files and moves them into
place with an atomic move operation. This commit also serializes and
deserializes the states without materilazing them into opaque memory.

Closes #7586
</description><key id="45093642">8010</key><summary>Add cluster and index state checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-07T10:53:51Z</created><updated>2015-06-06T19:22:37Z</updated><resolved>2014-10-15T09:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-10-08T09:56:47Z" id="58335226">If possible, I think it would be useful to grab a real binary state file written in the new format and stick it in the test resources directory, then we could write a test that would read it to verify that we don't accidentally change the format of the state file written in the future. This is similar to the translog tests that read specific versions of a translog file for correctness and specific corruptions.
</comment><comment author="s1monw" created="2014-10-08T14:18:15Z" id="58364109">&gt; If possible, I think it would be useful to grab a real binary state file written in the new format and stick it in the test resources directory, then we could write a test that would read it to verify that we don't accidentally change the format of the state file written in the future. This is similar to the translog tests that read specific versions of a translog file for correctness and specific corruptions.

I will work on this - pushed a first round of fixes for the other comments
</comment><comment author="s1monw" created="2014-10-08T20:56:16Z" id="58426230">@dakrone I updated the PR and added a bunch of tests to also test reading legacy format etc. I also picked up your suggestion of testing a real binary state etc. I think we are ready here but please take the time to review this again and put all comments you have. 
One thing that I am not happy with is the fact that we basically overriding files in some cases. Sometimes the `general-1.st` file is already there when we regenerate it which is wrong IMO. I want to introduce a generation to these files that we increment internally even if we don't bump the version. Yet, I think we should do this in a second PR
</comment><comment author="s1monw" created="2014-10-13T10:35:59Z" id="58874105">@dakrone wanna do another review on this?
</comment><comment author="s1monw" created="2014-10-13T10:36:09Z" id="58874121">@kimchy if you have time ^^
</comment><comment author="dakrone" created="2014-10-14T09:54:08Z" id="59016177">@s1monw gave this another review pass and left some comments.

I also tested this by starting ES, writing some data, shutting it down, corrupting the state file manually, and then starting ES back up, it spewed all kinds of errors and refused to start, so that's what I expected! :)
</comment><comment author="s1monw" created="2014-10-14T12:47:33Z" id="59036871">@dakrone I addressed all your comments
</comment><comment author="dakrone" created="2014-10-15T09:14:26Z" id="59178278">LGTM
</comment><comment author="s1monw" created="2014-10-15T09:45:43Z" id="59181702">thx @dakrone for reviewing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shard initialization fails with DocValues exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8009</link><project id="" key="" /><description>Hi,

  we have been running into strange errors lately. We get a lot of exceptions of the type: 

```
[2014-10-07 06:41:26,235][WARN ][cluster.action.shard     ] [Madcap] [my_index][1] sending failed shard for [my_index][1], node[-QGTVk8RRcuKuBwdqD8l1A], [P], s[INITIALIZING], indexUUID [u68VqfHsRii16gYXtPj1cQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[my_index][1] failed recovery]; nested: IllegalArgumentException[cannot change DocValues type from BINARY to SORTED_SET for field "custom.my_field"]; ]]
[2014-10-07 06:41:26,587][WARN ][indices.cluster          ] [Madcap] [my_index][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [my_index][1] failed recovery
  at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: cannot change DocValues type from BINARY to SORTED_SET for field "custom.my_field"
  at org.apache.lucene.index.FieldInfos$FieldNumbers.addOrGet(FieldInfos.java:198)
  at org.apache.lucene.index.IndexWriter.getFieldNumberMap(IndexWriter.java:868)
  at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:819)
  at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1420)
  at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:271)
  at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:692)
  at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:217)
  at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
  ... 3 more
```

  We are in a daily index situation so the index is quite new. It contains 10 to 20 millions of documents spread over 10 shards and 5 nodes. At some point (after hours of the index being green), one of the shards becomes INITALIZING and can never start (because of the aforementioned exception). The index is then in red state and we cannot set it back on track... In this case the only solution we have found is to scroll over the whole index and reindex the data into a new index (but we most likely have lost the data from the failing shard). The field that causes the issue has the following definition `{"type":"long","doc_values":true,"include_in_all":false}`. This mapping is inferred from a dynamic template `{"mapping":{"index":"not_analyzed","include_in_all":false,"doc_values":true,"type":"{dynamic_type}"},"match":"*"}`.
  One important note is that this part of the data is free-form (ie user input) and it is possible that some documents have conflicting types (one document having the field as string, the other as long); that's why the index has the setting `index.mapping.ignore_malformed` set to `true`.
  Also, it might not be relevant, but this only happened on days when we had at least one node that was restarted.
  We have noticed this issue since running ElasticSearch 1.3.4 (but can't be 100% sure that it did not happen before).

  We cannot isolate and reproduce the issue but have faced it several times over the past few days. Feel free to suggest actions we can undertake should it happen again, to get more details to help fix it. Also, if you have suggestions to help bypass the issue when it happens (so that we can avoid reindexing the data), that'd be great.

Thanks,
Emmanuel
</description><key id="45091629">8009</key><summary>Shard initialization fails with DocValues exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">egueidan</reporter><labels /><created>2014-10-07T10:28:12Z</created><updated>2014-12-31T16:54:39Z</updated><resolved>2014-12-31T16:54:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-07T10:53:21Z" id="58167191">@egueidan Thanks for reporting this issue.

&gt; One important note is that this part of the data is free-form (ie user input) and it is possible that some documents have conflicting types (one document having the field as string, the other as long); that's why the index has the setting index.mapping.ignore_malformed set to true.

This could indeed be the cause of the issue. Can you please provide us with all the mappings that you have on the `my_index` index?
</comment><comment author="egueidan" created="2014-10-07T13:44:20Z" id="58185823">Hi @jpountz,

thanks for looking into this. Sadly I'm not at liberty to share the full mapping here as it contains sensitive data... Here is a dummied down and anonymized version which I hope will help you get going. 

```
{
  "my_index": {
    "mappings": {
      "my_type": {
        "dynamic_templates": [
          {
            "custom_message": {
              "mapping": {
                "include_in_all": true,
                "analyzer": "simple",
                "type": "string"
              },
              "path_match": "custom.message"
            }
          },
          {
            "subpart_bytes": {
              "mapping": {
                "index": "not_analyzed",
                "include_in_all": false,
                "doc_values": true,
                "type": "byte"
              },
              "match_pattern": "regex",
              "path_match": "subpart\\.(fieldbyte1|fieldbyte2)"
            }
          },
          {
            "subpart_short": {
              "mapping": {
                "index": "not_analyzed",
                "include_in_all": false,
                "doc_values": true,
                "type": "short"
              },
              "match_pattern": "regex",
              "path_match": "subpart\\.(fieldshort1|fieldshort2)"
            }
          },
          {
            "dyn_fields": {
              "mapping": {
                "index": "not_analyzed",
                "include_in_all": false,
                "doc_values": true,
                "type": "{dynamic_type}"
              },
              "match": "*"
            }
          }
        ],
        "properties": {
          "custom": {
            "include_in_all": false,
            "properties": {
              ... many many fields (hundreds), of all types, all inferred from the dyn_fields template ...
              "my_field": {
                "type": "long",
                "doc_values": true,
                "include_in_all": false
              }
            }
          },
          "basic": {
            "properties": {
              "date": {
                "type": "date",
                "doc_values": true,
                "format": "dateOptionalTime",
                "include_in_all": false
              }
            }
          },
          "message": {
            "type": "string",
            "analyzer": "simple",
            "include_in_all": true
          },
          "subpart": {
            "include_in_all": false,
            "properties": {
              ... a few fields (&lt;10) like 'other' inferred via dyn_field as well ...
              "other": {
                "type": "string",
                "index": "not_analyzed",
                "doc_values": true,
                "include_in_all": false
              },
              ... the subpart_bytes and subpart_short inferred fields ...
              "fieldbyte1": {
                "type": "byte",
                "doc_values": true,
                "include_in_all": false
              },
              "fieldshort1": {
                "type": "short",
                "doc_values": true,
                "include_in_all": false
              },
              "fieldbyte2": {
                "type": "byte",
                "doc_values": true,
                "include_in_all": false
              },
              "fieldshort2": {
                "type": "short",
                "doc_values": true,
                "include_in_all": false
              }
            }
          }
        }
      }
    }
  }
}
```

Cheers,
Emmanuel
</comment><comment author="jpountz" created="2014-10-07T14:44:11Z" id="58195587">The thing I was looking for in your mappings is whether you have two types on the same index that have the `custom.my_field` field with doc values, but once as a string and once as a numeric? If it is the case, there is not much that can be done, I have plans to improve the situation in the future by failing mapping updates that introduce incompatible types, but that would only help discover the problem earlier, not solve it. The fix would be to index into several indices instead of several types, or to ensure at a higher level that two different types cannot get incompatible mappings (eg. by prefixing field names by the type name).
</comment><comment author="jpountz" created="2014-10-07T14:54:29Z" id="58197355">Another question: do you know on which elasticsearch version the problematic index has been created?
</comment><comment author="egueidan" created="2014-10-07T15:32:17Z" id="58203749">Concerning your first question, no, we only have one type on those indices.
Concerning your second question, this happened on indices created with 1.3.2 and indices created with 1.3.4.
We have many indices but it only happened on the largest indices (&gt; 10 million docs) with active indexing (40/50k docs/min).
</comment><comment author="s1monw" created="2014-10-08T05:07:30Z" id="58310370">would it be possilbe to get the `segments_N` and `*.si` files from this index / shard? Are you 100% sure you are not running any `1.4.0.Beta1` nodes or you downgraded somehow?
</comment><comment author="egueidan" created="2014-10-08T07:21:04Z" id="58319712">Sorry but the index is gone now but I'll get the files if we run into the issue again. All the nodes are running 1.3.4 (build_hash a70f3ccb52200f8f2c87e9c370c6597448eb3e45) and we haven't started testing 1.4.0.Beta1 (yet!).
If my memory serves me correctly the first time it happened was when I upgraded from 1.3.2 to 1.3.4 (ie rolling restart). Then when I added a new machine to the cluster (from 4 to 5). The last time one of our nodes went rogue (cluster was split 4/1 and we restarted the node). That's why I mentioned it only happened after some node (re)start (ie some recovery/relocation happened). Bare in mind we use daily indices and the issue only happened on the current day index - which is the only one receiving index requests. Also we recreate the indices from scratch (reindexing everything) when the issue happens, so this never happened on an index more than a few hours old.
</comment><comment author="egueidan" created="2014-10-08T08:16:53Z" id="58324764">Guys, as I was writing my last comment we had to restart one of our nodes... Immediately after we have started seing this exception again. At the moment, the cluster is green but we have lots of exceptions at indexation:

```
java.lang.IllegalArgumentException: cannot change DocValues type from SORTED_SET to BINARY for field "custom.xxxxxxxxxx"
  at org.apache.lucene.index.FieldInfos$FieldNumbers.addOrGet(FieldInfos.java:198)
  at org.apache.lucene.index.FieldInfos$Builder.addOrUpdateInternal(FieldInfos.java:304)
  at org.apache.lucene.index.FieldInfos$Builder.addOrUpdate(FieldInfos.java:289)
  at org.apache.lucene.index.DefaultIndexingChain.getOrAddField(DefaultIndexingChain.java:484)
  at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:373)
  at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:301)
  at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:241)
  at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:451)
  at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1539)
  at org.elasticsearch.index.engine.internal.InternalEngine.innerIndex(InternalEngine.java:572)
  at org.elasticsearch.index.engine.internal.InternalEngine.index(InternalEngine.java:492)
  at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:409)
  at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:446)
  at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:157)
  at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:535)
  at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:434)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
```

This happens for example on a field which has been discovered as double in the mapping yet we try to index a document with "Infinity" as value for that field.

I have the segment\* and *.si files from the shard/node reporting the exception, but for privacy reasons, I can't attach them on github. What would be the best way to share them privately with you?
</comment><comment author="s1monw" created="2014-10-08T12:40:59Z" id="58350991">This is actually a different exception. Your first exception (the original you opened the issue with) happens during IndexWriter creation so there somehow is a conflicting segment on that node and it might  be related to problems that we fixed in `1.3.4` where due to hash collisions on recovery we didn't transfer alll the files needed. 

Yet, lemme ask a couple of more questions to get closer to the source of the problem. 
- do I understand your correctly that this problem occurred after recovery to the node that was taken out of the cluster and later rejoined?
- where there nodes with version 1.3.2 in the cluster while this node joined
- do you still know if you recovered from a node of version 1.3.2 or from a newer node? 
- the node that failed with the first exception did it run 1.3.4 or 1.3.2?

This second failure is due to the fact that you are trying to index the same field as string while it's a double which obviously doesn't work.  Are you using dynamic mappings for this?
</comment><comment author="egueidan" created="2014-10-08T15:54:17Z" id="58380520">Yes, sorry, it is indeed a different exception. I mentioned it because it never happened before the restart of the machine this morning, although I know for a fact that there were similar messages (with string instead of a double) being inserted. Note that we are using `index.mapping.ignore_malformed: true` and the field mappings are discovered via dynamic templating.

I had a gut feeling it could be related and funny thing is right now the index with those exceptions is yellow with one replica shard perpetually initializing with many exceptions like:

```
[2014-10-08 15:37:40,085][WARN ][cluster.action.shard     ] [Fafnir] [index_xxxxx][6] received shard failed for [index_xxxxx][6], node[pwGu7LZWQ5e1mHjmiWyeVw], [R], s[INITIALIZING], indexUUID [LZGZL-kKS66nVPU7pFRCtA], reason [Failed to start shard, message [RecoveryFailedException[[index_xxxxx][6]: Recovery failed from [Rocket Racer][PdxQHUbMSY2h7co9Zg0xUw][server-4][inet[/10.0.0.16:9300]] into [Squirrel Girl][pwGu7LZWQ5e1mHjmiWyeVw][server-1][inet[/10.0.0.13:9300]]]; nested: RemoteTransportException[[Rocket Racer][inet[/10.0.0.16:9300]][index/shard/recovery/startRecovery]]; nested: RecoveryEngineException[[index_xxxxx][6] Phase[2] Execution failed]; nested: RemoteTransportException[[Squirrel Girl][inet[/10.0.0.13:9300]][index/shard/recovery/prepareTranslog]]; nested: IllegalArgumentException[cannot change DocValues type from BINARY to SORTED_SET for field "custom.field_xxx"]; ]]
```

To answer your questions this happens on a fully 1.3.4 cluster. The index was created on 1.3.4. There is no 1.3.2 node involved. To summarize what we see, it goes:
1. No issue, No exception about doc_values (none at all)
2. Restart of one machine
3. Exceptions about doc_values start to come in (the second one I reported that happens at document insert), Cluster back to green
4. After a few hours at least one shard cannot initialize (index red yesterday, index yellow right now)

Let me know if you need more info.
</comment><comment author="clintongormley" created="2014-10-15T16:41:53Z" id="59236154">Hi @egueidan 

Are the segment files small enough to email? In which case you can send them to: clinton dot gormley at elasticsearch dot com.

thanks for the detailed info
</comment><comment author="egueidan" created="2014-10-15T17:04:14Z" id="59239469">Hi @clintongormley,
Actually it's tiny. It should be in your inbox. Let me know if it does not go through.
Thanks
</comment><comment author="egueidan" created="2014-10-22T12:30:25Z" id="60077098">Hi,
  just to keep you updated, this issue keeps happening. We have just restarted a few servers and looking at the logs I have found this third variant of the exception:

```
[2014-10-22 12:12:28,525][WARN ][index.merge.scheduler    ] [Defensor] [some_index][1] failed to merge java.lang.IllegalArgumentException: cannot change DocValues type from SORTED_SET to BINARY for field "custom.durationInMs"
       at org.apache.lucene.index.FieldInfos$FieldNumbers.addOrGet(FieldInfos.java:198)
       at org.apache.lucene.index.FieldInfos$Builder.addOrUpdateInternal(FieldInfos.java:304)
       at org.apache.lucene.index.FieldInfos$Builder.add(FieldInfos.java:332)
       at org.apache.lucene.index.SegmentMerger.mergeFieldInfos(SegmentMerger.java:316)
       at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:94)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4225)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3820)
       at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
       at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:106)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
```

Thought it might be relevant.

Regards,
Emmanuel
</comment><comment author="clintongormley" created="2014-10-22T16:22:53Z" id="60112508">Hi @egueidan 

&gt; To answer your questions this happens on a fully 1.3.4 cluster. The index was created on 1.3.4. There is no 1.3.2 node involved.

When we look at the segment files that you sent, some of them are from Lucene 4.9 (1.3.2), not Lucene 4.9.1. So 1.3.2 is involved somewhere here - either you have mixed nodes in your cluster, or the index was originally created with 1.3.2.

Can you clarify?

thanks
</comment><comment author="egueidan" created="2014-10-22T18:18:09Z" id="60130271">Hi @clintongormley,
  I've checked again all my nodes are running 1.3.4 (build hash a70f3ccb52200f8f2c87e9c370c6597448eb3e45). In the cluster there exists some indices created in 1.3.2. But the issue definitely happens on indices created with 1.3.4. Maybe I mixed up the files, I'm sending you another set (based on other indices).
Thanks
</comment><comment author="s1monw" created="2014-10-22T19:43:51Z" id="60143250">I opened a lucene issue for this https://issues.apache.org/jira/browse/LUCENE-6019
</comment><comment author="egueidan" created="2014-10-23T07:19:08Z" id="60201198">@s1monw Glad you were able to pinpoint the issue and thanks a lot for taking the time. Any idea (even ballpark) of what version/when this will be integrated into elasticsearch? Regards.
</comment><comment author="mikemccand" created="2014-10-23T18:52:06Z" id="60289940">@egueidan the Lucene issue is fixed, and we're going to spin a new Lucene release with this fix.  This will prevent mis-use of Lucene's APIs from causing index corruption.

But we will also separately to fix Elasticsearch "handle" the same field name across types having different settings (since Elasticsearch currently stores multiple types in one Lucene index), likely by refusing mappings of the same field name across different types with different index settings (such as doc values type).

Net/net, even once we've fixed Elasticsearch/Lucene to not corrupt the index on inconsistent doc values types, you'll still have to fix your usage of Elasticsearch to not allow the same field name across different types to have different mappings settings.
</comment><comment author="egueidan" created="2014-10-23T19:57:25Z" id="60299205">Hi @mikemccand and thanks for the quick fix.
To be honest I'm a bit puzzled as we only have one type across all of our indices... Does this mean there might be more to the problem we are experiencing? Just to be more precise, we are in a situation where we index "user" data  into elasticsearch (ie we don't have full control over its content). We have dynamic templates so that the fields have doc_values enabled. Sometimes the data will be inconsistent (user submits conflicting field types). In case we receive a string after receiving a number for the same field, we have enabled `index.mapping.ignore_malformed`. When it's a string instead of an object or vice-versa then we get a mapper exception and deal with it. Anything wrong with that scenario?
</comment><comment author="mikemccand" created="2014-10-23T21:18:45Z" id="60311006">Hi @egueidan I don't fully understand what {{index.mapping.ignore_malformed}} should do; maybe someone else can chime in here.  It sounds like a spooky setting: it seems dangerous to ignore serious errors like a type change.
</comment><comment author="clintongormley" created="2014-12-31T16:54:39Z" id="68453887">Closing this in favour of #8688
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] fix CurrentTestFailedMarker to reset its state after each test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8008</link><project id="" key="" /><description>The currently used method `testRunStarted` is only called before any tests have been run, we need to reset that state before each test, that's why we need to use `testStarted`.
</description><key id="45090769">8008</key><summary>[TEST] fix CurrentTestFailedMarker to reset its state after each test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-07T10:17:33Z</created><updated>2014-10-07T10:25:37Z</updated><resolved>2014-10-07T10:25:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-07T10:23:04Z" id="58164391">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>key_as_string formating glued to key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8007</link><project id="" key="" /><description>Hi,

I'm doing some experiments on elasticsearch as it looks a very promising
solution in our system. While trying to do some date aggregation I've
noticed some odd behavior when trying to format date

_search?pretty

``` json
{
   "aggs":{
      "events":{
         "date_histogram":{
            "field":"@timestamp",
            "interval":"1d",
            "min_doc_count":0,
            "format":"yyyy-MM-dd"
         }
      }
   }
}
```

I get following response:

``` json
{
   "aggregations":{
      "events":{
         "buckets":[
            {
               "key_as_string":"yyyy-MM-dd1412467200000",
               "key":1412467200000,
               "doc_count":22
            },
            {
               "key_as_string":"yyyy-MM-dd1412553600000",
               "key":1412553600000,
               "doc_count":451
            }
         ]
      }
   }
}
```

Example data

``` json
{
  "_index": "i20141006",
  "_type": "monitoring",
  "_id": "bb28487c-1e83-4071-876b-da004f744ed0-7",
  "_score": 1,
  "_source": {
    "@timestamp": 1412672755565.4,
    "log_time": "2014-10-07T12:05:55",
    "event_id": 208
  }
}
```

If date filtering and aggregating is performed for log_date, everything woks as expected.

Elastic search version:

``` json
{
   "version":{
      "number":"1.3.2",
      "build_hash":"dee175dbe2f254f3f26992f5d7591939aaefd12f",
      "build_timestamp":"2014-08-13T14:29:30Z",
      "build_snapshot":false,
      "lucene_version":"4.9"
   }
}
```
</description><key id="45089504">8007</key><summary>key_as_string formating glued to key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tautvydas-greitai</reporter><labels /><created>2014-10-07T10:02:36Z</created><updated>2015-02-27T09:40:51Z</updated><resolved>2014-10-15T15:44:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T15:44:49Z" id="59227067">Hi @tautvydas-greitai 

I'm guessing that you have used dynamic mapping for your fields.  You're indexing a floating-point value for `@timestamp`, so Elasticsearch has automatically mapped it as a `double`.  The date `format` can only be applied to date fields.

If you change the mapping for `@timestamp` to type `date`, then it should all work as you expect.
</comment><comment author="drorata" created="2015-02-26T12:53:16Z" id="76173014">I have a similar problem: My `timestamp` is in `_source` and the `_mapping` returns:

```
"timestamp": {
              "type": "long"
           } 
```

If I understand correctly, given this the `format` key cannot work. It would be helpful to get an informative message suggesting that the functionality can only be used if the time is stored as `date`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter by specific value without mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8006</link><project id="" key="" /><description>What I'm trying to do is to get data  by filtering term with exact matching. I have ES 1.3.2 and I cannot do mapping, as attributes are dynamic (different users has different attributes). My data:

``` json
{ id: 111,
org_id: 11,
approval: "approved",
...
}


{ id: 112,
org_id: 11,
approval: "not approved",
...
}
```

This request returns results:

``` bash
curl -X GET 'host:9200/data/_search?pretty' -d '{
  "filter":{
    "term":{
      "approval":"approved"
    }
  }
}
```

But this not:

``` bash
curl -X GET 'host:9200/reports/_search?pretty' -d '{
  "filter":{
    "term":{
      "approval":"not approved"
    }
  }
}
```

Checked for typo and nothing bad was found. Isn't filter working without exact matching?
</description><key id="45087415">8006</key><summary>Filter by specific value without mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kritik</reporter><labels /><created>2014-10-07T09:38:49Z</created><updated>2014-10-07T09:41:08Z</updated><resolved>2014-10-07T09:41:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-07T09:41:08Z" id="58160111">Please use the mailing list for questions. We can definitely help you there.

That said, your filter won't work as the inverted index has indexed probably `not` and `approved`. Comparing `not approved` to this won't match.

Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Aggregation] Ordering buckets by their top document's score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8005</link><project id="" key="" /><description>Thank you for developing Elasticsearch! I'm using Elasticsearch for my web service, and it's great :)

Now, I want to renew my service, and I want to order buckets (grouped by `iid_or_id`) by their top `source_priority` document's score.

Finally, I come up with the solution by using `1.4.0.Beta1` feature `scripted metric aggregation`.

``` rb
  {
    query: { match_all: {} },
    size: 0,
    aggs: {
      group_by_iid_or_id: {
        terms: {
          field: 'iid_or_id',
          order: { max_priority_score: 'desc' },
          size: 0
        },
        aggs: {
          top_hit: {
            top_hits: {
              sort: [{ source_priority: { order: 'desc' } }],
              size: 1
            }
          },
          max_priority_score: {
            scripted_metric: {
              init_script: "_agg['p'] = -1; _agg['s'] = 0",
              map_script: "if (doc['source_priority'].value &gt; _agg['p']) { _agg['p'] = doc['source_priority'].value; _agg['s'] = _score }",
              reduce_script: "p = -1; s = 0; for (a in _aggs) { if (a['p'] &gt; p) { p = a['p']; s = a['s'] } }; return s"
            }
          }
        }
      }
    }
  }
```

But this fails with the following error.

```
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[OeQB_cJkRCmTDi4CbOlMfw][items_test][0]: AggregationExecutionException[Invalid terms aggregation order path [max_priority_score]. Terms buckets can only be sorted on a sub-aggregator path that is built out of zero or more single-bucket aggregations within the path and a final single-bucket or a metrics aggregation at the path end.]}{[OeQB_cJkRCmTDi4CbOlMfw][items_test][1]: AggregationExecutionException[Invalid terms aggregation order path [max_priority_score]. Terms buckets can only be sorted on a sub-aggregator path that is built out of zero or more single-bucket aggregations within the path and a final single-bucket or a metrics aggregation at the path end.]}{[OeQB_cJkRCmTDi4CbOlMfw][items_test][2]: AggregationExecutionException[Invalid terms aggregation order path [max_priority_score]. Terms buckets can only be sorted on a sub-aggregator path that is built out of zero or more single-bucket aggregations within the path and a final single-bucket or a metrics aggregation at the path end.]}{[OeQB_cJkRCmTDi4CbOlMfw][items_test][3]: AggregationExecutionException[Invalid terms aggregation order path [max_priority_score]. Terms buckets can only be sorted on a sub-aggregator path that is built out of zero or more single-bucket aggregations within the path and a final single-bucket or a metrics aggregation at the path end.]}{[OeQB_cJkRCmTDi4CbOlMfw][items_test][4]: AggregationExecutionException[Invalid terms aggregation order path [max_priority_score]. Terms buckets can only be sorted on a sub-aggregator path that is built out of zero or more single-bucket aggregations within the path and a final single-bucket or a metrics aggregation at the path end.]}]","status":500}
```

I think this because `max_priority_score` seems to be invalid order path, but I don't know how to solve this. Or I want to know another way to solve my original problem.
</description><key id="45059735">8005</key><summary>[Aggregation] Ordering buckets by their top document's score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pandora2000</reporter><labels /><created>2014-10-07T02:10:16Z</created><updated>2014-10-15T15:31:01Z</updated><resolved>2014-10-15T15:31:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T15:31:01Z" id="59224721">Hi @pandora2000 

I don't really understand what you are trying to achieve here, but the scripted agg returns a `value` which you are not referencing in your `order` clause, ie `max_priority_score.value`.

Not sure if that will work or not, but you should ask questions like these in the forum instead of here.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolator gets confused when query is added as a document that is not of .percolator type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8004</link><project id="" key="" /><description>It appears that the percolator gets confused and does not return a match after the first percolator query
if queries are unintentionally added as documents instead of percolator queries.  eg.

```
DELETE /twitter

# register FIRST percolator query
PUT /twitter/.percolator/FIRST
{
    "query" : {
        "match" : {
            "message" : "FIRST"
        }
    }
}

# create a document with the same query in the body
PUT /twitter/foo/FIRST_DOC
{
    "query" : {
        "match" : {
            "message" : "FIRST"
        }
    }
}
# register SECOND percolator query
PUT /twitter/.percolator/SECOND
{
    "query" : {
        "match" : {
            "message" : "SECOND"
        }
    }
}
# create a document with the same query in the body
PUT /twitter/foo/SECOND_DOC
{
    "query" : {
        "match" : {
            "message" : "SECOND"
        }
    }
}
# register THIRD percolator query
PUT /twitter/.percolator/THIRD
{
    "query" : {
        "match" : {
            "message" : "THIRD"
        }
    }
}
# create a document with the same query in the body
PUT /twitter/foo/THIRD_DOC
{
    "query" : {
        "match" : {
            "message" : "THIRD"
        }
    }
}

# Match found
GET /twitter/test/_percolate
{
  "doc":{
    "message":"FIRST"
  }
}

# Match not found
GET /twitter/test/_percolate
{
  "doc":{
    "message":"SECOND"
  }
}

# Match not found
GET /twitter/test/_percolate
{
  "doc":{
    "message":"THIRD"
  }
}

```
</description><key id="45049959">8004</key><summary>Percolator gets confused when query is added as a document that is not of .percolator type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">ppf2</reporter><labels /><created>2014-10-06T23:25:02Z</created><updated>2014-10-15T15:54:17Z</updated><resolved>2014-10-15T15:54:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-07T12:01:53Z" id="58173609">@ppf2 The reason that second and third percolator queries don't match is because of a mapping issue. When the first percolator query is parsed there is no message field, so it just uses the field name `message` (even though there is no mapping for it). When the second and third query are parsed there is a message field (`query.match.message`) which was introduced by the document indexed before and it uses that the field for the match query in the percolator document. 

There have been many mapping related issues with the percolator and because of this since 1.4.0.beta1 a field mapping needs to exist otherwise the indexing of a percolator query fails: https://github.com/elasticsearch/elasticsearch/pull/6928

So in this particular case if the `message` field is defined before adding a percolator query then the percolate requests do match with the previous added percolator queries.
</comment><comment author="clintongormley" created="2014-10-15T15:54:17Z" id="59228634">The fix in #6928 and requiring unambiguous field names (see #4081) will avoid this problem.  Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Duplicate document found while searching with parent/child setup - ES 1.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8003</link><project id="" key="" /><description>elasticsearchserver_node1:7104/metadata/asset/_search?q=_id:641804  returns two documents
But
elasticsearchserver_node1:7104/metadata/asset/641804?parent=80018039 returns only one document

There was not update to the document after initial index.

elasticsearchserver_node1:7104/metadata/asset/_search?q=_id:641804 returns following results
# 

{
    "took": 2,
    "timed_out": false,
    "_shards": {
        "total": 10,
        "successful": 10,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 1,
        "hits": [
            {
                "_index": "metadata",
                "_type": "asset",
                "_id": "641804",
                "_score": 1,
                "_source": {
                    "createdDate": 1410901296000,
                    "lastUpdatedDate": 1410901295989,
                    "id": 641804,
                    "status": "NEW",
                    "parentAssetId": 0,
                    "masterAssetId": null,
                    "source": "AUTOMATION",
                    "spaceCode": "creative-services",
                    "originatingAssetId": null,
                    "spaceShareCount": null,
                    "childCount": null,
                    "isAllMainFilesPresent": false,
                    "assetPath": [],
                    "assetExtension": {
                        "assetId": 641804,
                        "territoryCode": "LATAM",
                        "languageCode": null,
                        "partnerProvidedLocalizedName": null,
                        "personId": 0,
                        "characterId": 0
                    },
                    "assetFiles": [],
                    "assetAttributes": [],
                    "sharedSpaceCodes": [],
                    "futureEffectiveDate": null,
                    "assetWindows": [
                        {
                            "partnerId": "dcbd39e0-eff9-11e2-882e-12313924e0a4",
                            "windowStartDate": 1416016800000,
                            "windowEndDate": 1463281200000
                        }
                    ],
                    "assetSchema": "asset",
                    "slaveAsset": false,
                    "publishedDerivativePresent": false,
                    "sharedAcrossSpace": true,
                    "mainFiles": [],
                    "folderTypeAsset": false,
                    "genericAsset": false,
                    "metaTypeAsset": true,
                    "metaId": 80018039,
                    "sourceFiles": [],
                    "futurePublishedDerivativePresent": false,
                    "characterAsset": false,
                    "personAsset": false,
                    "masterAsset": false,
                    "timestamp": 1410901295989,
                    "_parent": "80018039"
                }
            },
            {
                "_index": "movie_metadata",
                "_type": "asset",
                "_id": "641804",
                "_score": 1,
                "_source": {
                    "_index": "metadata",
                    "_type": "asset",
                    "_id": "641804",
                    "_score": 1,
                    "_source": {
                        "createdDate": 1410901296000,
                        "lastUpdatedDate": 1410901295989,
                        "id": 641804,
                        "status": "NEW",
                        "parentAssetId": 0,
                        "masterAssetId": null,
                        "source": "AUTOMATION",
                        "spaceCode": "creative-services",
                        "originatingAssetId": null,
                        "spaceShareCount": null,
                        "childCount": null,
                        "isAllMainFilesPresent": false,
                        "assetPath": [],
                        "assetExtension": {
                            "assetId": 641804,
                            "territoryCode": "LATAM",
                            "languageCode": null,
                            "partnerProvidedLocalizedName": null,
                            "personId": 0,
                            "characterId": 0
                        },
                        "assetFiles": [],
                        "assetAttributes": [],
                        "sharedSpaceCodes": [],
                        "futureEffectiveDate": null,
                        "assetWindows": [
                            {
                                "partnerId": "dcbd39e0-eff9-11e2-882e-12313924e0a4",
                                "windowStartDate": 1416016800000,
                                "windowEndDate": 1463281200000
                            }
                        ],
                        "assetSchema": "asset",
                        "slaveAsset": false,
                        "publishedDerivativePresent": false,
                        "sharedAcrossSpace": true,
                        "mainFiles": [],
                        "folderTypeAsset": false,
                        "genericAsset": false,
                        "metaTypeAsset": true,
                        "metaId": 80018039,
                        "sourceFiles": [],
                        "futurePublishedDerivativePresent": false,
                        "characterAsset": false,
                        "personAsset": false,
                        "masterAsset": false,
                        "timestamp": 1410901295989,
                        "_parent": "80018039"
                    }
                }
            }
        ]
    }
}
# 

elasticsearchserver_node1:7104/metadata/asset/641804?parent=80018039  returns the following result

{
    "_index": "movie_metadata",
    "_type": "asset",
    "_id": "641804",
    "_version": 1,
    "found": true,
    "_source": {
        "createdDate": 1410901296000,
        "lastUpdatedDate": 1410901295989,
        "id": 641804,
        "status": "NEW",
        "parentAssetId": 0,
        "masterAssetId": null,
        "source": "AUTOMATION",
        "spaceCode": "creative-services",
        "originatingAssetId": null,
        "spaceShareCount": null,
        "childCount": null,
        "isAllMainFilesPresent": false,
        "assetPath": [],
        "assetExtension": {
            "assetId": 641804,
            "territoryCode": "LATAM",
            "languageCode": null,
            "partnerProvidedLocalizedName": null,
            "personId": 0,
            "characterId": 0
        },
        "assetFiles": [],
        "assetAttributes": [],
        "sharedSpaceCodes": [],
        "futureEffectiveDate": null,
        "assetWindows": [
            {
                "partnerId": "dcbd39e0-eff9-11e2-882e-12313924e0a4",
                "windowStartDate": 1416016800000,
                "windowEndDate": 1463281200000
            }
        ],
        "assetSchema": "asset",
        "slaveAsset": false,
        "publishedDerivativePresent": false,
        "sharedAcrossSpace": true,
        "mainFiles": [],
        "folderTypeAsset": false,
        "genericAsset": false,
        "metaTypeAsset": true,
        "metaId": 80018039,
        "sourceFiles": [],
        "futurePublishedDerivativePresent": false,
        "characterAsset": false,
        "personAsset": false,
        "masterAsset": false,
        "timestamp": 1410901295989,
        "_parent": "80018039"
    }
}
# 
</description><key id="45042598">8003</key><summary>Duplicate document found while searching with parent/child setup - ES 1.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">govindm</reporter><labels><label>feedback_needed</label></labels><created>2014-10-06T21:58:11Z</created><updated>2015-02-28T04:48:31Z</updated><resolved>2015-02-28T04:48:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="govindm" created="2014-10-06T21:59:06Z" id="58106807">Here is the ES version details

{
  "status" : 200,
  "name" : "i-03eb6751",
  "version" : {
    "number" : "1.0.1",
    "build_hash" : "5c03844e1978e5cc924dab2a423dc63ce881c42b",
    "build_timestamp" : "2014-02-25T15:52:53Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
</comment><comment author="govindm" created="2014-10-14T09:54:44Z" id="59016237">Any idea, if this is a known bug?
</comment><comment author="jpountz" created="2014-10-14T09:59:50Z" id="59016793">@govindm This is not a known bug. Can you run the following query so that we can see what routing values have been used for both documents:

`elasticsearchserver_node1:7104/metadata/asset/_search?q=_id:641804&amp;fields=_routing,_parent`
</comment><comment author="govindm" created="2014-10-17T02:17:28Z" id="59457839">I deleted the record and re-indexed again. I see this problem is occurs often. When it occurs again, I will run this query and report the result.
We don't set the routing explicitly but we do set the parent id, you could see that both doc has the same parent id.
</comment><comment author="clintongormley" created="2015-02-28T04:48:31Z" id="76510479">There's a good chance this was related to this issue: https://github.com/elasticsearch/elasticsearch/issues/8788

I'll close this one for now, feel free to reopen if you see the same thing on v1.4.4 or above.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: Clarify that index.routing.allocation.total_shards_per_node applies to replica and primary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8002</link><project id="" key="" /><description>Clarify that `index.routing.allocation.total_shards_per_node` applies to both primary and replica shards
</description><key id="45036581">8002</key><summary>Docs: Clarify that index.routing.allocation.total_shards_per_node applies to replica and primary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">suyograo</reporter><labels><label>docs</label><label>v1.3.0</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-06T21:05:01Z</created><updated>2014-10-14T01:21:01Z</updated><resolved>2014-10-14T01:10:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-13T10:59:29Z" id="58876067">LGTM please push this also to 1.3 branch
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Doc: Clarify that index.routing.allocation.total_shards_per_node means both primary and replica shards</comment><comment>Closes #8002</comment></comments></commit></commits></item><item><title>Groovy should be a mandatory dependency in ES pom</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8001</link><project id="" key="" /><description>In 1.4 mvel was removed and groovy was made the default scripting language however it is marked as an optional dependency in the Elasticsearch pom. Unless the user manually adds groovy as a dependency, Elasticsearch will not be able to run scripts throwing an exception that groovy is not supported.

Better yet, groovy could be nested (just like mvel before it) to prevent classloading issues which will happen considering groovy is more popular than mvel.
</description><key id="45031251">8001</key><summary>Groovy should be a mandatory dependency in ES pom</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">costin</reporter><labels><label>bug</label><label>low hanging fruit</label></labels><created>2014-10-06T20:19:39Z</created><updated>2014-10-28T11:12:24Z</updated><resolved>2014-10-28T11:12:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-10-06T20:22:00Z" id="58089992">By the way, it would be nice to have a lang-mvel project for those that don't want to migrate yet from mvel yet want to use the latest Elasticsearch version (despite mvel shortcomings).
</comment><comment author="dadoonet" created="2014-10-06T20:23:19Z" id="58090276">@costin We have one: https://github.com/elasticsearch/elasticsearch-lang-mvel
Not released yet for 1.4.0 though. Working on this.
</comment><comment author="dadoonet" created="2014-10-06T20:25:33Z" id="58090749">I went into the same issue. Actually, when it comes to Java projects, you just have to add the groovy dependency in your project if needed. See: https://github.com/elasticsearch/elasticsearch-river-rabbitmq/blob/master/pom.xml#L93-L98 for example.
Though not sure why we marked it as optional.
</comment><comment author="nik9000" created="2014-10-07T20:12:20Z" id="58252710">IIRC shading groovy is difficult because it tries to do reflection stuff.  I was sitting next to @dakrone while he was working on it last May and it was not as easy as it ought to be.
</comment><comment author="kimchy" created="2014-10-28T11:11:44Z" id="60740016">I think that in most cases where someone embeds ES, they are using it as a client node, where scripting is not needed. So I do think it makes sense to have it as an optional dependency. Shading would be a shame in terms of the cost to the jar size.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node attributes in _cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/8000</link><project id="" key="" /><description>It would be useful if `_cat/nodes` can output node attributes like rack_id, aws_zone and other user defined attributes. If users have a large cluster with nodes in multiple AZ's or racks, it would be great to have this info in `_cat/nodes` to understand the topology. 

Even if not displayed by default, we could allow users to pass in `nodes?h=rack_id`

Example:

```
host                     ip          heap.percent ram.percent load node.role master name rack_id
Suyogs-MacBook-Pro.local 192.168.1.6            0                  d         *      Jann rack_1
Suyogs-MacBook-Pro.local 192.168.1.6            0                  d         *      Hoder rack_2
```
</description><key id="45020763">8000</key><summary>Node attributes in _cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/metadave/following{/other_user}', u'events_url': u'https://api.github.com/users/metadave/events{/privacy}', u'organizations_url': u'https://api.github.com/users/metadave/orgs', u'url': u'https://api.github.com/users/metadave', u'gists_url': u'https://api.github.com/users/metadave/gists{/gist_id}', u'html_url': u'https://github.com/metadave', u'subscriptions_url': u'https://api.github.com/users/metadave/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/58244?v=4', u'repos_url': u'https://api.github.com/users/metadave/repos', u'received_events_url': u'https://api.github.com/users/metadave/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/metadave/starred{/owner}{/repo}', u'site_admin': False, u'login': u'metadave', u'type': u'User', u'id': 58244, u'followers_url': u'https://api.github.com/users/metadave/followers'}</assignee><reporter username="">suyograo</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-10-06T18:52:37Z</created><updated>2015-07-29T21:05:07Z</updated><resolved>2015-07-29T21:01:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2015-07-29T01:24:17Z" id="125796953">Sorry to just be noticing this, but I caught the email from @metadave's PR.

This has a similar problem to `_cat/fielddata` that #10249 is meant to address, namely that column names should never be dynamic. We always denormalize instead. See `_cat/shards` or `_cat/segments` for example.

In this case, since node attributes are such a small part of a node identity, I wonder if it doesn't make sense to have a new API, `_cat/nodeattrs` or somesuch?

```
% GET /_cat/nodeattrs
node    attr.name     attr.value
n1      zone          zone2323
n2      zone          zone8282
```

The default could have more info, maybe `host` and `port`.
</comment><comment author="metadave" created="2015-07-29T01:30:31Z" id="125799145">+1, I like how your `GET /_cat/nodeattrs` example looks
</comment><comment author="metadave" created="2015-07-29T21:05:07Z" id="126095410">Node attributes are available in `_cat/nodeattrs` via #12534 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestNodeAttrsAction.java</file></files><comments><comment>Add _cat/nodeattrs API</comment></comments></commit></commits></item><item><title>Admin: Filter non data nodes from _cat/allocation output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7999</link><project id="" key="" /><description>Closes #7998
</description><key id="45018957">7999</key><summary>Admin: Filter non data nodes from _cat/allocation output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/suyograo/following{/other_user}', u'events_url': u'https://api.github.com/users/suyograo/events{/privacy}', u'organizations_url': u'https://api.github.com/users/suyograo/orgs', u'url': u'https://api.github.com/users/suyograo', u'gists_url': u'https://api.github.com/users/suyograo/gists{/gist_id}', u'html_url': u'https://github.com/suyograo', u'subscriptions_url': u'https://api.github.com/users/suyograo/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1595958?v=4', u'repos_url': u'https://api.github.com/users/suyograo/repos', u'received_events_url': u'https://api.github.com/users/suyograo/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/suyograo/starred{/owner}{/repo}', u'site_admin': False, u'login': u'suyograo', u'type': u'User', u'id': 1595958, u'followers_url': u'https://api.github.com/users/suyograo/followers'}</assignee><reporter username="">suyograo</reporter><labels><label>enhancement</label></labels><created>2014-10-06T18:37:18Z</created><updated>2015-11-21T19:26:32Z</updated><resolved>2015-11-21T19:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-10-10T13:44:49Z" id="58657134">We should probably have a test that has a mix of data and non-data nodes and check that all data nodes are returned but no non-data nodes are returned.

Also, do we have an equivalent non-cat API. If so, we should make sure the two are consistent with each other
</comment><comment author="suyograo" created="2014-10-10T18:03:33Z" id="58693630">@colings86 There is no equivalent non-cat API, this info is got from the routing table in the cluster state. 

+1 on tests, will add it
</comment><comment author="clintongormley" created="2015-11-21T19:26:32Z" id="158675936">Not sure when but this has been implemented in 2.0. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_cat/allocation output should filter non data nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7998</link><project id="" key="" /><description>Shard allocation cannot happen on non data nodes, so there is no use showing `0b` for a non data node in the output. It is better to remove those rows

Example -- 

```
5 61.5gb 403.5gb 465.1gb 13 Suyogs-MacBook-Pro.local 192.168.1.6 Sphinxor
0     0b                    Suyogs-MacBook-Pro.local 192.168.1.6 Hoder
```
</description><key id="45018243">7998</key><summary>_cat/allocation output should filter non data nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/suyograo/following{/other_user}', u'events_url': u'https://api.github.com/users/suyograo/events{/privacy}', u'organizations_url': u'https://api.github.com/users/suyograo/orgs', u'url': u'https://api.github.com/users/suyograo', u'gists_url': u'https://api.github.com/users/suyograo/gists{/gist_id}', u'html_url': u'https://github.com/suyograo', u'subscriptions_url': u'https://api.github.com/users/suyograo/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1595958?v=4', u'repos_url': u'https://api.github.com/users/suyograo/repos', u'received_events_url': u'https://api.github.com/users/suyograo/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/suyograo/starred{/owner}{/repo}', u'site_admin': False, u'login': u'suyograo', u'type': u'User', u'id': 1595958, u'followers_url': u'https://api.github.com/users/suyograo/followers'}</assignee><reporter username="">suyograo</reporter><labels><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-10-06T18:31:15Z</created><updated>2015-11-21T19:26:26Z</updated><resolved>2015-11-21T19:26:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T19:26:26Z" id="158675934">Not sure when but this has been implemented in 2.0. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] Add plugin to list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7997</link><project id="" key="" /><description>This plugin provides trigram accelerated regular expression search for when
you need reasonably fast arbitrary matching.
</description><key id="45004163">7997</key><summary>[docs] Add plugin to list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-10-06T16:29:55Z</created><updated>2014-10-15T11:57:26Z</updated><resolved>2014-10-15T11:57:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T11:57:15Z" id="59194551">thanks @nik9000 - merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Add plugin to list</comment></comments></commit></commits></item><item><title>Adding JavaDocs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7996</link><project id="" key="" /><description>Hi,

I am a newbie to Elasticsearch, i was going through the code and found lack of Documentation. Because of which lot of confusion about what is happening.

As i am going through the code (fresh eye) will like to add JavaDocs while browsing the code.
</description><key id="44990702">7996</key><summary>Adding JavaDocs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jamalahmedmaaz</reporter><labels /><created>2014-10-06T14:37:20Z</created><updated>2014-10-15T11:13:02Z</updated><resolved>2014-10-15T11:13:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-10-06T14:41:25Z" id="58026761">If I'm correct, duplicate of #1203

Note that that with some IDEs you can attach source code which gives you javadoc.
</comment><comment author="nik9000" created="2014-10-06T14:44:12Z" id="58027237">@dadoonet, I'm pretty sure he's saying that he's adding some javadocs and will send a pull request with the new docs at some point.
</comment><comment author="jamalahmedmaaz" created="2014-10-06T14:52:16Z" id="58028579">@nik9000 You are right.

@dadoonet Can we reopen this.
</comment><comment author="dadoonet" created="2014-10-06T14:56:08Z" id="58029232">Argh! When you read too quickly. 

Reopening though we don't really need an issue for this but PR.

Thanks for the effort BTW.
</comment><comment author="clintongormley" created="2014-10-15T11:13:02Z" id="59190496">@jamalahmedmaaz I'm going to close this as we don't need an open issue just to declare intent.  When you have some javadocs to contribute, please open a pull request and we'll be happy to check and merge them.

Also, you will need to sign the CLA before we can do so: http://www.elasticsearch.org/contributor-agreement/

Looking forward to the PRs :)  thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove MasterFaultDetection.Listener.notListedOnMaster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7995</link><project id="" key="" /><description>It is never used in practice. We treat it as a master failure (using NodeDoesNotExistOnMasterException).
</description><key id="44987103">7995</key><summary>Remove MasterFaultDetection.Listener.notListedOnMaster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-06T14:06:19Z</created><updated>2015-06-07T11:59:10Z</updated><resolved>2014-10-06T15:13:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-06T14:24:21Z" id="58024012">LGTM
</comment><comment author="martijnvg" created="2014-10-06T14:40:42Z" id="58026658">Right, in practice this is redundant. LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>src/test/java/org/elasticsearch/discovery/ZenFaultDetectionTests.java</file></files><comments><comment>Discovery: remove MasterFaultDetection.Listener.notListedOnMaster</comment></comments></commit></commits></item><item><title>Added `_shards` header to all write responses.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7994</link><project id="" key="" /><description>The header indicates to how many shard copies (primary and replicas shards) a write was supposed to go to, to how many shard copies to write succeeded and potentially captures shard failures if writing into a replica shard fails.

For async writes it also includes the number of shards a write is still pending.
</description><key id="44983166">7994</key><summary>Added `_shards` header to all write responses.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:REST</label><label>enhancement</label><label>resiliency</label><label>v2.0.0-beta1</label></labels><created>2014-10-06T13:29:56Z</created><updated>2015-06-06T19:22:56Z</updated><resolved>2015-01-08T17:10:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-09T21:44:56Z" id="58582806">@javanna @bleskes I updated the PR and the provided feedback has been applied.

The other most noticeable change is that the replication logic in TransportShardReplicationOperationAction has been changed to keep better track of the state of replication while it is being performed. This helped to simplify the replication logic. 
</comment><comment author="bleskes" created="2014-10-13T10:57:22Z" id="58875869">I did a review cycle. I like the simplification in the `TransportShardReplicationOperation` a lot! 

Although we don't use the `ReplicaRequest` feature of `TransportShardReplicationOperation` at the moment. I think it's a shame to loose it. I agree that the old `PrimaryResponse` now have a better name and implementations should be bothered by it. Maybe make `shardOperationOnPrimary` return a tuple of Response,RepicaRequest? 

I have a concern regarding BWC of aggregated requests like bulk and delete_by_query. I might be missing something but I think we will currently use 0 for total and successful shards when aggregating responses from older nodes. If so, I think we should not aggregate if one of the sub request went to an older node (as we don't know at the moment). I'm think of suppressing the `_shards` section in that case on the top level of the request and also have a ShardInfo instance which returns -1 on all numbers. Make sense?

It would also be great to have some rest layer tests.
</comment><comment author="martijnvg" created="2014-11-05T11:22:03Z" id="61793300">@bleskes @javanna I applied your feedback and updated the PR.
</comment><comment author="bleskes" created="2014-11-06T10:51:11Z" id="61962074">@martijnvg as discussed this can go to master only. can you ping when the bwc code is removed?
</comment><comment author="martijnvg" created="2014-11-12T09:07:32Z" id="62688711">@bleskes I removed the bwc logic.
</comment><comment author="bleskes" created="2014-12-01T17:46:56Z" id="65104356">@martijnvg I went through it and left some mostly cosmetic comments. I really like the simplifications in the TSROA (guess what that is :)) and the base class ActionWriteResponse shard code.
</comment><comment author="martijnvg" created="2014-12-01T18:15:09Z" id="65108776">⌘n + TSROA = TransportShardReplicationOperationAction
:)
</comment><comment author="martijnvg" created="2014-12-01T22:50:17Z" id="65149550">@bleskes Thanks for the feedback! I updated the PR.
</comment><comment author="bleskes" created="2014-12-05T11:21:02Z" id="65777312">Changes look good to me. I still have a concern regarding the ShardInfo(responses, primaryFailures) constructor and the numbers we will return when, for example, an index replication operation will fail on all primary shards (total=0, faiulre=#primaries). I think we should fix it. Also add a unit test to make sure it does the right thing.

I also made one suggestion to change the tests to use the global cluster. O.w. nice code reuse now.
</comment><comment author="martijnvg" created="2014-12-05T21:38:28Z" id="65858585">@bleskes I rebased with master and updated the PR based on your feedback points (last four commits)
</comment><comment author="bleskes" created="2014-12-07T20:56:24Z" id="65954599">Thx @martijnvg!. Latest changes look good to me. I left some minor cosmetic comments about the XContent output.

I still have some concerns about the the failure count in the case of TransportIndexReplicationOperation. As it is now, one replica shard failing counts the same as one primary failure (and thus all it's replicas), i.e., both add up to one failure. I think we should have the failed shard count be the primary + replicas in the second case. The failure list will then have one exception for them all. Example, say you have 2 primary shards each with 1 replica and one of the primary failed, we will have `{ _total: 4, _successful: 2, _failed: 2, _failures: "...." }` (as opposed to `_failed: 1` the current code generates).
</comment><comment author="martijnvg" created="2014-12-08T20:42:25Z" id="66183441">@bleskes I addressed the minor feedback. 

Also I tried to address your concern by including replica failures too in case an operation failed on the primary shard. Not sure if we should repeat the primary exception or just tell we never executed on replica shards.
</comment><comment author="bleskes" created="2014-12-08T20:56:29Z" id="66185622">Thx @martijnvg .

&gt; Not sure if we should repeat the primary exception or just tell we never executed on replica shards.

yeah, I see what you mean.  I was thinking wrapping the primary failure with a new exception which has the right info in the message? (ala "Failed to execute on shard [](primary + [] replicas): [MESSAGE]")

left some other minor comments.
</comment><comment author="martijnvg" created="2014-12-09T12:24:34Z" id="66274934">Thanks for the feedback @bleskes. I updated the PR. Since we don't have exceptions (the Failure class has no failure wrapping) for the replica shards, I instead just wrap the original primary shard failure on the message of the replica shard failure. I think this is sufficient?
</comment><comment author="martijnvg" created="2015-01-06T09:40:07Z" id="68844680">@bleskes @javanna I applied the feedback and Boaz's commits (https://github.com/bleskes/elasticsearch/commit/a9b659d1c504ca2f3ec1416ceb3d01be9e4b81aa and https://github.com/bleskes/elasticsearch/commit/61804179be69c8f5e723e5da28fc07b35474f080), rebased to master and squashed everything to a single commit for clarity purposes. 

I think this is getting close to get merged now, would be great if someone else can take a look at the PR before it gets merged in.
</comment><comment author="imotov" created="2015-01-06T17:36:38Z" id="68900401">I left a couple of small comments. In general looks good to me.
</comment><comment author="martijnvg" created="2015-01-07T06:47:27Z" id="68985204">@imotov I updated the PR and applied your comments.
</comment><comment author="imotov" created="2015-01-07T15:23:13Z" id="69036454">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionWriteResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkShardResponse.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteResponse.java</file><file>src/main/java/org/elasticsearch/action/delete/IndexDeleteResponse.java</file><file>src/main/java/org/elasticsearch/action/delete/ShardDeleteResponse.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportIndexDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/TransportShardDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/IndexDeleteByQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/ShardDeleteByQueryResponse.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportIndexDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexResponse.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndexReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndicesReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/bulk/RestBulkAction.java</file><file>src/main/java/org/elasticsearch/rest/action/delete/RestDeleteAction.java</file><file>src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/index/RestIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>src/test/java/org/elasticsearch/bwcompat/BasicBackwardsCompatibilityTest.java</file><file>src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java</file><file>src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/document/ShardInfoTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Core: Added `_shards` header to all write responses.</comment></comments></commit></commits></item><item><title>No parser for element [aggs]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7993</link><project id="" key="" /><description>I'm getting this particular error:

It's very frustrating since all was running fine on my local machine (ES 1.1) until I tried this on my server (ES 1.3)

Some more info:
ES 1.3 latest (Ubuntu, installed using apt-get)
Symfony 2, FOSElastica latest version

Please let me know if you need any more info

```
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {

        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "identifier": "logistics"
              }
            }
          ]
        }
      }
    }
  },
  "aggs": {
    "price_stats": {
      "stats": {
        "field": "price"
      }
    },
    "pickup_path_term": {
      "terms": {
        "size": 0,
        "field": "pickup"
      }
    },
    "pickup_date_stats": {
      "stats": {
        "field": "pickup_date"
      }
    },
    "delivery_path_term": {
      "terms": {
        "size": 0,
        "field": "delivery"
      }
    },
    "delivery_date_stats": {
      "stats": {
        "field": "delivery_date"
      }
    },
    "category_path_term": {
      "terms": {
        "size": 0,
        "field": "category"
      }
    }
  },
  "from": 0,
  "size": 3
}
```

Detailed error:

```
{
  "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[BSh76RkLTjSjwUT29_Vq8g][abcdef_order][4]: SearchParseException[[abcdef_order][4]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"bool\":{\"must\":[{\"term\":{\"identifier\":\"logistics\"}}]}}}},\"aggregations\":{\"price_stats\":{\"stats\":{\"field\":\"price\"}},\"pickup_path_term\":{\"terms\":{\"size\":0,\"field\":\"pickup\"}},\"pickup_date_stats\":{\"stats\":{\"field\":\"pickup_date\"}},\"delivery_path_term\":{\"terms\":{\"size\":0,\"field\":\"delivery\"}},\"delivery_date_stats\":{\"stats\":{\"field\":\"delivery_date\"}},\"category_path_term\":{\"terms\":{\"size\":0,\"field\":\"category\"}}},\"from\":0,\"size\":3}]]]; nested: SearchParseException[[abcdef_order][4]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [No parser for element [aggregations]]]; }{[BSh76RkLTjSjwUT29_Vq8g][abcdef_order][2]: SearchParseException[[abcdef_order][2]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"bool\":{\"must\":[{\"term\":{\"identifier\":\"logistics\"}}]}}}},\"aggregations\":{\"price_stats\":{\"stats\":{\"field\":\"price\"}},\"pickup_path_term\":{\"terms\":{\"size\":0,\"field\":\"pickup\"}},\"pickup_date_stats\":{\"stats\":{\"field\":\"pickup_date\"}},\"delivery_path_term\":{\"terms\":{\"size\":0,\"field\":\"delivery\"}},\"delivery_date_stats\":{\"stats\":{\"field\":\"delivery_date\"}},\"category_path_term\":{\"terms\":{\"size\":0,\"field\":\"category\"}}},\"from\":0,\"size\":3}]]]; nested: SearchParseException[[abcdef_order][2]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [No parser for element [aggregations]]]; }{[BSh76RkLTjSjwUT29_Vq8g][abcdef_order][3]: SearchParseException[[abcdef_order][3]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"bool\":{\"must\":[{\"term\":{\"identifier\":\"logistics\"}}]}}}},\"aggregations\":{\"price_stats\":{\"stats\":{\"field\":\"price\"}},\"pickup_path_term\":{\"terms\":{\"size\":0,\"field\":\"pickup\"}},\"pickup_date_stats\":{\"stats\":{\"field\":\"pickup_date\"}},\"delivery_path_term\":{\"terms\":{\"size\":0,\"field\":\"delivery\"}},\"delivery_date_stats\":{\"stats\":{\"field\":\"delivery_date\"}},\"category_path_term\":{\"terms\":{\"size\":0,\"field\":\"category\"}}},\"from\":0,\"size\":3}]]]; nested: SearchParseException[[abcdef_order][3]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [No parser for element [aggregations]]]; }{[BSh76RkLTjSjwUT29_Vq8g][abcdef_order][0]: SearchParseException[[abcdef_order][0]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"bool\":{\"must\":[{\"term\":{\"identifier\":\"logistics\"}}]}}}},\"aggregations\":{\"price_stats\":{\"stats\":{\"field\":\"price\"}},\"pickup_path_term\":{\"terms\":{\"size\":0,\"field\":\"pickup\"}},\"pickup_date_stats\":{\"stats\":{\"field\":\"pickup_date\"}},\"delivery_path_term\":{\"terms\":{\"size\":0,\"field\":\"delivery\"}},\"delivery_date_stats\":{\"stats\":{\"field\":\"delivery_date\"}},\"category_path_term\":{\"terms\":{\"size\":0,\"field\":\"category\"}}},\"from\":0,\"size\":3}]]]; nested: SearchParseException[[abcdef_order][0]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [No parser for element [aggregations]]]; }{[BSh76RkLTjSjwUT29_Vq8g][abcdef_order][1]: SearchParseException[[abcdef_order][1]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"bool\":{\"must\":[{\"term\":{\"identifier\":\"logistics\"}}]}}}},\"aggregations\":{\"price_stats\":{\"stats\":{\"field\":\"price\"}},\"pickup_path_term\":{\"terms\":{\"size\":0,\"field\":\"pickup\"}},\"pickup_date_stats\":{\"stats\":{\"field\":\"pickup_date\"}},\"delivery_path_term\":{\"terms\":{\"size\":0,\"field\":\"delivery\"}},\"delivery_date_stats\":{\"stats\":{\"field\":\"delivery_date\"}},\"category_path_term\":{\"terms\":{\"size\":0,\"field\":\"category\"}}},\"from\":0,\"size\":3}]]]; nested: SearchParseException[[abcdef_order][1]: query[ConstantScore(BooleanFilter(+cache(identifier:logistics)))],from[-1],size[-1]: Parse Failure [No parser for element [aggs]]]; }]",
  "status": 400
}
```
</description><key id="44977274">7993</key><summary>No parser for element [aggs]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yellow1912</reporter><labels /><created>2014-10-06T12:24:48Z</created><updated>2014-11-26T16:52:25Z</updated><resolved>2014-10-06T12:43:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yellow1912" created="2014-10-06T12:43:57Z" id="58011295">I'm stupid. When I installed ElasticSearch, somehow I chose to keep the old config file which I don't remember having anyhow, it seems like that messed things up for me. Sorry everyone. And thank you very much for Fantastic Elastic.
</comment><comment author="sameersoni" created="2014-11-26T16:52:25Z" id="64675733">@yellow1912 - So what did you finally do to resolve it?
I am also facing the same issue on my ubuntu server, whereas it works fine on my local mac.
It started coming in recently when I started aggregating search results.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_source vs fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7992</link><project id="" key="" /><description>Hey im curious about source and fields retrieval.

Both feel broken because:
- "fields" is fast to retrieve BUT merges data but in the end does not retrieve the data as it has been put in / sent.
- "_source" is very slow (factor 5) to retrieve a single field and does return exact what has been put in.

Is there something in between?
I need the speed of fields (and maybe in ram, this would fit for our index) and the output as i put it in.
</description><key id="44973580">7992</key><summary>_source vs fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">julianhille</reporter><labels /><created>2014-10-06T11:35:19Z</created><updated>2015-04-30T19:36:57Z</updated><resolved>2014-10-15T11:48:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-10-06T12:31:25Z" id="58010079">I imagine this is something probably more for the mailing list but I find `_source` to be the same speed as `fields` even though my fields vary in size.

There isn't something between them because `_source` is designed to load the json of the document and filter it while `fields` is designed to load stored fields.  `fields` can also filter the source document for backwards compatibility with pre-1.0 releases which muddies the water.

The `fields` interface changes the result to make everything look like a stored field even if it does the source filtering.  Stored fields support multi-valued fields but don't have an object structure like json so that is why they look the way they do. 
</comment><comment author="clintongormley" created="2014-10-15T11:48:29Z" id="59193720">@nik9000 has summed it up correctly.  Closing
</comment><comment author="julianhille" created="2015-04-30T19:36:57Z" id="97940415">i still the that issue.

{
  "query": {
    "filtered": {
      "filter": {
        "term": {
          "state": 139
        }
      },
      "query": {
        "match_all": {}
      }
    }
  },
  "fields": [
    "id"
  ]
}
gives an average of 7ms

where as the same withL

{
  "query": {
    "filtered": {
      "filter": {
        "term": {
          "state": 139
        }
      },
      "query": {
        "match_all": {}
      }
    }
  },
  "_source": [
    "id"
  ]
}

has an average of 19ms
which is more then twice the duration of fields.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting with field _all doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7991</link><project id="" key="" /><description>Unlike documented in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html the script below doesn't return any highlights at all (elasticsearch 1.3.4).

``` bash
#!/bin/bash
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test/test/1 -d '{
    "content":"some content"
}'
sleep 1
curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "query":{
        "match":{"_all":"some"}
    },
    "highlight": {
        "fields":{
            "_all":{}
        }
    }
}'
```

This works:

``` bash
#!/bin/bash
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test/test/1 -d '{
    "content":"some content"
}'
sleep 1
curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "query":{
        "match":{"_all":"some"}
    },
    "highlight": {
        "fields":{
            "*":{}
        }
    }
}'
```
</description><key id="44953282">7991</key><summary>Highlighting with field _all doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">make</reporter><labels /><created>2014-10-06T07:08:12Z</created><updated>2014-10-15T11:46:37Z</updated><resolved>2014-10-15T11:46:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T11:46:37Z" id="59193535">The `_all` field needs to be stored in order to highlight on it. I've updated the documentation to reflect that.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Update highlighting.asciidoc</comment></comments></commit></commits></item><item><title>Move Clear Indices Cache, Flush, Optimize, Recovery, Refresh, Segments, Indices Stats, Indices Status  API to use TransportNodesOperationAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7990</link><project id="" key="" /><description>All of those APIs send requests to all shards (both primary and replicas) of their relevant indices. They currently use the TransportBroadcastOperationAction infrastructure.

TransportBroadcastOperationAction is built to send request for one shard per replication group and send a request per shard it needs (and falls back  upon failure). The above API do not need this fall back mechanism and are commonly sent to many shards at once (for example, `GET _stats` goes to all shards of all indices). This results in an inefficiency where we may send hundreds of requests to a single node (one per every shard it holds). It will be more efficient to group those request per node and use the TransportNodesOperationAction model. 
</description><key id="44927921">7990</key><summary>Move Clear Indices Cache, Flush, Optimize, Recovery, Refresh, Segments, Indices Stats, Indices Status  API to use TransportNodesOperationAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2014-10-05T20:29:03Z</created><updated>2015-11-22T10:19:04Z</updated><resolved>2015-08-29T20:45:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/recovery/ShardRecoveryResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsResponse.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/ShardStats.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/TransportUpgradeStatusAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusRequest.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/upgrade/get/UpgradeStatusResponse.java</file><file>core/src/main/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeAction.java</file><file>core/src/main/java/org/elasticsearch/cluster/routing/RoutingTable.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestSegmentsAction.java</file><file>core/src/main/java/org/elasticsearch/transport/RequestHandlerRegistry.java</file><file>core/src/main/java/org/elasticsearch/transport/TransportService.java</file><file>core/src/test/java/org/elasticsearch/action/IndicesRequestIT.java</file><file>core/src/test/java/org/elasticsearch/action/support/broadcast/node/TransportBroadcastByNodeActionTests.java</file><file>core/src/test/java/org/elasticsearch/benchmark/recovery/ReplicaRecoveryBenchmark.java</file><file>core/src/test/java/org/elasticsearch/cluster/routing/RoutingTableTest.java</file><file>core/src/test/java/org/elasticsearch/discovery/zen/ZenDiscoveryIT.java</file><file>core/src/test/java/org/elasticsearch/document/ShardInfoIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryBackwardsCompatibilityIT.java</file><file>core/src/test/java/org/elasticsearch/gateway/RecoveryFromGatewayIT.java</file><file>core/src/test/java/org/elasticsearch/index/shard/IndexShardTests.java</file><file>core/src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryIT.java</file><file>core/src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RecoveryWhileUnderLoadIT.java</file><file>core/src/test/java/org/elasticsearch/recovery/RelocationIT.java</file><file>core/src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java</file></files><comments><comment>Add mechanism for transporting shard-level actions by node</comment></comments></commit></commits></item><item><title>Docs: Fix curl statements in query-cache.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7989</link><project id="" key="" /><description /><key id="44914271">7989</key><summary>Docs: Fix curl statements in query-cache.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">henrikno</reporter><labels><label>docs</label></labels><created>2014-10-05T13:26:50Z</created><updated>2014-10-15T11:16:57Z</updated><resolved>2014-10-15T11:16:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T11:16:36Z" id="59190796">Thanks @henrikno . Merged
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Fix curl statements in query-cache.asciidoc</comment></comments></commit></commits></item><item><title>Nested Types are empty when creating a new index </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7988</link><project id="" key="" /><description>When I create a new index with a Nested object or Nested Object Collection, the index is created but no data is added. The Nested object is empty. I tried the documentated examples as well, and these yield the same result. Everything works fine for simple properties or an array with simple types.

Am I doing something wrong, or is this a bug?

greetings Damien

Using Version elasticsearch-1.3.2
</description><key id="44897231">7988</key><summary>Nested Types are empty when creating a new index </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienbod</reporter><labels /><created>2014-10-04T22:59:49Z</created><updated>2014-10-05T14:10:38Z</updated><resolved>2014-10-05T14:10:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="damienbod" created="2014-10-05T14:10:25Z" id="57937153">Sorry, I found the problem. Elasticsearch works fine, it's just the Elastic HQ does not display it properly.

greetings Damien
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change IndexPrimaryShardNotAllocatedException from 409 to 500</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7987</link><project id="" key="" /><description>Closes #7632

Change IndexPrimaryShardNotAllocatedException from 409 (RestStatus.CONFLICT) to 500 (RestStatus.INTERNAL_SERVER_ERROR)
</description><key id="44889546">7987</key><summary>Change IndexPrimaryShardNotAllocatedException from 409 to 500</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">reuben-sutton</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-04T17:48:30Z</created><updated>2015-06-07T17:01:24Z</updated><resolved>2014-10-20T19:24:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="reuben-sutton" created="2014-10-14T14:05:04Z" id="59049475">@clintongormley could you review please?
</comment><comment author="clintongormley" created="2014-10-16T13:11:33Z" id="59358279">I've labelled this for review - my biggest concern is whether any tests start failing?  @bleskes could you take a look at this please?
</comment><comment author="bleskes" created="2014-10-16T18:53:05Z" id="59411139">Test pass and it looks good to me. We can consider in a future change whether to remove all together (or use it more) - it's hardly used.

@reuben-sutton can I ask you to sign our CLA so I can merge it in: http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="reuben-sutton" created="2014-10-20T16:05:42Z" id="59786140">Done: https://elasticsearch.echosign.com/public/viewAgreement?aid=XEUNT4MXW5U4K4N&amp;eid=XEUZHMT247BX97C&amp;
</comment><comment author="bleskes" created="2014-10-20T19:28:38Z" id="59825317">@reuben-sutton merged. Thx!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/IndexPrimaryShardNotAllocatedException.java</file></files><comments><comment>Change IndexPrimaryShardNotAllocatedException from 409 (RestStatus.CONFLICT) to 500 (RestStatus.INTERNAL_SERVER_ERROR)</comment></comments></commit></commits></item><item><title>elasticsearch pagination not working in rails with will_paginate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7986</link><project id="" key="" /><description>I'm trying to integrate Tire into my site and I'm having difficulty with pagination. I've tried paginating the results outside of the context of Tire and will_paginate is working on that Array. However, when I try will_paginate within the context of Tire I'm having one large problem.

Will_Paginate will display the correct number of pages with consideration of :per_page but when I click on that page the results are not loaded, rather they are the same as on the first page. The page number is highlighted in the will_paginate navigation.

I'm not sure what is going on, but I did this and it is working.
@search_results = @search_results.paginate(:page =&gt; params[:page], :per_page =&gt; 5)

Does anyone have any thoughts? 
</description><key id="44881647">7986</key><summary>elasticsearch pagination not working in rails with will_paginate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirav-panchal</reporter><labels /><created>2014-10-04T12:43:33Z</created><updated>2014-10-15T15:31:16Z</updated><resolved>2014-10-15T15:31:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="reuben-sutton" created="2014-10-04T17:11:15Z" id="57912056">Tire has been deprecated for a long time.

The replacement is: https://github.com/elasticsearch/elasticsearch-ruby
</comment><comment author="nirav-panchal" created="2014-10-07T05:45:42Z" id="58139554">Thanks man.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disabling doc values silently ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7985</link><project id="" key="" /><description>If you create a field with `doc_values` set to `false` (or not specified), then trying to update the mapping to `true` throws an exception.

However, if `doc_values` is initially set to `true`, then trying to update it to `false` is silently ignored:

```
DELETE /t
PUT /t
POST /t/_mapping/t
{
  "properties": {
    "foo": {
      "type": "long",
      "doc_values": true
    }
  }
}

POST /t/_mapping/t
{
  "properties": {
    "foo": {
      "type": "long",
      "doc_values": false
    }
  }
}

GET /t/_mapping
```

Returns:

```
{
   "t": {
      "mappings": {
         "t": {
            "properties": {
               "foo": {
                  "type": "long",
                  "doc_values": true
               }
            }
         }
      }
   }
}
```
</description><key id="44879258">7985</key><summary>Disabling doc values silently ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-10-04T10:53:02Z</created><updated>2015-06-04T08:35:51Z</updated><resolved>2015-06-04T08:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-06T09:13:18Z" id="57991810">@jpountz I assigned this to you!
</comment><comment author="jpountz" created="2014-10-08T15:27:02Z" id="58375879">This was actually done on purpose in https://github.com/elasticsearch/elasticsearch/issues/4560, see `AbstractFieldMapper.merge`:

``` java
        if (!this.hasDocValues() &amp;&amp; fieldMergeWith.hasDocValues()) {
            // don't add conflict if this mapper has doc values while the mapper to merge doesn't since doc values are implicitely set
            // when the doc_values field data format is configured
            mergeContext.addConflict("mapper [" + names.fullName() + "] has different " + TypeParsers.DOC_VALUES + " values");
        }
```

If we did not add a conflict, this would mean that if you initially create your mappings with just `fielddata.format=doc_values` and later perform a mapping update with `fielddata.format=array`, you would get an exception because the mapping update doesn't have doc values enabled. Is it the expected behaviour?
</comment><comment author="clintongormley" created="2014-10-15T16:35:41Z" id="59235214">@jpountz - I don't understand the doc_values vs array comment, but I think we should at least report what we accept and what we don't.  So I'm happy for the attempt to disable doc_values to return an exception, rather than being silently ignored.
</comment><comment author="jpountz" created="2015-06-04T08:35:50Z" id="108785767">This has been fixed by @rjernst in the recent mappings fixes.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Order of the field name hashes depend on JVM implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7984</link><project id="" key="" /><description>Doing a simple multi-match query on a document that contains multiple nested field types with the same field names lead to different results when using JDK8 or JDK7. 

JDK 7 --&gt; 1 hit
            "version": "1.7.0_67",
            "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
            "vm_version": "24.65-b04",
            "vm_vendor": "Oracle Corporation"

JDK 8 --&gt; 5 hits
            "version": "1.8.0_20",
            "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
            "vm_version": "25.20-b23",
            "vm_vendor": "Oracle Corporation",

Sample data and more details can be found here: 
https://groups.google.com/forum/#!topic/elasticsearch/3m9vPPokslw
</description><key id="44876964">7984</key><summary>Order of the field name hashes depend on JVM implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timeu</reporter><labels /><created>2014-10-04T08:41:56Z</created><updated>2014-10-15T11:01:39Z</updated><resolved>2014-10-15T11:01:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-04T10:16:32Z" id="57900589">Hi @timeu 

Yes, Java 8 has changed hash randomization, which can result in the "short name" for fields being resolved to different fields.  In 2.0, we're intending to remove support for short names, to make field resolution unambiguous: see #4081 

In the meantime, you can refer to fields using the full path to remove this ambiguity.
</comment><comment author="clintongormley" created="2014-10-15T11:01:39Z" id="59189423">Closing this in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>more helpful MapperParsingException errors?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7983</link><project id="" key="" /><description>When adding some templates to elasticsearch I had:

dynamic_templates: [{"name": {&lt;stuff&gt;}, "other_name": {&lt;stuff&gt;}}]

when I needed:

[{"name": { &lt;stuff&gt; } }, {"other_name": { &lt;stuff&gt; } }]

however the error message 
 [2014-10-03 11:12:13,837][DEBUG][action.admin.indices.create] [logstash1p] [syslog-iophx-2014.10.03] failed to create org.elasticsearch.index.mapper.MapperParsingException: mapping [_default_]

Was less than helpful for determining where in the template to find the error.  It would be really cool if an error like that could be made a bit more descriptive to help with debugging

cheers
</description><key id="44854437">7983</key><summary>more helpful MapperParsingException errors?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CullyB</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-10-03T22:15:01Z</created><updated>2015-11-21T19:19:50Z</updated><resolved>2015-11-21T19:19:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="torrancew" created="2014-10-03T23:21:21Z" id="57879322">:+1: I find that template errors are among the harder ES issues to debug. It would be fantastic if the verbosity of these messages were increased a bit.
</comment><comment author="clintongormley" created="2015-11-21T19:19:50Z" id="158674697">This has been greatly improved in 2.0

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Provide option for highlighting to return not_analyzed version of the highlight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7982</link><project id="" key="" /><description>Use case:

```
     "analysis": {
      "analyzer": {
        "full_name": {
          "filter": [
            "standard",
            "lowercase",
            "asciifolding"
          ],
          "tokenizer": "standard",
          "type": "custom"
        }
      }
    }
  },
  "mappings": {
    "content_index": {
      "properties": {
        "title": {
          "type": "string",
          "analyzer": "full_name",
          "fields": {
            "raw": {
              "type": "string",
              "index": "not_analyzed"
            }
          }
        }
      }
    }
  }

PUT /highlight/content_index/1
{
  "title": "Policy-3---"
}
```

Highlighting for title returns the following per the analyzer used:

```
 "highlight": {
               "title": [
                  "&lt;em&gt;Policy&lt;/em&gt;-&lt;em&gt;3&lt;/em&gt;---"
               ]
            }
```

Can certainly post-process to workaround this.  Would be nice to be able to configure highlighting for it to return the raw form instead (or as an additional highlight):

```
   "highlight": {
              "title": [
                  "&lt;em&gt;Policy-3---&lt;/em&gt;"
               ]
```
</description><key id="44834243">7982</key><summary>Provide option for highlighting to return not_analyzed version of the highlight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Highlighting</label></labels><created>2014-10-03T18:47:57Z</created><updated>2016-11-25T16:29:54Z</updated><resolved>2016-11-25T16:29:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-10-06T13:19:37Z" id="58015300">Can you highlight (and search for that matter) against the field analyzed with the keyword analyzer?  You might try using the fast vector highlighter with `matched_fields`.

If you need the ultimate in rope to hang yourself with https://github.com/wikimedia/search-highlighter supports highlighting regular expressions [1].  For that matter it also supports `matched_fields` without as much index space overhead as the fast vector highlighter and other fun stuff like `skip_if_last_matched` so it might be worth looking at any way.

Full disclosure: I maintain the plugin so I'm probably a bigger fan of it then I should be.

[1]: There is a bug in regex highlighting and unicode characters which I've fixed but haven't released a new version to pick it up.  I'll do that in a couple of days.
</comment><comment author="clintongormley" created="2016-11-25T16:29:54Z" id="262991887">This works using `fragment_size: 0` with the new unified highlighter.

Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make it possible to delete snapshots with missing metadata file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7981</link><project id="" key="" /><description>...data file

Fixes #7980
</description><key id="44825862">7981</key><summary>Make it possible to delete snapshots with missing metadata file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-03T17:25:14Z</created><updated>2015-06-08T00:37:57Z</updated><resolved>2014-10-07T00:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-06T13:35:17Z" id="58017169">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot/Restore: snapshot with missing metadata file cannot be deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7980</link><project id="" key="" /><description>If snapshot metadata file disappears from a repository or it wasn't created due to network issues or master node crash during snapshot process, such snapshot cannot be deleted. Was originally reported in https://github.com/elasticsearch/elasticsearch/issues/5958#issuecomment-57136510
</description><key id="44824333">7980</key><summary>Snapshot/Restore: snapshot with missing metadata file cannot be deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label></labels><created>2014-10-03T17:08:55Z</created><updated>2015-02-27T19:41:26Z</updated><resolved>2014-10-07T00:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="saahn" created="2014-11-04T21:01:11Z" id="61713589">Hi, I just ran into this issue today and was wondering if you had any idea when this patch would be released. Is the target version 1.3.5 or 1.4? Thanks!
</comment><comment author="imotov" created="2014-11-04T23:03:28Z" id="61731509">@saahn yes, 1.3.5  and 1.4.0. You can check labels on the issue #7981 to see all versions that it was merged into.
</comment><comment author="sarkis" created="2015-02-26T10:05:40Z" id="76151889">We've run into (what I believe) is this issue. Running 1.4.4 and there is a snapshot that is:

IN_PROGRESS when I check localhost:9200/_snapshot/backups/snapshot_name endpoint
ABORTED when I check localhost:9200/_cluster/state

-XDELETE hangs when attempting to delete the snapshot

The reason I believe it is this issue - we upgraded and restarted Elasticsearch around the time this snapshot was running. 

@imotov I also tried to use your cleanup script - however it returns with "No snapshots found" and the snapshot is still stuck in the same states above.

Any other ideas on a way to force delete this snapshot? It is currently blocking us from getting any other snapshots created.
</comment><comment author="imotov" created="2015-02-26T12:57:46Z" id="76173545">@sarkis which type of repository and which version are you using? Could you post the snapshot part of the cluster state here?
</comment><comment author="sarkis" created="2015-02-26T17:54:07Z" id="76228548">@imotov snapshot part of cluster state: https://gist.github.com/sarkis/f46de23dc81b1dba0d1a

We're now on ES 1.4.4 the snapshot was started on ES 1.4.2, and we ran into troubles as the snapshot was running while upgrading 1.4.2 -&gt; 1.4.4. We are using an fs snapshot, more info:

{"backups_sea":{"type":"fs","settings":{"compress":"true","location":"/path/to/snapshot_dir"}}
</comment><comment author="imotov" created="2015-02-26T17:59:30Z" id="76229751">@sarkis how many nodes are in the cluster right now? Is the node bhwQqwZ2QuCUPjcZGrGpuQ still running? 
</comment><comment author="sarkis" created="2015-02-26T18:04:38Z" id="76230895">@imotov 1 gateway and 2 data nodes (1 set to master)

I cannot find that node name - I assume it was renamed upon rolling restart or possibly from the upgrade?
</comment><comment author="imotov" created="2015-02-26T18:12:25Z" id="76232485">@sarkis  Are you sure it doesn't appear in `curl "localhost:9200/_nodes?pretty"` output?
</comment><comment author="sarkis" created="2015-02-26T18:22:50Z" id="76234755">@imotov just double checked - nothing.
</comment><comment author="imotov" created="2015-02-26T18:40:24Z" id="76239067">@sarkis did you try running cleanup script after the upgrade or before? Did you restart master node during upgrade or it is still running 1.4.2? Does master node have proper access to the shared file system, or read/write operations with the shared files system still hang?
</comment><comment author="sarkis" created="2015-02-26T18:45:36Z" id="76240826">@imotov I tried running the cleanup script after the upgrade.

The master node was restarted and is running 1.4.4 - if I had known about this issue I would have stopped the snapshot before rolling restarts/upgrades :(

The snapshot directory is a nfs mount and the "elasticsearch" user does have proper read/write perms. I just double checked this on all nodes in the cluster.

Thanks a lot for the help and quick responses.
</comment><comment author="imotov" created="2015-02-26T18:58:05Z" id="76243585">@sarkis I am completely puzzled about what went wrong and I am still trying to figure out what happened and how to reproduce the issue. With a single master node, the snapshot should have disappeared during restart. There is simply no place for it to survive since snapshot information is not getting persisted on disk. Even if the snapshot somehow survived the restart, the cleanup script should have removed it. So, I feel that I am missing something important about the issue. 

When you said rolling restart, what did you mean? Could you describe the process in as many details as possible. Was snapshot stuck before the upgrade or was it simply taking long time. What was the upgrade process? Which nodes did you restart first? 
</comment><comment author="sarkis" created="2015-02-26T19:55:49Z" id="76256341">@imotov Sure - so we have 2 data nodes and 1 gateway node (total of 3 nodes). The rolling restart was done following the recommended way to do so via elasticsearch documentation: 

1) turn off allocation
2) upgrade / restart gateway
3) turn on allocation (wait for green)
4) turn off allocation
5) upgrade / restart non-master data node
6) turn on allocation (wait for green)
7) turn off allocation
8) upgrade / restart master data node
9) turn on allocation

I think we have tried everything we could at this point as well. Would you recommend removing/adding back the repo? What's the best way to just get around this? I understand you wanted to reproduce it on your end but I'd like to get snapshots working ASAP. 

Update: I know there isn't truly a "master" - I called the above nodes master and non-master based off of info from paramedic at the time of upgrades
</comment><comment author="imotov" created="2015-02-26T20:22:07Z" id="76262224">So they are all master-eligible nodes! That explains the first part - how snapshot survived restart. It doesn't explain how it happened in the first place, though. Removing and adding back the repo is not going to help. There are really only two ways to go - I can try to figure out what went wrong and fix cleanup script to clean the issue or you can do full cluster restart (shut down all master-eligible nodes and then start them back up). By the way, what do you mean by "gateway"?
</comment><comment author="sarkis" created="2015-02-26T20:26:49Z" id="76262966">~~We have a dedicated node for this: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html~~

On the full cluster restart - do you mean to shut them all down at the same time and bring them back up? Does that mean there will be data loss in the time it takes to bring them both back up?
</comment><comment author="imotov" created="2015-02-26T20:38:05Z" id="76266133">@sarkis not sure I am following. Are you using non-local gateway on one of your nodes? Which gateway are you using there? How did you configure this node comparing to all other nodes? Could you send me your complete cluster state (you can send it to igor.motov@elasticsearch.com if you don't want to post it here).
</comment><comment author="sarkis" created="2015-02-26T20:41:36Z" id="76267116">@imotov sorry for the confusion - our 3rd non-data, non-master node we refer to as a gateway is the entry point to the cluster. It's one and only purpose is to pass traffic through to the data nodes. Sending you the full cluster state via e-mail.
</comment><comment author="imotov" created="2015-02-26T20:53:06Z" id="76270320">OK, so "gateway" node doesn't have anything to do with [gateways](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html) and it's simply a client node. Got it! I will be waiting for the cluster state to continue investigation. Thanks!

Full cluster restart will make cluster unavailable for indexing and searching while nodes are restarting and shards are recovering. Depending on how you are indexing data it might or might not cause loss of data (if your client has a retry logic to reindex failed records, it shouldn't lead to any data loss).
</comment><comment author="sarkis" created="2015-02-26T21:48:46Z" id="76281387">@imotov sent the cluster state - let me know if I can do anything else. I am looking for a window we can do a full restart to see if this will fix our problem.
</comment><comment author="sarkis" created="2015-02-27T19:25:44Z" id="76455587">In case others come here with the same issue. @imotov's updated cleanup script (https://github.com/imotov/elasticsearch-snapshot-cleanup) for 1.4.4 worked in clearing up the ABORTED snapshots.
</comment><comment author="imotov" created="2015-02-27T19:41:26Z" id="76458333">Since it seems to be a different problem, I have created a [separate issue](https://github.com/elasticsearch/elasticsearch/issues/9924) for it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Snapshot/Restore: make it possible to delete snapshots with missing metadata file</comment></comments></commit></commits></item><item><title>Huge amount of non-heap memory usage in v1.3.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7979</link><project id="" key="" /><description>Howdy! I'm wondering if the whole "set your heap to 50% of system memory" is too ambiguous. Do you intend to mean "all of the JVM should take up 50% of system memory"?

I just installed 1.3.3 on a server with 2gb of RAM, and with this commandline (heap fixed at 800mb), I'm seeing the total process memory at an additional 300mb:

`/usr/bin/java -server -Djava.net.preferIPv4Stack=true -Des.config=/usr/local/etc/elasticsearch/elasticsearch.yml -Xms800m -Xmx800m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.pidfile=/usr/local/var/run/elkstack_centos_65.pid -Des.path.home=/usr/local/elasticsearch -cp :/usr/local/elasticsearch/lib/*:/usr/local/elasticsearch/lib/sigar/* org.elasticsearch.bootstrap.Elasticsearch`

ps output:
`497       8518  0.2 57.3 2162076 1175444 ?     SLl  14:09   0:25 /usr/bin/java` -- 1175444 is roughly 1176mb. That means the non-heap memory is almost a 3rd of the total process memory (an additional 300+ mb).

Is that expected? Should the recommendation of using 50% of system memory for heap really be something closer to 50% of the system memory?

FWIW, here's my node stats:

```
      "os" : {
        "timestamp" : 1412354968278,
        "uptime_in_millis" : 10674,
        "load_average" : [ 0.0, 0.0, 0.0 ],
        "cpu" : {
          "sys" : 0,
          "user" : 0,
          "idle" : 99,
          "usage" : 0,
          "stolen" : 0
        },
        "mem" : {
          "free_in_bytes" : 345624576,
          "used_in_bytes" : 1752281088,
          "free_percent" : 22,
          "used_percent" : 77,
          "actual_free_in_bytes" : 471433216,
          "actual_used_in_bytes" : 1626472448
        },
        "swap" : {
          "used_in_bytes" : 0,
          "free_in_bytes" : 0
        }
      },
      "process" : {
        "timestamp" : 1412354968278,
        "open_file_descriptors" : 126,
        "cpu" : {
          "percent" : 0,
          "sys_in_millis" : 3080,
          "user_in_millis" : 22960,
          "total_in_millis" : 26040
        },
        "mem" : {
          "resident_in_bytes" : 1203654656,
          "share_in_bytes" : 20033536,
          "total_virtual_in_bytes" : 2213965824
        }
      },
```

Heap stats:

```
Attaching to process ID 8518, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 24.65-b04

using parallel threads in the new generation.
using thread-local object allocation.
Concurrent Mark-Sweep GC

Heap Configuration:
   MinHeapFreeRatio = 40
   MaxHeapFreeRatio = 70
   MaxHeapSize      = 838860800 (800.0MB)
   NewSize          = 174456832 (166.375MB)
   MaxNewSize       = 174456832 (166.375MB)
   OldSize          = 348913664 (332.75MB)
   NewRatio         = 2
   SurvivorRatio    = 8
   PermSize         = 21757952 (20.75MB)
   MaxPermSize      = 174063616 (166.0MB)
   G1HeapRegionSize = 0 (0.0MB)
```

Why is my process at 1200mb when the heap is only 800mb? Thanks!
</description><key id="44822674">7979</key><summary>Huge amount of non-heap memory usage in v1.3.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinb3</reporter><labels /><created>2014-10-03T16:50:57Z</created><updated>2014-10-15T10:06:49Z</updated><resolved>2014-10-15T10:06:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T10:06:49Z" id="59184043">Hi @martinb3 

It is normal for the JVM and Elasticsearch to use some off-heap memory. Normally it is not very much, but in the case where your RAM is only 2GB, a small amount of memory can represent a large fraction of available RAM.

Most users of Elasticsearch use bigger boxes, at which point the direct memory usage is closer to a rounding error.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw exception if null_value is set to `null`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7978</link><project id="" key="" /><description>The mapping parser should throw an exception if "null_value" is set to `null`.  

Fixes #7273

``` bash
PUT /foo
{
  "mappings": {
    "bar": {
      "properties": {
        "exception": {
          "null_value": null,
          "type": "integer"
        }
      }
    }
  }
}
```

```
{
   "error": "MapperParsingException[mapping [bar]]; nested: MapperParsingException[Property [null_value] cannot be null.]; ",
   "status": 400
}
```

As a side note, looks like there are a lot of other properties which could be set to null and throw similar errors:  https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java#L146-L187

I'm not sure the best way to handle these other than an explicit check for nullness inside each clause...which seems gross.
</description><key id="44819996">7978</key><summary>Throw exception if null_value is set to `null`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-03T16:22:10Z</created><updated>2015-06-07T18:36:50Z</updated><resolved>2014-10-10T20:36:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-03T17:46:17Z" id="57829410">LGTM.
</comment><comment author="s1monw" created="2014-10-06T13:36:34Z" id="58017324">LGTM
</comment><comment author="s1monw" created="2014-10-06T13:36:50Z" id="58017355">I think this should go into 1.3 and 1.4 et.al - I labeled the issues accordingly
</comment><comment author="polyfractal" created="2014-10-10T20:36:28Z" id="58712810">Merged in master: https://github.com/elasticsearch/elasticsearch/commit/4e2dd770aab15857d8f7a01d25d941908498ccb6, 1.x: fcea4c559fcdf1c20025185515e5e3ee3c098999, 1.4: 4b5b01f23a0e0f26e36d8308897ff96cc2c4610a, 1.3: 49374db4e24ad3b772e0685c7239339a02086d5b
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Created a parameter parser to standardise script options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7977</link><project id="" key="" /><description /><key id="44819227">7977</key><summary>Created a parameter parser to standardise script options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Scripting</label><label>breaking</label><label>enhancement</label><label>v1.4.5</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-03T16:13:54Z</created><updated>2015-06-06T17:30:54Z</updated><resolved>2014-10-08T07:57:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-06T13:44:03Z" id="58018291">LGTM
</comment><comment author="colings86" created="2014-10-06T15:30:06Z" id="58034796">@s1monw thanks for the review, unfortunately I had forgotten to make the scripted metric aggregation use the ScriptParameterParser so have added a new commit to the PR
</comment><comment author="s1monw" created="2014-10-07T11:16:12Z" id="58169348">LGTM still :)
</comment><comment author="javanna" created="2015-04-04T08:17:42Z" id="89526124">@colings86 this change was backported to 1.x in a bw comp manner but breaks backwards compatibility on master I think, as only the new parameters get read on master now, is that right?

If so this PR might deserve the breaking label (although confusing cause it's breaking only on master branch).
</comment><comment author="colings86" created="2015-04-10T11:55:53Z" id="91531609">This is only a breaking change in 2.0
</comment><comment author="kurtzhong" created="2015-04-16T05:20:35Z" id="93645727">Is there any get around, for 1.4.4? 

When i use the script fields "(`init_script`, `map_script`, `combine_script`, `reduce_script`)" it throws me the following error: 

```
 ScriptException[dynamic scripting for [groovy] disabled]; ]
```

And when i use the `*_script_file` fields, it throws me: 

```
Unable to find on disk script 
```

So now i can't use the scripted metric aggregation on 1.4.4
</comment><comment author="colings86" created="2015-04-16T11:55:23Z" id="93716502">@kurtzhong I just tried the following gist on Elasticsearch 1.4.4 with the default config (apart form enabling CORS and setting the cluster name) and it worked. Could you try this on your cluster and let me know if it works? 

The location of the scripts is very important, they must be in a folder called `scripts` in your config folder (where your elasticsearch.yml file is) and they must have the correct extension for the language you are using (i.e. `.groovy` if you are using the default language, groovy).
</comment><comment author="jveldboom" created="2015-04-16T12:11:29Z" id="93718891">We had to explicitly set the `lang` to groovy for it to work.

```
{
    "script_score": {
        "params": {
            "mult": 1.2
        },
        "script": "sales-ytd",
        "lang":"groovy"
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aliases: Throw exception if index is null when creating alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7976</link><project id="" key="" /><description>Fixes a bug where alias creation would allow `null` for index name, which thereby applied the alias to _all_ indices.  This patch makes the validator throw an exception if the index is null.

Fixes #7863

``` bash
POST /_aliases
{
   "actions": [
      {
         "add": {
            "alias": "empty-alias",
            "index": null
         }
      }
   ]
}
```

``` json
{
   "error": "ActionRequestValidationException[Validation Failed: 1: Alias action [add]: [index] may not be null;]",
   "status": 400
}
```

Edit:

The reason this bug wasn't caught by the existing tests is because the old test for nullness only validated against a cluster which had zero indices.  The null index is translated into "_all", and since there are no indices, this fails because the index doesn't exist.  So the test passes.

However, as soon as you add an index, "_all" resolves and you get the situation described in the original bug report:  null index is accepted by the alias, resolves to "_all" and gets applied to everything.
</description><key id="44811125">7976</key><summary>Aliases: Throw exception if index is null when creating alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>bug</label></labels><created>2014-10-03T14:55:32Z</created><updated>2014-10-10T20:49:50Z</updated><resolved>2014-10-10T20:49:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-03T17:39:09Z" id="57828483">What is here looks good, but I think we should do more.  Right now not passing any index at all results in using `_all`, which I think is very dangerous?

```
Ryans-MacBook-Pro:~ rjernst$ curl -i -XPOST 'http://localhost:9200/_aliases' -d '{"actions":[{"add":{"alias":"myalias"}}]}'
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 21

{"acknowledged":true}
```
</comment><comment author="polyfractal" created="2014-10-03T18:41:26Z" id="57836536">@rjernst Just looked into this, and it appears the patch will also protect against complete omission of the field.

The [`indices()`](https://github.com/polyfractal/elasticsearch/blob/bugfix/null_alias/src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java#L146) method ensures that the array of indices is always initialized (even if empty), which means the new `else` clause introduced in the patch will throw an exception if indices is omitted entirely.

``` bash
$ curl -XPOST "http://localhost:9200/_aliases" -d'
{
   "actions":[
      {
         "add":{
            "alias":"empty-alias"
         }
      }
   ]
}'

{
   "error": "ActionRequestValidationException[Validation Failed: 1: Alias action [add]: [index] may not be null;]",
   "status": 400
}
```
</comment><comment author="rjernst" created="2014-10-03T18:47:20Z" id="57837324">Cool! LGTM.

My only thought is maybe the error message should be more generic, since it could be a missing "index" key, or index set to null?
</comment><comment author="s1monw" created="2014-10-06T13:41:47Z" id="58018017">LGTM too agreed with @rjernst 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file></files><comments><comment>Aliases: Throw exception if index is null when creating alias</comment></comments></commit></commits></item><item><title>trying to perform a terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7975</link><project id="" key="" /><description>I’m trying to perform a terms aggregation using elastic search 1.3.4 using NEST 1.0.3. I want my results to include bucket items with counts of 0 if that’s the case rather than ES throwing them away. My query is below as is the JSON it results in. My ElasticSearch cluster has 1 node which consists of two indexes which are exact copies of each other, they are each on their own primary shard.

My C# Nest.SearchDescriptor nestQuery = new Nest.SearchDescriptor() .Type("fieldOne") .Query(q =&gt; q.Term("fieldTwo", this.fieldTwo) &amp;&amp; q.Term("fieldThree", 1) ) .Aggregations(a =&gt; a .Terms("my_agg", t =&gt; t.Field("fieldFour.ID") .Size(Int16.MaxValue).MinimumDocumentCount(0)));
string json = Encoding.UTF8.GetString(new Nest.ElasticClient()
.Serializer.Serialize(nestQuery));

Nest.ISearchResponse&lt;object&gt; result = ESUtil.ExecuteNestQuery(nestQuery);

Nest.Bucket aggregationsBucket = 
((Nest.Bucket)result.Aggregations["my_agg"] as Nest.Bucket);
var bucketitems =
aggregationsBucket.Items.Cast&lt;Nest.KeyItem&gt;()
.ToDictionary(key =&gt; key.Key, e =&gt; e.DocCount);

The json this results in (and subsequently submits) is: { "aggs": { "my_agg": { "terms": { "field": "fieldFour.ID", "size": 32767, "min_doc_count": 0 } } }, "query": { "bool": { "must": [ { "term": { "fieldTwo": { "value": 4 } } }, { "term": { "fieldThree": { "value": 1 } } } ] } } }

The above results in 19 bucket items being returned when there should be 20, so it appears to be throwing one away for some reason. If I change one of my values so that the results should be 6 bucket items I still get 19 bucket items being returned with the correct 6 amongst them and 13 with a count of zero. How can I get it to return the correct amount of bucket items? i.e. 20 and 6 in the two scenarios mentioned above.
</description><key id="44810354">7975</key><summary>trying to perform a terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hodgezappa</reporter><labels /><created>2014-10-03T14:48:19Z</created><updated>2014-10-15T10:02:32Z</updated><resolved>2014-10-15T10:02:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T10:02:32Z" id="59183592">Hi @hodgezappa 

This issues list is for feature requests and bug reports.  For questions like these, please ask in the forum.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature: Query Cache by _type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7974</link><project id="" key="" /><description>Hi,

The new Query Cache is awasome! But it let's me re-think about index separation. It maybe would be a quick win to evict cache only for each _type

Marco
</description><key id="44809912">7974</key><summary>Feature: Query Cache by _type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcok</reporter><labels /><created>2014-10-03T14:43:47Z</created><updated>2014-10-03T17:29:43Z</updated><resolved>2014-10-03T17:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-03T14:48:56Z" id="57806135">@marcok I think the fact that queries can target several shards makes it hard to evict by type? Maybe there is some confusion about how queries are executed? When a queries spans several shards, elasticsearch doesn't run the query twice on each type and then merges the results. On the contrary, since all types are stored in the same lucene index at the shard level, the query is run only once. Setting a type is pretty much like adding a filter in practice.
</comment><comment author="marcok" created="2014-10-03T15:03:40Z" id="57808237">atm the cache is stored at the shard level and gets evicted when receiving any INS/DEL/UPD right?
I see no problem to evict only those caches based on a query with the _type in the filter which has been INS/DEL/UPD.
I agree it's not a very easy task to figure out which query cache is affected (parsing the query), particular if the cache is stored with a query hash as they cache key, which I assume it is. So maybe it was a stupid idea :)
</comment><comment author="jpountz" created="2014-10-03T15:15:29Z" id="57809938">&gt; atm the cache is stored at the shard level and gets evicted when receiving any INS/DEL/UPD right?

It is a bit more complicated than that. Elasticsearch write operations are not searchable immediately, you need to wait until something that we call a `refresh`, whose purpose is to make data visible to search operations. By default this runs every second and this cache is cleared whenever there is a refresh operation happening that actually does something (so that if no updates were performed in the last second, you don't clear the cache). Currently the cache key is the tuple formed by the request object and the version of the index (which is incremented whenever a refresh operation occurs).

Although we could make the cache more fine-grained as you suggest by having per-type caches and tracking which types have been modified but I'm not sure the complexity is worth the benefits? If someone feels like per-type cache would help a lot, maybe that means that data should actually live in several indices?
</comment><comment author="marcok" created="2014-10-03T17:29:43Z" id="57827337">"I'm not sure the complexity is worth the benefits"
I completely agree. For sure it would increase the cache hit ratio a lot in some scenarios, but if all the things goes much more complex its probably not worth. Anyway I close it, may you can put it on low prio wishlist :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs: rolling upgrade process seems incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7973</link><project id="" key="" /><description>When reading the [rolling upgrade process](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#rolling-upgrades), you can see that we wrote:
- disable allocation
- upgrade node1
- upgrade node2
- upgrade node3
- ...
- enable allocation

That won't work as after a node has been removed and restarted, no shard will be allocated anymore.
So closing node2 and remaining nodes, won't help to serve index and search request anymore.

We should write:
- disable allocation
- upgrade node1
- enable allocation
- wait for shards being recovered on node1
- disable allocation
- upgrade node2
- enable allocation
- wait for shards being recovered on node2
- disable allocation
- upgrade node3
- enable allocation
- wait for shards being recovered on node3
- disable allocation
- ...
- enable allocation

I think this documentation update should go in 1.3, 1.4, 1.x and master branches.
</description><key id="44808446">7973</key><summary>Docs: rolling upgrade process seems incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-03T14:29:52Z</created><updated>2014-10-24T14:46:44Z</updated><resolved>2014-10-24T14:46:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-10-03T14:41:51Z" id="57805147">+1
I do the disable/upgrade/re-enable/wait for green thing.
</comment><comment author="nemonster" created="2014-10-06T14:35:02Z" id="58025690">Think the step on waiting for the green thing should be stated explicitly in the instructions.
</comment><comment author="clintongormley" created="2014-10-15T11:52:33Z" id="59194089">Agreed - @dadoonet you want to send a PR for the docs? I don't think @palecur is going to be able to get to this any time soon.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: rolling upgrade process seems incorrect</comment></comments></commit></commits></item><item><title>Add Parent Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7972</link><project id="" key="" /><description>ES 1.4 added support for Children aggregation, which is a really great feature.
The next step would be to also support a Parent aggregation

Here is a quick example of a typical use case:
Let's say we've got 2 types, one containing Product information (name, description, categories, size, weight, etc) and another one containing Reseller information (location, price, etc..).
There is a parent child relationship between Products and Resellers (Product being the parent) 

In some cases, one may want to aggregate on Parent property.

```
GET parentchild/reseller/_search/
{
  "query": {
    ... // not relevant here
  },
  "sort": [
    {
      "price": {
        "order": "asc",
        "mode": "avg"
      }
    }
  ],
  "aggs": {
    "location": {
      "geo_distance": {
        "unit": "km",
        "field": "location",
        "origin": "lat, lon",
        "ranges": [
          { "from": 0, "to": 100},
          ....
        ]
      }
    },
    "price": {
      "range": {
        "field": "price",
        "ranges": [
          { "from": 0, "to": 50},
          ...
        ]
      }
    },
    "aggregateOnParent": {
      "parent": {},
      "aggs": {
          "category": {
            "term": {
               "field": "category" // Note that category is a field of Product (the parent)
            }
          }
        }
      }
    }
  }
}
```
</description><key id="44794698">7972</key><summary>Add Parent Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephane-bastian</reporter><labels /><created>2014-10-03T11:43:34Z</created><updated>2015-07-27T11:07:26Z</updated><resolved>2014-10-15T09:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T09:59:16Z" id="59183236">Hi @stephane-bastian 

Closing in favour of duplicate #5306.  Note: we ran into problems with trying to implement this. It may not be possible.
</comment><comment author="stephane-bastian" created="2015-07-24T10:42:07Z" id="124470210">Hi Clinton, 
I don't think the issue #5306 deals with the same problem. My understanding is that it's purpose is to provide an aggregation to count parent docs.

The parent aggregation I'm talking about is an aggregation that would allow to nest other aggregation on parent field. for instance nest a term aggregation of a field that lives in the parent doc.

If you feel that #5306 and this issue are slightly different could you please reopen this one so we keep track of it?
Thx 
</comment><comment author="clintongormley" created="2015-07-27T11:07:26Z" id="125167382">Hi @stephane-bastian 

It's the same issue.  We have no way, in aggregations today, of stepping up from child context to parent context.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Strange `filter` aggregation behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7971</link><project id="" key="" /><description>Currently I am observing a strange behavior of the `filter` aggregation that contradicts with the documentaion in my mind. In the [reference](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-filter-aggregation.html) it is stated that `filter` aggregation:

&gt; Defines a single bucket of all the documents in the current document set context that match a specified filter. Often this will be used to narrow down the current aggregation context to a specific set of documents.

OK, so one may think that a filter, used in the `filter` aggregation narrows down the whole document set that in its turn may be composed using a `query` and (or) a `filter`.

Now, let me provide an example. Suppose we have a set of realty ads and we want to know the max and the min price of flats that have only one room. We deal with real data and price can be empty (equal to zero) or have any inadequate value. So it is reasonable to wrap `min` and `max` aggregations with another `filter` aggregation that will filter only adequate price values for us. The query will look something like:

``` json
{
    "filter": {"term": {"room_count": 1}},
    "size": 0,
    "aggregations" : {
        "price" : {
            "filter" : {"range": {"price":{"gte" : 300000, "lte" : 10000000000}}},
            "aggregations" : {
                "min": {"min" : { "field" : "price" }},
                "max": {"max" : { "field" : "price" }}
            }
        }
    }
}
```

And the result:

``` json
{
    "hits": {
        "total": 54345
    },
    "aggregations": {
        "price": {
            "doc_count": 256659,
            "min": {
                "value": 300000
            },
            "max": {
                "value": 4049215000
            }
        }
    }
}
```

There is something strange in the result above that is not logic for me after reading the manual: the `doc_count` (256659) for the `filter` aggregation (that is supposed to be more narrow) is more than the `total` documents count (54345), but it is supposed that the `doc_count` will be less than the`total`.

OK, after some thinking one may suppose that a filter in the `filter` aggregation is not being applied over the query's `filter` — they do not form a superposition like in the way that may be concluded from the reference. So let's try to do a workaround — append the outer `filter` manually to the `filter` aggregation:

``` json
{
    "filter": {"term": {"room_count": 1}},
    "size": 0,
    "aggregations" : {
        "price" : {
            "filter" : {
                "and": [
                    {"term": {"room_count": 1}},
                    {"range": {"price":{"gte" : 300000, "lte" : 10000000000}}}
                ]
            },
            "aggregations" : {
                "min": {"min" : { "field" : "price" }},
                "max": {"max" : { "field" : "price" }}
            }
        }
    }
}
```

And we get the result:

``` json
{
    "hits": {
        "total": 54345
    },
    "aggregations": {
        "price": {
            "doc_count": 34167,
            "min": {
                "value": 320000
            },
            "max": {
                "value": 2011623600
            }
        }
    }
}
```

That looks more like a true result. The `doc_count` is now less than the `total`. The `min` and the `max` values had changed too.

Don't you think that something may be wrong: the aggregations manual or the `filter` aggregation's implementation?
</description><key id="44793973">7971</key><summary>Strange `filter` aggregation behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">golubev</reporter><labels /><created>2014-10-03T11:31:34Z</created><updated>2014-10-16T09:53:08Z</updated><resolved>2014-10-15T15:50:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Elgeor" created="2014-10-09T15:29:13Z" id="58526951">I have same issue
</comment><comment author="clintongormley" created="2014-10-15T15:50:17Z" id="59227974">@golubev The top-level `filter` is deprecated and has been renamed to `post_filter`, to indicate that it is applied **after** the aggregations are calculated.  You should be using your filter inside the main `query`, wrapped in a `filtered` query:

```
{
  "query": {
    "filtered": {
      "filter": {
        "term": {
          "room_count": 1
        }
      }
    }
  },
  "size": 0,
  "aggregations": {
  ...
```
</comment><comment author="Elgeor" created="2014-10-15T16:02:24Z" id="59229977">How this can be done in Java?
</comment><comment author="golubev" created="2014-10-16T09:37:59Z" id="59337148">@clintongormley , thank you for your reply!

Now I see from the [search requests reference](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_search_requests.html) that:

&gt; Also, the top-level filter parameter in search has been renamed to post_filter, to indicate that it should not be used as the primary way to filter search results (use a filtered query instead), but only to filter results AFTER facets/aggregations have been calculated.

I think that the top-level `filter` should be completely removed (so it will be possible to use only the `post-filter`) and the ES search API should throw errors when it receives a top-level `filter`. This way there will be no ambiguity for the developers like it had happened in this issue and in the #8011. If two people wrote about this it is very probable that even more people deal with the same issue.
</comment><comment author="golubev" created="2014-10-16T09:53:08Z" id="59338803">Hi @Elgeor 

All code snippets above describe interaction with the elasticsearch search API via JSON. If you are talking to the elasticsearch via the REST API (sending JSON via HTTP) - all examples are above.

If you are asking about how to do this in the elasticsearch Java API - I don't know as I am using a REST wrapper in PHP. You may either read [Java API reference](http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/query-dsl-queries.html) or ask a question on the stackoverflow.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mastering Elastic Search Queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7970</link><project id="" key="" /><description>Hi 

I wish to master the queries.
I wish to practice the queries on the Elastic Search Plugin UI.

Can anyone of you suggest the resource which will help me improve the querying mechanism.

Thanks
navajyothi
</description><key id="44789179">7970</key><summary>Mastering Elastic Search Queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">navindian</reporter><labels /><created>2014-10-03T10:14:36Z</created><updated>2014-10-15T09:53:16Z</updated><resolved>2014-10-15T09:53:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-15T09:53:16Z" id="59182508">Hi @navindian 

This issues list is for feature requests and bug reports. Please use the forum for questions like these.  I suggest you start reading the Definitive Guide in order to get started with Elasticsearch: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/index.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Makes script params consistent with other APIs in scripted_metric</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7969</link><project id="" key="" /><description>This change removes the script_type parameter form the Scripted Metric Aggregation and adds support for _file and _id suffixes to the init_script, map_script, combine_script and reduce_script parameters to make defining the source of the script consistent with the other APIs which use the ScriptService
</description><key id="44788998">7969</key><summary>Makes script params consistent with other APIs in scripted_metric</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-03T10:11:44Z</created><updated>2015-06-07T17:50:16Z</updated><resolved>2014-10-06T10:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-03T23:14:04Z" id="57878068">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't let `took` be negative.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7968</link><project id="" key="" /><description>`took` is computed based on the system clock and can be negative if the clock
time was updated during the execution of the search request. This commit
protects against these cases by replacing `took` with 1 if the elapsed time is
negative.
</description><key id="44787222">7968</key><summary>Don't let `took` be negative.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-03T09:47:24Z</created><updated>2015-06-07T17:14:54Z</updated><resolved>2014-10-03T10:26:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-10-03T10:03:46Z" id="57776464">LGTM
</comment><comment author="s1monw" created="2014-10-13T11:00:06Z" id="58876118">@jpountz any updates on this?
</comment><comment author="s1monw" created="2014-10-13T11:00:29Z" id="58876161">btw. I think this is a bug and we should push to 1.3 too?
</comment><comment author="s1monw" created="2014-10-13T11:01:05Z" id="58876209">grrr I didn't see it's closed ... nevermind
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/AbstractAsyncAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollScanAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file></files><comments><comment>Internal: Don't let `took` be negative.</comment></comments></commit></commits></item><item><title>Leniency makes simple_query_string query a match_all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7967</link><project id="" key="" /><description>As far as I can see since 1.3.3 when query is using "lenient" parameter it yelds results when we have format based failures (at least in 1.3.2 it does not return any results).
When searching by non-existing subfields (i.e. "field.nonexisting") no errors and no results are found (which seems to be correct)
The problem comes when I combine queries from both types - with "lenient" and format failure and with non-existing fields in a "should" boolean query
I'm tested with 1.3.4 version and 1.4.0-beta1
Here are the structure, test data and queries I tested with:
Structure
https://gist.github.com/shoteff/3ee9f2ad320410375c91
Data
https://gist.github.com/shoteff/f5c23bd6bb85f6fecdc9
Queries - here I also described what is working and what not
https://gist.github.com/shoteff/20fa2411ede09068ce95
</description><key id="44786026">7967</key><summary>Leniency makes simple_query_string query a match_all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">shoteff</reporter><labels><label>bug</label></labels><created>2014-10-03T09:31:15Z</created><updated>2014-10-22T08:37:48Z</updated><resolved>2014-10-22T08:37:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T21:01:03Z" id="59115833">Simple demo:

```
DELETE t

PUT /t/test/1
{
    "id": 1,
    "title": "this is test1"
}

GET /_validate/query?explain
{
  "query": {
    "bool": {
      "should": [
        {
          "simple_query_string": {
            "query": "this",
            "fields": [
              "id",
              "title"
            ],
            "default_operator": "and",
            "lenient": true
          }
        }
      ]
    }
  }
}
```

The above results in a match-all query:

```
{
   "valid": true,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "explanations": [
      {
         "index": "t",
         "valid": true,
         "explanation": "*:*"
      }
   ]
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryStringTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Make simple_query_string leniency more fine-grained</comment></comments></commit></commits></item><item><title>Tests: Improve BWC preconditions to error cleanly on major version difference.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7966</link><project id="" key="" /><description>This change makes running a BWC test on master against a 1.x version produce the following exception:

`java.lang.IllegalArgumentException: Backcompat elasticsearch version must be same major version as current. backcompat: 1.3.2, current: 2.0.0-SNAPSHOT`

It also fixes the @after cleanup to not throw an NPE if there was an error before the cluster was created (which is the case with this error). And finally it makes the backcompat version available (I needed this recently in the analysis bwc tests given a behavior change in lucene that needed to be worked around).
</description><key id="44730802">7966</key><summary>Tests: Improve BWC preconditions to error cleanly on major version difference.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v1.4.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-02T19:47:37Z</created><updated>2015-01-21T23:22:21Z</updated><resolved>2014-10-03T14:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-10-02T19:50:41Z" id="57694774">I forgot to mention, this also will randomly select a version from the backcompat dir, so you don't have to specify `-Dtests.bwc.version` unless you want to test a specific version.
</comment><comment author="javanna" created="2014-10-03T07:36:39Z" id="57764803">LGTM, thanks @rjernst !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/ElasticsearchBackwardsCompatIntegrationTest.java</file></files><comments><comment>Tests: Remove accidentally added bwc behavior for auto choosing a</comment><comment>version.</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/test/ElasticsearchBackwardsCompatIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Tests: Improve BWC preconditions to error cleanly when wire formats differ.</comment></comments></commit></commits></item><item><title>Term(s) Query/Filter : add _index support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7965</link><project id="" key="" /><description>This commit adds support for _index to the term query, terms query, term filter and terms filter.
All of the below queries work as expected :

```
GET /*/_search
{
  "query": {
    "term": {
    "_index": {
      "value": "reddit"
    }
  }}
}

GET /*/_search
{
  "query" :
  {
    "terms": {
      "_index": [
        "reddit2"
        ]
    }
  }
}

GET /*/_search
{
  "query":
   {
     "filtered": {
       "query": {
     "match_all": {}
       },
       "filter": {
          "terms":
          {
            "_index": [  "reddit"]
          }
       }
     }
   }
}

GET /*/_search
{"query":
   {
     "filtered": {
       "query": {
     "match_all": {}
       },
       "filter": {
     "term": { "_index": { "value": "reddit" }}
       }
     }
   } }
```

Since these are evaluated for each index (to resolve mappings) this change simply tests to see if the index is the correct one and returns a MATCH_ALL or MATCH_NO.

Closes #3316
</description><key id="44713017">7965</key><summary>Term(s) Query/Filter : add _index support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label></labels><created>2014-10-02T17:19:47Z</created><updated>2015-07-07T08:50:09Z</updated><resolved>2015-07-05T17:15:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-08T14:13:01Z" id="58363303">@GaelTadh I left some (minor) comments
</comment><comment author="markharwood" created="2015-07-03T16:57:58Z" id="118392454">This may now be superseded by https://github.com/elastic/elasticsearch/pull/12027 which works with the latest master and includes support for Match query etc too.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Fix config directory for external nodes in bwc tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7964</link><project id="" key="" /><description>The logging configuration was expected in the path.home folder which is set to
target/JX
when running the bwc tests from the console.
Therefore the logger could not be initialized with error message:

[INFO] Failed to configure logging...
org.elasticsearch.ElasticsearchException: Failed to load logging configuration
        at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:117)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:81)
        at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:96)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:180)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.nio.file.NoSuchFileException: /home/britta/es/target/J0/config
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55)
        at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
        at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:97)
        at java.nio.file.Files.readAttributes(Files.java:1686)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:109)
        at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
        at java.nio.file.Files.walkFileTree(Files.java:2602)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:107)
        ... 4 more
log4j:WARN No appenders could be found for logger (node).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

Setting the config directory fixes this.

Logs from external nodes are still not printed properly. They are inserted to the log
whenever the stdout is printed ([WARNING] JVM J0: stdout was not empty...)
</description><key id="44711564">7964</key><summary>[TEST] Fix config directory for external nodes in bwc tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-10-02T17:05:00Z</created><updated>2014-10-07T09:35:50Z</updated><resolved>2014-10-07T09:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-02T19:40:03Z" id="57693414">LGTM
</comment><comment author="javanna" created="2014-10-03T07:34:52Z" id="57764694">+1 thanks @brwe !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/ExternalNode.java</file></files><comments><comment>[TEST] Fix config directory for external nodes in bwc tests</comment></comments></commit></commits></item><item><title>Elastic restart breaks search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7963</link><project id="" key="" /><description>If I start with a clean data directory and:
1) start elastic
2) create index
3) configure mapping
4) index documents
5) search (details below) 
I get the one result that I expect.

If I then stop and restart elastic and re-execute the same search, I get zero results (for my search).

Theres probably something peculiar about the the mapping &amp; the query I am executing, but the behaviour definitely changes after a restart.

The query I execute is:

``` json
{
  "from" : 0,
  "size" : 20,
  "query" : {
    "filtered" : {
      "query" : {
        "match_all" : { }
      },
      "filter" : {
        "and" : {
          "filters" : [ {
            "type" : {
              "value" : "inventory"
            }
          }, {
            "term" : {
              "id" : "7774"
            }
          } ]
        }
      }
    }
  }
}
```

The document looks something like:

``` json
{
        "brand" : "HAL",
        "commodity" : "Aluminium",
        "counterParty" : "Acme Aloominem Company",
        "counterPartyCode" : "ACME",
        "grade" : "99.7%",
        "id" : 7774,
        "indirectTaxRegion" : null,
        "netGainLoss" : 0.0000,
        "netGainLossUnitOfMeasure" : "MT"
      }
```

The "id" is actually the id of the document.
The type is "inventory"

My index mappings are :

``` json
{
  "myindex" : {
    "mappings" : {
      "inventory" : {
        "index_analyzer" : "default_index",
        "search_analyzer" : "default_search",
        "_all" : {
          "auto_boost" : true
        },
        "properties" : {
          "id" : {
            "type" : "long",
            "include_in_all" : false
          },
          "counterPartyCode" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
        }
      }
    }
  }
}
```

There are more mappings, but I have 

Now, I acknowledge that there are some pecularities or questions that might arise:
Q: "Why are you using a term query using "7774" (ie string) instead of 7774 for a numerical (long) field or why are you not instead using a get operation?"

A: I present a simplified API to the User Interface - which either does a text-search (google style search) on the _all field, or it does an exact match on a given field. It just so happens that we pass Strings around - hence the string in the term query instead of a number.
</description><key id="44706615">7963</key><summary>Elastic restart breaks search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels /><created>2014-10-02T16:18:43Z</created><updated>2014-10-14T00:18:57Z</updated><resolved>2014-10-10T11:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nickminutello" created="2014-10-06T13:06:34Z" id="58013788">Still getting to the core of the problem, but it looks like it is matching on integer fields is what is misbehaving...
</comment><comment author="imotov" created="2014-10-06T15:28:49Z" id="58034599">Which version of elasticsearch is it? How do you index documents (using bulk or individual index requests)? Can you create a script that reproduces the issue on an empty cluster?
</comment><comment author="nickminutello" created="2014-10-06T15:52:43Z" id="58038567">Oops. 
Elastic v1.3.2
On Windows (AMD64) (so far thats all I have tested on)
On Java 1.7 (1.7.0_60) (so far thats all I have tested on)
</comment><comment author="imotov" created="2014-10-06T16:23:58Z" id="58043929">@nickminutello I tried to reproduce the issue using information that you provided but everything seem to work. I used fresh installation of v1.3.2 and the following script: https://gist.github.com/imotov/60f0c8f00819eb2d0004 I am getting this record back before and after the restart.
</comment><comment author="nickminutello" created="2014-10-06T16:58:26Z" id="58049500">Ok. Let me see if I can create a test case to reproduce it. It might be something peculiar to our mapping...
</comment><comment author="nickminutello" created="2014-10-06T18:21:24Z" id="58065553">Its definitely something related to the index creation/config or mapping.
If the server is restarted between steps (3) and (4) the problem is exhibited.

I will see if I can reproduce it in a simple stand-alone test case and post it tomorrow.
</comment><comment author="nickminutello" created="2014-10-07T11:41:48Z" id="58171632">In narrowing it down, I think there is a bit more indication what the problem is -  a clash of mappings...

Here is the code.
To reproduce the bug, nuke data directory &amp; run twice.
The first run will exit cleanly (ie find the inventory item).
The second will fail.

``` java

package elastic.bug;

import java.io.IOException;
import java.util.Map;

import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;
import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;
import org.elasticsearch.action.index.IndexResponse;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.client.AdminClient;
import org.elasticsearch.client.Client;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.node.Node;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.joda.JodaModule;

import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;
import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
import static org.junit.Assert.assertEquals;

public class SearchByIdFailure
{
    private static final String INDEX = "bugindex";
    private static final ObjectMapper json = new ObjectMapper();
    static {
        json.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);
        json.configure(SerializationFeature.WRITE_BIGDECIMAL_AS_PLAIN, true);
        json.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);
        json.configure(JsonGenerator.Feature.ESCAPE_NON_ASCII, true);
        json.registerModule(new JodaModule());
    }

    private static Node node;

    public static void main(String[] args) throws IOException
    {
        String elasticHome = "target";
        ImmutableSettings.Builder settings = settingsBuilder()
            .put("cluster.name", "Meow")
            .put("path.data", elasticHome + "/data")
            .put("transport.tcp.port", 9000)
            .put("http.port", 9001)
            .put("http.enabled", true);

        node = nodeBuilder()
            .loadConfigSettings(false)
            .client(false)
            .local(true)
            .settings(settings)
            .build();

        node.start();


        if (!indexExists())
        {
            createIndex(
                "{\"analysis\":{\"analyzer\":{\"logistics_index\":{\"tokenizer\":\"whitespace\",\"filter\":[\"lowercase\"]},\"logistics_search\":{\"tokenizer\":\"whitespace\",\"filter\":[\"lowercase\"]},\"default_index\":{\"type\":\"combo\",\"deduplication\":\"true\",\"sub_analyzers\":[\"logistics_index\",\"standard\"]},\"default_search\":{\"type\":\"combo\",\"deduplication\":\"true\",\"sub_analyzers\":[\"logistics_search\",\"standard\"]}}},\"number_of_replicas\":0}");
        }

        mapping("inventory", "{" +
            "  \"properties\" : {" +
            "    \"groupCompanyId\" : {" +
            "      \"include_in_all\" : false," +
            "      \"index\" : \"not_analyzed\"," +
            "      \"type\" : \"string\"" +
            "    }," +
            "    \"id\" : {" +
            "      \"include_in_all\" : false," +
            "      \"type\" : \"long\"" +
            "    }" +
            "  }" +
            "}");

        mapping("task", "{" +
            "  \"properties\" : {" +
            "    \"id\" : {" +
            "      \"include_in_all\" : false," +
            "      \"index\" : \"not_analyzed\"," +
            "      \"type\" : \"string\"" +
            "    }" +
            "  }" +
            "}");

        index("7774", "inventory", "{\"groupCompanyId\":\"9FB2FFDC0FF5797FE04014AC6F0616B6\",\"id\":7774,\"moo\":\"cow\"\n}");
        System.out.println("inventory = " + prettyPrint(shortInvSource));

        refresh();
        String searchSource = "{\"from\":0,\"size\":20,\"query\":{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"inventory\"}},{\"terms\":{\"groupCompanyId\":[\"0D13EF2D0E114D43BFE362F5024D8873\",\"0D593DE0CFBE49BEA3BF5AD7CD965782\",\"1E9C36CC45C64FCAACDEE0AF4FB91FBA\",\"33A946DC2B0E494EB371993D345F52E4\",\"6471AA50DFCF4192B8DD1C2E72A032C7\",\"9FB2FFDC0FF0797FE04014AC6F0616B6\",\"9FB2FFDC0FF1797FE04014AC6F0616B6\",\"9FB2FFDC0FF2797FE04014AC6F0616B6\",\"9FB2FFDC0FF3797FE04014AC6F0616B6\",\"9FB2FFDC0FF5797FE04014AC6F0616B6\",\"9FB2FFDC0FF6797FE04014AC6F0616B6\",\"AFE0FED33F06AFB6E04015AC5E060AA3\",\"NO_GROUP_COMPANY\"]}},{\"term\":{\"id\":\"7774\"}}]}}}},\"aggregations\":{\"aggregated_quantity\":{\"sum\":{\"field\":\"quantityNormalised\"}}}}";
        System.out.println("search = " + prettyPrint(searchSource));


        SearchResponse response = client()
            .prepareSearch(INDEX)
            .setSource(searchSource)
            .execute()
            .actionGet();

        assertEquals(1, response.getHits().getTotalHits());

        node.stop().close();
    }

    private static void mapping(String inventory, String propertiesSource)
    {
        admin()
            .indices()
            .preparePutMapping(INDEX)
            .setType(inventory)
            .setSource(propertiesSource)
            .execute()
            .actionGet();
    }

    private static void createIndex(String settingsSource)
    {
        admin()
            .indices()
            .prepareCreate(INDEX)
            .setSettings(settingsSource)
            .execute()
            .actionGet();
    }

    public static boolean indexExists()
    {
        IndicesExistsResponse indicesExistsResponse = admin()
            .indices()
            .prepareExists(INDEX)
            .execute().actionGet();
        return indicesExistsResponse.isExists();
    }

    private static void index(String id, String type, String content) throws IOException
    {
        //noinspection unchecked
        index(INDEX, type, id, json.readValue(content, Map.class));
    }

    private static String prettyPrint(String aggregationSource) throws IOException
    {
        return json.writerWithDefaultPrettyPrinter().writeValueAsString(json.readValue(aggregationSource, Map.class));
    }

    private static RefreshResponse refresh() {
        return admin().indices().prepareRefresh().execute().actionGet();
    }

    private static  IndexResponse index(String index, String type, String id, Map&lt;String, Object&gt; source) {
        return client().prepareIndex(index, type, id).setSource(source).execute().actionGet();
    }

    private static AdminClient admin() {
        return client().admin();
    }

    private static Client client()
    {
        return node.client();
    }
}


```
</comment><comment author="imotov" created="2014-10-08T13:08:50Z" id="58354307">@nickminutello how is shortInvSource defined?
</comment><comment author="nickminutello" created="2014-10-08T15:37:17Z" id="58377547">Gah. Sorry. (Thats the risk of making a last-moment edit in code in the github comment box - I should have left it alone). shortInvSource is just the source that has been passed to index. Either extract a variable - or nuke the line that prints out the json.
</comment><comment author="nickminutello" created="2014-10-08T15:38:21Z" id="58377734">Here is the edited version...

``` java

package elastic.bug;

import java.io.IOException;
import java.util.Map;

import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsResponse;
import org.elasticsearch.action.admin.indices.refresh.RefreshResponse;
import org.elasticsearch.action.index.IndexResponse;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.client.AdminClient;
import org.elasticsearch.client.Client;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.node.Node;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.joda.JodaModule;

import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;
import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
import static org.junit.Assert.assertEquals;

public class SearchByIdFailure
{
    private static final String INDEX = "bugindex";
    private static final ObjectMapper json = new ObjectMapper();
    static {
        json.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);
        json.configure(SerializationFeature.WRITE_BIGDECIMAL_AS_PLAIN, true);
        json.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);
        json.configure(JsonGenerator.Feature.ESCAPE_NON_ASCII, true);
        json.registerModule(new JodaModule());
    }

    private static Node node;

    public static void main(String[] args) throws IOException
    {
        String elasticHome = "target";
        ImmutableSettings.Builder settings = settingsBuilder()
            .put("cluster.name", "Meow")
            .put("path.data", elasticHome + "/data")
            .put("transport.tcp.port", 9000)
            .put("http.port", 9001)
            .put("http.enabled", true);

        node = nodeBuilder()
            .loadConfigSettings(false)
            .client(false)
            .local(true)
            .settings(settings)
            .build();

        node.start();


        if (!indexExists())
        {
            createIndex(
                "{\"analysis\":{\"analyzer\":{\"logistics_index\":{\"tokenizer\":\"whitespace\",\"filter\":[\"lowercase\"]},\"logistics_search\":{\"tokenizer\":\"whitespace\",\"filter\":[\"lowercase\"]},\"default_index\":{\"type\":\"combo\",\"deduplication\":\"true\",\"sub_analyzers\":[\"logistics_index\",\"standard\"]},\"default_search\":{\"type\":\"combo\",\"deduplication\":\"true\",\"sub_analyzers\":[\"logistics_search\",\"standard\"]}}},\"number_of_replicas\":0}");
        }

        mapping("inventory", "{" +
            "  \"properties\" : {" +
            "    \"groupCompanyId\" : {" +
            "      \"include_in_all\" : false," +
            "      \"index\" : \"not_analyzed\"," +
            "      \"type\" : \"string\"" +
            "    }," +
            "    \"id\" : {" +
            "      \"include_in_all\" : false," +
            "      \"type\" : \"long\"" +
            "    }" +
            "  }" +
            "}");

        mapping("task", "{" +
            "  \"properties\" : {" +
            "    \"id\" : {" +
            "      \"include_in_all\" : false," +
            "      \"index\" : \"not_analyzed\"," +
            "      \"type\" : \"string\"" +
            "    }" +
            "  }" +
            "}");

        index("7774", "inventory", "{\"groupCompanyId\":\"9FB2FFDC0FF5797FE04014AC6F0616B6\",\"id\":7774,\"moo\":\"cow\"\n}");

        refresh();
        String searchSource = "{\"from\":0,\"size\":20,\"query\":{\"filtered\":{\"query\":{\"match_all\":{}},\"filter\":{\"and\":{\"filters\":[{\"type\":{\"value\":\"inventory\"}},{\"terms\":{\"groupCompanyId\":[\"0D13EF2D0E114D43BFE362F5024D8873\",\"0D593DE0CFBE49BEA3BF5AD7CD965782\",\"1E9C36CC45C64FCAACDEE0AF4FB91FBA\",\"33A946DC2B0E494EB371993D345F52E4\",\"6471AA50DFCF4192B8DD1C2E72A032C7\",\"9FB2FFDC0FF0797FE04014AC6F0616B6\",\"9FB2FFDC0FF1797FE04014AC6F0616B6\",\"9FB2FFDC0FF2797FE04014AC6F0616B6\",\"9FB2FFDC0FF3797FE04014AC6F0616B6\",\"9FB2FFDC0FF5797FE04014AC6F0616B6\",\"9FB2FFDC0FF6797FE04014AC6F0616B6\",\"AFE0FED33F06AFB6E04015AC5E060AA3\",\"NO_GROUP_COMPANY\"]}},{\"term\":{\"id\":\"7774\"}}]}}}},\"aggregations\":{\"aggregated_quantity\":{\"sum\":{\"field\":\"quantityNormalised\"}}}}";
        System.out.println("search = " + prettyPrint(searchSource));


        SearchResponse response = client()
            .prepareSearch(INDEX)
            .setSource(searchSource)
            .execute()
            .actionGet();

        assertEquals(1, response.getHits().getTotalHits());

        node.stop().close();
    }

    private static void mapping(String inventory, String propertiesSource)
    {
        admin()
            .indices()
            .preparePutMapping(INDEX)
            .setType(inventory)
            .setSource(propertiesSource)
            .execute()
            .actionGet();
    }

    private static void createIndex(String settingsSource)
    {
        admin()
            .indices()
            .prepareCreate(INDEX)
            .setSettings(settingsSource)
            .execute()
            .actionGet();
    }

    public static boolean indexExists()
    {
        IndicesExistsResponse indicesExistsResponse = admin()
            .indices()
            .prepareExists(INDEX)
            .execute().actionGet();
        return indicesExistsResponse.isExists();
    }

    private static void index(String id, String type, String content) throws IOException
    {
        //noinspection unchecked
        index(INDEX, type, id, json.readValue(content, Map.class));
    }

    private static String prettyPrint(String aggregationSource) throws IOException
    {
        return json.writerWithDefaultPrettyPrinter().writeValueAsString(json.readValue(aggregationSource, Map.class));
    }

    private static RefreshResponse refresh() {
        return admin().indices().prepareRefresh().execute().actionGet();
    }

    private static  IndexResponse index(String index, String type, String id, Map&lt;String, Object&gt; source) {
        return client().prepareIndex(index, type, id).setSource(source).execute().actionGet();
    }

    private static AdminClient admin() {
        return client().admin();
    }

    private static Client client()
    {
        return node.client();
    }
}


```
</comment><comment author="imotov" created="2014-10-10T11:42:20Z" id="58644565">Yes, this is a version of #5851. A quick workaround is to use 

```
"term": {
    "inventory.id": "7774"
}
```
</comment><comment author="nickminutello" created="2014-10-11T17:17:58Z" id="58757065">We still have the issue of it working until there is a restart 
</comment><comment author="imotov" created="2014-10-14T00:18:57Z" id="58973543">@nickminutello what most likely happening here is before restart `id` is getting resolved into `inventory.id`, after restart when mapping is reloaded, it's getting resolved as `task.id`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Misbehaviour when using missing filter on fields which have the same name as _type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7962</link><project id="" key="" /><description>When using the missing filter on a field which has the same name as _type, it seems to get transformed to a match_all filter:

``` bash
/tmp $ cat t
#!/bin/sh

curl http://$HOST:9200/
curl -XDELETE http://$HOST:9200/foo
curl -XPOST http://$HOST:9200/foo -d '{}'
curl -XPOST http://$HOST:9200/foo/bar/_mapping -d '{
    "dynamic": "strict",
    "_index": {
        "enabled": true
    },
    "_id": {
        "index": "not_analyzed",
        "indexed": false,
        "store": true
    },
    "properties": {
        "foo": {
            "type": "string",
            "index": "not_analyzed"
        },
        "bar": {
            "type": "string",
            "index": "not_analyzed"
        }
    }
}
'
curl -XPOST http://$HOST:9200/foo/bar/123 -d '{"foo": "abc", "bar": "def"}'
curl -XPOST http://$HOST:9200/foo/bar/456 -d '{"foo": "abc", "bar": "def"}'

sleep 1

curl -XGET http://$HOST:9200/foo/bar/123/_explain?pretty -d '
{
    "query": {
        "filtered": {
            "filter": {
                "bool": {
                    "must": [
                        {
                            "missing": {"field": "bar"}
                        }
                    ]
                }
            }
        }
    }
}'

curl -XGET http://$HOST:9200/foo/bar/123/_explain?pretty -d '
{
    "query": {
        "filtered": {
            "filter": {
                "bool": {
                    "must": [
                        {
                            "missing": {"field": "foo"}
                        }
                    ]
                }
            }
        }
    }
}'

curl -XGET http://$HOST:9200/foo/bar/123/_explain?pretty -d '
{
    "query": {
        "filtered": {
            "filter": {
                    "missing": {"field": "bar"}
                }
            }
        }
    }
}'
/tmp $ HOST=xxx ./t
{
  "status" : 200,
  "name" : "xxx",
  "version" : {
    "number" : "1.3.4",
    "build_hash" : "a70f3ccb52200f8f2c87e9c370c6597448eb3e45",
    "build_timestamp" : "2014-09-30T09:07:17Z",
    "build_snapshot" : false,
    "lucene_version" : "4.9"
  },
  "tagline" : "You Know, for Search"
}
{"acknowledged":true}{"acknowledged":true}{"acknowledged":true}{"_index":"foo","_type":"bar","_id":"123","_version":1,"created":true}{"_index":"foo","_type":"bar","_id":"456","_version":1,"created":true}{
  "_index" : "foo",
  "_type" : "bar",
  "_id" : "123",
  "matched" : true,
  "explanation" : {
    "value" : 1.0,
    "description" : "ConstantScore(BooleanFilter(+*:*)), product of:",
    "details" : [ {
      "value" : 1.0,
      "description" : "boost"
    }, {
      "value" : 1.0,
      "description" : "queryNorm"
    } ]
  }
}
{
  "_index" : "foo",
  "_type" : "bar",
  "_id" : "123",
  "matched" : false,
  "explanation" : {
    "value" : 0.0,
    "description" : "ConstantScore(BooleanFilter(+cache(NotFilter(cache(BooleanFilter(_field_names:foo)))))) doesn't match id 0"
  }
}
{
  "_index" : "foo",
  "_type" : "bar",
  "_id" : "123",
  "matched" : true,
  "explanation" : {
    "value" : 1.0,
    "description" : "ConstantScore(cache(_type:bar)), product of:",
    "details" : [ {
      "value" : 1.0,
      "description" : "boost"
    }, {
      "value" : 1.0,
      "description" : "queryNorm"
    } ]
  }
}
```
</description><key id="44703111">7962</key><summary>Misbehaviour when using missing filter on fields which have the same name as _type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mcuelenaere</reporter><labels><label>bug</label></labels><created>2014-10-02T15:48:42Z</created><updated>2014-11-05T13:56:59Z</updated><resolved>2014-11-05T13:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T16:23:05Z" id="59073564">Hi @mcuelenaere 

Thanks for reporting this.  A simpler recreation follows:

```
DELETE /foo

POST /foo

POST /foo/bar/_mapping
{
    "properties": {
        "foo": {
            "type": "string",
            "index": "not_analyzed"
        },
        "bar": {
            "type": "string",
            "index": "not_analyzed"
        }
    }
}

POST /foo/bar/123
{"foo": "abc", "bar": "def"}

POST /foo/bar/456
{"foo": "abc", "bar": "def"}
```

This explain:

```
GET /foo/bar/123/_explain?pretty
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "missing": {
                "field": "foo"
              }
            }
          ]
        }
      }
    }
  }
}
```

Returns:

```
{
   "_index": "foo",
   "_type": "bar",
   "_id": "123",
   "matched": false,
   "explanation": {
      "value": 0,
      "description": "ConstantScore(BooleanFilter(+cache(NotFilter(cache(BooleanFilter(_field_names:foo)))))) doesn't match id 0"
   }
}
```

While this explain:

```
GET /foo/bar/123/_explain?pretty
{
  "query": {
    "filtered": {
      "filter": {
        "missing": {
          "field": "bar"
        }
      }
    }
  }
}
```

Incorrectly returns:

```
{
   "_index": "foo",
   "_type": "bar",
   "_id": "123",
   "matched": true,
   "explanation": {
      "value": 1,
      "description": "ConstantScore(cache(_type:bar)), product of:",
      "details": [
         {
            "value": 1,
            "description": "boost"
         },
         {
            "value": 1,
            "description": "queryNorm"
         }
      ]
   }
}
```
</comment><comment author="jpountz" created="2014-10-14T17:08:17Z" id="59080577">For reference this is an old bug, I could reproduce it with 1.1 and it probably also reproduces on earlier versions.

Since we don't index anything for inner nodes of a json object, when we get an exists/missing filter on a field `f` and `f` is mapped as an object, it is internally translated to `f.*` so that it can match anything that would be under `f`. The issue here is that when we try to look up an object mapper that has `bar` as a path, we get the root object mapper since `bar` is the name of the type (which is weird to me, I need to check if we have things that rely upon it).

So in the end we try to match on `bar.*`, which doesn't match any field, so we return a `match_all` filter.
</comment><comment author="jpountz" created="2014-11-05T13:56:59Z" id="61809917">I'm closing in favor of #4081, the issue here is that elasticsearch tried to interpret the first part of the field name as a type.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add doc values support to boolean fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7961</link><project id="" key="" /><description>This pull request makes boolean handled like dates and ipv4 addresses: things
are stored as as numerics under the hood and aggregations add some special
formatting logic in order to return true/false in addition to 1/0.

For example, here is an output of a terms aggregation on a boolean field:

```
   "aggregations": {
      "top_f": {
         "doc_count_error_upper_bound": 0,
         "buckets": [
            {
               "key": 0,
               "key_as_string": "false",
               "doc_count": 2
            },
            {
               "key": 1,
               "key_as_string": "true",
               "doc_count": 1
            }
         ]
      }
   }
```

Sorted numeric doc values are used under the hood.

Close #4678
Close #7851
</description><key id="44694257">7961</key><summary>Add doc values support to boolean fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-02T14:42:37Z</created><updated>2015-06-06T16:23:19Z</updated><resolved>2015-04-02T14:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-02T19:44:30Z" id="57694008">@jpountz this looks good to me but I think we need to make sure that then `index.created.version` is &gt;= 1.5.0 ? Otherwise we get inconsistent results?
</comment><comment author="jpountz" created="2014-10-02T22:03:07Z" id="57717125">@s1monw I pushed a new commit that only switches to this new field data on new indices. That said, I'm wondering if that is the right trade-off:
- if we only enable this behavior on new indices (what the last commit does) then requests across _indices_ that have several versions will fail (even if all nodes have the same version) since some indices will serialize booleans as strings and others as numbers
- if we enable this behavior all the time (what happens without the last commit) then requests will fail if not all nodes have the same version of elasticsearch

Another option is to only push this change to 2.0?
</comment><comment author="jpountz" created="2014-10-08T11:29:13Z" id="58344035">After discussing this with @s1monw I made the change 2.0-only.
</comment><comment author="jpountz" created="2014-10-12T15:57:37Z" id="58806331">@s1monw I updated the 2.0 migration guide with the breaking change for boolean fields.
</comment><comment author="s1monw" created="2014-10-12T20:07:34Z" id="58820335">Similar to what @rjernst did in https://github.com/elasticsearch/elasticsearch/pull/8013 I think I'd really like to see this to be tested against an old index. See https://github.com/elasticsearch/elasticsearch/pull/8013/files#diff-ec2b53cfb6bd9c938d5a48d14a34b609R43 otherwise LGTM
</comment><comment author="jpountz" created="2014-10-13T07:40:06Z" id="58856598">@s1monw There used to be a backward compatibility test (https://github.com/jpountz/elasticsearch/commit/9700deb1849173fe6312925f0a4a2050ab0ef1a7#diff-8) but I removed it when I made this change 2.0-only since we cannot run clusters that contain both 1.x and 2.0 nodes.
</comment><comment author="jpountz" created="2014-11-03T10:53:14Z" id="61462467">@s1monw I'm wondering that your last comment was maybe about #7954  ?
</comment><comment author="s1monw" created="2014-11-03T11:03:53Z" id="61463517">I don't think so while it applies to that too :) I think we should check that we can upgrade an existing boolean field or at least handle it just fine if somebody does a full restart with 2.0
</comment><comment author="jpountz" created="2015-04-01T13:08:41Z" id="88473744">@s1monw This PR has been stalled for some time so I rebased it to a recent master and added the bw compat tests that you were asking for. Could you take another look?

Note that I only regenerated the static bw index for 1.5 for now but I will need to do it for all versions before pushing if I don't want to break the bw tests.
</comment><comment author="jpountz" created="2015-04-01T14:35:56Z" id="88507802">@rjernst You might want to have a look at this one too as I know you've done a lot of stuff recently around bw compat testing and mappings.
</comment><comment author="s1monw" created="2015-04-02T09:08:47Z" id="88836457">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatterStreams.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java</file><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/BooleanFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/BooleanTermsTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Merge pull request #7961 from jpountz/enhancement/boolean_doc_values</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatterStreams.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java</file><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/BooleanFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/BooleanTermsTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Merge pull request #7961 from jpountz/enhancement/boolean_doc_values</comment></comments></commit></commits></item><item><title>Significant terms can throw error on index with deleted docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7960</link><project id="" key="" /><description>If an index contains a highly popular term and many deleted documents it can cause an error as reported in issue #7951  

The background count for docs should include deleted docs otherwise a term’s docFreq (which includes deleted docs) can exceed the number of docs reported in the index and cause an exception.

The randomisation that deletes documents is also removed from tests as this doc-accounting change would mean the specific scores being expected in tests would now be subject to random variability and so fail.

Closes #7951
</description><key id="44685448">7960</key><summary>Significant terms can throw error on index with deleted docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.5</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-02T13:25:14Z</created><updated>2015-06-07T17:50:23Z</updated><resolved>2014-10-03T13:41:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-02T22:16:54Z" id="57718627">The change looks good, I think it would be good to have a test too?
</comment><comment author="markharwood" created="2014-10-03T13:41:48Z" id="57797536">Committed in https://github.com/elasticsearch/elasticsearch/commit/f878f40ae59c686ed772bddc2404d45f62ebfbc7
Removed the labels on this PR and added them to the issue. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>1.3.x Cannot use ES client within Jersey Servlet, due to ASM incompatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7959</link><project id="" key="" /><description>Exactly the same as in #4858, but reintroduced in 1.3.x.
With ES 1.3.4 I get asm 4.1, and with jersey 1.17.1 I get asm 3.1, which causes:

```
java.lang.IncompatibleClassChangeError: Implementing class
        at java.lang.ClassLoader.defineClass1(Native Method) ~[na:1.7.0_67]
        at java.lang.ClassLoader.defineClass(ClassLoader.java:800) ~[na:1.7.0_67]
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) ~[na:1.7.0_67]
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) ~[na:1.7.0_67]
        at java.net.URLClassLoader.access$100(URLClassLoader.java:71) ~[na:1.7.0_67]
        at java.net.URLClassLoader$1.run(URLClassLoader.java:361) ~[na:1.7.0_67]
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[na:1.7.0_67]
        at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_67]
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[na:1.7.0_67]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425) ~[na:1.7.0_67]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) ~[na:1.7.0_67]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ~[na:1.7.0_67]
        at com.sun.jersey.api.core.ScanningResourceConfig.init(ScanningResourceConfig.java:79) ~[jersey-server-1.17.1.jar:1.17.1]
        at com.sun.jersey.api.core.PackagesResourceConfig.init(PackagesResourceConfig.java:104) ~[jersey-server-1.17.1.jar:1.17.1]
        at com.sun.jersey.api.core.PackagesResourceConfig.&lt;init&gt;(PackagesResourceConfig.java:78) ~[jersey-server-1.17.1.jar:1.17.1]
        at com.sun.jersey.api.core.PackagesResourceConfig.&lt;init&gt;(PackagesResourceConfig.java:89) ~[jersey-server-1.17.1.jar:1.17.1]
        at com.sun.jersey.spi.container.servlet.WebComponent.createResourceConfig(WebComponent.java:696) ~[jersey-servlet-1.17.1.jar:1.17.1]
        at com.sun.jersey.spi.container.servlet.WebComponent.createResourceConfig(WebComponent.java:674) ~[jersey-servlet-1.17.1.jar:1.17.1]
        at com.sun.jersey.spi.container.servlet.WebComponent.init(WebComponent.java:203) ~[jersey-servlet-1.17.1.jar:1.17.1]
        at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:374) ~[jersey-servlet-1.17.1.jar:1.17.1]
        at com.sun.jersey.spi.container.servlet.ServletContainer.init(ServletContainer.java:557) ~[jersey-servlet-1.17.1.jar:1.17.1]
        at javax.servlet.GenericServlet.init(GenericServlet.java:244) ~[javax.servlet-3.0.0.v201112011016.jar:na]
```
</description><key id="44673163">7959</key><summary>1.3.x Cannot use ES client within Jersey Servlet, due to ASM incompatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">henrikno</reporter><labels><label>discuss</label></labels><created>2014-10-02T11:00:36Z</created><updated>2016-05-13T15:17:36Z</updated><resolved>2014-10-15T18:41:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aritratony" created="2014-10-02T11:21:23Z" id="57614196">If you're using Maven, you can exclude asm like this:

```
   &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
        &lt;exclusions&gt;
            &lt;exclusion&gt;
                &lt;artifactId&gt;asm&lt;/artifactId&gt;
                &lt;groupId&gt;org.ow2.asm&lt;/groupId&gt;
            &lt;/exclusion&gt;
        &lt;/exclusions&gt;
    &lt;/dependency&gt;
```
</comment><comment author="roytmana" created="2014-10-03T18:51:26Z" id="57837876">Switch to latest jersey 2.x they now shadow asm and few other libs per my suggestion so you will not have any conflicts with es
</comment><comment author="henrikno" created="2014-10-05T14:55:59Z" id="57938446">@aritratony: I assume that it is safe for clients, so that's what I did.
ASM's FAQ recommends repackaging asm: http://asm.ow2.org/doc/faq.html#Q15
Shouldn't ES also shade asm?
</comment><comment author="clintongormley" created="2014-10-15T11:37:05Z" id="59192641">@rjernst is this because we now use lucene expressions? Any thoughts on this?
</comment><comment author="rjernst" created="2014-10-15T17:26:18Z" id="59242963">I don't think we can shade ASM in ES because it is on optional dependency.  Using expression scripts requires including the lucene expressions module (which transitively includes ASM).
</comment><comment author="s1monw" created="2014-10-15T18:41:35Z" id="59254585">@aritratony as long as you only use the client there shouldn't be any issues with excluding the dependency. I will close this as I agree with @rjernst 
</comment><comment author="kimchy" created="2014-10-15T18:52:41Z" id="59256390">@rjernst I wonder if it makes sense for Lucene expression to "jarjar" the ASM dependency? its very common to do so because of version conflicts.
</comment><comment author="rjernst" created="2014-10-15T20:10:50Z" id="59267864">@kimchy I don't think we should do this.  If users have conflicts, they can do this shading/jarjar themselves. At the lucene level, the "repackaging" can be questionable depending on dependency licenses? I'm saying in general, not in this specific case.  But there we should take the clear approach and keep lucene with concise dependencies which are visible, not embedded.
</comment><comment author="s1monw" created="2014-10-15T20:12:48Z" id="59268173">I agree here with @rjernst that's why we have it as a optional jar though
</comment><comment author="cs94njw" created="2016-05-13T15:17:36Z" id="219073837">I had the same issue with some other software, and the culprit was:

&lt;artifactId&gt;asm-debug-all&lt;/artifactId&gt;
&lt;groupId&gt;org.ow2.asm&lt;/groupId&gt;
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>start analyzer stopwords after one word</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7958</link><project id="" key="" /><description>Is it possible to add the mapping settings to start analyze with stopwords after first word??

for example:

```
Search: The **bed**
Results: The **bed** is white; the **bed** is red
```

Right now i have:

```
Search: **The**
Results: empty results
```

my question:

```
Search: **The**
Results: **The**ory of mules, **The**me is great
```
</description><key id="44663476">7958</key><summary>start analyzer stopwords after one word</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marsanla</reporter><labels /><created>2014-10-02T08:50:08Z</created><updated>2014-10-14T14:17:52Z</updated><resolved>2014-10-14T14:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T14:17:52Z" id="59051746">Hi @marsanla 

You may want to look at the `remove_trailing` parameter in the stop token filter: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Ambiguous explanation of fractional intervals for date histogram aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7957</link><project id="" key="" /><description>The date histogram aggregation states

&lt;pre&gt;
fractional values are allowed, for example 1.5 hours
&lt;/pre&gt;

and goes on to give an explanation using a value of `1.5h`

Given that the date histogram supports bucketing by both month and minute, it is not immediately obvious what the correct abbreviation for either is.

While easy enough to find out by testing (`M` for month, `m` for minute), I think the documentation could use an improvement in this regard.

Documentation page:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html
</description><key id="44652250">7957</key><summary>Ambiguous explanation of fractional intervals for date histogram aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">chrisguiney</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2014-10-02T05:27:02Z</created><updated>2014-11-08T16:50:14Z</updated><resolved>2014-11-08T16:50:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T14:14:37Z" id="59051166">Hi @chrisguiney 

Agreed. Perhaps a link to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/common-options.html#time-units  (written in asciidoc as `&lt;&lt;time-units&gt;&gt;`).  Fancy sending a pull request?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Update datehistogram-aggregation.asciidoc</comment></comments></commit></commits></item><item><title>Feature: Terms Aggregation with From Parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7956</link><project id="" key="" /><description>Forgive me if there is a better way to do this.

In the #6299, `from` was added to `top_hits` aggregations. If `terms` aggregations also had a `from` field, it would allow to to do `distinct` type of queries. 

Consider the following documents in an index:

```
[
  { "revision": "a1", "id": 1, "updatedAt": 0 },
  { "revision": "b1", "id": 1, "updatedAt": 1 },
  { "revision": "a2", "id": 2, "updatedAt": 2 }
]
```

We can use a `terms` aggregation on `id` + `top_hits` aggregation sorted by `updatedAt` to get the latest revision for each `id`. However to get the second page of this request would be non-trivial as it would require getting both the first and second page of the top level terms aggregation combined in one response.
</description><key id="44650906">7956</key><summary>Feature: Terms Aggregation with From Parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kelaban</reporter><labels /><created>2014-10-02T04:54:41Z</created><updated>2014-10-14T14:09:55Z</updated><resolved>2014-10-14T14:09:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-14T14:09:55Z" id="59050305">Hi @kelaban 

Unfortunately, there is no better way to support `from` in aggregations than retrieving all the buckets you need. See #4915 for more discussion.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multiple indexes setting for 'es.resource'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7955</link><project id="" key="" /><description>'es.resource' = 'apache-2014.09._/apache-access' or
'es.resource' = 'apache-2014.09.29,apache-2014.09.30/apache-access' 
are not working for 'select count(_) from test' which is HiveQL.
The count value will not be right.
'es.resource' configuration should support multiple indexes setting.
</description><key id="44637588">7955</key><summary>Multiple indexes setting for 'es.resource'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mungeol</reporter><labels /><created>2014-10-02T01:23:45Z</created><updated>2014-10-02T01:24:39Z</updated><resolved>2014-10-02T01:24:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Switch to murmurhash3 to route documents to shards.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7954</link><project id="" key="" /><description>We currently use the djb2 hash function in order to compute the shard a
document should go to. Unfortunately this hash function is not very
sophisticated and you can sometimes hit adversarial cases, such as numeric ids
on 33 shards.

Murmur3 generates hashes with a better distribution, which should avoid the
adversarial cases.

Here are some examples of how 100000 incremental ids are distributed to shards
using either djb2 or murmur3.

5 shards:
Murmur3: [19933, 19964, 19940, 20030, 20133]
DJB:     [20000, 20000, 20000, 20000, 20000]

3 shards:
Murmur3: [33185, 33347, 33468]
DJB:     [30100, 30000, 39900]

33 shards:
Murmur3: [2999, 3096, 2930, 2986, 3070, 3093, 3023, 3052, 3112, 2940, 3036, 2985, 3031, 3048, 3127, 2961, 2901, 3105, 3041, 3130, 3013, 3035, 3031, 3019, 3008, 3022, 3111, 3086, 3016, 2996, 3075, 2945, 2977]
DJB:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 900, 900, 900, 900, 1000, 1000, 10000, 10000, 10000, 10000, 9100, 9100, 9100, 9100, 9000, 9000, 0, 0, 0, 0, 0, 0]

Even if djb2 looks ideal in some cases (5 shards), the fact that the
distribution of its hashes has some patterns can raise issues with some shard
counts (eg. 3, or even worse 33).

Some tests have been modified because they relied on implementation details of
the routing hash function.

This change only affects indices that are created on or after elasticsearch 2.0.
</description><key id="44635099">7954</key><summary>Switch to murmurhash3 to route documents to shards.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>breaking</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-10-02T00:39:13Z</created><updated>2015-11-12T10:15:47Z</updated><resolved>2014-11-04T15:52:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-02T20:16:05Z" id="57698165">left some comments
</comment><comment author="s1monw" created="2014-10-03T06:02:38Z" id="57758453">I was thinking about this a bit more and I don't think we can do this safely in `1.5` Even if you have a checks for indices that are created with 1.5 you might have node clients in the cluster that are from a previous version and then your routing is messed up ie. `/_get` calls won't work correctly.  I also think we should then remove the ability to configure the hashfunction as much as possible.
</comment><comment author="jpountz" created="2014-10-03T23:16:00Z" id="57878418">@s1monw Good call, this would indeed fail. I will make this change 2.0-only.
</comment><comment author="jpountz" created="2014-10-08T11:21:44Z" id="58343355">I pushed a new commit that makes this change only happen as of 2.0 and makes the hash function an index setting for indices that have been created before 2.0. For newer indices the hash function is hard-coded to Murmur3.
</comment><comment author="jpountz" created="2014-10-12T16:12:46Z" id="58807167">I added a note about this change to the migration guide.
</comment><comment author="jpountz" created="2014-10-17T00:52:08Z" id="59452786">@s1monw I pushed a new iteration that
- adds a test that upgrades old indices (with both default routing and custom routing)
- switches to the 32-bits version of murmur3 (we didn't really need 128)
- makes the version and hash function first-class citizen of the index metadata
- logs a warning if you have the deprecated settings still set in the elasticsearch.yml

There is some noise in the tests because making the creation version a first-class citizen broke some tests that didn't set the version. I also fixed more tests that relied on implementation details of the routing function.
</comment><comment author="s1monw" created="2014-10-17T08:23:22Z" id="59482037">left some minor comments otherwise LGTM
</comment><comment author="drewr" created="2014-10-24T21:50:25Z" id="60454227">@jpountz Tried this branch to see if I could get better bulk distribution to battle an ingestion scaling issue.  It helped a little, but not as much as I was looking for.

In the course of testing, 30 shards on 30 nodes worked great.  Tried 31, 33, 37 shards, and when I did, after a bit (like, 6gb later, not immediately) the index would go red with a single shard unassigned.  The log on the node that was assigned that shard had this in the log:

```
[2014-10-24 21:28:48,949][WARN ][index.engine.internal    ] [ops27-data04-A] [foo][31] failed engine [merge exception]
org.apache.lucene.index.MergePolicy$MergeException: java.io.IOException: directory '/d/es/data/ops27-data04-A/org.elasticsearch.test.ops27.data/nodes/0/indices/foo/31/index' exists and is a directory, but cannot be listed: list() returned null
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:133)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)
```

There's nothing wrong with the directory on disk.  It has files, responds to filesystem commands.

I'm not sure if the branch is still a work-in-progress, but this surprised me a bit.  Just posting it here in case it's really a bug.  Thanks!
</comment><comment author="jpountz" created="2014-10-26T21:59:37Z" id="60534166">@drewr this is the exception you would get if you ran out of file descriptors, could you check your system limits?
</comment><comment author="drewr" created="2014-10-29T18:05:40Z" id="60974266">Interesting. I had never seen that particular symptom of too many open files...

I was in fact upping the limit, but I found a bug with the way Ubuntu handles `ulimit -n`, so those instances may in fact have had a low limit.  However, with the `1.x` branch of ES I hadn't had this issue.  That could be a test dissimilarity though.  Will try again.
</comment><comment author="jpountz" created="2014-11-03T10:56:52Z" id="61462816">Given that this PR has been out for a long time, I plan to merge it soon (if there are no objections). Then we can have another issue/PR if we think that PlainOperationRouting should be hardcoded.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/routing/operation/OperationRoutingModule.java</file><file>src/main/java/org/elasticsearch/cluster/routing/operation/hash/HashFunction.java</file><file>src/main/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunction.java</file><file>src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/LocalGatewayMetaState.java</file><file>src/test/java/org/elasticsearch/action/termvector/AbstractTermVectorTests.java</file><file>src/test/java/org/elasticsearch/action/termvector/GetTermVectorTests.java</file><file>src/test/java/org/elasticsearch/action/termvector/MultiTermVectorsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/ToAndFromJsonMetaDataTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/RoutingBackwardCompatibilityUponUpgradeTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocatePostApiFlagTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ConcurrentRebalanceRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/DisableAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ElectReplicaAsPrimaryDuringRelocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/FailedNodeRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/FilterRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/IndexBalanceTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/PreferLocalPrimariesToRelocatingPrimariesTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/PreferPrimaryAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryElectionRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/PrimaryNotRelocatedWhileBeingRecoveredTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ReplicaAllocatedAfterPrimaryTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ShardVersioningTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardOneReplicaRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/TenShardsOneReplicaRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ThrottlingAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/operation/hash/murmur3/Murmur3HashFunctionTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterSerializationTests.java</file><file>src/test/java/org/elasticsearch/cluster/serialization/ClusterStateToStringTests.java</file><file>src/test/java/org/elasticsearch/cluster/structure/RoutingIteratorTests.java</file><file>src/test/java/org/elasticsearch/gateway/local/state/meta/MetaDataStateFormatTest.java</file><file>src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationTests.java</file><file>src/test/java/org/elasticsearch/indices/stats/IndexStatsTests.java</file><file>src/test/java/org/elasticsearch/indices/store/IndicesStoreTests.java</file><file>src/test/java/org/elasticsearch/mget/SimpleMgetTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/rest/action/admin/indices/upgrade/UpgradeReallyOldIndexTest.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTests.java</file><file>src/test/java/org/elasticsearch/search/basic/TransportTwoNodesSearchTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Switch to murmurhash3 to route documents to shards.</comment></comments></commit></commits></item><item><title>Fix NPE in ScriptService when script file with no extension is deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7953</link><project id="" key="" /><description>Fixes #7689
</description><key id="44615385">7953</key><summary>Fix NPE in ScriptService when script file with no extension is deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-10-01T20:33:22Z</created><updated>2015-06-08T00:25:22Z</updated><resolved>2014-10-03T19:21:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-10-01T20:37:39Z" id="57534340">left one comment - other than that LGTM - shoud this go into 1.3 too?
</comment><comment author="rjernst" created="2014-10-01T20:45:17Z" id="57535410">The NPE fix, and test look good to me.  But do we really need these try/catches around listeners? Aren't all the listeners impl details of our system? I'm afraid we will just silently ignore bugs in them then, instead of actually catching them.
</comment><comment author="imotov" created="2014-10-02T12:42:15Z" id="57623740">@rjernst because these listeners are called on a separate scheduled thread, catching and logging these failures earlier on the stacktrace allows as to isolate the failure easier. The tricky part about #7689 was that it was caused by an uncaught exception within listener in ScriptService. This exception wasn't mentioned anywhere in the ticket but it essentially messed up logic of the resource watcher and caused it to fail on every check afterwards with the error that was reported in the bug. So, I think this change (besides improving resource checker resiliency) will actually help us to find bugs easier. 

I have updated the ticket with missed logging statement and improved test a bit to include a check that ScriptService is actually functional.

If v1.3.5 will ever happen I would love to see this fix there. This bug is kind of annoying. 
</comment><comment author="s1monw" created="2014-10-02T19:46:00Z" id="57694182">LGTM
</comment><comment author="imotov" created="2014-10-03T19:24:35Z" id="57842316">I didn't push it into 1.3 branch because 1.3 branch is missing changes added #6896 that this fix is relying on.
</comment><comment author="s1monw" created="2014-10-03T19:28:29Z" id="57843224">no worries - thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item></channel></rss>