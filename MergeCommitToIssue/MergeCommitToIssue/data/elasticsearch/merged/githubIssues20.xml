<?xml version="1.0" encoding="utf-8"?><rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Need to be able to specify include_upper / include_lower with Range and Date-Range Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5249</link><project id="" key="" /><description>Currently, the behaviour appears to be that when specifying a range aggregation, the lower value is included, but the upper value is not.

If I index the following data:

``` javascript
{ "integer" : 0,  "float" : 1.0, "date" : "2014-03-10" }
{ "integer" : 1,  "float" : 1.1, "date" : "2014-03-11" }
{ "integer" : 2,  "float" : 1.2, "date" : "2014-03-12" }
{ "integer" : 3,  "float" : 1.3, "date" : "2014-03-13" }
{ "integer" : 4,  "float" : 1.4, "date" : "2014-03-14" }
{ "integer" : 5,  "float" : 1.5, "date" : "2014-03-15" }
{ "integer" : 6,  "float" : 1.6, "date" : "2014-03-16" }
{ "integer" : 7,  "float" : 1.7, "date" : "2014-03-17" }
{ "integer" : 8,  "float" : 1.8, "date" : "2014-03-18" }
{ "integer" : 9,  "float" : 1.9, "date" : "2014-03-19" }
{ "integer" : 10, "float" : 2.0, "date" : "2014-03-20" }
```

And then (illustrating with dates) do a range aggregation:

``` javascript
"date_range" : {
  "date_range" : {
    "field" : "date",
    "ranges" : [ {
      "from" : "2014-03-15",
      "to" : "2014-03-17"
    } ]
  }
}
```

I get the following results:

``` javascript
"date_range" : {
  "buckets" : [ {
    "key" : "2014-03-15T00:00:00.000Z-2014-03-17T00:00:00.000Z",
    "from" : 1.3948416E12,
    "from_as_string" : "2014-03-15T00:00:00.000Z",
    "to" : 1.3950144E12,
    "to_as_string" : "2014-03-17T00:00:00.000Z",
    "doc_count" : 2
  } ]
}
```

Here we can see that its not including both from and to in the aggregation (or otherwise we would expect a doc_count of 3. The same is true if you do a range aggregation on the integer or decimal number.

Doing a different aggregation illustrates that its the upper thats getting omited:

``` javascript
"date_range_upper" : {
  "date_range" : {
    "field" : "date",
    "ranges" : [ {
      "from" : "2014-03-01",
      "to" : "2014-03-10"
    } ]
  }
}
```

Gives results

``` javascript
"date_range_upper" : {
  "buckets" : [ {
    "key" : "2014-03-01T00:00:00.000Z-2014-03-10T00:00:00.000Z",
    "from" : 1.393632E12,
    "from_as_string" : "2014-03-01T00:00:00.000Z",
    "to" : 1.3944096E12,
    "to_as_string" : "2014-03-10T00:00:00.000Z",
    "doc_count" : 0
  } ]
}
```

It needs to be possible to include the upper (or omit the lower) value in the aggregation specification.

Here is an example of what I need to do:
I want to do an aggregation as follows:

``` javascript
"aggregations": {
    "intentDate": {
        "date_range": {
            "field": "intentDate",
            "ranges": [
                {
                    "key": "Overdue",
                    "to": "2014-02-17"
                },
                {
                    "key": "March",
                    "from": "2014-02-18",
                    "to": "2014-03-17"
                },
                {
                    "key": "April",
                    "from": "2014-03-18",
                    "to": "2014-04-15"
                }
            ]
        }
    }
}
```

I want the upper value to be inclusive in the range.
Then I want to take the results to create a filter - without having to do any date/datetime manipulation to translate between them.
</description><key id="28235692">5249</key><summary>Need to be able to specify include_upper / include_lower with Range and Date-Range Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2014-02-25T10:10:05Z</created><updated>2015-12-03T19:39:20Z</updated><resolved>2015-10-14T14:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cn081" created="2014-10-22T13:57:22Z" id="60088259">+1
</comment><comment author="chevin99" created="2014-11-10T14:18:44Z" id="62389554">+1
</comment><comment author="clintongormley" created="2014-12-29T12:44:06Z" id="68254375">Hi @nickminutello 

Sorry it has taken a while to get to this.  Excluding the upper bound is intentional, and is designed to make it easier to create ranges with the minimum of arithmetic.  You just need to add your chosen interval (eg 1 month) to the `from` value in each range, eg:

```
from:            to:
2014-01-18      2014-02-18
2014-02-18      2014-03-18
2014-03-18      2014-04-18
```

instead of having to also manipulate the `to` value:

```
from:            to (inclusive):
2014-01-18      2014-02-17
2014-02-18      2014-03-17
2014-03-18      2014-04-17
```

This seems easier to me, especially when an inclusive range also has to deal with date rounding, eg do you mean `2014-04-17 00:00:00` or `2014-04-17 23:59:59.9999`?

Do you still think that inclusive upper bounds are a good idea?

Back in the day of facets, I remember that providing different comparators (eg `gt`, `gte` etc) was problematic for some reason.  Not sure if the same limitation applies to aggs.  (@jpountz?)
</comment><comment author="jpountz" created="2014-12-29T13:36:30Z" id="68257680">@clintongormley I'm not against supporting it, but one issue is that it would require to break the response format in order to indicate which bounds are inclusive or exclusive (preferably using the lt/lte/gt/gte notation to be consistent with filters)?
</comment><comment author="clintongormley" created="2015-02-28T05:05:55Z" id="76511012">@jpountz which means that the consuming client application would potentially have to check for `to`, `from`, `gt`, `gte`, `lt`, and `lte` keys.... 

@nickminutello i'm guessing you've worked around it, but is this still something that you (or others) want?
</comment><comment author="jpountz" created="2015-02-28T10:26:19Z" id="76520540">@clintongormley Agreed it would make things more complex on the client side.
</comment><comment author="nickminutello" created="2015-02-28T11:36:03Z" id="76522515">Sorry, @clintongormley I missed the message. 

In my case, my date range was constructed according business rules (exchange conventions + trading calendars plus rules on how to deal with holidays) so in my domain date range specifications were always taken mean inclusive of upper and lower bounds. 

So I had to hack the creation of the filter to +1 on the request (and then remember to -1 on the date in the results....)
</comment><comment author="jpountz" created="2015-08-26T14:47:31Z" id="135046112">Now that we have a `filters` aggregation, I'm tempted to just recommend it instead if someone needs different include/exclude behaviour than the range aggregation. Opinions?
</comment><comment author="clintongormley" created="2015-08-27T09:35:11Z" id="135364386">@jpountz would there be any difference in performance?
</comment><comment author="clintongormley" created="2015-10-14T14:49:11Z" id="148073558">Given the discussion above, I'm going to close this issue as won't fix
</comment><comment author="nickminutello" created="2015-10-14T14:52:37Z" id="148074443">Fine by me
</comment><comment author="djschny" created="2015-12-03T19:39:20Z" id="161757658">Adding @dakrone as he was in favor of the switch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Iteration on #5237 to add unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5248</link><project id="" key="" /><description>This pull request iterates on #5237 to add unit tests to terms and histogram aggregations when sorting on sub-aggregations that return NaN (eg. an average aggregation over an empty set).
</description><key id="28235176">5248</key><summary>Iteration on #5237 to add unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-02-25T10:01:34Z</created><updated>2014-07-16T21:48:09Z</updated><resolved>2014-03-04T08:54:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-27T16:09:33Z" id="36258154">@uboness I added randomization of the sub-aggregation, can you give it another look?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Implement BlendedTermQuery#extractTerms to support highlighing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5247</link><project id="" key="" /><description>some of the highlighters require term extraction to be implemented in
order to work. BlendedTermQuery doesn't implement the trivial extraction.

Closes #5246
</description><key id="28232635">5247</key><summary>Implement BlendedTermQuery#extractTerms to support highlighing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Highlighting</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T09:18:04Z</created><updated>2015-06-07T22:57:22Z</updated><resolved>2014-02-25T09:32:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-25T09:20:52Z" id="35988918">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>MultiMatchQuery fails to highlight with new cross field mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5246</link><project id="" key="" /><description>the new BlendedTermQuery doesn't implement:

```
@Override
public void extractTerms(Set&lt;Term&gt; terms) {
 //...
}
```

which causes an unsupported op exception to be thrown. This code has not yet been released.
</description><key id="28232198">5246</key><summary>MultiMatchQuery fails to highlight with new cross field mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T09:10:12Z</created><updated>2014-02-25T09:32:31Z</updated><resolved>2014-02-25T09:32:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queries/BlendedTermQuery.java</file><file>src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>src/main/java/org/elasticsearch/search/highlight/CustomQueryScorer.java</file><file>src/test/java/org/apache/lucene/queries/BlendedTermQueryTest.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Implement BlendedTermQuery#extractTerms to support highlighing.</comment></comments></commit></commits></item><item><title>Does not return stored field in nested object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5245</link><project id="" key="" /><description>If the field in nested object set `store : yes`, elasticsearch does not return stored data.
Otherwise, if the field in nested object set `store : no`, elasticsearch return stored data from _source.

Reproduce commands as follows.
- `book.title` field is  `store : no`.
- `book.contents` field is  `store : yes`.

```
PUT /library

POST /library/books/_mapping
{
  "books" : {
    "properties": {
      "book" : {
        "type": "nested",
        "properties": {
          "title" : { "type": "string", "analyzer": "standard", "store": "no"},
          "contents" : {"type": "string", "analyzer": "standard", "store": "yes"}
        }
      }
    }
  }
}

PUT /library/books/1
{
  "book" : {
    "title" : "ElasticSearch Server", 
    "contents" : "Learn the basics of ElasticSearch"
  }
}

GET /library/books/_search
{
  "_source" : ["book.title","book.contents"],
  "fields": [
    "book.title",
    "book.contents"
  ], 
  "query": {
    "nested": {
      "path": "book",
      "query": {
        "query_string" : {
          "query" : "ElasticSearch",
          "fields" : ["book.title", "book.contents"]
        }
      }
    }
  }
}
```

Result of the query.

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.625,
      "hits": [
         {
            "_index": "library",
            "_type": "books",
            "_id": "1",
            "_score": 0.625,
            "_source": {
               "book": {
                  "title": "ElasticSearch Server",
                  "contents": "Learn the basics of ElasticSearch"
               }
            },
            "fields": {
               "book.title": [
                  "ElasticSearch Server"
               ]
            }
         }
      ]
   }
}
```
</description><key id="28218031">5245</key><summary>Does not return stored field in nested object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels /><created>2014-02-25T02:33:58Z</created><updated>2014-09-06T13:19:55Z</updated><resolved>2014-03-03T13:31:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2014-02-25T06:04:38Z" id="35977868">I try to debug this problem.

The cause of this bug is the difference between two numbers.

Two numbers, `numStoredFields` and `fieldInfos` in visitDocument() method of o.a.l.codecs.compressing.CompressingStoredFieldReader.java.

https://github.com/apache/lucene-solr/blob/lucene_solr_4_6_1/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java#L341

CompressingStoredFieldReader has `fieldInfos` instance property.
The `fieldInfos` has 7 fields in my case.
But the `numStoredFields` is 2 in my case.
</comment><comment author="johtani" created="2014-02-25T07:56:25Z" id="35983169">I don't know why this difference occur, currently.
</comment><comment author="martijnvg" created="2014-02-26T11:44:04Z" id="36116760">@johtani The reason why you can't get the stored field values for a nested object is because it is stored in a separate Lucene document. 

If nested is enabled in the mapping a single ES document is stored as separate Lucene documents. Each nested object will be a single Lucene document. Also the main / root document will be a separate Lucene document. ES will always translates matches back to the root Lucene document. The _source is always associated with the root Lucene document. When fields are being fetched the translation to Lucene root document already has taken place, so accessing the nested stored fields isn't possible, but accessing the values from the _source will work.

I don't think this is a bug, but rather an limitation of how ES currently works with nested docs. Once #3022 has been implemented, accessing the stored fields of nested Lucene docs is possible. 
</comment><comment author="johtani" created="2014-02-26T23:54:07Z" id="36193748">@martijnvg Thanks detail information.

About this limitation, I think that it is useful if written on a document.

And I have another question.
The reason why I checked this case is because the highlight does not work in nested query.
I think that it is caused by the same reason, right?
</comment><comment author="martijnvg" created="2014-02-27T08:27:32Z" id="36220333">&gt; About this limitation, I think that it is useful if written on a document.

+1 make sense

Did you highlight on a stored field inside nested doc? Then that is caused by the same reason. I'd expect highlighting on the _source to work.
</comment><comment author="martijnvg" created="2014-03-03T13:31:32Z" id="36509797">Updated the docs about the mentioned limitation.
</comment><comment author="sekhar4233" created="2014-08-28T06:03:14Z" id="53677008">DELETE xyz

POST xyz

POST xyz/mail/_mapping
{
    "mail" : {
        "properties" : {
            "attachments" : {
                "type" : "nested",
                "include_in_parent": true,
                "properties": {
                      "file_name":{
                          "type":"string",
                          "index": "not_analyzed",
                          "store":true
                      },
                      "sharewith":{
                          "type":"string",
                          "index": "not_analyzed",
                          "store":true
                      },
                      "content":{
                          "type": "string",
                          "store":true
                      }
                 }
            },
            "from": {
                "type": "string",
                "index": "not_analyzed"
            },
            "body": {
                "type": "string"
            }
        }
    }
}

GET   xyz/mail/_mapping

POST  xyz/mail/1
{
  "from":"bala@mactores.com",
  "body":"this is simple mail from bala krishna heroor",
  "attachments":[
     { 
         "file_name":"1.txt",
         "sharewith":"sekhar4233@gmail.com",
         "content":"This is email from balakrishna heroor"
     },
     { 
         "file_name":"2.txt",
         "sharewith":"chetan@gmail.com",
         "content":"This is email from balakrishna heroor recived by sekhar reddy"
     }
  ]
}

POST abc/mail/_search
{
    "fields": [
       "attachments.file_name",
       "attachments.sharewith",
       "attachments.content",
       "from",
       "body"
    ], 
    "highlight": {

```
}, 
"query": {
    "nested": {
       "path": "attachments",
       "query": {
          "query_string": {
              "default_field": "attachments.content",
              "query": "sekhar"
           }
       }
    }
}
```

}

I am using nested types. The last query is not highlighting nothing. Can u please tell me anything wrong i did.
</comment><comment author="clintongormley" created="2014-09-06T13:19:37Z" id="54712079">@sekhar4233 highlighting is not currently supported on nested fields.  this workaround works, although it may highlight some nested docs which don't match the query:

```
DELETE xyz

POST xyz

POST xyz/mail/_mapping
{
  "properties": {
    "attachments": {
      "type": "nested",
      "include_in_parent": true,
      "properties": {
        "file_name": {
          "type": "string",
          "index": "not_analyzed"
        },
        "sharewith": {
          "type": "string",
          "index": "not_analyzed"
        },
        "content": {
          "type": "string"
        }
      }
    },
    "from": {
      "type": "string",
      "index": "not_analyzed"
    },
    "body": {
      "type": "string"
    }
  }
}

POST xyz/mail/1
{
  "from": "bala@mactores.com",
  "body": "this is simple mail from bala krishna heroor",
  "attachments": [
    {
      "file_name": "1.txt",
      "sharewith": "sekhar4233@gmail.com",
      "content": "This is email from balakrishna heroor"
    },
    {
      "file_name": "2.txt",
      "sharewith": "chetan@gmail.com",
      "content": "This is email from balakrishna heroor recived by sekhar reddy"
    }
  ]
}

POST xyz/mail/_search
{
  "highlight": {
    "highlight_query": {
      "match": {
        "attachments.content": "sekhar"
      }
    },
    "fields": {
      "attachments.content": {}
    }
  },
  "query": {
    "nested": {
      "path": "attachments",
      "query": {
        "query_string": {
          "default_field": "attachments.content",
          "query": "sekhar"
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve speed of running snapshot cancelation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5244</link><project id="" key="" /><description>Closes #5242
</description><key id="28217743">5244</key><summary>Improve speed of running snapshot cancelation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-25T02:27:01Z</created><updated>2015-06-07T22:57:50Z</updated><resolved>2014-03-11T00:54:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-02-26T06:35:22Z" id="36095445">LGTM, +1
</comment><comment author="spinscale" created="2014-03-07T12:13:25Z" id="37018841">LGTM, maybe change the commit comment, when and how this improves cancellation (like done in the issue)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Force single-segment merges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5243</link><project id="" key="" /><description>Although not recommended in general, the `optimize` API currently has two main use-cases:
- making read-only shards slightly more efficient by merging them into a single segment,
- reclaiming space by expunging deleted documents.

But forcing merges can also be useful to force a codec change, for example:
- if you want to make sure to not have Lucene 3.x segments anymore
- if you are using an unsupported codec and would like to switch back to the default one before an upgrade

However the optimize API won't do anything if there is already a single segment in the shard. So it would be useful to be able to force a merge, no matter how many segments there are in a shard.
</description><key id="28210343">5243</key><summary>Force single-segment merges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Index APIs</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-24T23:50:48Z</created><updated>2015-06-06T18:42:32Z</updated><resolved>2014-03-14T17:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-25T08:36:46Z" id="35985633">++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/index/engine/Engine.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/merge/policy/ElasticsearchMergePolicy.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/optimize/RestOptimizeAction.java</file><file>src/test/java/org/elasticsearch/common/lucene/uid/VersionsTests.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Add an option to force _optimize operations.</comment></comments></commit></commits></item><item><title>The delete snapshot operation on a running snapshot may take a long time on large shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5242</link><project id="" key="" /><description>The delete snapshot operation on a running snapshot should cancel the snapshot execution. However, it interrupts the snapshot only when currently running snapshot files are completely copied, which might take a long time for large files. 
</description><key id="28202520">5242</key><summary>The delete snapshot operation on a running snapshot may take a long time on large shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-24T22:16:21Z</created><updated>2014-03-12T13:34:42Z</updated><resolved>2014-03-11T00:54:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file></files><comments><comment>Improve speed of running snapshot cancelation</comment></comments></commit></commits></item><item><title>default distance now in meters; also adding documentation for the arcDis...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5241</link><project id="" key="" /><description>...tanceInMiles function
</description><key id="28199967">5241</key><summary>default distance now in meters; also adding documentation for the arcDis...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spra85</reporter><labels /><created>2014-02-24T21:44:31Z</created><updated>2014-06-19T07:54:06Z</updated><resolved>2014-05-06T12:16:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T12:16:37Z" id="42295089">Hi

Thanks for the PR.  This has already been fixed by 36219a1786c9a73ed6c595ff4f04b68bd6d455d8
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The get snapshot operation can take a long time when large shards are snapshotted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5240</link><project id="" key="" /><description>The get snapshot operation is using the same blob store thread pool to obtain snapshot information. If all thread in the pool are used to snapshot large shard segment files, it might appear that the get snapshot operation hangs. 
</description><key id="28199389">5240</key><summary>The get snapshot operation can take a long time when large shards are snapshotted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels /><created>2014-02-24T21:37:30Z</created><updated>2014-08-20T18:05:10Z</updated><resolved>2014-08-20T18:05:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-08-20T18:05:10Z" id="52817037">It was fixed by #5287.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recovery API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5239</link><project id="" key="" /><description>Adds a new API endpoint at /_recovery as well as to the Java API. The
recovery API allows one to see the recovery status of all shards in the
cluster. It will report on percent complete, recovery type, and which
files are copied.

Closes #4637
</description><key id="28186850">5239</key><summary>Recovery API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-02-24T19:03:45Z</created><updated>2014-10-21T23:41:19Z</updated><resolved>2014-03-07T00:14:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>RPM requires jre 1.6 even it 1.7 is installed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5238</link><project id="" key="" /><description>Hello,

if i want to install the RPM i have to install Java 1.6 even if 1.7 is already installed and the JAR file works with Java 1.7.

```
root@logstash01 ~]# /bin/rpm -i /opt/logstash/swdl/logstash-1.3.3-1_centos.noarch.rpm 
error: Failed dependencies:
    jre &gt;= 1.6.0 is needed by logstash-1.3.3-1_centos.noarch

root@logstash01 ~]# java -version
java version "1.7.0_40"
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)
```
</description><key id="28167205">5238</key><summary>RPM requires jre 1.6 even it 1.7 is installed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dwerder</reporter><labels /><created>2014-02-24T15:00:25Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2014-04-07T08:22:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-02-24T16:47:58Z" id="35906680">Hi,

In the 1.4.0 release this will be fixed. For dependencies we will have "java7-runtime-headless" or "java6-runtime-headless".
That should fix this issue.

if you want to test it out we got a Beta1 package that already should have this fix.
http://download.elasticsearch.org/logstash/logstash/packages/centos/logstash-1.4.0.beta1-1_centos.201402190242.126680a.noarch.rpm
</comment><comment author="dwerder" created="2014-02-24T21:02:08Z" id="35937000">```
[22:00:33 STG2 root@logstash01 src]# rpm -i logstash-1.4.0.beta1-1_centos.201402190242.126680a.noarch.rpm
error: Failed dependencies:
        jre &gt;= 1.6.0 is needed by logstash-1.4.0.beta1-1_centos.201402190242.126680a.noarch
```

Jave env installed via official Oracle JVM RPM for x86_64

```
[22:02:45 STG2 root@logstash01 src]# env | grep java
JRE_HOME=/usr/java/default/jre
PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/java/default/bin:/root/bin
JAVA_HOME=/usr/java/default
JDK_HOME=/usr/java/default

[22:03:16 STG2 root@logstash01 src]# which java
/usr/bin/java

[22:03:53 STG2 root@logstash01 src]# java -version
java version "1.7.0_40"
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)
```
</comment><comment author="jordansissel" created="2014-02-24T23:05:44Z" id="35951635">I think the problem here is that _rpm_ believes you do not have java installed. The data you have provided shows you clearly have java, but simply that rpm doesn't know about it.

So, why doesn't rpm think you have java? Did you install java without using yum/rpm? If so, that is the problem. If you do have java installed via rpm, who produced the package? What does the package provide?
</comment><comment author="dwerder" created="2014-02-25T05:43:21Z" id="35976714">Maybe the problem is, that I install JDK and not JRE?
Its the official Oracle JDK installed via "yum localinstall jdk-7u40-linux-x64.rpm"

```
 root@logstash01 ~]# rpm -qa | grep jdk
jdk-1.7.0_40-fcs.x86_64

 root@logstash01 ~]# yum info jdk-1.7.0_40-fcs.x86_64
Installed Packages
Name        : jdk
Arch        : x86_64
Epoch       : 2000
Version     : 1.7.0_40
Release     : fcs
Size        : 192 M
Repo        : installed
From repo   : /jdk-7u40-linux-x64
Summary     : Java Platform Standard Edition Development Kit
URL         : URL_REF
License     : http://java.com/license
Description : The Java Platform Standard Edition Development Kit (JDK) includes both
            : the runtime environment (Java virtual machine, the Java platform classes
            : and supporting files) and development tools (compilers, debuggers,
            : tool libraries and other tools).
            :
            : The JDK is a development environment for building applications, applets
            : and components that can be deployed with the Java Platform Standard
            : Edition Runtime Environment.

```
</comment><comment author="dwerder" created="2014-03-27T08:45:19Z" id="38779523">So RPM Package version logstash-1.4.0-1_c82dc09.noarch.rpm still does not accept Oracle JDK 1.7:

``` bash
rpm -i /opt/logstash/swdl/logstash-1.4.0-1_c82dc09.noarch.rpm 
error: Failed dependencies:
    jre &gt;= 1.6.0 is needed by logstash-1.4.0-1_c82dc09.noarch
```

And if  i install it with `--nodeps` , then i get tons of warnings like that:

```
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
warning: user jls does not exist - using root
warning: group jls does not exist - using root
```

And another question is why you named that package with that hasnumber instead of centos ( logstash-1.4.0-1_**c82dc09**.noarch.rpm )
</comment><comment author="loren" created="2014-04-04T17:45:51Z" id="39591917">It does appear that the issue is around JRE vs JDK. The RPM wants a JRE, but if you have the JDK installed it seems like it ought to be satisfied, no?

```
$  rpm -q --whatprovides jdk
jdk-1.7.0_45-fcs.x86_64
$  rpm -q --whatprovides jre
no package provides jre
```
</comment><comment author="dwerder" created="2014-04-04T21:06:21Z" id="39611451">The problem is discussed on other places as well. See this discussion: 

https://logstash.jira.com/browse/LOGSTASH-1020

https://groups.google.com/forum/#!topic/logstash-users/oiHMjjdS5Ks

It seems there is no easy way to solve this and the elasticsearch RPM just has no RPM requirement for JRE or JDK, because they run into the same issue.

So maybe the best way is just to remove this dependency if it is such a pain in the ass ;)
</comment><comment author="spinscale" created="2014-04-07T08:22:28Z" id="39705944">I will close this non-elasticsearch specific ticket here, as the logstash bugtracker already contains a pretty similar issue with https://logstash.jira.com/browse/LOGSTASH-1020
</comment><comment author="lookfirst" created="2014-04-19T00:19:14Z" id="40855240">https://github.com/elasticsearch/logstash/pull/1290
</comment><comment author="mythic2000" created="2016-01-03T01:39:44Z" id="168449241">Hi 
this issue can be solved.
step1 :down load jre-8uversion-linux-x64.rpm
step 2: rpm -ivh jre-8uversion-linux-x64.rpm

then install your rpm package
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changing the sorting for terms aggs,</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5237</link><project id="" key="" /><description>This pull request fixes the sorting of aggregations as discussed in issue #5236.

It forces the `NaN` values to be pushed to the bottom of the list, no matter if you are sorting asc/desc. This way top lists show the most relevant aggregates in the top. Also it actually does what the comments above it already said it should do.

Closes #5236.
</description><key id="28162201">5237</key><summary>Changing the sorting for terms aggs,</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thanodnl</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-24T13:48:20Z</created><updated>2015-06-07T22:59:38Z</updated><resolved>2014-03-04T08:54:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-24T13:56:23Z" id="35887928">The fix look good. Can you write a unit test for this? (If that looks complicated to you, I can take care of this, just let me know!)
</comment><comment author="thanodnl" created="2014-02-24T14:30:13Z" id="35890944">I've been looking into the test, but I find it hard to see where to add what :).

I found `StringTermsTests` (and my changes didn't break it) where some tests for terms aggregations are, including sorting on sub aggregate. But for this test I need to change the source dataset in a way I cannot really comprehense.

Would you mind adding a test?
</comment><comment author="jpountz" created="2014-02-24T14:43:30Z" id="35892256">Indeed, this would be easier to have a different test case.

&gt; Would you mind adding a test?

Sure, I will work on it!
</comment><comment author="uboness" created="2014-02-25T15:07:42Z" id="36016601">left one comment... other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms aggregations order wrong when sorting NaN's</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5236</link><project id="" key="" /><description>I have a strong believe there is an issue in the sorting of term aggregations.

Have a look [here](https://github.com/elasticsearch/elasticsearch/blob/ad8a482d19/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java#L215). If we look at the comment above it indicates that it would like to push NaN's to the bottom of the list (which would be the correct behaviour according to me). But when I test this out it does not work

Loading the following test data:

```
$ curl -XDELETE 'localhost:9200/sorting?pretty=true'
$ curl -XPOST 'localhost:9200/_bulk?pretty=true' -d '
{ index: { _index: "sorting", _type: "object" }}
{ t: "a", a:1, b:1 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "a", a:2, b:4 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "a", a:3, b:9 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "b", a:4, b:16 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "b", a:5, b:25, c: 42 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "b", a:6, b:36, c: 50 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "c", a:7, b:49 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "c", a:8, b:64 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "c", a:9, b:81 }
{ index: { _index: "sorting", _type: "object" }}
{ t: "c", a:10, c:100 }
'
```

You can start running some aggregations:

```
$ curl 'localhost:9200/sorting/_search?pretty=true' -d '
{
  "size": 0,
  "aggs": {
    "t": {
      "terms": {
        "field": "t",
        "order": {
          "aStats.min": "desc"
        }
      },
      "aggs": {
        "aStats": {
          "stats": {
            "field": "c"
          }
        }
      }
    }
  }
}
'
```

When we run this aggregations if turns out that a term without value's in the `c` field ends in the top.

```
{
  "aggregations": {
    "t": {
      "buckets": [
        {
          "key": "a",
          "doc_count": 3,
          "aStats": {
            "count": 0,
            "min": null,
            "max": null,
            "avg": null,
            "sum": null
          }
        },
        {
          "key": "c",
          "doc_count": 4,
          "aStats": {
            "count": 1,
            "min": 100,
            "max": 100,
            "avg": 100,
            "sum": 100
          }
        },
        {
          "key": "b",
          "doc_count": 3,
          "aStats": {
            "count": 2,
            "min": 42,
            "max": 50,
            "avg": 46,
            "sum": 92
          }
        }
      ]
    }
  }
}
```

When running the same analysis with facetting you would see that the term without the values in `c` are pushed to the bottom of the list

```
$ curl 'localhost:9200/sorting/_search?pretty=true' -d '
{
  "size": 0,
  "facets": {
    "t": {
      "terms_stats": {
        "key_field": "t",
        "value_field": "c",
        "order": "min"
      }
    }
  }
}
'
```

This looks to me as correct behaviour.

I have done some research on why this is happening, and in fact the if statement referenced above is comparing the aggregated metric to `Double.NaN`. In java [it turns out](http://stackoverflow.com/questions/8819738/why-does-double-nan-double-nan-return-false) that NaN is not equal to NaN :), luckily the guys working at java thought of this and added a function to check for NaN values `Double.isNaN`. Changing the line accordingly makes the return statement next work since it is skipped always at the moment. But...

On line [216](https://github.com/elasticsearch/elasticsearch/blob/ad8a482d19/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java#L216) it returns 1 or -1 depending on the ordering provided. This would result in NaN floating to the top of the list when ordering descending. Which has the strange effect that NaN's would be at the top of the list. My believe is that is should always `return 1` if `v1` is `NaN`.

Last part of the bug is that only `v1` is being checked to be `NaN`. You would also need to check `v2` for being `NaN` and `return -1`(!) if so. This would, as the comment suggest always push 'NaN' values to the bottom of the sorted list. This resembles the most to how facets sort at the moment.

Concrete effects of this bug is that we are not able to use aggregations for a table like view (which is the main benefit of using aggs, since you can get multiple columns at once) to show terms sorted descending on the avg of a sparse field we have in our collection of documents which was able when using facets.

Please have a look, and note that this error is made on two spots in the same file. ([second spot is on line 230](https://github.com/elasticsearch/elasticsearch/blob/ad8a482d19/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java#L230)).

I tested (not via the test suite, but by hand) out a fix locally and that seems works for us. If you would like to share my patch by opening a pull request I could, although I need to check my code against your guidelines, and add some automated tests :)
</description><key id="28159922">5236</key><summary>Terms aggregations order wrong when sorting NaN's</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">thanodnl</reporter><labels><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-24T13:09:53Z</created><updated>2014-03-21T10:04:28Z</updated><resolved>2014-03-04T08:53:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-24T13:34:42Z" id="35886181">Thanks for opening such a detailed bug report, your observation and the way you propose to fix this issue sound good to me so a pull request would be highly welcome!
</comment><comment author="thanodnl" created="2014-02-24T13:49:00Z" id="35887275">I opened the pull request, If you need me to change things just let me know.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java</file></files><comments><comment>Fix sorting of NaN values in terms aggregations.</comment></comments></commit></commits></item><item><title>feature request : aggregation reducer functions (aka buffer functions / window functions)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5235</link><project id="" key="" /><description>i would like to do group-by type aggregations e.g. given documents :

```
{name: "foo", location: "london", revenue: 100}
{name: "foo", location: "paris", revenue: 500}
{name: "bar", location: "sydney", revenue: 15}
{name: "bar", location: "new york", revenue: 23}
```

to produce output by grouping on name, collecting locations and summing revenues: 

```
{name: "foo", location: ["london","paris"], revenue: 600}
{name: "bar", location: ["sydney", "new york"], revenue: 38}
```

this is difficult with the currently available aggregations

to my naive eye it seems like it might be relatively easy to create a metric aggregation which operates very similarly to cascalog's defparallelagg https://github.com/nathanmarz/cascalog/wiki/Guide-to-custom-operations#wiki-aggregators

so the metric aggregation would define two scripts : one to initialise an aggregate container given a document, and another to combine aggregate containers

e.g.

```
{aggs:
  {name: 
    {terms: {field: "name", size: 0},
     aggs: 
       {location_revenue: 
         {reduce: {init: "{name: doc.name, 
                           location: [doc.location], 
                           revenue: doc.revenue}", 
                   combine: "{name: doc1.name, 
                              location: doc1.location.concat(doc2.location), 
                              revenue: doc1.revenue+doc2.revenue}"}}}}}}
```
</description><key id="28159623">5235</key><summary>feature request : aggregation reducer functions (aka buffer functions / window functions)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">mccraigmccraig</reporter><labels /><created>2014-02-24T13:03:52Z</created><updated>2014-02-24T21:15:47Z</updated><resolved>2014-02-24T21:15:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mccraigmccraig" created="2014-02-24T21:15:47Z" id="35938924">i must have been having a slow morning : clearly this query is quite doable with current aggregations

```
{aggs:
  {name: 
    {terms: {field: "name", size: 0},
     aggs: 
       {name: {terms: {field: "name"}}
        location: {terms: {field: "location"}}
        revenue: {sum: {field: "revenue"}}}}}}
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Init script should allow the user to investigate stdout and stderr if initializing fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5234</link><project id="" key="" /><description>I was trying to test some changes on a virtual machine that didn't have enough ram to start Elasticsearch in the configuration I had it set in but didn't see the error messages because the init script just died:

```
 * Starting ElasticSearch Server
```

Starting Elasticsearch manually using the same configuration spat out the helpful:

```
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x000000077ad30000, 2060255232, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 2060255232 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /tmp/hs_err_pid8570.log
```

It'd be nice to be able to read that message if it fails to start via the init script.
</description><key id="28159521">5234</key><summary>Init script should allow the user to investigate stdout and stderr if initializing fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-24T13:01:44Z</created><updated>2014-05-22T13:45:44Z</updated><resolved>2014-02-24T17:10:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-24T16:57:43Z" id="35907930">Hey Nik,

I  think this one is similar to #4429

Do you think it makes sense to write into an additional file in `/var/log` - not sure, if there are better alternatives, open for any feedback (and the need to add it to both packages).
</comment><comment author="nik9000" created="2014-02-24T17:10:08Z" id="35909421">/var/log/ is fine with me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove nanoTime in global cluster randomization.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5233</link><project id="" key="" /><description>Remove nanoTime in global cluster randomization in favor of deriving the seed from the main master seed.

The problem here is that if you don't give cluster's seed then test times
fluctuate oddly, even for a fixed -Dtests.seed=... This shouldn't be the
case -- ideally, the test ran with the same master seed should reproduce
pretty much with the same execution time (and internal logic, obviously).

From the code point of view "global" variables are indeed a problem
because JUnit has no notion of before-suite hooks. And RandomizedRunner
doesn't support context accesses in static class initializers (this is
intentional because there is no way to determine when such initializers
will be executed). A workaround is to move such static global variables to
lazily-initialized methods and invoke them (once) in @BeforeClass hooks.

I've changed the code slightly so that nanoTime is _not_ used if
tests.cluster_seed is not provided. Instead, tests.cluster_seed becomes
the current master seed. This will work both from IDEs and from Maven. I
felt tempted to remove the entire tests.cluster_seed layer (because any
static seed should/ could be derived from the master seed) but I don't
know what you guys are doing with it so I leave this decision to a
follow-up commit.
</description><key id="28159071">5233</key><summary>Remove nanoTime in global cluster randomization.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dweiss</reporter><labels><label>test</label><label>v0.90.13</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-24T12:54:11Z</created><updated>2014-07-13T18:27:45Z</updated><resolved>2014-02-26T08:49:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-24T13:05:49Z" id="35884179">Dawid, gooood catch! I think I messed that up. I agree we should get rid of the cluster seed and derive  from the master seed. Will you be able to add another commit doing this? I'd very much appreciate that!
</comment><comment author="dweiss" created="2014-02-24T13:06:58Z" id="35884243">Yeah, sure. Give me some time, I'll try to clean this up.
</comment><comment author="s1monw" created="2014-02-24T13:07:23Z" id="35884266">thanks so much! 
</comment><comment author="dweiss" created="2014-02-25T11:49:43Z" id="35999850">Ok, I think it's ready.
</comment><comment author="dweiss" created="2014-02-25T11:56:43Z" id="36000318">No. Wait, the tests failed. I'm digging.
</comment><comment author="dweiss" created="2014-02-25T12:47:04Z" id="36003678">No, it's fine -- I switched branches in between and hence the problem. I re-ran the tests and they passed.
</comment><comment author="s1monw" created="2014-02-26T08:49:55Z" id="36103539">I plulled this in yesterday! thanks dawid
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/junit/RestTestSuiteRunner.java</file></files><comments><comment>[TEST] Removed last occurences of cluster_seed, no longer used</comment></comments></commit></commits></item><item><title>Slow cluster startup with zen discovery.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5232</link><project id="" key="" /><description>When a cluster with a large number of nodes starts up, the joining of the nodes becomes slow, as the cluster state update is blocking. The master node adds the nodes one by one and waits after every join (zen-disco-receive) for the updated cluster state to be distributed.

This issue occurs in elasticsearch version 1.0 and is related to #3736 introducing the wait during the processing of the cluster state updates.

When setting discovery.zen.publish_timeout:0 the startup of the cluster is as fast as before, as the master node is not waiting for the individual updates to be acked by the client nodes.

A solution to the problem might be that the updates of the cluster state would be processed on the master and only distributed after all have been applied. Or that the master would not wait for the state to be acked by the client nodes during startup.
</description><key id="28154084">5232</key><summary>Slow cluster startup with zen discovery.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">miccon</reporter><labels /><created>2014-02-24T11:23:42Z</created><updated>2015-02-14T11:16:38Z</updated><resolved>2014-12-29T16:01:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bluelu" created="2014-02-27T13:01:41Z" id="36239213">Please note that this happens on an empty cluster with no indexes at all and slow means &gt; 30 minutes

More details are here:
https://groups.google.com/forum/#!topic/elasticsearch/zxHeqW932f8
</comment><comment author="dakrone" created="2014-07-04T14:20:46Z" id="48049431">@miccon @bluelu this should be helped by https://github.com/elasticsearch/elasticsearch/pull/6342 , are you still seeing this issue with Elasticsearch 1.2.1?
</comment><comment author="bluelu" created="2014-07-04T14:29:48Z" id="48050367">Thanks. We will have time at the end of next week to test this with a seperate installation of elasticsearch. We will let you know then.

As this is a production cluster, we can not upgrade right away and easily (also because of #6372 (delaying the start) and #6295)
</comment><comment author="kimchy" created="2014-07-04T14:33:32Z" id="48050638">@bluelu I suspect that this might happen because the join request was being to aggressive in its timeouts to wait for the cluster to ack the join, and got into continuous joining state. Now, the timeout is bigger (#6342), so I think it will help. If not, I would love to help in figuring out what takes too long.
</comment><comment author="bluelu" created="2014-07-04T14:43:27Z" id="48051347">Thanks. We will observe both when we test and will let you know. Still the issue with the traffic remains (#6295) (e.g. when you have more than 500 nodes and a few thousand indexes...).  Then also larger installations will run quite nicely :-)
</comment><comment author="avleen" created="2014-07-07T17:34:57Z" id="48210549">@dakrone We see this with 1.2.1.
I just set `discovery.zen.publish_timeout:0` and it's going much faster so far. Just restarted our cluster and 28 nodes joined in about 2 minutes rather than 10+ minutes. It does still slow down once it hit ~20-ish nodes in the cluster (the first ~20 joined really fast, now we're down to about 1 every 15 seconds).

Will test 1.3.0 when it lands.
</comment><comment author="clintongormley" created="2014-08-01T09:33:29Z" id="50866244">@avleen have you had a chance to test this on 1.3?
</comment><comment author="avleen" created="2014-08-04T03:03:05Z" id="51013651">Hey Clinton! We haven't upgraded to 1.3 yet. We plan on doing that in about
a week. I'll fill you in then!

On Fri, Aug 1, 2014 at 5:33 AM, Clinton Gormley notifications@github.com
wrote:

&gt; @avleen https://github.com/avleen have you had a chance to test this on
&gt; 1.3?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/5232#issuecomment-50866244
&gt; .
</comment><comment author="bluelu" created="2014-08-12T10:02:54Z" id="51895281">Unfortunately we haven't yet have time to update our cluster and test with the new version. We also tried this on a smaller cluster, and didn't have the issue that it was starting slowly, so we must test it with more servers.
@avleen, does the new version work for you? does it resolve the issue?
</comment><comment author="avleen" created="2014-08-12T14:07:08Z" id="51918330">We upgraded to 1.3.
We found the first ~20-25 nodes join the cluster really fast (maybe less
than 2 minutes).
After that it starts to get extremely slow again.

It starts to take 15 or 30 seconds for each node to join again. Not 15 to
30 seconds, 15 or 30 seconds.
The nodes seemed to join one at a time, and close to those intervals.
In the end a full cluster restart still takes us at least 20 minutes.
On Aug 12, 2014 6:03 AM, "Thibaut" notifications@github.com wrote:

&gt; Unfortunately we haven't yet have time to update our cluster and test with
&gt; the new version. We also tried this on a smaller cluster, and didn't have
&gt; the issue that it was starting slowly, so we must test it with more servers.
&gt; @avleen https://github.com/avleen, does the new version work for you?
&gt; does it resolve the issue?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/5232#issuecomment-51895281
&gt; .
</comment><comment author="avleen" created="2014-08-16T04:44:18Z" id="52383885">Further update.
Just did another full cluster restart.
It took 15 minutes for all 64 instances of elasticsearch to finish joining
the cluster.
After the server joins started slowing down, there is a very exact pause of
15 or 30 seconds between nodes joining.

Is there a 15s timeout here which might be coming into play?

On Tue, Aug 12, 2014 at 10:07 AM, Avleen Vig avleen@gmail.com wrote:

&gt; We upgraded to 1.3.
&gt; We found the first ~20-25 nodes join the cluster really fast (maybe less
&gt; than 2 minutes).
&gt; After that it starts to get extremely slow again.
&gt; 
&gt; It starts to take 15 or 30 seconds for each node to join again. Not 15 to
&gt; 30 seconds, 15 or 30 seconds.
&gt; The nodes seemed to join one at a time, and close to those intervals.
&gt; In the end a full cluster restart still takes us at least 20 minutes.
&gt; On Aug 12, 2014 6:03 AM, "Thibaut" notifications@github.com wrote:
&gt; 
&gt; &gt; Unfortunately we haven't yet have time to update our cluster and test
&gt; &gt; with the new version. We also tried this on a smaller cluster, and didn't
&gt; &gt; have the issue that it was starting slowly, so we must test it with more
&gt; &gt; servers.
&gt; &gt; @avleen https://github.com/avleen, does the new version work for you?
&gt; &gt; does it resolve the issue?
&gt; &gt; 
&gt; &gt; ## 
&gt; &gt; 
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; https://github.com/elasticsearch/elasticsearch/issues/5232#issuecomment-51895281
&gt; &gt; .
</comment><comment author="clintongormley" created="2014-08-18T09:42:06Z" id="52471012">/cc @kimchy 
</comment><comment author="kimchy" created="2014-09-07T11:34:18Z" id="54744227">improve_zen branch just landed in master/1.x, this includes a lot of improvements when it comes to forming a cluster. We ran 100s nodes test (with not data in the one I refer to, just to see how quickly a cluster is formed) and the results were very good (less than 30s to form 100s nodes clusters).

Even with improve_zen, the logic is still similar though, when a node joins, a full cluster state cycle is needed (more lightweight, batched, but still). Maybe the 30s come from the publish timeout, thats the one that can explain it, but I then don't understand why one of the nodes that are part of the cluster not answering in the proper time. Maybe next time you can set `discovery.zen.publish` logging to `DEBUG`, and see if we see messages like "timed out waiting for all nodes to process published".
</comment><comment author="clintongormley" created="2014-12-29T12:30:09Z" id="68253597">@miccon @avleen just pinging to hear about your experiences with cluster startup on v1.4.
</comment><comment author="bluelu" created="2014-12-29T15:58:29Z" id="68269215">I'm working together with @miccon. Except for the slow allocation of unallocated shards (https://github.com/elasticsearch/elasticsearch/issues/6372) , the joining of nodes is now very fast. This issue can be closed.
</comment><comment author="clintongormley" created="2014-12-29T16:01:44Z" id="68269506">thanks for letting us know @bluelu 
</comment><comment author="avleen" created="2015-02-14T03:35:43Z" id="74359717">This happened a while ago but we just upgraded to 1.4.2 and found that joining is pretty much instant now. Thanks everyone!
</comment><comment author="bleskes" created="2015-02-14T11:16:38Z" id="74371206">@avleen happy to hear.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_cache/clear docs lies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5231</link><project id="" key="" /><description>Docs say that it will clear all caches without args, but filter cache stays untouched:

```
web245 ~ # curl 'http://web245:9200/_cat/nodes?v&amp;h=host,hp,hm,fm,fcm'
host   hp       hm    fm   fcm
web245 75   11.9gb 3.9gb 1.4gb
web234 13 1011.2mb    0b    0b
web467 73   11.9gb 3.9gb 1.4gb

web245 ~ # curl 'http://web245:9200/_cache/clear'
{"_shards":{"total":1182,"successful":1182,"failed":0}}

web245 ~ # curl 'http://web245:9200/_cat/nodes?v&amp;h=host,hp,hm,fm,fcm'
host   hp       hm fm   fcm
web245 42   11.9gb 0b 1.4gb
web234 12 1011.2mb 0b    0b
web467 42   11.9gb 0b 1.4gb
```

Even after explicitly saying to clear filter cache it only gets cleared on one node:

```
web245 ~ # curl 'http://web245:9200/_cache/clear?filter'
{"_shards":{"total":1182,"successful":1182,"failed":0}}

web245 ~ # curl 'http://web245:9200/_cat/nodes?v&amp;h=host,hp,hm,fm,fcm'
host   hp       hm fm   fcm
web245 41   11.9gb 0b    0b
web234 10 1011.2mb 0b    0b
web467 42   11.9gb 0b 1.4gb
```

After a minute filter cache was cleared, though.
</description><key id="28151219">5231</key><summary>_cache/clear docs lies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-02-24T10:31:33Z</created><updated>2014-02-24T10:38:09Z</updated><resolved>2014-02-24T10:38:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-02-24T10:36:36Z" id="35874228">Hiya

The filter cache is not cleared immediately, but it is scheduled to be cleared. I've updated the docs to reflect this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Added note about dely in clearing filter cache.</comment></comments></commit></commits></item><item><title>Fields not correctly returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5230</link><project id="" key="" /><description>Ever field is in Version 1.0 wrapped into an array. Existing queries fails in our system.
Retrieved from a batch file using curl localhost:9200/appdef/_search?q=*&amp;fields=type,pattern

PS: The changes in ES 1.0 drive me crazy as we are close to a release and testing with latest ES version has many surprises. Things working since several month start to crash ...

Result looks like this: 
{
      "_index" : "appdef",
      "_type" : "appPattern",
      "_id" : "wbcVEQ7hSGqAAmFQW-SwDg",
      "_score" : null,
      "fields" : {
        "type" : [ "c" ],
        "pattern" : [ "Apple.*Stocks" ],
}
</description><key id="28124167">5230</key><summary>Fields not correctly returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seti123</reporter><labels /><created>2014-02-23T18:47:23Z</created><updated>2014-02-24T08:37:29Z</updated><resolved>2014-02-24T08:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-24T08:37:29Z" id="35866398">Hi @seti123 , 
what you see is the result of #4542 , which is listed among the [breaking changes](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_return_values.html) that cam e with `1.0`.

As stated in the breaking change explanation: `fields` options should always be used to retrieve fields that have been explicitly stored in lucene (`store`:`yes` in the mapping). If you need to extract specific fields from the `_source`, you can use [source filtering](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-source-filtering.html) which was added exactly with this goal in mind and is way more flexible as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title> expand_wildcards not accepted by /_cluster/state as advertised</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5229</link><project id="" key="" /><description>As per documentation (0) all APIs that support multi-index also support expand_wildcards parameter, that is not the case with at least /_cluster/state:

```
curl -X PUT localhost:9200/test_index
{"acknowledged":true}
curl -X POST localhost:9200/test_index/_close
{"acknowledged":true}
curl localhost:9200/*/_settings?expand_wildcards=closed
{"test_index":{"settings":{"index":{"uuid":"CkMTVYfhS12TmHrWrNSkfQ","number_of_replicas":"1","number_of_shards":"5","version":{"created":"2000099"}}}}}
curl localhost:9200/_cluster/state/metadata/*?expand_wildcards=closed
{"cluster_name":"es_client_test","metadata":{"templates":{},"indices":{}}}
```

0 - http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/multi-index.html
</description><key id="28118945">5229</key><summary> expand_wildcards not accepted by /_cluster/state as advertised</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-02-23T13:58:40Z</created><updated>2015-01-22T16:19:59Z</updated><resolved>2015-01-22T16:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-01-12T17:39:10Z" id="69610503">Since I just looked at IndicesOptions in another issue and start to get to know the rest apis a little bit I can look into this.

So far I think the ClusterStateRequest has no way to set IndicesOptions other than the default one (here LENIENT_EXPAND_OPEN) which does not expand wildcards. I could add overwritting these parameters with the request parameters like in e.g RestIndicesStatsAction, but for this I would need to introduce setter and internal field for indicesOptions in ClusterStateRequest. 

Most of the index-related actions inherit this from BroadcastOperationRequest, but since the ClusterStateRequest extends MasterNodeReadOperationRequest, theres no such setter / internal field here. Wondering where to put it. If the "expand_wildcard" parameter is supposed to work for all Cluster-related commands, then I think it should be somewhere higher up in the hierarchy. 
</comment><comment author="cbuescher" created="2015-01-12T18:21:56Z" id="69617895">I just saw that ClusterSearchShardsRequest is doing the same thing I would do in  ClusterStateRequest:

```
private IndicesOptions indicesOptions = IndicesOptions.lenientExpandOpen();
public ClusterSearchShardsRequest indicesOptions(IndicesOptions indicesOptions) 
```
</comment><comment author="javanna" created="2015-01-13T08:57:15Z" id="69713456">Hey @cbuescher what you proposed sounds good to me, let's add the ability to set `IndicesOptions` to `ClusterStateRequest`.
</comment><comment author="cbuescher" created="2015-01-14T09:16:13Z" id="69888580">From reading the documentation above on multi-index parameters I guess that also "ignore_unavailable" and "allow_no_indices" should work here. Do I just add them to the cluster.state.json and write some test for them or does that make no sense here?
</comment><comment author="javanna" created="2015-01-14T09:25:02Z" id="69889498">@cbuescher yes you got it right, those params need to be added to the spec and some tests for them would be appreciated ;)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestTest.java</file><file>src/test/java/org/elasticsearch/bwcompat/ClusterStateBackwardsCompat.java</file><file>src/test/java/org/elasticsearch/cluster/SimpleClusterStateTests.java</file></files><comments><comment>Rest: Adding support of multi-index query parameters for _cluster/state</comment></comments></commit></commits></item><item><title>Minor improvements to Table class and add tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5228</link><project id="" key="" /><description>This pull request includes the following changes:
- Replace `ElasticsearchIllegalArgumentException` with `ElasticsearchIllegalStateException` which seems more appropriate.
- Add a few more preliminary checks to some methods. Some may be too pedantic and may not worth it. Feel free to remove those
  if you think the little overhead is not justified.
- Add unit tests for the `Table` class.
</description><key id="28100991">5228</key><summary>Minor improvements to Table class and add tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2014-02-22T18:22:01Z</created><updated>2014-07-16T21:48:12Z</updated><resolved>2014-03-18T22:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-03-18T14:31:15Z" id="37938900">This looks good to me. Going to merge to 1.0, 1.1, and master.
</comment><comment author="jpountz" created="2014-03-18T14:34:47Z" id="37939331">+1
</comment><comment author="drewr" created="2014-03-18T22:30:01Z" id="37997013">Merged f38d6f8a1 (master), ad876960db (1.x), 2589c5cd3 (1.0)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregatation calculating wrong average value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5227</link><project id="" key="" /><description>The desire average would be 38/4 = 9.5.

POST /sports/athlete/13
{"name":"john","rating": ["8", "10"]}
POST /sports/athlete/14
{"name":"john","rating": ["10", "10"]}

POST /sports/athlete/_search
{
   "size": 0, 
   "aggregations": {
      "the_name": {
         "terms": {
            "field": "name",
            "order": {
               "rating_avg": "desc"
            }
         },
         "aggregations": {
            "rating_avg": {
               "avg": {
                  "field": "rating"
               }
            }
         }
      }
   }
}

However the query return a value of 9.3333, which is 28/3. The duplicated value in a doc has been ignored.
</description><key id="28099602">5227</key><summary>Aggregatation calculating wrong average value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lordaugustus</reporter><labels /><created>2014-02-22T17:08:19Z</created><updated>2014-02-23T12:12:16Z</updated><resolved>2014-02-22T23:21:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-22T23:21:44Z" id="35818491">Indeed, field data only stores unique values per document per field, so the second John would have a single rating associated with it: `[ "10" ]`. One way to work around this limitation would be to have one document per rating, so instead of having two johns which have 2 ratings each, you would have 4 johns with 1 rating each.
</comment><comment author="lordaugustus" created="2014-02-23T05:12:32Z" id="35824176">Is it consider a bug or is there design consideration behind this? 
</comment><comment author="jpountz" created="2014-02-23T12:12:16Z" id="35830225">I wouldn't call it a bug, but this is indeed a limitation of field data. The contract on field data is that it stores the set of unique values per document and in order. We may add support for returning frequencies in the future in order to solve the issue that you described but this would be quite a large change.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query under nested query do not have the right scope</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5226</link><project id="" key="" /><description>The following is the example query

put /test2/type1/_mapping
{
    "type1" : {
        "properties" : {
            "obj1" : {
                "type" : "nested"
            }
        }
    }
}
put /test2/type1/1
{
    "obj1":[
        {
            "name":"blue",
            "message":"Hello World"  
        },
         {
            "name":"red",
            "message":"go elsewhere"  
        }
    ]
}

get /test2/type1/_search
{
    "nested" : {
        "path" : "obj1",
        "query" : {
            "bool" : {
                "must" : [
                    {
                        "match" : {"obj1.name" : "red"}
                    },
                    {
                        "query" : {
                            "query_string": {
                                "fields" : "obj1.message"
                                "query" : "Hello"

```
                    }
                }

            ]
        }
    }
}
```

}

This should not return a result as "red" and "Hello Word" is within different nested document. 
I think the problem is due to query act on a global path like "obj1.message", which points to multiple nested document, instead of the documents that the nested document the sibling query(in this case, match name:red) is currently acting on.
</description><key id="28090937">5226</key><summary>Query under nested query do not have the right scope</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lordaugustus</reporter><labels /><created>2014-02-22T06:51:34Z</created><updated>2014-02-22T07:21:21Z</updated><resolved>2014-02-22T07:21:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Make sure get field mapping request is executed on node hosting the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5225</link><project id="" key="" /><description>PR for #5177
</description><key id="28070280">5225</key><summary>Make sure get field mapping request is executed on node hosting the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T20:25:36Z</created><updated>2015-06-07T23:18:17Z</updated><resolved>2014-02-24T15:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-24T13:32:43Z" id="35886032">this PR LGTM I think you should push it and add some lines to the issue that this might barf during rolling upgrades.
</comment><comment author="martijnvg" created="2014-02-24T15:42:36Z" id="35898806">Merged in.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create empty buckets in date_/histogram aggregation at the edges, beyond the value space of the data </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5224</link><project id="" key="" /><description>This issue exists for a long time since the date_histogram facet, it can be circumvented in some situations with `min_doc_count` but it don't work in all cases.

ElasticSearch should fill the gaps for filter/query ranges or at least, add an option on date_histogram to enable this.
</description><key id="28064844">5224</key><summary>Create empty buckets in date_/histogram aggregation at the edges, beyond the value space of the data </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">greenboxal</reporter><labels><label>enhancement</label><label>feature</label><label>v1.1.0</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T19:09:05Z</created><updated>2014-03-29T21:01:18Z</updated><resolved>2014-03-20T14:53:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-02-21T19:44:38Z" id="35765893">&gt; This issue exists for a long time since the date_histogram facet, it can be circumvented in some situations with min_doc_count but it don't work in all cases.

Can you be more specific about the cases where `min_doc_count` is not enough?

&gt; ElasticSearch should fill the gaps for filter/query ranges or at least, add an option on date_histogram to enable this.

sorry not following you... what do you mean by "filter/query ranges"?
</comment><comment author="uboness" created="2014-02-21T21:23:48Z" id="35775116">do you refer here to the use case where where the queries/filters you define span beyond the existing data? Or in other words, have the ability to compute empty buckets on the edges?
</comment><comment author="greenboxal" created="2014-02-21T22:22:34Z" id="35780122">Exactly.
</comment><comment author="uboness" created="2014-02-21T23:51:45Z" id="35786038">gotcha... ok.. well, it's something we're definitely considering adding. I'll keep this ticket open just rename it a bit to give it the right context.

Thx
</comment><comment author="mattweber" created="2014-03-20T18:40:28Z" id="38205279">This is great!  Thanks!
</comment><comment author="greenboxal" created="2014-03-20T18:46:24Z" id="38205980">Good work :D
</comment><comment author="jxstanford" created="2014-03-20T18:53:49Z" id="38206836">+1

John

jxstanford@gmail.com
@jxstanford

On Mar 20, 2014, at 11:40:55, Matt Weber notifications@github.com wrote:

&gt; This is great! Thanks!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file></files><comments><comment>Added extended_bounds support for date_/histogram aggs</comment></comments></commit></commits></item><item><title>Made SearchContextHighlight.Field class immutable to prevent from unwanted updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5223</link><project id="" key="" /><description>A Field instance can map to multiple actual fields when using wildcard expressions. Each actual field should use the proper highlighter depending on the available data structure (e.g. term_vectors), while we currently select the highlighter for the first field and we keep using the same for all the fields that match the wildcard expression.

Modified also how the PercolateContext sets the forceSource option, in a global manner now rather than per field.

Closes #5175
</description><key id="28064294">5223</key><summary>Made SearchContextHighlight.Field class immutable to prevent from unwanted updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T19:01:26Z</created><updated>2015-06-07T23:19:12Z</updated><resolved>2014-02-24T23:27:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-02-24T09:50:22Z" id="35870921">LGTM, left one comment
</comment><comment author="jpountz" created="2014-02-24T10:37:23Z" id="35874279">This looks good to me overall. However, one thing that I find a bit error-prone are all the `globalForceSource() || field.forceSource()` checks. Instead of just setting a boolean, maybe `SearchContextHighlight.globalForceSource()` should return a copy of the current instance where all fields have `forceSource=true`?
</comment><comment author="jpountz" created="2014-02-24T13:12:30Z" id="35884612">+1 to push!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Be less verbose logging ClusterInfoUpdateJob failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5222</link><project id="" key="" /><description /><key id="28063283">5222</key><summary>Be less verbose logging ClusterInfoUpdateJob failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T18:45:59Z</created><updated>2015-06-08T15:22:06Z</updated><resolved>2014-03-27T19:50:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-24T10:54:17Z" id="35875454">This patch results in no chance (not even with another loglevel) to access the stack trace anymore. Is that ok for this case?
</comment><comment author="dakrone" created="2014-02-26T15:52:57Z" id="36140113">@spinscale good idea, I decided to change this to log the full exception if trace logging is enabled, at the trace level.
</comment><comment author="kimchy" created="2014-04-04T18:41:20Z" id="39597390">can we re-open this? I think its cleaner to warn log the full exception, bug if the exception is a cluster block exception (for example, during startup), simply trace log it.
</comment><comment author="dakrone" created="2014-04-04T18:59:27Z" id="39599307">@kimchy sure, I'll make that change.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java</file></files><comments><comment>Be less verbose logging ClusterInfoUpdateJob failures</comment></comments></commit></commits></item><item><title>Highlighting on a wildcard field name causes the wildcard expression to be returned rather than the actual field name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5221</link><project id="" key="" /><description>Wildcards expressions are supported to specify which fields need to be highlighted. When using a wildcard expression, in some cases the highlighted fragments generated by the fast vector highlighter contain the wildcard expression as field name instead of the actual field name.

This happens also with the other when returning errors like:
- "the field [field*] should be indexed with positions and offsets in the postings list to be used with postings highlighter"
- "the field [field*] should be indexed with term vector with position offsets to be used with fast vector highlighter"
- "source is forced for field [field*] but type [type1] has disabled _source"

In all the above cases, the actual field name should be returned (e.g. `field1` rather than `field*`)

Relates to #5175 .
</description><key id="28059783">5221</key><summary>Highlighting on a wildcard field name causes the wildcard expression to be returned rather than the actual field name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T17:53:40Z</created><updated>2015-06-07T23:03:26Z</updated><resolved>2014-02-21T17:56:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Fixed field names returned when using wildcard expression to specify fields to highlight</comment></comments></commit></commits></item><item><title>forceSource highlighting field option doesn't have any effect when set using the Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5220</link><project id="" key="" /><description>The `forceSource` highlighting option can either be specified for all the fields or for specific fields. However, changing the per-field option through Java API doesn't have any effect.
</description><key id="28054889">5220</key><summary>forceSource highlighting field option doesn't have any effect when set using the Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T16:45:38Z</created><updated>2015-06-07T23:09:38Z</updated><resolved>2014-02-21T17:03:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Fixed per-field forceSource highlighting option</comment></comments></commit></commits></item><item><title>TTL doesn't work when indexing documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5219</link><project id="" key="" /><description>I intend to set TTL when indexing a document, I tried both in Java API (setTTL() method) and in REST API, but it doesn't take effect. While It works when defining _ttl in type mapping.
### 1. index with ttl

```
$ curl -XPUT 'localhost:9200/test/beijing/1?_ttl=5000' -d '{"name": "beijing"}' 
```

I still can get the document after 2mins, 5mins:

```
$ curl -XGET 'localhost:9200/test/beijing/1?pretty
```
### 2. _ttl in mapping

```
curl -XPUT 'localhost:9200/test/tianjin/_mapping?pretty' -d '
{
"tianjin": {
    "properties": {
        "name": {"type": "string"}
    },
        "_ttl": {"enabled": true, "default": 5000}
  }
}'

$ curl -XPUT 'localhost:9200/test/tianjin/1' -d '{"name": "tianjin"}'
after about 2mins, I cannot get the document any more: 
curl -XGET 'localhost:9200/test/tianjin/1?pretty'
{
  "_index" : "test",
  "_type" : "tianjin",
  "_id" : "1",
  "exists" : false
}
```
</description><key id="28053166">5219</key><summary>TTL doesn't work when indexing documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkcoder</reporter><labels /><created>2014-02-21T16:22:43Z</created><updated>2014-02-22T00:33:40Z</updated><resolved>2014-02-22T00:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2014-02-21T17:19:12Z" id="35751876">@nkcoder for TTL to work you have to explicitly enable _ttl in mapping like you did in 2.

It is better to ask this kind of questions in the mailing list (I think you did) and to open an issue here only if it is confirmed to be a bug there.
</comment><comment author="nkcoder" created="2014-02-22T00:33:40Z" id="35788278">@Paikan thank you for your answer and friendly advice. I got it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo: Geo bounding box filter returning no results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5218</link><project id="" key="" /><description>My search:

``` json
{
  "filter": {
    "geo_bounding_box": {
      "location": {
        "top_left": {
          "lat": 90,
          "lon": -180
        },
        "bottom_right": {
          "lat": -90,
          "lon": 180
        }
      }
    }
  }
}
```

It returns no results.

If I change a value, it works.
</description><key id="28050389">5218</key><summary>Geo: Geo bounding box filter returning no results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">pierrre</reporter><labels><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T15:45:22Z</created><updated>2014-09-12T09:19:51Z</updated><resolved>2014-09-12T09:17:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-21T15:47:55Z" id="35742375">Haha! So basically you don't want to filter anything, right?
I guess we should in that case detect that and don't apply the filter at all.

@chilling WDYT?
</comment><comment author="pierrre" created="2014-02-21T15:50:04Z" id="35742573">Yes, I don't want to filter anything.
But I expect to get results.

If I use 90/-180 &amp; -90/179.9999 , it works.
</comment><comment author="exAspArk" created="2014-08-18T13:30:06Z" id="52491558">same problem, it works if I have longitude `179.9999` or `-179.9999`
</comment><comment author="clintongormley" created="2014-08-18T13:37:36Z" id="52492485">Validation of geo-points is turning this filter into:

```
GeoBoundingBoxFilter(loc, [90.0, 180.0], [-90.0, 180.0])
```

This probably isn't the right thing to do in this case, but using a geo-filter in this way is really bad for performance.  You don't want to use this as a "match-all" query
</comment><comment author="clintongormley" created="2014-08-18T13:37:52Z" id="52492522">@colings86 what do you think?
</comment><comment author="colings86" created="2014-08-19T15:56:45Z" id="52655687">A workaround for this bug is to set the normalize flag to false, making the original request in this issue:

```
{
  "filter": {
    "geo_bounding_box": {
      "normalize": false,
      "location": {
        "top_left": {
          "lat": 90,
          "lon": -180
        },
        "bottom_right": {
          "lat": -90,
          "lon": 180
        }
      }
    }
  }
}
```

This will stop the -180 being converted to +180 but will require the calling code to ensure the top and bottom are within [-90, 90] and the left and right are within [-180,180]
</comment><comment author="colings86" created="2014-09-12T09:17:07Z" id="55379232">Closed by https://github.com/elasticsearch/elasticsearch/pull/7340
</comment><comment author="pierrre" created="2014-09-12T09:19:51Z" id="55379478">Thanks! :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recycler: better lifecycle control for pooled instances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5217</link><project id="" key="" /><description> Recycler: better lifecycle control for pooled instances

Changes &amp; additional test for all changes. Successfully ran full test suite in both eclipse and mvn. 

Closes #5214
</description><key id="28048838">5217</key><summary>Recycler: better lifecycle control for pooled instances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T15:23:47Z</created><updated>2015-06-07T15:14:44Z</updated><resolved>2014-02-25T07:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-02-24T11:22:58Z" id="35877223">As requested the changeset now adds the abstract superclass (to provide a no-op impl of destroy()). I've also added calling destroy() on release() when the Recycler is over capacity and reduced copy&amp;paste in the test.
</comment><comment author="hhoffstaette" created="2014-02-25T07:54:24Z" id="35983048">Pushed to master/1.x in https://github.com/elasticsearch/elasticsearch/commit/bac09e292697be62581f521015cbaebf3ce68104
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixes two minor typos.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5216</link><project id="" key="" /><description>Fixes to minor issues Aggregations' docs.
</description><key id="28045663">5216</key><summary>Fixes two minor typos.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">tmacam</reporter><labels /><created>2014-02-21T14:38:30Z</created><updated>2014-07-11T17:40:27Z</updated><resolved>2014-04-07T11:02:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T11:02:17Z" id="39717656">Those two typos got fixed in the meantime, thanks anyway @tmacam!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Document the indices segments response format.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5215</link><project id="" key="" /><description>In particular it specifies that `memory_in_bytes` might return -1 (see https://github.com/elasticsearch/elasticsearch/issues/5201). I didn't want to give details why we might return -1 since the reason why may change in the future, but feel free to let me know if we should do so and I'll fix.
</description><key id="28034242">5215</key><summary>Document the indices segments response format.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T11:01:05Z</created><updated>2014-06-27T23:28:47Z</updated><resolved>2014-02-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-21T11:10:02Z" id="35721436">LGTM should go  into all branches IMO
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recycler: better lifecycle control for pooled instances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5214</link><project id="" key="" /><description>The default DequeueRecycler.close() currently simply clears its internal queue and drops all instances on the floor for GC. During work for issue #5159 I found that this can be improved:
- closing a recycler should properly dispose of the pooled instances via the factory
- the factory currently conflates the functionality of per-instance "clean up &amp; return to pool" and "destroy instance" behind the (according to Adrien badly named) cleanup() callback method.

So I suggest that we:
- rename Recycler.C.clear(T) to recycle(T)
- add a Recycler.C.destroy(T) to indicate that this particular instance needs to die
- destroy() can then also be used when the recycler wants to kill off excess instances (on return).

While we're renaming things I'd also like to vote to rename the Recycler.C and Recycler.V interfaces to something more descriptive. :)
</description><key id="28023505">5214</key><summary>Recycler: better lifecycle control for pooled instances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T07:30:40Z</created><updated>2015-06-07T15:20:34Z</updated><resolved>2014-02-25T07:59:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-21T12:13:08Z" id="35725570">+1
</comment><comment author="hhoffstaette" created="2014-02-25T07:59:18Z" id="35983342">Fixed in https://github.com/elasticsearch/elasticsearch/commit/bac09e292697be62581f521015cbaebf3ce68104
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cache/recycler/CacheRecycler.java</file><file>src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/AbstractRecyclerC.java</file><file>src/main/java/org/elasticsearch/common/recycler/DequeRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/NoneRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/Recycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/Recyclers.java</file><file>src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java</file><file>src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/ConcurrentRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/LockedRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/NoneRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/QueueRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/SoftConcurrentRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/SoftThreadLocalRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/ThreadLocalRecyclerTests.java</file></files><comments><comment>Recycler: better lifecycle control for pooled instances (#5214)</comment></comments></commit></commits></item><item><title>Open correct (renamed) index on restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5213</link><project id="" key="" /><description>Closes #5212
</description><key id="28014066">5213</key><summary>Open correct (renamed) index on restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label></labels><created>2014-02-21T02:14:47Z</created><updated>2015-06-07T23:10:02Z</updated><resolved>2014-02-23T00:19:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-21T08:27:55Z" id="35707874">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restore of an existing index using rename doesn't completly open the index after restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5212</link><project id="" key="" /><description>To reproduce: 
- snapshot an index A
- create index B with the same number of shards as A
- close index B
- restore A while renaming it to B
- search index B

Observed behavior:
- the search fails with `ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]` error

Expected behavior:
- the search should work
</description><key id="28013886">5212</key><summary>Restore of an existing index using rename doesn't completly open the index after restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T02:09:54Z</created><updated>2015-08-13T15:30:21Z</updated><resolved>2014-02-23T00:18:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Open correct (renamed) index on restore</comment></comments></commit></commits></item><item><title>Restore process should replace the mapping and settings if index already exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5211</link><project id="" key="" /><description>... exists

Closes #5210
</description><key id="28013798">5211</key><summary>Restore process should replace the mapping and settings if index already exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T02:07:25Z</created><updated>2015-06-07T23:10:36Z</updated><resolved>2014-02-23T00:19:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-21T08:28:52Z" id="35707925">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Restore of an existing index doesn’t restore mappings and settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5210</link><project id="" key="" /><description>To reproduce: 
- snapshot an index
- modify mappings and settings of the index
- close the index
- restore the index

Observed behavior:
- the mappings and settings are not reverted back to the original (snapshotted) state

Expected behavior:
- the mappings and settings should match the state in the snapshot
</description><key id="28012877">5210</key><summary>Restore of an existing index doesn’t restore mappings and settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-21T01:43:35Z</created><updated>2014-04-25T08:07:18Z</updated><resolved>2014-02-23T00:18:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Restore process should replace the mapping and settings if index already exists</comment></comments></commit></commits></item><item><title>GetFieldMapping API is not available in ES 1.0.0 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5209</link><project id="" key="" /><description>We just installed ES 1.0.0 but the GetFieldMapping API always returned empty response.

http://localhost:9200/index/doc/_mapping/field/*
always responded {} 

This problem is causing Kibana 3 milestone 5 to display the following error:

No index found at https://server/logstash-2014.02.20/_mapping/field/*
Please create at least one index.If you're using a proxy ensure it is configured correctly.
</description><key id="28007544">5209</key><summary>GetFieldMapping API is not available in ES 1.0.0 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">namdle</reporter><labels /><created>2014-02-20T23:51:52Z</created><updated>2014-12-29T12:22:08Z</updated><resolved>2014-12-29T12:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-24T08:41:56Z" id="35866636">I think you're hitting #5177 . How many nodes and shards do you have? Do you use dedicated master nodes?
</comment><comment author="namdle" created="2014-02-24T16:41:13Z" id="35905875">Our cluster has 3 nodes: one dedicated master (no data) and two data nodes. This problem happened on any indices regardless of number of shards. We tried curling /_mapping/field/\* on indices with 1, 2, and 5 shards. We called the API on each node of the cluster. All of them returned {}. 

ElasticSearch info:

{
"status": 200,
"name": "devops",
"version": {
"number": "1.0.0",
"build_hash": "a46900e9c72c0a623d71b54016357d5f94c8ea32",
"build_timestamp": "2014-02-12T16:18:34Z",
"build_snapshot": false,
"lucene_version": "4.6"
},
"tagline": "You Know, for Search"
}
</comment><comment author="bleskes" created="2014-03-03T13:03:28Z" id="36507922">It really sounds like #5177 is causing this problem. Can you upgrade to 1.0.1 and check it was solved?
</comment><comment author="clintongormley" created="2014-12-29T12:22:08Z" id="68253155">No more info. Assuming fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Refactor SimpleQueryParser settings into separate Settings class, add "lenient" option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5208</link><project id="" key="" /><description>Fixes #5011
</description><key id="28004774">5208</key><summary>Refactor SimpleQueryParser settings into separate Settings class, add "lenient" option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T23:06:07Z</created><updated>2015-06-07T15:15:18Z</updated><resolved>2014-02-26T18:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-26T17:36:15Z" id="36152514">LGTM
</comment><comment author="dakrone" created="2014-02-26T18:05:36Z" id="36156175">Merged in 5429019920b94ed2b3463b987fe1109513273df1
</comment><comment author="clintongormley" created="2014-08-18T11:42:14Z" id="52481337">@dakrone Docs missing for this change
</comment><comment author="dakrone" created="2014-08-18T13:56:51Z" id="52495097">@clintongormley pushed 99b0faed14b9301d6cd2a3abe2fa847e8a335c44 and a46cf503bea83d04d532bd7e595fbcf75cfd8828 to document this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix possible exception in toCamelCase method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5207</link><project id="" key="" /><description /><key id="28003457">5207</key><summary>Fix possible exception in toCamelCase method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">Paikan</reporter><labels><label>:REST</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T22:47:12Z</created><updated>2015-06-07T23:10:57Z</updated><resolved>2014-02-20T23:24:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-20T23:24:34Z" id="35682055">Merged, thanks!
</comment><comment author="s1monw" created="2014-02-21T08:37:57Z" id="35708419">@jpountz this should also go into `0.90` no? I added the tags for the branches were it was commited already...
</comment><comment author="jpountz" created="2014-02-21T11:24:42Z" id="35722660">Pushed to 0.90 as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove useless URL instantiation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5206</link><project id="" key="" /><description /><key id="28001234">5206</key><summary>Remove useless URL instantiation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">Paikan</reporter><labels><label>:Internal</label><label>enhancement</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T22:16:26Z</created><updated>2015-06-07T15:15:58Z</updated><resolved>2014-02-20T23:24:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-20T23:24:47Z" id="35682069">Merged, thanks again!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changed the caching of FieldDataSource in aggs to be based on field name...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5205</link><project id="" key="" /><description>... + required Value Source type as a combi key (used to be only field name). This fixes a problem where multiple aggregations where defined on the same field, yet require different types of value sources.

Closes #5190
</description><key id="27999428">5205</key><summary>Changed the caching of FieldDataSource in aggs to be based on field name...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T21:53:21Z</created><updated>2015-06-07T23:13:16Z</updated><resolved>2014-02-20T22:37:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-20T22:25:29Z" id="35676919">+1, thanks for taking care of this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node Indices Status API is significantly slower</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5204</link><project id="" key="" /><description>`curl http://localhost:9200/_cluster/nodes/_local/stats?all=true` used to take a few milliseconds against my development installation (0.90.2 on OS X). I upgraded to 1.0.0 and adjusted my URL, and now it takes anywhere from 2-4 full seconds.

```
13:28:38 ~ → time curl http://localhost:9200/_nodes/_local/stats/indices
{"cluster_name":"elasticsearch_andrew","nodes":{"6QXHJw0uTAe7GRKo073LOg":{"timestamp":1392931719502,"name":"Saturnyne","transport_address":"inet[/127.0.0.1:9300]","host":"jester.local","ip":["inet[/127.0.0.1:9300]","NONE"],"indices":{"docs":{"count":1258,"deleted":221},"store":{"size_in_bytes":795157,"throttle_time_in_millis":0},"indexing":{"index_total":2,"index_time_in_millis":63,"index_current":0,"delete_total":30,"delete_time_in_millis":106,"delete_current":0},"get":{"total":0,"time_in_millis":0,"exists_total":0,"exists_time_in_millis":0,"missing_total":0,"missing_time_in_millis":0,"current":0},"search":{"open_contexts":0,"query_total":0,"query_time_in_millis":0,"query_current":0,"fetch_total":0,"fetch_time_in_millis":0,"fetch_current":0},"merges":{"current":0,"current_docs":0,"current_size_in_bytes":0,"total":0,"total_time_in_millis":0,"total_docs":0,"total_size_in_bytes":0},"refresh":{"total":56,"total_time_in_millis":105},"flush":{"total":10,"total_time_in_millis":352},"warmer":{"current":0,"total":73,"total_time_in_millis":1},"filter_cache":{"memory_size_in_bytes":0,"evictions":0},"id_cache":{"memory_size_in_bytes":0},"fielddata":{"memory_size_in_bytes":0,"evictions":0},"percolate":{"total":0,"time_in_millis":0,"current":0,"memory_size_in_bytes":0,"memory_size":"0b","queries":0},"completion":{"size_in_bytes":0},"segments":{"count":121,"memory_in_bytes":324127668},"translog":{"operations":0,"size_in_bytes":0}}}}}
real    0m2.211s
user    0m0.005s
sys 0m0.004s
```

On production instances I scrape these stats with python and collectd to monitor my cluster.
</description><key id="27997740">5204</key><summary>Node Indices Status API is significantly slower</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">loe</reporter><labels /><created>2014-02-20T21:29:40Z</created><updated>2014-08-21T18:15:47Z</updated><resolved>2014-07-11T10:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="loe" created="2014-02-20T21:30:51Z" id="35671295">I should add my machine is quite fast. Core i7, 8GB RAM, SSDs. It is only the indices stats that are slow, all others (java, os, process, http etc. all return in a few milliseconds as before).
</comment><comment author="s1monw" created="2014-02-20T21:30:59Z" id="35671308">this might be related to https://github.com/elasticsearch/elasticsearch/issues/5201 ?
</comment><comment author="s1monw" created="2014-02-20T21:34:36Z" id="35671680">is it possible that you have some old Lucene 3 segments in your index that were created with `0.20`?
</comment><comment author="kimchy" created="2014-02-20T21:35:39Z" id="35671778">FYI: the segments API will give back the Lucene version the segment was created with
</comment><comment author="loe" created="2014-02-20T21:44:41Z" id="35672729">I do indeed have a variety of different "versions", 3.6, 3.6.2 and 4.3.

Is there a way to upgrade these?
</comment><comment author="s1monw" created="2014-02-20T22:01:22Z" id="35674565">@loe you can upgrade your segments which usually happens during a merge. If you optimize you index it will be the latest format. Yet this might take time and might even have an impact on your cluster while you run it so be careful with this - it really depends on how much data is in there though.
</comment><comment author="loe" created="2014-02-20T22:18:56Z" id="35676277">So I ran optimize against my indexes (`curl -XPOST http://localhost:9200/_all/_optimize`) but I still have some 3.6.2 segments in my shards. Do I need to have more churn in order to trip merges?
</comment><comment author="s1monw" created="2014-02-21T08:45:40Z" id="35708834">hmm this is odd - what did the optimize call return?
</comment><comment author="loe" created="2014-02-22T01:00:05Z" id="35789413">`total":36,"successful":36,"failed":0}}`
</comment><comment author="s1monw" created="2014-02-24T14:57:26Z" id="35893680">hey @loe my assumption is that there is really just one segment and that means it will not re-write the segment. I don't think you have a chance to do that at this point.
</comment><comment author="clintongormley" created="2014-07-11T10:29:47Z" id="48715918">The optimize call now supports a `force` option #5293 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>upgrading to 1.0 docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5203</link><project id="" key="" /><description>Thought it might be helpful to mention the full cluster restart in a more prominent location. 
</description><key id="27995199">5203</key><summary>upgrading to 1.0 docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kurtado</reporter><labels /><created>2014-02-20T20:54:00Z</created><updated>2014-06-25T13:02:41Z</updated><resolved>2014-04-07T11:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T11:51:33Z" id="39720992">I'd say this is going to be replaced by #5652, the upgrade guide. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disable RAM usage estimation on Lucene 3.x segments.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5202</link><project id="" key="" /><description>Close #5201
</description><key id="27993438">5202</key><summary>Disable RAM usage estimation on Lucene 3.x segments.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Stats</label><label>enhancement</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T20:29:37Z</created><updated>2015-06-07T15:16:48Z</updated><resolved>2014-02-20T22:48:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-20T20:49:57Z" id="35667224">LGTM - I am happy that we write the memory as a `long` since it's now negative. 
</comment><comment author="jpountz" created="2014-02-20T20:56:34Z" id="35667891">Hehe. This is the first thing that I checked!
</comment><comment author="s1monw" created="2014-02-20T21:00:12Z" id="35668240">;) me too :warning: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Work around Lucene 3.x segments' costly RAM usage estimations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5201</link><project id="" key="" /><description>This is a follow-up to [LUCENE-5462](https://issues.apache.org/jira/browse/LUCENE-5462), Lucene 3.x segments happen to be very costly when it comes to memory usage estimations and this can make the stats API use too much CPU and memory.

This will be fixed in Lucene 4.7 but until then we should disable memory usage estimation on 3.x segments.
</description><key id="27989946">5201</key><summary>Work around Lucene 3.x segments' costly RAM usage estimations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T19:42:23Z</created><updated>2014-03-03T16:28:15Z</updated><resolved>2014-02-20T22:45:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/engine/SegmentsStats.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file></files><comments><comment>Disable RAM usage estimation on Lucene 3.x segments.</comment></comments></commit></commits></item><item><title>[TEST] Plugins: wait for REST Service to start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5200</link><project id="" key="" /><description>When running tests for site plugins, it could happen that the REST Service is not fully started and not ready immediately to serve HTTP requests.
It gives `503 Service Unavailable` error in that case.

This patch will gives 5 seconds before failing the test.
</description><key id="27974717">5200</key><summary>[TEST] Plugins: wait for REST Service to start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T16:28:10Z</created><updated>2014-06-28T20:43:22Z</updated><resolved>2014-02-20T16:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-20T16:38:49Z" id="35641060">LGTM
</comment><comment author="dadoonet" created="2014-02-20T16:59:11Z" id="35643373">Merged in 0.90, 1.0, 1.x, master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>dynamic templates are parsed OK but fail during index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5199</link><project id="" key="" /><description>When applying dynamic templates, the following config gets applied OK, but triggers the following error message when ES tries to create the index:

```
org.elasticsearch.index.mapper.MapperParsingException: A dynamic template must be defined with a name
```

``` json
        "dynamic_templates" : [ {
          "number_fields" : {
            "mapping" : {
              "type" : "float"
            },
            "match_pattern" : "regex",
            "match" : "^(duration|.*_num|.*_dur)$",
            "match_mapping_type" : "string"
          },
          "integer_fields" : {
            "mapping" : {
              "type" : "integer"
            },
            "match_pattern" : "regex",
            "match" : "^(integer|.*_int|.*_len|port|pid|uid|gid|PID|UID|GID|TID|PRI|.*_NUM)$",
            "match_mapping_type" : "string"
          },
          "string_fields" : {
            "mapping" : {
              "type" : "multi_field",
              "fields" : {
                "raw" : {
                  "index" : "not_analyzed",
                  "ignore_above" : 256,
                  "type" : "string"
                },
                "{name}" : {
                  "index" : "analyzed",
                  "omit_norms" : true,
                  "type" : "string"
                }
              }
            },
            "match_mapping_type" : "string",
            "match" : "*"
          }
        } ],
```

When Only one key exists, e.g. `"string_fields"`, then all is good
</description><key id="27966769">5199</key><summary>dynamic templates are parsed OK but fail during index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faxm0dem</reporter><labels /><created>2014-02-20T14:50:12Z</created><updated>2014-12-29T12:21:20Z</updated><resolved>2014-12-29T12:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-03-27T04:53:18Z" id="38769036">@faxm0dem the dynamic template should be an array of objects

this is what you got:

``` json
{
  "dynamic_templates": [
    {
      "number_fields": {
        "mapping": {
          "type": "float"
        },
        "match_pattern": "regex",
        "match": "^(duration|.*_num|.*_dur)$",
        "match_mapping_type": "string"
      },
      "integer_fields": {
        "mapping": {
          "type": "integer"
        },
        "match_pattern": "regex",
        "match": "^(integer|.*_int|.*_len|port|pid|uid|gid|PID|UID|GID|TID|PRI|.*_NUM)$",
        "match_mapping_type": "string"
      },
      "string_fields": {
        "mapping": {
          "type": "multi_field",
          "fields": {
            "raw": {
              "index": "not_analyzed",
              "ignore_above": 256,
              "type": "string"
            },
            "{name}": {
              "index": "analyzed",
              "omit_norms": true,
              "type": "string"
            }
          }
        },
        "match_mapping_type": "string",
        "match": "*"
      }
    }
  ]
}
```

and this is the correct one:

``` json
{
  "dynamic_templates": [
    {
      "number_fields": {
        "mapping": {
          "type": "float"
        },
        "match_pattern": "regex",
        "match": "^(duration|.*_num|.*_dur)$",
        "match_mapping_type": "string"
      }
    },
    {
      "integer_fields": {
        "mapping": {
          "type": "integer"
        },
        "match_pattern": "regex",
        "match": "^(integer|.*_int|.*_len|port|pid|uid|gid|PID|UID|GID|TID|PRI|.*_NUM)$",
        "match_mapping_type": "string"
      }
    },
    {
      "string_fields": {
        "mapping": {
          "type": "multi_field",
          "fields": {
            "raw": {
              "index": "not_analyzed",
              "ignore_above": 256,
              "type": "string"
            },
            "{name}": {
              "index": "analyzed",
              "omit_norms": true,
              "type": "string"
            }
          }
        },
        "match_mapping_type": "string",
        "match": "*"
      }
    }
  ]
}
```
</comment><comment author="faxm0dem" created="2014-03-27T07:18:24Z" id="38774993">Thanks, I'll try that.
However, I there should be an input validation, what's your opinion on that?
</comment><comment author="clintongormley" created="2014-12-29T12:21:20Z" id="68253115">Closing in favour of #8802
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature/5122</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5198</link><project id="" key="" /><description>This is a first take on enabling templating for search requests - feedback welcome.

Note: This does not yet deal with referencing template and template paramters in the search request url parameters.
</description><key id="27966416">5198</key><summary>Feature/5122</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels /><created>2014-02-20T14:45:32Z</created><updated>2014-06-28T09:06:07Z</updated><resolved>2014-03-28T07:58:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-20T16:02:20Z" id="35636680">I like the general idea. Yet I think the the impl is problematic since you can provide a `template` as well as `query` etc. on the top level. Another idea would be to use the entire body as the template and communicate via a flag like:

``` JSON
GET localhost:9200/_search/template?param1=all&amp;param2=2 
{
  "query": {
               "match_{{param1}}": {}
   },
  "size" : {{param2}}
}
```

that way we can just pass ordinary search requests and there is no ambiguity here at all. On the java API level we can simply have a flag that indicates that this is a template request.

What do you think?
</comment><comment author="MaineC" created="2014-02-20T16:07:27Z" id="35637339">+1 - looks much better than my first draft
</comment><comment author="spinscale" created="2014-03-28T07:58:52Z" id="38895892">closing this, introduced via https://github.com/elasticsearch/elasticsearch/pull/5353
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>The seed format is suite:method, clarified it in the docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5197</link><project id="" key="" /><description /><key id="27962513">5197</key><summary>The seed format is suite:method, clarified it in the docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels /><created>2014-02-20T13:49:16Z</created><updated>2014-07-16T21:48:19Z</updated><resolved>2014-02-20T15:02:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-20T15:02:24Z" id="35629708">pushed thx
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE in PluginsService when starting elasticsearch with a wrong user</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5196</link><project id="" key="" /><description>When starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.

To reproduce:
- create a plugins directory with `rwx` rights for root user only
- launch elasticsearch from another account (elasticsearch for example)

It was supposed to be fixed with #4186, but sadly it's not :-(

Closes #5195.
</description><key id="27960725">5196</key><summary>NPE in PluginsService when starting elasticsearch with a wrong user</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T13:21:13Z</created><updated>2015-06-07T23:12:13Z</updated><resolved>2014-02-24T10:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-21T16:20:53Z" id="35745889">codewise this is ok, but can you please test this with a couple of weird chmods (or even test it automatically?)? To me it looks like the plugins service can be even unittested, but changing permissions is not easily possible... seems like manual testing
</comment><comment author="spinscale" created="2014-02-24T09:55:21Z" id="35871219">tested this on mac os with directories 'chmod'ed to 644 or 000 and no exception was thrown (this was what its all about, right?)

LGTM
</comment><comment author="dadoonet" created="2014-02-24T10:53:45Z" id="35875413">Pushed in master and 1.x.
Thanks for the review @spinscale!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE in PluginsService when starting elasticsearch with a wrong user</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5195</link><project id="" key="" /><description>When starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.

To reproduce:
- create a plugins directory with `rwx` rights for root user only
- launch elasticsearch from another account (elasticsearch for example)

It was supposed to be fixed with #4186, but sadly it's not :-(
</description><key id="27960562">5195</key><summary>NPE in PluginsService when starting elasticsearch with a wrong user</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T13:18:35Z</created><updated>2014-02-24T10:52:50Z</updated><resolved>2014-02-24T10:52:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/plugins/PluginsService.java</file></files><comments><comment>NPE in PluginsService when starting elasticsearch with a wrong user</comment></comments></commit></commits></item><item><title>Cat count api is slower than it could be</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5194</link><project id="" key="" /><description>```
web245 ~ # time curl http://web245:9200/_cat/count
1392898347 16:12:27 8263080431

real    0m17.314s
user    0m0.002s
sys 0m0.001s

web245 ~ # time curl -s 'http://web245:9200/_cat/indices?h=docs.count' | awk 'BEGIN { s = 0; } { s += $1; } END { print s; }'
8263081560

real    0m0.073s
user    0m0.003s
sys 0m0.000s

web245 ~ # time curl http://web245:9200/_cat/count
1392898391 16:13:11 8263083221

real    0m15.231s
user    0m0.002s
sys 0m0.001s
web245 ~ # time curl -s 'http://web245:9200/_cat/indices?h=docs.count' | awk 'BEGIN { s = 0; } { s += $1; } END { print s; }'
8263083976

real    0m0.097s
user    0m0.001s
sys 0m0.001s
```

15+ seconds vs less than 100ms, this seems to be wrong.
</description><key id="27957158">5194</key><summary>Cat count api is slower than it could be</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>adoptme</label></labels><created>2014-02-20T12:14:57Z</created><updated>2014-12-29T12:16:40Z</updated><resolved>2014-12-29T12:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-04-15T14:17:30Z" id="40486366">`/_cat/indices` is using index statistics from Lucene segment info, which is very fast but not flexible.  `/_cat/count` is using the count API underneath which is flexible (can issue any query), but slower because it actually has to perform search-like behavior.
</comment><comment author="bobrik" created="2014-04-15T14:21:12Z" id="40486825">I don't see how I can supply query to `/_cat/count`. If I can't, why not make it work with segment info?
</comment><comment author="clintongormley" created="2014-12-29T12:16:40Z" id="68252876">Actually, these two APIs provide different information.  The `_cat/indices` API tells you how many documents are in the index in total, while the `_cat/count` API tells you how many top-level (ie non-nested) documents there are.  This info is only accessible via a query, so this API shouldn't change.

Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update misc docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5193</link><project id="" key="" /><description>- Update link to our puppet module
- Remove link to custom es rpm's as we have our own.
</description><key id="27953125">5193</key><summary>Update misc docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electrical</reporter><labels /><created>2014-02-20T11:09:13Z</created><updated>2014-07-16T21:48:20Z</updated><resolved>2014-04-07T12:28:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve geo distance accuracy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5192</link><project id="" key="" /><description>Geo distance computations assume that the earth is round although it is an ellipsoid. Taking the ellipsoid shape into account would be costly but thanks to [LUCENE-5271](https://issues.apache.org/jira/browse/LUCENE-5271), `SloppyMath.haversin` is now going to use an approximate value of the diameter of the earch at the average of the two latitudes instead of assuming a uniform diameter. We should use the same trick for the non-sloppy `arc` distance computation.
</description><key id="27949715">5192</key><summary>Improve geo distance accuracy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T10:13:54Z</created><updated>2015-06-07T15:18:15Z</updated><resolved>2014-02-26T21:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2014-02-20T10:37:00Z" id="35607635">@jpountz cool, go for it +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/test/java/org/apache/lucene/util/SloppyMathTests.java</file></files><comments><comment>Improve `arc` geo-distance accuracy.</comment></comments></commit></commits></item><item><title>Add support for matching mode when attaching REST handlers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5191</link><project id="" key="" /><description>At the moment, ElasticSearch does a matching mode `equals` when attaching REST handlers using the method `RestController#registerHandler`:

```
    @Inject
public MyRestHandler(Settings settings, Client client, RestController controller) {
    super(settings, client);

    // Define REST endpoints
    controller.registerHandler(Method.GET, "(...)",this);
```

In some cases, it's not suitable since we would want to attach an handle on a subset of URIs. For such cases, the matching mode `starts with` would be more suitable:

```
controller.registerHandler(Method.GET, "/myuri",
                   RestMatchingMode.STARTS_WITH, this);
```

This means that all sub URIs (like `/myuri/segment1`, `/myuri/segment2/segment3/segment4`, and so on) would handle by the same class. This would be useful if we want manage URIs in a custom way (URI format, ...) that isn't supported by ElasticSearch. This is typically the case for OData URIs.

To implement such feature, we need to add an enumeration to list all support matching modes:

```
public enum RestMatchingMode {
    EQUALS, STARTS_WITH
}
```

We also need to update the `RestController` to add the corresponding methods:

```
public void registerHandler(RestRequest.Method method,
                                      String path, RestHandler handler) {
    registerHandler(method, path, RestMatchingMode.EQUALS, handler);
}

public void registerHandler(RestRequest.Method method, String path,
                                     RestMatchingMode matchingMode, RestHandler handler) {
    (...)
}
```

As the matching is handled by the classes `PathTrie` and `TreeNode`, we need to add in it the support of matching mode at both methods insert and retrieve.

```
public synchronized void insert(String[] path, int index,
                     RestMatchingMode matchingMode, T value) {
   (...)
   TrieNode&lt;T&gt; node = children.get(key);
   if (node == null) {
        if (index == (path.length - 1)) {
            node = new TrieNode&lt;T&gt;(token, value, this, wildcard, matchingMode);
        } else {
            node = new TrieNode&lt;T&gt;(token, null, this, wildcard, RestMatchingMode.EQUALS);
        }
        children = newMapBuilder(children).put(key, node).immutableMap();
    (...)
}

public T retrieve(String[] path, int index, Map&lt;String, String&gt; params) {
    (...)
    if (index == (path.length - 1)) {
        return node.value;
    }

    // In the case of starts with matching, return the value
    if (RestMatchingMode.STARTS_WITH.equals(node.getMatchingMode())) {
        return node.value;
    }
    (...)
```

Moreover we simply need to add the property matchingMode in the class TreeNode

```
public class TrieNode&lt;T&gt; {
    (...)
    private RestMatchingMode matchingMode; 
    (...)

    public TrieNode(String key, T value, TrieNode parent, String wildcard,
                               RestMatchingMode matchingMode) {
        this.key = key;
        this.wildcard = wildcard;
        this.isWildcard = (key.equals(wildcard));
        this.parent = parent;
        this.value = value;
        this.children = ImmutableMap.of();
        this.matchingMode = matchingMode;
        (...)
    }
    (...)
}
```
</description><key id="27941965">5191</key><summary>Add support for matching mode when attaching REST handlers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">templth</reporter><labels><label>feedback_needed</label></labels><created>2014-02-20T07:27:41Z</created><updated>2015-01-30T09:57:38Z</updated><resolved>2015-01-30T09:57:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hahncj55408" created="2014-06-24T21:48:07Z" id="47035528">I was stuck on this for a long long while.
I finally got past this as a hold up by including variables in the registerHandler method, like so:
`restController.registerHandler(Method.GET,"/_myplugin/myhandler/{my_dataobject}/{my_do_id}", this);`

One can access the variables from the request, param method:
`String class = request.param("dataobject");`
`String id= request.param("my_do_id");`

I don't know if this will help you, but hopefully it helps someone else stuck at the same point i was.

Chris
</comment><comment author="clintongormley" created="2014-07-04T09:56:43Z" id="48026885">Please could you provide more information about your use case?  We're trying to figure out if there is a better way to do this.
</comment><comment author="hahncj55408" created="2014-07-10T21:04:25Z" id="48664168">Were you talking to me, clinton?  If so:
We're building an authentication/authorization service into elastic.  It was decided that the authentication data should be stored in a separate database.  The plugin described is can be used to access this data using rest calls, we added it to elastic for convenience.
With all that said, each unique table in the database could have a unique path, and as we add or change tables in this database, i didn't want to change the paths we're registering.
So I needed the ability to register a handler with a variable path.  
</comment><comment author="clintongormley" created="2014-07-11T11:54:55Z" id="48721885">@hahncj55408 yes you, and @templth  :)

thanks for your feedback, we'll discuss some more
</comment><comment author="templth" created="2014-07-11T12:06:16Z" id="48722685">Regarding the use case, I think about a way to have a custom format (for example OData with format like `OData/OData.svc/Category(1)/Products?$top=2&amp;$orderby=name`, see http://www.odata.org/documentation/odata-version-3-0/url-conventions/) for the last part of the URL that we want to manage by ourselves... For this we want to attach an handler on this last part with a mode `starts_with`. This handler will be responsible to handle this part of the URI and extract the data it needs with some specific processing.

@clintongormley Hope I'm clear ;-) If not, feel free to tell me!
</comment><comment author="clintongormley" created="2014-07-25T09:50:24Z" id="50129385">Would you be interested in sending a PR?
</comment><comment author="clintongormley" created="2014-09-26T18:26:15Z" id="57001414">No further feedback. Closing for now. Feel free to reopen if you get back to this.
</comment><comment author="templth" created="2014-09-30T07:32:31Z" id="57277167">@clintongormley I created the PR for this code (see https://github.com/elasticsearch/elasticsearch/pull/7923). Tell me if it suits you...
</comment><comment author="clintongormley" created="2014-10-14T12:09:46Z" id="59032494">Thanks @templth - I've reopened this issue and marked the PR for review.

In the meantime, could I ask you to sign the CLA please. http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="templth" created="2014-11-12T08:51:48Z" id="62687082">@clintongormley Great! Feel free to contact me if you need. For information, I asked my company for the CLA. I'm waiting for its answer...
</comment><comment author="clintongormley" created="2014-11-12T12:15:43Z" id="62709641">@templth even with your company CLA, we'll still need a personal CLA from you.
</comment><comment author="templth" created="2015-01-05T09:58:03Z" id="68687602">@clintongormley Just got the permission from my company! I signed the CLA as an individual contributing under an existing company contributor agreement. I'll check if my company signed its one...
</comment><comment author="templth" created="2015-01-26T16:59:47Z" id="71494891">@clintongormley My company told me that they sign the company CLA ;-)
</comment><comment author="jpountz" created="2015-01-30T09:57:38Z" id="72178260">Closed (see #7923)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"Missing" aggregation fails when object containing aggregation field is missing as well</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5190</link><project id="" key="" /><description>say our data structure is

```
invoice:{
  vendor:{
    id:10
    name: "google"
  }
}
```

vendor object is optional to the invoice and may be absent

when calculating missing aggregation on vendor.id it fails with 

```
earchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[mr2aF25CTPGrkvzftHq9Rg][award][0]: ClassCastException[org.elasticsearch.search.aggregations.support.FieldDataSource$Bytes$FieldData cannot be cast to org.elasticsearch.search.aggregations.support.FieldDataSource$Numeric]}]
```

I would expect missing aggregation to treat missing parent objects of the field the aggregation is calculated for as if the field itself is missing or it would be virtually impossible to guarantee that such aggregation would finish successfully over deeply nested graphs where any part of the path to the missing field may be absent
</description><key id="27940314">5190</key><summary>"Missing" aggregation fails when object containing aggregation field is missing as well</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">roytmana</reporter><labels><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-20T06:35:22Z</created><updated>2014-02-20T22:39:36Z</updated><resolved>2014-02-20T22:37:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-20T08:42:31Z" id="35599158">Indeed the missing aggregation should work on fields that are not mapped and count every hit as a document that misses the field.

I just tried to reproduce this issue without luck, could you please try to either provide us with a curl recreation or give the stacktrace of the `ClassCastException` in the logs? Thanks.
</comment><comment author="roytmana" created="2014-02-20T14:34:31Z" id="35626601">@jpountz No logs just what I get in the response (maybe I need to switch in debug mode?)

here is the recreation. it does not fail if missing is the only aggregation or the sibling bucket aggs is on some other field only when you do say terms on a field and next to it missing on the same field
so the issue may be slightly different that I thought initially

https://gist.github.com/roytmana/9114933
</comment><comment author="uboness" created="2014-02-20T16:14:59Z" id="35638224">hi @roytmana, this is indeed a bug, will be working on fixing it. Thx for reporting!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>src/test/java/org/elasticsearch/search/aggregations/CombiTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/RandomTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file></files><comments><comment>Changed the caching of FieldDataSource in aggs to be based on field name + required Value Source type as a combi key (used to be only field name). This fixes a problem where multiple aggregations where defined on the same field, yet require different types of value sources.</comment></comments></commit></commits></item><item><title>Leaking file handles?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5189</link><project id="" key="" /><description>I know this issue has been raised a couple times in the past, but none of them has provided me a solution so far. I've also seen similar issues being raised in forums.

We're running ES 0.90.7 on a two-node EC2 cluster with the following settings:
- Total RAM 17GB
- ES_HEAP_SIZE 9GB
- open files (ulimit -n) 65535

We have five different indexes (13GB, 7GB, 4GB, 22MB, 4MB), we are running about 150K indexing operations per day (mostly new docs, but also some updates). Depending on days, we're running between 60K and 120K search operations.

Here is a recurrent pattern that has been consistently happening over the past couple months and which requires us to restart the cluster every 3/4 days or so.
The graphics below show the evolution of the heap size, the process CPU and the open file handles from one restart (~12PM on Feb 15th) to the next one (~8PM on Feb 19th). Initially, everything runs smoothly. Node 2 was the master in that run and was the one processing the ActiveMQ river. 

Then on Feb 18th around 12PM (orange vertical bar), something starts happening. At that time, the same things keep happening on the master node:
- the process CPU starts increasing and stays high
- the file handles count keeps increasing (but more steeply)
- The GC doesn't seem to be able to release memory

![capture decran 2014-02-20 a 05 26 24](https://f.cloud.github.com/assets/1280019/2215402/70a7e0ba-99f0-11e3-8756-05c31daa3561.png)

Everything keeps running more or less smoothly for a day or so up until the file handles are exhausted. Just before they do, we restart the problematic node (green vertical bar), the second node becomes the master and the same pattern repeats with that node.

Another thing worth noting is that about 90% of the open file handles are marked (deleted) and belong 99% to the biggest index, i.e. the one that is 13GB.

We're going to upgrade to 0.90.11 anytime soon to see if Lucene 4.6.1 will be of any help, but in the meantime, we'd appreciate any kind of insights as to why this kind of things is happening.
</description><key id="27938741">5189</key><summary>Leaking file handles?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">consulthys</reporter><labels /><created>2014-02-20T05:42:06Z</created><updated>2014-04-18T11:45:12Z</updated><resolved>2014-03-25T08:13:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-20T08:36:24Z" id="35598819">Can you run and attach the output of a hot_threads operation when the CPU is high? This will show you, which elasticsearch java classes use the most CPU
Do you hit your system with scan search queries? That could be a reason for open file handles on deleted files.

I dont know anything about the active mq river? Does this also happen when it is switched off? Or does this also happen on the node if the river is running on the other one?
</comment><comment author="consulthys" created="2014-02-20T08:56:48Z" id="35600049">Thanks for your answer Alexander.
We've been monitoring hot threads regularly, but nothing struck us so far. We will look again very closely within the next 2-3 days and get some samples regularly all over the critical time period.

We do no use scan search queries, only the default one (query_then_fetch). Although, granted, as most of our queries return no docs (i.e. with size=0) and are mostly run for facetting purposes, we could switch to search_type=count for those, that will probably improve things a bit.

We cannot switch off the river, unfortunately. Though, it seems to consistently happen on the node which is running the river (whether master or not).
</comment><comment author="s1monw" created="2014-02-20T09:01:00Z" id="35600313">just a blind guess but this looks like a big merge - do you have stats of used disk space during that period? Do you have any non-standard configurations and are you doing something special during that time?
</comment><comment author="consulthys" created="2014-02-20T09:06:58Z" id="35600735">We have about 250GB available disk space on those machines and ES is the only thing running on them. So substracting system files and our 25GB indexed data, that leaves about 200GB of free space.

We have a pretty standard configuration, we only tweaked the heap sizes for the filters and fielddata caches to better handle our situation. Aside from that nothing else is running.

I'll watch again the disk space and other processes the next time it happens and report the findings here.
</comment><comment author="bleskes" created="2014-02-20T09:08:07Z" id="35600801">As an experiment - is it possible for you add a third node (potentially on the same server as one of the current node), turn off it's master and data roles and force the river to be allocated on it. This will separate the river functionality from the other nodes and would allow us to see whether the problem is river specific.

The config on the river node (in the elasticsearch.yml) should be something like

```
node.master: false
node.data: false

node.river: rabbitmq
```

On the other two nodes, add the following to the elaticsearch.yml:

```
node.river: _none_
```

You will have to restart the nodes for this to have affect.
</comment><comment author="consulthys" created="2014-02-20T09:23:16Z" id="35601825">That's effectively one measure we were planning to enforce. We will try that as well.
Thanks guys!!
</comment><comment author="bleskes" created="2014-02-20T09:24:30Z" id="35601910">Just note that it is a temporary research solution as it will make the river node a single point of failure - the river will never be re-assigned upon a failure of that node.
</comment><comment author="consulthys" created="2014-02-26T16:33:59Z" id="36145187">Quick note to point out that we've taken two measures so far:
- upgrading to 0.90.11 (Lucene 4.6.1)
- making sure that all queries which only return facets and no hits are using search_type=count

Both of these measures have had no effect so far, as the same pattern is repeating.

We've yet to experiment what @bleskes brought up above, but it's not trivial to do it in production with minimal interruption. Stay tuned...
</comment><comment author="s1monw" created="2014-03-12T20:35:29Z" id="37460998">any updates on this one?
</comment><comment author="consulthys" created="2014-03-13T10:19:57Z" id="37517578">Nope, we haven't been able to run the experiment depicted by @bleskes yet, but we will soon.

One thing I'm eager to discover is whether
1. the GC starts going south (and the CPU as well) because the file handles start leaking, or
2. the file handles start leaking because the GC starts going south (and the CPU as well). 

Judging by the above charts, it looks like the GC starts behaving weird (i.e. when the heap can't be cleaned correctly anymore) before the file handles count starts increasing, but nothing clear cut.
</comment><comment author="consulthys" created="2014-03-13T12:26:15Z" id="37526844">Here's another interesting capture from yesterday when we reindexed 10M+ documents. The red node is initially the master and runs the river. We've let it proceed until the heap reached 99%, at which point we restarted it (green vertical line) and the blue node became master.

What we can see is that both nodes were actively indexing documents, yet the open file handle count only increases on the master.

Also when the blue node became master and picked up the river, its file handle count increased much more quickly, probably because its heap was already around 70% and kept increasing steadily up to 99%, where we restarted it and the red node became master again.

The dashed gray lines denote that whenever the file handle count crosses a certain threshold and keeps increasing, the CPU goes way up. Or maybe as commented earlier, it's vice versa, i.e. that whenever the CPU goes way up (because heap is stressed because GC can't do its job), the file handles are not recycled anymore and keep increasing.

It also looks like the heap on the master node never has as much leeway as the heap on the worker node, i.e. no "sawtooth" pattern.

![screenshot](https://f.cloud.github.com/assets/1280019/2409157/192530da-aaa9-11e3-8141-c14831f5362a.png)
</comment><comment author="dadoonet" created="2014-03-13T12:59:10Z" id="37529464">I gave a Quick look at ActiveMQ river plugin and it looks like they create a new Bulk request for every new message: https://github.com/domdorn/elasticsearch-river-activemq/blob/master/src/main/java/org/elasticsearch/river/activemq/ActiveMQRiver.java#L263

Looks bad to me.

May be I misread the code though.
</comment><comment author="consulthys" created="2014-03-13T13:12:07Z" id="37530623">Thank you David, I appreciate your input. That might be a good point. Though, the ActiveMQ river plugin was forked from the official RabbitMQ river plugin, which does pretty much the same: https://github.com/elasticsearch/elasticsearch-river-rabbitmq/blob/master/src/main/java/org/elasticsearch/river/rabbitmq/RabbitmqRiver.java#L323

We'll know soon if that's an issue, the third node has been prepared and the experiment is in progress. Stay tuned...
</comment><comment author="consulthys" created="2014-03-17T11:53:54Z" id="37807411">We've ran the experiment that @bleskes described above. The problem doesn't seem to be tied to the river (Node 3 in green), as the count of (deleted) open file handles still increases on the master node (Node 2 in blue).

I'm curious what makes the master node special in terms of why the file handles keep leaking on it, but not on the other worker node (Node 1 in red).

Another interesting piece of info is that the deleted docs ratio on both nodes is between 30% and 35%, which seems like a lot. The HQ plugin hints at slow IO when that ratio goes over 25%.

We're also making strong use of filter and fielddata caches (both set to only use 30% of the heap), but I'm unsure yet if and how this could be an issue.

I appreciate any further insights. Thanks.

![capture decran 2014-03-17 a 12 30 15](https://f.cloud.github.com/assets/1280019/2435601/aaf367a6-adca-11e3-9a7a-27a7fbc5948d.png)
</comment><comment author="bleskes" created="2014-03-17T12:37:13Z" id="37810275">@consulthys thx for the input. This indeed suggest the river is not the issue. I wonder if you have some background process that continuously checks for some master-level (i.e., cluster meta data) info like cluster health. Maybe that one doesn't properly close it's connections. 
</comment><comment author="consulthys" created="2014-03-17T15:46:25Z" id="37831334">We don't have anything else but ES running on those nodes. We do check the cluster health remotely via HTTP (mostly to get those graphs above), but the vast majority of (deleted) open file handles are not socket handles, but file handles to Lucene index files (lots of cfs, fdt, etc) and other ES internal files (doc, tim, pos, pay, etc).
</comment><comment author="consulthys" created="2014-03-18T06:09:54Z" id="37902383">In order to shed some more light on those file handles I was mentioning earlier, here is a quick breakdown for the current set of the 12,194 open (deleted) file handles that we have right now:
- cfs: 4046 (lucene compound file)
- pos: 1358 (positions file)
- pay: 1358 (payload file)
- tim: 1358 (terms dictionary)
- doc: 1358 (frequencies and skip data)
- fdt: 1358 (field data)
- nvd: 1358 (doc values data)

It somehow looks like an old Lucene bug (https://issues.apache.org/jira/browse/LUCENE-2762) but I'd assume it's fixed by now. I have yet to dive into the ES code that uses IndexReader/Writer/Searcher, but appreciate any kinds of insights regarding this.

Also, could it be that our filter cache is not sized adequately (currently 30% of 9GB heap)? When I look at the charts below representing the evolution of the filters and fielddata cache sizes during the experiment, it looks like the filters cache could use some more space. I'm not sure if this would have any impact at all, but I'm just pointing it out.

![capture decran 2014-03-18 a 05 15 47](https://f.cloud.github.com/assets/1280019/2444423/2f35d70e-ae54-11e3-82dc-e147e938c3b8.png)
</comment><comment author="bleskes" created="2014-03-18T09:03:55Z" id="37911312">@consulthys thx for the info. Although we typically only discuss issues here (and answer more general questions on the mailing list),I figured you earned the question :) - look at the amount of rejections from your filter cache and query performance (do you see spikes?) to decide wether you want to give more space to you filter cache.

That said - let's go back to the file handles. It's not uncommon to have so many file handles and that's OK. What I don't understand is why the master node seem to have it's file handles grow during indexing and just drop (same goes for high cpu usage). In the last image you gave both nodes are the same. I would be interested in the same kind of analysis of the master node when it's unusually high. Also can you give the output `GET _nodes/hot_threads` at the same time? This may give us insights into what it is actually doing.
</comment><comment author="consulthys" created="2014-03-18T09:21:27Z" id="37912527">Thank you @bleskes.

I'm ok with having a high file handle count on any node, too, as long as we know that it will decrease at some point. The thing is that it just never decreases. When you see a drop, it's because we had to restart the node. So far, we've basically **never** seen the file handle count decrease at all on the master, it's always steadily increasing and is only a matter of time until the resources (CPU, RAM, file handles, etc) get exhausted. Even though a couple K new documents are being indexed every couple seconds, I'd still expect the file handles count to grow/shrink as needed, but I'd expect to see the count shrink eventually, and most of all, I'd expect the (deleted) file handles to really be deleted at some point. This is telling me that something within ES (or Lucene) must still be referencing those files, and that's the big unknown for me so far.

I'm going to dig up some more info (hot threads et al) and get back here as soon as I have something.
</comment><comment author="consulthys" created="2014-03-18T12:41:49Z" id="37927849">Moreover, if you think I should be moving this to the mailing list until we figure out if this is an issue or not, please let me know. My guts tell me that there's something here, and I can hardly believe I'm the only one witnessing this kind of behavior.
</comment><comment author="bleskes" created="2014-03-18T17:03:41Z" id="37959018">We have everything here already, so I think we can do it here. Let me know when you have news.
</comment><comment author="consulthys" created="2014-03-20T14:13:30Z" id="38171081">As discussed, I'm providing some new insights on the hot threads running on the master node. What I did was to call `_nodes/hot_threads` on the master node every minute since the beginning of the last run until we had to restart the master a couple minutes ago (i.e. a time span of ~34h). That gives us ~5,3K samples to work on. From those, I'm filtering out all threads whose CPU usage is below 10% and the threads from the [management] pool as they are not that interesting. That leaves us 83 hot thread samples whose CPU usage ranges from 10% to 80%+, let's call them "very hot threads". Let me know if you need all 5K samples, but at quick glance, I think 95% of those are just noise.

What we can see is that the very hot threads appearing most often come (unsurprisingly) from the following pools:
- bulk
- refresh 
- search
- Lucene merge

Note: For the latter group, you'll see I've renamed the index on which the merge operation is being carried out to "biggest_index", "2nd_biggest_index" and "3rd_biggest_index", because the real index names as such don't really mean anything without more business context.

You can find the gist here:
https://gist.github.com/consulthys/49fd0fb6ad71222f4a75

Let's see if we can find any meaningful nugget of information in there.

Also, for reference purposes, here is a discussion I could find on the mailing list which deals with a very similar topic, but no answers so far.
https://groups.google.com/forum/?fromgroups#!searchin/elasticsearch/open$20file$20handle/elasticsearch/i-qpbSj1Qrc/R84KR50d7OwJ
</comment><comment author="consulthys" created="2014-03-24T09:41:20Z" id="38425660">In order to bring the hot threads output I shared earlier into perspective with the stats charts, I've also added on the chart the hot threads pools we're interested in (bulk, refresh, etc) and their usage percentage along the same timeline as the other charts. The result is shown below and what gets immediately apparent is that the refresh + Lucene merge threads are getting extra busy in the end (far right) which ultimately led to a restart of the master node (in red) since the heap was exhausted. 
![capture decran 2014-03-24 a 10 19 23](https://f.cloud.github.com/assets/1280019/2497503/ff1f6326-b337-11e3-8199-c1b06ddf90ab.png)

I'm also attaching a zoom of the last 90 minutes of hot threads activity capture:
![capture decran 2014-03-24 a 10 30 36](https://f.cloud.github.com/assets/1280019/2497483/bf376fe2-b337-11e3-930c-2726da404edf.png)
</comment><comment author="bleskes" created="2014-03-24T10:24:41Z" id="38428983">I've looked at the hot threads dumps and also at the latest charts (where filter evictions + cpu are interesting). It seems you are using a plugin to add an update by query functionality: https://github.com/yakaz/elasticsearch-action-updatebyquery . If I read the code correctly, that one makes all it's search on the primary shards and that puts those under heavy load. This makes we wonder if the issue is not so much which node is master but which holds the primary shards. As soon as the node is restarted, the shards on the other node are promoted to primary, which explains the pattern you see.

As an experiment - can you disable that plugin and see if this helps?
</comment><comment author="consulthys" created="2014-03-24T10:53:24Z" id="38431151">Thanks for your input. We're indeed using the updatebyquery plugin (mainly because of https://github.com/elasticsearch/elasticsearch/issues/1607 and https://github.com/elasticsearch/elasticsearch/issues/2230). Unfortunately, we cannot disable that plugin in production.

On "master vs primary", I think that would make perfect sense. Most of the time, all primary shards are on the master since we're only bringing down one node at a time and waiting for full cluster recovery before restarting the second node. Doing so will always put primary shards on the master, indeed. But, I do remember having restarted both nodes at the same time and seeing the primary shards being distributed among both nodes. When that happened (see chart below), the "open files count" for the worker node wasn't flat as in the latter chart above, but instead the count was also increasing as on the master.

![capture decran 2014-03-24 a 11 37 37](https://f.cloud.github.com/assets/1280019/2498009/606d947e-b340-11e3-9344-59e252d713ac.png)

So, we're zeroing in onto something here. It definitely looks like "master node or not" is not the real issue, but "primary shard or not" is, especially in the context of using the updatebyquery plugin which does all the work on the primary shards. Maybe @martijnvg and potentially @ofavre could shed some light on this.

As a side note, do you happen to have any insider info on the timeline for completing #2230?
</comment><comment author="consulthys" created="2014-03-24T13:24:03Z" id="38443084">Looking at the source code of [TransportShardUpdateByQueryAction.java](https://github.com/yakaz/elasticsearch-action-updatebyquery/blob/master/src/main/java/org/elasticsearch/action/updatebyquery/TransportShardUpdateByQueryAction.java), I'm wondering if the index searcher/reader are being properly closed/released in all situations as pointed out in other discussions, such as:
- http://elasticsearch-users.115913.n3.nabble.com/Open-deleted-file-handles-with-elasticsearch-td4019658.html
- http://www.gossamer-threads.com/lists/lucene/java-user/52759
- http://www.gossamer-threads.com/lists/lucene/general/167136
</comment><comment author="martijnvg" created="2014-03-24T14:48:25Z" id="38452956">@consulthys Yes, there might be a bug there. Not sure yet, it is a long time ago since I wrote that code and a lot has changed since then (the es code base). It should be released once the SearchContext is released. I'm rebasing the updatebyquery branch now and I'll double check is the searcher is released properly.

The query is executed only on the primary shards, so that the re-index operations can immediately happen in on the primary shards and then can be send to the replicas without executing the query there as well.

The reason that update by query is never pushed to ES, is that update by query operation can be a long running operation and there is no way of finding if an update by query is running and the cancel it.
</comment><comment author="ofavre" created="2014-03-24T14:52:56Z" id="38453530">@martijnvg You'd better work from the plugin as it has already been rebased a few times, and there are a few additions that have been made to your original code. Thanks!
</comment><comment author="martijnvg" created="2014-03-24T17:08:45Z" id="38471589">The update by query code doesn't close the SearchContext when on a shard no documents match with the query (which is likely to happen) or no docs match at all with the specified query.

In the TransportShardUpdateByQueryAction#doExecuteInternal() method in the following if statement:

``` java
if (docsToUpdateCount == 0) {
   ....
}
```

The following statement should be added in order to close the search context and release any resources associated with it:

``` java
searchContext.release();
```
</comment><comment author="consulthys" created="2014-03-25T07:53:48Z" id="38538263">@martijnvg and @ofavre I've pulled the 1.4 branch of the updatebyquery plugin, applied the proposed one-liner, repackaged the plugin and restarted both nodes. I'm monitoring right now and we'll see how it goes today, but one great news is that I can already see the count of deleted files decrease and get down to 0, which it never ever did. The open files count is currently the same on both nodes and steady around 2K after 3 hours of operation.

@bleskes Thanks for showing the direction ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Completion suggester with matches not at the beginning of the word</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5188</link><project id="" key="" /><description>Hello,

is it possible to configure it to match text which is not at the beginning of the word:
Right now, I did that:
for the mapping:

{
    "articles" : {
        "properties" : {
            "tags_suggest": {
               "type": "completion",
               "index_analyzer": "simple",
               "search_analyzer": "simple",
               "payloads": false
            },
            "authoredBy_suggest": {
               "type": "completion",
               "index_analyzer": "simple",
               "search_analyzer": "simple",
               "payloads": false
            },
          "views" : {"type":"object", "index":"no"},
          "metadata" : {"type":"object", "index":"no"}
        }
    }
}

When I search, I only get matches at the beginning:
Should I use another option or analyzer?

Thanks

Ex:
{
    "articles_suggest":{
          "text":"wom",
          "completion": {
               "size" : 100,
              "field" : "tags_suggest"
          }
      }
}

articles_suggest": [
        {
            "text": "wom",
            "offset": 0,
            "length": 3,
            "options": [
                {
                    "text": "women",
                    "score": 5
                },
                {
                    "text": "womanhood",
                    "score": 1
                },
                {
                    "text": "women 101",
                    "score": 1
                },
                {
                    "text": "women and black dudes",
                    "score": 1
                },
                {
                    "text": "women and cats",
                    "score": 1
                },
                {
                    "text": "women and sports",
                    "score": 1
                },
                {
                    "text": "women are the new men",
                    "score": 1
                },
                {
                    "text": "women in comedy",
                    "score": 1
                },
                {
                    "text": "women in politics",
                    "score": 1
                },
                {
                    "text": "women love cats",
                    "score": 1
                },
                {
                    "text": "women love shopping",
                    "score": 1
                },
                {
                    "text": "women pioneers",
                    "score": 1
                },
                {
                    "text": "women's cycling",
                    "score": 1
                },
                {
                    "text": "women's equality act",
                    "score": 1
                },
                {
                    "text": "womenspeak",
                    "score": 1
                },
                {
                    "text": "womp womp",
                    "score": 1
                }
</description><key id="27923592">5188</key><summary>Completion suggester with matches not at the beginning of the word</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heichwald</reporter><labels /><created>2014-02-20T00:35:10Z</created><updated>2014-02-20T08:28:55Z</updated><resolved>2014-02-20T08:28:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-20T08:28:54Z" id="35598422">At the moment, the completion suggester is a pure prefix suggester. You can circumvent this a little bit by using several inputs.

Please use the mailing list for questions like this, as we try to keep github issues for bugs and features mainly.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Togglable stacktrace display</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5187</link><project id="" key="" /><description>Make it flaggable with system property whether ES will display 
only Throwable message or full stack trace on startup, as 
suggested by @imotov in #5103 discussion. Defaults to 
displaying stack trace, but it can be suppressed by 
`-Des.suppress-stack-traces=true` .

Closes #5102
</description><key id="27920602">5187</key><summary>Togglable stacktrace display</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">timorantalaiho</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-19T23:42:32Z</created><updated>2015-06-07T15:24:47Z</updated><resolved>2014-04-04T15:18:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-27T23:50:53Z" id="38875271">you could get this by setting bootstrap level logging to DEBUG, right? @imotov maybe I am missing something?
</comment><comment author="imotov" created="2014-03-28T18:54:33Z" id="38955506">@kimchy yes you can get it, but only in the log file. When you run it in foreground you only get error message. I guess we could piggy back on the bootstrap:DEBUG setting and use it in foreground as well. So, it would console output and log would be consistent. 
</comment><comment author="kimchy" created="2014-03-28T18:59:00Z" id="38955926">@imotov ++, makes sense.
</comment><comment author="timorantalaiho" created="2014-03-30T12:17:43Z" id="39024079">Thankyou for the comments! Is [this](https://github.com/timorantalaiho/elasticsearch/commit/e2ca1d542201123e462ca57ce81f4f8cd6259142) what we're after, then?
</comment><comment author="kimchy" created="2014-03-30T15:14:15Z" id="39027894">@timorantalaiho something similar, just without changing the default bootstrap category to DEBUG
</comment><comment author="timorantalaiho" created="2014-03-30T15:52:00Z" id="39028947">All right, [removed it](https://github.com/timorantalaiho/elasticsearch/commit/058aeb0befb76851ee2e117c76d6b146002faf23) even though I as an ES user would greatly appreciate getting the stack traces of startup by default :)
</comment><comment author="javanna" created="2014-04-04T15:18:10Z" id="39576303">Merged, thanks @timorantalaiho !!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix yamlBuilder() to return YAML builder instead of SMILE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5186</link><project id="" key="" /><description>Closes #5185
</description><key id="27917935">5186</key><summary>Fix yamlBuilder() to return YAML builder instead of SMILE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kelseyfrancis</reporter><labels><label>:Internal</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-19T23:01:21Z</created><updated>2015-06-07T23:14:17Z</updated><resolved>2014-02-20T23:01:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-20T08:07:05Z" id="35597224">Thanks for spotting this! Can you sign the CLA http://www.elasticsearch.org/contributor-agreement/ so that we can merge this in?
</comment><comment author="kelseyfrancis" created="2014-02-20T12:49:01Z" id="35617917">CLA signed.
</comment><comment author="jpountz" created="2014-02-20T23:01:35Z" id="35680216">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>XContentBuilder.yamlBuilder() incorrectly returns a SMILE builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5185</link><project id="" key="" /><description /><key id="27917798">5185</key><summary>XContentBuilder.yamlBuilder() incorrectly returns a SMILE builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kelseyfrancis</reporter><labels><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-19T22:59:07Z</created><updated>2014-02-21T23:15:33Z</updated><resolved>2014-02-20T23:00:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java</file></files><comments><comment>Fix yamlBuilder() to return YAML builder instead of SMILE</comment></comments></commit></commits></item><item><title>Allow using the FVH and Postings Highlighter without storing extra data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5184</link><project id="" key="" /><description>Right now this is a work in progress more for review then anything.  I'll squash it and make it better later.

I'm sure there are tons of things wrong with this but it could save me a ton of disk space and speed up highlighting.  But, yeah, it is a huge hack.

Closes #5183
</description><key id="27917584">5184</key><summary>Allow using the FVH and Postings Highlighter without storing extra data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-19T22:56:00Z</created><updated>2014-06-17T14:36:39Z</updated><resolved>2014-03-31T17:25:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-21T01:31:39Z" id="35690366">What I think I have left:
~~1.  termSet support for the Postings highlighter~~
2.  Decide if I actually should sort the terms.  If we filter them there aren't too many to sort and it might speed up getting the FHV's getting the frequency because it'll change random reads to be more linear.  Maybe.  The Postings highlighter doesn't seem to care.
3.  Double and triple check tests
4.  More tests not part of the HighlighterSearchTests?  Lower level, probably
5.  Cleanup whitespace and formatting issues

More stuff?
</comment><comment author="nik9000" created="2014-02-21T13:42:19Z" id="35730925">~~6.  See about using the recycler~~
~~7.  Recheck value lookup caching.  I have a suspicion it might not be working/worth the trouble.~~
</comment><comment author="nik9000" created="2014-02-21T22:22:08Z" id="35780078">~~8. Recheck performance against master.~~
</comment><comment author="nik9000" created="2014-02-21T22:31:44Z" id="35780833">1.  If user sets `term_vector_over_x` then use FVH by default.
</comment><comment author="nik9000" created="2014-02-24T13:39:12Z" id="35886511">I'm going to set this aside for a few days until someone has time to review it.  This is what I (personally) think I could get out of this:
1.  Ability to try the postings highlighter in production without a full reindex cycle.
2.  A modest space savings on my indexes and a decent highlighting performance improvement.
3.  The chance to sacrifice some of this improvement to save more space for data that is searched less frequently.

In the instance I'm able to easily test I'm seeing 5% space savings with performance improvement and 25-30% for a performance decrease of under a tenth of a millisecond per document.  The index is only a gig with replication so none of this takes disk seek time into account but I imagine that'll tip things further towards storing fewer term vectors.
</comment><comment author="nik9000" created="2014-02-24T13:46:09Z" id="35887047">Oh, I'll also see some decreased load from indexing.  I'm not sure how much.  If indexes is spread evenly across all the documents and everything behaves like my test index (ha!) then it'll be 15%.
</comment><comment author="nik9000" created="2014-03-03T17:26:19Z" id="36534437">Is this too nasty a hack to be worth it?  I'd love to be able to work more on this or something with similar goals but I don't think it is worth it for me to do anything else without a review from someone closer to the project.
</comment><comment author="jpountz" created="2014-03-04T00:33:27Z" id="36579060">I like this feature! You are exploring it in the context of highlighting but I think computing term vectors dynamically could be very useful to the term vectors API as well.

Maybe this pull request should be splitted into smaller pull requests in order to help get the change in. I think there are at least 3 different changes:
1. add the ability to compute term vectors on the fly (useful for both highlighting and the term vectors API)
2. add an option to the string field mapper to only store term vectors on large fields to save index space
3. add the ability to compute offsets on the fly

I think 2. is more tricky than it sounds because if a single field has two values, one that is large and the other one that is small, then either none of these fields or both of them should have term vectors.

Regarding 3. I'm still wondering whether it should be done. I like the postings highlighter because it is fast and I'm not sure I want to enable users to have bad (slow) experience with it because of dynamically-generated offsets. :-) (I'm less concerned about on-the-fly term vectors since term vectors are already slow).

Implementation-wise, I see that you reimplemented some indexing logic. Maybe a clean way to do that would be to contribute term vectors support to MemoryIndex?
</comment><comment author="nik9000" created="2014-03-04T13:59:20Z" id="36625869">&gt; Maybe this pull request should be splitted into smaller pull requests in order to help get the change in. I think there are at least 3 different changes:

Fine by me.  I'll do that soonish.

&gt; I think 2. is more tricky than it sounds because if a single field has two values, one that is large and the other one that is small, then either none of these fields or both of them should have term vectors.

Yeah.  What I've got works for my use case because my multi-valued fields are all short string.  I imagine that is pretty common.  But it works accidentally which isn't good.

&gt; Regarding 3. I'm still wondering whether it should be done. I like the postings highlighter because it is fast and I'm not sure I want to enable users to have bad (slow) experience with it because of dynamically-generated offsets. :-) (I'm less concerned about on-the-fly term vectors since term vectors are already slow).

The value in computing offsets on the fly is to see what the postings highlighter would do with your document.  Its not something you'd want to do all the time like you might want with the term vectors.  Its "easy" to jam it into the term vector computing logic but it adds another hack to maintain.

I think If I can convince you it is a good idea to do this we should combine the proposed PR number 3 and number 1.  It'd make my life easier.

&gt; Implementation-wise, I see that you reimplemented some indexing logic. Maybe a clean way to do that would be to contribute term vectors support to MemoryIndex?

I started with MemoryIndex!  It spits out term vectors no problem.  I dropped it in favor of that hand written storage (which is I cribbed from MemoryIndex anyway) for a few reasons:
1.  MemoryIndex doesn't hook into Elasticsearch's page caching infrastructure and it does make big arrays of bytes and ints.  Probably not insurmountable.
2.  Limiting the terms that you store vectors for is curcial from a performance perspective.  It felt cleaner to do it in the hand rolled code then by adding a term filter.  Maybe that is just craziness.  I think I could squeeze more performance out of it this way, especially if I mess around with reusing the BytesRefHash across separate highlighted fields and use that Attribute that spits out the term with the with the hashcode.
3.  MemoryIndex wants to sort terms which is technically correct but not required for either highlighter.  This isn't a big deal for documents with few terms or if you are limiting the vectors to just a few terms.  I have decent sized documents and hadn't implemented the filter yet so MemoryIndex looked like it was really getting in the way. Maybe it wasn't and filtering the terms would have been better.

I was wondering if it'd be useful to port IntBlockPool to BigArrays, maybe in such a way that I could submit it back to Lucene.  I know it is used in a bunch of places but I'm not sure how big a hot spot they are.  It is involved in term vector writing....

Did I mention that only storing term vectors for large fields speeds up indexing quite a bit compared to blindly storing them for everything?  Its pretty cool.
</comment><comment author="jpountz" created="2014-03-04T16:25:56Z" id="36642836">&gt; Limiting the terms that you store vectors for is curcial from a performance perspective.
&gt;  MemoryIndex wants to sort terms

Good points, I can definitely imagine how this helps performance. On the other hand, using MemoryIndex is appealing since it would make this feature very easily maintainable. @s1monw what do you think?

&gt; I was wondering if it'd be useful to port IntBlockPool to BigArrays, maybe in such a way that I could submit it back to Lucene. I know it is used in a bunch of places but I'm not sure how big a hot spot they are. It is involved in term vector writing....

Maybe we could just change the page size of `BigArrays` from 16KB to 32KB so that it can be used to write allocators for `IntBlockPool`?

&gt; Did I mention that only storing term vectors for large fields speeds up indexing quite a bit compared to blindly storing them for everything? Its pretty cool.

I'm not surprised, term vectors are very expensive! (it's a bit crazy that highlighters use them)
</comment><comment author="nik9000" created="2014-03-14T15:57:40Z" id="37663987">&gt; Good points, I can definitely imagine how this helps performance. On the other hand, using MemoryIndex is appealing since it would make this feature very easily maintainable. @s1monw what do you think?

I'm wondering where to go from here.  I'll do more work on this, but I'd like some advice on what is next.  Should I concentrate on making the lying readers less deceitful?  Should I look at plugging this into the term vector api?

&gt; Maybe we could just change the page size of BigArrays from 16KB to 32KB so that it can be used to write allocators for IntBlockPool?

I made an XIntBlockPool which works OK.  Certainly not perfect. 
</comment><comment author="nik9000" created="2014-03-31T17:25:18Z" id="39115970">Abandoning in favor of getting this Elasticsearch plugin released which has this:  https://github.com/nik9000/expiremental-highlighter
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/termvector/TermVectorWriter.java</file><file>src/main/java/org/elasticsearch/index/termvectors/ShardTermVectorService.java</file><file>src/test/java/org/elasticsearch/action/termvector/GetTermVectorTests.java</file></files><comments><comment>Term Vectors API: Computes term vectors on the fly if not stored in the index.</comment></comments></commit></commits></item><item><title>Allow using the FVH and Postings Highlighter without storing extra data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5183</link><project id="" key="" /><description>It'd be cool to be able to force the fvh or postings highlighter even if you haven't stored the data required for them to run.  Elasticsearch would re-analyze the field and twist the results appropriately for the highlighter.

Further, it'd be cool if you could set a minimum field length for which to save term vectors.  Combine this with the above and you the fvh actually gets much faster on small fields.  Not as fast as the plain highlighter or the postings highlighters, but if you are addicted to other fvh features it is nice.  Also, this allows you to hit time cpu cost for index time cpu and disk space by cranking that size up beyond where it is more efficient to re-analyze.  Again, this is really for folks addicted to the fvh.
</description><key id="27917375">5183</key><summary>Allow using the FVH and Postings Highlighter without storing extra data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-19T22:52:27Z</created><updated>2014-03-31T17:25:46Z</updated><resolved>2014-03-31T17:25:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-31T17:25:46Z" id="39116027">Abandoning in favor of getting this Elasticsearch plugin released which has this:  https://github.com/nik9000/expiremental-highlighter
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Suggest] Bug when mapping have a source field at the higher level of the document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5182</link><project id="" key="" /><description>Hello,

I was trying to play with the suggest feature and get a headache when my first basic example was working but my complexe one wasn't giving results at all. 
I just figured out that when you have a source field at the higher level of your mapping/document, the suggest feature doesn't return anything. 

Here is my mapping which the suggest feature doesn't work : 

```
{
  "source" : {
    "properties" : {
      "domain" : {
        "type" : "completion",
        "analyzer" : "autocomplete",
        "payloads" : true,
        "preserve_separators" : false,
        "preserve_position_increments" : false,
        "max_input_len" : 50
      },
      "forum" : {
        "type" : "completion",
        "analyzer" : "autocomplete",
        "payloads" : true,
        "preserve_separators" : false,
        "preserve_position_increments" : false,
        "max_input_len" : 50
      },
      "source" : {
        "properties" : {
          "name" : {
            "type" : "completion",
            "analyzer" : "autocomplete",
            "payloads" : true,
            "preserve_separators" : false,
            "preserve_position_increments" : false,
            "max_input_len" : 50
          }
        }
      }
    }
  }
}
```

Here is my new working one. I just replaced source by toto. 

```
{
  "source" : {
    "properties" : {
      "domain" : {
        "type" : "completion",
        "analyzer" : "autocomplete",
        "payloads" : true,
        "preserve_separators" : false,
        "preserve_position_increments" : false,
        "max_input_len" : 50
      },
      "forum" : {
        "type" : "completion",
        "analyzer" : "autocomplete",
        "payloads" : true,
        "preserve_separators" : false,
        "preserve_position_increments" : false,
        "max_input_len" : 50
      },
      "toto" : {
        "properties" : {
          "name" : {
            "type" : "completion",
            "analyzer" : "autocomplete",
            "payloads" : true,
            "preserve_separators" : false,
            "preserve_position_increments" : false,
            "max_input_len" : 50
          }
        }
      }
    }
  }
}
```
</description><key id="27916378">5182</key><summary>[Suggest] Bug when mapping have a source field at the higher level of the document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">loicbertron</reporter><labels /><created>2014-02-19T22:37:24Z</created><updated>2014-07-11T10:19:15Z</updated><resolved>2014-07-11T10:19:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-20T08:26:41Z" id="35598294">Hey,

can you please create a reproducible example instead of only providing your mappings (there is also no information about the analyzer being used)? It is hard to see, what is actually failing this way, maybe it is just a bad query/analyze setup... I tried the following and it works, but things may be very different in your example

```
curl -X DELETE Localhost:9200/foo

curl -X PUT Localhost:9200/foo
curl -X PUT Localhost:9200/foo/source/_mapping -d '
{
  "source" : {
    "properties" : {
      "domain" : {
        "type" : "completion",
        "analyzer" : "standard",
        "payloads" : true,
        "preserve_separators" : false,
        "preserve_position_increments" : false,
        "max_input_len" : 50
      },
      "forum" : {
        "type" : "completion",
        "analyzer" : "standard",
        "payloads" : true,
        "preserve_separators" : false,
        "preserve_position_increments" : false,
        "max_input_len" : 50
      },
      "source" : {
        "properties" : {
          "name" : {
            "type" : "completion",
            "analyzer" : "standard",
            "payloads" : true,
            "preserve_separators" : false,
            "preserve_position_increments" : false,
            "max_input_len" : 50
          }
        }
      }
    }
  }
}
'

curl -X PUT 'Localhost:9200/foo/source/1?refresh=true' -d '{
  "domain" : "foo",
  "forum" : "bar",
  "source" : {
    "name" : "baz"
  }
}'

curl -X POST localhost:9200/foo/_suggest -d '
{
  "test" : {
    "text" : "b",
    "completion" : {
      "field" : "source.name"
    }
  }
}'
```

Thanks!
</comment><comment author="clintongormley" created="2014-07-11T10:19:15Z" id="48715113">it is likely that this related to ambiguous field names. 

no more info from OP, closing this in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Breaking change to result.hits.hits?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5181</link><project id="" key="" /><description>Assume the following mapping:

```
"theIndex": {
      "mappings": {
         "doclib": {
            "properties": {
               "docFileName": {
                  "type": "string",
                  "term_vector": "with_positions_offsets",
                  "index_options": "offsets"
               }
           }
        }
     }
}
```

When _search is done against /theIndex/doclib/_search, docFileName returns a list (Array[]) instead of the expected string. Is this a breaking change and, if so, where was it documented?

Example query return using the Sense Chrome plugin:

```
{
   "took": 307,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 263,
      "max_score": 0.91934675,
      "hits": [
         {
            "_index": "c4study_1",
            "_type": "doclib",
            "_id": "Qx4-5KnNQ8m-Lr-sGOoMhA",
            "_score": 0.91934675,
            "fields": {
               "docFileName": [
                  "something.docx"
               ]
            },
     }
}
```
</description><key id="27892924">5181</key><summary>Breaking change to result.hits.hits?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bscottm</reporter><labels /><created>2014-02-19T17:26:27Z</created><updated>2014-02-19T17:34:51Z</updated><resolved>2014-02-19T17:31:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bscottm" created="2014-02-19T17:27:35Z" id="35524314">Should have mentioned that I'm using 1.0.0.
</comment><comment author="dadoonet" created="2014-02-19T17:31:46Z" id="35524783">Yes. It's here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_return_values.html
</comment><comment author="bscottm" created="2014-02-19T17:34:51Z" id="35525112">Hmmm... cleverly disguised in plain sight. &lt;sigh!&gt;
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added support for aliases to index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5180</link><project id="" key="" /><description>This PR finalizes the work that has been done in #2739 towards adding support for aliases to index templates.

Aliases can now be specified when creating an index template as follows:

```
curl -XPUT localhost:9200/_template/template_1 -d '
{
    "template" : "te*",
    "settings" : {
        "number_of_shards" : 1
    },
    "aliases" : {
        "alias1" : {},
        "alias2" : {
            "filter" : {
                "term" : {"user" : "kimchy" }
            },
            "routing" : "kimchy"
        },
        "{index}-alias" : {}
    }
}
'
```

The `{index}` placeholder within the alias name will be replaced with the actual index name that the template gets applied to during index creation.

Closes #1825
</description><key id="27885275">5180</key><summary>Added support for aliases to index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Index Templates</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-19T15:55:08Z</created><updated>2016-08-30T08:12:23Z</updated><resolved>2014-03-06T10:26:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-23T17:33:23Z" id="35837272">I like it!. I think we miss one use case, where the index name is `logging-2012.01.01`, and we want to create an alias classed `alas-2012.01.01`. I can't come up with a simple way to design it, maybe allow to define a regular expression from the index name to extract? Something like: `{index:xxx}`?

Also, are we good with `{...}` as the replacement chars? Should it be `${..}`?
</comment><comment author="clintongormley" created="2014-02-24T10:56:46Z" id="35875613">&gt; Also, are we good with {...} as the replacement chars? Should it be ${..}?

We already use `"type": "{dynamic_type}"` in dynamic templates, so I'm fine with `{}` being reserved.

&gt; I think we miss one use case, where the index name is logging-2012.01.01, and we want to create an alias classed alas-2012.01.01. I can't come up with a simple way to design it, maybe allow to define a regular expression from the index name to extract? Something like: {index:xxx}?

I'm not understanding this use case.  Typically alias names are kinda hard coded, eg `last_3_months`, rather than being a direct rename of an index. (and i'm also struggling to see how one would specify this syntax with the current layout).

I suppose you could have something like `alias-{year}` so `index-2014-01` -&gt; `alias-2014`.  Perhaps:

```
{
  "aliases": {
      "alias-{\1}": {
           "regex": "index-(\d+).+"
      }
  }
}
```

So the presence of the `regex` key would trigger a regex replace on the alias name.  Not convinced that this is worth doing.

The other thing I'm missing is the ability to automatically remove older indices from aliases, eg the `last_3_months` use case.  Easier to do via cron though, so probably not needed.
</comment><comment author="uboness" created="2014-02-24T13:25:00Z" id="35885484">&gt; Also, are we good with {...} as the replacement chars? Should it be ${..}?

With query templates now in place, how about we move all our placeholders to mustache syntax (ie. `{{placeholder}}`)
</comment><comment author="kimchy" created="2014-02-24T13:55:23Z" id="35887833">@clintongormley actually, most of the cases for aliases in LS was not around "last-3-months", since Kibana solves it easily thought the main interaction that people have with it. What people were after was using aliases as a way to filter data, in which case, using the same alias created that follows the naming rules as the index name, but with a filter.
</comment><comment author="jordansissel" created="2014-02-24T14:52:31Z" id="35893171">@kimchy +1 to the use case you describe; kibana indeed solves the "last-N-months" problem. 

Templated aliases helps logstash (or any time-oriented indexing name) users get filtered aliases. In the past, we've given some hand-wavey answer "Well just set up a cron job to create aliases every day!" and that answer was lame. This feature is sweeeet.
</comment><comment author="jordansissel" created="2014-02-24T14:57:00Z" id="35893629">&gt; I think we miss one use case, where the index name is logging-2012.01.01, and we want to create an alias classed alas-2012.01.01

I agree some users might ask for this, but I'm not totally sure it's needed. The difference between `alas-2012.01.01` and `alas-logstash-2012.01.01` isn't much, though maybe some users would like to strip the "logstash-" part as you propose. I don't have any data to support what users might want; I will ask!
</comment><comment author="torrancew" created="2014-02-24T23:13:17Z" id="35952248">In a previous life, my department was subject to regular audits of the garden variety (SOX, PCI, SOC, etc). As such, I often find myself wondering if monthly/yearly aliases would have provided more operational effectiveness during audit season. I begin to imagine a wonderful fantasy land where I simply run (or even cron) a custom script that queries the desired subset of logs via an alias, and outputs a report that is in the format the auditor desires. It's great - people are cheering, audits practically run themselves, and everything is magical.

Then, I wake up. I realize that every audit I went through during that time ended in precisely the same way - the auditor staring over my shoulder as I show them the log visualization tool's interface with the query I promise him or her is accurate for their question. Eventually the auditor says, "Thank you, now please e-mail me the **full screenshot of this**". I tried many times, but never succeeded, in getting them to accept other, more easily automatable formats.

As such, I'm on the fence about the "last-N-months" problem - I see a theoretical value to it, but I myself have never worked in an environment where that theoretical value would be realized or preferred to what Kibana provides. Likewise, if you're already writing a custom script for such a report, it would not be much more effort to automate the generation of the index list (though I don't know if you'd risk overflowing the max URI length at some point).

All in all, I'm +1 to the feature in general (adding filtered aliases via mapping is something I've been longing for for a while now), and have no real strong opinion on the "last-N-months/last-year" use case - especially when I consider the amount of resources such a query would be likely to consume.
</comment><comment author="javanna" created="2014-02-28T23:00:46Z" id="36403629">Thanks for your comments guys, seems like we are on the right track here. Currently the alias name needs to contain the whole index name, and it seems to me that adding the ability to use only a part of it wouldn't buy much compared to how it would complicate the syntax.

I lean towards keeping things simple for now, unless we find a magic syntax other than regexes :)
</comment><comment author="jordansissel" created="2014-02-28T23:07:32Z" id="36404048">&gt; unless we find a magic syntax other than regexes

If we find such a magic syntax, I would love to purge regexes from my life entirely ;)
</comment><comment author="kimchy" created="2014-03-05T10:50:24Z" id="36730514">code looks great!
</comment><comment author="stevencdavis" created="2014-03-05T16:20:30Z" id="36760199">&gt; I think we miss one use case, where the index name is logging-2012.01.01, and we want to create an alias classed alas-2012.01.01

Our use case is similar to this -- we have versioned indices named per month (e.g. `projectA-2014.01__v1`, `projectA-2014.02__v1`), and we create aliases to make it easy to query a whole year or all of our documents for a particular project (e.g. aliases `projectA-2014`, `projectA`) and also to allow us to use the aliases as a means of updating index versions without downtime (i.e. by allowing the application level to use a particular alias while swapping out `projectA-...__v1` for `projectA..._v2` -- I believe this is a best-practice).  This allows us to logically group indices and to keep our data and processing versioned with zero downtime.  Also, last-N-months wouldn't work for us because we have many projects (e.g. `projectA`, `projectB`, etc.), and we create aliases for each of these.  

Currently, we have a cron job that checks periodically to see if any new indices have been created that need additional aliases, but obviously this is a bit hacky.  Am I missing an obvious alternative method, or is this really not a common approach? 
</comment><comment author="javanna" created="2014-03-06T10:09:54Z" id="36841241">Hi @stevencdavis, it is a common approach, the problem here is finding the right syntax to be able to use partial indices names as part of alias names. We'll get this PR merged as it seems to be a good start and give these other requirements some more thoughts, we can always improve this later on.
</comment><comment author="javanna" created="2014-03-06T10:26:19Z" id="36842394">Merged, thanks a lot @jbrook !
</comment><comment author="chetandhembre" created="2016-08-30T08:12:01Z" id="243365479">Hey.. Is there any changes in this after this PR?
I have per day base index from logstash like `test-2016-08-29` / `test-2016-08-30`. I want to create aliase like `test`.  I am doing this because I do not want to use logstash created index name in my application code. 
above PR helps you to add index name i aliase but not partial name of it.
Is there way to do it? or do I have alternative approach to my problem?
Thanks you.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>date_histogram against empty index results in ArrayIndexOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5179</link><project id="" key="" /><description>Hi,

I am using 1.0.0. and see:
&lt;code&gt;
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[kwtJyjumSJ-DRx68nXaVhw][data][1]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][0]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][4]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][3]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][2]: ArrayIndexOutOfBoundsException[0]}]",
  "status" : 500
}
&lt;/code&gt;

when running date_histogram query. It happens when I run the query on the whole index while index is empty, or when I run the query on some type which has no documents (while other types in the index have docs).

Steps to reproduce:

&lt;pre&gt;
&lt;code&gt;
curl -XDELETE 'http://localhost:9200/_all/'
&lt;/code&gt;
&lt;/pre&gt;

&lt;pre&gt;
&lt;code&gt;
curl -XPUT 'http://localhost:9200/data/' -d '{    "mappings" : {
      "_default_" : {
        "_source" : { "enabled" : false },
        "_timestamp" : { "enabled" : true, "store" : true },
        "_all" : {"enabled" : false},
        "properties" : {
          "timestamp" : { "type" : "date", "index" : "not_analyzed", "store": "true" },
          "message" : { "type" : "string", "index" : "analyzed", "analyzer" : "standard", "store": "true" },
          "type" : { "type" : "string", "index" : "not_analyzed", "store": "true" }
        }
      }
    }    
}'
&lt;/code&gt;
&lt;/pre&gt;

&lt;pre&gt;
&lt;code&gt;
curl "http://localhost:9200/data/_search?fields=*&amp;pretty" -d '{"query":{"filtered":{"filter":{"range":{"timestamp":{"gte":1392026400000,"lte":1392631200000}}},"query":{"query_string":{"query":"mesage:*"}}}},"aggs":{"data":{"date_histogram":{"field":"timestamp","interval":"60m"},"aggs":{"data_type":{"terms":{"field":"_type"}}}}}}'
&lt;/code&gt;
&lt;/pre&gt;

</description><key id="27878370">5179</key><summary>date_histogram against empty index results in ArrayIndexOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">bsmid</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-19T14:29:04Z</created><updated>2015-06-07T23:15:01Z</updated><resolved>2014-02-21T02:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-02-21T01:49:11Z" id="35691396">Hi @bsmid,
This is indeed a bug, but the cause is different than what you describe (also based on your recreation above). It's caused when running a `histogram` or `date_histogram` on unmapped fields and then adding a sub-aggregation to them.

In the example above, even though you created a default mapping, you didn't actually index anything and no concrete mappings were created on the index. 

In any case... I'll fix it soon...

Thx for reporting!

ps, you can remove the `"index" : "not_analyzed"` from the `timestamp` field... this setting only applies to `string` types
</comment><comment author="bsmid" created="2014-02-21T06:36:30Z" id="35702918">Thx for fixing @uboness !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/CombiTests.java</file></files><comments><comment>Fixed an issue where and IndexOutOfBoundsException was thrown when a date_/histogram aggregation was defined on unmapped field and also had a sub aggregation. The root cause there was that in such case, the estimated bucket count was 0, and the code was not designed to handle that well.</comment></comments></commit></commits></item><item><title>Warning: "Message not fully read (request)" @ transport.netty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5178</link><project id="" key="" /><description>Hi there,
Im running an elasticsearch cluster with 2 nodes containing about 6 million events. Today both nodes started to throw the following warning:

[2014-02-19 13:45:07,633][WARN ][transport.netty          ] [Node1-TI] Message not fully read (request) for [1920] and action [], resetting
[2014-02-19 13:45:07,633][WARN ][transport.netty          ] [Node2-TI] Message not fully read (request) for [1921] and action [], resetting

{
  "cluster_name" : "Cluster-TI",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 20,
  "active_shards" : 40,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0
}

elasticsearch version: 1.0
java version: jdk1.7.0_51 / JRE 1.7.0_51-b13 on both nodes
OS: RHEL6.5 x64
config: default, except unicast between nodes, loglevel=debug and heap size configuration
ES_HEAP_SIZE="8g"
JAVA_OPTS="-server -d64 $JAVA_OPTS"

Any suggestions what produces the mentioned warning message? Apart from that, cluster is running fine.
</description><key id="27872851">5178</key><summary>Warning: "Message not fully read (request)" @ transport.netty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bluesmanshoes</reporter><labels /><created>2014-02-19T12:59:36Z</created><updated>2014-08-18T13:41:24Z</updated><resolved>2014-03-11T15:07:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-20T08:09:31Z" id="35597367">Hey,

did you see any exceptions in the logs or only these messages? Anything more which might help? Did any network outage happen or was there an exception (like a OutOfMemoryException) happening anytime before that event?

Can you paste the output of `curl 'localhost:9200/_nodes/jvm?pretty'` please?

For testing, you could also remove the JAVA_OPTS, to make sure they dont have any effect (those settings might be redundant anyway, see http://docs.oracle.com/javase/7/docs/technotes/guides/vm/server-class.html) 
</comment><comment author="bluesmanshoes" created="2014-02-20T17:13:11Z" id="35644867">Hi, and thanks for answering. I will post an extract of the log file later. Here is the output from the curl command:

```
{
  "cluster_name" : "Cluster-TI",
  "nodes" : {
    "Sgfj_-XCSbG6jaSGIQiH2A" : {
      "name" : "Node2-TI",
      "transport_address" : "inet[/xx.xx.xx.36:9300]",
      "host" : "xxxxxxxxxx",
      "ip" : "xx.xx.xx.36",
      "version" : "1.0.0",
      "build" : "a46900e",
      "http_address" : "inet[/xx.xx.xx.36:9200]",
      "attributes" : {
        "master" : "true"
      },
      "jvm" : {
        "pid" : 21491,
        "version" : "1.7.0_51",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "24.51-b03",
        "vm_vendor" : "Oracle Corporation",
        "start_time" : 1392901349365,
        "mem" : {
          "heap_init_in_bytes" : 8589934592,
          "heap_max_in_bytes" : 8555069440,
          "non_heap_init_in_bytes" : 24313856,
          "non_heap_max_in_bytes" : 136314880,
          "direct_max_in_bytes" : 8555069440
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Par Eden Space", "Par Survivor Space",                                                                                         "CMS Old Gen", "CMS Perm Gen" ]
      }
    },
    "KNvD27sBQFOGvvS3nZ_FTA" : {
      "name" : "Node1-TI",
      "transport_address" : "inet[/xx.xx.xx.35:9300]",
      "host" : "xxxxxxxxxxx",
      "ip" : "xx.xx.xx.35",
      "version" : "1.0.0",
      "build" : "a46900e",
      "http_address" : "inet[/xx.xx.xx.35:9200]",
      "attributes" : {
        "master" : "true"
      },
      "jvm" : {
        "pid" : 22672,
        "version" : "1.7.0_51",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "24.51-b03",
        "vm_vendor" : "Oracle Corporation",
        "start_time" : 1392901447100,
        "mem" : {
          "heap_init_in_bytes" : 8589934592,
          "heap_max_in_bytes" : 8555069440,
          "non_heap_init_in_bytes" : 24313856,
          "non_heap_max_in_bytes" : 136314880,
          "direct_max_in_bytes" : 8555069440
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Par Eden Space", "Par Survivor Space",                                                                                         "CMS Old Gen", "CMS Perm Gen" ]
      }
    }
  }
}
```
</comment><comment author="bluesmanshoes" created="2014-02-20T17:19:41Z" id="35645553">The first Time it starts after an gc (see below), stopped after restarting the process, while gc where ongoing.

```
[2014-02-17 08:13:15,203][WARN ][monitor.jvm              ] [SPortalCN1-TI] [gc][young][64545][79] duration [1.2s], collections [1]/[1.9s], total [1.2s]/[1.1m], memory [2.9gb]-&gt;[3gb]/[7.9gb], all_pools {[young] [166mb]-&gt;[6.7mb]/[266.2mb]}{[survivor] [33.2mb]-&gt;[33.2mb]/[33.2mb]}{[old] [2.8gb]-&gt;[3gb]/[7.6gb]}
```

Second time it starts after the following parse failure, but restart didn't solved the problem this time.

```
org.elasticsearch.search.SearchParseException: [events][1]: from[90],size[10]: Parse Failure [Failed to parse source [{"from":90,"size":10,"sort":[{"lastResponse":{"order":"desc"}}]}]]
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:586)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:489)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:474)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:467)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$4.run(TransportSearchTypeAction.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.search.SearchParseException: [events][1]: from[90],size[10]: Parse Failure [No mapping found for [lastResponse] in order to sort on]
        at org.elasticsearch.search.sort.SortParseElement.addSortField(SortParseElement.java:198)
        at org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:172)
        at org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:80)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:574)
        ... 11 more
```
</comment><comment author="bluesmanshoes" created="2014-02-24T10:57:15Z" id="35875654">The warnings now stopped for no particular reason (no restart, nothing)...
</comment><comment author="spinscale" created="2014-02-24T11:02:10Z" id="35875954">do you have rolling indices and queries against those (so that one specific index is not queried anymore)?
</comment><comment author="meconlin" created="2014-03-10T21:37:25Z" id="37237717">I just updated my cluster to 1.0.0 and am having this same issue, message comes up every 5 seconds. 

```
2014-03-10 17:35:57,894[WARN ][transport.netty          ] [vel-hfs-1-1] Message not fully read (request) for [31614711] and action [], resetting
```

All of my nodes were upgraded to 1.0.0. 
All JVMs are running on Java 1.7.

Any thoughts?
</comment><comment author="clintongormley" created="2014-03-11T10:05:38Z" id="37279613">Please post the output of:

```
curl -XGET "http://localhost:9200/_nodes/jvm?pretty"
```
</comment><comment author="meconlin" created="2014-03-11T12:44:22Z" id="37290971">sure : [GIST HERE](https://gist.github.com/meconlin/9484829)

You will notice all 54 nodes are using 1.7.0_09 and all are 1.0.0. 
More than 12 hours later and one restart and still getting netty WARNs in the long. Somebody is chatting some nonsense on this cluster!
</comment><comment author="clintongormley" created="2014-03-11T13:44:28Z" id="37296212">Do you have any Java transport clients which are querying the cluster? Are they the same version?

Could you post some of the lines from the log file with a bit more context?

ta
</comment><comment author="meconlin" created="2014-03-11T14:48:20Z" id="37303626">@clintongormley, Thanks, you were right! Found it with a little tcpdump, had an old rexster process on another dev box hammering away on my cluster. 
</comment><comment author="clintongormley" created="2014-03-11T15:07:10Z" id="37306000">Splendid
</comment><comment author="rahst12" created="2014-05-15T00:55:40Z" id="43158180">I'm getting exactly the same warning out.  I don't have tcpdump installed on my cluster..  Any suggestions for figuring this?
</comment><comment author="rahst12" created="2014-05-15T01:02:00Z" id="43158508">Ha.. Well no help to anyone else, but I found a rogue server in the cluster, so I turned it off.  By rogue it was decommissioned a couple elasticsearch upgrades ago, and by mistake got rebooted, and thus the service started by default and it kept trying to join.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GetFieldMappings API will not return field mapping if the index is not hosted on the node executing it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5177</link><project id="" key="" /><description>The current implementation of the GetFieldMapping API uses information from the index service which is only available on a node if that node actively hosts a shard from that index. If the information is missing the call will act as if the type/field was not found and will not return information for it.

During a rolling upgrade from &lt;= 0.90.11 or 1.0.0 the get field mapping api might fail. This has to do with the way this issue has been fixed. The way how internally the request got handled has changed in order completely to fix it properly at the cost that during a rolling upgrade this api may fail.
</description><key id="27868525">5177</key><summary>GetFieldMappings API will not return field mapping if the index is not hosted on the node executing it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-19T11:43:21Z</created><updated>2014-02-24T15:38:17Z</updated><resolved>2014-02-24T15:38:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="darkwarriors" created="2014-02-20T16:23:17Z" id="35639220">any news on this bug? we are facing the same error by upgrading ES from .90 to 1.0.

Regards
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/test/java/org/elasticsearch/indices/mapping/SimpleGetFieldMappingsTests.java</file></files><comments><comment>Change GetFieldMapping API to broadcast requests to nodes hosting the relevant indices.</comment></comments></commit></commits></item><item><title>Added script to extract release notes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5176</link><project id="" key="" /><description>Just call ./es_release_notes.pl &lt;issue-tag&gt; to get all release notes.

By default html output is returned, but you can switch to markdown by calling
./es_release_notes.pl &lt;issue-tag&gt; markdown
</description><key id="27868103">5176</key><summary>Added script to extract release notes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-02-19T11:35:21Z</created><updated>2014-06-20T03:55:44Z</updated><resolved>2014-02-19T12:44:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting on a wildcard field name uses the same highlighter for all fields that match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5175</link><project id="" key="" /><description>`HighlightPhase.hitExecute(SearchContext context, HitContext hitContext)`

Assume that the field object represents a wildcard field (i.e. field.field() returns "*")
Assume that the index contains two fields:
field_1 with "index_options":"freq"
field_2 with "index_options":"offsets"
Assume that a highlighter has not been explicitly specified (i.e. `field.highlighterType()==null`)

The object fieldNamesToHighlight will contain both of the above field names.
The first pass through the for (String fieldName : fieldNamesToHighlight) loop will evaluate the statement: `if (field.highlighterType() == null){}` to true.
The block will evaluate the index_options of the first field (from a HashSet, so essentially random choice) to determine the appropriate highlighter. The highlighter will then be set using `field.highlighterType("postings")`.

Subsequent executions of the fieldNamesToHighlight for loop will evaluate the statement: `if (field.highlighterType() == null){}` to false, based on the settings from the first execution.

Result: The highlighter chosen for the (arbitrary) first field will be used for all subsequent fields. If the selected highlighter is not the plain highlighter then there is the potential for the code to throw an IllegalArgumentException due to a mismatch between the highlighter and the indexing options of the field.

The solution is to treat the user provided configuration (the field object) as immutable. For each iteration of the fieldNamesToHighlight loop, extract field.highlighterType() to a local variable. Test that local variable for null - and set to a selected highlighter as appropriate.
</description><key id="27867210">5175</key><summary>Highlighting on a wildcard field name uses the same highlighter for all fields that match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ccw-morris</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-19T11:18:28Z</created><updated>2014-02-24T23:27:45Z</updated><resolved>2014-02-24T23:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ccw-morris" created="2014-02-19T11:55:46Z" id="35490936">For completeness, I can reproduce it with the following commands:

```
curl -XPUT 'http://localhost:9202/test' -d '{ "mappings": { "document": { "properties": { "field1": { "type":"string" }, "field2": { "type":"string", "index_options":"offsets" } } } } }'
curl -XPUT 'http://localhost:9202/test/document/1' -d '{ "field2":"This is the first field", "field1":"This is the second field" }'
curl -XPOST 'http://localhost:9202/_search' -d '{ "query": { "term": { "field2": "field" } }, "highlight": { "fields": { "field*": {} } } }'
```

However, please note that the bug relies on a specific ordering of a HashSet of String objects (field names). To reproduce it you may have to modify which field name has postings. Also, you may have to modify the regular expression that selects the fields: "field_" and "_" seems to produce different results. On production I can get the error with "*", but not with the test scripts above.
</comment><comment author="javanna" created="2014-02-20T09:52:57Z" id="35603770">Great catch @ccw-morris , will fix this!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/test/java/org/elasticsearch/search/highlight/CustomHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Made SearchContextHighlight.Field class immutable to prevent from erroneously updating it, as it doesn't necessarily map to a single field</comment></comments></commit></commits></item><item><title>Wildcard * support in field name for Filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5174</link><project id="" key="" /><description>For wildcard in field name, currently we can have
{
    "query_string" : {
        "fields" : ["city.*"],
        "query" : "this AND that OR thus",
        "use_dis_max" : true
    }
}
It would be nice if there is also a "fields" option like this for filter, so that one can filter on the inner object.

"filter" : {
            "terms" : { "user" : ["kimchy", "elasticsearch"]}
}

Turning "user" to "user.*" wouldn't work currently.
</description><key id="27854286">5174</key><summary>Wildcard * support in field name for Filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lordaugustus</reporter><labels><label>discuss</label></labels><created>2014-02-19T06:47:19Z</created><updated>2015-04-10T16:37:32Z</updated><resolved>2015-04-10T16:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-02-20T18:10:49Z" id="35651180">Currently `terms` filter can accept only one field. We are considering changing this in https://github.com/elasticsearch/elasticsearch/pull/5137. We can probably address this issue there as well. 
</comment><comment author="stormizin" created="2014-05-09T21:59:21Z" id="42718455">+1
</comment><comment author="pquentin" created="2015-04-10T14:43:47Z" id="91580047">To make things clear, #5137 was merged, and supporting wildcard in field names was abandoned because it was out of the scope of the PR. clintongormley provided a nice overview of the difficulties this would pose: https://github.com/elastic/elasticsearch/pull/5137#issuecomment-42740126.
</comment><comment author="dakrone" created="2015-04-10T16:37:32Z" id="91610844">We discussed this and agree that this would be very complex and should not be changed for the reasons specified in https://github.com/elastic/elasticsearch/pull/5137#issuecomment-42740126
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add third party license generation profile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5173</link><project id="" key="" /><description>Create a profile to expose http://mojo.codehaus.org/license-maven-plugin/usage.html.  That is used to generate third party license file
</description><key id="27827810">5173</key><summary>Add third party license generation profile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-02-18T21:06:45Z</created><updated>2014-07-16T21:48:24Z</updated><resolved>2014-02-20T18:12:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrsolo" created="2014-02-20T18:12:02Z" id="35651277">Pushed https://github.com/elasticsearch/elasticsearch/commit/db57f7ed0eacc21b7fca743e65fdc3c0582078a0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for combining fields to the unified highlighter (matched_fields)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5172</link><project id="" key="" /><description>Like the FVH can combine fields, it'd be nice to be able to combine fields in the postings highlighter.  See #3750 for the FVH.  See https://issues.apache.org/jira/browse/LUCENE-4652 for the Lucene twin.
</description><key id="27824800">5172</key><summary>Add support for combining fields to the unified highlighter (matched_fields)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2014-02-18T20:26:23Z</created><updated>2017-06-05T15:06:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-31T17:25:59Z" id="39116052">Abandoning in favor of getting this Elasticsearch plugin released which has this:  https://github.com/nik9000/expiremental-highlighter
</comment><comment author="ajhalani" created="2014-09-11T01:04:37Z" id="55206347">So the lucene ticket mentioned is resolved, does this mean it can be exposed in ElasticSearch as well without the need of the "experiemental-highlighter" plugin ? Would be nice :)
</comment><comment author="clintongormley" created="2016-11-06T12:23:31Z" id="258677598">This should be doable now that we're using Lucene's postings highlighter instead of our own custom version.
</comment><comment author="nik9000" created="2017-06-05T15:06:12Z" id="306211859">Now that we're dropping the postings highlighter in favor of the unified highlighter, I think this should be about the unified highlighter instead.</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cat api help</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5171</link><project id="" key="" /><description>```
# curl http://web245:9200/_cat
=^.^=
```

That would be great to list available endpoints with brief descriptions by calling something like `/_cat?help` or `/_cat/help` or even maybe just `/_cat`.

Just like `git` without args (or `git help`) shows possible options.
</description><key id="27822210">5171</key><summary>Cat api help</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-02-18T19:52:27Z</created><updated>2014-02-18T20:00:32Z</updated><resolved>2014-02-18T20:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-18T19:56:45Z" id="35426202">you can do `/_cat?h` which will show the list of endpoints, I added support for `/_cat?help` as well.
</comment><comment author="drewr" created="2014-02-18T19:59:28Z" id="35426475">Planning to make `/_cat` do this as well, described in #5106.
</comment><comment author="bobrik" created="2014-02-18T20:00:32Z" id="35426600">Sorry, missed #5106.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix SearchContext occasionally closed prematurely </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5170</link><project id="" key="" /><description>PR for #5165
</description><key id="27822054">5170</key><summary>Fix SearchContext occasionally closed prematurely </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jaymode</reporter><labels><label>:Search</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-18T19:50:29Z</created><updated>2015-06-07T23:19:51Z</updated><resolved>2014-02-19T10:28:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-18T20:43:41Z" id="35431195">I left one comment but in general this looks great... Can you maybe change the commit message to follow this pattern:

```
Short description (80 chars max)

Long description over multiple lines (80 Chars per line)

Closes #5165
```

thanks so much!
</comment><comment author="jaymode" created="2014-02-18T21:03:47Z" id="35433458">@s1monw just updated per your comments regarding the SearchService and the commit message
</comment><comment author="s1monw" created="2014-02-18T21:18:25Z" id="35435166">looks good I will try merging this in tomorrow!
</comment><comment author="s1monw" created="2014-02-19T10:28:09Z" id="35484768">pushed thanks so much
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>remove RoutingAllocation.Result from cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5169</link><project id="" key="" /><description>We should remove `RoutingAllocation.Result` from the cluster state, as it doesn't really provide any useful data, and just inflates the size of the cluster state.
</description><key id="27820642">5169</key><summary>remove RoutingAllocation.Result from cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-18T19:32:43Z</created><updated>2014-03-21T10:04:55Z</updated><resolved>2014-02-27T17:12:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/RerouteExplanation.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocation.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingExplanations.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocationCommands.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/Decision.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/NodeVersionAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/RebalanceOnlyWhenActiveAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ReplicaAfterPrimaryActiveAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/reroute/RestClusterRerouteAction.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file></files><comments><comment>Add `explain` flag support to the reroute API</comment></comments></commit></commits></item><item><title>Issue 5164: Deleting all indices documentaion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5168</link><project id="" key="" /><description>Pull request to fix:
https://github.com/elasticsearch/elasticsearch/issues/5164
</description><key id="27816644">5168</key><summary>Issue 5164: Deleting all indices documentaion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">constantijn</reporter><labels /><created>2014-02-18T18:37:17Z</created><updated>2014-06-26T22:25:18Z</updated><resolved>2014-02-19T09:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file></files><comments><comment>[DOCS] Javadoc: delete index request does not support empty String</comment></comments></commit></commits></item><item><title>Issue 5164: Deleting all indices documentaion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5167</link><project id="" key="" /><description>Also fixing the docs for DeleteIndexRequest while i'm at it. Issue:
https://github.com/elasticsearch/elasticsearch/issues/5164
</description><key id="27816617">5167</key><summary>Issue 5164: Deleting all indices documentaion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">constantijn</reporter><labels /><created>2014-02-18T18:36:46Z</created><updated>2014-07-10T09:27:42Z</updated><resolved>2014-02-19T09:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file></files><comments><comment>[DOCS] Javadoc: delete index request does not support empty String</comment></comments></commit></commits></item><item><title>Documentation: aggregations/facets are not executed in the context of top level filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5166</link><project id="" key="" /><description>When running top level aggregations filters are ignored, while query values are not. I am going by the following line from [the guide](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations.html): "(e.g. a top-level aggregation executes within the context of the executed query/filters of the search request)" in thinking that both queries and filters should affect top-level aggregate results. Test case below, tested against ES 1.0.0.

```
# Add data
curl -X PUT localhost:9200/test/test/2 -d '{ "term": "term1", "price":100}'
curl -X PUT localhost:9200/test/test/2 -d '{ "term": "term2", "price":100}'
curl -X PUT localhost:9200/test/test/3 -d '{ "term": "term3", "price":500}'

# Test with query
# aggregate correctly returns count of 2
curl -X GET localhost:9200/test/test/_search -d '{
  "query": {
    "range": {
      "price": {
        "from": 0,
        "to": 400
      }
    }
  },
  "aggregations": {
    "term": {
      "stats": {
        "field": "price"
      }
    }
  }
}'

# Test with filter
# aggregate incorrectly returns count of 3 (ignores filter)
curl -X GET localhost:9200/test/test/_search -d '{
  "filter": {
    "range": {
      "price": {
        "from": 0,
        "to": 400
      }
    }
  },
  "aggregations": {
    "term": {
      "stats": {
        "field": "price"
      }
    }
  }
}'
```

Hope this is useful, let me know if I can be of assistance!
</description><key id="27809894">5166</key><summary>Documentation: aggregations/facets are not executed in the context of top level filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Fizzadar</reporter><labels /><created>2014-02-18T17:08:44Z</created><updated>2014-07-31T10:56:40Z</updated><resolved>2014-07-31T10:56:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-18T19:26:02Z" id="35422692">a `post_filter` only affects the search but no the facets or aggregations see http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/search-request-post-filter.html you should rather us a filtered query in this case: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filtered-query.html
</comment><comment author="Fizzadar" created="2014-02-18T20:35:35Z" id="35430320">Thank you, I shall use filtered query. However - is the above functionality correct (where top-level `filter` doesn't affect aggregates)? If so I will make a pull request to the documentation as the quoted statement above is misleading. 
</comment><comment author="s1monw" created="2014-02-18T20:49:02Z" id="35431751">you mean because we state in the docs that this will not affect facets? I agree this should be updated and it is indeed misleading. We should update the docs to say facets and aggregations and it should also mention the filtering there: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-filter-aggregation.html maybe you can just change the topic of this issue to reflect that? @uboness @jpountz can you guys take care of this?
</comment><comment author="uboness" created="2014-02-18T20:53:46Z" id="35432248">sure... I'll add a note to the docs. Thx @Fizzadar / @s1monw
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Rewrote post-filter.asciidoc</comment></comments></commit></commits></item><item><title>SearchContext is occasionally closed prematurely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5165</link><project id="" key="" /><description>We have noticed that sometime all shards do not respond to a DFS query then fetch and we get only 4/5 shards responding (and in debugging noticed the same for query then fetch). Turning the logs to debug we see the following message when only 4/5 shards respond.

```
2014-02-18 00:01:19,574 DEBUG (elasticsearch[dev][search][T#2]) log4j.Log4jESLogger&lt;109&gt;: [dev] [17] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [17] 
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:455) 
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:279) 
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:236) 
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:148)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.run(TransportSearchDfsQueryThenFetchAction.java:132) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744) 
```

Looking into what causes this, I was able to reproduce the issue more quickly by setting the SearchService reaper thread to run almost continuously by explicitly setting "search.keep_alive_interval" to a low value in the milliseconds range vs every minute. (we do see the same behavior without modifying this value, but 

I saw two issues occur with some extra debugging. The first is that when SearchContext is created, the default value of lastAccessedTime is 0 and if the reaper runs against that context quickly enough, the context will be freed before it is used. 

```
2014-02-18 15:32:53,394 DEBUG (elasticsearch[dev][scheduler][T#1]) log4j.Log4jESLogger&lt;104&gt;: [dev] freeing search context 1390 time: 1392737573376 lastAccessTime: 0 keepAlive: 300000
2014-02-18 15:32:53,399 DEBUG (elasticsearch[dev][search][T#3]) log4j.Log4jESLogger&lt;109&gt;: [dev] [1390] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [1390]
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:455)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:279)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:236)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:148)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.run(TransportSearchDfsQueryThenFetchAction.java:132)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

The second the reaper calls context.lastAccessTime() multiple times, but the value can change after the first if statement and an incorrect value will be used in the next statement (such as -1 when the context is being used).

```
2014-02-18 15:24:38,721 DEBUG (elasticsearch[dev][scheduler][T#1]) log4j.Log4jESLogger&lt;104&gt;: [dev] freeing search context 1691 time: 1392737078619 lastAccessTime: -1 keepAlive: 300000
2014-02-18 15:24:38,725 DEBUG (elasticsearch[dev][search][T#4]) log4j.Log4jESLogger&lt;109&gt;: [dev] [1691] Failed to execute query phase
org.elasticsearch.search.SearchContextMissingException: No search context found for id [1691]
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:455)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:279)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:236)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:148)
        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.run(TransportSearchDfsQueryThenFetchAction.java:132)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

This [gist](https://gist.github.com/jaymode/9074150) contains code that I have used to resolve the issues. If this needs to be submitted as a pull request, I can do that as well.
</description><key id="27806388">5165</key><summary>SearchContext is occasionally closed prematurely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-18T16:29:00Z</created><updated>2014-09-08T09:15:28Z</updated><resolved>2014-02-19T10:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-18T17:01:13Z" id="35406471">I looked at the gist and I think you should submit this as a PR! Good catch! Can you make sure you sign the CLA as well so we can pull this in quickly. 

One comment about the initialization I think we should initialize the new context with `-1` instead so we just skip it on the reaper?
</comment><comment author="jaymode" created="2014-02-18T18:36:24Z" id="35417287">Will create a PR and sign the CLA shortly.

Regarding initialization, I thought about using `-1` but decided against it as there could be some unknown case, where lastAccessTime is always `-1`, so the search context will never be cleaned up by the reaper. Initializing to the estimated time allows it to be cleaned up by the reaper in that case. Let me know what you think.
</comment><comment author="s1monw" created="2014-02-18T18:48:10Z" id="35418596">@jaymode I agree on the `-1` and the special casing. We actually set it to `-1` each time we access the context. to prevent the reaper from closing the context while we are using it. I think we should do the same when initialising. There should be some logic that frees the context if there is an exception thrown too.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/test/java/org/elasticsearch/search/StressSearchServiceReaperTest.java</file><file>src/test/java/org/elasticsearch/test/TestCluster.java</file></files><comments><comment>Add test and randomization for #5165</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file></files><comments><comment>Fix SearchContext from being closed prematurely</comment></comments></commit></commits></item><item><title>Can't delete all indexes from the Java API in 1.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5164</link><project id="" key="" /><description>In 0.90.x i was able to delete all my indices from the java api by calling 

```
client.admin().indices().prepareDelete(new String[] {}).execute().actionGet();
```

However this fails in 1.0.0 with 

org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: index / indices is missing;
    at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)
    at org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest.validate(DeleteIndexRequest.java:72)
    _snip long stacktrace_

which points me to

```
public ActionRequestValidationException validate() {
    ActionRequestValidationException validationException = null;
    if (indices == null || indices.length == 0) {
        validationException = addValidationError("index / indices is missing", validationException);
    }
    return validationException;
}
```

So that's what now throws the error, however the documentation still says:

```
/**
 * Deletes an index based on the index name.
 *
 * @param indices The indices to delete. Empty array to delete all indices.
 */
DeleteIndexRequestBuilder prepareDelete(String... indices);
```

Is this a bug, or is there a new preferred way to delete all indices that isn't reflected yet in the documentation?

(In case anyone is wondering: I use this in my unit test test setup to make sure I have a clean slate every time).
</description><key id="27806238">5164</key><summary>Can't delete all indexes from the Java API in 1.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">constantijn</reporter><labels><label>docs</label></labels><created>2014-02-18T16:27:11Z</created><updated>2014-02-19T09:37:31Z</updated><resolved>2014-02-19T09:35:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-18T16:45:23Z" id="35404345">You need now to use `_all` if you want to remove all indices.

See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-delete-index.html

I think we should fix the Javadoc here. Wanna contribute and send a PR?
</comment><comment author="constantijn" created="2014-02-18T17:55:45Z" id="35412817">Sure, coming up first thing tomorrow morning :)
</comment><comment author="constantijn" created="2014-02-18T18:39:26Z" id="35417629">Done
(Ended up using the github website to make pull requests, turned into 2 pull requests since it didn't let me change 2 files in 1 web edited pull request)
</comment><comment author="kimchy" created="2014-02-18T19:53:07Z" id="35425778">@martijnvg should we validate here the zero length and fail at all?
</comment><comment author="constantijn" created="2014-02-19T08:30:42Z" id="35476335">@kimchy

```
client.admin().indices().prepareDelete("_all").get();
```

does look a lot more descriptive then 

```
client.admin().indices().prepareDelete(new String[] {}).get();
```

Which requires a peek at the docs to see passing an empty array means deleting all indices.

My 0.02
</comment><comment author="dadoonet" created="2014-02-19T08:33:22Z" id="35476465">@constantijn Thanks. Could you please sign the CLA: http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="constantijn" created="2014-02-19T08:55:10Z" id="35477798">@dadoonet Already done :)
</comment><comment author="dadoonet" created="2014-02-19T09:37:31Z" id="35480903">Rebased and Pushed. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file></files><comments><comment>[DOCS] Javadoc: delete index request does not support empty String</comment></comments></commit></commits></item><item><title>Extensible injection of cluster/node/network failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5163</link><project id="" key="" /><description>In parallel to thinking about improving master election we want to have a better way to run both randomized and specific tests for node disconnection, straggler behaviour and packet loss (as far as possible). A lot of scenarios can be run in-VM by injecting "faulty" transport behaviour and verifying node behaviour: queue/thread buildup, memory, timeout behaviour etc.

We want to focus on controlled &amp; logical (i.e. in-VM/simulated) failures first because even though simulating physical network problems is relatively easy (inject qdisc into network stack, iptables etc.) it would be several orders of magnitude slower to run.
</description><key id="27791347">5163</key><summary>Extensible injection of cluster/node/network failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>test</label></labels><created>2014-02-18T12:59:32Z</created><updated>2014-12-29T12:03:39Z</updated><resolved>2014-12-29T12:03:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-02-18T14:16:12Z" id="35387599">Would this help https://github.com/netty/netty/tree/master/transport/src/test/java/io/netty/channel/local?

I'm assuming it depends a lot on the implementation but probably having a local/mock transport that we could control would be the starting point (as oppose to using an injection fault framework).
</comment><comment author="hhoffstaette" created="2014-02-18T14:19:01Z" id="35387859">There are several ideas floating around, mostly involving a transport wrapper that is injected in tests and which has a pipeline of attached  (one or more) "failure filters" - dropped packets, delayed methods etc.
</comment><comment author="dadoonet" created="2014-02-18T15:03:32Z" id="35392511">Can't wait to have this! I need that to simulate easily some network issues. Sounds great.
</comment><comment author="s1monw" created="2014-12-29T12:03:39Z" id="68252185">this has been fixed on the zen improvement branch long ago... moving out
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reduce number of warnings throughout the code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5162</link><project id="" key="" /><description>A newly imported tree results in a project with ~4090 warnings (in Eclipse). Even though the majority of these may be harmless (i.e. not result in runtime failures or wrong logic), they exist for a reason and are a problem because they might hide actual warnings we could/should care about.

Todo: analyze severity/frequency &amp; suggest mitigation. Should be mostly menial work (ideal for one-fix-a-day) and very often a single fix will fix hundreds of warnings (e.g. non-generic use of a class that is supposed to be used as generic).

Relates to issue #5160.
</description><key id="27791042">5162</key><summary>Reduce number of warnings throughout the code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">hhoffstaette</reporter><labels><label>enhancement</label><label>test</label></labels><created>2014-02-18T12:53:59Z</created><updated>2014-07-04T10:31:57Z</updated><resolved>2014-07-04T10:31:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-18T13:42:20Z" id="35384743">+1 to fix these warnings
</comment><comment author="s1monw" created="2014-02-18T14:07:38Z" id="35386846">I am all for it! we should do a fix-it friday for this, what do you think @jpountz @hhoffstaette 
</comment><comment author="hhoffstaette" created="2014-02-18T14:24:22Z" id="35388361">Let me create some rough stats first so that we see what the distribution &amp; density across source, jsr166, tests is.
</comment><comment author="jpountz" created="2014-02-18T14:24:31Z" id="35388376">+1 I guess we just need to assign packages to ourselves and try to fix as many warnings as possible
</comment><comment author="costin" created="2014-02-18T19:01:54Z" id="35420153">I'm curious how many of these can be fixed automatically by Eclipse and IntelliJ. The latter seems to be quite good at it - it's worth a try nevertheless.
</comment><comment author="hhoffstaette" created="2014-02-18T19:36:06Z" id="35423863">@s1monw:
- Cleaning up all warnings is theoretically (but IMHO not realistically) possible, but would mean we have to touch the jsr166 classes. That might be fixable with a few select annotations, but still I'd rather avoid that since they are straight copies.
- Aborting the build is not possible as-is because javac is dumb as a rock; these warnings are specific to the Eclipse compiler. For a continuous overview (or even aborting the build) something like publishing Findbugs/PMD reports would be much more effective (also more thorough since findbugs is really intelligent) and can be automated easily, esp. since ES is a single project. The question then becomes what "useful" warnings/alerts are, which is an entirely different discussion for another time. FWIW I think Findbugs would be awesome since it is much more intelligent and can be extended with custom problem finder plugins (like, e.g. verifying refcounting).

For now I really just want to get rid of the majority of _unnecessary_ warnings so that we can then focus on the ones that might be either false positives ("tricky" code like the assert check etc.) or actual potential problems. I'll publish the overview/analysis tomorrow.
</comment><comment author="hhoffstaette" created="2014-02-18T20:04:05Z" id="35426977">Btw can someone say how things work in IntelliJ? Does it not show any warnings, or has everyone just turned them off? Just curious.
</comment><comment author="hhoffstaette" created="2014-02-19T10:44:04Z" id="35485876">Short overview about what's what, sorted by frequency and name, with separated frequency for src (main) vs. tests. Note that this does _not_ imply severity for potential bugs.

| Frequency | src | test | Description |
| --: | --: | --: | :-- |
| 1453 | 207 | 1246 | "raw type" |
| 565 | 420 | 145 | "Empty block" |
| 491 | 491 | 0 | "Access restriction" |
| 375 | 134 | 241 | "deprecated" |
| 373 | 304 | 69 | "Type safety" (various!) |
| 229 | 220 | 9 | "The serializable class has no serialUUID" |
| 161 | 99 | 62 | "The import is never used" |
| 111 | 108 | 3 | "Potential null pointer access" |
| 88 | 52 | 36 | "Unnecessary cast" |
| 79 | 24 | 55 | "The value of the local variable is not used" |
| 56 | 26 | 30 | "Resource leak" |
| 55 | 52 | 3 | "The value of the field is not used" |
| 37 | 27 | 10 | "Unnecessary @SuppressWarnings" |
| 13 | 9 | 4 | "Dead code" |
| 12 | 9 | 3 | "Unnecessary semicolon" |
| 9 | 9 | 0 | "The type parameter T is hiding the type T" |
| 7 | 2 | 5 | "Possible accidental assignment" |
| 6 | 4 | 2 | "The static field should be accessed in a static way" |
| 2 | 2 | 0 | "annotation type Element should not be used as a superinterface" |
| 1 | 1 | 0 | "Comparing identical expressions" |

The embedded JSR166 util.concurrent.\* classes are a pretty big offender all by themselves, mostly due to Unsafe (for CAS/atomics) but also a surprising number of potential NPEs and others.

| Frequency | src | test | Description |
| --: | --: | --: | :-- |
| 632 | 632 | 0 | misc. |

Again, these should not be touched as they are 1:1 copied from the jsr166 repo.

A few observations about the messages and where they occur:
- "deprecated code" is mostly in reference to Lucene classes
- "Empty block" seems harmless, but is anything but: in the absence of documentation (a single comment would do!) it's impossible to decide whether a method is empty because it does not do anything _intentionally_ (overiding a superclass stub), or whether it was added just to silence an interface, was forgotten or still needs to be implemented. This can make refactorings a much worse minefield than necessary.
- "Raw types" and "Type safety" are mostly fallout from lack of/abuse of generics. In tests they are often just used out of laziness, in code they can have API impact (when data structures esapce/are exposed as return values)
- "Possible accidental assignment" is AFAICT exclusively due to the asserts, though I think I saw a few occurrences elsewhere (short-circuited branches etc.). IMHO really bad.
- The missing serialVersionUUID could be used for the serialization/versioning, though IMHO we shouldn't use Serialization at all anyway (in favor of explicitly modeled wire protocols).
- "Dead code" can just be removed; in tests it might indicate old assumptions or assumptions that _seem_ to be verified, but actually are not.
- "Resource leaks" seem universally nasty but may or may not be actual leaks. In most of the tests they refer to Closeables that do not "really" need to be closed, since they don't have external references. Still pretty bad IMHO, and sooner or later we will have a need for refcouting/offheap/hard lifecycle control.

So there you go. I'll let this sit here for now because I have more important things to do, but I hope it's clear that this is more difficult than a single "fix-it Friday". We really don't have to "break this over the knee" as we say here..
</comment><comment author="hhoffstaette" created="2014-02-19T10:56:32Z" id="35486775">FWIW (in reference to https://github.com/elasticsearch/elasticsearch/issues/5162#issuecomment-35423863): a friend of mine recommends SonarQube: http://www.sonarqube.org/ for continuous overview/reporting. That is a much better/more sustainable approach than invasively failing the build or similar measures, which just make builds more complicated/slower/fragile.
</comment><comment author="dadoonet" created="2014-02-19T10:58:09Z" id="35486886">@hhoffstaette +1 BTW Sonar team just moved to elasticsearch! :-)
</comment><comment author="dadoonet" created="2014-02-19T11:03:39Z" id="35487305">BTW could you tell me where is this one `Comparing identical expressions`?
That's one of the cool thing with Eclipse. Displaying warnings for the full project is really easy and powerful.

O_o
</comment><comment author="hhoffstaette" created="2014-02-19T11:17:46Z" id="35488327">On 19.02.2014 12:04, David Pilato wrote:

&gt; |Comparing identical|

Comparing identical expressions
CompletionFieldMapper.java
/elasticsearch/src/main/java/org/elasticsearch/index/mapper/core
line 432
</comment><comment author="hhoffstaette" created="2014-02-21T10:19:53Z" id="35716224">Due to popular demand: the warnings ordered by name: https://gist.github.com/hhoffstaette/9131947
</comment><comment author="costin" created="2014-02-21T10:26:00Z" id="35717087">Interesting - there are some obvious big groups in there that could be easily addressed:
- usage of Unsafe
- unused imports (fixed through organize imports)
- Class is a raw type - fixed typically with `Class&lt;?&gt;`
- lack of `serialVersionUID` - I would just tell the compiler to ignore it
- Unnecessary XXX - fixed through simple removal.
</comment><comment author="aravindhan07" created="2014-02-21T10:34:45Z" id="35718369"> For some reason, when I import the project, eclipse uses jre1.6 as the default jre system library. Is there a reason for using java version 6 rather than 7? The access restrictions can be resolved by adding jre7 as a system library in the java build path.
</comment><comment author="hhoffstaette" created="2014-02-21T10:38:31Z" id="35718942">For those who (like me) were not aware of it: we already have an initial pmd/findbugs report: http://build.elasticsearch.com/job/es_static_master/
</comment><comment author="hhoffstaette" created="2014-02-21T10:42:58Z" id="35719539">@aravindhan07 My understanding is that this happens due to source/target compliance and eclipse automatically picks a "matching" JDK. You can configure what levels match which installed JDK in your global Eclipse settings.
As for the access restrictions: they probably _should not_ be solved, they only happen because we embed jsr166 code. Long term it might be better to try and get rid of that in favor of simply referencing the versions in newer versions of Netty.
</comment><comment author="samuelaustin" created="2014-02-21T11:41:55Z" id="35723722">For the "Resource leak" warning, it seems that in some cases the closable object is being returned and therefore isn't closed (e.g. `getStoreDirectory` in `DirectoryUtils.java`). 

In other cases it seems that the warning could be easily solved by simply closing the given object (e.g. `SynonymTokenFilterFactory` constructor).

Would it be an idea to go through them and see if they can easily be closed on a case by case basis?
</comment><comment author="aravindhan07" created="2014-02-21T12:09:45Z" id="35725364">For "raw types" errors, is it encouraged to use a wildcard for generic class declarations and instantiations?
</comment><comment author="s1monw" created="2014-07-04T10:31:57Z" id="48029389">this is a rabbit hole - I am closing this issue for now since it didn't bring much helpful insights. We should fix this as we go.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>New mapping error: _mapping/field/* 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5161</link><project id="" key="" /><description>```
curl -v 'http://web245:9200/statistics-20140216/_mapping/field/*'
* About to connect() to web245 port 9200 (#0)
*   Trying 192.168.0.245...
* connected
* Connected to web245 (192.168.0.245) port 9200 (#0)
&gt; GET /statistics-20140216/_mapping/field/* HTTP/1.1
&gt; User-Agent: curl/7.26.0
&gt; Host: web245:9200
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 404 Not Found
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 2
&lt;
* Connection #0 to host web245 left intact
{}* Closing connection #0
```

```
# curl -v 'http://web245:9200/statistics-20140216/_mapping'
* About to connect() to web245 port 9200 (#0)
*   Trying 192.168.0.245...
* connected
* Connected to web245 (192.168.0.245) port 9200 (#0)
&gt; GET /statistics-20140216/_mapping HTTP/1.1
&gt; User-Agent: curl/7.26.0
&gt; Host: web245:9200
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 1648
&lt;
{"statistics-20140216":{"mappings":{"markers":{"_all":{"enabled":false},"properties":{"@message":{"type":"string"},"@timestamp":{"type":"date","format":"dateOptionalTime"}}},"precise":{"_all":{"enabled":false},"_routing":{"required":true,"path":"@key"},"properties":{"@key":{"type":"string","index":"not_analyzed"},"@precise":{"type":"double"},"@timestamp":{"type":"date","format":"dateOptionalTime"}}},"events":{"_all":{"enabled":false},"_routing":{"required":true,"path":"@key"},"properties":{/* skipped properties here */}}}}}* Closing connection #0
```

Kibana complains that my proxy is misconfigured.

This happened after 0.90.10 -&gt; 1.0.0 upgrade on one of our clusters. All staging clusters were ok, except this one in production :)

What is wrong and how to fix it?
</description><key id="27790671">5161</key><summary>New mapping error: _mapping/field/* 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bobrik</reporter><labels /><created>2014-02-18T12:46:30Z</created><updated>2014-03-03T13:26:27Z</updated><resolved>2014-03-03T13:26:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-02-18T12:49:55Z" id="35380889">Cluster is still recovering, almost 900gb to go, but we're at `yellow` already:

``` json
{
  "cluster_name" : "statistics",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 521,
  "active_shards" : 595,
  "relocating_shards" : 0,
  "initializing_shards" : 2,
  "unassigned_shards" : 445
}
```
</comment><comment author="kimchy" created="2014-02-18T12:54:29Z" id="35381175">we changed some Apis with the move to 1.0. Kibana M5 should work with 1.0, might want to ping Kibana list
</comment><comment author="bobrik" created="2014-02-18T12:56:40Z" id="35381342">@kimchy `GET /{indices}/_mapping/{types}` advertised to be working and it works in other clusters, but not in this one. Take a look at first curl request, should it return 404?
</comment><comment author="bobrik" created="2014-02-18T12:59:57Z" id="35381574">Those queries go directly to elasticsearch node, no proxy involved.
</comment><comment author="bobrik" created="2014-02-18T13:06:13Z" id="35382004">Good cluster:

``` json
{
  "status" : 200,
  "name" : "portraits02",
  "version" : {
    "number" : "1.0.0",
    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",
    "build_timestamp" : "2014-02-12T16:18:34Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
```

```
# curl -X PUT http://web580:9200/test/test/hey -d '{"one": 1}'
# curl 'http://web580:9200/test/_mapping/field/*'
{"test":{"mappings":{"test":{"_source":{"full_name":"_source", "mapping" : {}},"one":{"full_name":"one", "mapping" : {"one":{"type":"long"}}},"_timestamp":{"full_name":"_timestamp", "mapping" : {}},"_index":{"full_name":"_index", "mapping" : {}},"_uid":{"full_name":"_uid", "mapping" : {}},"_boost":{"full_name":"_boost", "mapping" : {}},"_type":{"full_name":"_type", "mapping" : {}},"_version":{"full_name":"_version", "mapping" : {}},"_id":{"full_name":"_id", "mapping" : {}},"_routing":{"full_name":"_routing", "mapping" : {}},"_ttl":{"full_name":"_ttl", "mapping" : {}},"_size":{"full_name":"_size", "mapping" : {}},"_all":{"full_name":"_all", "mapping" : {}},"_parent":{"full_name":"_parent", "mapping" : {}}}}}}
```

Bad cluster:

``` json
{
  "status" : 200,
  "name" : "statistics03",
  "version" : {
    "number" : "1.0.0",
    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",
    "build_timestamp" : "2014-02-12T16:18:34Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
```

```
# curl -X PUT http://web245:9200/test/test/hey -d '{"one": 1}'
# curl 'http://web245:9200/test/_mapping/field/*'
{}
```
</comment><comment author="bleskes" created="2014-02-18T13:12:46Z" id="35382466">@bobrik that's indeed peculiar. It behaves as if the bad cluster has no fields defined for index test.

ca you get a complete mapping on the bad cluster and gist it ?

```
curl 'http://web245:9200/_mapping`
```

thx.
</comment><comment author="bobrik" created="2014-02-18T13:19:29Z" id="35382937">@bleskes that's 350kb of json and I'm afraid that I'm not allowed to publish it in the wild. I could email it to you if you like.

Test index mapping looks as it should though.

``` json
{"test":{"mappings":{"test":{"properties":{"one":{"type":"long"}}}}}}
```

Also, everything else (search, indexing) seems to be operational.
</comment><comment author="bleskes" created="2014-02-18T13:23:44Z" id="35383254">just to double check, what was the command you used to get that one?
</comment><comment author="bobrik" created="2014-02-18T13:25:42Z" id="35383415">That was:

```
# curl 'http://web245:9200/test/_mapping'
{"test":{"mappings":{"test":{"properties":{"one":{"type":"long"}}}}}}
```

To double-check:

```
# curl 'http://web245:9200/test/_mapping/field/*'
{}
```
</comment><comment author="bleskes" created="2014-02-18T13:39:15Z" id="35384500">thx bob. I can't reproduce it here so we'd have to go on a bit of a fishing expedition. What's the output of:

```
curl 'http://web245:9200/test/_mapping/field/one'
curl 'http://web245:9200/test/test/_mapping/field/one'
curl 'http://web245:9200/test/test/_mapping/field/*'
```
</comment><comment author="bobrik" created="2014-02-18T13:43:40Z" id="35384838">All `{}`. I guess I could spawn another node, move that small index there and give you tar.gz of `/var/lib/elasticsearch`. Can it help?

I also have master-only node and could fetch metadata from it, like /var/lib/elasticsearch/statistics/nodes/0/indices/test/_state/state-2
</comment><comment author="bleskes" created="2014-02-18T13:46:20Z" id="35385036">if you're running all the nodes from that folder it would be indeed great if you can zip it and post it somewhere for me to download.
</comment><comment author="bobrik" created="2014-02-18T13:47:42Z" id="35385146">Is data from mater-only node sufficient?
</comment><comment author="bleskes" created="2014-02-18T13:50:50Z" id="35385399">Sure, let's try that first.
</comment><comment author="bobrik" created="2014-02-18T13:53:22Z" id="35385611">It should be in your gmail inbox.
</comment><comment author="bleskes" created="2014-02-19T07:57:14Z" id="35474348">got it and I can reproduce it now locally. Will look at it asap
</comment><comment author="bleskes" created="2014-02-19T11:44:54Z" id="35490180">hi bob, I found the source of the problem and opened a dedicated issue for it: #5177 . I'll ping you once it's fixed. Thx  for sharing the data.
</comment><comment author="bobrik" created="2014-02-19T11:46:28Z" id="35490307">@bleskes cool, will wait.
</comment><comment author="bobrik" created="2014-02-19T11:56:47Z" id="35490992">@bleskes btw it doesn't work even if I try to ask each node in the cluster, not sure if this is relevant.
</comment><comment author="bleskes" created="2014-03-03T13:04:38Z" id="36508001">@bobrik #5177 was fixed with 1.0.1 . can you upgrade check things are good now?
</comment><comment author="bobrik" created="2014-03-03T13:26:27Z" id="36509446">@bleskes looks like it works now. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Eclipse maven goal does not result in a buildable tree</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5160</link><project id="" key="" /><description>The eclipse:eclipse goal creates the necessary project files for Eclipse import. Unfortunately the resulting project will not build out of the box, mostly because of two issues:
- use of (forbidden/unaccessible) Unsafe/CAS by the in-tree jsr166 classes
- runtime check for enabled assertions results in an error about:
  "Possible accidental assignment in place of a comparison. A condition expression should not be reduced to an assignment"

The maven goal can add per-project compiler flags to reduce these errors to warnings/ignore them. The same goes for the other &gt;4000 warnings (mostly generics, unused imports, dead code, deprecations etc. - see separate issue)

Whether these two in particular should be warnings or outright ignored is an open issue. The jsr166 code should not be touched as it's a straight import from the jsr166 repository and should remain as pristine as possible for tracking upstream changes.
</description><key id="27790507">5160</key><summary>Eclipse maven goal does not result in a buildable tree</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>enhancement</label></labels><created>2014-02-18T12:43:11Z</created><updated>2014-12-29T12:22:35Z</updated><resolved>2014-12-29T12:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-18T13:35:25Z" id="35384202">There aren't any eclipse users that _commit_ to Elasticsearch. There are a few contributors that use it. I use eclipse and ignore the warnings. I also use the maven integration rather than the eclipse goal. 

The upshot is that you are probably best qualified to fix it. I'll review it but don't have time to learn the appropriate maven invocations. 

Sent from my iPhone

&gt; On Feb 18, 2014, at 7:43 AM, Holger Hoffstätte notifications@github.com wrote:
&gt; 
&gt; The eclipse:eclipse goal creates the necessary project files for Eclipse import. Unfortunately the resulting project will not build out of the box, mostly because of two issues:
&gt; 
&gt; use of (forbidden/unaccessible) Unsafe/CAS by the in-tree jsr166 classes
&gt; 
&gt; runtime check for enabled assertions results in an error about:
&gt; "Possible accidental assignment in place of a comparison. A condition expression should not be reduced to an assignment"
&gt; 
&gt; The maven goal can add per-project compiler flags to reduce these errors to warnings/ignore them. The same goes for the other &gt;4000 warnings (mostly generics, unused imports, dead code, deprecations etc. - see separate issue)
&gt; 
&gt; Whether these two in particular should be warnings or outright ignored is an open issue. The jsr166 code should not be touched as it's a straight import from the jsr166 repository and should remain as pristine as possible for tracking upstream changes.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2014-02-18T14:06:55Z" id="35386776">I can only +1 this - getting rid of the warnings is great and IMO we should fail the build once this is fixed if a warning is detected!
</comment><comment author="hhoffstaette" created="2014-02-18T14:21:52Z" id="35388111">The point of this particular issue is mostly to get a building tree out of the box, a) for me and b) to encourage contributions from other users who use Eclipse. Having to hunt for magic compiler flags is just a needless hurdle.
Exactly which warnings we ignore via the compiler is a long story (esp. about the jsr166 classes), but it should at least compile.
</comment><comment author="dakrone" created="2014-02-18T15:38:17Z" id="35396128">&gt; There aren't any eclipse users that _commit_ to Elasticsearch.

I use Eclipse and [emacs-eclim](https://github.com/senny/emacs-eclim) occasionally to commit to Elasticsearch, a buildable ES out of the box would be very nice. +1 on this issue.
</comment><comment author="s1monw" created="2014-12-29T12:04:13Z" id="68252209">is this still an issue @jpountz don't you use eclipse?
</comment><comment author="rmuir" created="2014-12-29T12:14:25Z" id="68252762">I use eclipse and I don't see these issues: mvn eclipse:eclipse works for me.
</comment><comment author="s1monw" created="2014-12-29T12:22:35Z" id="68253174">cool thanks @rmuir 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>BytesStreamOutput could be more efficient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5159</link><project id="" key="" /><description>org.elasticsearch.common.io.stream.BytesStreamOutput is used esp. for response compression and creates many small arrays during growth. This can be made more efficient by paging or other means.
</description><key id="27790109">5159</key><summary>BytesStreamOutput could be more efficient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-18T12:34:53Z</created><updated>2014-06-06T16:54:40Z</updated><resolved>2014-03-04T14:19:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hhoffstaette" created="2014-03-04T14:19:55Z" id="36627708">Finally pushed into master &amp; 1.x.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/stream/BytesStreamOutput.java</file><file>src/main/java/org/elasticsearch/index/translog/fs/FsTranslog.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/test/java/org/elasticsearch/common/io/streams/BytesStreamsTests.java</file></files><comments><comment>Rewrite BytesStreamOutput on top of BigArrays/ByteArray.</comment></comments></commit></commits></item><item><title>Set permission in debian postinst script correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5158</link><project id="" key="" /><description>The old post installation script on debian set all data to
644 inside of /etc/elasticsearch, which does not work, when
there are subdirectories

Closes #3820
</description><key id="27789519">5158</key><summary>Set permission in debian postinst script correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-18T12:23:42Z</created><updated>2015-06-08T00:07:29Z</updated><resolved>2014-02-18T15:01:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-18T12:24:05Z" id="35379198">IMO also a 0.90 candidate
</comment><comment author="s1monw" created="2014-02-18T14:43:49Z" id="35390137">LGTM
</comment><comment author="s1monw" created="2014-02-18T14:44:12Z" id="35390186">+1 to go for `0.90`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Imprecise units for storage/bandwidth/etc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5157</link><project id="" key="" /><description>I noticed that storage and bandwidth limits (e.g. on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-indices.html#throttling) are expressed in imprecise units, which is confusing to a global audience. The unit "mb" is supposed to mean "megabyte", but actually means "millibit". This is confusing for storage (typically expressed in KB/MB/GB aka Kilo/Mega/Giga Byte), but also esp. for network bandwidth, which is often expressed in bits with capital prefix and lowercase unit, e.g. Megabit are Mb (and ~1/8th of an MB).

Suggestion for fix: proper &amp; consistent conversion to SI units.
</description><key id="27789093">5157</key><summary>Imprecise units for storage/bandwidth/etc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hhoffstaette</reporter><labels><label>docs</label><label>enhancement</label></labels><created>2014-02-18T12:14:48Z</created><updated>2014-12-29T11:58:55Z</updated><resolved>2014-12-29T11:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T11:58:55Z" id="68251903">I don't think anybody is confusing `mb` with millibit.  It'd be more confusing to introduce case sensitivity into our units, especially at this late stage.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java API config error results in Guice complaining with hundreds of exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5156</link><project id="" key="" /><description>So if I use the API to startup a node, like this:

 ImmutableSettings.Builder settingsBuilder = ImmutableSettings.settingsBuilder();
settingsBuilder.put("cluster.name", clusterName);

....

 Settings settings = settingsBuilder.build();
 node = NodeBuilder.nodeBuilder().settings(settings).node();
 node.start();

if there are wrong settings, I won't get any specific error (to the setting itself), just a huge list of exceptions about Guice and a problem with circular references. This is confusing and took me a google search to understand what was going on. If this could be avoided and a more informative message could be output, that'd be much better.
</description><key id="27783990">5156</key><summary>Java API config error results in Guice complaining with hundreds of exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dariodariodario</reporter><labels /><created>2014-02-18T10:39:47Z</created><updated>2014-12-29T11:53:01Z</updated><resolved>2014-12-29T11:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T11:53:01Z" id="68251572">Closing in favour of #6732
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GET _warmer should return a 404 for warmers that do not exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5155</link><project id="" key="" /><description>Doing the following in any of the 0.90 releases

``` Shell
curl -XPOST "http://localhost:9200/my-new-index"
curl -XGET "http://localhost:9200/my-new-index/_warmer/this-warmer-does-not-exists"
```

Will result in:

``` json
{
   "error": "IndexWarmerMissingException[index_warmer [this-warmer-does-not-exists] missing]",
   "status": 404
}
```

However in the 1.0 GA release the latter returns a 200 with the following body:

``` json
{}
```
</description><key id="27783805">5155</key><summary>GET _warmer should return a 404 for warmers that do not exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>bug</label></labels><created>2014-02-18T10:36:42Z</created><updated>2016-01-07T09:54:06Z</updated><resolved>2016-01-07T09:54:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2014-02-18T15:15:16Z" id="35393711">Ok so this seems to be according to the spec:

```
Similarly, the return values for GET have been unified with the following rules:

Only return values that exist. If you try to GET a mapping which doesn’t exist, 
then the result will be an empty object: {}. We no longer throw a 404 if the requested 
mapping/warmer/alias/setting doesn’t exist.
```

from: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/_indices_apis.html

Can anyone explain the reason for this behaviour?
</comment><comment author="grantr" created="2014-03-21T22:31:41Z" id="38331209">This seems odd to me as well. Why return a 200 response when retrieving an object that doesn't exist? This is an obvious place to return a 404.

Returning a 200 directly contradicts the behavior of indices-exists: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/indices-exists.html#indices-exists

Warmers should support a similar exists api with the HEAD verb.
</comment><comment author="javanna" created="2015-03-20T07:46:32Z" id="83944378">I agree that the get warmer api should be changed to return 404 when the warmer doesn't exist.
</comment><comment author="jpountz" created="2016-01-07T09:54:06Z" id="169613565">Closing: warmers will go away in 3.0 #15614
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>client type configurable via settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5154</link><project id="" key="" /><description>Add a client (factory) that can be completely configured via settings, including whether it is a node client or a transport client.
Also see #5153.

Example configuration

```
client.type: node
...
```

```
client.type: transport
...
```

Not having the node exposed would require closing the node on client close.
</description><key id="27778764">5154</key><summary>client type configurable via settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brackxm</reporter><labels><label>discuss</label></labels><created>2014-02-18T09:10:40Z</created><updated>2015-10-14T14:24:04Z</updated><resolved>2015-10-14T14:24:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T14:24:04Z" id="148065588">Transport client no longer loads the config file at all. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>transport client configurable via settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5153</link><project id="" key="" /><description>Allow the transport client to be configurable via settings.
Currently hosts &amp; ports need to added via addTransportAddress().
It would be more flexible to also allow hosts &amp; ports to be configured via settings.

For example, this could be done via a comma separated list

```
client.transport.address: host1:9300,host2:9300
```

Or with separate lists for hosts &amp; ports.

Or map style

```
client.transport.address.a.host: host1
client.transport.address.a.port: 9300
client.transport.address.b.host: host2
client.transport.address.b.port: 9300
```
</description><key id="27778302">5153</key><summary>transport client configurable via settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brackxm</reporter><labels><label>discuss</label></labels><created>2014-02-18T09:01:15Z</created><updated>2015-10-14T14:23:32Z</updated><resolved>2015-10-14T14:23:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T16:28:10Z" id="91608416">@spinscale can you comment on this? Is the need for this removing with transport profiles?
</comment><comment author="spinscale" created="2015-04-10T17:22:47Z" id="91627322">@dakrone unsure about your question here, would rather say no. This looks to me as an abbreviation for the `TransportClient` configuration, that has nothing to do with the profiles on the server side. It's just about specifying the destination addresses as part of the settings instead of calling `TransportClient.addTransportAddress`
</comment><comment author="dakrone" created="2015-04-10T17:42:32Z" id="91633056">@spinscale ahh okay, I misunderstood this to be on the server side instead of the client (duh, I see "client" in the title now), thanks for the clarification.
</comment><comment author="clintongormley" created="2015-10-14T14:23:32Z" id="148065435">Transport client no longer loads the config file at all.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unbound threadpools considered harmful</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5152</link><project id="" key="" /><description>ElasticSearch server and java client both have an unbound, undocumented threadpool that can cause lockup of the whole JVM process and take down the whole server machine: #5151.

The minimum change is documenting the unbound generic pool &amp; instructions for reconfiguring it at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html.

IMHO unbound threadpool is a Bad Idea from the start. Bound pool with some reasonable size queue and abort policy would not cause jvm/server lockup in a runaway case, instead you would get some meaningful information in the log.
</description><key id="27775404">5152</key><summary>Unbound threadpools considered harmful</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">tkurki</reporter><labels><label>enhancement</label></labels><created>2014-02-18T07:53:20Z</created><updated>2015-04-03T12:12:19Z</updated><resolved>2014-10-14T12:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tkurki" created="2014-02-18T07:53:50Z" id="35360349">https://groups.google.com/forum/#!msg/elasticsearch/oKQMoOKhSU8/09-P9bU4t_wJ
</comment><comment author="scottbessler" created="2014-05-21T18:51:40Z" id="43798796">This appears to have bit us as well. We noticed it when we had an `ActionListener` on a `ListenableActionFuture&lt;BulkResponse&gt;` who took perhaps too long to execute and/or had concurrency issues (it was actually firing a Guava Eventbus event where the handlers are synchronized). We started seeing exceptions like:

```
org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:249)
```

And then with Yourkit attached we saw the same symptom of 7000+ of these threads being spun up, and hanging the jvm.
</comment><comment author="clintongormley" created="2014-05-22T08:30:21Z" id="43861212">@scottbessler What version of Elasticsearch are you using?
</comment><comment author="kimchy" created="2014-05-22T12:45:20Z" id="43882771">The idea of the generic thread pool is to have an option to fork to a thread without having to block on it or get rejected, which is needed in some cases. In places where it is used, its good, but there are places where it simply should not be used, specifically, the listening thread pool when the pool is threaded, we should have a dedicated thread pool for it that is bounded.
</comment><comment author="scottbessler" created="2014-05-22T13:26:19Z" id="43887244">0.90.6
</comment><comment author="clintongormley" created="2014-05-22T13:33:00Z" id="43887982">@scottbessler It appears to be fixed in 0.90.8 onwards #5151 #4162
</comment><comment author="scottbessler" created="2014-05-22T16:44:10Z" id="43913470">The race condition may be fixed, but the unbounded undocumented thread pool lurks waiting for a similar issue to crush JVMs. If nothing else it seems that at least letting users configure this threadpool (even if the default is unbounded) would at least let them choose their poison.
</comment><comment author="kimchy" created="2014-07-11T08:45:00Z" id="48707560">@scottbessler yea, though in all places (except for the listeners today), we are very careful at when we use the generic thread pool. We need to look into potentially changing the generic thread pool to be bounded, but have unlimited queue, since we rely on the fact that it won't block when trying to add something to it. Regardless, the listeners callback should have their own thread pool.
</comment><comment author="mgreene" created="2014-09-04T04:48:03Z" id="54407172">@kimchy I'm curious as to why you suggest a bounded thread pool but unbounded queue. In essence, a unbounded thread pool will thrash CPU and potentially blow out the stack space on the JVM. An unbounded queue can blow out the heap.

I think the great thing (and also a PITA) about the Java concurrency semantics is that w/r/t to queues and thread pools, they encourage you to deal with the back pressure. You can elect to simplify your code by adding an unbounded queue or pool but in turn are allowing back pressure to manifest itself in weird and cryptic ways that's harder to troubleshoot in production.
</comment><comment author="kamaradclimber" created="2014-09-04T12:19:35Z" id="54464444">I think we are hurt by one of this unbounded thread pool (called "generic").
Here is a thread dump:

```
"elasticsearch[Trump][generic][T#62615]" daemon prio=10 tid=0x00007f23f9db7800 nid=0x9b0c waiting on condition [0x00007f1e0f82a000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x00000006cbd1e030&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
    at java.util.concurrent.CopyOnWriteArrayList.remove(CopyOnWriteArrayList.java:507)
    at org.elasticsearch.cluster.service.InternalClusterService.remove(InternalClusterService.java:182)
    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.onTimeout(TransportMasterNodeOperationAction.java:179)
    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:492)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```

we currently have 28k threads with the same stack

The original reason is probably elsewhere (I should file a separate ticket for this as well) but the unbound nature of the threadpool bites us here.
</comment><comment author="mukulv" created="2014-09-30T02:04:07Z" id="57257621">Hi, I am hitting this issue in our servers. Is this included in latest version 1.3.3? If not, while I will wait for version with fix, is there anything I can do to reduce the probability of hitting it in my client as it brings my server machine down?

&gt; &gt; "elasticsearch[Horus][generic][T#16356]" daemon prio=10 tid=0x00007f27ca746800 nid=0x5cd8 runnable [0x00007f25a4444000]
&gt; &gt;    java.lang.Thread.State: RUNNABLE
&gt; &gt;         at java.util.Arrays.fill(Arrays.java:1921)
&gt; &gt;         at org.elasticsearch.common.io.stream.HandlesStreamOutput$HandleTable.clear(HandlesStreamOutput.java:192)
&gt; &gt;         at org.elasticsearch.common.io.stream.HandlesStreamOutput$HandleTable.&lt;init&gt;(HandlesStreamOutput.java:149)
&gt; &gt;         at org.elasticsearch.common.io.stream.HandlesStreamOutput.&lt;init&gt;(HandlesStreamOutput.java:39)
&gt; &gt;         at org.elasticsearch.common.io.stream.HandlesStreamOutput.&lt;init&gt;(HandlesStreamOutput.java:43)
&gt; &gt;         at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:548)
&gt; &gt;         at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:189)
&gt; &gt;         at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:68)
&gt; &gt;         at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:109)
&gt; &gt;         at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:259)
&gt; &gt;         at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:262)
&gt; &gt;         at org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:89)
&gt; &gt;         at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
&gt; &gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
&gt; &gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
&gt; &gt;         at java.lang.Thread.run(Thread.java:745)
</comment><comment author="clintongormley" created="2014-10-14T12:02:56Z" id="59031745">Hi @jfiedler 

PR #7837 adds a listener thread pool, which should fix the issue that you are seeing (in 1.4).  The generic threadpool is still cached and unbounded, but we are very careful about what gets run on these threads.

Closing this ticket.  
</comment><comment author="dernasherbrezon" created="2015-04-03T12:12:19Z" id="89268480">There are still unbounded thread pools exist at NettyTransport (https://github.com/elastic/elasticsearch/blob/1032429d31efce215a8c9abfe4a41830d821da72/src/main/java/org/elasticsearch/transport/netty/NettyTransport.java#L312)

I would expect them configured the same way index/search/&amp;etc threadpools. Could create pull request if needed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>core/src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file><file>core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file><file>core/src/test/java/org/elasticsearch/search/SearchWithRejectionsIT.java</file><file>core/src/test/java/org/elasticsearch/threadpool/SimpleThreadPoolIT.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolSerializationTests.java</file><file>core/src/test/java/org/elasticsearch/threadpool/ThreadPoolTypeSettingsValidatorTests.java</file><file>core/src/test/java/org/elasticsearch/threadpool/UpdateThreadPoolSettingsTests.java</file><file>test-framework/src/main/java/org/elasticsearch/test/InternalTestCluster.java</file></files><comments><comment>Forbid changing thread pool types</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/ListenableActionFuture.java</file><file>src/main/java/org/elasticsearch/action/TransportActionNodeProxy.java</file><file>src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>src/main/java/org/elasticsearch/threadpool/ThreadPool.java</file></files><comments><comment>Add a listener thread pool</comment><comment>Today, when executing an action (mainly when using the Java API), a listener threaded flag can be set to true in order to execute the listener on a different thread pool. Today, this thread pool is the generic thread pool, which is cached. This can create problems for Java clients (mainly) around potential thread explosion.</comment><comment>Introduce a new thread pool called listener, that is fixed sized and defaults to the half the cores maxed at 10, and use it where listeners are executed.</comment><comment>relates to #5152</comment><comment>closes #7837</comment></comments></commit></commits></item><item><title>TransportClient behavior when the server node is not available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5151</link><project id="" key="" /><description>We have experienced ES client jamming the jvm and the whole virtual server when the ES server is intermittently not reachable (either down on purpose or with a flaky connection).

When the issue bites we end up with thousands of threads, which will cause OutOfMemory, but the huge number of threads in the generic threadpool exhausts OS resources and the whole virtual server becomes unusable. If there is no previous terminal session that allows killing the offending JVM process we must reboot the whole server.

ES version 0.90.7. 
</description><key id="27773956">5151</key><summary>TransportClient behavior when the server node is not available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tkurki</reporter><labels /><created>2014-02-18T07:12:47Z</created><updated>2014-07-11T14:01:08Z</updated><resolved>2014-02-20T11:51:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tkurki" created="2014-02-18T07:16:28Z" id="35358452">Thread dump of the problem in action: https://gist.github.com/tkurki/9066012
</comment><comment author="tkurki" created="2014-02-18T07:19:58Z" id="35358628">Same type of problem indicated here:
https://groups.google.com/forum/#!msg/elasticsearch/oKQMoOKhSU8/09-P9bU4t_wJ
https://github.com/elasticsearch/elasticsearch/issues/1930
</comment><comment author="tkurki" created="2014-02-18T07:23:15Z" id="35358831">It may be that the issue is already fixed:
https://github.com/elasticsearch/elasticsearch/commit/c324251fc2b4c12adbed05396fe17a3682e01933#diff-359977d2ee02995c3dea93913c31d4b3L255

We will try 0.90.11 next. Upgrade to 1.0 series is not possible immediately because of inhouse rivers.
</comment><comment author="timorantalaiho" created="2014-02-18T08:12:04Z" id="35361351">I am intrigued by [the fencepost change](https://github.com/elasticsearch/elasticsearch/commit/c324251fc2b4c12adbed05396fe17a3682e01933#diff-359977d2ee02995c3dea93913c31d4b3L255) that @tkurki linked, because the comment of the commit and the issue it closes seem to be about a monitoring enhancement, and the change to the `if` condition seems to be completely unrelated to that issue, but it might have fixed the race condition problem we are experiencing. @bleskes , do you remember anything about what triggered you to do that change?
</comment><comment author="bleskes" created="2014-02-18T09:44:56Z" id="35367551">I vaguely recall running into a racing condition during the development of that commit. Though I don't recall the details it had to do with the a connect exception both being picked up by the RetryListener as a callback here: https://github.com/elasticsearch/elasticsearch/commit/c324251fc2b4c12adbed05396fe17a3682e01933#diff-359977d2ee02995c3dea93913c31d4b3L259 and explictly called in a catch clause afterwards. A &gt;= check is anyway safer then == in this case.

@tkurki can report on how it goes with 0.90.11?
</comment><comment author="tkurki" created="2014-02-20T11:51:50Z" id="35613058">After upgrading to 0.90.11 we have not experienced this issue - the same test setup works now reliably. Apparently the fix for #4162 fixed this.
</comment><comment author="javanna" created="2014-07-11T14:01:08Z" id="48733463">Related to the `&gt;=` check mentioned above, it is indeed safer thatn `==`. The fact that the retry counts can even get higher than the actual number of connected nodes is that the retry listener gets notified multiple times for the same failure. See #6829 .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>wrong loading via homebrew</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5150</link><project id="" key="" /><description>I have ran elasticsearch via homebrew, but get errors as follow when execute elasticsearch --config=/usr/local/opt/elasticsearch/config/elasticsearch.yml

getopt: illegal option -- -
getopt: illegal option -- c
getopt: illegal option -- o
getopt: illegal option -- n
getopt: illegal option -- i
getopt: illegal option -- g
getopt: illegal option -- =
getopt: illegal option -- /
getopt: illegal option -- u
getopt: illegal option -- s
getopt: illegal option -- r
getopt: illegal option -- /
getopt: illegal option -- l
getopt: illegal option -- o
getopt: illegal option -- c
getopt: illegal option -- a
getopt: illegal option -- l
getopt: illegal option -- /
getopt: illegal option -- o

when elasticsearch has been running as well, but it is create new folder after launch.
</description><key id="27773626">5150</key><summary>wrong loading via homebrew</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">y00rb</reporter><labels /><created>2014-02-18T07:04:47Z</created><updated>2014-02-19T07:50:46Z</updated><resolved>2014-02-19T07:50:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-18T15:12:03Z" id="35393399">hey,

can you run `elasticsearch -v` - that command line option only works for elasticsearch 1.0
</comment><comment author="spinscale" created="2014-02-18T15:18:02Z" id="35394002">FYI: elasticsearch 1.0 is already in homebrew, so upgrading it might solve your issue
</comment><comment author="y00rb" created="2014-02-19T06:07:56Z" id="35469212">:+1: 
it is works 1.0
thank you so much!
</comment><comment author="y00rb" created="2014-02-19T06:08:22Z" id="35469232">please close the issue
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Locations not showing up in search despite the fact that they meet criteria</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5149</link><project id="" key="" /><description>Sample query: 
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "geo_distance_range": {
          "from": "1km",
          "to": "40000km",
          "profile.locationGeo": {
            "lat": 40,
            "lon": -70
          }
        }
      }
    }
  }
}

Mappings include: 
locationGeo: {
lat_lon: true
type: geo_point
geohash: true
}

One profile ex:
               "locationGeo": {
                  "lat": 32.96179,
                  "lng": -96.8291685
               }
</description><key id="27769603">5149</key><summary>Locations not showing up in search despite the fact that they meet criteria</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jayzalowitz</reporter><labels /><created>2014-02-18T04:59:25Z</created><updated>2014-02-22T20:24:34Z</updated><resolved>2014-02-20T18:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-02-19T03:05:05Z" id="35461517">It should be 

```
"lon": -96.8291685
```
</comment><comment author="jayzalowitz" created="2014-02-22T17:25:57Z" id="35808376">I fixed and it still isnt working
</comment><comment author="imotov" created="2014-02-22T20:24:34Z" id="35813576">@jayzalowitz this is what I have got based on your description: https://gist.github.com/imotov/9161756 

Could you modify this script to demonstrate your problem?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for char filters in the analyze API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5148</link><project id="" key="" /><description>Allow char filters to be used in the analyze API. Potentially breaks AnalyzeRequest serialization. The REST action contains the now ambiguous 'filter' parameter, which will denote a 'token_filter', not a 'char_filter'.

One additional item I noticed is the exception message for invalid token filters. To me it appears like an overzealous copy and paste, but should the exception contain the token filter name, not the tokenizer? I can fix this item as well.

Example:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java?source=cc#L173
</description><key id="27768853">5148</key><summary>Add support for char filters in the analyze API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">brusic</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-18T04:31:33Z</created><updated>2015-06-07T15:21:33Z</updated><resolved>2014-03-06T11:24:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-18T10:18:51Z" id="35369965">Thanks for your PR @brusic ! Looks good, I left a few comments, if you can fix those we can get this in soon ;)
</comment><comment author="brusic" created="2014-02-18T15:41:07Z" id="35396471">Made the changes suggested. I went ahead and changed the exception messages that I referenced above. I do believe they were incorrect.
</comment><comment author="brusic" created="2014-02-19T17:13:06Z" id="35522710">All great comments. I was adhering to the standards that were defined in each file and not the global Elasticsearch guidelines. Great way to learn them. :) Will make the appropriate changes after work.

Should I contain using Version.V_1_1_0 as the serialization check? Somewhat of a chicken and the egg problem, especially when you do not commit/release directly. I did notice there are issues tagged v1.0.1, but there is no corresponding Version in the master branch (it does exist in 1.0). No rush for a release on my behalf (easy workaround by using a custom analyzer), looking more for guidelines.
</comment><comment author="s1monw" created="2014-02-19T17:21:58Z" id="35523696">hey @brusic no worries about the guidelines - we do reviews everytime so we carry over knowledge! The change looks good though mostly cosmetics! I think you should keep the `Version.V_1_1_0` since this is not a bugfix so it won't go to `1.0.1` - not sure if we will ever release that version.
</comment><comment author="javanna" created="2014-02-26T12:44:14Z" id="36120973">Hey @brusic sorry it took a while, I left a few comments, if you can address those this is ready to be pushed. Thanks!
</comment><comment author="brusic" created="2014-02-26T15:47:39Z" id="36139447">I thought I had a line note reply to Simon's comment, but apparently I do not.

As of now, the setters are inconsistent. Some check for a null value, others do not. Also, the setters prevalidate the input despite the existence of an explicit validate method. I prefer consistency over avoiding null checks. :)

Either way, the ultimate endgoal of validating the input is achieved. If you want, I can extend the removal of null checks to TransportAnalyzeAction. In this case, one level of nested ifs is removed and the factory array does not need to have two assignments.

EDIT: the comment is in fact still there , just hidden: https://github.com/elasticsearch/elasticsearch/pull/5148#discussion_r9948285
</comment><comment author="javanna" created="2014-02-28T12:30:10Z" id="36345785">Thanks @brusic I read the hidden comments around consistency...agreed... IMO we can keep the checks only in the `validate` as this is how we do things in most of the cases.

If you can do that I think this is ready.
</comment><comment author="javanna" created="2014-03-06T11:24:27Z" id="36846391">Thanks @brusic !
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java</file><file>src/test/java/org/elasticsearch/indices/analyze/AnalyzeActionTests.java</file></files><comments><comment>Added support for char filters in the analyze API</comment></comments></commit></commits></item><item><title>Stream corruption error post upgrade to 1.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5147</link><project id="" key="" /><description>I am getting this error. On client we have Java(TM) SE Runtime Environment (build 1.7.0-b147) and on server we have Java(TM) SE Runtime Environment (build 1.7.0_51-b13)

[Aardwolf] Message not fully read (response) for [0] handler future(org.elasticsearch.client.transport.TransportClientNodesService$SimpleNodeSampler$1@14b89cc2), error [true], resetting
[Aardwolf] failed to get node info for [#transport#-1][inet[/10.80.140.59:9300]], disconnecting...
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:168)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:122)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.StreamCorruptedException: unexpected end of block data
        at java.io.ObjectInputStream.readObject0(Unknown Source)
        at java.io.ObjectInputStream.defaultReadFields(Unknown Source)
        at java.io.ObjectInputStream.defaultReadObject(Unknown Source)
        at java.net.InetSocketAddress.readObject(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
        at java.io.ObjectInputStream.readSerialData(Unknown Source)
        at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
        at java.io.ObjectInputStream.readObject0(Unknown Source)
        at java.io.ObjectInputStream.defaultReadFields(Unknown Source)
        at java.io.ObjectInputStream.readSerialData(Unknown Source)
        at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
        at java.io.ObjectInputStream.readObject0(Unknown Source)
        at java.io.ObjectInputStream.defaultReadFields(Unknown Source)
        at java.io.ObjectInputStream.readSerialData(Unknown Source)
        at java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
        at java.io.ObjectInputStream.readObject0(Unknown Source)
        at java.io.ObjectInputStream.readObject(Unknown Source)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:166)
        ... 23 more
</description><key id="27752530">5147</key><summary>Stream corruption error post upgrade to 1.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mohitanchlia</reporter><labels /><created>2014-02-17T22:08:39Z</created><updated>2014-12-29T11:48:49Z</updated><resolved>2014-12-29T11:48:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-17T23:28:10Z" id="35332779">sadly, you need to have both JVM versions above 1.7 u25 due to a bug in the JVM breaking backward compatibility.
</comment><comment author="mohitanchlia" created="2014-02-18T00:03:33Z" id="35334764">Can you point me to that bug?
</comment><comment author="brusic" created="2014-02-18T04:55:48Z" id="35352595">The issue is actually in 7u21:

https://bugzilla.redhat.com/show_bug.cgi?id=952657
http://hg.openjdk.java.net/jdk7u/jdk7u-dev/jdk/rev/7ca8a40795d8

Since 7u40, 7u45 and 7u51 all have issues with Lucene, your best option is 7u25. The important part is to not have mismatches across the 7u21 version.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Significant_terms aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5146</link><project id="" key="" /><description>A new aggregation that identifies terms that are significant rather than merely popular in a result set.

Significance is related to the changes in document frequency observed between everyday use in the corpus and frequency observed in the result set. The asciidocs include extensive details on the various applications of this feature.
</description><key id="27730038">5146</key><summary>Significant_terms aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-17T15:58:52Z</created><updated>2015-06-06T18:41:22Z</updated><resolved>2014-03-14T10:40:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-19T11:28:25Z" id="35489066">This looks good to me but I think we should try to share more code with terms aggregations before merging this in. I'm wondering if we could just remove the long aggregator (it would still work on longs but through their string representation) and make the significant terms aggregator extend the string terms aggregator and just override `build(empty)Aggregation`.
</comment><comment author="markharwood" created="2014-02-19T11:36:07Z" id="35489579">Would swapping longs for their string representations mean a lot more RAM/net traffic? There can be a lot of "candidate" buckets generated before final reductions are made. 
</comment><comment author="jpountz" created="2014-02-19T13:40:22Z" id="35499295">Indeed it would. As a trade-off, maybe we could try to share code with the long terms aggregator in a similar way to what I described for the string terms aggregator?
</comment><comment author="jpountz" created="2014-03-10T19:38:34Z" id="37224801">@markharwood It looks good to me in general, I think there is an hppc hash table that should be replaced with a BytesRefHash to save object creations (we don't have anything against hppc structures but try to avoid to have numbers of object creations that are linear with the number of unique values as the latter can be quite high). Other than that, there are a few lines that are missing spaces around equals signs or at the beginning of single-line comments, it would be nice if you could try to clean it up.
</comment><comment author="markharwood" created="2014-03-11T11:44:16Z" id="37286793">Thanks for review, Adrien. 
</comment><comment author="jpountz" created="2014-03-11T21:18:00Z" id="37351231">Thanks Mark, the fix looks good. My understanding is that this cache is useful when using the significant terms aggregation as a sub-aggregation, maybe it should be disabled when there is no parent aggregation? Or would it still be useful?

Another thought I had while reading this PR is that `buildAggregation` can do lots of random seeks in the terms dictionary. It might be interesting to explore how we can make it more sequential in a future pull request (no need to delay this change).
</comment><comment author="uboness" created="2014-03-12T00:10:19Z" id="37361961">Done... well.. first of all... this is just awesome!! I left some comments, but overall it looks good!
</comment><comment author="markharwood" created="2014-03-12T10:27:26Z" id="37393708">Is there a circumstance where that would mask a release failure if an exception is thrown by Releasables.release()?
</comment><comment author="jpountz" created="2014-03-12T10:29:45Z" id="37393906">I don't think so, Releasables.release() will throw the first exception that it got while trying to release the provided Releasables.
</comment><comment author="markharwood" created="2014-03-12T10:32:48Z" id="37394138">OK. 
Thanks for the review, @uboness, starting work on your changes.
</comment><comment author="jpountz" created="2014-03-13T14:39:38Z" id="37540272">+1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationBuilders.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/TransportAggregationModule.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/BucketSignificancePriorityQueue.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/InternalSignificantTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantLongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantStringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/significant/UnmappedSignificantTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/SignificantTermsTests.java</file></files><comments><comment>Significant_terms aggregation identifies terms that are significant rather than merely popular in a set.</comment><comment>Significance is related to the changes in document frequency observed between everyday use in the corpus and</comment><comment>frequency observed in the result set. The asciidocs include extensive details on the applications of this feature.</comment></comments></commit></commits></item><item><title>Make _exists_/_missing_ behave consistently with exists/missing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5145</link><project id="" key="" /><description>`_exists_` and `_missing_` miss field name expansion that `exists` and
`missing` have, which allows these filters to work on `object` fields.

Close #5142
</description><key id="27724412">5145</key><summary>Make _exists_/_missing_ behave consistently with exists/missing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-17T15:00:25Z</created><updated>2015-06-07T23:20:46Z</updated><resolved>2014-02-20T08:11:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-19T10:18:52Z" id="35484082">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Network outage keeps split brain status (no recovery by ES)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5144</link><project id="" key="" /><description>Hi,

we have a 4 node ES cluster running ("plain" Zen discovery - no cloud stuff). Two nodes are in one DC - two nodes in another DC.

When the network connection between both DCs fails, ES forms two two-node ES clusters - a split brain. When the network is operative again, the split brain situation is remains persistent.

I've setup a small local test with a 4 node ES cluster:

```
+--------+                         +--------+
| Node A | ----\             /---- | Node C |
+--------+      \.........../      +--------+
+--------+      /           \      +--------+
| Node B | ----/             \---- | Node D |
+--------+                         +--------+
               Single ES cluster
```

When the network connection fails, two two node clusters exists (split brain). I've simulated that with "iptables -A INPUT/OUTPUT -s/d &lt;IP&gt; -j DROP" statements.

```
+--------+                         +--------+
| Node A | ----\             /---- | Node C |
+--------+      \           /      +--------+
+--------+      /           \      +--------+
| Node B | ----/             \---- | Node D |
+--------+                         +--------+
  ES cluster                      ES cluster
```

When the network between nodes AB and CD is operative again, the single cluster status is not restored (split brain is persistent).

It did not make a difference, whether unicast or multicast ZEN discovery is used.

Another issue is that operating system keepalive settings affects the time after which ES detects a node failure. Keepalive timeout settings (e.g. net.ipv4.tcp_keepalive_time/probes/intvl) directly influence the node failure detection.

There should be some task, that regularly polls the "alive" status of all known other nodes.
</description><key id="27721107">5144</key><summary>Network outage keeps split brain status (no recovery by ES)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snazy</reporter><labels /><created>2014-02-17T14:10:54Z</created><updated>2014-12-29T11:48:32Z</updated><resolved>2014-12-29T11:48:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="snazy" created="2014-02-17T14:11:20Z" id="35284545">Forgot to mention: tested with ES 1.0.0 (and an older 0.90.3)
</comment><comment author="dadoonet" created="2014-02-17T14:18:51Z" id="35285181">Did you try to set `minimum_master_node` to 3? See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election
</comment><comment author="snazy" created="2014-02-17T14:43:00Z" id="35287230">Setting minimum_master_nodes to 3 is not an option. If I understand correctly, it would force all 4 nodes to stop working at all - means: no service at all. This wouldn't cover the case, that two nodes are taken down for maintenance work. And what if there a three DCs (each with 2 nodes) - a setting of minimum_master_nodes=5 would only allow one node to fail before ES stops working.

IMHO there should be a regular job inside ES, that checks the existence of other nodes (either via unicast or via multicast) and triggers (re-)discovery if necessary - the split brain situation must be resolved.
</comment><comment author="dadoonet" created="2014-02-17T15:36:35Z" id="35293698">Exactly. Cluster will stop working until network connection is up again.
What do you expect? Which part of the cluster should hold the master in case of network outage?

Cross Data center replication is not supported yet and you should consider:
- use the great snapshot and restore feature to snapshot from a DC and restore in the other one
- index in both DC (so two distinct clusters) from a client level
- use Tribe node feature to search or index on multiple clusters

I think we should move this conversation to the mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed multi term queries support in postings highlighter for non top-level queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5143</link><project id="" key="" /><description>In #4052 we added support for highlighting multi term queries using the postings highlighter. That worked only for top-level queries though, and not for multi term queries that are nested for instance within a bool query, or filtered query, or a constant score query.

The way we can make this work is by walking the query structure and temporarily overriding the query rewrite method with a method that allows for multi terms extraction.

Closes #5127 
</description><key id="27713346">5143</key><summary>Fixed multi term queries support in postings highlighter for non top-level queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Highlighting</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-17T13:02:59Z</created><updated>2015-06-07T23:24:17Z</updated><resolved>2014-02-21T20:46:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-17T13:55:03Z" id="35283294">The change looks good to me.

On a related node: should we have an option to not rewrite queries in order to ensure highlighting remains fast? cc @s1monw 
</comment><comment author="javanna" created="2014-02-17T13:58:02Z" id="35283488">Makes sense to me, I also plan to look at the upcoming changes in Lucene 4.7 aka faster ways to highlight multi term queries.
</comment><comment author="roytmana" created="2014-02-17T19:20:56Z" id="35314459">Does it also fixes wildcard highlighting issue?
</comment><comment author="jpountz" created="2014-02-17T19:23:31Z" id="35314664">@roytmana yes, this change will make wildcard queries highlighted
</comment><comment author="roytmana" created="2014-02-17T19:37:04Z" id="35315793">thank you @jpountz 
Looks like it is still in @javanna repository. I will test it once it has been committed  to ES repo
</comment><comment author="nik9000" created="2014-02-17T20:02:58Z" id="35317926">If this change would make multi term queries highlight at the speed they do with the plain highlighter then please. I'd prefer the default be the faster option with an option to do the highlighting for multi term queries. If not that then at least a prominent warning. Or something. 

Sent from my iPhone

&gt; On Feb 17, 2014, at 8:55 AM, Adrien Grand notifications@github.com wrote:
&gt; 
&gt; The change looks good to me.
&gt; 
&gt; On a related node: should we have an option to not rewrite queries in order to ensure highlighting remains fast? cc @s1monw
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="javanna" created="2014-02-18T09:34:33Z" id="35366781">Hi @nik9000 I see your point... on the other hand if one wants to highlight multi term queries that's what it takes at this time. It feels pretty bad not to return any snippet by default to keep highlighting fast. My take is that the best way to keep highlighting fast is not to highlight multi term queries, that's it. I'd add a warning to the docs that explains what we do with multi term queries and the fact that it can slow things down.

Also we might be able to improve this once lucene 4.7 gets released, as mentioned above.
</comment><comment author="nik9000" created="2014-02-18T11:55:02Z" id="35376949">Sounds good to me. 

Sent from my iPhone

&gt; On Feb 18, 2014, at 4:34 AM, Luca Cavanna notifications@github.com wrote:
&gt; 
&gt; Hi @nik9000 I see your point... on the other hand if one wants to highlight multi term queries that's what it takes at this time. It feels pretty bad not to return any snippet by default to keep highlighting fast. My take is that the best way to keep highlighting fast is not to highlight multi term queries, that's it. I'd add a warning to the docs that explains what we do with multi term queries and the fact that it can slow things down.
&gt; 
&gt; Also we might be able to improve this once lucene 4.7 gets released, as mentioned above.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="roytmana" created="2014-02-20T16:56:03Z" id="35643014">Hi @javanna, will it be merged into 1.0 branch so I can test it? 
</comment><comment author="s1monw" created="2014-02-21T11:36:49Z" id="35723413">ok LGTM in general while I really don't like how this works in general :(
</comment><comment author="javanna" created="2014-02-21T11:43:19Z" id="35723801">Neither do I. One other option is to remove the `setRewriteMethod` call in any case and let users set the `rewrite_method` themselves as it's exposed in the query DSL. Even better, as soon as lucene 4.7 is out we can look into [LUCENE-5415](https://issues.apache.org/jira/browse/LUCENE-5415) and hopefully replace our current logic when it comes to multi term queries.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_exists_ doesn't work on objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5142</link><project id="" key="" /><description>At least in 1.0.0

``` shell
# create index and object
curl -X PUT 'http://127.0.0.1:9200/wtf/?pretty'
curl -X PUT 'http://127.0.0.1:9200/wtf/test/1?pretty' -d '{"complex": { "object": "it is" }, "simple": "value"}'

# no results
curl -X POST 'http://127.0.0.1:9200/wtf/test/_search?pretty&amp;q=_exists_:complex'
# has results
curl -X POST 'http://127.0.0.1:9200/wtf/test/_search?pretty&amp;q=_exists_:simple'

# has results
curl -X POST 'http://127.0.0.1:9200/wtf/test/_search?pretty' -d '{ "query": { "filtered": { "filter": { "not": { "missing": { "field": "complex" } } } } } }'

# has results
curl -X POST 'http://127.0.0.1:9200/wtf/test/_search?pretty' -d '{ "query": { "filtered": { "filter": { "not": { "missing": { "field": "simple" } } } } } }'
```

This behaviour  should be either fixed or noted in docs.
</description><key id="27712337">5142</key><summary>_exists_ doesn't work on objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-17T12:44:58Z</created><updated>2014-02-20T08:12:19Z</updated><resolved>2014-02-20T08:09:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-17T12:58:19Z" id="35254471">The `exists` filter works by looking at matches in the inverted index for any value of the given field. The reason why it doesn't work on object is that they are not indexed. In the example that you gave, the inverted index only has entries for `complex.object` and `simple`.

I will a a note to the documentation.
</comment><comment author="bobrik" created="2014-02-17T13:02:51Z" id="35254778">So `missing` filter will use field data and not inverted index, right?
</comment><comment author="jpountz" created="2014-02-17T13:09:24Z" id="35255207">The `missing` filter works exactly as if you wrapped the `exists` filter under a `not` filter, so the same limitations apply.
</comment><comment author="bobrik" created="2014-02-17T13:34:28Z" id="35256986">Since this works:

```
curl -X POST 'http://127.0.0.1:9200/wtf/test/_search?pretty' -d '{ "query": { "filtered": { "filter": { "exists": { "field": "complex" } } } } }'
```

and this doesn't:

```
curl -X POST 'http://127.0.0.1:9200/wtf/test/_search?pretty&amp;q=_exists_:complex'
```

I understand that `_exists_` in query only uses inverted index, but `exists` filter knows that `complex` is object and cannot be found in index so field data should be used. Do I get this right?
</comment><comment author="jpountz" created="2014-02-17T14:28:58Z" id="35286052">I had to check the code to understand what happens, the difference is that `exists` tries to be smarter than `_exists_` when matching an object by using sub-fields. That is, `exists` is able to translate `"exists": { "field": "complex"}` to `"exists": { "field": "complex.object"}`. Let me see if I can fix `_exists_` to behave consistently with `exists`.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>src/main/java/org/elasticsearch/index/query/ExistsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/MissingFilterParser.java</file></files><comments><comment>Make _exists_/_missing_ behave consistently with exists/missing.</comment></comments></commit></commits></item><item><title>Delete by query doesn't work in persistent http connection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5141</link><project id="" key="" /><description>This is probably connected with #2665.

Here's test-case in php:

``` php
&lt;?php

$conn = curl_init();
$host = 'http://127.0.0.1:9200';

function send($conn, $host, $method, $path, $query = array(), $body = null) {
    // uncomment the next line to make new connection for every request
    // $conn = curl_init();

    $url = $host . $path . '?' . http_build_query($query);

    curl_setopt($conn, CURLOPT_URL, $url);

    if ($body) {
        curl_setopt($conn, CURLOPT_POSTFIELDS, $body);
    }

    curl_setopt($conn, CURLOPT_CUSTOMREQUEST, $method);

    ob_start();
    curl_exec($conn);
    $result = ob_get_clean();

    echo $method . ' ' . $url . ' ' . $body."\n";
    echo '&gt; ' . $result . "\n";

    return $result;
}

send($conn, $host, "DELETE", '/elastica_test/');
send($conn, $host, "PUT", '/elastica_test/', array(), '{"index":{"number_of_shards":1,"number_of_replicas":0}}');
send($conn, $host, "PUT", '/elastica_test/test/1', array(), '{"name":"ruflin nicolas"}');
send($conn, $host, "PUT", '/elastica_test/test/2', array(), '{"name":"ruflin"}');
send($conn, $host, "PUT", '/elastica_test/test/3', array(), '{"name":"ian"}');
send($conn, $host, "POST", '/elastica_test/_refresh');
send($conn, $host, "POST", '/elastica_test/_search', array(), '{"query":{"query_string":{"query":"ruflin*"}}}');
send($conn, $host, "POST", '/elastica_test/_search', array(), '{"query":{"query_string":{"query":"nicolas"}}}');
// comment the next request to fix everything
send($conn, $host, "POST", '/elastica_test/_search', array(), '{"query":{"query_string":{"query":"ian"}}}');
send($conn, $host, "DELETE", '/elastica_test/_query', array('q' =&gt; 'nicolas'));
send($conn, $host, "POST", '/elastica_test/_refresh');
send($conn, $host, "POST", '/elastica_test/_search', array(), '{"query":{"query_string":{"query":"ruflin*"}}}');
```

The last query should respond with just one hit, but it returns document that was removed. This works as it should if new connection is made for every request (see function `send`), it also works if I comment search (sic!) request before delete-by-query.

I'm sorry for php, I couldn't make it work with curl or nc.
</description><key id="27709252">5141</key><summary>Delete by query doesn't work in persistent http connection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2014-02-17T11:44:22Z</created><updated>2014-03-13T08:33:57Z</updated><resolved>2014-03-11T18:42:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-02-26T14:20:07Z" id="36129116">Any news about this issue?

Using new connection for every single request doesn't look like a good idea.
</comment><comment author="bobrik" created="2014-03-11T17:18:16Z" id="37322852">On 0.9.11 you could spot in logs:

```
[2014-03-11 21:14:54,830][DEBUG][action.deletebyquery     ] [Nick Fury] [elastica_test][0], node[v4P07UGDRMeJm8z-q3fWlg], [P], s[STARTED]: Failed to execute [delete_by_query {[elastica_test][], query [{"query":{"query_string":{"query":"ian"}}}]}]
org.elasticsearch.index.query.QueryParsingException: [elastica_test] No query registered for [query]
        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:211)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:284)
        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:225)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:426)
        at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnPrimary(TransportShardDeleteByQueryAction.java:113)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

And this is after:

```
send($conn, $host, "DELETE", '/elastica_test/_query', array('q' =&gt; 'nicolas'));
```

where query is different. Clearly requests are messed up.
</comment><comment author="kimchy" created="2014-03-11T17:36:21Z" id="37325162">it doesn't seem like the delete by query relates to persistent http connections from the client? Maybe the client resets the connections on errors and it simply tries to reconnect and fail all the time?

Note that the format of the body of delete by query changed between 0.90. and 1.0, maybe the failure is because of that.
</comment><comment author="clintongormley" created="2014-03-11T17:36:28Z" id="37325180">```
send($conn, $host, "DELETE", '/elastica_test/_query', array('q' =&gt; 'nicolas'));
```

Judging by your other request, that `q` is being sent in the body, not in the query string as it should be.

Alternatively, you could do:

```
DELETE /elastica_test/_query
{ 
    "query_string":  {
        "query": "nicholas"
    }
}
```

And from 1.0 onwards:

```
DELETE /elastica_test/_query
{ 
    "query: {
        "query_string":  {
            "query": "nicholas"
        }
    }
}
```
</comment><comment author="bobrik" created="2014-03-11T18:06:33Z" id="37328991">@clintongormley it's query string, body is the 6th parameter.

Here is output for 1.0.0:

```
DELETE http://127.0.0.1:9200/elastica_test/?
&gt; {"acknowledged":true}
PUT http://127.0.0.1:9200/elastica_test/? {"index":{"number_of_shards":1,"number_of_replicas":0}}
&gt; {"acknowledged":true}
PUT http://127.0.0.1:9200/elastica_test/test/1? {"name":"ruflin nicolas"}
&gt; {"_index":"elastica_test","_type":"test","_id":"1","_version":1,"created":true}
PUT http://127.0.0.1:9200/elastica_test/test/2? {"name":"ruflin"}
&gt; {"_index":"elastica_test","_type":"test","_id":"2","_version":1,"created":true}
PUT http://127.0.0.1:9200/elastica_test/test/3? {"name":"ian"}
&gt; {"_index":"elastica_test","_type":"test","_id":"3","_version":1,"created":true}
POST http://127.0.0.1:9200/elastica_test/_refresh?
&gt; {"_shards":{"total":1,"successful":1,"failed":0}}
POST http://127.0.0.1:9200/elastica_test/_search? {"query":{"query_string":{"query":"ruflin*"}}}
&gt; {"took":31,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"elastica_test","_type":"test","_id":"1","_score":1.0, "_source" : {"name":"ruflin nicolas"}},{"_index":"elastica_test","_type":"test","_id":"2","_score":1.0, "_source" : {"name":"ruflin"}}]}}
POST http://127.0.0.1:9200/elastica_test/_search? {"query":{"query_string":{"query":"nicolas"}}}
&gt; {"took":3,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.8784157,"hits":[{"_index":"elastica_test","_type":"test","_id":"1","_score":0.8784157, "_source" : {"name":"ruflin nicolas"}}]}}
POST http://127.0.0.1:9200/elastica_test/_search? {"query":{"query_string":{"query":"ian"}}}
&gt; {"took":2,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.4054651,"hits":[{"_index":"elastica_test","_type":"test","_id":"3","_score":1.4054651, "_source" : {"name":"ian"}}]}}
DELETE http://127.0.0.1:9200/elastica_test/_query?q=nicolas
&gt; {"_indices":{"elastica_test":{"_shards":{"total":1,"successful":1,"failed":0}}}}
POST http://127.0.0.1:9200/elastica_test/_refresh?
&gt; {"_shards":{"total":1,"successful":1,"failed":0}}
POST http://127.0.0.1:9200/elastica_test/_search? {"query":{"query_string":{"query":"ruflin*"}}}
&gt; {"took":2,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"elastica_test","_type":"test","_id":"1","_score":1.0, "_source" : {"name":"ruflin nicolas"}},{"_index":"elastica_test","_type":"test","_id":"2","_score":1.0, "_source" : {"name":"ruflin"}}]}}
```

@kimchy there are no errors as you can see.
</comment><comment author="clintongormley" created="2014-03-11T18:19:35Z" id="37330727">@bobrik Sorry yes, I misread.

The curl docs say that if you plan on reusing the same connection for a GET after a POST, then you should reset it with CURLOPT_HTTPGET: http://curl.haxx.se/libcurl/c/curl_easy_setopt.html#CURLOPTHTTPGET

I wonder if the same thing applies to a DELETE?

Perhaps try it?
</comment><comment author="bobrik" created="2014-03-11T18:42:04Z" id="37333565">I've put tcp proxy in the middle and found out that curl reused body from previous requests if they were not cleared explicitly. Gonna fix that it [ruflin/Elastica](https://github.com/ruflin/Elastica) then.
</comment><comment author="ruflin" created="2014-03-13T08:33:57Z" id="37509746">@clintongormley Thanks for the input. It's good to learn something new every day ;-)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Test wether Elasticsearch is already up before launching it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5140</link><project id="" key="" /><description>I noticed that when launching elasticsearch twice, I can see two processes under linux
</description><key id="27705760">5140</key><summary>Test wether Elasticsearch is already up before launching it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">omebarki</reporter><labels /><created>2014-02-17T10:41:15Z</created><updated>2014-07-16T21:48:29Z</updated><resolved>2014-02-19T13:53:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-02-19T02:31:53Z" id="35459955">That is intentional. We want to allow running multiple copies of elasticsearch at the same time. If this is not desired behavior in your case, you can set `node.max_local_storage_nodes: 1` to prevent more than one data or master node to run using the same data directory.
</comment><comment author="omebarki" created="2014-02-19T13:53:37Z" id="35500438">OK, thanks a lot for your answer.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Optimize multiple cluster state processing on receiving nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5139</link><project id="" key="" /><description>Nodes that receive the cluster state, and they have several of those pending, can optimize and try and process potentially only one of those.
</description><key id="27704013">5139</key><summary>Optimize multiple cluster state processing on receiving nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-17T10:10:19Z</created><updated>2015-06-07T15:22:13Z</updated><resolved>2014-02-17T10:10:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file></files><comments><comment>Optimize multiple cluster state processing on receiving nodes</comment><comment>Nodes that receive the cluster state, and they have several of those pending, can optimize and try and process potentially only one of those.</comment><comment>closes #5139</comment></comments></commit></commits></item><item><title>Package versioning ( Deb &amp; RPM ) modifications</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5138</link><project id="" key="" /><description>Currently when releasing multiple packages of the same version result in some issues this is because we don't add a package version.
The package version is used to determine which package is the latest at a new install or available for for upgrading.

The new package naming convention should be:

elasticsearch-x.x.x-yy(.zzz)

x.x.x is the actual version; for example 1.0.0
yy is the package version. For stable/final releases this is '1'. for any release before that we start at '0.1' and increment it for every release.
zzz is optional and can contain any string like 'Beta1' or 'RC2'.

The resulting package naming for 1.0.0 is:

elasticsearch-1.0.0-0.1.Beta1
elasticsearch-1.0.0-0.2.Beta2
elasticsearch-1.0.0-0.3.RC1
elasticsearch-1.0.0-0.4.RC2
elasticsearch-1.0.0-1
</description><key id="27701000">5138</key><summary>Package versioning ( Deb &amp; RPM ) modifications</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electrical</reporter><labels><label>:Packaging</label></labels><created>2014-02-17T09:15:18Z</created><updated>2015-10-14T14:22:36Z</updated><resolved>2015-10-14T14:22:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T11:48:04Z" id="68251293">@electrical is this still an issue?
</comment><comment author="electrical" created="2014-12-29T12:25:43Z" id="68253337">This is still an issue yeah with the current naming convention for packages.
</comment><comment author="clintongormley" created="2015-10-14T14:22:36Z" id="148065193">Our current package naming now allows for packages to be sorted correctly, without requiring a package version.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw parsing exception if terms filter or query has more than one field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5137</link><project id="" key="" /><description>Closes #5014
</description><key id="27682683">5137</key><summary>Throw parsing exception if terms filter or query has more than one field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-16T21:35:58Z</created><updated>2015-06-07T16:56:22Z</updated><resolved>2014-10-12T20:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-17T10:47:51Z" id="35246222">The TermsFilter constructor that we use indeed only supports filtering on a single field but there is another constructor that takes a list of terms (instead of a list of BytesRefs) and supports filtering based on terms that come from different fields. Maybe we could use it?
</comment><comment author="s1monw" created="2014-02-18T13:53:56Z" id="35385656">+1 to fixing the terms filter parser to allow different fields!
</comment><comment author="kimchy" created="2014-02-18T13:56:26Z" id="35385872">Is it really a common use case, that justifies complicating the terms filter? compared to having a terms filter that is then OR'ed using the regular query DSL?
</comment><comment author="s1monw" created="2014-02-18T13:57:16Z" id="35385942">I don't think it complicates it at all. to me it just does what you would expect.
</comment><comment author="kimchy" created="2014-02-18T14:02:57Z" id="35386432">maybe we can see an example of how this would look DSL wise, and decide based on that. No problem with adding it if it makes sense.
</comment><comment author="s1monw" created="2014-02-18T14:10:26Z" id="35387108">to me it woudl just look like this:

``` json
{
    "constant_score" : {
        "filter" : {
            "terms" : { 
                   "user" : ["kimchy", "elasticsearch"],
                   "company" : ["elasticsearch"]
           }
        }
    }
}
```
</comment><comment author="kimchy" created="2014-02-18T14:14:24Z" id="35387426">That can work, though it will be the first filter that works across fields in ES, and I personally find the lack of boolean logic between fields strange (not very evident if its AND or OR). Would love to hear other people thoughts.
</comment><comment author="dadoonet" created="2014-02-18T17:01:53Z" id="35406553">Naive answer: writing filter like this makes me think that we want that both Terms filters to match.
So to me, it's a `AND` here.
</comment><comment author="imotov" created="2014-02-19T03:35:40Z" id="35462859">We are handling boolean logic to some degree through [execution mode](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-terms-filter.html#_execution_mode). So, perhaps, we can extend it to the case of multiple fields. It might become messy though. 
</comment><comment author="s1monw" created="2014-02-19T08:20:28Z" id="35475679">so to me the confusion is in the name. It's called `terms_filter` and a `term` is a `(field, value)` tuple. I don't see how multiple fields make anything more complex or messy. You have _N_ terms and you wanna fitler on them nothing changes if you have multiple fields they are still _N_ terms? Exec mode is still the same you have a set of terms that's it.
</comment><comment author="imotov" created="2014-02-20T18:21:04Z" id="35652187">@s1monw it becomes more complicated because before it was one dimensional (multiple values). Now it's two dimensional: (multiple fields and multiple values). Your simplification (which is natural for you since it follows Lucene design) is only one possible way to interpret the query. It also doesn't fit the current query syntax nicely since you don't specify this query as:

```
{
    "constant_score" : {
        "filter" : {
            "terms" : [{
                "field": "user",
                "value": "kimchy"
            }, {
                "field": "user",
                "value": "elasticsearch"
            }, {
                "field": "company",
                "value": "elasticsearch"
            }]
        }
    }
}
```
</comment><comment author="s1monw" created="2014-02-20T18:33:09Z" id="35653404">fair enough - I thought my example was simple and consistent:

``` JSON
{
    "constant_score" : {
        "filter" : {
            "terms" : { 
                   "user" : ["kimchy", "elasticsearch"],
                   "company" : ["elasticsearch"]
           }
        }
    }
}
```

but apparently it isn't... I don't have strong feelings but it feels to me it's the wrong thing todo to throw  an exception....
</comment><comment author="lordaugustus" created="2014-02-22T11:01:28Z" id="35800096">A wildcard form like this
"tag.*" : ["sushi", "booyah"]
would be useful for people storing inner fields that are not predefined, like
"tag.en", "tag.de".
Maybe it is not a good schema design for multilingual data? I am not sure, but the proposed change would definitely be helpful in this case.
</comment><comment author="clintongormley" created="2014-05-10T12:12:53Z" id="42740126">I'm torn on this.  This is a frequent mistake that users make; they clearly expect something like this to work. 

I agree completely with @kimchy that it is not obvious whether the logic is AND or OR.   And I agree with @dadoonet that the most likely interpretation is as an AND, so:

```
{
    "constant_score" : {
        "filter" : {
            "terms" : { 
                   "user" : ["kimchy", "elasticsearch"],
                   "company" : ["elasticsearch"]
           }
        }
    }
}
```

... would be interpreted as:

```
user IN ["kimchy","elasticsearch"] AND company = 'elasticsearch'
```

But then there is no easy way to specify that the AND should be an OR. In the example of wildcarded field names, the user would expect an OR:

```
{ "terms": { "user.*": ["kimchy","elasticsearch"] }}
```

In fact, looking at the current `terms` query &amp; filter, there is already room for parameter/fieldname clashes, eg:

```
{ "terms": {
    "execution": "bool",
    "field": ["values"..."]
}}
```

So you can't have a field called `execution`, `_cache`, `_cache_key`, `_name` or `minimum_match`.  I'm OK with disallowing fields starting with `_`, but I'm not crazy about having `execution` and `minimum_match` at the same level as fieldname.  In other clauses we handle this with, eg:

```
{ "regexp": {
    "field": {
        "value": "xx",
        "flags": "..."
}}
```

We could do:

```
{ "terms": {
    "fieldname": {
        "execution": "bool",
        "values": ["foo","bar"]
    },
    "other_fieldname": ["one","two"]
}}
```

But that still doesn't give us place to include the overall AND/OR logic for multiple fields, unless we add a top-level `_operator` flag.  Of course, elsewhere in the DSL that flag is called `operator`, which makes this inconsistent.

One way to do it would be to have different filter names, eg 

```
{ "any": { "user.*": ["kimchy", "elasticsearch"]}}
{ "all": {
    "user" : ["kimchy", "elasticsearch"],
    "company" : ["elasticsearch"]
}
```

(We're missing `terms` in the name, but `any_terms` is wrong because it is really `terms_in_any_field`)

And really, that's not very far from:

```
{ "and": [
    { "terms": { "user": ["kimchy", "elasticsearch" ]}},
    { "term": { "company": "elasticsearch" }}
]}
```

...but it doesn't handle wildcards in the field name.

So, lots of rumination without any solutions.  What I dislike about all of these suggestions is that they complicate the `terms` filter and require the user to remember things (what's the default operator? etc)

My inclination would be to leave things as they are, except that we throw an exception if more than one field name is specified.
</comment><comment author="clintongormley" created="2014-08-22T07:13:44Z" id="53030138">Revisiting this PR: changing the behaviour of the `terms` filter/query is beyond the scope of the current work, and this PR accurately reflects the situation as it is today.  I think we should review and merge.
</comment><comment author="GaelTadh" created="2014-10-10T11:12:00Z" id="58641993">LGTM but also looks like this is the current behavior. Did this PR already get merged ? If so can it be closed ?
</comment><comment author="s1monw" created="2014-10-10T19:41:20Z" id="58706160">I agree with @GaelTadh lets get this in @imotov 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JAVA_HOME is not checked in ubuntu start script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5136</link><project id="" key="" /><description>I've installed on Ubuntu 12.04 from the elasticsearch.org apt repository. According to the instructions on [the setup page of the documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html), the start script should be checking JAVA_HOME. What actually happens is that it only checks for JAVA_HOME in the /etc/default/elasticsearch file, not in the current environment. 

I'm not sure whether it's the documentation or the start script that is wrong but one of them should be updated. 
</description><key id="27659934">5136</key><summary>JAVA_HOME is not checked in ubuntu start script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kevinstembridge</reporter><labels /><created>2014-02-15T23:33:42Z</created><updated>2014-02-17T12:56:23Z</updated><resolved>2014-02-17T12:56:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-17T07:54:10Z" id="35235091">Hey,

How did you test this? Reading the `/etc/init.d/elasticsearch` you can very well just configure your custom JAVA_HOME (that variable is not touched or changed if it already is configured). The following worked for me (just downloaded some arbitrary jdk to test)

```
export JAVA_HOME=/tmp/openjdk-1.7.0-u40-unofficial-linux-i586-image/bin/java
/etc/init.d/elasticsearch start
# ps auxw | grep elasticsearch
elasticsearch      25401  4.2 22.5 1230532 114368 ?      Sl   07:50   0:02 /tmp/openjdk-1.7.0-u40-unofficial-linux-i586-image/bin/java .... [and way more here]
```

Can you give some more information what is not working as expected in your case?
</comment><comment author="kevinstembridge" created="2014-02-17T11:10:11Z" id="35247637">Thanks for checking this out for me. I've just had another go at figuring out what was wrong. I was invoking 'sudo /etc/init.d/elasticsearch start', which apparently doesn't retain environment variables. Passing the -E flag to sudo works. 
</comment><comment author="spinscale" created="2014-02-17T12:56:23Z" id="35254349">great, thanks for your feedback!

Closing then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>apt-get installs wrong 1.0.0 version of elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5135</link><project id="" key="" /><description>Hi,

Recently discovered that apt-get upgrade elasticsearch installs 1.0.0RC2 and _not_ 1.0.0. At attempt to force the version causes a warning that the package is about to be downgraded

```
# apt-get install elasticsearch=1.0.0
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be DOWNGRADED:
  elasticsearch
0 upgraded, 0 newly installed, 1 downgraded, 0 to remove and 0 not upgraded.
```

A brief chat on irc shows that this is known and fix is coming (however, I couldn't seem to find a github ticket so that I can watch when this is fixed - hence this ticket.)

```
18:20 &lt;@honzakral&gt; dharrigan: it is,we are aware of that and fix is coming - it's based on the sorting algorithm apt uses to determine the latest version
```

Thank you and eagerly looking forward to this! :-)

-=david=-
</description><key id="27654337">5135</key><summary>apt-get installs wrong 1.0.0 version of elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dharrigan</reporter><labels /><created>2014-02-15T18:34:01Z</created><updated>2014-02-17T09:13:25Z</updated><resolved>2014-02-17T09:13:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-17T08:54:03Z" id="35238461">hey,

can you retest by running `apt-get update` and `apt-get upgrade` and tell if it works?
</comment><comment author="dharrigan" created="2014-02-17T09:05:03Z" id="35239126">Hi,

Thanks for the message. I see that it's going to try and install Elasticsearch 1.0.0-1. 

```
# apt-get install elasticsearch 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
  elasticsearch
1 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
Need to get 18.4 MB of archives.
After this operation, 1,226 kB of additional disk space will be used.
Get:1 http://packages.elasticsearch.org/elasticsearch/1.0/debian/ stable/main elasticsearch all 1.0.0-1 [18.4 MB]
Fetched 18.4 MB in 4s (4,542 kB/s)         
(Reading database ... 89414 files and directories currently installed.)
Preparing to replace elasticsearch 0.90.11 (using .../elasticsearch_1.0.0-1_all.deb) ...
Unpacking replacement elasticsearch ...
Processing triggers for ureadahead ...
Setting up elasticsearch (1.0.0-1) ...
```

If that is correct (as you expect), then all good here! :-)

-=david=-
</comment><comment author="spinscale" created="2014-02-17T09:13:25Z" id="35239649">Great! Thanks a lot for reporting and testing!

Closing then.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>AWS Plugin 2.0.0RC1 with 1.0.0 ES does not seem to be working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5134</link><project id="" key="" /><description>I followed the instructions at http://www.elasticsearch.org/tutorials/elasticsearch-on-ec2/ but they are out of date.

I tried to finesse the versions and compatibility flags but to no avail...

Here is my config:

```
cloud:
    aws:
        access_key: akey
        secret_key: skey
    discovery:
        type: ec2
    gateway:
        type: s3
        s3:
              bucket: mybucket
```

Here is the output ... seems like the plugin is loaded but it does not mention s3

```
[ec2-user@ip-10-73-158-58 elasticsearch-1.0.0]$ sudo bin/elasticsearch
[2014-02-15 10:39:53,109][INFO ][node                     ] [Crimebuster] version[1.0.0], pid[21364], build[a46900e/2014-02-12T16:18:34Z]
[2014-02-15 10:39:53,109][INFO ][node                     ] [Crimebuster] initializing ...
[2014-02-15 10:39:53,145][INFO ][plugins                  ] [Crimebuster] loaded [cloud-aws], sites []
[2014-02-15 10:39:55,626][DEBUG][discovery.zen.ping.multicast] [Crimebuster] using group [224.2.2.4], with port [54328], ttl [3], and address [null]
[2014-02-15 10:39:55,632][DEBUG][discovery.zen.ping.unicast] [Crimebuster] using initial hosts [], with concurrent_connects [10]
[2014-02-15 10:39:55,634][DEBUG][discovery.zen            ] [Crimebuster] using ping.timeout [3s], master_election.filter_client [true], master_election.filter_data [false]
[2014-02-15 10:39:55,635][DEBUG][discovery.zen.elect      ] [Crimebuster] using minimum_master_nodes [-1]
[2014-02-15 10:39:55,636][DEBUG][discovery.zen.fd         ] [Crimebuster] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2014-02-15 10:39:55,652][DEBUG][discovery.zen.fd         ] [Crimebuster] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2014-02-15 10:39:56,832][DEBUG][gateway.local            ] [Crimebuster] using initial_shards [quorum], list_timeout [30s]
[2014-02-15 10:39:57,227][DEBUG][gateway.local.state.meta ] [Crimebuster] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
[2014-02-15 10:39:57,242][DEBUG][gateway.local.state.meta ] [Crimebuster] took 14ms to load state
[2014-02-15 10:39:57,243][DEBUG][gateway.local.state.shards] [Crimebuster] took 0s to load started shards state
[2014-02-15 10:39:57,287][INFO ][node                     ] [Crimebuster] initialized
[2014-02-15 10:39:57,288][INFO ][node                     ] [Crimebuster] starting ...
[2014-02-15 10:39:57,474][INFO ][transport                ] [Crimebuster] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.73.158.58:9300]}
[2014-02-15 10:39:57,492][TRACE][discovery                ] [Crimebuster] waiting for 30s for the initial state to be set by the discovery
[2014-02-15 10:39:57,509][TRACE][discovery.zen.ping.multicast] [Crimebuster] [1] sending ping request
[2014-02-15 10:39:59,010][TRACE][discovery.zen.ping.multicast] [Crimebuster] [1] sending ping request
[2014-02-15 10:40:00,513][TRACE][discovery.zen            ] [Crimebuster] full ping responses: {none}
[2014-02-15 10:40:00,513][DEBUG][discovery.zen            ] [Crimebuster] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2014-02-15 10:40:00,521][INFO ][cluster.service          ] [Crimebuster] new_master [Crimebuster][ludNsyJbQyigbqt7XjBsLg][ip-10-73-158-58][inet[/10.73.158.58:9300]], reason: zen-disco-join (elected_as_master)
[2014-02-15 10:40:00,571][TRACE][discovery                ] [Crimebuster] initial state set from discovery
[2014-02-15 10:40:00,571][INFO ][discovery                ] [Crimebuster] elasticsearch/ludNsyJbQyigbqt7XjBsLg
[2014-02-15 10:40:00,604][INFO ][http                     ] [Crimebuster] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.73.158.58:9200]}
[2014-02-15 10:40:00,630][INFO ][gateway                  ] [Crimebuster] recovered [0] indices into cluster_state
[2014-02-15 10:40:00,631][INFO ][node                     ] [Crimebuster] started
```
</description><key id="27646699">5134</key><summary>AWS Plugin 2.0.0RC1 with 1.0.0 ES does not seem to be working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raymcdermott</reporter><labels /><created>2014-02-15T10:45:52Z</created><updated>2014-02-15T10:53:47Z</updated><resolved>2014-02-15T10:53:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="raymcdermott" created="2014-02-15T10:53:47Z" id="35152869">Oops - now I see that I should use the local gateway ... sorry for being a little trigger happy here
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Source filtering with wildcards broken when given multiple patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5133</link><project id="" key="" /><description>```
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch", "retweeted": false
}'
```

No source fields delivered:

```
curl -XGET 'http://localhost:9200/twitter/tweet/1?_source=*.id,retweeted&amp;pretty=yes'
```

`retweeted` returned:

```
curl -XGET 'http://localhost:9200/twitter/tweet/1?_source=retweeted,*.id&amp;pretty=yes'
```

Closes #5132.
</description><key id="27646116">5133</key><summary>Source filtering with wildcards broken when given multiple patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-15T10:01:35Z</created><updated>2015-06-07T23:22:27Z</updated><resolved>2014-02-17T17:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-02-15T12:05:27Z" id="35154569">hey david, thanks for picking it up. Left some comments. Let me know what you think.
</comment><comment author="dadoonet" created="2014-02-16T20:25:26Z" id="35211152">Agreed with all comments @bleskes. I applied them. Let me know.
</comment><comment author="bleskes" created="2014-02-17T15:09:41Z" id="35289627">+1!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/test/java/org/elasticsearch/search/source/SourceFetchingTests.java</file></files><comments><comment>Source filtering with wildcards broken when given multiple patterns</comment></comments></commit></commits></item><item><title>Source filtering with wildcards broken when given multiple patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5132</link><project id="" key="" /><description>```
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch", "retweeted": false
}'
```

No source fields delivered:

```
curl -XGET 'http://localhost:9200/twitter/tweet/1?_source=*.id,retweeted&amp;pretty=yes'
```

`retweeted` returned:

```
curl -XGET 'http://localhost:9200/twitter/tweet/1?_source=retweeted,*.id&amp;pretty=yes'
```

This happens because the filter [breaks out of the loop](https://github.com/elasticsearch/elasticsearch/blob/f2710c16ebd918f646be9d0ab64b4871c25be4c2/src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java#L178) instead of continuing to check whether any of the other `includes` may match. Replacing the `break` with a `continue` fixes this. But I hesitate to submit a pull request because I didn't have time to fully understand this code.

I understand that this is a corner case but it's a new feature in 1.0 and the example in the documentation does not work due to this bug.
</description><key id="27628001">5132</key><summary>Source filtering with wildcards broken when given multiple patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">lfrancke</reporter><labels /><created>2014-02-14T21:58:46Z</created><updated>2014-02-17T17:20:06Z</updated><resolved>2014-02-17T17:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-15T09:45:44Z" id="35151734">Agreed. I can reproduce it and fix it.
Currently running tests and will send a PR if there is no side effect.

BTW, you can send PR if you think it fixes thing and we will be happy to comment on PR if there is anything wrong or if it needs some modification. :-)

Thanks!
</comment><comment author="dadoonet" created="2014-02-15T10:03:19Z" id="35152071">Tests pass with the modification. PR #5133 created with a fix and a test.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/test/java/org/elasticsearch/search/source/SourceFetchingTests.java</file></files><comments><comment>Source filtering with wildcards broken when given multiple patterns</comment></comments></commit></commits></item><item><title>Add "locale" parameter to query_string and simple_query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5131</link><project id="" key="" /><description>Both default to `Locale.ROOT`

Fixes #5128
</description><key id="27627789">5131</key><summary>Add "locale" parameter to query_string and simple_query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-14T21:55:20Z</created><updated>2015-06-07T15:22:47Z</updated><resolved>2014-02-20T22:51:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-19T14:36:32Z" id="35504431">except of the fact that this uses java 1.7 only APIs this looks awesome!
</comment><comment author="dakrone" created="2014-02-20T17:37:06Z" id="35647549">@s1monw I removed the 1.7-specific APIs from this
</comment><comment author="s1monw" created="2014-02-20T19:37:48Z" id="35660334">I think this looks great - Can we maybe have a LocaleUtils that has a `Locale parse(String)` and a `String toString(Locale)` method with some `// JAVA7 - use toLanguageTag` in it? I think we can hide this behind a utils class and once we move to Java7 which is hopefully not far out we can use the new API across the board?
</comment><comment author="dakrone" created="2014-02-20T19:50:02Z" id="35661518">Sounds good, I'll do that. Hopefully going java 7 minimum is not too far out :)
</comment><comment author="s1monw" created="2014-02-20T20:47:16Z" id="35666988">LGTM +1 to squash and push
</comment><comment author="dakrone" created="2014-02-20T22:51:44Z" id="35679384">Merged in 8f8cc72
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ESLogger logs wrong class name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5130</link><project id="" key="" /><description>ESLogger logs wrong class name, method name, line number.

version: 1.0.0

configuration:
  log4j
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c][%C.%M:%L] %m%n"
  slf4j + logback
    &lt;pattern&gt;[%date{ISO8601}][%-5.5level][%-25.25logger][%class{36}.%method:%line] %msg%n&lt;/pattern&gt;

ESLogger logs this.(wrong class name!):
  org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:114

ESLogger always logs the class name "Log4jESLogger".
I thought Elasticsearch can avoid it using other log4j/slf4j methods and I changed.
</description><key id="27623070">5130</key><summary>ESLogger logs wrong class name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">snuffkin</reporter><labels /><created>2014-02-14T20:34:00Z</created><updated>2014-10-29T10:51:49Z</updated><resolved>2014-10-29T10:33:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-15T17:22:58Z" id="35161703">actually, we should probably use the same trick we do in the jdk logger, and use the name we get from the factory. Also, much better to have 2 implementations for slf4j one, decided on the factory creation, one for location aware and one not, and decide on which one in the slf4j factory.
</comment><comment author="snuffkin" created="2014-02-16T12:21:42Z" id="35195419">I checked JdkLogger's trick. JdkLogger logged category as class name. It is wrong. I fixed that JdkLogger logs correct class name. I used the same trick as the java.util.logging package.
And Slf4jESLogger decides logger when it is created.
</comment><comment author="snuffkin" created="2014-02-16T14:26:24Z" id="35197907">I used following JAVA_OPTS and configuration for JdkLogger.

JAVA_OPTS:

```
-Djava.util.logging.config.file="%ES_HOME%/config/logging.properties"
```

logging.properties:

```
handlers = java.util.logging.ConsoleHandler
java.util.logging.ConsoleHandler.level = INFO
java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter
java.util.logging.SimpleFormatter.format=[%1$tFT%1$tT,%1$tL][%4$s][%3$-25s][%2$s] %5$s%n
```
</comment><comment author="kimchy" created="2014-02-16T16:44:34Z" id="35201360">but you don't want to log the class name of the logger, you want to log the "category" name the logger was created for, no?
</comment><comment author="snuffkin" created="2014-02-16T18:04:37Z" id="35205196">Oh, I had mistake. I wanted to say this:

&gt; JdkLogger logged ~~category~~ logger's name as class name. It is wrong.

I don't want to log class name like "Log4jESLogger".
Now, "Log4jESLogger" is logged in every lines of log file.

I want to log class name of the class call ESLogger.
</comment><comment author="clintongormley" created="2014-08-08T10:04:02Z" id="51583852">@snuffkingit is this issue solved for you? i don't really understand where this PR got to.
</comment><comment author="yeroc" created="2014-09-24T21:04:08Z" id="56738456">I just noticed this issue myself.  The patch looks correct for Log4j at least.  The trick (as in the patch) is to use the more low-level Logger.log() call specifying the class name of the Log4j wrapper class so that Log4j knows to skip that one when determining location information.  

Can we get this applied?
</comment><comment author="colings86" created="2014-10-10T10:56:15Z" id="58640732">For context, before this change, you get the following logs when starting a node with the log4j conversionPattern in the initial description:

```
[2014-10-10 11:51:58,087][INFO ][node                     ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] version[2.0.0-SNAPSHOT], pid[36528], build[${build/NA]
[2014-10-10 11:51:58,090][INFO ][node                     ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] initializing ...
[2014-10-10 11:51:58,100][INFO ][plugins                  ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] loaded [], sites []
[2014-10-10 11:52:00,080][INFO ][node                     ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] initialized
[2014-10-10 11:52:00,081][INFO ][node                     ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] starting ...
[2014-10-10 11:52:00,144][INFO ][transport                ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.188:9300]}
[2014-10-10 11:52:00,158][INFO ][discovery                ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] {CLUSTER_NAME}/8l292n_-QKOWUcK741SiWA
[2014-10-10 11:52:03,928][INFO ][cluster.service          ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] new_master [Deathbird][8l292n_-QKOWUcK741SiWA][colins-mbp.default][inet[/192.168.1.188:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-10 11:52:03,945][INFO ][http                     ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.188:9200]}
[2014-10-10 11:52:03,945][INFO ][node                     ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] started
[2014-10-10 11:52:04,464][INFO ][gateway                  ][org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118] [Deathbird] recovered [3] indices into cluster_state
```

Note that all the log lines record the class/method/line number as `org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:118`
</comment><comment author="colings86" created="2014-10-10T11:00:16Z" id="58641041">After applying this change the same logs will now look like the following:

```
[2014-10-10 11:58:17,121][INFO ][node                     ][org.elasticsearch.node.internal.InternalNode.&lt;init&gt;:136] [Gaia] version[2.0.0-SNAPSHOT], pid[36768], build[${build/NA]
[2014-10-10 11:58:17,124][INFO ][node                     ][org.elasticsearch.node.internal.InternalNode.&lt;init&gt;:138] [Gaia] initializing ...
[2014-10-10 11:58:17,133][INFO ][plugins                  ][org.elasticsearch.plugins.PluginsService.&lt;init&gt;:150] [Gaia] loaded [], sites [elasticsearch-groovinator]
[2014-10-10 11:58:19,243][INFO ][node                     ][org.elasticsearch.node.internal.InternalNode.&lt;init&gt;:203] [Gaia] initialized
[2014-10-10 11:58:19,243][INFO ][node                     ][org.elasticsearch.node.internal.InternalNode.start:222] [Gaia] starting ...
[2014-10-10 11:58:19,306][INFO ][transport                ][org.elasticsearch.transport.TransportService.doStart:95] [Gaia] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.188:9300]}
[2014-10-10 11:58:19,319][INFO ][discovery                ][org.elasticsearch.discovery.DiscoveryService.doStart:85] [Gaia] {CLUSTER_NAME}/o-STQydBRqG416MKj0oH_A
[2014-10-10 11:58:23,086][INFO ][cluster.service          ][org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run:401] [Gaia] new_master [Gaia][o-STQydBRqG416MKj0oH_A][colins-mbp.default][inet[/192.168.1.188:9300]], reason: zen-disco-join (elected_as_master)
[2014-10-10 11:58:23,103][INFO ][http                     ][org.elasticsearch.http.HttpServer.doStart:91] [Gaia] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.188:9200]}
[2014-10-10 11:58:23,104][INFO ][node                     ][org.elasticsearch.node.internal.InternalNode.start:256] [Gaia] started
[2014-10-10 11:58:23,599][INFO ][gateway                  ][org.elasticsearch.gateway.GatewayService$GatewayRecoveryListener$1.clusterStateProcessed:272] [Gaia] recovered [3] indices into cluster_state
```
</comment><comment author="colings86" created="2014-10-10T12:39:35Z" id="58649739">@snuffkingit This looks good, sorry for the delay in reviewing this. I have rebased it onto the current master and added a few tests so have had to open a new pull request for it (https://github.com/elasticsearch/elasticsearch/pull/8052), but the commit still has you listed as the author.  Thanks for submitting, please could i ask you to sign our CLA so that i can get it merged in. http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="snuffkin" created="2014-10-10T22:07:18Z" id="58722392">Thank you for accepting this request! I have finished to sign CLA.
</comment><comment author="colings86" created="2014-10-17T15:19:27Z" id="59528539">@snuffkingit thanks for signing the CLA, I hope to get a final review soon and get this merged in
</comment><comment author="colings86" created="2014-10-29T10:51:49Z" id="60903890">@snuffkingit Thanks for the PR, I've now merged it into our repo and the fix should be available form version 1.4
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/logging/jdk/ESLogRecord.java</file><file>src/main/java/org/elasticsearch/common/logging/jdk/JdkESLogger.java</file><file>src/main/java/org/elasticsearch/common/logging/jdk/JdkESLoggerFactory.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/Log4jESLogger.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java</file><file>src/main/java/org/elasticsearch/common/logging/slf4j/Slf4jESLogger.java</file><file>src/test/java/org/elasticsearch/common/logging/LoggingConfigurationTests.java</file><file>src/test/java/org/elasticsearch/common/logging/jdk/JDKESLoggerTests.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java</file></files><comments><comment>Core: Fix location information for loggers</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/logging/jdk/ESLogRecord.java</file><file>src/main/java/org/elasticsearch/common/logging/jdk/JdkESLogger.java</file><file>src/main/java/org/elasticsearch/common/logging/jdk/JdkESLoggerFactory.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/Log4jESLogger.java</file><file>src/main/java/org/elasticsearch/common/logging/slf4j/Slf4jESLogger.java</file><file>src/test/java/org/elasticsearch/common/logging/jdk/JDKESLoggerTests.java</file><file>src/test/java/org/elasticsearch/common/logging/log4j/Log4jESLoggerTests.java</file></files><comments><comment>Core: Fix location information for loggers</comment></comments></commit></commits></item><item><title>fast vector highlighter not working properly with the percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5129</link><project id="" key="" /><description>Results when using fast vector highlighter with the percolator are wrong. The test below reproduces this. The reason is a bug in MemoryIndex, see https://issues.apache.org/jira/browse/LUCENE-5444

Will be resolved with lucene 4.7

```
DELETE phrasetest


PUT phrasetest
{
   "mappings": {
      "doc": {
         "properties": {
            "body": {
               "properties": {
                  "text": {
                     "type": "string",
                     "term_vector": "with_positions_offsets"
                  }
               }
            }
         }
      }
   }
}

POST phrasetest/_refresh

POST phrasetest/.percolator/Q
{
   "query": {
      "match": {
         "text": {
            "query": "foo bar"
         }
      }
   }
}


POST phrasetest/doc/_percolate
{
   "doc": {
      "body": [
         {
            "text": "la la"
         },
         {
            "text": "foo bar foo bar foo"
         }
      ]
   },
   "size": 1,
   "highlight": {
      "fields": {
         "body.text": {
            "type": "fvh",
            "number_of_fragments": 20
         }
      }
   }
}

```

yields:

```
...
"highlight": {
            "body.text": [
               "&lt;em&gt;la &lt;/em&gt;la",
               "fo&lt;em&gt;o b&lt;/em&gt;a&lt;em&gt;r f&lt;/em&gt;o&lt;em&gt;o b&lt;/em&gt;ar foo"
            ]
         }
...
```
</description><key id="27615670">5129</key><summary>fast vector highlighter not working properly with the percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>bug</label></labels><created>2014-02-14T18:37:56Z</created><updated>2015-06-07T23:23:00Z</updated><resolved>2014-02-26T21:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/analysis/miscellaneous/XASCIIFoldingFilter.java</file><file>src/main/java/org/apache/lucene/queryparser/XSimpleQueryParser.java</file><file>src/main/java/org/apache/lucene/search/XReferenceManager.java</file><file>src/main/java/org/apache/lucene/search/XSearcherManager.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/engine/SegmentsStats.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefOrdValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleScriptDataComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/GeoDistanceComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/LongValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/NumberComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/StringScriptDataComparator.java</file><file>src/main/java/org/elasticsearch/index/gateway/fs/FsIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NestedFieldComparatorSource.java</file><file>src/test/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/test/engine/MockInternalEngine.java</file></files><comments><comment>Upgrade to Lucene 4.7</comment></comments></commit></commits></item><item><title>query_string and simple_query_string should allow selective a Locale</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5128</link><project id="" key="" /><description>`query_string` currently defaults to `Locale.getDefault()` while `simple_query_string` uses `Locale.ROOT`.

We should add a "locale" option to both queries that allows the user to select a locale, and unify the defaults to be the same.
</description><key id="27615210">5128</key><summary>query_string and simple_query_string should allow selective a Locale</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-14T18:31:11Z</created><updated>2014-08-19T16:13:35Z</updated><resolved>2014-02-20T22:51:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-14T18:39:55Z" id="35111721">+1
</comment><comment author="dakrone" created="2014-02-14T18:44:19Z" id="35112141">@s1monw what do you think the default should be? `Locale.getDefault()` or `Locale.ROOT`?
</comment><comment author="s1monw" created="2014-02-14T18:45:11Z" id="35112218">IMO it should be `ROOT` so it's consistent whereever you use it and then it can be change via the request if you like to
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoBoundingBoxTests.java</file></files><comments><comment>Geo: Fixes BoundingBox across complete longitudinal range</comment></comments></commit><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java</file><file>src/main/java/org/elasticsearch/common/util/LocaleUtils.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Add "locale" parameter to query_string and simple_query_string</comment></comments></commit></commits></item><item><title>Postings Highlighter does not highlight trailing wildcard matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5127</link><project id="" key="" /><description>In 1.0 Postings highlighter does not highlight trailing wildcard matches. I tried with both simple_query_string and query_string and things like photo\* does not get highlighted
</description><key id="27614372">5127</key><summary>Postings Highlighter does not highlight trailing wildcard matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">roytmana</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-14T18:16:50Z</created><updated>2014-02-21T22:09:25Z</updated><resolved>2014-02-21T20:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-14T18:28:42Z" id="35110751">Hi @roytmana , 
do you mean that highlighting was working fine with the same query in 0.90? Can you post a recreation please?
</comment><comment author="roytmana" created="2014-02-14T18:33:50Z" id="35111199">I already migrated everything including index metadata to 1.0 so I can't
confirm with 100% certainty it was working in 0.90 but if you recall you
and I were working on exactly the same issue a while ago and I believe it
was fixed.

Before I start working on a recreation (need to put it together from
scratch) Do you think that it should NOT work by design?

On Fri, Feb 14, 2014 at 1:29 PM, Luca Cavanna notifications@github.comwrote:

&gt; Hi @roytmana https://github.com/roytmana ,
&gt; do you mean that highlighting was working fine with the same query in
&gt; 0.90? Can you post a recreation please?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5127#issuecomment-35110751
&gt; .
</comment><comment author="s1monw" created="2014-02-14T18:33:52Z" id="35111204">wait that has just been implemented in [lucene](https://issues.apache.org/jira/browse/LUCENE-5415) - I don't think we have support for MTQ in postings highlighter yet? This is coming with Lucene 4.7 
</comment><comment author="javanna" created="2014-02-14T18:42:38Z" id="35111993">@roytmana I'm asking because I do remember I worked on this and we didn't touch anything in 1.0, thus I expect it to work on both 0.90 and 1.0. We also have tests for this which are green all the time.

@s1monw we have our own custom postings highlighter, to which we added support for wildcards a while ago. Once lucene 4.7 is released I'll have a look at this again though ;)
</comment><comment author="roytmana" created="2014-02-14T18:56:57Z" id="35113283">@javanna  let me create a recreation and test with it explicitly specifying highlighter in query. maybe something else has changed. I will post it shortly
</comment><comment author="roytmana" created="2014-02-14T19:31:43Z" id="35116475">It does not work. Here is a recreation (note I could not test actual curl as it does not take json on windows so I used different tools so excuse me if the curl syntax is broken )  

```
curl  -XDELETE http://localhost:9200/test

curl  -XPOST http://localhost:9200/test -d '{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "ht": {
      "dynamic": "strict",
      "properties": {
        "name": {
          "type": "string",
          "index_options": "offsets"
        }
      }
    }
  }
}'

curl  -XPOST http://localhost:9200/test/ht -d '{"name":"photo equipment"}'
curl  -XPOST http://localhost:9200/test/ht -d '{"name":"photography"}'

curl -XPOST "http://localhost:8680/ec-search/test/ht/_search" -d'
{
   "query": {
      "bool": {
         "should": [
            {
               "simple_query_string": {
                  "fields": [
                     "_all"
                  ],
                  "query": "photo"
               }
            }
         ]
      }
   },
   "highlight": {
      "fields": {
         "name": {
            "type": "postings"
         }
      }
   }
}'

curl -XPOST "http://localhost:8680/ec-search/test/ht/_search" -d'
{
   "query": {
      "bool": {
         "should": [
            {
               "simple_query_string": {
                  "fields": [
                     "_all"
                  ],
                  "query": "photo*"
               }
            }
         ]
      }
   },
   "highlight": {
      "fields": {
         "name": {
            "type": "postings"
         }
      }
   }
}'
```

First (no wildcard) query returned highlight

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.625,
      "hits": [
         {
            "_index": "test",
            "_type": "ht",
            "_id": "XU_c0rhUSBiu2KfVPjP-sg",
            "_score": 0.625,
            "_source": {
               "name": "photo equipment"
            },
            "highlight": {
               "name": [
                  "&lt;em&gt;photo&lt;/em&gt; equipment"
               ]
            }
         }
      ]
   }
}
```

second did not: 

```
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "ht",
            "_id": "XU_c0rhUSBiu2KfVPjP-sg",
            "_score": 1,
            "_source": {
               "name": "photo equipment"
            }
         },
         {
            "_index": "test",
            "_type": "ht",
            "_id": "tJOe5F7kQJiSMjOvTNPwpg",
            "_score": 1,
            "_source": {
               "name": "photography"
            }
         }
      ]
   }
}
```
</comment><comment author="roytmana" created="2014-02-14T21:12:34Z" id="35125352">Another observation that is not directly related to the wildcards issue.

When query is done on the _all field while highlighting is done on specific fields contributing to all, it works well when the fields are of string type. When the fields are numeric or date there will be no highlighting. However highlighting is done if searching on those numeric/date fields individually

I guess it can't be helped due to field type loss in _all? But if it did work it would have been really great.

Please let me know if I should create a ticket for it or not
</comment><comment author="javanna" created="2014-02-17T12:26:36Z" id="35252442">Hi @roytmana thanks for the recreation, I'm looking into this.
The problem is the same against both 0.90 and 1.0, wildcards do work but only when they are in the top-level query :) and not within compound queries. The fact that you query a specific type makes it a filtered query, which triggers this issue.
</comment><comment author="javanna" created="2014-02-21T20:59:29Z" id="35772929">This was solved in #5143.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for `lowercase_expanded_terms` flag to simple_query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5126</link><project id="" key="" /><description>Default the flag to true, making `simple_query_string` behave similarly to `query_string`.

Fixes #5008
</description><key id="27612520">5126</key><summary>Add support for `lowercase_expanded_terms` flag to simple_query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-14T17:46:26Z</created><updated>2015-06-07T15:22:53Z</updated><resolved>2014-02-14T18:49:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-14T17:57:23Z" id="35107892">Looks good to me, +1 to merge!
</comment><comment author="s1monw" created="2014-02-14T18:05:55Z" id="35108671">I think it looks great as well but see my comment on the locale... I think we should open a new issue to fix that properly 
</comment><comment author="s1monw" created="2014-02-14T18:42:53Z" id="35112017">cool thanks - +1 to push this
</comment><comment author="dakrone" created="2014-02-14T18:49:45Z" id="35112616">Merged to 1.x and master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Exposed shard id related to a failure in delete by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5125</link><project id="" key="" /><description>Relates to #5095, where we want to expose the potential shard failures obtained from the delete by query api. Although the failure is available, the shard id where it happened is not (always `-1`).

Refactored TransportIndexReplicationOperationAction to be able to expose the shard id related to a shard failure.

The `ShardOperationFailedException` is now created within `TransportIndexReplicationAction` passing in the current shard id as a constructor argument.
Also replaced `AtomicReferenceArray&lt;Object&gt;` with `AtomicReferenceArray&lt;ShardActionResult&gt;`, where `ShardActionResult` wraps the `ShardResponse` or the failure, containing all the needed info.
</description><key id="27609257">5125</key><summary>Exposed shard id related to a failure in delete by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Search</label><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-14T16:57:25Z</created><updated>2015-06-07T23:25:01Z</updated><resolved>2014-02-26T08:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-02-23T01:38:59Z" id="35821086">Looks good, but it would be nice to add asserts to [DeleteByQueryTests#testFailure](https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java#L112) to make sure that shard &gt;= 0 in response.failures, since this is the part that you fixed.
</comment><comment author="javanna" created="2014-02-24T09:19:12Z" id="35868924">Makes sense, applied changes suggested by @imotov.
</comment><comment author="imotov" created="2014-02-26T02:04:17Z" id="36083227">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Data loading from an Analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5124</link><project id="" key="" /><description>I have a dictionary-based analyzer implementation, with the dictionary being loaded from the plugin jar. I want to allow the user to add their words to the dictionary by using a supplementary dictionary. And I want a flexible way of handling this custom list.

Using the standard Settings implementation isn't a valid option since this is can be a list of many words, each word having some additional descriptors, so its not a string suitable for storing in settings.

One possibility is to provide the analyzer with a path of some sort via settings, but to be able to run on the cloud using SaaS providers this is not a good option. Another option is to provide a download link, but again - I don't want my Analyzer downloading the internet on initialization, plus that may be a security breach.

Instead, I'm interested in a way to leverage ES as a document store. Loading a document with this configuration seems optimal considering all options, but I'm not sure if it is supported at all. If it is - would love to hear how; otherwise, would love to see this added.
</description><key id="27598951">5124</key><summary>Data loading from an Analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>discuss</label></labels><created>2014-02-14T14:33:19Z</created><updated>2017-05-15T07:59:17Z</updated><resolved>2014-07-25T09:48:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2014-02-14T14:35:19Z" id="35088196">I should add this custom list is just a long string that assumes parsing on the analyzer's part when loading
</comment><comment author="jprante" created="2014-02-14T15:11:04Z" id="35091581">Idea for implementation
- add a virtual file system implementation to ES that can read from a single ES doc in the cluster by addressing index/type/id. For example with URI scheme "vfs://" or "es://" This implementation should be aware of multi-tenancy, i.e. the index is mappable to a concrete index
- let users create supplementary dictionary files locally (MIME type text/plain UTF-8)
- optionally, for upload, add a REST API for receiving such text files and storing in a doc, e.g. in a  {"mimetype" : "text/plain", "content":{ ... }} JSON wrapper, no indexing. Alternatively, using the standard index API.
- in the analyzer setting, the user selects the path to the dictionary file by accessing ES doc over the virtual file system by something like "path" : "vfs://index/type/id"
</comment><comment author="synhershko" created="2014-06-24T00:25:00Z" id="46918804">@jprante I like this idea, but I wonder if a vfs:// type of thing isn't overboard and over-engineering a solution?

I'd be happy to brainstorm this further and then go into implementing it - I'm sure there can be other use cases as well and will be happy to hear ES's opinion on this
</comment><comment author="clintongormley" created="2014-07-01T12:28:38Z" id="47649548">@synhershko you mean allowing an analyzer to fetch a file from an ES index in the same way that it does from the local filesystem now?

That's an intriguing idea, and fits well with https://github.com/elasticsearch/elasticsearch/issues/5484 which allows scripts/templates to be stored in an index.

One part that is missing is the ability to "refresh" analyzers when the list is updated.  This same problem exists when the list is stored in a file as well.

Another issue with this particular request is that the index would have to be allocated before other indices, otherwise the other index may not be able to instantiate its analyzers at startup.
</comment><comment author="synhershko" created="2014-07-01T14:41:02Z" id="47664321">@clintongormley yes, exactly

I'm a bit hesitant about thinking of this as an index - the old Percolator never really sat well with me and there were better reasons to have it as its own index than this or scripts. The entire allocation, replication etc is a lot of unnecessary overhead that even made me write my own percolator at some point.

As you indicated - the problem exists anyway now when loading from disk so I wouldn't consider this a problem, and analyzers can either poll sources for changes, reload periodically or be explicitly reloaded using a Service and a REST endpoint. Pick 1 or all 3 via config.

If there was a way to load stored data not via the index storage, see more reasoning above, then the allocation issue you mention could also be avoided. If not, then the analyzers could be pinged for lazy loading once the indexes service is up.
</comment><comment author="konradkonrad" created="2014-07-21T16:08:45Z" id="49626612">For similar reasons as @kimchy s first comment in #5484 I am a supporter of this. 

Furthermore I don't like the necessity to distribute a file across all nodes in order to update synonyms:

Imagine you want to create a system where a user can tweak the synonym lists for a webshop. The user may want to do several iterations, re-check indexing- as well as query-results. In order to build such a system, IMO it is favorable if we don't need to overcome the gap between elasticsearch and the file-system. If docs in the `.scripts` index (or `.ressource`) would be adressable from analyzer settings, building such a system seems much more coherent to me, i.e.:
- one could design versioning etc. in elasticsearch, 
- a central 'address' for the synonyms avoids the possibility of 'analyzer split brain' situations, where one node wouldn't receive a file. 

Does that make sense?
</comment><comment author="clintongormley" created="2014-07-25T09:48:47Z" id="50129237">We've discussed this internally, and there are various issues with this approach, including security, dependencies etc, and agreed not to support this.
</comment><comment author="synhershko" created="2014-07-25T11:33:32Z" id="50137824">@clintongormley that's unfortunate. Are there alternatives for solving this challenge? it is a real challenge I'm facing almost daily in various scenarios.

Also, how does that affect #4063  ?
</comment><comment author="clintongormley" created="2014-07-25T11:41:34Z" id="50138437">@synhershko as far as I know #4603 is still planned. As far as provisioning the file to multiple servers, i think the safest way to do is with puppet or chef or similar.
</comment><comment author="synhershko" created="2014-07-25T11:46:49Z" id="50138903">@clintongormley it is more of an issue of hardcoding paths in the analyzer (which may or may not be custom) than provisioning of the files. Requiring the Analyzer to look for files on the FS and taking into account Windows / Linux paths etc (no pun intended) is what this issue was mainly about.
</comment><comment author="clintongormley" created="2014-07-25T12:01:49Z" id="50140097">@synhershko why not do the same thing as the Hunspell filter does?
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html
</comment><comment author="synhershko" created="2014-07-25T12:04:12Z" id="50140262">@clintongormley I'll look into it, thanks. Hopefully ES provides the supporting infrastructure and not much handling needs to be done on the Analyzer level.
</comment><comment author="jprante" created="2014-07-25T12:12:06Z" id="50140885">@synhershko I think we should accept that dynamic resource provisioning for analyzers might not make it into the core. To me, resource loading for analyzers only from file system is not acceptable either. My suggestion is to follow the `.scripts` index idea, by extending it to an analyzer provisioning API for an internal `.analyzer` index, making use of the `ResourceWatcher` API.  The `ResourceWatcher` could be extended into an implementation that checks for`.analyzer` index modifications each minute or so. 
Using the `AnalysisModule` extension points from a plugin, it could be possible to register new dynamically configurable versions of tokenizers/filters/analyzers, turning some of them who read configs from file into ones that are also `ResourceWatcher` aware.
</comment><comment author="clintongormley" created="2014-07-25T12:15:11Z" id="50141141">@jprante then you have dependency issues.  an index that uses an analyzer that depends on a file stored in another index can only be recovered after that index has recovered...
</comment><comment author="synhershko" created="2014-07-25T12:22:59Z" id="50141821">@clintongormley exactly why I said I don't like the store-in-an-index approach, I'm sure there can be better solutions. I'll be happy to discuss if you decide to pursue.
</comment><comment author="jprante" created="2014-07-25T12:26:47Z" id="50142148">@clintongormley at recovery, such analyzers should not use `ResourceWatcher` (I am surprised that analyzers are used in recovery). The analyzer depends of course on static setup at node initialization time. Only updates are dynamically provisioned, after node is up.
</comment><comment author="babadofar" created="2017-05-15T07:59:17Z" id="301403820">Hi, are there any plans to do something on this? I need a better way to handle dynamic updates of synonyms :) </comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to get snapshot status for running snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5123</link><project id="" key="" /><description>Closes #4946
</description><key id="27597236">5123</key><summary>Add ability to get snapshot status for running snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-14T14:05:59Z</created><updated>2015-06-07T15:23:00Z</updated><resolved>2014-03-18T01:08:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-24T11:30:12Z" id="35877691">I will look at this asap - I guess tomorrow or tonight
</comment><comment author="s1monw" created="2014-03-11T09:53:59Z" id="37278680">I left some comments but in general it looks good
</comment><comment author="imotov" created="2014-03-17T01:42:51Z" id="37779011">@s1monw, I've updated the PR based on your comments. Thanks!
</comment><comment author="s1monw" created="2014-03-17T20:12:00Z" id="37864144">@imotov I left  one minor comment I think this LGTM
</comment><comment author="s1monw" created="2014-03-17T20:26:37Z" id="37865696">LGTM +1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow for executing search requests based on pre-defined templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5122</link><project id="" key="" /><description>Pretty much related to #4879 - the goal is to allow for templates to be used not only for individual queries but on the search request level.

For development templates submitted with the search request would look like the following:

``` json
GET _search
{
    "template" : {
        "content" : {
            "query": {
                "match_{{te_1}}": {}
            },
            "fac{{te_2}}": {
            "list-id": {
                "terms": {
                    "field": "list-{{te_3}}",
                    "size": 10
                }
            }
            }, 
            "fields": ["subject", "from"]
        },
        "params" : {
            "te_1" : "all",
            "te_2" : "ets",
            "te_3" : "id"
        }
    }
}
```

... (exact name of the template parameters (here "content" and "params") open for debate) resulting in a query that looks like this:

``` json
GET _search
{
    "query": {
        "match_all": {}
    },
    "facets": {
        "list-id": {
            "terms": {
                "field": "list-id",
                "size": 10
            }
        }
    }, 
    "fields": ["subject", "from"]
}
```

Ultimately it should be possible to permanently register (or load from configuration) these templates. The goal would be able to only reference the template and parameters in the GET URL parameters only.
</description><key id="27596316">5122</key><summary>Allow for executing search requests based on pre-defined templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels /><created>2014-02-14T13:50:41Z</created><updated>2014-04-01T08:51:36Z</updated><resolved>2014-04-01T08:34:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-03-27T09:06:55Z" id="38780984">Big +1 for this. Not sure if it should be in another issue or not, but I'd love to have something like:

Put/Update a search template:

```
PUT _search/template/templatename
{
    "template" : {
        "content" : {
            "query": {
                "match_{{te_1}}": {}
            },
            "fac{{te_2}}": {
            "list-id": {
                "terms": {
                    "field": "list-{{te_3}}",
                    "size": 10
                }
            }
            }, 
            "fields": ["subject", "from"]
        }
    }
}
```

Get a search template:

```
GET _search/template/templatename
```

Remove a search template:

```
DELETE _search/template/templatename
```

Run the template:

```
GET /_search/template
{
    "template": "templatename" ,
    "params": {
        "te_1" : "all",
        "te_2" : "ets",
        "te_3" : "id"
    }
}
```
</comment><comment author="skade" created="2014-04-01T08:29:17Z" id="39181579">I'd love to have what @dadoonet suggests as well. This would make the feature similar to other APIs, e.g. templates.
</comment><comment author="spinscale" created="2014-04-01T08:34:12Z" id="39181943">This is indeed the next step. We should however create a new issue for that as this one here is already in 1.1.0.
</comment><comment author="dadoonet" created="2014-04-01T08:51:36Z" id="39183287">@skade Issue opened in #5637 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update get.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5121</link><project id="" key="" /><description>Minor improvements.

curl -XHEAD doesn't actually print anything so I've changed to use -I which actually prints the headers received.
</description><key id="27581913">5121</key><summary>Update get.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lfrancke</reporter><labels /><created>2014-02-14T08:59:59Z</created><updated>2014-07-16T21:48:32Z</updated><resolved>2014-02-14T12:23:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-14T12:23:09Z" id="35078938">Merged, thanks! 

I added another commit to change the `-I` to `-XHEAD -i`, as it is more explicit. Hope you don't mind! Also realized the `_version` was missing too in the response example, fixed that too. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>nGram tokenizer token_chars appear to be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5120</link><project id="" key="" /><description>For more detail, please see the post on Stackoverflow: http://stackoverflow.com/questions/21764781/elasticsearch-ngram-filters-out-punctuation

Summed up, this is in ElasticSearch: "C.A12345"
I'm using an ngram analyzer with token_chars [].  If I search for "12345" I get a result. If I search for "A12345" or "C.A12345" I get nothing. 

Same result if I use: "token_chars": [ "letter", "digit", "whitespace", "punctuation", "symbol"]

I believe the conjunction of multiple types in a query is causing an issue.
</description><key id="27571624">5120</key><summary>nGram tokenizer token_chars appear to be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bsee08</reporter><labels><label>non-issue</label></labels><created>2014-02-14T03:26:45Z</created><updated>2014-02-19T21:54:40Z</updated><resolved>2014-02-19T21:54:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-14T09:18:27Z" id="35067854">can you maybe provide a full gist that reproduces the issue with curl commands? this would be great. I'd also be curious what version you are using?
</comment><comment author="bsee08" created="2014-02-19T16:47:34Z" id="35519746">Hopefully Sense commands are ok.  I'm on version 1.0.  In the code below I'm first creating the mapping. I copy anything I want analyzed into the "meta_data" field.  When I go to search, a search of "1234" returns a result. A search of "A1234" returns nothing.  

PUT /project/
{
    "settings" : {
        "index" : {
            "analysis" : {
                "analyzer" : {
                    "my_ngram_analyzer" : {
                        "tokenizer" : "my_ngram_tokenizer"
                    }
                },
                "tokenizer" : {
                    "my_ngram_tokenizer" : {
                        "type" : "nGram",
                        "min_gram" : "1",
                        "max_gram" : "10",
                        "token_chars": []
                    }
                }
            }
        }
    },
    "mappings": {
        "project": {
            "_all" : {"enabled" : true},
            "properties" : {
                "FullName":{"type":"string","store":"yes", "copy_to" : "meta_data"},
                "meta_data" : { "type" : "string", "store":"yes", "index_analyzer": "my_ngram_analyzer" }
            }
        }
    }
}
PUT /project/project/1
{
  "FullName": "C.A1234.5678"
}

GET /_search?pretty=true
{ 
    "query": {
        "match": {
            "meta_data": "1234"
        }
    }
}
</comment><comment author="s1monw" created="2014-02-19T18:12:00Z" id="35529332">you have a couple of typos in you re-creation:

```
GET /search?pretty=true
{ 
"query": {
"match": {
"metadata": "1234"
}
```

this should be `meta_data`

```
"FullName":{"type":"string","store":"yes", "copyto" : "meta_data"},
```

this should be `copy_to` no?
</comment><comment author="s1monw" created="2014-02-19T18:16:47Z" id="35529843">I actually wrote a testcase that I will push soon to replicate the issue but it's working fine for me though...
</comment><comment author="bsee08" created="2014-02-19T18:27:02Z" id="35530942">Yes, the _ characters appear to be missing.  I copied it direct from Sense, and my Sense case still has the _.
</comment><comment author="s1monw" created="2014-02-19T21:54:35Z" id="35553951">ok so the problem here is that you are only specifying an `index_analyzer` this means your searches will get an `standard` analyzer and that one will lowercase the `A` and you won't get a hit. If you use `analyzer` instead of `index_analyzer` you will see docs retrieved or you can also just use a `lower` token filter with the ngram analyzer and that will work as well. I updated the test to reflect that. see https://github.com/elasticsearch/elasticsearch/commit/d5f1e6f2363f9c328baaa138b65ca79a617fe7fc
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/index/analysis/NGramTokenizerFactoryTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Added tests for empty token chars on ngram tokenizer</comment></comments></commit></commits></item><item><title>Delay startup of marvel plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5119</link><project id="" key="" /><description>When starting up an ES node with the marvel plugin installed, before the node can connect to or start a cluster it complains every second that it cannot connect to upload stats which generates a lot of noise in the logs;

```
[2014-02-14 10:32:10,991][ERROR][marvel.agent.exporter    ] error connecting to [localhost:9200]
java.net.ConnectException: Connection refused
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
        at sun.net.www.http.HttpClient.&lt;init&gt;(HttpClient.java:211)
        at sun.net.www.http.HttpClient.New(HttpClient.java:308)
        at sun.net.www.http.HttpClient.New(HttpClient.java:326)
        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:317)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:198)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:246)
        at org.elasticsearch.marvel.agent.exporter.ESExporter.exportNodeStats(ESExporter.java:135)
        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:274)
        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:174)
        at java.lang.Thread.run(Thread.java:744)
[2014-02-14 10:32:10,991][ERROR][marvel.agent.exporter    ] could not connect to any configured elasticsearch instances: [localhost:9200]
```

It'd be nicer if marvel could wait until cluster state is established and then start stats collection.
</description><key id="27562651">5119</key><summary>Delay startup of marvel plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">markwalkom</reporter><labels /><created>2014-02-13T23:42:20Z</created><updated>2015-01-05T08:00:57Z</updated><resolved>2015-01-05T08:00:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-03-03T15:22:35Z" id="36519913">Sorry for not getting back to you earlier. Marvel does delay startup for 5 seconds. Although that may not be enough, you also don't want to wait too long. The reason is that when sending data to a secondary cluster, you want events to be visible asap. Think about a node started event.

That said - the error reporting could (and should be) greatly reduced. I've made a note of it and will ping you when it is fixed.
</comment><comment author="cdeck" created="2014-06-04T17:02:04Z" id="45119418">+1
</comment><comment author="clintongormley" created="2014-12-29T11:47:34Z" id="68251262">@bleskes is this still an issue?
</comment><comment author="bleskes" created="2015-01-05T08:00:57Z" id="68677818">should be fixed as of marvel 1.1. With Marvel 1.3, if we a host target is not configured and we know we send locally, we wait for the local http server to start before starting to send data.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add _cat/segments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5118</link><project id="" key="" /><description>Currently displays the same information as the _segments API
</description><key id="27554448">5118</key><summary>Add _cat/segments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-02-13T21:35:05Z</created><updated>2014-08-21T15:07:53Z</updated><resolved>2014-04-11T16:09:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-02-13T21:38:44Z" id="35029162">This pull request should solve  #4711. Let me know if there are any issues you need me to resolve with this change
</comment><comment author="colings86" created="2014-02-15T11:30:44Z" id="35153570">Updated the pull request with the changes suggested
</comment><comment author="colings86" created="2014-02-19T20:58:06Z" id="35547559">Updated with your suggestions
</comment><comment author="jpountz" created="2014-04-11T16:20:02Z" id="40221990">Sorry for having closed this issue, I have no idea why Github closed it when I pushed an unrelated commit. Please feel free to reopen it.
</comment><comment author="javanna" created="2014-04-14T09:06:43Z" id="40346050">I think github did the right thing somehow as this PR got merged a while ago but it was left open ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature request: i.r.a.total_primary_shards_per_node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5117</link><project id="" key="" /><description>Similar to `index.routing.allocation.total_shards_per_node`, but would limit the number of primary shards on a node.

Use case:
When doing bulk indexing with replication set to `async`, having multiple primary shards on one node becomes a bottleneck - the load on the node is higher, it has to do more merges than other nodes, and things are generally slower.
Having primaries on separate nodes (and, preferably, on nodes without replicas) helps with throughput.
</description><key id="27553579">5117</key><summary>Feature request: i.r.a.total_primary_shards_per_node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels /><created>2014-02-13T21:23:33Z</created><updated>2014-02-19T08:16:25Z</updated><resolved>2014-02-19T05:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-14T09:28:49Z" id="35068520">Take a look at [`cluster.routing.allocation.balance.primary`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-update-settings.html#_balanced_shards) it should do pretty much what you want but still balance well. Try to raise it a bit until you get to a state you are happy with. Lemme know if that helps 
</comment><comment author="avleen" created="2014-02-14T18:12:56Z" id="35109300">Thanks Simon! I'll try tuning this up. I wasn't sure what those setting did, docs said they do "something", but their exact behavior was unclear :-)
</comment><comment author="avleen" created="2014-02-19T05:19:43Z" id="35467197">Tuning this up from the default of `0.05` to `1.0` seems to have been sufficient for now.
Thanks Simon!
</comment><comment author="s1monw" created="2014-02-19T08:16:25Z" id="35475437">@avleen I will try to improve the docs here to make this more clear. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>poor performance of parent/child queries in function_score query and rescore </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5116</link><project id="" key="" /><description>looks like function_score and rescore boost queries are not reusing parent/children documents loaded by main query

for example:

``` JSON
{
   "query": {
      "bool": {
         "must": [
            {
               "geo_bounding_box": {
                  "@center": {
                     "top_right": "52.68,-7.2",
                     "bottom_left": "52.62,-7.28"
                  },
                  "_cache": true,
                  "type": "indexed"
               }
            },
            {
               "has_child": {
                  "type": "skill",
                  "query": {
                     "match": {
                        "@title": "plumber"
                     }
                  }
               }
            }
         ]
      }
   },
   "rescore": {
      "window_size": 50,
      "query": {
         "query_weight": 0.5,
         "rescore_query": {
            "has_child": {
               "query": {
                  "filtered": {
                     "query": {
                        "function_score": {
                           "boost_mode": "replace",
                           "score_mode": "first",
                           "script_score": {
                              "script": "...SCRIPT..."
                           }
                        }
                     }
                  }
               },
               "score_type": "max",
               "type": "skill"
            }
         },
         "rescore_query_weight": 0.5
      }
   }
}
```

executes in around 3 sec (for around 30m parent/child documents)

when this query:

``` JSON
{
   "query": {
      "bool": {
         "must": [
            {
               "geo_bounding_box": {
                  "@center": {
                     "top_right": "52.68,-7.2",
                     "bottom_left": "52.62,-7.28"
                  },
                  "_cache": true,
                  "type": "indexed"
               }
            },
            {
               "has_child": {
                  "type": "skill",
                  "query": {
                     "match": {
                        "@title": "plumber"
                     }
                  }
               }
            }
         ]
      }
   },
   "rescore": {
      "window_size": 50,
      "query": {
         "query_weight": 0.5,
         "rescore_query": {
            "has_child": {
               "query": {
                  "filtered": {
                     "filter": {
                        "bool": {
                           "must": [
                              {
                                 "match": {
                                    "@title": "plumber"
                                 }
                              },
                              {
                                 "has_parent": {
                                    "type": "person",
                                    "filter": {
                                       "geo_bounding_box": {
                                          "@center": {
                                             "top_right": "52.68,-7.2",
                                             "bottom_left": "52.62,-7.28"
                                          },
                                          "_cache": true,
                                          "type": "indexed"
                                       }
                                    }
                                 }
                              }
                           ]
                        }
                     },
                     "query": {
                        "function_score": {
                           "boost_mode": "replace",
                           "score_mode": "first",
                           "script_score": {
                              "script": "...SCRIPT..."
                           }
                        }
                     }
                  }
               },
               "score_type": "max",
               "type": "skill"
            }
         },
         "rescore_query_weight": 0.5
      }
   }
}
```

executes in around 100 milliseconds

the only difference is that rescore query in second example contains 'inverted' main query to filter child documents

this same problem occurs when using function_score query

it will be more intuitive if this kind of boost/rescore queries could reuse results from the main query for parent child relationship (maybe controlled by additional field in rescore query)

i know that this kind of behavior is probably intentional (as sometimes we might want to boost by child documents that are not returned by main query), still having to set inverted main query in rescore query is doubling the size of request sent to the server
</description><key id="27545511">5116</key><summary>poor performance of parent/child queries in function_score query and rescore </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2014-02-13T19:34:51Z</created><updated>2014-12-29T11:47:03Z</updated><resolved>2014-12-29T11:47:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T11:47:03Z" id="68251240">Closing in favour of #8134
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add preserve original token option to ASCIIFolding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5115</link><project id="" key="" /><description>Closes #4931
</description><key id="27530790">5115</key><summary>Add preserve original token option to ASCIIFolding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-13T16:22:50Z</created><updated>2015-06-06T18:41:31Z</updated><resolved>2014-02-14T18:46:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-13T19:36:00Z" id="35016547">LGTM I will pull this one in tomorrow
</comment><comment author="s1monw" created="2014-02-14T09:34:35Z" id="35068872">@nik9000 I just saw that the docs change doesn't include a `coming` tag can you add that like this:

```
diff --git a/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc b/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc
index 7de1711..911ba7e 100644
--- a/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc
+++ b/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc
@@ -20,8 +20,8 @@ equivalents, if one exists.  Example:
 }
 --------------------------------------------------

-Accepts `preserve_original` setting which defaults to false but if true
-will keep the oringal token as well as emit the folded token.  For
+coming[1.1.0, Accepts `preserve_original` setting which defaults to false but if true
+will keep the oringal token as well as emit the folded token.]  For
 example:

 [source,js]
```

this allows us to mark changes in the docs with the version they were added
</comment><comment author="nik9000" created="2014-02-14T12:15:07Z" id="35078513">Yeah!  I'll get to it in an a couple hours. 

Sent from my iPhone

&gt; On Feb 14, 2014, at 4:34 AM, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; @nik9000 I just saw that the docs change doesn't include a coming tag can you add that like this:
&gt; 
&gt; diff --git a/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc b/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc
&gt; index 7de1711..911ba7e 100644
&gt; --- a/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc
&gt; +++ b/docs/reference/analysis/tokenfilters/asciifolding-tokenfilter.asciidoc
&gt; @@ -20,8 +20,8 @@ equivalents, if one exists.  Example:
&gt;  }
&gt; 
&gt; ---
&gt; 
&gt; -Accepts `preserve_original` setting which defaults to false but if true
&gt; -will keep the oringal token as well as emit the folded token.  For
&gt; +coming[1.1.0, Accepts `preserve_original` setting which defaults to false but if true
&gt; +will keep the oringal token as well as emit the folded token.]  For
&gt;  example:
&gt; 
&gt;  [source,js]
&gt; this allows us to mark changes in the docs with the version they were added
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="s1monw" created="2014-02-14T14:49:20Z" id="35089565">&gt; &gt; Yeah!  I'll get to it in an a couple hours.
&gt; &gt; no worries - no rush
</comment><comment author="nik9000" created="2014-02-14T18:31:25Z" id="35110984">Done.
</comment><comment author="s1monw" created="2014-02-14T18:46:43Z" id="35112363">pushed thx
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unable to run elasticsearch - Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version 1.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5114</link><project id="" key="" /><description>I got an error using elasticsearch 1.0.0 ( with version 0.90.x without any problem).

```
[root@logger01 /Software/bin]# Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/Elasticsearch
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.bootstrap.Elasticsearch
    at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.
```

OS FreeBSD 10.0 x64

java -version:

```
java -version
openjdk version "1.6.0_32"
OpenJDK Runtime Environment (build 1.6.0_32-b30)
OpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)
```

thanks for any help.
Stefan
</description><key id="27528404">5114</key><summary>Unable to run elasticsearch - Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version 1.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">StefanSa</reporter><labels><label>non-issue</label></labels><created>2014-02-13T15:54:17Z</created><updated>2017-07-25T09:21:05Z</updated><resolved>2014-02-14T13:02:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-13T21:55:08Z" id="35030807">is there a possibility, that you have a 0.90 and 1.0 installation mixed up somehow? The startup script seems to be 1.0, but the resources cannot be found it seems...

How did you install elasticsearch?

I dont have a lot of freebsd experience so I definately need your help here :-)

Thanks!
</comment><comment author="StefanSa" created="2014-02-14T11:03:05Z" id="35074461">Hi Alex,
I found the following interesting things.
This is the original command from FreeBSD elasticsearch startup script.

```
/usr/local/openjdk6/bin/java -Des.pidfile=/var/run/elasticsearch.pid -server -Xms16g -Xmx16g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.config=/usr/local/etc/elasticsearch/elasticsearch.yml -cp /usr/local/lib/elasticsearch/elasticsearch-0.90.11.jar:/usr/local/lib/elasticsearch/*:/usr/local/lib/elasticsearch/sigar/* org.elasticsearch.bootstrap.ElasticSearch
```

I'm testing the following:
Delete all the files in "/usr/local/lib/elasticsearch" and copy the v1.0.0 files there.
It did, by the way with all .090.x versions without problem.

Changed "elasticsearch-0.90.11.jar" to "elasticsearch-1.0.0.jar" and start the command new,
but i get directly this ClassNotFoundException.

However i can v1.0.0 with the original startup-scrip from elasticsearch distribution start.
This ist the start command from this script.

```
/usr/local/openjdk6/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.foreground=yes -Des.path.home=/Software/elasticsearch-1.0.0 -cp :/Software/elasticsearch-1.0.0/lib/elasticsearch-1.0.0.jar:/Software/elasticsearch-1.0.0/lib/*:/Software/elasticsearch-1.0.0/lib/sigar/* org.elasticsearch.bootstrap.Elasticsearch
```

Wondering why wants to start v1.0.0 not with the original startup-script FreeBSD in "/usr/local/lib/elasticsearch"?
</comment><comment author="s1monw" created="2014-02-14T11:10:28Z" id="35074878">We renamed `ElasticSearch.java` to `Elasticsearch.java` in `1.0` that is why it's not finding the class
</comment><comment author="StefanSa" created="2014-02-14T11:34:39Z" id="35076229">Thanks Simon for the fast reply. But where do i change what?
</comment><comment author="StefanSa" created="2014-02-14T11:50:46Z" id="35077187">OK found it.
Thanks for any help here.
Stefan
</comment><comment author="s1monw" created="2014-02-14T13:02:43Z" id="35081265">cool! I will close this! thanks for the feedback
</comment><comment author="blikenoother" created="2014-07-15T13:27:13Z" id="49030817">I am getting same error, installed es version is 1.2.0 and included jar dependency version is 1.2.2
</comment><comment author="clintongormley" created="2014-07-15T13:45:18Z" id="49033133">Have you tried the fix above?
</comment><comment author="blikenoother" created="2014-07-15T14:02:52Z" id="49035598">I tried above solution but it didn't worked. I am using sun jdk with ubuntu 14.04 version, there was no elasticsearch-1.0.0.jar in my installation
</comment><comment author="blikenoother" created="2014-07-15T14:14:08Z" id="49037250">is there a patch or script to fix above problem?
</comment><comment author="blikenoother" created="2014-07-16T06:57:54Z" id="49130172">Solved, thanks. But new error came, Could not initialize class org.elasticsearch.Version :(
</comment><comment author="flavianh" created="2014-07-28T13:02:46Z" id="50334911">@blikenoother if you found a solution, please describe it
</comment><comment author="jiajie999" created="2014-08-20T09:28:06Z" id="52753075"># java -version

java version "1.6.0_28"
OpenJDK Runtime Environment (IcedTea6 1.13.0pre) (rhel-1.66.1.13.0.el6-x86_64)
OpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)

Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.
</comment><comment author="clintongormley" created="2014-08-20T09:31:02Z" id="52753330">@jiajie999 you need java 7
</comment><comment author="jiajie999" created="2014-08-21T05:39:49Z" id="52880478">@clintongormley Thanks! 
</comment><comment author="flavianh" created="2014-08-21T09:37:23Z" id="52898433">+1 had the same issue. Maybe this should be stressed out in the installation page as an explicit dependency (I think it's mentioned somewhere else though).
</comment><comment author="Aardvark71" created="2014-09-18T15:22:24Z" id="56054587">+1 Same issue as @jiajie999  here. Only java 6 (1.6.0_26) installed  and Elasticsearch-1.3.2 installation gives the cryptic message. [Exception in thread "main" java.lang.UnsupportedClassVersionError:  .... Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.]

Is there a legacy package that works with java 6 ?
</comment><comment author="s1monw" created="2014-09-18T15:24:47Z" id="56054969">@Aardvark71 the last version that worked with java6 was 1.1 afaik. Where do you even get these ancient JVMs what keeps you from updateing to a JVM that is actually still supported by the vendors?
</comment><comment author="Aardvark71" created="2014-09-19T11:37:34Z" id="56166400">@s1monw Thanks for the info. And regarding where we get these ancient JVMs: The kind of environment where one is also forced to still use IE8 and so on.. &gt;&gt; corporate enviroment
</comment><comment author="arstedt" created="2014-09-23T08:15:04Z" id="56488092">Did a first time install of elasticsearch (elasticsearch-1.3.2-1.noarch) via yum on a CentOS6.5 server. And ran into "Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch.  Program will exit." yum install java-1.7.0-openjdk.x86_64 fixed that issue.
Please consider adding a dependency (=&gt; java-1.7.0-openjdk) in the rpm.
</comment><comment author="committedoutlook9407135094" created="2014-12-31T09:36:34Z" id="68432211">Guys !! 

We are using pentaho 5.0.1 which internally uses java 1.7 . We are trying to connect to Elastic Search server 1.4 which is a local server we are using for testing. Previously we used to connect to 0.19 using a es_0.19_patch file. But after upgradation we are unable to connect to the es server. Can you please let us know where can we find a patch file to connect to ES 1.4 server. Please help us to achieve this.
Any help is highly appreciated. 
</comment><comment author="dadoonet" created="2014-12-31T10:12:37Z" id="68433674">@committedoutlook9407135094 I answered in #9115. You should open an issue in Pentaho project.
</comment><comment author="d4rkd0s" created="2016-11-18T19:34:46Z" id="261621433">Okay so what is the issue here? 
I have installed on `16.10x64` with `apt-get install elasticsearch` and I have `Java 1.0.0_111`

```
Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/common/jackson/dataformat/yaml/snakeyaml/error/YAMLException
        at org.elasticsearch.common.jackson.dataformat.yaml.YAMLFactory._createParser(YAMLFactory.java:426)
        at org.elasticsearch.common.jackson.dataformat.yaml.YAMLFactory.createParser(YAMLFactory.java:327)
        at org.elasticsearch.common.xcontent.yaml.YamlXContent.createParser(YamlXContent.java:90)
        at org.elasticsearch.common.settings.loader.XContentSettingsLoader.load(XContentSettingsLoader.java:45)
        at org.elasticsearch.common.settings.loader.YamlSettingsLoader.load(YamlSettingsLoader.java:46)
        at org.elasticsearch.common.settings.ImmutableSettings$Builder.loadFromStream(ImmutableSettings.java:982)
        at org.elasticsearch.common.settings.ImmutableSettings$Builder.loadFromUrl(ImmutableSettings.java:969)
        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:110)
        at org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.java:144)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:215)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.common.jackson.dataformat.yaml.snakeyaml.error.YAMLException
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 11 more
```
</comment><comment author="nik9000" created="2016-11-18T19:40:27Z" id="261622891">@d4rkd0s I'm not sure what your issue is though I don't suspect any java with 1.0.0 in the version number is going to help you. Also this issue is long dead. I think you'd be better of asking on discuss.elastic.co.
</comment><comment author="d4rkd0s" created="2016-11-20T19:11:21Z" id="261798076">I found out my issue was with Debian see more info here: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=829078

I resolved it by simply using 16.04 Ubuntu instead of 16.10. Also there was talk of skipping downstream and going right to upstream from Elasticsearch and downloading from there to prevent this issue/bug.

All I know is its YAML and Debian, something with that.
</comment><comment author="clintongormley" created="2016-11-21T10:41:39Z" id="261902862">Turns out this is a problem with the Elasticsearch package provided by Debian, not the official Elasticsearch debian package provided by us.</comment><comment author="Pytlicek" created="2017-07-25T09:21:04Z" id="317680385">After upgrade to 5.5.0 i have this problem.
What helped me : Uninstall openjdk9 install openjdk8
Symlink config : `ln -s /etc/elasticsearch /usr/share/elasticsearch/config`
Correcting rights : 
```
chmod g+rwx /usr/share/elasticsearch/config/elasticsearch.yml /usr/share/elasticsearch/config/logging.yml
chmod g+rwx /usr/share/elasticsearch/config/elasticsearch.yml /usr/share/elasticsearch/config/elasticsearch.yml
```</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix typo in similarity docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5113</link><project id="" key="" /><description>DRF similarity -&gt; DFR similarity
</description><key id="27527671">5113</key><summary>Fix typo in similarity docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">extesy</reporter><labels /><created>2014-02-13T15:45:37Z</created><updated>2014-07-16T21:48:34Z</updated><resolved>2014-02-13T16:29:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T16:31:18Z" id="34996359">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changed Marvel names to IAFD actresses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5112</link><project id="" key="" /><description>Marvel names are boring
</description><key id="27509306">5112</key><summary>Changed Marvel names to IAFD actresses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">glandais</reporter><labels /><created>2014-02-13T10:52:16Z</created><updated>2014-07-01T10:27:22Z</updated><resolved>2014-03-27T23:52:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Docs for 1.0 search breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5111</link><project id="" key="" /><description>[Breaking changes](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/_search_requests.html) say that top-level query parameter is _required_ and links to: 
- [Count api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-count.html) which says: `NOTE: in contrast to Search the query being sent in the body must not be nested in a query key.`
- [Validate api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-validate.html) which shows example _without_ top-level query parameter.

Is that should be fixed or I'm wrong?
</description><key id="27498919">5111</key><summary>Docs for 1.0 search breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>docs</label></labels><created>2014-02-13T07:20:24Z</created><updated>2014-02-13T12:37:52Z</updated><resolved>2014-02-13T10:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T10:48:14Z" id="34967275">Thanks for pointing this out @bobrik, I just pushed a fix for it.
</comment><comment author="bobrik" created="2014-02-13T11:46:23Z" id="34971053">@javanna count api example is still wrong.
</comment><comment author="javanna" created="2014-02-13T12:37:52Z" id="34974125">@bobrik good catch, fixed the count example too. Thanks again
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] fixed count and validate query docs, they now require a top-level query object, same as other apis</comment></comments></commit></commits></item><item><title>Allow specifying nested fields in simple_query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5110</link><project id="" key="" /><description>Fixes #5091
</description><key id="27494373">5110</key><summary>Allow specifying nested fields in simple_query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>bug</label><label>v0.90.12</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-13T04:50:19Z</created><updated>2015-06-07T23:25:12Z</updated><resolved>2014-02-13T16:50:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-13T07:09:25Z" id="34953556">LGTM make sure you push this to all the branches since it's a bugfix
</comment><comment author="dakrone" created="2014-02-13T16:50:52Z" id="34998745">Merged to 0.90, 1.0, 1.x, and master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] should use setPostFilter instead of setFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5109</link><project id="" key="" /><description /><key id="27492224">5109</key><summary>[DOCS] should use setPostFilter instead of setFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-02-13T03:28:43Z</created><updated>2014-06-28T21:36:02Z</updated><resolved>2014-02-13T09:29:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T09:30:27Z" id="34961398">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix highlighting in percolate existing doc api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5108</link><project id="" key="" /><description>Currently no highlight snippets are generated when percolating en existing document, which is a bug. This PR makes highlighting also work in percolating an existing doc, just like the regular percolate api.
</description><key id="27484546">5108</key><summary>Fix highlighting in percolate existing doc api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-13T00:37:55Z</created><updated>2015-06-07T23:25:43Z</updated><resolved>2014-02-14T16:56:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-13T07:11:17Z" id="34953645">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Field containing all tokens generated by analysis of a document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5107</link><project id="" key="" /><description>It would be nice to have a field where, all the tokens that analysis of a document generates get stored together (possibly with an opt-out like include_in_all).  This way you could set it as the default field and provide real google like searching without having to understand the underlying document's structure, or Lucene query syntax.
</description><key id="27472748">5107</key><summary>Field containing all tokens generated by analysis of a document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kclaggett</reporter><labels /><created>2014-02-12T21:39:50Z</created><updated>2014-12-29T11:43:23Z</updated><resolved>2014-12-29T11:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-29T11:43:23Z" id="68251033">Hi @kclaggett 

Sorry it has taken a while to get to this one.  Unfortunately, search is not as simple as this.  Putting all tokens into a single field will give very poor results.  Cross-field queries are a better choice here.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation error:  _cat?help</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5106</link><project id="" key="" /><description>The documentation for the cat API in MASTER and 1.0.0 says you can GET `_cat?help` to retrieve the full list of commands.  This just returns the cute little cat ASCII art.  Digging into the code comments, it looks like the actual command is `_cat?h`, which works nicely.

&gt; All the cat commands accept a query string parameter help to see all the headers and info they provide, and the /_cat?help command lists all the available commands.

```
$ http localhost:9200/_cat?help
HTTP/1.1 200 OK
Content-Length: 6
Content-Type: text/plain; charset=UTF-8

=^.^=
```

```
$ http localhost:9200/_cat?h
HTTP/1.1 200 OK
Content-Length: 280
Content-Type: text/plain; charset=UTF-8

=^.^= try:
/_cat/allocation
/_cat/shards
/_cat/shards/{index}
/_cat/master
/_cat/nodes
/_cat/indices
/_cat/indices/{index}
/_cat/count
/_cat/count/{index}
/_cat/recovery
/_cat/recovery/{index}
/_cat/health
/_cat/pending_tasks
/_cat/aliases
/_cat/aliases/{alias}
/_cat/thread_pool
```
</description><key id="27469120">5106</key><summary>Documentation error:  _cat?help</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">insyte</reporter><labels /><created>2014-02-12T20:51:37Z</created><updated>2014-03-07T20:39:24Z</updated><resolved>2014-03-07T20:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-02-12T22:01:37Z" id="34923405">I'd say this is a bug in `/_cat` actually - all the other endpoints use `?help` instead.  cc @drewr
</comment><comment author="drewr" created="2014-02-12T22:07:54Z" id="34924063">Yep, that's an oversight in `RestCatAction`.  What do you think about just providing the list of commands without any `?help`?
</comment><comment author="insyte" created="2014-02-12T23:05:50Z" id="34929620">On 12 Feb 2014, at 16:08, Drew Raines wrote:

&gt; Yep, that's an oversight in `RestCatAction`.  What do you think about 
&gt; just providing the list of commands without any `?help`?

Personally I think that would make a lot of sense and would likely be 
more discoverable.

```
Ben Beuchler                                       Clockwork Active 
Media
Director of Systems Administration                           
612.746.1858
```
</comment><comment author="s1monw" created="2014-02-12T23:21:33Z" id="34930915">+1 to return the help without specifying `?help` in `/_cat` - I think both should work for consistency!
</comment><comment author="kimchy" created="2014-02-18T20:01:30Z" id="35426707">Missed this issue and actually pushed a fix for this.

UPDATE: pushed suport for `help` parameter in `/_cat` endpoint.
</comment><comment author="bobrik" created="2014-02-18T20:02:09Z" id="35426760">+1 for listing endpoints without `?help`.
</comment><comment author="bobrik" created="2014-02-18T20:10:08Z" id="35427605">Found weird bug though, providing `?h` for any command in 1.0.0 provides as many empty lines as regular output should be. `?help` works fine.

For 3-node cluster:

```
# curl http://web245:9200/_cat/nodes?h


```

@kimchy is that fixed too?
</comment><comment author="drewr" created="2014-02-18T20:14:43Z" id="35428108">It's not a bug, though it's somewhat confusing.  The `h` flag in an actual command accepts a list of headers to display.  If that's supplied, but empty, maybe we should warn.  The only problem with that is if you intentionally don't want _any_ columns displayed, but I can't think of a good reason off hand to need that even when feeding the output to a program.
</comment><comment author="bobrik" created="2014-02-18T20:22:58Z" id="35429008">This could probably return 400. Too often `h` considered as a short version of `help` and this is quite confusing to see empty lines instead of help or warning.
</comment><comment author="bobrik" created="2014-02-19T12:30:22Z" id="35493415">@drewr misspelled headers are skipped now. Having some feedback instead of missing columns would be cool.
</comment><comment author="drewr" created="2014-03-07T20:39:24Z" id="37065248">Just addressing the `/_cat` issue here. Let's move the header discussion(s) to different ones if you don't mind @bobrik.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestCatAction.java</file></files><comments><comment>Display all available endpoints by default at /_cat</comment></comments></commit></commits></item><item><title>Drupal integration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5105</link><project id="" key="" /><description>Right now, almost all users of Drupal (one of the biggest Content Management Systems) who need real search capabilities use Solr. Solr is the only real implementation of Drupal's Search API (https://drupal.org/project/search_api) that has any common usage. The possibility to integrate elastic search as a Drupal module would be great one, and increasing the visibility among the Drupal community would be great for elastic search.

You can see at https://drupal.org/project/apachesolr that over 14,000 sites report using the Solr integration module. While only 13 sites report using an elasticsearch module (https://drupal.org/project/elasticsearch) and 26 use https://drupal.org/project/search_api_elasticsearch.

With the upcoming release of Drupal 8 later this year, it's a great opportunity for Elastic Search - Drupal integration. Because transitions between major Drupal versions are more like migrations than upgrades, it gives Drupal users an opportunity to evaluate their choices of technology between major releases. And if there's an elastic search module available around the time of Drupal 8's release, it will give Drupal users a (superior and) viable alternative to Solr that could really take off in the Drupal community for their search needs.

This issue is to request an 'official' elastic search module for Drupal or to see if there might be any interest in contribution to the above elastic search modules for Drupal 8. With only a few eyes on those modules, the Drupal 8 versions probably won't see much adoption. But if there's more people trying it out and using it and contributing to it, people will feel more comfortable adopting it.
</description><key id="27466117">5105</key><summary>Drupal integration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rightaway</reporter><labels /><created>2014-02-12T20:14:35Z</created><updated>2014-07-22T05:52:26Z</updated><resolved>2014-07-22T05:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="VeggieMeat" created="2014-07-22T02:44:01Z" id="49692000">Currently, ~350 sites reporting usage statistics report using Search API Elasticsearch. Additionally, I personally know of several hundred sites not reporting statistics that also use the module. We're also committed to Drupal 8 support the day that Drupal 8 is released.

I'm the maintainer of the Search API Elasticsearch module.
</comment><comment author="clintongormley" created="2014-07-22T05:52:26Z" id="49700907">Given that none of the ES developers are Drupal devs, i think this is best left in the safe hands of the community :)

Good luck, and thanks for building the integration @VeggieMeat 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DocValues and ConcurrentModificationException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5104</link><project id="" key="" /><description>Possibly a bug in Lucene rather than Elasticsearch:

```
[2014-02-12 19:46:55,768][DEBUG][action.admin.indices.stats] [hostname] [logstash-2014.02.12][0], node[idCsLCeJQOuc_GEON7LCCQ], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@10e4c9b1]
org.elasticsearch.transport.RemoteTransportException: [hostname][inet[/ip.add.re.ss:9300]][indices/stats/s]
Caused by: java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
        at java.util.HashMap$ValueIterator.next(HashMap.java:954)
        at org.apache.lucene.codecs.lucene45.Lucene45DocValuesProducer.ramBytesUsed(Lucene45DocValuesProducer.java:291)
        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsReader.ramBytesUsed(PerFieldDocValuesFormat.java:308)
        at org.apache.lucene.index.SegmentDocValues.ramBytesUsed(SegmentDocValues.java:103)
        at org.apache.lucene.index.SegmentReader.ramBytesUsed(SegmentReader.java:555)
        at org.elasticsearch.index.engine.internal.InternalEngine.getReaderRamBytesUsed(InternalEngine.java:1123)
        at org.elasticsearch.index.engine.internal.InternalEngine.segmentsStats(InternalEngine.java:1135)
        at org.elasticsearch.index.shard.service.InternalIndexShard.segmentStats(InternalIndexShard.java:532)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:161)
        at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:197)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:53)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:413)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:399)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
```

Elasticsearch 1.0.0 (GA), when trying to bulk index.
</description><key id="27465709">5104</key><summary>DocValues and ConcurrentModificationException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">avleen</reporter><labels><label>bug</label></labels><created>2014-02-12T20:09:11Z</created><updated>2014-02-26T21:21:50Z</updated><resolved>2014-02-26T21:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-13T07:12:45Z" id="34953715">Yes this is a lucene issue. It should go away once everything is loaded so it's not a critical bug though although irritating. I will open a Lucene issues and fix it in Lucene
</comment><comment author="s1monw" created="2014-02-13T07:16:14Z" id="34953856">here is he corresponding lucene issue: https://issues.apache.org/jira/browse/LUCENE-5443
</comment><comment author="s1monw" created="2014-02-13T19:43:24Z" id="35017376">this one if fixed in lucene so it will be fixed with the `4.7` upgrade
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/analysis/miscellaneous/XASCIIFoldingFilter.java</file><file>src/main/java/org/apache/lucene/queryparser/XSimpleQueryParser.java</file><file>src/main/java/org/apache/lucene/search/XReferenceManager.java</file><file>src/main/java/org/apache/lucene/search/XSearcherManager.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/env/NodeEnvironment.java</file><file>src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactory.java</file><file>src/main/java/org/elasticsearch/index/engine/SegmentsStats.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefOrdValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleScriptDataComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/GeoDistanceComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/LongValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/NumberComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/StringScriptDataComparator.java</file><file>src/main/java/org/elasticsearch/index/gateway/fs/FsIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NestedFieldComparatorSource.java</file><file>src/test/java/org/elasticsearch/index/analysis/ShingleTokenFilterFactoryTests.java</file><file>src/test/java/org/elasticsearch/indices/warmer/SimpleIndicesWarmerTests.java</file><file>src/test/java/org/elasticsearch/test/engine/MockInternalEngine.java</file></files><comments><comment>Upgrade to Lucene 4.7</comment></comments></commit></commits></item><item><title>Closes #5102 - Show stacktrace of startup exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5103</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/issues/5102

This makes it a lot easier to debug a problem
like

{2.0.0-SNAPSHOT}: Startup Failed ...
- NumberFormatException[For input string: ""]

because now you see from where the error comes
from (which might be from a plugin!).
</description><key id="27465397">5103</key><summary>Closes #5102 - Show stacktrace of startup exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">timorantalaiho</reporter><labels /><created>2014-02-12T20:05:08Z</created><updated>2014-06-12T10:02:22Z</updated><resolved>2014-04-04T14:38:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-14T17:05:14Z" id="35102942">Thinking more about this, the point of the `buildErrorMessage` method was exactly to hide stacktraces and print a nicer, yet meaningful, error message and avoid printing out big stacktraces which might scare the hell out of users, especially if not familiar with java... :)

I do see your point, but if the error comes from a plugin, then I'd expect the plugin to log the stacktrace if needed. Thoughts?
</comment><comment author="timorantalaiho" created="2014-02-17T07:20:46Z" id="35233546">I must say I disagree on all counts :) Because I cannot imagine non-technical people trying to solve errors that can be seen on ES startup output. If you look at a non-trivial server program starting, you should be prepared to see technical stuff. Logging is not a foolproof option, because you cannot always count on logging to work on startup (and as you can see in [the diff|https://github.com/elasticsearch/elasticsearch/pull/5103/files], even the original code does not rely on logger when running with es.foreground=true). 

And finally, there is the same problem with the own configuration of ES, not just plugins. Try configuring a space (" ") for both transport http and tcp ports, and all you get is

```
[2014-02-17 09:15:55,407][INFO ][node                     ] [L396] initialized
[2014-02-17 09:15:55,407][INFO ][node                     ] [L396] starting ...
{0.90.11}: Startup Failed ...
- NumberFormatException[For input string: ""]
```

Years and years of programmer time have been lost by swallowing stack traces in JVM programs. I contributed a couple of hours to that pile last week ;)
</comment><comment author="javanna" created="2014-02-17T08:28:40Z" id="35236891">I see your point @timorantalaiho and do appreciate your feedback!
Just to be clear, I'm not against your PR, just a bit on the fence as I see what the intention of the orginal code was.
Anybody else have an opinion about this?
</comment><comment author="imotov" created="2014-02-19T02:15:07Z" id="35459138">I agree that sometimes stack trace is indeed helpful. Maybe we can add a setting and depending on this settings the buildErrorMessage() method will produce either just an error message or an error message with a stack trace. 
</comment><comment author="timorantalaiho" created="2014-02-19T23:44:25Z" id="35565264">Thanks for comments! @imotov , I implemented the toggling you suggest (with system property) in #5187 . A new pull request, because I would prefer this current one which I think is simpler. But either way is fine, and sure this can be refined still.
</comment><comment author="javanna" created="2014-04-04T14:38:54Z" id="39571662">Closing in favor of #5187
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Display full stack trace of exception that occurs on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5102</link><project id="" key="" /><description>If an exception is raised by code triggered from Bootstrap class, only its message (and the message of the possible cause stack) is displayed, not the full stack trace.

This means that if for example some plugin reads a System property that it expects to be numeric but which is accidentally set as empty, you only see something like this

```
{2.0.0-SNAPSHOT}: Startup Failed ...
- NumberFormatException[For input string: ""]
```

I propose the whole stack trace would be displayed in these cases, either on System.err when no logging is configured or on ERROR level via log4j. This would make these kinds of configuration errors loads easier to debug, with the small price of uglier error messages.

Pull request is coming up.
</description><key id="27465200">5102</key><summary>Display full stack trace of exception that occurs on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">timorantalaiho</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-12T20:04:09Z</created><updated>2014-04-04T15:17:44Z</updated><resolved>2014-04-04T15:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-21T20:46:52Z" id="35771776">Oh oh, closed the wrong issue, sorry :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file></files><comments><comment>Show stacktrace of startup exception</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Fixed multi term queries support in postings highlighter for non top-level queries</comment></comments></commit></commits></item><item><title>fixed markup and typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5101</link><project id="" key="" /><description /><key id="27460467">5101</key><summary>fixed markup and typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ghost</reporter><labels /><created>2014-02-12T19:16:26Z</created><updated>2014-07-16T21:48:37Z</updated><resolved>2014-02-13T09:37:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T09:37:13Z" id="34961853">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>multi-field terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5100</link><project id="" key="" /><description>It would be nice if the aggregation could be done on multiple fields to get a list of unique keys. The result should include the fields per key (where it found the term):
   "example" : {
      "buckets" : [ {
         "key" : "java",
         "doc_count" : 5
         "fields": ["island", "programming language"]
       }
       ...
      ]
   }
</description><key id="27458352">5100</key><summary>multi-field terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">atoomkern</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2014-02-12T18:53:39Z</created><updated>2017-06-12T13:16:28Z</updated><resolved>2014-09-07T09:10:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abronner" created="2014-04-07T11:20:27Z" id="39718839">+1
</comment><comment author="ltamrazov" created="2014-04-08T14:21:37Z" id="39853471">+1

I agree. Multi-field support would be nice for other aggregations as well, especially for statistical ones such as avg. An example would be to calculate an average across multiple fields. Currently we have to compute the sum and count for each field and do the calculation ourselves. 
</comment><comment author="JnBrymn" created="2014-04-30T12:12:15Z" id="41789087">+1 already needed this
</comment><comment author="fernandolins" created="2014-04-30T21:14:41Z" id="41851580">+1
</comment><comment author="cwarny" created="2014-05-02T17:11:12Z" id="42054992">+1
</comment><comment author="clirix" created="2014-05-23T07:52:12Z" id="43980630">+1
</comment><comment author="ariasdelrio" created="2014-05-26T10:26:11Z" id="44176592">+1 already need this
</comment><comment author="shane-axiom" created="2014-06-03T01:01:42Z" id="44909355">+1
</comment><comment author="nai0om" created="2014-06-19T06:44:19Z" id="46528473">+1
</comment><comment author="jreddin" created="2014-06-26T13:57:11Z" id="47228438">+1
</comment><comment author="ddebernardy" created="2014-06-30T19:32:50Z" id="47575866">+1
</comment><comment author="ibaiul" created="2014-07-03T12:32:29Z" id="47923503">+1 
I already needed this. My dirty solution was to create a new field in the document with the combination of both values and use the terms aggregation against the new combined field, e.g.
Document: {"island":"fiji", "programming_language": "php"}
New Document: {"island":"fiji", "programming_language": "php", "combined_field": "fiji-php"}
</comment><comment author="emalock3" created="2014-07-04T01:27:21Z" id="48001106">+1
</comment><comment author="halfblood369" created="2014-07-08T09:11:46Z" id="48288755">+1
</comment><comment author="mmounirou" created="2014-07-09T16:35:11Z" id="48499663">+1
</comment><comment author="reidjax" created="2014-07-21T16:46:37Z" id="49631982">+1
</comment><comment author="njjkgeerts" created="2014-08-06T09:19:36Z" id="51311458">+1
</comment><comment author="dpogretskiy" created="2014-08-06T09:46:37Z" id="51313754">+1
</comment><comment author="mrdanadams" created="2014-08-11T21:14:33Z" id="51841812">+1
</comment><comment author="jpountz" created="2014-08-12T13:03:17Z" id="51910335">This is something that can already be done using scripts. For example, if you have two fields `f` and `g`, you can run a terms aggregation on the union of the values of these fields by running the following aggregation (it works with both groovy and mvel):

``` json
GET test/_search
{
  "aggs": {
    "t": {
      "terms": {
        "script": "doc['f'].values + doc['g'].values"
      }
    }
  }
}
```

It might not be very performant, so if you plan on running a terms aggregation on several fields on a regular basis, you might want to use the `copy_to` directive in your mappings in order to copy field values to a dedicated field at indexing time and use this field to run the aggregations:

``` json
PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "f": {
          "type": "string",
          "index": "not_analyzed",
          "copy_to": "f_and_g"
        },
        "g": {
          "type": "string",
          "index": "not_analyzed",
          "copy_to": "f_and_g"
        },
        "f_and_g": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    }
  }
}

[Add data]

GET test/_search
{
  "aggs": {
    "t": {
      "terms": {
        "field": "f_and_g"
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2014-08-12T13:46:44Z" id="51915490">The reason why we're not planning on supporting this directly is that it would be much slower and heavier than a normal `terms` aggregation.  The `terms` agg uses global ordinals (rather than concrete values) for counting, but the global ordinals for two different fields are completely separate, so we would have to look up each concrete value independently, which would be a huge performance cost.

With the solutions that @jpountz has suggested, the performance cost is obvious to the user: either you pay the price at aggregation time (with a script) or at index time (with the copy_to) field.  We'd rather make this cost obvious to the user, instead of providing functionality which performs poorly.
</comment><comment author="shane-axiom" created="2014-08-21T17:32:46Z" id="52954245">That makes sense. Perhaps a section saying as much could be added to the aggregations documentation, since this was a popular request?
</comment><comment author="clintongormley" created="2014-08-22T14:52:12Z" id="53069982">@shane-axiom good suggestion. Would you be interested in sending a docs PR?
</comment><comment author="shane-axiom" created="2014-09-07T20:17:27Z" id="54759119">Sorry @clintongormley, this was on my TODO list but haven't had any time. Thanks very much for adding this.
</comment><comment author="jphilippe92" created="2014-10-15T12:24:36Z" id="59197120">+1
</comment><comment author="olegmorajko" created="2014-11-06T10:08:37Z" id="61955513">+1 
</comment><comment author="xiaohelong" created="2014-11-26T04:06:31Z" id="64512358">here I got what I want , if you can ,please write a book about how to query search, give more examples,now ,there is so little sample query on web .
</comment><comment author="clintongormley" created="2014-11-26T08:31:06Z" id="64529782">@xiaohelong2005 

&gt; if you can ,please write a book about how to query search, give more examples,now ,there is so little sample query on web .

We have: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/index.html
</comment><comment author="xiaohelong" created="2014-11-26T15:50:30Z" id="64665475">@clintongormley 
hello, as a developer, the document you give help me a lot , but in my experience, the query examples are not enough to start, ES is a good software,its' aim is to provide search , in my project ,search or query 's syntax are easy to have problem,when get the example,changed based on the example,it maybe more easier to a new starter, encourage the user to have it on.
above all, it's only my personl suggestions and experience. I have  projects based on ES
</comment><comment author="nostalgiaz" created="2015-03-10T16:06:22Z" id="78086160">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Docs: Added explanation of how to do multi-field terms agg</comment></comments></commit></commits></item><item><title>return odd result searching number range query gte and lte </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5099</link><project id="" key="" /><description>POST /10178/_search
      {"from":0,"size":50,"filter":{"range":{"twitterFollowers":{"gt":500}}}}

And in result i Got 
An user json which has 

{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 8,
      "max_score": 1,
      "hits": [
         {
            "_index": "10178",
            "_type": "user",
            "_id": "105381",
            "_score": 1,
            "_source": {
               "userId": "105381",
               "twitterFollowers": 73,

```
        }
     },
```
</description><key id="27447505">5099</key><summary>return odd result searching number range query gte and lte </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amee9451</reporter><labels /><created>2014-02-12T16:39:19Z</created><updated>2014-12-29T11:41:08Z</updated><resolved>2014-12-29T11:41:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-12T17:22:02Z" id="34892439">can you provide a gist to reproduce this issue and also tell us which version you are using?
</comment><comment author="clintongormley" created="2014-12-29T11:41:08Z" id="68250933">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduced a new IMMEDIATE priority - higher than URGENT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5098</link><project id="" key="" /><description>- applied to cluster update settings, reroute, node join/leave events, node failure

Closes #5062
</description><key id="27447112">5098</key><summary>Introduced a new IMMEDIATE priority - higher than URGENT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-12T16:34:25Z</created><updated>2015-06-07T15:30:47Z</updated><resolved>2014-03-06T16:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-06T09:06:04Z" id="36836693">LGTM!, I like IMMEDIATE!, maybe we can change the commit title to reflect it when pushed.
</comment><comment author="uboness" created="2014-03-06T09:56:25Z" id="36840211">@kimchy yeah.. the title above is already outdated... I changed it already, but the PR just doesn't show the updated one
</comment><comment author="uboness" created="2014-03-06T16:27:05Z" id="36905801">closed by https://github.com/elasticsearch/elasticsearch/commit/67cdda29b7bb10d7ba25b02eccd0518d89e0f5f8
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix `highlight_query` in percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5097</link><project id="" key="" /><description>When highlighting with the percolator a `highlight_query` caused UnsupportedOperationException
</description><key id="27431457">5097</key><summary>Fix `highlight_query` in percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Percolator</label><label>bug</label></labels><created>2014-02-12T13:01:20Z</created><updated>2015-06-07T23:26:26Z</updated><resolved>2014-02-12T13:29:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-12T13:02:57Z" id="34866402">LGTM
</comment><comment author="brwe" created="2014-02-12T13:29:32Z" id="34868296">closing, is a duplicate of https://github.com/elasticsearch/elasticsearch/pull/5090
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java heap space error when using large value for size parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5096</link><project id="" key="" /><description>In order to make sure that fetch all results, I'm using an extremely large value for the 'size' parameter. When I put this value to 200.000.000 (I know, it might be a bit of an exaggeration) I'm getting a heap space error:

```
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [query], [reduce] 
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:246) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:219) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:216) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:203) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216) ~[elasticsearch-0.90.3.jar:na]
Caused by: java.lang.OutOfMemoryError: Java heap space
    at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:64) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]
    at org.apache.lucene.util.PriorityQueue.&lt;init&gt;(PriorityQueue.java:37) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]
    at org.elasticsearch.search.controller.ScoreDocQueue.&lt;init&gt;(ScoreDocQueue.java:31) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:256) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryThenFetchAction.java:85) ~[elasticsearch-0.90.3.jar:na]
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:409) ~[elasticsearch-0.90.3.jar:na]
```

If I put the value to 100.000.000 it just works fine.
</description><key id="27418613">5096</key><summary>Java heap space error when using large value for size parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeroenr</reporter><labels /><created>2014-02-12T09:02:23Z</created><updated>2014-02-12T10:56:59Z</updated><resolved>2014-02-12T09:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-12T09:33:37Z" id="34852228">Hi @jeroenr , what you describe is pretty much expected at this time as your `size` is crazy high, although it depends on how much memory you have available. We could try and prevent this from happening, but if you need to fetch all results the way to go is the [scan search type](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-search-type.html#scan).
</comment><comment author="jeroenr" created="2014-02-12T09:46:29Z" id="34853127">Hi @javanna thanks for quick your response. I just found that even with 200.000.000 it's already throwing the same exception. I'm just used doing it this way with Solr. I just assumed elasticsearch would be a bit more lazy with memory allocation when passing the size parameter.

I will definitely look into the scan search type, although it would mean extra logic in the API that I'm building (other type of query based on pagination parameters).
</comment><comment author="javanna" created="2014-02-12T09:52:05Z" id="34853536">Just remember that the number of docs loaded in the lucene priority queue is `from + size`, thus the bigger the `from` is the higher the risk is that you run into troubles. But this is deep pagination, it's totally different from fetching all results, the usecase you described in the first place.

When you know you need to fetch all results, just switch to the scan search, which is meant exactly for that purpose.
</comment><comment author="s1monw" created="2014-02-12T10:56:59Z" id="34858174">@jeroenr solr does exactly the same thing we do here and solr will also go OOM if you do that at some point.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add failures reason to delete by query response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5095</link><project id="" key="" /><description>closes #5093

It will return an array of failures. e.g.

``` json
 "tweet-36": {
    "_shards": {
      "total": 5,
      "successful": 4,
      "failed": 1,
      "failures": [
        {
          "index": "tweet-36",
          "reason": "EsRejectedExecutionException[rejected execution (queue capacity 200) on org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1@176685ee]"
        }
      ]
    }
  }
```
</description><key id="27410290">5095</key><summary>Add failures reason to delete by query response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-12T05:04:42Z</created><updated>2015-06-07T15:26:04Z</updated><resolved>2014-02-26T12:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T14:28:40Z" id="34982584">I left a small comment, other than that this looks good, thanks!
</comment><comment author="kzwang" created="2014-02-13T22:03:54Z" id="35031760">Hi @javanna I've changed that to use `XContentBuilderString`
</comment><comment author="javanna" created="2014-02-14T13:08:29Z" id="35081600">Thanks @kzwang ! Just realized that the shard id is never set though (always `-1`). I'll try to address that before pushing your contribution.
</comment><comment author="javanna" created="2014-02-26T12:30:01Z" id="36119962">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Recovery API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5094</link><project id="" key="" /><description>Unifies all recovery code to use common state tracking classes and
methods. Reports on all cluster-wide shard recoveries from snapshots,
gateways and peers.

Refactors the cat API to use this new internal recovery API.

See #4637
</description><key id="27409994">5094</key><summary>Recovery API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-02-12T04:54:50Z</created><updated>2014-10-21T23:41:03Z</updated><resolved>2014-03-07T00:13:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete by query doesn't return failure reasons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5093</link><project id="" key="" /><description>If we run delete by query and there are some shards failed, we only got failed shards count but doesn't contains failure reasons

e.g. #5083 
</description><key id="27409985">5093</key><summary>Delete by query doesn't return failure reasons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-12T04:54:21Z</created><updated>2017-06-28T23:06:38Z</updated><resolved>2014-02-26T12:15:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ssanghavi" created="2014-02-19T08:33:37Z" id="35476474">Is there an API to check if there was a failure, something similar to BulkResponse.hasFailures?
This would help at least in logging when there is a failure ...
</comment><comment author="javanna" created="2014-02-20T09:38:10Z" id="35602826">Hi @ssanghavi , via java API you can check the failures by iterating through the `IndexDeleteByQueryResponse` that you can retrieve through the`DeleteByQueryResponse#getIndices` method. What is missing is returning them in the rest response.
</comment><comment author="kujenga" created="2017-06-26T23:35:33Z" id="311210894">Any updates on getting this functionality through the REST response? I'm currently experiencing the same behavior.</comment><comment author="javanna" created="2017-06-27T08:43:59Z" id="311293595">hi @kujenga as far as I can see the delete by query api now returns a failures `array`. My last comment is not valid anymore, in fact delete by query was completely rewritten since then.</comment><comment author="kujenga" created="2017-06-27T13:44:22Z" id="311362435">@javanna got it, thanks. I'm seeing an empty `failures` array but a non-zero number failure count indicated in the response, which sounded similar to this issue.</comment><comment author="nik9000" created="2017-06-28T13:08:14Z" id="311654533">@kujenga what version are you seeing this on? Anything before 5.0 is beyond my ability to comment on, but if it happens in 5.x we likely still have the bug so it is worth tracking down. Also, do you get any related logs?</comment><comment author="kujenga" created="2017-06-28T23:06:38Z" id="311816534">@nik9000 we're on 2.3, but it's good news that upgrading may improve this functionality. I'll dig through the logs to see if there's anything relevant 👍, thanks for the tip.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file></files><comments><comment>Add failures reason to delete by query response</comment></comments></commit></commits></item><item><title>Setting to disable searching against all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5092</link><project id="" key="" /><description>It would be nice to be able to turn off the ability to query every index at once when none are specified.

Something similar to the setting:
action.disable_delete_all_indices

Such as:
action.disable_search_all_indices

If that were set to true, doing:

```
curl -XPOST localhost:9200/_search?q=test
```

Would throw an error.
</description><key id="27405793">5092</key><summary>Setting to disable searching against all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mbarker</reporter><labels /><created>2014-02-12T02:40:32Z</created><updated>2014-02-13T16:26:25Z</updated><resolved>2014-02-13T15:39:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ferhatsb" created="2014-02-13T08:43:04Z" id="34958352">There is a property used by other rest actions &lt;pre&gt; rest.action.multi.allow_explicit_index: false &lt;/pre&gt; 
but not with search api as I see.
</comment><comment author="javanna" created="2014-02-13T13:07:03Z" id="34975914">I think the way to go here would be to implement [url-based access control](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/url-access-control.html) by putting elasticsearch behind a proxy and deny operations that map to all indices (both the ones that don't specify the index name and the ones that specify `_all` as index).

The `rest.action.multi.allow_explicit_index` setting is useful too but it's something additional. It's used in multi index operations like bulk, multiget, multisearch and multipercolate, which allow to specify an index in the url and potentially override it in the request body (the index in the url will then only be used as a fallback). By setting `rest.action.multi.allow_explicit_index` to `false` (default `true`) you can prevent users from overriding the index name in the request body, to be able to implement [url-based access control](), as you can tell from the url which index gets accessed.
</comment><comment author="mbarker" created="2014-02-13T15:39:54Z" id="34990490">It looks like the functionality I want was added in the just release 1.0: https://github.com/elasticsearch/elasticsearch/pull/4453
</comment><comment author="javanna" created="2014-02-13T16:20:04Z" id="34995069">True @mbarker, `allow_no_indices` might help. On the other hand it can be set per request which is probably not what you need, and there's no global setting for it at this time.
</comment><comment author="mbarker" created="2014-02-13T16:26:25Z" id="34995795">Oh I didn't realize that was on the request level, figured it was a global setting. The url-based access control might work.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[0.9.11] simple_query_string does not recognize fields by their nested name but only by index_name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5091</link><project id="" key="" /><description>```
{
  simple_query_string: {
    query: 'hello world',
    fields:['award.title']
  }
}
```

returns no results while

```
{
  simple_query_string: {
    query: 'hello world',
    fields:['award_title']
  }
}
```

returns expected results. the issue is with fields:[] in first case I use logical (dot-notation) field name while ion the second I use actual lucene name (index_name in the mappings below)

```
"award": {
  "properties":{
            "title": {
              "type": "multi_field",
              "path": "just_name",
              "fields": {
                "title": {
                  "type": "string",
                  "index_analyzer": "stemmed",
                  "search_analyzer": "stemmed",
                  "index_options": "offsets",
                  "index_name": "award_title",
                  "boost": 3
                },
                "all": {
                  "type": "string",
                  "index_name": "i_all",
                  "boost": 3
                },
                "all_shingle": {
                  "type": "string",
                  "analyzer": "not_stemmed_shingle",
                  "index_name": "i_all_shingle",
                  "boost": 3
                },
                "all_stem": {
                  "type": "string",
                  "index_analyzer": "stemmed",
                  "search_analyzer": "stemmed",
                  "index_name": "i_all_stem",
                  "boost": 3
                }
              }
            }
...
  }
}
```
</description><key id="27402852">5091</key><summary>[0.9.11] simple_query_string does not recognize fields by their nested name but only by index_name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">roytmana</reporter><labels /><created>2014-02-12T01:24:25Z</created><updated>2014-02-13T16:49:30Z</updated><resolved>2014-02-13T16:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-02-12T03:03:55Z" id="34834442">I'm guessing this is because XSimpleQueryParser uses Lucene's query creators, and Lucene doesn't resolve the field using the mapping (just a guess). I'll take a look at this.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Allow specifying nested fields in simple_query_string</comment></comments></commit></commits></item><item><title>Make highlight query also work in the percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5090</link><project id="" key="" /><description>Percolate is throwing unsupported operation exception when highlight query is being used. This PR fixes that.
</description><key id="27402159">5090</key><summary>Make highlight query also work in the percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.0.1</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-12T01:09:36Z</created><updated>2015-06-07T23:26:32Z</updated><resolved>2014-02-12T15:16:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-12T15:14:48Z" id="34877869">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Java API] IdsQueryBuilder allow merging with list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5089</link><project id="" key="" /><description>Currently IdsQueryBuilder does support only appending arrays of identifiers and converting them to lists:

```
public IdsQueryBuilder addIds(String... ids) {
        values.addAll(Arrays.asList(ids));
        return this;
    }
```

In cases of huge numbers of IDs submitted to builder it could be more wise of performance to merge with lists directly. Or are there any better ideas?
</description><key id="27396497">5089</key><summary>[Java API] IdsQueryBuilder allow merging with list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">nfx</reporter><labels /><created>2014-02-11T23:20:31Z</created><updated>2015-06-09T08:02:52Z</updated><resolved>2015-06-09T08:02:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-12T10:41:51Z" id="34857155">Hey,

I am not sure I follow your request. Can you explain, which performance exactly you intend to improve? Right now the datastructure used there is a list. We could possibly use a set to prevent duplicate or use a HPPC collection to be more garbage collection friendly/faster. Is this what you mean with better merging performance?

Also wondering how much ids you intend to put into a single request. Keep in mind, that you will need to get the data from elasticsearch and return it to the client. It might not be a good idea to get several million entries per request.

Any helping comments are greatly appreciated here! Thanks a lot!
</comment><comment author="nfx" created="2015-03-09T22:08:58Z" id="77954692">It's just for the ease of use - applications collect ids as lists and it's easier to pass them directly inside of method calls
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/query/IdsQueryBuilder.java</file></files><comments><comment>IdsQueryBuilder: Allow to add a list in addition to array</comment></comments></commit></commits></item><item><title>Add support for distances in nautical miles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5088</link><project id="" key="" /><description>Added the DistanceUnit.NAUTICALMILES enumeration label with the corresponding "nm" unit suffix.

Closes #5085
</description><key id="27386537">5088</key><summary>Add support for distances in nautical miles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brian-from-fl</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-11T21:09:04Z</created><updated>2015-06-07T15:28:28Z</updated><resolved>2014-02-14T11:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-11T21:10:40Z" id="34807283">cool stuff - can you sign the CLA so we can pull this in?
</comment><comment author="brian-from-fl" created="2014-02-11T21:24:29Z" id="34808684">The CLA was submitted just before this pull request; I have the PDF file so it should be on its way.
</comment><comment author="chilling" created="2014-02-11T23:39:26Z" id="34822453">Nice One! But I think we should name it "nmi" rather than "nm" to avoid conflicts with metric units.
</comment><comment author="brian-from-fl" created="2014-02-11T23:51:59Z" id="34823368">I've seen "nmi" suggested, but for any distances related to aviation (distances between two points, maximum ranges, and so on), I've only seen "nm".  For one of many examples, the ranges of the various Gulfstream jets are given in "nm":

http://www.gulfstream.com/careers/our_products.html

But I'm all for changing the second string to "mni" (unless DistanceUnit supports more than 2 strings... I didn't have time to explore that one after I thought of it). But geospatial distances and aircraft distances never confuse "nm" with nanometers! So both "nm" and "nmi" would seem to be useful; let the client decide which to use. Does that make sense?
</comment><comment author="chilling" created="2014-02-12T00:06:32Z" id="34824383">Sure, within a specific field there won't be confusion. My concern is more about setting up more metric units. In this case nanometers and their naming. IMO we should have the "mi" suffix on miles and keep "m" for meters.
</comment><comment author="nik9000" created="2014-02-12T01:11:33Z" id="34828504">With nm being the SI symbol for nanometer I'd go with nmi for nautical miles.  A shame they share the symbol.
</comment><comment author="chilling" created="2014-02-12T01:14:27Z" id="34828682">yes, indeed!
</comment><comment author="brian-from-fl" created="2014-02-12T01:34:19Z" id="34829773">In that case, I wouldn't recommend pulling in this change. When I receive a feed that uses "nm" for nautical miles, as 100% of the aviation world uses, I'll need to front ES with my own converter. And in that case, I'll just convert it down to meters and the current DistanceUnit already supports it.

On the other hand, I can't imagine nanometers being useful for geo distances. Academics aside, a human hair is about 100,000 nanometers; is ES going to be used to search for the nearest bacterium at the end of hair number 21,456,234? 

http://www.rense.com/general74/small.htm
</comment><comment author="chilling" created="2014-02-12T02:36:58Z" id="34833187">I still think, supporting nautical miles is a good and reasonable idea. Also we're discussing measuring distances, a part of ES that should kept as general as possible. Maybe it doesn't make sense (yet) to talk about nanometer distances in the current context, but we should not spoil future applications of this.

In my opinion we should primarily support the SI units and their abbreviations and search for reasonable alternatives on non-SI units. Also, if we turn the argument and ask what the abbreviation _nm_ stands for, I would say even people of the aviation world ponder on the related field.

But for this special kind of purpose it's possible to set a default unit separately by the `unit` option. By setting this to `NAUTICMILES`, no matter which name we finally agree on, a value without an explicit unit specified, will be a nauticmile. I guess this will replace your own converter.
</comment><comment author="brian-from-fl" created="2014-02-12T03:29:54Z" id="34835558">The DistanceUnit is case-sensitive? Then how about "NM" (another accepted, though not common, abbreviation for nautical miles), and also "nmi"?

Nobody in aviation would ever doubt what "nm" stood for, or they wouldn't last long in aviation! But  I fully understand your argument, and can envision that nanometers might be extremely useful for the medical and bioengineering fields, for example (just not for geo-distances, of course). 

So in the interests of SI coexistence I propose "NM" and "nmi". And then, of course. "nauticalmiles" for consistency with the other supported enumerations. How does this sound?
</comment><comment author="chilling" created="2014-02-12T03:32:02Z" id="34835637">I agree! This is a great idea!
</comment><comment author="brian-from-fl" created="2014-02-12T03:33:51Z" id="34835719">I had to define NAUTICALMILES with "nmi" before MILES with "mi" because "mi" is a subset of "nmi" (same reason that METERS had to come last). But now the test case succeeds with both NM and nmi as suffix strings. If you think this is acceptable, I'll commit and push this change.

I might even try to rebase it into one commit message but not sure. I'm solidly booked for the next few days. But if you like this compromise and you think I should rebase then let me know. Thanks!
</comment><comment author="chilling" created="2014-02-12T03:38:52Z" id="34835949">@brian-from-fl yes it needs to be defined before miles. I guess rebase is not necessary right now, but please try. Thanks.
</comment><comment author="brian-from-fl" created="2014-02-12T03:44:11Z" id="34836148">I don't have my expert handy and there are only two comments; the most recent one is accurate and complete. I committed, and then pushed up to my nm-branch. Do I need to make another pull request?
</comment><comment author="brian-from-fl" created="2014-02-12T03:45:07Z" id="34836195">By the way, both of these test cases passed! (After I fixed the order!!!!)

```
    assertThat(DistanceUnit.Distance.parseDistance("53nmi").unit, equalTo(DistanceUnit.NAUTICALMILES));
    assertThat(DistanceUnit.Distance.parseDistance("53NM").unit, equalTo(DistanceUnit.NAUTICALMILES));
```
</comment><comment author="chilling" created="2014-02-12T03:47:35Z" id="34836296">That's great. You don't need to open another pull request but it would be kind, if you can squash both commits.
</comment><comment author="chilling" created="2014-02-12T03:56:42Z" id="34836642">sorry, I forgot to ask you if you can insert the new unit into `docs/reference/api-conventions.asciidoc` into the _distance unit_ section. Just to have it all in one commit.
</comment><comment author="brian-from-fl" created="2014-02-12T04:05:02Z" id="34836979">Re: Rebase. A bit confusing to this git newbie. If I make the most recent commit message all-inclusive, is that OK?

Re: docs. How's this? "Nautical miles" is two words; Is this acceptable or do you have a better suggestion:

```
    Nautical_mile:: 'NM' or 'nmi' or 'nauticalmiles'
```
</comment><comment author="chilling" created="2014-02-12T04:12:55Z" id="34837273">no worries. On docs just insert `Nautical mile::  `NM`, `nmi` or `nauticalmiles``. On git checkout to the master brach, pull current head, checkout to your branch, rebase to master. After that just squash the three commits into one and force push.
</comment><comment author="brian-from-fl" created="2014-02-12T04:47:55Z" id="34838602">I did the rebase and squash thing... went ok including the most recent commit message update. Then a push failed. Needed to pull, but then a merge conflict. After correcting that, the commit message is not what it was but... as I said, this is my very first day with git and GitHub. I hope it's ok as it stands. My apologies if it's worse than before.
</comment><comment author="brian-from-fl" created="2014-02-12T04:53:02Z" id="34838786">Had to fix the asciidoc since I noticed it used backtics and not single quotes. 

Then on review, I noticed that the TimeValue is missing `ms` (milliseconds). But that's for another day, I suppose?
</comment><comment author="chilling" created="2014-02-12T04:56:32Z" id="34838935">Yes, I thing the timevalues are another story. on the current branch you're always able to override your last commit message by `git commit --amend`.
</comment><comment author="chilling" created="2014-02-12T05:02:26Z" id="34839188">I guess you solved all the conflicts and you're on the latest master now? so look up the git log for the last sha1 id that's not associated with your changes and do `git rebase -i $sha1$` and replace all _pick_ with _s_. but not the one in the first line, run the tests again.
</comment><comment author="s1monw" created="2014-02-12T09:24:06Z" id="34851607">given that these are the official symbols for nautic miles `symbol M, NM or nmi` I am +1 on this.
</comment><comment author="costin" created="2014-02-13T11:36:28Z" id="34970469">+1 on using the International System of Units conventions; I'm not a fan of using upper vs lower case but these are the official names so we should support them as such.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Type-exists HEAD request returns 404 unless type exists in all specified indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5087</link><project id="" key="" /><description>In 0.90.10 and 0.90.11, with logstash generated indexes logstash-YYYY.MM.DD, wildcard resolution fails for HEAD index existence checks.  This causes failures in elasticsearch-hadoop M2 and other tools that use this paradigm to check for index existence.

``` bash
~$ curl -I -XHEAD 'http://localhost:9200/logstash-*/log4jlogs'
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
```

``` bash
~$ curl -I -XGET 'http://localhost:9200/logstash-*/log4jlogs/_search'
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 11083
```
</description><key id="27382285">5087</key><summary>Type-exists HEAD request returns 404 unless type exists in all specified indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bwmeier</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2014-02-11T20:21:50Z</created><updated>2016-11-06T12:15:13Z</updated><resolved>2016-11-06T12:15:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bwmeier" created="2014-02-11T20:24:44Z" id="34801599">Additional information:  The wildcard resolution works for certain expressions, so testing should be aware of this.

``` bash
~$ curl -I -XHEAD 'http://localhost:9200/logstash-2014.02.11/log4jlogs'
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

~$ curl -I -XHEAD 'http://localhost:9200/logstash-2014.02.*/log4jlogs'
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

~$ curl -I -XHEAD 'http://localhost:9200/logstash-2014.*/log4jlogs'
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

~$ curl -I -XHEAD 'http://localhost:9200/logstash-*/log4jlogs'
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
```
</comment><comment author="spinscale" created="2014-02-12T11:04:33Z" id="34858669">Hey,

I managed to reproduce your issue

``` sh
curl -X DELETE http://localhost:9200/
curl -X PUT http://localhost:9200/logstash-2013.01.10/log4jlogs/1 -d '{"foo":"bar"}'
curl -X PUT http://localhost:9200/logstash-2014.01.10/foo/1 -d '{"foo":"bar"}'
curl -X PUT http://localhost:9200/logstash-2014.02.10/log4jlogs/1 -d '{"foo":"bar"}'
curl -X PUT http://localhost:9200/logstash-2014.02.11/log4jlogs/1 -d '{"foo":"bar"}'
curl -I -XHEAD 'http://localhost:9200/logstash-2014.02.*/log4jlogs'
curl -I -XHEAD 'http://localhost:9200/logstash-2014.*/log4jlogs'
curl -I -XHEAD 'http://localhost:9200/logstash-*/log4jlogs'
```

The problem is getting more obvious then. The reason, you are receiving a 404 is most likely, that the `log4jlogs` type is not included in one of the indices which matched the wildcard search. Then elasticsearch returns a 404. I need to think about, if it is more useful to sort of invert this logic and return 200 if the type is included in any of the matched indices.

That would be the behaviour you are expecting, right?
</comment><comment author="bwmeier" created="2014-02-12T16:24:33Z" id="34885622">Yes that's definitely what I would expect.  As a general principle, I'd expect that if the associated GET query would return successfully (with or without data) then the HEAD query would return 200 as well.  

I noticed this because the elasticsearch-hadoop-1.3.0.M2 build no longer works in this circumstance, so you might want to loop @costin in on the conversation.
</comment><comment author="bwmeier" created="2014-02-12T16:32:47Z" id="34886696">Ah, I commented before I checked.  The search query returns successfully even if the type does not exist, it only fails if the index does not exist.  This makes for an interesting question as to what the behavior should be.

However, I definitely think that if the associated _search query would return data, then the HEAD query should return 200, by principle of least surprise.
</comment><comment author="kleptog" created="2016-07-08T15:55:18Z" id="231397813">FWIW, a 404 is also returned when using _all, so it's not specifically related to index wildcard resolution.

ES 2.0.0
</comment><comment author="clintongormley" created="2016-11-06T12:15:13Z" id="258677159">&gt; However, I definitely think that if the associated _search query would return data, then the HEAD query should return 200, by principle of least surprise.

I think I disagree with this.  Search is just one use case.  What if you were using this to answer the question "does this type exist in all indices or do I need to update the mapping"?

In fact, searching on a non-existent type doesn't return an error - it just returns zero results, which seems safe.

I think we should leave this as it is.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>In MVEL .empty can be way way way slower then .isEmpty()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5086</link><project id="" key="" /><description>This happens when you use `.empty` to check if a field is empty and it actually is empty.  I've made a gist to reproduce it: https://gist.github.com/nik9000/8937202

High level:
1.  Hit ~1 million documents with a script score.
2.  Do something like `(doc['foo'].empty ? 0 : doc['foo'].value) * doc['bar']`.  The `.empty` is the key here.
3.  If most of the documents don't have a `foo` then this is really slow.  Like, two seconds slow.
4.  Instead, switch to this `(doc['foo'].isEmpty() ? 0 : doc['foo'].value) * doc['bar']`.  That is faster.  .36 seconds or so.  Not super speedy, but much better.

I'm not completely sure what is happening but my guess is that the MVEL optimizer is optimizing .empty is a call to .isEmpty on an instance of either ScriptDocValues or ScriptDocValues.Empty.  When it hits the wrong one it recovers by catching an IllegalArgumentException thrown by the JVM and then does some munging.  The act of filling in the stack trace for that IllegalArgumentException is really really really slow.  So slow, I see it in the hot threads: https://gist.github.com/nik9000/8937335 .

Workaround: use `.isEmpty()` instead of `.empty`.

I'm not sure what the fix ought to be.
</description><key id="27358187">5086</key><summary>In MVEL .empty can be way way way slower then .isEmpty()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-11T15:50:50Z</created><updated>2014-12-29T14:21:30Z</updated><resolved>2014-12-29T11:36:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-14T14:28:55Z" id="35087655">No one has responded in a few days and I don't have time to do much other than the work around.  Would it be reasonable for me to add this information to the documentation until someone has time to fix this?
</comment><comment author="s1monw" created="2014-02-14T14:42:58Z" id="35088919">@nik9000 sorry you opened that while 1.0 was so hot I couldn't touch anything else - gimme another day or two :)
</comment><comment author="nik9000" created="2014-02-14T14:48:46Z" id="35089504">Cool cool.
</comment><comment author="clintongormley" created="2014-12-29T11:36:56Z" id="68250720">MVEL has been removed. Closing
</comment><comment author="nik9000" created="2014-12-29T14:21:30Z" id="68260767">Thanks!  Groovy has much better performance and seems much more stable!
On Dec 29, 2014 6:37 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; Closed #5086 https://github.com/elasticsearch/elasticsearch/issues/5086.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/5086#event-212429701
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enhance DistanceUnit to recognize nautical miles.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5085</link><project id="" key="" /><description>The DistanceUnit class should add the DistanceUnit.NAUTICALMILES enumeration. For example:

NAUTICALMILES(1852.0, "NM", "nmi"),

I'll let someone else decide what the second, longer, string should actually be. Most aeronautical charts use "nm" but I've also seen "nmi".
</description><key id="27357602">5085</key><summary>Enhance DistanceUnit to recognize nautical miles.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brian-from-fl</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-11T15:43:59Z</created><updated>2015-06-07T15:28:15Z</updated><resolved>2014-02-14T10:50:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2014-02-14T11:14:57Z" id="35075127">@brian-from-fl thanks a lot
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/unit/DistanceUnit.java</file><file>src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java</file></files><comments><comment>Added the `DistanceUnit.NAUTICALMILES` enumeration</comment><comment>label with the corresponding *NM* and *nmi* unit</comment><comment>suffixes. Update the docs to match.</comment></comments></commit></commits></item><item><title>Nested JSON problem, get MapperParsingException failed to parse in 0.90.10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5084</link><project id="" key="" /><description>Hi all,
get MapperParsingException failed to parse in 0.90.10

```
[2014-02-11 16:05:09,402][DEBUG][action.bulk              ] [Thunderbolt] [logstash-2014.02.11][4] failed to execute bulk item (index) index {[logstash-2014.02.11][suricata][deuCC2bkRvehNSA62tuuHw], source[{"tags":["suricata"],"@version":1,"@timestamp":"2014-02-11T16:05:07.540+01:00","host":"ipd1.felten-group.com","file":{"filename":"/SpamResolverNG/SpamResolverNG.dll","magic":"data","state":"CLOSED","stored":false,"size":115},"message":"{\"time\":\"02\\/11\\/2014-15:05:07.540410\",\"event_type\":\"file\",\"src_ip\":\"84.39.152.31\",\"src_port\":80,\"dest_ip\":\"192.168.100.120\",\"dest_port\":3255,\"proto\":\"TCP\",\"http\":{\"url\":\"\\/SpamResolverNG\\/SpamResolverNG.dll?DoNewRequest\",\"hostname\":\"resolver1.altn.ctmail.com\",\"http_refer\":\"&lt;unknown&gt;\",\"http_user_agent\":\"Mozilla\\/4.0 (compatible; Win32; Commtouch Http Client)\"},\"file\":{\"filename\":\"\\/SpamResolverNG\\/SpamResolverNG.dll\",\"magic\":\"data\",\"state\":\"CLOSED\",\"stored\":false,\"size\":115}}","type":"suricata","received_at":"2014-02-11 16:05:07 +0100","event_type":"file","src_ip":"84.39.152.31","src_port":80,"proto":"TCP","http":{"url":"/SpamResolverNG/SpamResolverNG.dll?DoNewRequest","hostname":"resolver1.altn.ctmail.com","http_refer":"&lt;unknown&gt;","http_user_agent":"Mozilla/4.0 (compatible; Win32; Commtouch Http Client)"},"dst_ip":"192.168.100.120","dst_port":3255,"geoip":{"ip":"84.39.152.31","country_code2":"DE","country_code3":"DEU","country_name":"Germany","continent_code":"EU","latitude":51.0,"longitude":9.0,"timezone":"Europe/Berlin","location":[9.0,51.0]}}]}
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [file]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:416)
    at org.elasticsearch.index.mapper.multifield.MultiFieldMapper.parse(MultiFieldMapper.java:204)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:514)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:456)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:516)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:460)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:353)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:402)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:156)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:701)
Caused by: org.elasticsearch.ElasticSearchIllegalArgumentException: unknown property [filename]
    at org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateFieldForString(StringFieldMapper.java:310)
    at org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateField(StringFieldMapper.java:261)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:405)
```

and this is my template:

```
{
  "template" : "logstash-*",
  "settings" : {
    "index.refresh_interval" : "5s",
    "analysis" : {
      "analyzer" : {
        "default" : {
          "type" : "standard",
          "stopwords" : "_none_"
        }
      }
    }
  },
  "mappings" : {
    "_default_" : {
       "_all" : {"enabled" : true},
       "dynamic_templates" : [ {
         "string_fields" : {
           "match" : "*",
           "match_mapping_type" : "string",
           "mapping" : {
             "type" : "multi_field",
               "fields" : {
                 "{name}" : {"type": "string", "index" : "analyzed", "omit_norms" : true },
                 "raw" : {"type": "string", "index" : "not_analyzed", "ignore_above" : 256}
               }
           }
         }
       } ],
       "properties" : {
         "@version": { "type": "string", "index": "not_analyzed" },
         "ipver":{"type":"long"},
         "protocol":{"type":"long"},
         "size":{"type":"long"},
         "sp":{"type":"long"},
         "stored":{"type":"boolean"},
         "@timestamp":{"type":"date", "format":"dateOptionalTime"},
         "dp":{"type":"long"},
         "rcvd":{"type":"long"},
         "sent":{"type":"long"},
         "sid":{"type":"long"},
         "policy_id":{"type":"long"},
         "size":{"type":"long"},
         "ids_priority":{"type":"long"},
         "duration":{"type":"long"},
         "src_port":{"type":"long"},
         "src_xlated_port":{type: "long"},
         "dst_port":{"type":"long"},
         "dst_xlated_port":{type: "long"},
         "TTL":{"type":"long"},
         "geoip" : {
           "type" : "object",
             "dynamic": true,
             "path": "full",
             "properties" : {
                "location" : { "type" : "geo_point" }
             }
         }
       }
    }
  }
}
```

and my mapping:

```
{"kibana-int":{"temp":{"properties":{"dashboard":{"type":"string"},"group":{"type":"string"},"title":{"type":"string"},"user":{"type":"string"}}},"dashboard":{"properties":{"dashboard":{"type":"string"},"group":{"type":"string"},"title":{"type":"string"},"user":{"type":"string"}}}},"logstash-2014.02.11":{"_default_":{"dynamic_templates":[{"string_fields":{"mapping":{"type":"multi_field","fields":{"raw":{"index":"not_analyzed","ignore_above":256,"type":"string"},"{name}":{"index":"analyzed","omit_norms":true,"type":"string"}}},"match":"*","match_mapping_type":"string"}}],"properties":{"@timestamp":{"type":"date","format":"dateOptionalTime"},"@version":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs"},"TTL":{"type":"long"},"dp":{"type":"long"},"dst_port":{"type":"long"},"dst_xlated_port":{"type":"long"},"duration":{"type":"long"},"geoip":{"dynamic":"true","properties":{"location":{"type":"geo_point"}}},"ids_priority":{"type":"long"},"ipver":{"type":"long"},"policy_id":{"type":"long"},"protocol":{"type":"long"},"rcvd":{"type":"long"},"sent":{"type":"long"},"sid":{"type":"long"},"size":{"type":"long"},"sp":{"type":"long"},"src_port":{"type":"long"},"src_xlated_port":{"type":"long"},"stored":{"type":"boolean"}}},"suricata":{"dynamic_templates":[{"string_fields":{"mapping":{"type":"multi_field","fields":{"raw":{"index":"not_analyzed","ignore_above":256,"type":"string"},"{name}":{"index":"analyzed","omit_norms":true,"type":"string"}}},"match":"*","match_mapping_type":"string"}}],"properties":{"@timestamp":{"type":"date","format":"dateOptionalTime"},"@version":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs"},"TTL":{"type":"long"},"alert":{"properties":{"action":{"type":"multi_field","fields":{"action":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"category":{"type":"multi_field","fields":{"category":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"gid":{"type":"long"},"rev":{"type":"long"},"severity":{"type":"long"},"signature":{"type":"multi_field","fields":{"signature":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"signature_id":{"type":"long"}}},"dns":{"properties":{"id":{"type":"long"},"rdata":{"type":"multi_field","fields":{"rdata":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"rrname":{"type":"multi_field","fields":{"rrname":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"rrtype":{"type":"multi_field","fields":{"rrtype":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"ttl":{"type":"long"},"type":{"type":"multi_field","fields":{"type":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}}}},"dp":{"type":"long"},"dst_ip":{"type":"multi_field","fields":{"dst_ip":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"dst_port":{"type":"long"},"dst_xlated_port":{"type":"long"},"duration":{"type":"long"},"event_type":{"type":"multi_field","fields":{"event_type":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"file":{"type":"multi_field","fields":{"file":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"geoip":{"dynamic":"true","properties":{"area_code":{"type":"long"},"city_name":{"type":"multi_field","fields":{"city_name":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"continent_code":{"type":"multi_field","fields":{"continent_code":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"country_code2":{"type":"multi_field","fields":{"country_code2":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"country_code3":{"type":"multi_field","fields":{"country_code3":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"country_name":{"type":"multi_field","fields":{"country_name":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"dma_code":{"type":"long"},"ip":{"type":"multi_field","fields":{"ip":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"latitude":{"type":"double"},"location":{"type":"geo_point"},"longitude":{"type":"double"},"postal_code":{"type":"multi_field","fields":{"postal_code":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"real_region_name":{"type":"multi_field","fields":{"real_region_name":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"region_name":{"type":"multi_field","fields":{"region_name":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"timezone":{"type":"multi_field","fields":{"timezone":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}}}},"host":{"type":"multi_field","fields":{"host":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"http":{"properties":{"hostname":{"type":"multi_field","fields":{"hostname":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"http_content_type":{"type":"multi_field","fields":{"http_content_type":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"http_method":{"type":"multi_field","fields":{"http_method":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"http_refer":{"type":"multi_field","fields":{"http_refer":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"http_user_agent":{"type":"multi_field","fields":{"http_user_agent":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"length":{"type":"long"},"protocol":{"type":"multi_field","fields":{"protocol":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"redirect":{"type":"multi_field","fields":{"redirect":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"status":{"type":"multi_field","fields":{"status":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"url":{"type":"multi_field","fields":{"url":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"xff":{"type":"multi_field","fields":{"xff":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}}}},"ids_priority":{"type":"long"},"ipver":{"type":"long"},"message":{"type":"multi_field","fields":{"message":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"policy_id":{"type":"long"},"proto":{"type":"multi_field","fields":{"proto":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"protocol":{"type":"long"},"rcvd":{"type":"long"},"received_at":{"type":"multi_field","fields":{"received_at":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"sent":{"type":"long"},"sid":{"type":"long"},"size":{"type":"long"},"sp":{"type":"long"},"src_ip":{"type":"multi_field","fields":{"src_ip":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"src_port":{"type":"long"},"src_xlated_port":{"type":"long"},"stored":{"type":"boolean"},"tags":{"type":"multi_field","fields":{"tags":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"tls":{"properties":{"fingerprint":{"type":"multi_field","fields":{"fingerprint":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"issuerdn":{"type":"multi_field","fields":{"issuerdn":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"subject":{"type":"multi_field","fields":{"subject":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}},"version":{"type":"multi_field","fields":{"version":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}}}},"type":{"type":"multi_field","fields":{"type":{"type":"string","norms":{"enabled":false}},"raw":{"type":"string","index":"not_analyzed","norms":{"enabled":false},"index_options":"docs","include_in_all":false,"ignore_above":256}}}}}}}
```

What is wrong here ?
Thanks for any help
</description><key id="27357565">5084</key><summary>Nested JSON problem, get MapperParsingException failed to parse in 0.90.10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">StefanSa</reporter><labels /><created>2014-02-11T15:43:28Z</created><updated>2014-02-13T12:46:49Z</updated><resolved>2014-02-13T12:46:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-02-12T01:40:21Z" id="34830153">In your mapping `file` is mapped as a `string`:

```
"file": {
  "type": "multi_field",
  "fields": {
    "file": {
      "type": "string",
      "norms": {
        "enabled": false
      }
    },
    "raw": {
      "type": "string",
      "index": "not_analyzed",
      "norms": {
        "enabled": false
      },
      "index_options": "docs",
      "include_in_all": false,
      "ignore_above": 256
    }
  }

```

but in your document `file` is an object

```
  "file": {
    "filename": "/SpamResolverNG/SpamResolverNG.dll",
    "magic": "data",
    "state": "CLOSED",
    "stored": false,
    "size": 115
  }
```
</comment><comment author="StefanSa" created="2014-02-12T07:53:58Z" id="34846512">Hi imotov,
I had a discussion with Ivan Brusic  and he thinks that it may be a bug.
https://groups.google.com/d/msg/elasticsearch/1P3fM0oa7gU/8g0qqUxfPSoJ

Can someone from the dev's look at this?
thanks for any help
</comment><comment author="imotov" created="2014-02-13T01:58:18Z" id="34940847">@StefanSa I think this behavior can be easily explained by first indexing something like:

```
{
  "file": "blah.txt"
}
```

and then trying to index file as inner object:

```
{
 "file": {
    "filename": "/SpamResolverNG/SpamResolverNG.dll",
    "magic": "data",
    "state": "CLOSED",
    "stored": false,
    "size": 115
  }
}
```

You might even be able to find the record that created this mapping in the first place if you try to search for (unless it was long time ago and this record is now gone).

```
curl "localhost:9200/_search/_all/suricata?pretty" -d '{
  "query": {
    "constant_score":{
      "filter":{
        "exists":{
          "field":"file"
        }
      }
    }
  }
}'
```

So, it doesn't look like a bug to me. But if you think this is a bug, could you create a repro (http://www.elasticsearch.org/help) as Ivan Brusic already suggested? 
</comment><comment author="StefanSa" created="2014-02-13T12:46:49Z" id="34974622">OK imotov, your right.
You can close this ticket.
Thanks for your time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Delete by query does not delete all documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5083</link><project id="" key="" /><description>I am new to elasticsearch and facing an issue with 'delete by query' api. Please correct me if something wrong with approach or understanding

Steps to reproduce
1. Create about 50 documents in 50 different indices
2. Use delete by query api to delete all the documents created above.
3. Verify if all documents are deleted

Expected: all documents in all indices should be deleted
Actual: Few documents are left over, there is no consistency as to which ones ...

Gist "https://gist.github.com/ssanghavi/8935108" can help reproduce the issue

Also tried to delete using "_all/query" but even that does not work
If I delete documents from each index at a time i.e. loop over, then it works fine

I am using a single node on my desktop
</description><key id="27349417">5083</key><summary>Delete by query does not delete all documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ssanghavi</reporter><labels><label>non-issue</label></labels><created>2014-02-11T13:58:54Z</created><updated>2014-02-26T08:42:36Z</updated><resolved>2014-02-12T11:41:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ssanghavi" created="2014-02-11T14:05:09Z" id="34756754">Forgot to mention the elasticsearch versions

version              test passes/failed?
0.90.11                 FAIL
0.90.9                   FAIL
0.20.2                   PASS

So this seems to be broken in 0.90.*
</comment><comment author="s1monw" created="2014-02-11T15:19:42Z" id="34763919">I think since 0.90 you need to run a refresh after the delete by query. Can you try if that fixes the problem?
</comment><comment author="ssanghavi" created="2014-02-11T18:27:04Z" id="34787584">thanks for quick reply, refresh did not help ...
curl -s -XPOST 'http://localhost:9200/_all/_refresh'

Delete by query produces below output where it failed to delete. However I do not see anything in logs
"twitter-24":{"_shards":{"total":5,"successful":1,"failed":4}}
</comment><comment author="ssanghavi" created="2014-02-11T18:30:46Z" id="34787996">URL in previous comment got corrupted its actually
curl -s -XPOST 'http://localhost:9200/_all/_refresh'
</comment><comment author="kzwang" created="2014-02-12T04:34:00Z" id="34838068">@ssanghavi 
Can you try add a config `threadpool.index.queue_size` to something larger than 200 e.g. `threadpool.index.queue_size: 1000` or set to -1 and try again

I think the queue size for index thread pool is default to 200 and that delete by query will add more than 200 jobs, so it will fail

The IndexDeleteByQueryResponse contains failures information but not returned to client, I found this error there

```
EsRejectedExecutionException[rejected execution (queue capacity 200) on org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1@63808bcc]
```
</comment><comment author="javanna" created="2014-02-12T11:41:20Z" id="34860968">Hi @ssanghavi, the `_shards` section in the response is very important. You got failures back, thus it makes sense that not all documents got deleted.

The reason why that happened seems to be indeed the number of concurrent tasks that need to be executed, which relates to how many shards your request hits on a single node. 50 indices \* 5 shards = 250 which is higher than the queue size (200). I wouldn't recommend to increase the threadpool size, but rather to scale out to more nodes, or try and split your request into more requests that hit less shards each.

In 0.20 we had an unbounded queue size, which is why you didn't see this happen. but you do see it in 0.90.x.

Closing this as it is the expected behaviour, I'll follow up on getting back the actual shard failures though (#5093).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enable percolation of nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5082</link><project id="" key="" /><description /><key id="27346013">5082</key><summary>Enable percolation of nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Percolator</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-11T13:14:41Z</created><updated>2015-06-06T18:42:45Z</updated><resolved>2014-02-15T08:29:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-02-12T15:23:12Z" id="34878791">+1 Looks great @brwe 

I think we can pull this in.
</comment><comment author="martijnvg" created="2014-02-14T19:36:57Z" id="35116952">+1 The last commit looks good to me 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/percolator/MultiDocumentPercolatorIndex.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorIndex.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/percolator/SingleDocumentPercolatorIndex.java</file><file>src/test/java/org/elasticsearch/percolator/MultiPercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Enable percolation for nested documents</comment></comments></commit></commits></item><item><title>How to query between different values of a same field in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5081</link><project id="" key="" /><description>I have successfully installed logstash,elasticsearch and kibana.I have imported my magento logs in elasticsearch. my logs have field like  product_id of product ,its time stamp,also a field called "action".Action have values like added,removed,purchased and viewed that show that particular product  in log has any one of above action. 
i want to do a query which gives me logs of  only those products according to product id that are added and removed from the cart. so that i can visualize that these are the products which are added most and removed most. 
How is it possible in elasticsearch.
</description><key id="27342485">5081</key><summary>How to query between different values of a same field in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PalakGautam</reporter><labels><label>non-issue</label></labels><created>2014-02-11T12:05:52Z</created><updated>2014-02-11T12:33:07Z</updated><resolved>2014-02-11T12:33:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-11T12:33:07Z" id="34749864">please ask questions like this on the mailing list this is only for bugs and dev issues.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Regression in RC1 and RC2 (was working on Beta2 and before) for fields query attribute.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5080</link><project id="" key="" /><description>The results when I add a `fields=fields1,field2` in the query string in wrong since 1.0.0.RC1. Each fields is returned as an array.

Here is a simple test. I create a new index with one document and I do a `_search` request with `fields=...` parameter

``` sh
curl -XDELETE localhost:9200/fieldtest
curl -XPUT localhost:9200/fieldtest
curl -XPUT localhost:9200/fieldtest/document/1 -d '
{
    "user": "seb",
    "tags": ["maptimize", "elasticsearch"]
}
'
curl -XPOST http://localhost:9200/fieldtest/_refresh
curl -XPOST 'http://localhost:9200/fieldtest/_search?fields=user,tags&amp;pretty'
```

The output before RC1 is 

``` json
{
  "took" : 28,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "fieldtest",
      "_type" : "document",
      "_id" : "1",
      "_score" : 1.0,
      "fields" : {
        "tags" : [ "maptimize", "elasticsearch" ],
        "user" : "seb"
      }
    } ]
  }
}
```

which is fine, tags is an array and user a string.
But since RC1 and RC2 the output is

``` json
{
  "took" : 29,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "fieldtest",
      "_type" : "document",
      "_id" : "1",
      "_score" : 1.0,
      "fields" : {
        "tags" : [ "maptimize", "elasticsearch" ],
        "user" : [ "seb" ]
      }
    } ]
  }
}
```

tags is still an array as expected, but user is also an array! An `_search` without the param `fields=...` returns user as a string.
</description><key id="27329083">5080</key><summary>Regression in RC1 and RC2 (was working on Beta2 and before) for fields query attribute.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sgruhier</reporter><labels /><created>2014-02-11T07:14:56Z</created><updated>2014-02-11T10:52:23Z</updated><resolved>2014-02-11T10:52:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-11T10:52:23Z" id="34743647">This is a breaking change that was made in 1.0.0.RC1. It's documented here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/_return_values.html , including an explanation on why that was done.

Instead, you can use [source filtering](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-request-source-filtering.html) to retrieve just a part of the `_source`.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>whymatch in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5079</link><project id="" key="" /><description>I wanted to extract the total number of times a search term occurs..in a particular field..
Even if the search term is a  phrase,the query should return the total number of times that phrase
occurs and also the frequency of individual term of that phrase in each field.

Thanks
Navneet Mathpal
</description><key id="27326275">5079</key><summary>whymatch in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NavneetMathpal</reporter><labels /><created>2014-02-11T05:29:16Z</created><updated>2014-02-11T08:40:11Z</updated><resolved>2014-02-11T08:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-11T08:40:11Z" id="34735167">please ask questions like this on the google group, as we try to use this for bugs only.

you might want to check out http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/docs-termvectors.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>window_size not enforced in rescoring phase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5078</link><project id="" key="" /><description>looks like window_size is not enforced in rescoring phase

for example **index**:
- 12 shards
- 2 replicas
- 2m documents

i will expect that query below will return max 12 first results with score 100 (one per shard) and then remaining ones will have score 0
instead all results on the page have score 100

``` JSON
POST index/type/_search?from=0&amp;size=100
{
    "query": {
         "match_all": { }
    },
    "rescore": {
      "window_size": 1,
      "query": {
         "rescore_query": {
              "function_score": {
                                   "query": { "match_all": {  } },
                                   "boost_mode" : "replace",
                                   "boost_factor" : 100
                                }
         },
         "query_weight": 0,
         "rescore_query_weight": 1
       }
    }
}

```
</description><key id="27323025">5078</key><summary>window_size not enforced in rescoring phase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2014-02-11T03:22:58Z</created><updated>2014-02-11T14:59:29Z</updated><resolved>2014-02-11T14:37:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-11T13:43:27Z" id="34754911">Which version are you checking?  I think I changed the behavior in
https://github.com/elasticsearch/elasticsearch/pull/4749 to be what you
expect.  So you'd see it after 1.0.0.RC2.  Before that the rescore was
running on all documents pulled back by the Lucene query code.  It pulls
back max(rescore.window_size, size + offset).

Nik

On Feb 10, 2014, at 10:23 PM, Karol Gwaj notifications@github.com wrote:

looks like window_size is not enforced in rescoring phase

for example _index_:
- 12 shards
- 2 replicas
- 2m documents

i will expect that query below will return max 12 first results with score
100 (one per shard) and then remaining ones will have score 0
instead all results on the page have score 100

POST index/type/_search?from=0&amp;size=100{
    "query": {
         "match_all": { }
    },
    "rescore": {
      "window_size": 1,
      "query": {
         "rescore_query": {
              "function_score": {
                                   "query": { "match_all": {  } },
                                   "boost_mode" : "replace",
                                   "boost_factor" : 100
                                }
         },
         "query_weight": 0,
         "rescore_query_weight": 1
       }
    }}

## 

Reply to this email directly or view it on
GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5078
.
</comment><comment author="s1monw" created="2014-02-11T14:37:09Z" id="34759647">what you are seeing here is the result of the fact that the rescorer runs per shard which it has to but the results can come from multiple shards and that means you have 1 document per shard that has a score of 100 in the worst case. I think there is not much we can / should do about this. 
</comment><comment author="karol-gwaj" created="2014-02-11T14:37:44Z" id="34759705">im on 1.0.0.RC1,
i will try to upgrade to RC2  
</comment><comment author="karol-gwaj" created="2014-02-11T14:40:20Z" id="34759928">@s1monw,
i aware os the fact that rescorer runs per shard
in the case above i have index with 12 shards
when i set rescore window to 1 i will expect only 12 results with score 100 (one per shard as specified window size)
but instead im getting 100 of them (full page)
</comment><comment author="s1monw" created="2014-02-11T14:59:29Z" id="34761827">yeah sorry I misread - it seems to be fixed I just added a test for this pushed to `1.x` and `master`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file></files><comments><comment>Added test #5078</comment></comments></commit></commits></item><item><title>code coverage hookup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5077</link><project id="" key="" /><description>Add logic to perform code coverage analysis using jacoco http://www.eclemma.org/jacoco/trunk/doc/

Maven profiles are used so not to disrupt the current build flow.  Randomized testing plug-in is moved to the profiles because of configuration conflict if the code is instrumented by jacoco.
</description><key id="27303705">5077</key><summary>code coverage hookup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-02-10T21:52:33Z</created><updated>2014-07-16T21:48:41Z</updated><resolved>2014-02-14T18:14:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-10T23:20:30Z" id="34702702">Do you know if it would somehow possible not to duplicate the randomized-testing configurations between the default profile and the test-coverage profile? Otherwise I'm afraid they would quickly diverge.
</comment><comment author="mrsolo" created="2014-02-11T00:03:51Z" id="34710953">&gt; Do you know if it would somehow possible not to duplicate the randomized-testing configurations 
&gt; between the default profile and the test-coverage profile? Otherwise I'm afraid they would quickly 
&gt; diverge.

There is.

The main issue is this line inside randomizer

 &lt;argLine&gt;
-                                ${tests.jvm.argline}
-  &lt;/argLine&gt;

That overwrites the coverage use of argLine

[INFO] argLine set to -javaagent:/home/jenkins/.m2/repository/org/jacoco/org.jacoco.agent/0.6.4.201312101107/org.jacoco.agent-0.6.4.201312101107-runtime.jar=destfile=/home/jenkins/workspace/es_coverage_metal/jdk/JDK7/label/metal/target/jacoco.exec,excludes=jsr166e/**:jsr166y/**:org/apache/lucene/**

and disrupts the instrumentation.

One solution is to move to inherited pom.xml structure.  However that requires more extensive work.

a)  Parent pom.xml needs to be constructed.  Since     &lt;packaging&gt;jar&lt;/packaging&gt; can not be used as a parent pom.

b) For child pom, configuration override and pluginManagement can be used.
</comment><comment author="s1monw" created="2014-02-11T14:11:02Z" id="34757262">Can't you just pass what you pass to argLine  to -Dtest.jvm.argline instead of the randomized stuff? that should work as well then we don't need to change anything here?
</comment><comment author="mrsolo" created="2014-02-11T16:54:23Z" id="34774629">Most likely not since argLine manipulation is done by the jacoco plug in in the build phase?
Reverse may be doable though, that is set argLine based on test.jvm.argline.

I will investigate both cases.
</comment><comment author="mrsolo" created="2014-02-11T21:20:30Z" id="34808283">Simplified version submitted.  The default profile will only manipulate 
&lt;configuration&gt;
                &lt;argLine&gt;${tests.jvm.argline}&lt;/argLine&gt;
 &lt;/configuration&gt;

With env "tests.jvm.argline=-server -XX:+UseG1GC -XX:+UseCompressedOops" setting
Run with default profile

mvn test

18360 ttys000    0:02.10 /usr/bin/java -Xmx512m -Xss256k -XX:MaxPermSize=128m -XX:MaxDirectMemorySize=512m -Des.logger.prefix= -server -XX:+UseG1GC -XX:+UseCompressedOops -Dtests.prefix=tests -Dtests.seed=51DBEAE652CF827F -Des.logger.level=INFO -Des.node.local= -Des.node.mode= -Djava.awt.headless=true -Djava.io.tmpdir=. -Djava.security.policy=/Users/bhwang/elasticsearch-mrsolo/dev-tools/tests.policy -Djunit4.tempDir=/Users/bhwang/elasticsearch-mrsolo/target -Dtests.appendseed= -Dtests.assertion.disabled= -Dtests.awaitsfix= -Dtests.badapples= -Dtests.class= -Dtests.client.ratio= -Dtests.cluster_seed= -Dtests.enable_mock_modules= -Dtests.failfast= -Dtests.integration= -Dtests.iters= -Dtests.jvm.argline=-server -XX:+UseG1GC -XX:+UseCompressedOops  ...

Run with coverage profile

mvn test -Pcoverage

18385 ttys000    0:11.04 /usr/bin/java -Xmx512m -Xss256k -XX:MaxPermSize=128m -XX:MaxDirectMemorySize=512m -Des.logger.prefix= -javaagent:/Users/bhwang/.m2/repository/org/jacoco/org.jacoco.agent/0.6.4.201312101107/org.jacoco.agent-0.6.4.201312101107-runtime.jar=destfile=/Users/bhwang/elasticsearch-mrsolo/target/jacoco.exec,excludes=jsr166e/**:jsr166y/**:org/apache/lucene/*\* -Dtests.prefix=tests -Dtests.seed=A92A59D9AA01A3BC -Des.logger.level=INFO -Des.node.local= -Des.node.mode= -Djava.awt.headless=true -Djava.io.tmpdir=. -Djava.security.policy=/Users/bhwang/elasticsearch-mrsolo/dev-tools/tests.policy -Djunit4.tempDir=/Users/bhwang/elasticsearch-mrsolo/target -Dtests.appendseed= -Dtests.assertion.disabled= -Dtests.awaitsfix= -Dtests.badapples= -Dtests.class= -Dtests.client.ratio= -Dtests.cluster_seed= -Dtests.enable_mock_modules= -Dtests.failfast= -Dtests.integration= -Dtests.iters= -Dtests.jvm.argline=-server -XX:+UseG1GC -XX:+UseCompressedOops ... 
</comment><comment author="s1monw" created="2014-02-12T09:12:59Z" id="34850900">this looks much better to me @jpountz what do you think?
</comment><comment author="jpountz" created="2014-02-14T00:10:08Z" id="35042452">I agree, +1 to merge!
</comment><comment author="s1monw" created="2014-02-14T08:18:34Z" id="35064359">LGTM +1 bill go ahead and merge it in!
</comment><comment author="mrsolo" created="2014-02-14T18:14:07Z" id="35109409">https://github.com/elasticsearch/elasticsearch/commit/dffc7cd06d5cf0a39a888b9a726da96a9229da2d merge to master only
</comment><comment author="s1monw" created="2014-02-14T18:44:27Z" id="35112155">can you merge it to 1.x as well I'd like these branches to be as close as possible :)
</comment><comment author="mrsolo" created="2014-02-14T19:24:28Z" id="35115843">Done https://github.com/elasticsearch/elasticsearch/commit/b38091ad9a7439fd452ead15f27aab7aa81767a1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `custom_score` and `custom_boost_factor` queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5076</link><project id="" key="" /><description>`custom_boost_factor` and `custom_score` were deprecated in `0.90.5`
and their documentation was removed already in `1.0`. This commit
removes all support for those queries since they are supercede by
`function_score`.
</description><key id="27302902">5076</key><summary>Remove `custom_score` and `custom_boost_factor` queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>breaking</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T21:41:57Z</created><updated>2015-06-06T17:06:17Z</updated><resolved>2014-02-11T14:08:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-11T09:54:01Z" id="34739807">a lot of tests have been deleted here. Do those tests have already equivalent tests using function score? like the custom filter score tests for example.
</comment><comment author="spinscale" created="2014-02-11T09:58:07Z" id="34740070">I found another mention of `custom_score` in `docs/reference/search/request/sort.asciidoc` - I think we need to rephrase there as well
</comment><comment author="spinscale" created="2014-02-11T10:03:19Z" id="34740413">Also, I think it makes sense to create a migration_1.x asciidoc, where we mention the removal of this...
</comment><comment author="s1monw" created="2014-02-11T11:03:43Z" id="34744392">I will also fix `docs/reference/search/request/sort.asciidoc` @brwe did copy all the custom score tests   so we can just trash all the ones here which is great!
</comment><comment author="s1monw" created="2014-02-11T11:14:46Z" id="34745062">@spinscale I added a new commit 
</comment><comment author="spinscale" created="2014-02-11T13:07:03Z" id="34752145">LGTM then
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Variable renamings to reduce unnecessary variable naming diversity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5075</link><project id="" key="" /><description>I've been working on a research machine learning-based tool (link: http://groups.inf.ed.ac.uk/naturalize/ ) tool that analyzes source code identifiers and makes suggestions for renaming them. The goal is to reduce unnecessary diversity in variable naming and improve code readability. This pull request is only a small sample of the suggestions made for elasticsearch.

No functional changes were made in any of the commits
</description><key id="27292868">5075</key><summary>Variable renamings to reduce unnecessary variable naming diversity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mallamanis</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T19:29:20Z</created><updated>2015-06-07T15:28:54Z</updated><resolved>2014-04-07T10:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T10:28:13Z" id="39715314">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support richer segment patterns in common.path.PathTrie</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5074</link><project id="" key="" /><description>PathTrie builds a trie by splitting path specifications like so:

```
/{index}/{type}/{id} =&gt; * as index -&gt; * as type -&gt; * as id
```

The elasticsearch-jetty plugin uses this structure to manage security constraints for different API endpoints. However, because of [this line in PathTrie.java](https://github.com/elasticsearch/elasticsearch/blob/6c23ace68fd9579e02d570d70aea8880ed3929be/src/main/java/org/elasticsearch/common/path/PathTrie.java#L114), any path segment that contains a wildcard is reduced to match anything, ignoring any prefixes or suffixes:

```
/category-{index}/{type}/{id} =&gt; * as index -&gt; * as type -&gt; * as id
```

I'm sure more than just the jetty plugin would benefit from a trie that supported richer patterns (even simple ones) at each level of a path. The improvement would require a different implementation of PathTrie, but does not need an API change, meaning it could be implemented transparently.
</description><key id="27284088">5074</key><summary>Support richer segment patterns in common.path.PathTrie</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thejohnfreeman</reporter><labels><label>:REST</label><label>feedback_needed</label></labels><created>2014-02-10T17:26:16Z</created><updated>2015-04-26T20:08:51Z</updated><resolved>2015-04-26T20:08:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="templth" created="2014-02-11T09:54:59Z" id="34739861">Hi,

I have the same issue (see #4974). Parameters in URI can only correspond to the whole content of a segment. I find that it's not flexible enough. For example, I can't use URIs with the following format (OData format):

```
/odata/v4/{index}/odata.svc/{type}({id})
```

Thierry
</comment><comment author="javanna" created="2015-03-25T12:27:31Z" id="86004966">First of all sorry it took ages to comment on this. I am conflicted about this, primarily because we would introduce support for something that we don't need ourselves, which would complicate the codebase and would be potentially hard to test and maintain.
</comment><comment author="dakrone" created="2015-04-10T16:25:46Z" id="91607949">@thejohnfreeman is this something you would be able to contribute? (along with tests). I'm concerned the same way that @javanna is about adding something we don't use ourselves and making it testable and maintainable.
</comment><comment author="clintongormley" created="2015-04-26T20:08:50Z" id="96429524">Closing for now. Feel free to reopen if you are interested in @dakrone 's suggestion
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>LUCENE-5436 backport?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5073</link><project id="" key="" /><description>Our ES server fell over today thanks to LUCENE-5436. Could this fix be expedited to .90 and back ported to the .1x branches?
</description><key id="27283267">5073</key><summary>LUCENE-5436 backport?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">2bitoperations</reporter><labels /><created>2014-02-10T17:15:30Z</created><updated>2014-02-10T17:28:02Z</updated><resolved>2014-02-10T17:20:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-10T17:20:26Z" id="34656969">this has been backported to `0.90` last week about 20 min after I committed it to lucene trunk / 4x see:
93e9d2146e77f6c0523875b93c768ab7f81cfe04
</comment><comment author="s1monw" created="2014-02-10T17:28:02Z" id="34657845">here is the reference to the pull request: https://github.com/elasticsearch/elasticsearch/pull/5043
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Migrating NodesInfo API to use plugins instead of singular plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5072</link><project id="" key="" /><description>In order to be consistent (and because in 1.0 we switched from
parameter driven information to specifzing the metrics as part of the URI)
this patch moves from 'plugin' to 'plugins' in the Nodes Info API.

Important: This breaks BWC for 1.0 a bit more than the switch already done, but is now documented in the migration document as well.
</description><key id="27275817">5072</key><summary>Migrating NodesInfo API to use plugins instead of singular plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Stats</label><label>breaking</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T15:45:59Z</created><updated>2015-06-06T17:06:48Z</updated><resolved>2014-02-11T09:16:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-10T15:58:13Z" id="34647579">Left a minor comment. LGTM.
</comment><comment author="s1monw" created="2014-02-10T17:29:15Z" id="34657970">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add note to statistical facet docs about documents missing faceted field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5071</link><project id="" key="" /><description>Hey,

I was not sure how the computing of statistical facets worked in terms of how it treated documents that were missing the faceted field. I visited the #elasticsearch channel on irc.freenode.net, where @HonzaKral was very helpful with what happens - `"documents with missing fields are excluded"` - so I thought it would be a good idea to add a note to the docs and share this knowledge with the rest of the community in an accessible way :)

Cheers!
</description><key id="27275712">5071</key><summary>Add note to statistical facet docs about documents missing faceted field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">AnSavvides</reporter><labels /><created>2014-02-10T15:44:37Z</created><updated>2014-07-16T21:48:42Z</updated><resolved>2014-04-04T20:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-07T12:24:16Z" id="37019495">Hey,

we already have some explanation for this, see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets.html

```
missing : The number of documents which have no value for the faceted field
```

As several facets do support `missing`, docs have not been put into any special facet description. Makes sense?
</comment><comment author="AnSavvides" created="2014-04-04T20:52:39Z" id="39610151">Yup makes sense @spinscale, cheers!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix AndDocIdSet#IteratorBasedIterator to not violate initial doc state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5070</link><project id="" key="" /><description>AndDocIdSet#IteratorBasedIterator was potentially initialized with
NO_MORE_DOCS which violates the initial state of DocIdSetIterator and
could lead to undefined behavior when used in a search context.

Closes #5049
</description><key id="27275424">5070</key><summary>Fix AndDocIdSet#IteratorBasedIterator to not violate initial doc state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T15:40:37Z</created><updated>2015-06-07T23:27:01Z</updated><resolved>2014-02-10T17:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-10T16:34:30Z" id="34651571">+1 In addition to fixing the bug, I think this makes the iterator creation much better!
</comment><comment author="s1monw" created="2014-02-10T16:53:30Z" id="34653855">@jpountz pushed a new commit
</comment><comment author="jpountz" created="2014-02-10T17:02:01Z" id="34654879">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Setting index.codec.bloom.load to false on a running ES still holds most of the BloomFilter in Java Heap (need to close/reopen index or restart ES to reclaim Heap space)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5069</link><project id="" key="" /><description>Based on what  we have seen so far, we are still unable to release all of the org.elasticsearch.common.util.BloomFilter memory after updating all indices  index.codec.bloom.load from [true] to [false].  An ES restart, or an explicit close/open of each index will release the memory from heap.
</description><key id="27271457">5069</key><summary>Setting index.codec.bloom.load to false on a running ES still holds most of the BloomFilter in Java Heap (need to close/reopen index or restart ES to reclaim Heap space)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sameerpokarna</reporter><labels /><created>2014-02-10T14:54:22Z</created><updated>2014-12-29T11:36:31Z</updated><resolved>2014-12-29T11:36:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sameerpokarna" created="2014-02-11T03:25:25Z" id="34723385">It appears that the org.elasticsearch.common.util.BloomFilter is used by the index.cache.id.simple.SimpleIdCache and is left referenced (and held in Java Heap) by the org.elasticsearch.indices.ttl.IndicesTTLService$PurgerThread, even after flag is set to false. Opening and closing all of the indices releases the memory retained by org.elasticsearch.common.util.BloomFilter.
</comment><comment author="s1monw" created="2014-02-11T09:32:17Z" id="34738321">hey, lemme ask you some questions... Once you change the value `index.codec.bloom.load` do you still have searches going on on the indices? If so are some of the searches of type `scan` /  `scroll`? Does this also keep holding on to the bloom filter if you wait until all the searches are gone our of scope?
</comment><comment author="sameerpokarna" created="2014-02-11T11:46:36Z" id="34747089">No, I have no searches going on.
</comment><comment author="sameerpokarna" created="2014-02-11T13:32:35Z" id="34754089">Appears like the clusterChanged() method of IndexCache needs to implement the mechanism to clear the idCache...
</comment><comment author="s1monw" created="2014-02-11T14:13:09Z" id="34757454">I don't see where the ID cache holds on to the bloom filters really. The SegmentCoreReader (core cache key) is removed if the reader goes out of scope so I am not seeing how this relates to the clsuterChanged event?
</comment><comment author="sameerpokarna" created="2014-02-11T14:49:35Z" id="34760864">![image010](https://f.cloud.github.com/assets/6449715/2137781/9de40750-932b-11e3-965a-da0334a9b9e0.jpeg)

Attaching a yourkit screenshot, after changing setting to false. Hope this helps.
</comment><comment author="sameerpokarna" created="2014-02-11T14:57:06Z" id="34761590">If I close the index, the IndicesClusterStateService.clusterChanged() -&gt; applyCleanedIndices() appears to clean up the bloom filters.
indexInjector.getInstance(IndexCache.class).close();

When the bloom filter is reset, the clusterChanged of IndexCache does get called, but there is no action that is taken on idCache by this method
        // clear the query parser cache if the metadata (mappings) changed...
        if (event.metaDataChanged()) {
            queryParserCache.clear();
        }
</comment><comment author="s1monw" created="2014-02-11T15:06:33Z" id="34762583">so the way this works is that the we reopen the index reader when you set this method. Yet, the reader that are open while you put the setting in might still be used in searches or other places of the system ie. their ref count is not set to `0` and in-turn their core cache keys are still `alive` once the ref count goes to `0` it will free all the resources via the `public void onClose(Object coreCacheKey)` so there might be somebody that still holds on to an EngineSearcher or so?
</comment><comment author="sameerpokarna" created="2014-02-11T17:04:20Z" id="34775740">You are correct that the Lucene indexwriter is reopened, but ElasticSearch IndexCache is still holding on to the references, and I see this being cleared only in following scenarios:
1. InternalIndicesService.removeIndex()
2. At the end of percolate
All other cases of this usage are when other objects are being closed.
I do not find the reference where the reources are freed after the ref count goes down to 0. Can you please point me to that ?
(Please note that I am using 0.90.9)
</comment><comment author="clintongormley" created="2014-12-29T11:36:31Z" id="68250703">Bloom filters have now been removed. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Made possible to dynamically update `discovery.zen.publish_timeout` cluster setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5068</link><project id="" key="" /><description>`discovery.zen.publish_timeout` controls how long the master node is going to try and wait for all the nodes to respond to a cluster state publish before going ahead with the following updates in the queue (default 30s). Up until now changing the settings required restarting each node. The setting is now dynamic and can be changed through the cluster update settings api.

Closes #5063
</description><key id="27265563">5068</key><summary>Made possible to dynamically update `discovery.zen.publish_timeout` cluster setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T13:37:36Z</created><updated>2015-06-07T15:30:07Z</updated><resolved>2014-02-11T11:05:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-11T10:58:09Z" id="34744020">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update documentation to prevent dropping tcp connections</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5067</link><project id="" key="" /><description>See mailinglist thread https://groups.google.com/d/msg/elasticsearch/hRQRLW-9zAo/Wm24krbQ8roJ

```
I found out that a firewall in the middle was dropping open connections with x amount of time of inactivity.
ES wasn't really happy with this apparently.

Solution:

set network.tcp.keep_alive true

Add following params to sysctl.conf:

net.ipv4.tcp_keepalive_time = 60
net.ipv4.tcp_keepalive_probes = 6
net.ipv4.tcp_keepalive_intvl = 10
```

I think it makes sense to at least mention this somewhere in the docs in order to help the people.
</description><key id="27262477">5067</key><summary>Update documentation to prevent dropping tcp connections</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2014-02-10T12:47:43Z</created><updated>2016-04-25T07:28:06Z</updated><resolved>2016-04-25T07:28:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cmoad" created="2016-03-17T19:20:45Z" id="198043719">I have reason to believe this issue effected us as well on Google Compute Engine, which drops idle TCP connections after 10 minutes.

https://cloud.google.com/compute/docs/troubleshooting#communicatewithinternet

Their recommended settings:
`sudo /sbin/sysctl -w net.ipv4.tcp_keepalive_time=60 net.ipv4.tcp_keepalive_intvl=60 net.ipv4.tcp_keepalive_probes=5`
</comment><comment author="spinscale" created="2016-04-25T07:28:06Z" id="214180629">This should be closed by #10189
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] add DynamoDB river plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5066</link><project id="" key="" /><description>Add DynamoDB river plugin to plugins page

https://github.com/kzwang/elasticsearch-river-dynamodb
</description><key id="27259361">5066</key><summary>[DOCS] add DynamoDB river plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-02-10T11:52:37Z</created><updated>2014-07-16T21:48:44Z</updated><resolved>2014-02-13T09:39:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T09:39:40Z" id="34962046">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cat: ip doesn't agree with bound_address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5065</link><project id="" key="" /><description>A node with `network.host` set to `127.0.0.1` will show an IP that has nothing to do with the `lo` interface.

```
$ curl -s localhost:9200/_nodes\?all | python -mjson.tool | fgrep bound
            "bound_address": "inet[/127.0.0.1:9200]",
            "bound_address": "inet[/127.0.0.1:9300]",
$ curl -s localhost:9200/_cat/nodes\?h=ip,host
192.168.56.10 es1
```

Repro on OS X and Ubuntu 12.04.
</description><key id="27244101">5065</key><summary>cat: ip doesn't agree with bound_address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label></labels><created>2014-02-10T06:28:47Z</created><updated>2015-10-14T14:09:59Z</updated><resolved>2015-10-14T14:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-10T10:31:02Z" id="34617209">I think this is the correct address to return, this is the IP address of the box. To understand what it was configured with in relation to bound and publish hosts, then we should expose those 2 as options as well.
</comment><comment author="drewr" created="2014-02-10T16:35:18Z" id="34651646">One big reason for it is to iterate through them and call an API.  Most of the times ES is running on `0.0.0.0` but not always.  If it isn't, then there's no way to talk to ES.

I'm not sure we need another column over just changing this one.  I can't imagine a case where I have ES running explicitly on `_eth1:ipv4_` and I like that `/_cat/nodes` shows me `eth0`.  It would be too cumbersome to remember to flip on another column.
</comment><comment author="s1monw" created="2014-03-12T20:18:07Z" id="37458789">any resolution on this  @drewr 
</comment><comment author="drewr" created="2014-03-12T20:20:04Z" id="37459068">@kimchy thoughts about my response?
</comment><comment author="kimchy" created="2014-03-20T12:00:36Z" id="38158759">I am not sure that the correct setting is the local address. Even in the transport, we have what we bound to, and the publish address of how to connect to it. With the publish address, we try and not use a local address.

I think that the fix, if we want to, is not in the cat API then, but in how we set the host name and ip on the DiscoveryNode. Now, we just try and get the address the "box" is configured with, and we should maybe take the configuration into account and resolve the same way we resolve a publish address to initialize the node with.
</comment><comment author="s1monw" created="2014-03-24T11:25:27Z" id="38433513">pushed to 1.2 for now
</comment><comment author="drewr" created="2014-05-12T19:28:09Z" id="42876667">Been mulling this over and I like @kimchy original suggestion to just add `bound_host` and `publish_host` columns.  It's more explicit when you're not sure what's going to happen when instantiating `DiscoveryNode`.
</comment><comment author="clintongormley" created="2014-07-11T08:26:13Z" id="48706077">@drewr are you going to get this in for 1.3?
</comment><comment author="clintongormley" created="2015-10-14T14:09:59Z" id="148061930">The returned address is now the publish host. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Updating plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5064</link><project id="" key="" /><description>To update a plugin, we need to remove it first otherwise, elasticsearch will complain that it's already installed:

```
sudo bin/plugin -install mobz/elasticsearch-head

-&gt; Installing mobz/elasticsearch-head...
Failed to install mobz/elasticsearch-head, reason: plugin directory /usr/share/elasticsearch/plugins/head already exists. To update the plugin, uninstall it first using -remove mobz/elasticsearch-head command
```

I propose that we provide an `update` switch:

```
sudo bin/plugin -update mobz/elasticsearch-head
```

This should be some sugar that runs the following:

```
sudo bin/plugin -remove mobz/elasticsearch-head
sudo bin/plugin -install mobz/elasticsearch-head
```
</description><key id="27238660">5064</key><summary>Updating plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">F21</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-02-10T02:41:24Z</created><updated>2015-12-01T11:42:30Z</updated><resolved>2015-12-01T11:42:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="damienalexandre" created="2014-03-10T16:03:36Z" id="37199292">:+1: 
</comment><comment author="brusic" created="2014-03-10T16:42:41Z" id="37204005">Updating a plugin is not so simple. For pure "site" plugins such as head, updating can be done without issues, but in cases of plugins with Java classes, the process would not be possible because the classes have already been loaded and a restart would be required (although plugins can now have their own class loader, but dealing with reloading classes is too much of a pain).

There is no plugin state recorded in Elasticsearch AFAIK, so it probably cannot differentiate if an update request is for a "site" plugin or not. Updating site plugins should be possible and I also would like to see this feature.
</comment><comment author="meteormatt" created="2015-12-01T11:32:11Z" id="160941298">:+1: 
</comment><comment author="dadoonet" created="2015-12-01T11:42:30Z" id="160943383">Closing as per discussion happened in #15000 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/io/FileSystemUtils.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/test/java/org/elasticsearch/common/io/FileSystemUtilsTests.java</file><file>src/test/java/org/elasticsearch/plugins/PluginManagerTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>plugins: disable support for config dir</comment></comments></commit></commits></item><item><title>Allow to dynamically change `discovery.zen.publish_timeout` using cluster update settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5063</link><project id="" key="" /><description /><key id="27237617">5063</key><summary>Allow to dynamically change `discovery.zen.publish_timeout` using cluster update settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T01:58:29Z</created><updated>2015-06-07T15:29:06Z</updated><resolved>2014-02-11T11:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kevinkluge" created="2014-02-11T11:18:03Z" id="34745266">discussed with Simon and Uri, pushing to 1.1.  
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/cluster/settings/Validator.java</file><file>src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoverySettings.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckClusterUpdateSettingsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/cluster/settings/ClusterSettingsTests.java</file></files><comments><comment>Made possible to dynamically update `discovery.zen.publish_timeout` cluster setting</comment></comments></commit></commits></item><item><title>Zen Disco join/leave should have higher than URGENT priority</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5062</link><project id="" key="" /><description>When processing node joining and leaving, we should have the utmost priority for it, higher than URGENT. We need another level of fine grained events.
</description><key id="27237008">5062</key><summary>Zen Disco join/leave should have higher than URGENT priority</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T01:33:24Z</created><updated>2014-03-12T08:24:55Z</updated><resolved>2014-03-06T16:25:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-10T01:51:00Z" id="34595345">probably also cluster update settings, and reroute
</comment><comment author="bleskes" created="2014-02-10T07:55:33Z" id="34607222">+1 on these extra 2. Should apply to all manually initiated API calls. DevOps need to be able to intervene,even/especially when the queues are full.
</comment><comment author="uboness" created="2014-02-10T14:18:07Z" id="34635604">I can have a look at it.. not sure we should push it to 1.0.0 though
</comment><comment author="s1monw" created="2014-02-10T17:23:27Z" id="34657320">I'd also want to move this to 1.1 though - this is really something that should bake in
</comment><comment author="kevinkluge" created="2014-02-11T09:53:50Z" id="34739792">Simon, Uri and I discussed -- pushing to 1.1.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/cluster/service/PendingClusterTask.java</file><file>src/main/java/org/elasticsearch/common/Priority.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterServiceTests.java</file><file>src/test/java/org/elasticsearch/common/util/concurrent/PrioritizedExecutorsTests.java</file></files><comments><comment>Introduced a new IMMEDIATE priority - higher than URGENT</comment></comments></commit></commits></item><item><title>Bulk process of shard started/failed should not execute on already processed events</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5061</link><project id="" key="" /><description>This can lead to a starvation, where each new shard started event will cause the full queue to be drained, and not allowing other URGENT events to get in.
</description><key id="27235465">5061</key><summary>Bulk process of shard started/failed should not execute on already processed events</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-10T00:20:13Z</created><updated>2015-06-07T15:43:30Z</updated><resolved>2014-02-10T00:22:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file></files><comments><comment>Bulk process of shard started/failed should not execute on already processed events</comment><comment>closes #5061</comment></comments></commit></commits></item><item><title>Fixed parsing time zones as numeric value in DateHistogramParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5060</link><project id="" key="" /><description>Closes #5057
</description><key id="27231238">5060</key><summary>Fixed parsing time zones as numeric value in DateHistogramParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2014-02-09T21:09:01Z</created><updated>2014-06-16T15:13:42Z</updated><resolved>2014-02-09T22:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-09T21:18:58Z" id="34587048">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Travis CI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5059</link><project id="" key="" /><description>It'd be cool if Elasticsearch ran its tests on Travis or something else public that lets you run against multiple versions of Java.  I know I only ever test against what I have installed before sending a pull request.  I see there is a `.travis.yml` file in the repository but the builds haven't run for a few weeks and haven't passed.
</description><key id="27223338">5059</key><summary>Travis CI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-09T14:18:43Z</created><updated>2014-12-29T11:35:01Z</updated><resolved>2014-12-29T11:35:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-02-09T14:26:09Z" id="34575154">tried to enable travis a few weeks ago, our tests seem to crash travis, and worst, it stalls all the other builds in the org that we have... (like the ruby, perl builds).

We have an internal jenkins build system that runs the tests across multiple machine types, JVMs, and modes. Hopefully we will be able to make it open soonish.
</comment><comment author="nik9000" created="2014-02-09T19:54:20Z" id="34584119">I've done a bit of research.  I imagine the Travis stalling thing is caused by their maximum of 5 builds per user/organization (https://twitter.com/Stof70/status/424989723711397888).  You can't yet limit the number of build slots used by a particular project which would work around that (https://github.com/travis-ci/travis-ci/issues/1366).

I think the crashes are due to timeouts after ~40 minutes.  The documentation says the timeout is 50 minutes but I've seen several things timeout after 40.
</comment><comment author="nik9000" created="2014-02-10T16:03:33Z" id="34648202">So I poked at it on and off yesterday.  I think the root of the problem is that some tests fail with ES_TEST_LOCAL=false on Travis.  They also happen to fail on my laptop which is convenient.  Anyway, the failure represents itself as the test hanging for a very long time.  Eventually the test fails and, at least on travis, logs a ton of errors.  So many that Travis gives up recording them all.  I don't see it fail quite like that, but it still fails.  This causes the failure for me:

```
mvn test -Dtests.seed=DB99E9BF3586634B -Dtests.class=org.elasticsearch.search.facet.termsstats.ShardSizeTermsStatsFacetTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/New_York -Des.logger.level=INFO -Des.node.local=false -Dtests.cluster_seed=2EDAD28438681
```

And I'm on this revision: 836a08c07579e409cded85336e1b6d25076a27ee
</comment><comment author="s1monw" created="2014-02-10T16:05:55Z" id="34648447">@nik9000 can you provide the stacktrace of the failure and tell me what env you are running?
</comment><comment author="nik9000" created="2014-02-10T16:10:27Z" id="34648922">Yeah, sorry:  https://gist.github.com/nik9000/8918627

Highlights:

``` xml
      &lt;property name="java.version" value="1.7.0_51"/&gt;
      &lt;property name="java.vm.name" value="OpenJDK 64-Bit Server VM"/&gt;
      &lt;property name="os.arch" value="amd64"/&gt;
      &lt;property name="os.name" value="Linux"/&gt;
      &lt;property name="os.version" value="3.8.0-35-generic"/&gt;
```
</comment><comment author="s1monw" created="2014-02-10T16:30:20Z" id="34651126">hey @nik9000 I ran this seed like 100 times on 3 different machines and all I see is 

```
[INFO] BUILD SUCCESS
```

can you reproduce this? and if so can you reproduce it if you run `mvn clean`?
</comment><comment author="nik9000" created="2014-02-10T16:51:15Z" id="34653551">Fails for me every time.  Takes a few minutes doing it too.  Even with `es.node.local=true` and a clean.  Now my command looks like:

```
 mvn clean test -Dtests.seed=DB99E9BF3586634B -Dtests.class=org.elasticsearch.search.facet.termsstats.ShardSizeTermsStatsFacetTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/New_York -Des.logger.level=INFO -Des.node.local=true -Dtests.cluster_seed=2EDAD28438681
```

It passes quite quickly for me if I run the test in Eclipse.

I also tried it in a fresh window and that didn't help either.

I'm not really sure if this is what is causing Travis to fail any more but it couldn't hurt to find and fix it.
</comment><comment author="s1monw" created="2014-02-10T16:52:37Z" id="34653742">do you have 2 or more network interfaces that are connected to different networks? I have seen problems if you have WLAN on as well as wired network active?
</comment><comment author="nik9000" created="2014-02-10T17:04:42Z" id="34655209">I don't even have a wired interface on this machine.

For reference, this is what travis's network interfaces look like:

```
$ ifconfig
lo Link encap:Local Loopback
inet addr:127.0.0.1 Mask:255.0.0.0
inet6 addr: ::1/128 Scope:Host
UP LOOPBACK RUNNING MTU:16436 Metric:1
RX packets:25 errors:0 dropped:0 overruns:0 frame:0
TX packets:25 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:0
RX bytes:16585 (16.5 KB) TX bytes:16585 (16.5 KB)

venet0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00
inet addr:127.0.0.2 P-t-P:127.0.0.2 Bcast:0.0.0.0 Mask:255.255.255.255
inet6 addr: 2607:f700:8000:12e:2aaa:65ca:333c:a758/128 Scope:Global
UP BROADCAST POINTOPOINT RUNNING NOARP MTU:1500 Metric:1
RX packets:6494 errors:0 dropped:0 overruns:0 frame:0
TX packets:3474 errors:0 dropped:1 overruns:0 carrier:0
collisions:0 txqueuelen:0
RX bytes:7445644 (7.4 MB) TX bytes:216969 (216.9 KB)

venet0:0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00
inet addr:172.30.77.185 P-t-P:172.30.77.185 Bcast:0.0.0.0 Mask:255.255.255.255
UP BROADCAST POINTOPOINT RUNNING NOARP MTU:1500 Metric:1
```
</comment><comment author="nik9000" created="2014-02-10T21:39:47Z" id="34685762">Is this being caused by `1.7.0_51`?  I know both travis and I share it.
</comment><comment author="s1monw" created="2014-02-10T21:43:58Z" id="34686185">well can you try with a different JVM we know `1.7.0_51` has issues though. I still wonder if that is network setup related though
</comment><comment author="s1monw" created="2014-02-20T19:41:08Z" id="35660649">nik did you dig deeper here?
</comment><comment author="joshk" created="2014-05-05T10:08:26Z" id="42174293">Anything we can do at Travis to help get ElasticSearch testing on Travis CI?
</comment><comment author="kimchy" created="2014-05-05T10:10:31Z" id="42174423">@joshk thanks for the suggestions!, the main problem we had, was that it seemed like travis builds organization projects in a serial manner, so if the elasticsearch project would take a long time, it would stall building all our other projects (like the different language clients). That was the main reason we disabled it. If this is solved, would be very happy to enable it and report back failures if we see them on the elasticsearch project itself.
</comment><comment author="joshk" created="2014-05-05T11:01:35Z" id="42177279">Hi @kimchy,

It is correct that the concurrent job limit is bound by the 'owner', so in this case, the elasticsearch organization.

If you require increased job concurrency, could you email support@travis-ci.com and we can see what is possible.

I would love to see you guys testing on Travis, especially for your Pull Requests :)
</comment><comment author="kimchy" created="2014-05-05T11:12:03Z" id="42177819">@joshk mail sent
</comment><comment author="clintongormley" created="2014-12-29T11:35:01Z" id="68250620">Closing as we now have public builds: http://build.elasticsearch.org/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>added local flag &amp; master_timeout support to cluster pending tasks api (0.90)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5058</link><project id="" key="" /><description>This PR was created against 0.90 branch so that we can backport to 0.90, in a backwards compatible manner, the support for `local` flag and `master_timeout` in cluster pending tasks api.

With #3345 we added support for the `local` flag across the board to all cluster state read operations. The change was only applied to master and 1.x branches though, while having `local` flag and `master_timeout` support backported to 0.90 for cluster pending tasks api would help a lot. 
</description><key id="27220970">5058</key><summary>added local flag &amp; master_timeout support to cluster pending tasks api (0.90)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.12</label></labels><created>2014-02-09T11:17:30Z</created><updated>2014-06-17T08:20:17Z</updated><resolved>2014-02-13T18:43:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>date_histogram aggregation and  time_zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5057</link><project id="" key="" /><description>According to http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-datehistogram-aggregation.html

the time_zone attribute can take integer value but in DateHistogramParser.java this attribute is only parsed when the value is a string.

So for example: 
  time_zone: 1 
gives: 

Parse Failure [Unknown key for a VALUE_NUMBER in [agg_name]: [time_zone].]]

Version: ES 1.0RC2 and 1.0 branch.
</description><key id="27206850">5057</key><summary>date_histogram aggregation and  time_zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">nnegativ</reporter><labels><label>bug</label><label>v1.0.0</label><label>v1.0.1</label><label>v1.1.0</label></labels><created>2014-02-08T19:26:01Z</created><updated>2014-02-09T22:01:35Z</updated><resolved>2014-02-09T22:01:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-02-08T21:21:29Z" id="34556639">indeed... will be fixed, thx!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file></files><comments><comment>Fixed parsing time zones as numeric value in DateHistogramParser</comment></comments></commit></commits></item><item><title>[DOCS] Add GitHub community river plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5056</link><project id="" key="" /><description /><key id="27200355">5056</key><summary>[DOCS] Add GitHub community river plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mihneadb</reporter><labels /><created>2014-02-08T13:21:59Z</created><updated>2014-07-16T21:48:45Z</updated><resolved>2014-02-11T11:02:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-11T11:02:11Z" id="34744303">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Doc fix explaining resynchronization with the Cancel command.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5055</link><project id="" key="" /><description>Added line to reroute doc explaining resync process with Reroute/Cancel command.

Closes #5025
</description><key id="27187122">5055</key><summary>Doc fix explaining resynchronization with the Cancel command.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2014-02-08T01:20:04Z</created><updated>2014-06-17T02:14:49Z</updated><resolved>2014-02-08T01:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Add GridFS repository community plugin</comment></comments></commit></commits></item><item><title>ElasticSearch installation instructions for Azure should point to the Private Key not cert</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5054</link><project id="" key="" /><description>Should be the private key, not the cert.

Expected:

SSH_OPTIONS="-o User=elasticsearch -o IdentityFile=/tmp/azure-private.key -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"

Actual:
SSH_OPTIONS="-o User=elasticsearch -o IdentityFile=/tmp/azure-certificate.pem -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"

http://www.elasticsearch.org/blog/azure-cloud-plugin-for-elasticsearch/
</description><key id="27182055">5054</key><summary>ElasticSearch installation instructions for Azure should point to the Private Key not cert</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaunmccarthy</reporter><labels /><created>2014-02-07T23:13:34Z</created><updated>2014-02-09T21:12:56Z</updated><resolved>2014-02-09T21:12:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-02-09T21:12:56Z" id="34586834">Fixed! Thanks :+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mapping: Fix possibility of losing meta configuration on field mapping update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5053</link><project id="" key="" /><description>Create a new schema with a field and the _timestamp field enabled:

curl -XPOST http://localhost:9200/foo -d '
{
        "mappings": {
                "product": {
                        "_timestamp" : { "enabled" : true },
                        "properties": {
                                "field1": {  "type": "integer" }
                        }
                }
       }
}'

Retrieve the mapping, all looks good:
curl http://localhost:9200/foo/_mapping
{"foo":{"product":{"_timestamp":{"enabled":true},"properties":{"field1":{"type":"integer"}}}}}

Now add another field:
curl -XPUT http://localhost:9200/foo/product/_mapping -d '
{
       "product" : {
                "properties": {
                        "field2" : {"type" : "integer" }
                }
        }
}
'
Get the mapping again:
curl http://localhost:9200/foo/_mapping
{"foo":{"product":{"properties":{"field1":{"type":"integer"},"field2":{"type":"integer"}}}}}

The _timestamp field is gone!
</description><key id="27158670">5053</key><summary>Mapping: Fix possibility of losing meta configuration on field mapping update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">shaunhall</reporter><labels><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-07T18:09:22Z</created><updated>2014-06-19T07:21:43Z</updated><resolved>2014-06-19T07:13:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shaunhall" created="2014-02-07T18:09:50Z" id="34482218">version 0.90.3
</comment><comment author="spinscale" created="2014-02-10T12:15:42Z" id="34625118">Hey

I cannot reproduce this with any of the 0.90 release (tried .0, .3 and .11).

The URLs you provided are not valid (must be `_mapping` and the PUT call actual create a document called `mapping` in the `foo` index. Can you fix this and provide a working example, so we can either find out if this is a bug on elasticsearch side or maybe your HTTP calls are not correct.

Thanks!
</comment><comment author="shaunhall" created="2014-02-10T19:59:55Z" id="34675340">Sorry, formatting fail :). Have updated the CURLs. Thanks.
</comment><comment author="shaunhall" created="2014-03-06T17:44:32Z" id="36914836">any update? I'll fix this if not
</comment><comment author="hanneskaeufler" created="2014-06-17T16:29:06Z" id="46331308">Whoah, I just ran into this today. Heres my test case:
https://gist.github.com/hanneskaeufler/2b6d69a2fc1a77509500

I´m running version 1.1.2

Is that expected behaviour or a bug?
</comment><comment author="spinscale" created="2014-06-17T16:59:06Z" id="46335096">hey there,

this looks like a bug indeed from a birds eye view, I will take a look as soon as I can!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/index/IndexTypeMapperIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/size/SizeMappingIntegrationTests.java</file><file>src/test/java/org/elasticsearch/timestamp/SimpleTimestampTests.java</file><file>src/test/java/org/elasticsearch/ttl/SimpleTTLTests.java</file></files><comments><comment>Mapping: Fix possibility of losing meta configuration on field mapping update</comment></comments></commit></commits></item><item><title>Removed 0.90.* deprecation and addition notifications</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5052</link><project id="" key="" /><description>This PR updates the 1.x documentation to remove all the added[] and deprecated[] notifications from 0.90.x releases.

it also removes the `numeric_range`, `custom_filters_score`, `custom_boost_factor` and `custom_script_score` docs.

Do we want to do this before 1.0?
</description><key id="27158220">5052</key><summary>Removed 0.90.* deprecation and addition notifications</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v1.0.0</label></labels><created>2014-02-07T18:01:58Z</created><updated>2015-06-30T12:45:26Z</updated><resolved>2014-02-07T19:53:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-07T18:03:08Z" id="34481643">+1 to remove it for 1.0
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Removed 0.90.* deprecation and addition notifications</comment></comments></commit></commits></item><item><title>Shard recovery failures due to OutOfMemoryError</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5051</link><project id="" key="" /><description>As the size of my indices started to grow, the frequency of these errors increased:

```
[2014-02-07 16:04:00,646][WARN ][cluster.action.shard     ] [appname-2.example.com] [appname-2013.11.24][1] sending failed shard for [appname-2013.11.24][1], node[86u1ewhBStSECH5uUkkOFQ], [R], s[INITIALIZING], indexUUID [_na_], reason [Failed to start shard, message [RecoveryFailedException[[appname-2013.11.24][1]: Recovery failed from [appname-1.example.com][8OOCSY-ORReONYEnwq0BNw][inet[/10.80.171.13:9300]]{instance_size=small, max_local_storage_nodes=1} into [appname-2.example.com][86u1ewhBStSECH5uUkkOFQ][inet[/10.28.156.176:9300]]{instance_size=small, max_local_storage_nodes=1}]; nested: RemoteTransportException[[appname-1.example.com][inet[/10.80.171.13:9300]][index/shard/recovery/startRecovery]]; nested: RecoveryEngineException[[appname-2013.11.24][1] Phase[2] Execution failed]; nested: RemoteTransportException[[appname-2.example.com][inet[/10.28.156.176:9300]][index/shard/recovery/prepareTranslog]]; nested: OutOfMemoryError[Java heap space]; ]]
[2014-02-07 16:04:03,458][WARN ][indices.cluster          ] [appname-2.example.com] [appname-2013.12.29][0] failed to start shard
org.elasticsearch.indices.recovery.RecoveryFailedException: [appname-2013.12.29][0]: Recovery failed from [appname-1.example.com][8OOCSY-ORReONYEnwq0BNw][inet[/10.80.171.13:9300]]{instance_size=small, max_local_storage_nodes=1} into [appname-2.example.com][86u1ewhBStSECH5uUkkOFQ][inet[/10.28.156.176:9300]]{instance_size=small, max_local_storage_nodes=1}
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:303)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$300(RecoveryTarget.java:65)
        at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:171)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: org.elasticsearch.transport.RemoteTransportException: [appname-1.example.com][inet[/10.80.171.13:9300]][index/shard/recovery/startRecovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [appname-2013.12.29][0] Phase[2] Execution failed
        at org.elasticsearch.index.engine.robin.RobinEngine.recover(RobinEngine.java:1156)
        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:589)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)
        at org.elasticsearch.indices.recovery.RecoverySource.access$1600(RecoverySource.java:61)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:337)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:323)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: org.elasticsearch.transport.RemoteTransportException: [appname-2.example.com][inet[/10.28.156.176:9300]][index/shard/recovery/prepareTranslog]
Caused by: java.lang.OutOfMemoryError: Java heap space
```

The errors occur on the two smaller nodes in the cluster with a 4.4G heap, and the larger nodes with 9G recover ok. It's to the point now where the cluster comes up yellow or red after a restart, depending on which shards the small nodes fail to recover.

This is on ES 0.90.11, and the two indices in the log entry are each around 60G with 2 shards.

Although I could just replace those nodes with higher memory instances, I wanted to confirm that this is the expected behavior, or if there's an upper limit to how large a shard should be to be able to recover it given a heap size. I expect the indices to continue to grow, so I'm afraid of reaching the point where a 9G heap isn't enough to recover.
</description><key id="27149731">5051</key><summary>Shard recovery failures due to OutOfMemoryError</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msonnabaum</reporter><labels><label>feedback_needed</label></labels><created>2014-02-07T16:30:18Z</created><updated>2014-11-10T10:55:19Z</updated><resolved>2014-11-10T10:55:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T14:25:15Z" id="49880649">Hi @msonnabaum 

Sorry it has taken a while to get to this.  There are many reasons why the OOM could have happened, only some of which might actually have been caused by recovery.  The shard size itself shouldn't be a problem.  I'm guessing it is too late to go back and find the real reason here. 

Are you still seeing this problem?
</comment><comment author="msonnabaum" created="2014-07-23T16:01:25Z" id="49894989">I did see it again, and fixed it by raising the heap. I can probably recreate it again by lowering the heap. If I can, what should I be looking for?
</comment><comment author="clintongormley" created="2014-07-23T17:05:56Z" id="49903941">Hi @msonnabaum 

thanks for getting back to me.

&gt; what should I be looking for?

The real reason for the OOM.  May be worth logging at DEBUG instead of INFO.  The OOM may be coming from something completely unrelated to recovery.  Are you querying at the same time, or is it just during recovery? Also, I'd be interested to know if you see the same thing on 1.3  (released today)

thanks
</comment><comment author="clintongormley" created="2014-09-26T18:15:06Z" id="56999925">Any more info @msonnabaum ?
</comment><comment author="clintongormley" created="2014-11-10T10:55:19Z" id="62368944">No more feedback, so closing. Please feel free to reopen with more details if you see the same problems in 1.4.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Array as result if search request contains "fields" section.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5050</link><project id="" key="" /><description>After update to v1.0.0.rc2 we faced with the issue: If search query contains "fields" section then result contains specified fields with values as array with one element - the actual value.

Example request:
{"query":{"term":{"id":"11"}},"fields":["id","type"]}

Result example (part):
{
    _index: "test",
    _type: "doc",
    _id: "A31133A23423477F3A-EC91-4E9A-85E1-FB8FF037BF17_060a2b340101010101010f0013-000000-51e64cf0e2ac0008-060e2b347f7f-2a80",
   _score: 3.3353748,
   fields: {
      id: [ "11" ],
      type: [ "type1" ]
}}
But "id", "type" fields should contain string value.
</description><key id="27147822">5050</key><summary>Array as result if search request contains "fields" section.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreyMelnik</reporter><labels /><created>2014-02-07T16:18:15Z</created><updated>2014-02-07T16:38:54Z</updated><resolved>2014-02-07T16:38:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-02-07T16:38:54Z" id="34465903">This is indeed a breaking change and it's documented here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/_return_values.html , including an explanation for why it was done.  

Instead, you can use the source filtering to retrieve just a part of it: http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-request-source-filtering.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>AssertionError during percolation using constant score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5049</link><project id="" key="" /><description>When elasticsearch is running with assertions enabled, a constant score query that doesn't match a record can cause AssertionError to be thrown. To reproduce start elasticsearch with assertions enabled and execute the following script

on 0.90 branch:

```
curl -XDELETE localhost:9200/test-idx
curl -XPUT 'localhost:9200/test-idx/'
curl -XPUT 'localhost:9200/_percolator/test-idx/1' -d '{
    "query" : {
        "constant_score":{
            "filter": {
                "and": [{
                    "query": {
                        "query_string" : {
                            "query" : "root"
                        }
                    }
                }, {
                    "term" : {
                        "message" : "tree"
                    }
                }]
            }
        }
    }
}'
curl -XGET 'localhost:9200/test-idx/message/_percolate' -d '{
    "doc" : {
        "message" : "A new bonsai tree in the office"
    }
}'
```

to reproduce on master:

```
curl -XDELETE localhost:9200/test-idx
curl -XPUT 'localhost:9200/test-idx/.percolator/1' -d '{
    "query" : {
        "constant_score":{
            "filter": {
                "and": [{
                    "query": {
                        "query_string" : {
                            "query" : "root"
                        }
                    }
                }, {
                    "term" : {
                        "message" : "tree"
                    }
                }]
            }
        }
    }
}'
curl -XGET 'localhost:9200/test-idx/message/_percolate' -d '{
    "doc" : {
        "message" : "A new bonsai tree in the office"
    }
}'
```

On the master the issue is not as prominent since the error is not returned to the user. On the current master the result of execution of the script above is the following error in the log:

```
[2014-02-07 10:49:32,594][WARN ][percolator               ] [Blizzard II] [[31]] failed to execute query
java.lang.AssertionError
    at org.apache.lucene.search.Scorer.score(Scorer.java:61)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.score(ConstantScoreQuery.java:256)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:543)
    at org.elasticsearch.percolator.PercolatorService.percolate(PercolatorService.java:232)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:194)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:55)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$2.run(TransportBroadcastOperationAction.java:226)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
```
</description><key id="27147323">5049</key><summary>AssertionError during percolation using constant score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-07T16:15:14Z</created><updated>2014-02-10T21:25:20Z</updated><resolved>2014-02-10T17:15:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/docset/AndDocIdSet.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Fix AndDocIdSet#IteratorBasedIterator to not violate initial doc state</comment></comments></commit></commits></item><item><title>aggregation error ArrayIndexOutOfBoundsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5048</link><project id="" key="" /><description>mapping extract

```
                "extension": {
                    "type": "string", // eg .xls, no empty fields
                    "index": "not_analyzed",
                },
                "sharepath": {
                    "type": "string", // eg //1.2.3.4/Someshare/, no empty fields
                    "index": "not_analyzed",
                },
                "doc_type": {
                    "type": "string", // eg Spreadsheet Files, no empty fields
                    "index": "not_analyzed"
                },
```

query works

```
{
  "query": {
    "filtered": {
      "filter": {
        "and": [
          {
            "range": {
              "modified": {
                "lt": 1391775892000
              }
            }
          }
        ]
      },
      "query": {
        "match_all": {}
      }
    }
  },
  "aggs": {
    "sharepath": {
      "terms": {
        "field": "sharepath",
        "size": 2147483647
      },
      "aggs": {
        "total_size_sharepath": {
          "filter": {
              "term": {
                  "doc_type": "Spreadsheet Files"
              }
          },
          "aggs": {
            "total_size": {
              "stats": {
                "field": "size"
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}
```

query fails (more or less same query as before)

```
{
  "query": {
    "filtered": {
      "filter": {
        "and": [
          {
            "range": {
              "modified": {
                "lt": 1391775892000
              }
            }
          }
        ]
      },
      "query": {
        "match_all": {}
      }
    }
  },
  "aggs": {
    "extension": {
      "terms": {
        "field": "extension",  // previously 'sharepath' which works
        "size": 2147483647
      },
      "aggs": {
        "total_size_extension": {
          "filter": {
            "term": {
              "doc_type": "Spreadsheet Files"
            }
          },
          "aggs": {
            "total_size": {
              "stats": {
                "field": "size"
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}
```

Stacktrace

```
[2014-02-07 14:07:35,301][DEBUG][action.search.type       ] [Copycat] [files_v1][3], node[XlVxAUsKRNinZGxwgkLyeg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@71c8e67c]
java.lang.ArrayIndexOutOfBoundsException: 51
  at org.elasticsearch.common.util.BigArrays$LongArrayWrapper.get(BigArrays.java:118)
  at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketDocCount(BucketsAggregator.java:79)
  at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.buildAggregation(FilterAggregator.java:73)
  at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:88)
  at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.buildAggregation(StringTermsAggregator.java:121)
  at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.buildAggregation(StringTermsAggregator.java:41)
  at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:132)
  at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:137)
  at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:230)
  at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
  at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
  at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
  at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
  at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:701)
```
</description><key id="27138629">5048</key><summary>aggregation error ArrayIndexOutOfBoundsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">j0hnsmith</reporter><labels><label>bug</label><label>v1.0.2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-07T14:38:28Z</created><updated>2014-07-17T11:11:36Z</updated><resolved>2014-03-04T08:58:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-07T14:50:11Z" id="34442412">Thanks for reporting this issue. Which version of Elasticsearch are you using? If not 1.0 RC2, could you try to reproduce with 1.0 RC2, there used to be such an issue in 1.0 Beta1 and Beta2 but this should be fixed now (by this commit: d0143703a19f6f8bac28096ab96f81ac3477a6eb).
</comment><comment author="j0hnsmith" created="2014-02-07T14:57:15Z" id="34443097">Thanks for your quick response. I'm using Beta2 as the mongo river plugin doesn't work with RC2 yet. 

I've just spent 20 mins looking back through the recent commits for a fix, either I missed it or didn't go back far enough ;). I should be able to test against RC2 on Monday.
</comment><comment author="redox" created="2014-02-25T07:18:28Z" id="35981466">Same kind of stacktrace here, using 1.0.0 (buildhash= a46900e9c72c0a623d71b54016357d5f94c8ea32)

```
Caused by: java.lang.ArrayIndexOutOfBoundsException: 6712
    at org.elasticsearch.common.util.BigArrays$ObjectArrayWrapper.get(BigArrays.java:257)
    at org.elasticsearch.search.aggregations.AggregatorFactories$1.buildAggregation(AggregatorFactories.java:102)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:107)
    at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.buildAggregation(StringTermsAggregator.java:235)
    at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.buildAggregation(StringTermsAggregator.java:50)
    at org.elasticsearch.search.aggregations.AggregatorFactories$1.buildAggregation(AggregatorFactories.java:102)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:107)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.buildAggregation(FilterAggregator.java:72)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:107)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.buildAggregation(FilterAggregator.java:72)
    at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:134)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:135)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:244)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:623)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:612)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```
</comment><comment author="redox" created="2014-02-25T17:32:12Z" id="36034667">Looks good so far @jpountz :+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java</file></files><comments><comment>Fix NPE/AIOOBE when building a bucket which has not been collected.</comment></comments></commit></commits></item><item><title>[DOCS] Pedantic docfixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5047</link><project id="" key="" /><description>Hello there,
over the last months I collected a bunch of mostly tiny fixes for the documentation. Please consider merging these into master, to make the elasticsearch documentation more awesome.

Note: I'm not a native speaker and mostly stumbled upon these while trying to translate the documentation to German.
</description><key id="27138216">5047</key><summary>[DOCS] Pedantic docfixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels /><created>2014-02-07T14:31:42Z</created><updated>2014-07-16T21:48:46Z</updated><resolved>2014-03-07T13:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Multiple doc fixes</comment></comments></commit></commits></item><item><title>Log warning in case of double release of the IndexSearcher instead of throwing an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5046</link><project id="" key="" /><description>Double release might rarely happen on long-running requests when the Reaper thread tries to release the searcher at the same time as the request finishes.
</description><key id="27134350">5046</key><summary>Log warning in case of double release of the IndexSearcher instead of throwing an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-02-07T13:22:13Z</created><updated>2014-07-16T21:48:46Z</updated><resolved>2014-02-07T14:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-07T13:58:35Z" id="34438221">I think the mock check is obsolete but otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>relocating replicas displays wrong information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5045</link><project id="" key="" /><description>It seems that a system where a node has been excluded using "shard allocation filtering", the information as given by the rest API is wrong. The cluster in this example is composed of 3 nodes, and NodeB has been filtered

```
$ es shards | awk '$1 == "webclusterlogs-2014.02.04" &amp;&amp; $2 == 5 { print }'
webclusterlogs-2014.02.04               5 p STARTED       84661  78.7mb    82580177 134.158.108.171 NodeA
webclusterlogs-2014.02.04               5 r RELOCATING    84661  78.7mb    82578811 134.158.108.25  NodeB -&gt; 134.158.104.214 NodeC
```

How could the data come from NodeB when it doesn't host the shard? Also, when monitoring network activity, it is clear that it is in fact NodeA sending the data to NodeC

Version is from rpm distribution elasticsearch-0.90.10-1.noarch on NodeA and NodeB, and elasticsearch-0.90.7-1.noarch on NodeC
</description><key id="27132848">5045</key><summary>relocating replicas displays wrong information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">faxm0dem</reporter><labels><label>feedback_needed</label></labels><created>2014-02-07T12:51:21Z</created><updated>2015-02-28T05:06:19Z</updated><resolved>2015-02-28T05:06:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-07T12:55:38Z" id="34434142">hey, how do you retrieve the data? I don't know your es shards command can you provide a curl statement for this?
</comment><comment author="faxm0dem" created="2014-02-07T14:24:48Z" id="34440191">I'm using `download.elasticsearch.org/es2unix/es`. I'm unfamiliar with API calls regarding shards. Please advise
</comment><comment author="s1monw" created="2014-02-07T14:25:23Z" id="34440243">yeah so i actually wonder if it is a bug in es2unix?
</comment><comment author="faxm0dem" created="2014-02-07T14:55:33Z" id="34442936">I don't think so: `elasticsearch-head` sees the same information.
I'm still searchign for the right `curl` command
</comment><comment author="faxm0dem" created="2014-02-07T14:59:54Z" id="34443363">so I just checked with the curl command, and it's definitely confirmed.
</comment><comment author="clintongormley" created="2014-12-24T19:25:02Z" id="68070703">Hi @faxm0dem 

Sorry it has been a while since we last looked at this ticket.  Is this a bug you are still seeing?  If so, could you give us the steps to recreate it?

thanks
</comment><comment author="faxm0dem" created="2014-12-29T14:13:40Z" id="68260174">I'll test again the next time I have the opportunity.
You can close this in the meantime if you like
</comment><comment author="clintongormley" created="2014-12-29T14:43:05Z" id="68262614">@faxm0dem i'll leave this open for the meantime.  If you get a chance to confirm one way or another, would be appreciated

thanks
</comment><comment author="clintongormley" created="2015-02-28T05:06:19Z" id="76511029">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>delete by query appears to be broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5044</link><project id="" key="" /><description>Tested with 1.0.0.RC2 and RC1.

To repro:
curl -XPUT 'http://localhost:9200/test/'
curl -XPUT 'http://localhost:9200/test/mytype/1' -d '{"name":"jack"}'
curl -XDELETE 'http://localhost:9200/test/mytype/_query' -d '{"ids":{"values":["1"]}}'

{"_indices":{"test":{"_shards":{"total":5,"successful":0,"failed":5}}}}

In the log, I see:

[2014-02-06 22:19:53,701][DEBUG][action.deletebyquery     ] [Shathra] [test][2], node[EGFe578KStSDkYbFLtXUCw], [P], s[STARTED]: Failed to execute [delete_by_query {[test][mytype], query [{"ids":{"values":["1"]}]}]
org.elasticsearch.index.query.QueryParsingException: [test] request does not support [ids]
    at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:303)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:444)
    at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnPrimary(TransportShardDeleteByQueryAction.java:118)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
</description><key id="27113282">5044</key><summary>delete by query appears to be broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">brooksrbrown</reporter><labels><label>docs</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-07T06:42:18Z</created><updated>2014-02-07T07:51:24Z</updated><resolved>2014-02-07T07:50:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brooksrbrown" created="2014-02-07T07:00:03Z" id="34408637">Okay, I've looked at the source code, and I can see that the query needs to be wrapped, i.e., {"query":&lt;the unwrapped query&gt;}. So, assuming elasticsearch.org is okay with this, this may be more of a documentation issue. This currently doesn't show that the query must be wrapped:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/docs-delete-by-query.html

Best regards,
Brooks
</comment><comment author="dadoonet" created="2014-02-07T07:42:23Z" id="34410498">Agree: It appears in breaking changes: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/_search_requests.html

Docs need to be updated in 1.0, 1.x and master.
</comment><comment author="dadoonet" created="2014-02-07T07:51:17Z" id="34411739">Done. Thanks for reporting it!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] delete by query requires a top-level query parameter</comment></comments></commit></commits></item><item><title>Use patched version of ReferenceManager to prevent infinite loop in ReferenceManager#acquire()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5043</link><project id="" key="" /><description>Some users on the mailinglist reported possible infinite loops while acquiring a new searcher. Yet the version this is reported are 0.90.7 but this seems like a general problem addressed in LUCENE-5436. I just committed a fix for the infinite loop such that it throws an IllegalStateException and this PR ports the fix from Lucene to Elasticsearch as a X class.
</description><key id="27080738">5043</key><summary>Use patched version of ReferenceManager to prevent infinite loop in ReferenceManager#acquire()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>enhancement</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-06T20:21:35Z</created><updated>2015-06-07T15:44:05Z</updated><resolved>2014-02-06T20:53:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-06T20:32:03Z" id="34367501">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geohash grid aggregations's precision should support distance unit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5042</link><project id="" key="" /><description>The precision parameter of the geohash grid agg current allows specifying precision using a number:

```
"geohash_grid" : {
     "field" : "location",
      "precision" : 3
}
```

we should also support unit distances, like the `geohash_precision` of the geo point mapping does.
</description><key id="27063413">5042</key><summary>Geohash grid aggregations's precision should support distance unit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Geo</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-02-06T16:39:43Z</created><updated>2017-03-14T00:52:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xando" created="2015-07-03T18:55:58Z" id="118408769">Hi. Is this is something that could get into future version of ES? 
</comment><comment author="jqb" created="2016-06-22T12:04:48Z" id="227722957">Hi, any chance to move this forward anytime soon? 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed the string() code literal in the java client index api doc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5041</link><project id="" key="" /><description>Just a simple fix of the missing code blocks.
</description><key id="27060825">5041</key><summary>Fixed the string() code literal in the java client index api doc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">evanwong</reporter><labels /><created>2014-02-06T16:10:39Z</created><updated>2014-06-24T09:19:52Z</updated><resolved>2014-02-06T16:31:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-06T16:31:44Z" id="34341589">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>org.elasticsearch.index.mapper.MapperParsingException: No content is provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5040</link><project id="" key="" /><description>Hello,
I'm trying to make scrutmydocs working with an elasticsearch demo node.
LINK: http://www.scrutmydocs.org

I have on my server:

```
1 elasticsearch 0.90.11 node (bobone) with fsriver 0.4.0; mapper attachments 2.0.0 RC1; head-plugin
1 tomcat on which scrutmydocs 0.3.0 is deployed
```

I have configured 1 local FS river "river1-tst" the target is on my local filesystem:
"/products/elasticsearch/logs"

When I try to start the river, elasticsearch logs are showing that:

```
first message:
```

[2014-02-05 15:34:00,510][INFO ][cluster.metadata ] [Professor Power] [river] updatemapping river1-tst

```
then numerous errors like those:
```

"org.elasticsearch.index.mapper.MapperParsingException: No content is provided"

Any idea ? What am I doing wrong ?
Note that the user "appliman" used by tomcat and elasticsearch, has read access on the files located on /products/elasticsearch /logs/

Thanks for your help.
Jérôme

Full trace sample:

n[2014-02-05 15:11:20,669][INFO ][node ] [Agony] stopped\n[2014-02-05 15:11:20,670][INFO ][node ] [Agony] closing ...\n[2014-02-05 15:11:20,694][INFO ][node ] [Agony] closed\n\n\"}]}\norg.elasticsearch.index.mapper.MapperParsingException: No content is provided.\n\tat org.elasticsearch.index.mapper.attachment.AttachmentMapper.parse(AttachmentMapper.java:337)\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:514)\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:456)\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:516)\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:460)\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:374)\n\tat org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:397)\n\tat org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:156)\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n\tat java.lang.Thread.run(Thread.java:680)\n[2014-02-05 15:17:12,122][DEBUG][action.bulk ] [Professor Power] [docs][1] failed to execute bulk item (index) index {[docs][doc][e8c97f233f2183c2f652341de9f7b451], source[{\"file\":{\"filename\":\"elasticsearch_index_indexing_slowlog.log\",\"last_modified\":1391609333000,\"indexing_date\":\"2014-02-05T14:17:10.367Z\",\"content_type\":\"application/octet-stream\",\"url\":\"file:///products/elasticsearch/logs/elasticsearch_index_indexing_slowlog.log\",\"filesize\":0},\"path\":{\"encoded\":\"57ffac5b15768b89c1cc110ac27e1b\",\"root\":\"57ffac5b15768b89c1cc110ac27e1b\",\"virtual\":\"\",\"real\":\"/products/elasticsearch/logs/elasticsearch_index_indexing_slowlog.log\"},\"meta\":{\"author\":null,\"title\":null,\"date\":null,\"keywords\":[]},\"content\":\"\"}]}\norg.elasticsearch.index.mapper.MapperParsingException: No content is provided.\n\tat org.elasticsearch.index.mapper.attachment.AttachmentMapper.parse(AttachmentMapper.java:337)\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.j"}]}
org.elasticsearch.index.mapper.MapperParsingException: No content is provided.
at org.elasticsearch.index.mapper.attachment.AttachmentMapper.parse(AttachmentMapper.java:337)
at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:514)
at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:456)
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:516)
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:460)
at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:374)
at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:397)
at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:156)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
at java.lang.Thread.run(Thread.java:680)
[2014-02-05 15:34:05,465][WARN ][fr.pilato.elasticsearch.river.fs.river.FsRiver] There was failures while executing bulk
</description><key id="27059841">5040</key><summary>org.elasticsearch.index.mapper.MapperParsingException: No content is provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jerome83136</reporter><labels /><created>2014-02-06T15:59:22Z</created><updated>2014-02-06T16:55:55Z</updated><resolved>2014-02-06T16:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-06T16:06:53Z" id="34338172">This is not related to elasticsearch but to scrutmydocs project which is not up to date.
You already opened an issue in the right place there: https://github.com/scrutmydocs/scrutmydocs/issues/76
</comment><comment author="jerome83136" created="2014-02-06T16:55:54Z" id="34344316">Hi David,
OK, I was not sure if it were an elasticsearch or scrutmydocs issue; so I posted on both sides.
Thanks for your help and sorry for the mistake.
Regards
Jérôme
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix BytesRef owning issue in string terms aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5039</link><project id="" key="" /><description>The byte[] array that was used to store the term was owned by the BytesRefHash
which is used to compute counts. However, the BytesRefHash is released at some
point and its content may be recycled.

MockPageCacheRecycler has been improved to expose this issue (putting random
content into the arrays upon release).

Number of documents/terms have been increased in RandomTests to make sure page
recycling occurs.

Close #5021
</description><key id="27053935">5039</key><summary>Fix BytesRef owning issue in string terms aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-06T14:43:31Z</created><updated>2015-06-07T23:35:26Z</updated><resolved>2014-02-10T09:11:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-06T14:48:24Z" id="34329156">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Java API: BulkProcessor does not call afterBulk when bulk throws eg NoNodeAvailableException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5038</link><project id="" key="" /><description>When using a BulkProcessor with setting concurrentRequests &gt; 0 and an exception occurs on row 283 (client.bulk(...)) other than InterruptedException then it is not caught and subsequently the afterBulk function is never called.

It is reproducible by running:
- a transport client without an elasticsearch to connect to
- with the BulkProcessor configured with concurrentRequests &gt; 0
- sending in enough documents so that the bulk is sent

This problem leads to that you only get exception for 1 of the documents in the bulk and there is no way to know that the other documents also failed.
</description><key id="27050377">5038</key><summary>Java API: BulkProcessor does not call afterBulk when bulk throws eg NoNodeAvailableException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">anrask</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-06T13:51:43Z</created><updated>2014-09-10T19:42:07Z</updated><resolved>2014-06-13T15:54:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karlney" created="2014-02-06T13:56:34Z" id="34324583">FYI this bug is related to #4153, #4155 and #4158
</comment><comment author="vorce" created="2014-03-31T08:32:29Z" id="39064730">Any progress on this yet?
</comment><comment author="btiernay" created="2014-05-27T12:50:42Z" id="44270825">I'm trying to implement a workaround for #6314 and this is required. Any update here for a fix?
</comment><comment author="awnixon" created="2014-06-13T14:01:18Z" id="46014365">Hello, 

I'm seeing an issue with the bulk request API where I am sending many bulk requests simultaneously.

When my index reaches a certain size and the amount of memory assigned to each shard is increased (i.e. increasing from 56Mb allocated to 128Mb allocated) the pending bulk requests fail to respond (i.e. no call to 'onResponse' or 'onFailure' in the BulkRequest ActionListener) effectively locking my application from sending any additional requests as the semaphore flags acquired before calling 'execute' are never released.

Could this issue relate to a problem that I am seeing with the bulk request API? 

Thanks,
Andrew
</comment><comment author="javanna" created="2014-06-13T14:20:06Z" id="46016570">Hi @awnixon ,
if you mean the semaphore within `BulkProcessor`, that's now released in a finally block. Which version of elasticsearch are you using though? This reminds me of #4153, it might be your problem if you're using an old version of elasticsearch,
</comment><comment author="awnixon" created="2014-06-13T14:33:38Z" id="46018224">Hi @javanna,

Within my own client application I manage the number of bulk requests which may be sent using a semaphore flag which is acquired before calling execute and released once a response is received.
The issue you referenced does sound similar to my own, however I am using the 2.0.0-SNAPSHOT version (as of the 9th of June). 

Also, I do not see any exceptions coming from the elasticsearch node. Simply the statement that the amount of memory allocated to each shard is to be increased at which point the number of pending requests in my client application steadily increases without any response from the server. I am still able to execute curl -XGet requests (i.e. elastisearch cluster state is reported as green on request).

I hope that helps. 

All the best, 
Andrew 
</comment><comment author="javanna" created="2014-06-13T14:59:17Z" id="46021420">Then the problem can't be in the `BulkProcessor` as you are not using it. I'd suggest to switch to it though. There might be an issue in your client code that handles the semaphore?
</comment><comment author="awnixon" created="2014-06-13T18:47:57Z" id="46046813">Thanks for the suggestion. I've switched to the BulkProcessor and found the
situation to be the same where I set the level of concurrency to &gt; 1.
I believe that I have found the root cause of my issue however. Whilst
using concurrent concurrent bulk requests I'm reaching (or exceeding) the
specified "indices.memory.max_index_buffer_size" (which I've set to 256Mb
during testing). When I increase the "indices.memory.max_index_buffer_size"
value my issue no longer occurs. Would you expect elasticsearch to handle
this situation by rejecting some/all pending requests or should it be
returning to the current set of pending requests once the
"index_buffer_size" for each shard has been updated?

On 13 June 2014 15:59, Luca Cavanna notifications@github.com wrote:

&gt; Then the problem can't be in the BulkProcessor as you are not using it.
&gt; I'd suggest to switch to it though. There might be an issue in your client
&gt; code that handles the semaphore?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/5038#issuecomment-46021420
&gt; .
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkProcessorTests.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file></files><comments><comment>Java API: Make sure afterBulk is always called in BulkProcessor after beforeBulk</comment></comments></commit></commits></item><item><title>How to use the service on Windows with multiple nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5037</link><project id="" key="" /><description>Hi,

I'm using elasticsearch on my windows 2012 server and want to use the included service that starts automatically. I'm having 2 nodes (just leaning about that) but i can't get this to work. When i start each node individually i see all of my nodes on http://localhost:9200/_plugin/head/ but all my command prompts are still visible. Is there a solution for this? 

I'm using elasticsearch for my webshop (5000 products), facets and search. How many nodes, shards and replicas should i use?
</description><key id="27046328">5037</key><summary>How to use the service on Windows with multiple nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Nesse</reporter><labels /><created>2014-02-06T12:39:19Z</created><updated>2014-02-06T13:01:55Z</updated><resolved>2014-02-06T13:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-06T13:00:32Z" id="34320441">Please use the mailing list for questions.
See http://www.elasticsearch.org/help/
</comment><comment author="Nesse" created="2014-02-06T13:01:55Z" id="34320546">I'm sorry, will take a look.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature/4879</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5036</link><project id="" key="" /><description>PR for #4879
</description><key id="27039928">5036</key><summary>Feature/4879</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels /><created>2014-02-06T10:41:18Z</created><updated>2014-06-28T08:59:29Z</updated><resolved>2014-02-20T13:38:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-11T15:51:21Z" id="34767409">I think this looks pretty awesome. I left some comments but in general we are very close here. Lets chat about the dependency and how we handle that...
</comment><comment author="MaineC" created="2014-02-11T15:53:00Z" id="34767609">Thanks a lot for your comments - fixing now. Dependency: Sure.
</comment><comment author="s1monw" created="2014-02-11T16:09:23Z" id="34769505">thank you!
</comment><comment author="spinscale" created="2014-02-13T22:18:52Z" id="35033277">I like the functionality a lot (just what I need it in one of my playground projects :-)

Two minor things that came to my mind while reading the PR:
- IIRC the `DefaultMustacheFactory` uses internally an unbounded guava cache for storing the templates. Does LRU make more sense, exposing stats seems overkill
- The `ScriptService` has the possibility to refresh files on the filesystem? Does this still work due to the above cache for the mustache templates?
</comment><comment author="kimchy" created="2014-02-15T17:13:04Z" id="35161429">Few notes:
- Agreed on mustache being optional dependency, I don't think we should shade it at all. We need to make sure in our packaging (zip, tar.gz, deb, rpm) it is included as part of the lib dir.
- I would love to get to a state where the result of running the script is UTF8 bytes (see for example using a thread local `UTF8StreamWriter`, we do it in `StreamOutput`). That way we don't need to convert to a string, and then parse the json from a string.
</comment><comment author="s1monw" created="2014-02-18T12:28:30Z" id="35379474">this looks great to me we only need to speak about the including of Guava in the package... lets talk soon
</comment><comment author="s1monw" created="2014-02-20T10:01:57Z" id="35604403">I did another review, ran the tests and did a release build to make sure it all works and starts up etc. This looks awesome. I left one more comment (minor) but once that is fixed I am +1 to push
</comment><comment author="MaineC" created="2014-02-20T10:33:39Z" id="35607384">Above commit changes the yaml test from using match_all to using a term query.
</comment><comment author="s1monw" created="2014-02-20T11:00:43Z" id="35609326">awesome! Isabel please squash the commits into one and feel free to push to `1.x` and `master`!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Marvel and Paramedic to config file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5035</link><project id="" key="" /><description /><key id="27035948">5035</key><summary>Add Marvel and Paramedic to config file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2014-02-06T09:29:40Z</created><updated>2014-06-13T09:59:45Z</updated><resolved>2014-02-06T17:16:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-06T17:16:36Z" id="34346604">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrading analysis plugins fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5034</link><project id="" key="" /><description>When an analysis plugins provides default index settings using `PreBuiltAnalyzerProviderFactory`,  `PreBuiltTokenFilterFactoryFactory`, `PreBuiltTokenizerFactoryFactory` or `PreBuiltCharFilterFactoryFactory` it fails when upgrading it with elasticsearch superior or equal to 0.90.5.

Related issue: #4936

Fix is needed in core. But, in the meantime, analysis plugins developers can fix that issue by overloading default prebuilt factories.

For example:

``` java
public class StempelAnalyzerProviderFactory extends PreBuiltAnalyzerProviderFactory {

    private final PreBuiltAnalyzerProvider analyzerProvider;

    public StempelAnalyzerProviderFactory(String name, AnalyzerScope scope, Analyzer analyzer) {
        super(name, scope, analyzer);
        analyzerProvider = new PreBuiltAnalyzerProvider(name, scope, analyzer);
    }

    @Override
    public AnalyzerProvider create(String name, Settings settings) {
        return analyzerProvider;
    }

    public Analyzer analyzer() {
        return analyzerProvider.get();
    }
}
```

And instead of:

``` java
    @Inject
    public PolishIndicesAnalysis(Settings settings, IndicesAnalysisService indicesAnalysisService) {
        super(settings);
        indicesAnalysisService.analyzerProviderFactories().put("polish", new PreBuiltAnalyzerProviderFactory("polish", AnalyzerScope.INDICES, new PolishAnalyzer(Lucene.ANALYZER_VERSION)));
    }
```

do

``` java
    @Inject
    public PolishIndicesAnalysis(Settings settings, IndicesAnalysisService indicesAnalysisService) {
        super(settings);
        indicesAnalysisService.analyzerProviderFactories().put("polish", new StempelAnalyzerProviderFactory("polish", AnalyzerScope.INDICES, new PolishAnalyzer(Lucene.ANALYZER_VERSION)));
    }
```

Closes #5030
</description><key id="27034820">5034</key><summary>Upgrading analysis plugins fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>bug</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-06T09:05:56Z</created><updated>2015-06-07T23:34:31Z</updated><resolved>2014-02-07T11:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-06T09:51:27Z" id="34307257">It looks pretty cool though. now the hard part comes... can you write a test for it? We have the ability to set the index version in the settings so you could write a test that loads a dummy pluging that registers the prebuild stuff and create the index with a mapping and a different index version.

There are many utils for this like `ElasticserachTestCase#randomVersion()` or `ElasticsearchTestCase#getPreviousVersion()`...
To create the index you just need to do this:

```
Settings versionSettings = ImmutableSettings.builder().put(IndexMetaData.SETTING_VERSION_CREATED, randomVersion()).build();
```
</comment><comment author="dadoonet" created="2014-02-06T09:52:27Z" id="34307325">Great! I was wondering how to test it automatically! Going to look at this.
</comment><comment author="dadoonet" created="2014-02-06T16:39:03Z" id="34342431">@s1monw Could you review it please? Tests fails without the patch and works when I apply the patch
</comment><comment author="dadoonet" created="2014-02-06T19:03:53Z" id="34357947">@s1monw PR updated with test and headers updated. I think we are good now.
</comment><comment author="s1monw" created="2014-02-07T11:15:28Z" id="34427584">David this looks great +1 to push 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use the new command line syntax in the init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5033</link><project id="" key="" /><description>Hi,

I've noticed that in the init script provided by the 1.0.0.RC2 Debian package, the daemon is started with pre-1.0 arguments : 

```
DAEMON_OPTS="-d -p $PID_FILE -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR"
```

According to [the documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/_system_and_settings.html) we could use a simpler/shorter invocation : 

```
DAEMON_OPTS="-d -p $PID_FILE --default.config=$CONF_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.work=$WORK_DIR --default.path.conf=$CONF_DIR"
```

It's definitely not a blocker, but it would be nice to be consistent.
</description><key id="27032846">5033</key><summary>Use the new command line syntax in the init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jlecour</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.4.0</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-06T08:17:23Z</created><updated>2015-06-07T16:50:17Z</updated><resolved>2014-10-28T18:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-03-07T12:29:27Z" id="37019793">the new arguments are simply an addition and to make it easier for command-line users, but we will not remove the old style.

unsure if it makes sense to switch to the new ones or to not confuse people, when they upgrade from 0.90 here. What do you think?
</comment><comment author="jlecour" created="2014-03-07T13:03:05Z" id="37021893">If the core team has decided that a the new syntax is better than the old one (and I agree), why would you impose the old one to everybody?

Plus, the old syntax is pre-1.0, so people using this should be aware that anything from a pre-1.0 version is subject to change anytime. And, if someone upgrade but keep her old init script, it would still work.

In a more general point of view, I'm for adopting the best-practices, canonical syntax, … in a project whenever it's introduced. It's a signal that this is the preferred way and people cargo-culting those things at least use the correct form.
</comment><comment author="clintongormley" created="2014-10-20T13:02:20Z" id="59746132">@jlecour Sorry it has taken a while to look at this.

I agree with you. Please could I ask you sign the CLA so that we can get this in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="jlecour" created="2014-10-20T14:46:37Z" id="59768974">:memo: :white_check_mark: 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Use the simpler command line syntax in the Debian init script</comment></comments></commit></commits></item><item><title>Improve documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5032</link><project id="" key="" /><description>The way it was written before I was unsure what the actual config parameter was because of the redundant "multicast" in there. I think it makes more sense to have the "enabled" parameter in the table as well.
</description><key id="27019358">5032</key><summary>Improve documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">lfrancke</reporter><labels /><created>2014-02-06T01:01:39Z</created><updated>2014-06-24T04:35:42Z</updated><resolved>2014-02-06T08:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="RaazTripathi" created="2014-02-06T05:36:37Z" id="34294087">Good inhance 
</comment><comment author="dadoonet" created="2014-02-06T08:33:22Z" id="34302086">Thanks! Closed with 583f148
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] add azure and gce discovery plugins</comment></comments></commit></commits></item><item><title>Startup error with 1.0.0RCx - java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchIllegalArgumentException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5031</link><project id="" key="" /><description>Hi,

while testing with the Chef cookbook and the latest elasticsearch versions 1.0.0RC1 and RC2 I ran into a startup error. I tried openjdk with Version 6 and 7 both failed. The same configurations works, if I use 0.90.11 as version.

Trace:

```
[2014-02-05 18:40:25,247][INFO ][node                     ] [Kiber the Cruel] version[1.0.0.RC2], pid[22688], build[a9d736e/2014-02-03T15:02:11Z]
[2014-02-05 18:40:25,248][INFO ][node                     ] [Kiber the Cruel] initializing ...
[2014-02-05 18:40:25,248][DEBUG][node                     ] [Kiber the Cruel] using home [/usr/local/elasticsearch-1.0.0.RC2], config [/usr/local/elasticsearch-1.0.0.RC2/config], data [[/usr/local/elasticsearch-1.0.0.RC2/data]], logs [/usr/local/elasticsearch-1.0.0.RC2/logs], work [/usr/local/elasticsearch-1.0.0.RC2/work], plugins [/usr/local/elasticsearch-1.0.0.RC2/plugins]
[2014-02-05 18:40:25,254][TRACE][plugins                  ] [Kiber the Cruel] --- adding plugin [/usr/local/elasticsearch-1.0.0.RC2/plugins/cloud-aws]
[2014-02-05 18:40:25,261][TRACE][plugins                  ] [Kiber the Cruel] found a jvm plugin [cloud-aws], [Cloud AWS Plugin]
[2014-02-05 18:40:25,270][INFO ][plugins                  ] [Kiber the Cruel] loaded [cloud-aws], sites []
[2014-02-05 18:40:25,310][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder
[2014-02-05 18:40:25,316][TRACE][env                      ] [Kiber the Cruel] obtaining node lock on /usr/local/elasticsearch-1.0.0.RC2/data/elasticsearch/nodes/0 ...
[2014-02-05 18:40:25,327][DEBUG][env                      ] [Kiber the Cruel] using node location [[/usr/local/elasticsearch-1.0.0.RC2/data/elasticsearch/nodes/0]], local_node_id [0]
[2014-02-05 18:40:25,335][TRACE][env                      ] [Kiber the Cruel] node data locations details:
 -&gt; /usr/local/elasticsearch-1.0.0.RC2/data/elasticsearch/nodes/0, free_space [77gb], usable_space [76.2gb]

[2014-02-05 18:40:26,482][TRACE][monitor.sigar            ] [Kiber the Cruel] sigar loaded successfully
[2014-02-05 18:40:26,615][DEBUG][bootstrap                ] Exception
org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchIllegalArgumentException
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2199)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3938)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4821)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.get(ConstructorInjectorStore.java:50)
        at org.elasticsearch.common.inject.ConstructorBindingImpl.initialize(ConstructorBindingImpl.java:50)
        at org.elasticsearch.common.inject.InjectorImpl.initializeBinding(InjectorImpl.java:372)
        at org.elasticsearch.common.inject.BindingProcessor$1$1.run(BindingProcessor.java:148)
        at org.elasticsearch.common.inject.BindingProcessor.initializeBindings(BindingProcessor.java:204)
        at org.elasticsearch.common.inject.InjectorBuilder.initializeStatically(InjectorBuilder.java:119)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:102)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:187)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:68)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:201)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchIllegalArgumentException
        at java.lang.Class.getDeclaredConstructors0(Native Method)
        at java.lang.Class.privateGetDeclaredConstructors(Class.java:2493)
        at java.lang.Class.getDeclaredConstructors(Class.java:1901)
        at org.elasticsearch.common.inject.spi.InjectionPoint.forConstructorOf(InjectionPoint.java:177)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.createConstructor(ConstructorInjectorStore.java:59)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.access$000(ConstructorInjectorStore.java:29)
...skipping...
org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchIllegalArgumentException
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2199)
        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3934)
        at org.elasticsearch.common.cache.LocalCache.getOrLoad(LocalCache.java:3938)
        at org.elasticsearch.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4821)
        at org.elasticsearch.common.inject.internal.FailableCache.get(FailableCache.java:51)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.get(ConstructorInjectorStore.java:50)
        at org.elasticsearch.common.inject.ConstructorBindingImpl.initialize(ConstructorBindingImpl.java:50)
        at org.elasticsearch.common.inject.InjectorImpl.initializeBinding(InjectorImpl.java:372)
        at org.elasticsearch.common.inject.BindingProcessor$1$1.run(BindingProcessor.java:148)
        at org.elasticsearch.common.inject.BindingProcessor.initializeBindings(BindingProcessor.java:204)
        at org.elasticsearch.common.inject.InjectorBuilder.initializeStatically(InjectorBuilder.java:119)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:102)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
        at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:187)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:68)
        at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:201)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchIllegalArgumentException
        at java.lang.Class.getDeclaredConstructors0(Native Method)
        at java.lang.Class.privateGetDeclaredConstructors(Class.java:2493)
        at java.lang.Class.getDeclaredConstructors(Class.java:1901)
        at org.elasticsearch.common.inject.spi.InjectionPoint.forConstructorOf(InjectionPoint.java:177)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.createConstructor(ConstructorInjectorStore.java:59)
        at org.elasticsearch.common.inject.ConstructorInjectorStore.access$000(ConstructorInjectorStore.java:29)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:37)
        at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:33)
        at org.elasticsearch.common.inject.internal.FailableCache$1.load(FailableCache.java:39)
        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3524)
        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2317)
        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
        ... 19 more
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.ElasticSearchIllegalArgumentException
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 32 more 
```

Right now it looks like there is something wrong in /usr/local/etc/elasticsearch/elasticsearch-env.sh (nothing changed!) and/or the startup script.

I tried to  start ES manually with
`sudo su elasticsearch -c "ES_INCLUDE=/usr/local/etc/elasticsearch/elasticsearch-env.sh /usr/local/bin/elasticsearch -p /usr/local/var/run/elasticsearch/ESTEST_DEV_MASTER_i_f4715cd4.pid`
but it failed with the same error.

Is there any important change in 1.0.0 which might impact the startup procedure/ of the Chef cookbook?

Thanks
</description><key id="27005992">5031</key><summary>Startup error with 1.0.0RCx - java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchIllegalArgumentException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jensihnow</reporter><labels /><created>2014-02-05T22:08:42Z</created><updated>2014-02-09T13:15:51Z</updated><resolved>2014-02-05T22:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-05T22:17:58Z" id="34260504">Upgrade your aws plugin version to 2.0.0.RC1.
It should work.

Look at compatibility matrix: https://github.com/elasticsearch/elasticsearch-cloud-aws/blob/master/README.md
</comment><comment author="jensihnow" created="2014-02-09T13:15:51Z" id="34573438">Worked, Thanks for the hint!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrading analysis plugins fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5030</link><project id="" key="" /><description>When an analysis plugins provides default index settings using `PreBuiltAnalyzerProviderFactory`,  `PreBuiltTokenFilterFactoryFactory`, `PreBuiltTokenizerFactoryFactory` or `PreBuiltCharFilterFactoryFactory` it fails when upgrading it with elasticsearch superior or equal to 0.90.5.

Related issue: #4936 

Fix is needed in core. But, in the meantime, analysis plugins developers can fix that issue by overloading default prebuilt factories.

For example:

``` java
public class StempelAnalyzerProviderFactory extends PreBuiltAnalyzerProviderFactory {

    private final PreBuiltAnalyzerProvider analyzerProvider;

    public StempelAnalyzerProviderFactory(String name, AnalyzerScope scope, Analyzer analyzer) {
        super(name, scope, analyzer);
        analyzerProvider = new PreBuiltAnalyzerProvider(name, scope, analyzer);
    }

    @Override
    public AnalyzerProvider create(String name, Settings settings) {
        return analyzerProvider;
    }

    public Analyzer analyzer() {
        return analyzerProvider.get();
    }
}
```

And instead of:

``` java
    @Inject
    public PolishIndicesAnalysis(Settings settings, IndicesAnalysisService indicesAnalysisService) {
        super(settings);
        indicesAnalysisService.analyzerProviderFactories().put("polish", new PreBuiltAnalyzerProviderFactory("polish", AnalyzerScope.INDICES, new PolishAnalyzer(Lucene.ANALYZER_VERSION)));
    }
```

do 

``` java
    @Inject
    public PolishIndicesAnalysis(Settings settings, IndicesAnalysisService indicesAnalysisService) {
        super(settings);
        indicesAnalysisService.analyzerProviderFactories().put("polish", new StempelAnalyzerProviderFactory("polish", AnalyzerScope.INDICES, new PolishAnalyzer(Lucene.ANALYZER_VERSION)));
    }
```
</description><key id="27005540">5030</key><summary>Upgrading analysis plugins fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-05T22:03:55Z</created><updated>2014-04-29T09:46:14Z</updated><resolved>2014-02-07T11:21:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltCharFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenizerFactoryFactory.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltCharFilters.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenFilters.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenizers.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalysisBinderProcessor.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalysisPlugin.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalyzer.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalyzerProvider.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyCharFilterFactory.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyIndicesAnalysis.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyIndicesAnalysisModule.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyTokenFilterFactory.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyTokenizerFactory.java</file><file>src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationTests.java</file></files><comments><comment>Upgrading analysis plugins fails</comment></comments></commit></commits></item><item><title>Expose lowFreqBoost and highFreqBoost in Common Term Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5029</link><project id="" key="" /><description>I think that it is a good step towards the query being a better stand in for stop words.  Especially if #5024 is merged.
</description><key id="27003384">5029</key><summary>Expose lowFreqBoost and highFreqBoost in Common Term Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Query DSL</label><label>feedback_needed</label></labels><created>2014-02-05T21:45:27Z</created><updated>2015-02-28T05:02:54Z</updated><resolved>2015-02-28T05:02:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T19:20:16Z" id="68070523">Hi @nik9000 

Is this still something you'd like to see?
</comment><comment author="clintongormley" created="2015-02-28T05:02:54Z" id="76510939">No more info provided. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add multi-field support to common term query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5028</link><project id="" key="" /><description>Add support for matching multiple fields to the common term query. Like this:
{ "common": {
  "body": {
    "high_freq_operator": "and",
    "fields": ["title", "body"],
    "query": "nelly the elephant not as a cartoon",
  }
} }
The extra fields come from the "fields" array and the name of the object
containing the query denotes the field from which the common-ness is
derived.

Closes #5024
</description><key id="26998980">5028</key><summary>Add multi-field support to common term query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-05T21:07:29Z</created><updated>2014-06-15T09:19:25Z</updated><resolved>2014-03-05T16:50:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-05T21:08:10Z" id="34245197">WIP needs:
1.  A review on the funky syntax for adding the fields.  Feels bolted on
because it is.
2.  Gotta get the multi match query to use it.
3.  Review of the (smaller than expected) change to Lucene.
4.  Docs.  Including a working rest example.
5.  More tests!?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `explain` flag support to the reroute API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5027</link><project id="" key="" /><description>By specifying the `explain` flag, an explanation for the reason a
command can or cannot be executed is returned. No allocation commands
are actually performed.

Returns a response similar to:

``` json
{
  "state": { ... cluster state ...},
  "acknowledge": true,
  "explanations" : [ {
    "command" : "cancel",
      "parameters" : {
        "index" : "decide",
        "shard" : 0,
        "node" : "IvpoKRdtRiGrQ_WKtt4_4w",
        "allow_primary" : false
      },
      "decisions" : [ {
        "decider" : "cancel_allocation_command",
        "decision" : "YES",
        "explanation" : "..."
        } ]
     }, {
      "command" : "move",
      "parameters" : {
        "index" : "decide",
        "shard" : 0,
        "from_node" : "IvpoKRdtRiGrQ_WKtt4_4w",
        "to_node" : "IvpoKRdtRiGrQ_WKtt4_4w"
       },
       "decisions" : [ {
         "decider" : "same_shard",
         "decision" : "NO",
         "explanation" : "shard cannot be allocated on same node [IvpoKRdtRiGrQ_WKtt4_4w] it already exists on"
       },
       etc, etc, etc
       ]
  }]
}
```

An example of the full output is in #2483

Closes #5169
</description><key id="26995887">5027</key><summary>Add `explain` flag support to the reroute API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-05T20:44:09Z</created><updated>2015-06-07T15:19:58Z</updated><resolved>2014-02-27T17:31:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-19T09:36:28Z" id="35480833">I have a couple of problems with how this at the current stage. Implementation wise the code looks great but I think from my understanding a reroute call with `explain = true` implies `dryRun=true` no matter what you set there. Is that correct? If so I really thing we shouldn't do this. It will confuse the hell out of people if we treat this not as a flag but as a different action. I personally thing we should make explain the default and execute it all the time. so basically we should only have a flag for `dryRun` and always build an explain. if folks are not interested in the response they can just ignore it. If you want to simulate you set `dryRun=true` and you always see what is going on, makes sense? 
</comment><comment author="dakrone" created="2014-02-19T14:23:58Z" id="35503212">Yea, I understand what you're saying. I will work on removing the RoutingAllocation.Result from the cluster state, which should allow me to return the explain results every time, even while executing the commands.
</comment><comment author="dakrone" created="2014-02-21T18:07:55Z" id="35756579">@s1monw okay, I have made a lot of changes, please take another look
- removed AllocationExplanation from cluster state, as per #5169 (the RoutingTable is still part of the state, which is why the entire `Result` object is not removed)
- made the RoutingExplanations part of the `Result` object now
- you can now do `.explain(true)` and execute the reroute commands in addition to explaining (or still do a dry_run with explanations)
- removed the separate .explain() methods on the allocation commands to removed the duplicated code
- added version checks for the changed serialization in the Reroute stuff and the ClusterState
</comment><comment author="s1monw" created="2014-02-26T21:39:06Z" id="36180575">I like this much better I left some comments I think we are close...
</comment><comment author="dakrone" created="2014-02-26T22:05:09Z" id="36183338">@s1monw updated again with the changes you mentioned.
</comment><comment author="s1monw" created="2014-02-27T11:16:16Z" id="36232034">LGTM
</comment><comment author="dakrone" created="2014-02-27T17:31:17Z" id="36267891">Merged to 1.x and master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Radius attribute for circle shape is actually diameter!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5026</link><project id="" key="" /><description>this query

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "geo_shape": {
          "location": {
            "shape": {
              "type": "Circle",
              "coordinates": [
                37.55079142636748,
                55.522330826005124
              ],
              "radius": 20040000
            }
          }
        }
      }
    }
  }
}
```

would't give any error, even diameter of this circle (r*2) &gt; 40075 km (equator length)

but this query

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "geo_shape": {
          "location": {
            "shape": {
              "type": "Circle",
              "coordinates": [
                37.55079142636748,
                55.522330826005124
              ],
              "radius": 40080000
            }
          }
        }
      }
    }
  }
}
```

will

```
InvalidShapeException[distance must be &lt;= 180; got 180.0223829375521];
```

while the actual  radius is &gt; 360

moreover, https://github.com/geojson/geojson-spec/wiki/Proposal---Circles-and-Ellipses-Geoms says that radius default unit is kilometer.
</description><key id="26993236">5026</key><summary>Radius attribute for circle shape is actually diameter!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ogerman</reporter><labels /><created>2014-02-05T20:12:45Z</created><updated>2014-11-05T16:55:05Z</updated><resolved>2014-11-05T16:55:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="scruffyfox" created="2014-05-12T16:52:22Z" id="42857551">+1

Radiuses currently being treated as diameter.
</comment><comment author="ogerman" created="2014-11-05T16:55:05Z" id="61841350">Fixed by #7301 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster API command to rebuild a shard in place</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5025</link><project id="" key="" /><description>It would be useful to have the ability to command a node to rebuild an existing replica shard in place, deleting the local copy and re-initializing it from the master.  The same effect can currently be obtained by moving the replica to another host and moving it back, but rebuilding in place would be simpler.  This probably makes the most sense as a 'rebuild' or 'resync' option under the _cluster/reroute command.

EDIT:  The cancel command can be used to tear down an existing replica and force a rebuild somewhere else in the cluster.  This is not clearly specified in the reference docs.  I suggest rewording the following:
Cancel allocation of a shard (or recovery). Accepts index and shard for index name and shard number, and node for the node to cancel the shard allocation on. It also accepts allow_primary flag to explicitly specify that it is allowed to cancel allocation for a primary shard.
to read:
Cancel allocation of a shard (or recovery). Accepts index and shard for index name and shard number, and node for the node to cancel the shard allocation on. It also accepts allow_primary flag to explicitly specify that it is allowed to cancel allocation for a primary shard.  This can be used to force resynchronization of existing replicas from the primary shard by cancelling them and allowing them to be reinitialized through the standard reallocation process.
</description><key id="26993003">5025</key><summary>Cluster API command to rebuild a shard in place</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels /><created>2014-02-05T20:09:11Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-02-08T01:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Doc fix explaining resynchronization with the Cancel command.</comment></comments></commit></commits></item><item><title>CommonTermsQuery should support querying fields other than its source of commonness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5024</link><project id="" key="" /><description>I'd like to be able to do a CommonTermsQuery against multiple fields.  One field would determine if the term is common and the others would be ORed together.  Something like:

``` js
{ "common": {
  "body": {
    "high_freq_operator": "and",
    "fields": ["title", "body"],
    "query": "nelly the elephant not as a cartoon",
  }
} }
```

would become something like:

``` js
{ "bool": {
  "must": {
    "bool": {
      "must": [
        { "bool": {
          "should": [{ "term": { "title": "nelly"}}, { "term": { "body": "nelly"}}],
          "minimum_should_match": 1
        },
        { "bool": {
          "should": [{ "term": { "title": "elephant"}}, { "term": { "body": "elephant"}}],
          "minimum_should_match": 1
        },
        { "bool": {
          "should": [{ "term": { "title": "cartoon"}}, { "term": { "body": "cartoon"}}],
          "minimum_should_match": 1
        }
      ]
    }
  },
  "should": {
    "bool": {
      "should": [
        { "bool": {
          "should": [{ "term": { "title": "the"}}, { "term": { "body": "the"}}],
          "minimum_should_match": 1
        },
        { "bool": {
          "should": [{ "term": { "title": "not"}}, { "term": { "body": "not"}}],
          "minimum_should_match": 1
        },
        { "bool": {
          "should": [{ "term": { "title": "as"}}, { "term": { "body": "as"}}],
          "minimum_should_match": 1
        },
        { "bool": {
          "should": [{ "term": { "title": "a"}}, { "term": { "body": "a"}}],
          "minimum_should_match": 1
        }
      ]
    }
  }
} }
```

This bug has a Lucene twin: https://issues.apache.org/jira/browse/LUCENE-5435
</description><key id="26986562">5024</key><summary>CommonTermsQuery should support querying fields other than its source of commonness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-05T18:46:02Z</created><updated>2014-03-31T17:26:24Z</updated><resolved>2014-03-31T17:26:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-05T21:39:34Z" id="34251565">this seems very much related to what #5005 does? I think it can support the same behavior?
</comment><comment author="nik9000" created="2014-02-05T21:52:38Z" id="34254711">Wow, that looks like a much larger effort I should have been paying attention to.  I'll set this aside for a while and look there.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] add azure and gce discovery plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5023</link><project id="" key="" /><description>Clean EC2 disco doc
Add Azure disco doc
Add Google Compute Engine doc
</description><key id="26979296">5023</key><summary>[DOCS] add azure and gce discovery plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-05T17:08:41Z</created><updated>2014-07-11T09:32:06Z</updated><resolved>2014-02-06T08:32:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-02-05T19:22:42Z" id="34226917">Other than my comments, looks good
</comment><comment author="dadoonet" created="2014-02-06T08:32:05Z" id="34302003">Pushed in 0.90, 1.0, 1.x and master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sorting highlighting should support tiebreakers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5022</link><project id="" key="" /><description>I've noticed it is possible to get lots of snippets with the same score when highlighting.  For example, if you highlight a multi-valued field.  It'd be nice to be able to break ties with things like the snippet's position in the document or the snippet's length.
</description><key id="26970168">5022</key><summary>Sorting highlighting should support tiebreakers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-05T15:40:16Z</created><updated>2014-03-31T17:27:27Z</updated><resolved>2014-03-31T17:27:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-31T17:27:27Z" id="39116234">Abandoning in favor of getting this Elasticsearch plugin released which has an option to return top scoring fragments in source order which breaks the tie pretty nicely:  https://github.com/nik9000/expiremental-highlighter
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations return different counts when invoked twice in a row</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5021</link><project id="" key="" /><description>Hi,

A couple of days ago I started a thread on the mailing list (https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/c_xLCPOpvjc) about this issue, and the responses on it are slim.

The problem exists in the aggregations api since version 1.0.0.RC1 and is confirmed by me to also occur in 1.0.0.RC2.

The problem is that when you do a terms aggregation on an index sharded in multiple shards (10 in my case) it start to return inconsistent numbers. With this I mean that the numbers are different the second time compared to the first time. You cannot show these numbers to users as when they reload the analytics it shows totally different numbers than before without anything changing to the data.

I created a test suit as a gist for you to recreate the problem your self. It is hosted at: https://gist.github.com/thanodnl/8803745.

But since it contains datafiles it is kind of bugged in the web interface of github. Best you can clone this gist by running: `$ git clone https://gist.github.com/8803745.git`

cd into the newly created directory and run: `$ ./aggsbug.load.sh` to load the test set into your local database. This can take a couple of minutes since it is loading ~1M documents. I tried to recreate it with a smaller set, but then the issue is not appearing.

Once the data is loaded you can run a contained test with: `$ ./aggsbug.test.sh`. This will call the same aggregation twice, store the output, and later print the diff of the output.

If you recreated the bug the output of the test should be something like:

```
$ ./aggsbug.test.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1088  100   950  100   138    192     27  0:00:05  0:00:04  0:00:01   206
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1086  100   948  100   138   2867    417 --:--:-- --:--:-- --:--:--  2872
diff in 2 aggs calls:
2c2
&lt;   "took" : 4918,

---
&gt;   "took" : 325,
18c18
&lt;         "doc_count" : 3599

---
&gt;         "doc_count" : 3228
21c21
&lt;         "doc_count" : 2517

---
&gt;         "doc_count" : 2254
24c24
&lt;         "doc_count" : 2207

---
&gt;         "doc_count" : 2007
27c27
&lt;         "doc_count" : 2207

---
&gt;         "doc_count" : 1971
30c30
&lt;         "doc_count" : 1660

---
&gt;         "doc_count" : 1478
33c33
&lt;         "doc_count" : 1534

---
&gt;         "doc_count" : 1401
36c36
&lt;         "doc_count" : 1468

---
&gt;         "doc_count" : 1330
39c39
&lt;         "doc_count" : 1079

---
&gt;         "doc_count" : 952
```

When ran against 1.0.0.Beta2 the output is what is to be expected:

```
$ ./aggsbug.test.sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1087  100   949  100   138    208     30  0:00:04  0:00:04 --:--:--   208
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1086  100   948  100   138   1525    222 --:--:-- --:--:-- --:--:--  1526
diff in 2 aggs calls:
2c2
&lt;   "took" : 4525,

---
&gt;   "took" : 611,
```

You see the output of the aggs is not occurring in the diff during the test, and the only diff between the two runs is the time it took to calculate the result.
</description><key id="26959460">5021</key><summary>Aggregations return different counts when invoked twice in a row</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">thanodnl</reporter><labels><label>bug</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-05T13:31:09Z</created><updated>2014-02-07T08:53:01Z</updated><resolved>2014-02-07T08:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-05T14:54:59Z" id="34182268">Thanks for reporting this issue, this looks like a bad bug indeed. I'll look into it.
</comment><comment author="jprante" created="2014-02-05T19:30:19Z" id="34227715">More info from what I found.

ES 1.0.0.RC2
Mac OS X 10.8.5
Darwin Jorg-Prantes-MacBook-Pro.local 12.5.0 Darwin Kernel Version 12.5.0: Sun Sep 29 13:33:47 PDT 2013; root:xnu-2050.48.12~1/RELEASE_X86_64 x86_64
java version "1.8.0"
Java(TM) SE Runtime Environment (build 1.8.0-b128)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b69, mixed mode)
G1GC enabled

ES 1.0.0.RC2
RHEL 6.3
Linux zephyros 2.6.32-279.el6.x86_64 #1 SMP Wed Jun 13 18:24:36 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
java version "1.8.0"
Java(TM) SE Runtime Environment (build 1.8.0-b128)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b69, mixed mode)
G1GC enabled

On Mac, counts may change between first and subsequent runs. On the first run, the counts are lower than on the subsequent runs.

On Linux, the effect is more subtle. Counts do not change between runs. But, it seems different shard count lead to deviating entries, on the lower buckets.

Here are two Linux examples, using Nils' data set. First is 10 shards, second is 5 shards, the lower three buckets differ.

shards=10

```
{
  "took" : 143,
  "timed_out" : false,
  "_shards" : {
    "total" : 10,
    "successful" : 10,
    "failed" : 0
  },
  "hits" : {
    "total" : 1060387,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "a" : {
      "buckets" : [ {
        "key" : "totaltrafficbos",
        "doc_count" : 3599
      }, {
        "key" : "mai93thm",
        "doc_count" : 2517
      }, {
        "key" : "mai90thm",
        "doc_count" : 2207
      }, {
        "key" : "mai95thm",
        "doc_count" : 2207
      }, {
        "key" : "totaltrafficnyc",
        "doc_count" : 1660
      }, {
        "key" : "confessions",
        "doc_count" : 1534
      }, {
        "key" : "incidentreports",
        "doc_count" : 1468
      }, {
        "key" : "nji80thm",
        "doc_count" : 1071
      }, {
        "key" : "pai76thm",
        "doc_count" : 1039
      }, {
        "key" : "txi35thm",
        "doc_count" : 357
      } ]
    }
  }
}
```

shards=5

```
{
  "took" : 302,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1060387,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "a" : {
      "buckets" : [ {
        "key" : "totaltrafficbos",
        "doc_count" : 3599
      }, {
        "key" : "mai93thm",
        "doc_count" : 2517
      }, {
        "key" : "mai90thm",
        "doc_count" : 2207
      }, {
        "key" : "mai95thm",
        "doc_count" : 2207
      }, {
        "key" : "totaltrafficnyc",
        "doc_count" : 1660
      }, {
        "key" : "confessions",
        "doc_count" : 1534
      }, {
        "key" : "incidentreports",
        "doc_count" : 1468
      }, {
        "key" : "nji80thm",
        "doc_count" : 1180
      }, {
        "key" : "pai76thm",
        "doc_count" : 936
      }, {
        "key" : "nji78thm",
        "doc_count" : 422
      } ]
    }
  }
}
```
</comment><comment author="jprante" created="2014-02-05T19:54:23Z" id="34230473">I just learned it is already known that the bucket counts differ over shard numbers, also for facets https://github.com/elasticsearch/elasticsearch/issues/1305
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BytesRefHash.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/test/java/org/elasticsearch/cache/recycler/MockPageCacheRecycler.java</file><file>src/test/java/org/elasticsearch/search/aggregations/RandomTests.java</file></files><comments><comment>Fix BytesRef owning issue in string terms aggregations.</comment></comments></commit></commits></item><item><title>RPMs: Add timeout to shutdown with KILL signal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5020</link><project id="" key="" /><description>If the thread pools of an elasticsearch node cannot be shutdown
immediately, a wait of 10 seconds is added. This clashes with the
RPM scripts, as by default the init functions wait for 3 seconds
for a service to shutdown before a KILL signal is sent, resulting
in an unclean shutdown - not from an elasticsearch point of view,
but from init system point of view, as some lock files are left
around.

In order to prevent this the init script as well as the systemd
configuration now feature the same timeout than the debian package,
which is 20 seconds.

The await statement, which causes the 10 second delay can be found in
InternalNode.close()
</description><key id="26955564">5020</key><summary>RPMs: Add timeout to shutdown with KILL signal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Packaging</label><label>bug</label><label>v0.90.11</label><label>v1.0.0</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-05T12:20:58Z</created><updated>2015-06-07T23:35:43Z</updated><resolved>2014-02-05T12:21:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-05T12:21:18Z" id="34160906">this was fixed in d3fb28cd6bee6240ad386f05f75fcc79b266f13d
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add delete cluster setting and throw exception when validation fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5019</link><project id="" key="" /><description>Added `DELETE`, `POST` and modified `PUT` method for "/_cluster/settings"

`DELETE` request will delete transient and persistent settings, there are two parameters `delete_transient` and `delete_persistent` to control delete transient and persistent settings, they are all default to true. It will return the deleted settings

`POST` will add the settings in request to the cluster settings (same as previous `PUT`)

`PUT` will clear the existing settings and add the settings in the request

Also will throw exception when validation for settings fail

cloeses #3670 and #5018
</description><key id="26954220">5019</key><summary>add delete cluster setting and throw exception when validation fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-02-05T11:55:13Z</created><updated>2014-07-04T12:38:56Z</updated><resolved>2014-07-04T12:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jippi" created="2014-02-06T22:22:21Z" id="34380120">:+1: 
</comment><comment author="s1monw" created="2014-04-10T12:07:45Z" id="40070267">hey @kzwang I can't tell you how much I'd love to get this in - I wanted to use it for our test but it's a bit more tricky I think than it looks like. It's not enough to delete the setting we really need to reset to what it's default was. for instance look at `ConcurrentRebalanceAllocatinDecider.java`

``` Java
 class ApplySettings implements NodeSettingsService.Listener {
        @Override
        public void onRefreshSettings(Settings settings) {
            int clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance);
            if (clusterConcurrentRebalance != ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance) {
                logger.info("updating [cluster.routing.allocation.cluster_concurrent_rebalance] from [{}], to [{}]", ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance, clusterConcurrentRebalance);
                ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance = clusterConcurrentRebalance;
            }
        }
    }
```

once the setting was applied it sticks to where it is used and will be reused even if we delete it here is the relevant line:

``` Java
int clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance);
```

so we need to find a way to reset it to the default but I don't have a really good answer how to do that... 
</comment><comment author="kzwang" created="2014-04-10T12:33:30Z" id="40072265">Hi @s1monw,
For those classes, the default value is set in the constructor, such as `this.clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, 2);`
why can't we just reset the setting to that value? It looks like every time the `onRefreshSettings` is called, full settings will be passed, so it the setting is null we should use the default value instead of previous value?
</comment><comment author="s1monw" created="2014-04-10T12:49:28Z" id="40073641">so  I think this can work but it's tricky. What we need to do is this

``` Java
this.clusterConcurrentRebalance = this.defaultClusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, 2);

//... and then

int clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.defaultClusterConcurrentRebalance);
```

since the constant might not be the default that comes from the node setting. I personally think a prerequsite for this is a way to fetch the current value that is used for each of the settings so we can actually test the update / deletes. does this make sense?
</comment><comment author="s1monw" created="2014-04-10T12:52:56Z" id="40073947">one way of doing this could be adding something like this:

``` Java
class ApplySettings implements NodeSettingsService.Listener {

        public void currentSettings(ImmutableSettings.Builder builder) {
            builder.put(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance);
        }
        @Override
        public void onRefreshSettings(Settings settings) {
            int clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance);
            if (clusterConcurrentRebalance != ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance) {
                logger.info("updating [cluster.routing.allocation.cluster_concurrent_rebalance] from [{}], to [{}]", ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance, clusterConcurrentRebalance);
                ConcurrentRebalanceAllocationDecider.this.clusterConcurrentRebalance = clusterConcurrentRebalance;
            }
        }
    }
```

where we can pull the current settings from?
</comment><comment author="kzwang" created="2014-04-10T13:01:35Z" id="40074694">what I'm thinking is the default value in code is just a static field that should never changed, and we'll pass all settings (cluster wide, node setting from config file etc.) to `onRefreshSettings`, so if the setting is null it means user doesn't config that anywhere, so we should use the default static one.
</comment><comment author="s1monw" created="2014-04-10T13:03:12Z" id="40074862">well that is not true - the values might be configured on a node level in the `elasticsearch.yaml` which is what should be used. The default is only if it has never been configured.
</comment><comment author="kzwang" created="2014-04-10T13:09:32Z" id="40076011">``` java
private static final int defaultClusterConcurrentRebalance = 2;

// in constructor
int clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.defaultClusterConcurrentRebalance);

// adn change this method to read both cluster setting and node setting
@Override
        public void onRefreshSettings(Settings settings, Settings nodeSettings) {
            int clusterConcurrentRebalance = settings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, nodeSettings.getAsInt(CLUSTER_ROUTING_ALLOCATION_CLUSTER_CONCURRENT_REBALANCE, ConcurrentRebalanceAllocationDecider.this.defaultClusterConcurrentRebalance));
//...
        }
```

when construct the class, it will read node setting, when setting updated, it will read cluster setting first, if not found, try node setting, if still not found, use the static default one
</comment><comment author="s1monw" created="2014-04-10T13:13:47Z" id="40077244">oh yeah that is a great idea - I like it. If we can pair it with getting the effective value via a GET call that would be awesome!
</comment><comment author="s1monw" created="2014-04-16T20:33:42Z" id="40648584">@kzwang would you be willing to work on this further? If you don't have time we can find somebody to take it from here?
</comment><comment author="kzwang" created="2014-04-16T21:06:11Z" id="40652245">@s1monw I'll work on it within next couple of days
</comment><comment author="s1monw" created="2014-04-16T21:07:08Z" id="40652337">awesome! thanks so much!
</comment><comment author="reedbj" created="2014-04-16T22:22:56Z" id="40659974">I am sure glad to see this is being looked at, a much needed feature - especially with deprecated settings in the mix!
</comment><comment author="clintongormley" created="2014-07-04T12:38:56Z" id="48039360">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update cluster settings doesn't tell when fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5018</link><project id="" key="" /><description>When update cluster settings, if it's not dynamically updateable or fails to validate, it will just been ignored and log on server side, it doesn't tell the client why it's failed
</description><key id="26953933">5018</key><summary>Update cluster settings doesn't tell when fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-02-05T11:48:39Z</created><updated>2014-07-04T12:39:11Z</updated><resolved>2014-07-04T12:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T12:37:32Z" id="48039243">Closed in favour of #6732 
</comment><comment author="clintongormley" created="2014-07-04T12:39:11Z" id="48039376">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Setting "max_expansions" to "0" in a fuzzy query causes a "SearchPhaseExecutionException" on version 1.0.0.RC2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5017</link><project id="" key="" /><description>The query

``` json
{
   "size": 10,
   "query": {
      "fuzzy": {
         "key": {
            "value": "oslo",
            "fuzziness": 1,
            "prefix_length": 1,
            "max_expansions": 0
         }
      }
   }
}
```

returns

``` json
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[yef5AB81QRawVqKl0QwuEg][matcher][0]: QueryPhaseExecutionException[[matcher][0]: query[key:oslo~1],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: RuntimeException[java.lang.NullPointerException]; nested: NullPointerException; }]",
   "status": 500
}
```

while

``` json
{
   "size": 10,
   "query": {
      "fuzzy": {
         "key": {
            "value": "asdfasfsdfasd",
            "fuzziness": 1,
            "prefix_length": 1,
            "max_expansions": 0
         }
      }
   }
}
```

returns

``` json
{
   "took": 6,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}
```

the difference is that the value `oslo` exists in the field `key` in one or more documents while `asdfasfsdfasd` does not. Setting `max_expansions` &gt; 0 does not crash. According to documentation http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/query-dsl-fuzzy-query.html "0" is the default value of this property. In my opinion passing the default value of a property should not cause the query to crash.
</description><key id="26952047">5017</key><summary>Setting "max_expansions" to "0" in a fuzzy query causes a "SearchPhaseExecutionException" on version 1.0.0.RC2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nhhagen</reporter><labels><label>bug</label></labels><created>2014-02-05T11:10:57Z</created><updated>2015-06-07T23:38:43Z</updated><resolved>2014-10-16T20:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T14:27:07Z" id="49880918">Still failing with this stack trace:

```
Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:162)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:261)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:135)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:156)
    ... 7 more
Caused by: java.lang.NullPointerException
    at org.apache.lucene.search.TopTermsRewrite$1.collect(TopTermsRewrite.java:116)
    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:79)
    at org.apache.lucene.search.TopTermsRewrite.rewrite(TopTermsRewrite.java:66)
    at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:288)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:636)
    at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:683)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:132)
    ... 10 more
```
</comment><comment author="rjernst" created="2014-08-05T03:32:19Z" id="51146081">I think this is simply a bug in validation.  0 should not be a valid value (what would that mean?). The fact that it "works" with a non-existent term is a fluke.

@nhhagen, What was your intention with having 0 expansions?
</comment><comment author="nhhagen" created="2014-10-11T08:28:21Z" id="58742510">To be honest I do not remember. Should it even work?
</comment><comment author="clintongormley" created="2014-10-16T10:53:20Z" id="59344658">agreed - validation bug. we shouldn't allow 0
</comment><comment author="rjernst" created="2014-10-16T20:23:18Z" id="59424265">Validation was added in Lucene 4.10 (https://issues.apache.org/jira/browse/LUCENE-5869) so the validation exists in ES 1.4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>regression: Multifield mapping failing on 1.0.0RC1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5016</link><project id="" key="" /><description>When creating A Multi_field mapping on 1.0.0.RC1, the mapping returned is "string", instead of "multi_field". When using 0.90.10, the same curl request work correctly, returning "multi_field" as mapping:

``` json
curl -XDELETE http://localhost:9200/test_index/product/_mapping
curl -XDELETE http://localhost:9200/test_index

curl -XPUT http://localhost:9200/test_index
curl -XPUT http://localhost:9200/test_index/product/_mapping -d '
{
    "product": {
        "properties": {
            "mf_test" : {
                "type" : "multi_field",
                "fields" : {
                    "name" : {"type" : "string", "index" : "analyzed"},
                    "untouched" : {"type" : "string", "index" : "not_analyzed"}
                }
            }
        }
    }
}'
curl -XGET http://localhost:9200/test_index/product/_mapping

 curl -XPUT 'http://localhost:9200/test_index/product/1' -d '
 {
    "id": 1,
    "mf_test" : "anything"
 }'

curl -XGET http://localhost:9200/test_index/product/_mapping


{
    "test_index": {
        "mappings": {
            "product": {
                "properties": {
                    "mf_test": {
                        "type": "string",
                        "index": "no",
                        "fields": {
                            "name": {
                                "type": "string"
                            },
                            "untouched": {
                                "type": "string",
                                "index": "not_analyzed",
                                "norms": {
                                    "enabled": false
                                },
                                "index_options": "docs"
                            }
                        }
                    }
                }
            }
        }
    }
}
```

as you see in the result of the get mapping request the type of "mf_test" is "string" instead of multifield.
</description><key id="26951954">5016</key><summary>regression: Multifield mapping failing on 1.0.0RC1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nahap</reporter><labels /><created>2014-02-05T11:09:29Z</created><updated>2014-02-05T13:19:37Z</updated><resolved>2014-02-05T11:27:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-05T11:27:26Z" id="34157701">Hi

I think it's documented here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/_multi_fields.html

Feel free to reopen if needed.
</comment><comment author="nahap" created="2014-02-05T13:19:37Z" id="34164760">Ok, then i had to reread the documentation, Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make size=0 return all buckets for the geohash_grid aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5015</link><project id="" key="" /><description>Close #4875
</description><key id="26944186">5015</key><summary>Make size=0 return all buckets for the geohash_grid aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-02-05T08:35:50Z</created><updated>2014-07-10T09:48:32Z</updated><resolved>2014-02-07T09:05:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter terms without logical groups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5014</link><project id="" key="" /><description>This query is correct?

``` json
{
    "filter": {
        "terms": {
            "make": [3], 
            "model": [157]
        }
    }
}
```

If so, it does not meet my expectations. Where he got document with condition model = 3?

``` json
{
    "took": 5,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 3,
        "max_score": 1.0,
        "hits": [                        
            {
                "_index": "auto",
                "_type": "cars",
                "_id": "94010_171028",
                "_score": 1.0,
                "_source": {
                    "make": 3,
                    "model": 157
                }
            },
            {
                "_index": "auto",
                "_type": "cars",
                "_id": "98293_144151",
                "_score": 1.0,
                "_source": {
                    "make": 3,
                    "model": 157
                }
            },
            {
                "_index": "auto",
                "_type": "cars",
                "_id": "46275_145865",
                "_score": 1.0,
                "_source": {
                    "make": 21,
                    "model": 3
                }
            }
        ]
    }
}
```

This query returns correct result:

``` json
{
    "filter": {
        "and": {
            "filters": [
                {"terms": {"make": [3]}},
                {"terms": {"model": [157]}}
            ]
        }
    }
}
```

What variant of used query correctly?
</description><key id="26942301">5014</key><summary>Filter terms without logical groups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vlastv</reporter><labels /><created>2014-02-05T07:40:23Z</created><updated>2014-10-12T20:04:02Z</updated><resolved>2014-10-12T20:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-02-09T01:57:14Z" id="34562964">No, this filter is not correct. The terms filter can accept multiple term values but it can only search in a single field. If it encounters multiple fields, it's using all term values to search the last field. In other words your query is equivalent to 

```
{
    "filter": {
        "terms": {
            "model": [3, 157]
        }
    }
}
```

Perhaps, we should change our parser to throw an exception in this case.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/TermsFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/TermsQueryParser.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file></files><comments><comment>Throw parsing exception if terms filter or query has more than one field</comment></comments></commit></commits></item><item><title>Jetty plugin no longer works</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5013</link><project id="" key="" /><description>Trying to use Jetty with ES RC1, and I get the following on startup:

[2014-02-03 17:11:24,674][ERROR][bootstrap ] {1.0.0.RC1}:
Initialization Failed ...
- ExecutionError[java.lang.NoClassDefFoundError:
  org/elasticsearch/ElasticSearchException]
       NoClassDefFoundError[org/elasticsearch/ElasticSearchException]

ClassNotFoundException[org.elasticsearch.ElasticSearchException]

The same works with an earlier version. Is this expected?

Thanks,
Hans
</description><key id="26936700">5013</key><summary>Jetty plugin no longer works</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hglkrijger</reporter><labels /><created>2014-02-05T04:10:46Z</created><updated>2014-03-27T00:14:13Z</updated><resolved>2014-02-05T04:23:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2014-02-05T04:22:28Z" id="34136566">Hi @hglkrijger, it seems like the jetty plugin does not support ES 1.x by now. In 1.x ES moved from `ElasticSearchException` to `ElasticsearchException`. More general ES is now spelled `Elasticsearch`. So this issue is up to the Jetty plugin: https://github.com/sonian/elasticsearch-jetty/issues/43
</comment><comment author="pmusa" created="2014-02-05T14:58:06Z" id="34182612">As ES 1.x is still a release candidate, I think there will be no plugin version for now.
After ES 1.x release, I will make the changes so it can work.

I already requested the "pull request" for 0.90.10 but until now no answer.

You can use the plugin I forked, just send me an email that I will send it
to you.

Let me know if you need anything else.

Regards,
Pablo Musa
</comment><comment author="sbourke" created="2014-03-18T15:46:22Z" id="37948821">Hey guys - ES is now 1.0.X. Any idea about turn around for the fix to the plugin?
</comment><comment author="pmusa" created="2014-03-18T15:53:43Z" id="37949769">While they do not provide one, you can check the following link ;)

http://www.emergi.net/elasticsearch/plugins.html

--Pablo

2014-03-18 12:46 GMT-03:00 Steven notifications@github.com:

&gt; Hey guys - ES is now 1.0.X. Any idea about turn around for the fix to the
&gt; plugin?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5013#issuecomment-37948821
&gt; .
</comment><comment author="RobbieHer" created="2014-03-26T23:11:28Z" id="38752215">Hi, I have a ES 1.0.0 cluster and would like to protect the channel between the cluster nodes via SSL. Can you confirm if the ES Jetty plugin provided either by sonian or emergi.net, encrypts this channel ( which usually happens on ports 9300 to 9400) , or is this plugin solely for encrypting http traffic handled usually by ports 9200 to 9300?
</comment><comment author="uboness" created="2014-03-26T23:21:14Z" id="38752942">The jetty plugin is not maintained by us. better ask all questions related to this plugin at the appropriate project: https://github.com/sonian/elasticsearch-jetty
</comment><comment author="pmusa" created="2014-03-27T00:06:19Z" id="38755775">Uboness is correct, you should use the proper project.

As far as I know only http is supported.

2014-03-26 20:21 GMT-03:00 uboness notifications@github.com:

&gt; The jetty plugin is not maintained by us. better ask all questions related
&gt; to this plugin at the appropriate project:
&gt; https://github.com/sonian/elasticsearch-jetty
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/5013#issuecomment-38752942
&gt; .
</comment><comment author="RobbieHer" created="2014-03-27T00:14:13Z" id="38756262">Thanks. I will follow-up with the appropriate project. 

So far, it looks like only http is supported. A transport layer SSL plugin was written, but was rejected.
https://github.com/elasticsearch/elasticsearch/pull/2105
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add 'explain' option for cluster reroute command to get details feedback</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5012</link><project id="" key="" /><description>We should add an `explain` parameter so people can get detailed feedback about why a shard can or cannot be allocated to a node. Something like this:
### For these commands:

``` sh
curl -XPOST 'localhost:9200/_cluster/reroute?explain&amp;pretty' -d '{
  "commands" : [
    {
      "cancel" : {
        "index" : "decide", "shard" : 0, "node": "IvpoKRdtRiGrQ_WKtt4_4w"
      }
    },
    {
      "move" : {
        "index" : "decide", "shard" : 0,
        "from_node" : "IvpoKRdtRiGrQ_WKtt4_4w", "to_node" : "IvpoKRdtRiGrQ_WKtt4_4w"
      }
    }
  ]
}'
```
### This result:

``` json
{
  "explanations" : [ {
    "command" : "cancel",
    "parameters" : {
      "index" : "[decide]",
      "shard" : 0,
      "node" : "IvpoKRdtRiGrQ_WKtt4_4w",
      "allow_primary" : false
    },
    "decisions" : [ {
      "decider" : "CancelAllocationCommand",
      "decision" : "NO",
      "explanation" : "can't cancel [decide][0] on node [Wysper][IvpoKRdtRiGrQ_WKtt4_4w][Xanadu.local][inet[/172.16.1.8:9300]], shard is primary and started"
    } ]
  }, {
    "command" : "move",
    "parameters" : {
      "index" : "[decide]",
      "shard" : 0,
      "from_node" : "IvpoKRdtRiGrQ_WKtt4_4w",
      "to_node" : "IvpoKRdtRiGrQ_WKtt4_4w"
    },
    "decisions" : [ {
      "decider" : "SameShard",
      "decision" : "NO",
      "explanation" : "shard cannot be allocated on same node [IvpoKRdtRiGrQ_WKtt4_4w] it already exists on"
    }, {
      "decider" : "Filter",
      "decision" : "YES",
      "explanation" : "node passes include/exclude/require filters"
    }, {
      "decider" : "ReplicaAfterPrimaryActive",
      "decision" : "YES",
      "explanation" : "shard is primary"
    }, {
      "decider" : "Throttling",
      "decision" : "YES",
      "explanation" : "below shard recovery limit of [2]"
    }, {
      "decider" : "Enable",
      "decision" : "YES",
      "explanation" : "allocation disabling is ignored"
    }, {
      "decider" : "Disable",
      "decision" : "YES",
      "explanation" : "allocation disabling is ignored"
    }, {
      "decider" : "Awareness",
      "decision" : "YES",
      "explanation" : "no allocation awareness enabled"
    }, {
      "decider" : "ShardsLimit",
      "decision" : "YES",
      "explanation" : "total shard limit disabled: [-1] &lt;= 0"
    }, {
      "decider" : "NodeVersion",
      "decision" : "YES",
      "explanation" : "target node version [2.0.0-SNAPSHOT] is same or newer than source node version [2.0.0-SNAPSHOT]"
    }, {
      "decider" : "DiskThreshold",
      "decision" : "YES",
      "explanation" : "disk threshold decider disabled"
    }, {
      "decider" : "SnapshotInProgress",
      "decision" : "YES",
      "explanation" : "no snapshots are currently running"
    } ]
  } ]
}
```
</description><key id="26910242">5012</key><summary>Add 'explain' option for cluster reroute command to get details feedback</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>feature</label></labels><created>2014-02-04T19:52:37Z</created><updated>2014-02-04T19:54:23Z</updated><resolved>2014-02-04T19:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-02-04T19:54:23Z" id="34099726">Moved to #2483 (closing this as duplicate)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simple Query String: Add `lenient` flag to support *value* parse failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5011</link><project id="" key="" /><description>This is related to the issue: https://github.com/elasticsearch/elasticsearch/issues/1932
</description><key id="26903913">5011</key><summary>Simple Query String: Add `lenient` flag to support *value* parse failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">amitsoni13</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T18:27:59Z</created><updated>2014-02-27T07:09:58Z</updated><resolved>2014-02-26T18:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="amitsoni13" created="2014-02-27T07:09:58Z" id="36216547">Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>refactor SimpleQueryParser settings into separate class, add "lenient" option</comment></comments></commit></commits></item><item><title>Ignore case when parsing `script_values_sorted|unique` in aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5010</link><project id="" key="" /><description>Close #5009
</description><key id="26898236">5010</key><summary>Ignore case when parsing `script_values_sorted|unique` in aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:REST</label><label>bug</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T17:10:31Z</created><updated>2015-06-07T23:39:23Z</updated><resolved>2014-02-06T16:55:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-06T11:00:14Z" id="34312458">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Camel-case support in aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5009</link><project id="" key="" /><description>In #5007 Simon noticed that some options can't be provided in camel case although they should.
</description><key id="26891668">5009</key><summary>Camel-case support in aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T15:55:21Z</created><updated>2014-02-06T11:03:58Z</updated><resolved>2014-02-06T11:03:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-04T16:04:24Z" id="34074720">yeah I guess we should try to keep consistency so +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregatorParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java</file></files><comments><comment>Ignore case when parsing `script_values_sorted|unique` in aggregations.</comment></comments></commit></commits></item><item><title>Simple Query string does not work for prefix query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5008</link><project id="" key="" /><description>The prefix feature of the simple query string does not work.

&lt;pre&gt;
{
  "query" : {
    "simple_query_string" : {
      "query" : "Professiona*",
      "flags" : 24
      }
    }
}
&lt;/pre&gt;


The same query works if using 'query_string' instead of 'simple_query_string' (and removing the 'flags' attribute of course).
</description><key id="26883569">5008</key><summary>Simple Query string does not work for prefix query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">abaxanean</reporter><labels /><created>2014-02-04T14:07:26Z</created><updated>2014-02-14T18:49:32Z</updated><resolved>2014-02-14T18:49:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-02-04T16:23:22Z" id="34076943">Hi @bax1989, I looked into this. This is because the SimpleQueryParser (from Lucene) creates a PrefixQuery for the term before it goes through any kind of analysis. so it is similar to a term query in that the case matters. I was able to reproduce your search with:

``` sh
curl -XPOST 'localhost:9200/sqp/doc/11?refresh' -d'{
  "body": "Professional carpet cleaning"
}'
echo
curl -XPOST 'localhost:9200/sqp/_search?pretty' -d'
{
  "query": {
    "simple_query_string": {
      "query": "Professiona*"
    }
  }
}'
```

And then correctly match it as soon as I switched to using `professiona*` instead of `Professiona*`.

Seeing as how searching for `Professional` seems to work fine (ie, gets analyzed), it would be nice if prefix queries were analyzed, however, since `simple_query_string` is taken from Lucene, it may be trickier to process things beforehand. I will look into it, in the meantime you should be able to lowercase your prefix queries to get them to work.
</comment><comment author="abaxanean" created="2014-02-04T16:31:42Z" id="34077942">Yes, lower-casing the term solves the problem, thank you
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SimpleQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Add support for `lowercase_expanded_terms` flag to simple_query_string</comment></comments></commit></commits></item><item><title>Add script support to value_count aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5007</link><project id="" key="" /><description>The diff on the documentation is simply a revert of ba415b8ad21f57801b14d85c102189cfd7cbbf5a

Close #5001
</description><key id="26881661">5007</key><summary>Add script support to value_count aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T13:35:46Z</created><updated>2015-06-06T18:42:57Z</updated><resolved>2014-02-04T16:01:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-04T13:37:49Z" id="34059448">minor comment but otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix hashCode values of aggregations' BytesValues.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5006</link><project id="" key="" /><description>This commit removes FilterBytesValues which is very trappy as the default
implementation forwards all method calls to the delegate. So if you do any
non-trivial modification to the terms or to the order of the terms, you need
to remember to override currentValueHash, copyShared, and this is very
error-prone.

FieldDataSource.WithScript.BytesValues and ScriptBytesValues now return correct
hash codes, future bugs here would be catched by the new assertion in
SortedUniqueBytesValues.

This bug was causing performance issues with scripts as all terms were assumed
to have the same hash code.

Close #5004
</description><key id="26875006">5006</key><summary>Fix hashCode values of aggregations' BytesValues.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T11:25:21Z</created><updated>2015-06-07T23:40:32Z</updated><resolved>2014-02-04T13:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-04T11:29:59Z" id="34051088">the fix LGTM can we have a test somehow that catches this?
</comment><comment author="jpountz" created="2014-02-04T12:35:36Z" id="34055016">Added tests.
</comment><comment author="s1monw" created="2014-02-04T12:37:23Z" id="34055102">LGTM Thanks for the test
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added `cross_fields` type to multi_match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5005</link><project id="" key="" /><description>`cross_fields` attempts to treat fields with the same analysis
configuration as a single field and uses maximum score promotion or
combination of the scores based depending on the `use_dis_max` setting.
By default scores are combined.

Relates to #2959
</description><key id="26874419">5005</key><summary>Added `cross_fields` type to multi_match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T11:14:05Z</created><updated>2015-06-06T18:43:07Z</updated><resolved>2014-02-06T16:16:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-04T15:53:37Z" id="34073429">I merged in the changes from clinton and updated some minor stuff in the docs. I also added some more test randomization. I think it's ready
</comment><comment author="mattweber" created="2014-02-04T16:18:18Z" id="34076343">+1, so many people on the mailing list have been hit by this "misunderstanding" of the multi-match query.  The doc updates in this PR alone will help considerably!  I love how you group the fields by analyzer without requiring the user to do it by hand (not that it is hard).

Is is going to be in 1.0?  I only see a 1.1 tag.
</comment><comment author="markharwood" created="2014-02-04T16:37:21Z" id="34078671">What is the expected behaviour when (stupidly) querying across a combo of number and text fields? Currently get a NumberFormatException from parser on text search terms. Might user expect e.g. "37 the high street" to work on house number (numeric) and street name (text) fields without error?
</comment><comment author="clintongormley" created="2014-02-04T17:03:48Z" id="34081711">@markharwood if the user specifies an analyzer for the query, then i would have expected this to work, but apparently it doesn't.

```
PUT /t/t/1
{
  "first_name": "will",
  "last_name": "smith",
  "count": 12
}

GET /_validate/query?explain
{
  "query": {
    "multi_match": {
      "query": "will smith 12",
      "fields": [
        "*_name",
        "count"
      ],
      "type": "cross_fields",
      "analyzer": "standard",
      "operator": "and"
    }
  }
}
```

returns an explanation of:

```
+blended("will", fields: [first_name, count, last_name]) 
+blended("smith", fields: [first_name, count, last_name]) 
+blended("12", fields: [first_name, count, last_name])
```

But it doesn't:

```
GET /_search
{
  "query": {
    "multi_match": {
      "query": "will smith 12",
      "fields": [
        "*_name",
        "count"
      ],
      "type": "cross_fields",
      "analyzer": "standard",
      "operator": "and"
    }
  }
}
# no results
```
</comment><comment author="markharwood" created="2014-02-04T17:58:32Z" id="34087703">@clintongormley I've tried specifying 'standard' analyzer and it looks like it doesn't get used because the mapping for integer fields doesn't want to use any analyzer on the input (due to IntegerFieldMapper.useTermQueryWithQueryString returns true). So the query string does not get tokenized for this field and IntegerFieldMapper just barfs on the full string "37 the high st" as an invalid number.
A long winded way of saying "this won't work"...
So we either needed to document the shortcomings or try fix.
</comment><comment author="s1monw" created="2014-02-05T12:12:10Z" id="34160331">@clintongormley @markharwood I just pushed a fix for the numeric problem your test should work now. I will pull this in if nobody objects.
</comment><comment author="s1monw" created="2014-02-05T21:27:37Z" id="34249251">@jpountz I fixed the last two comments and squashed after rebase to master. Can you take a last look?
</comment><comment author="s1monw" created="2014-02-06T08:06:04Z" id="34300502">@mattweber `1.0` is frozen we don't push features into it anymore. As you can guess GA is close so this will only be `1.1` but since we don't plan to wait too long with the next minor release I don't think this is too much of a bummer :)
</comment><comment author="jpountz" created="2014-02-06T09:48:02Z" id="34307004">+1!
</comment><comment author="markharwood" created="2014-02-06T10:34:14Z" id="34310487">@s1monw Just tried this again and was still getting java.lang.NumberFormatException: For input string: "song 2"
running this:

```
"query" :
{
      "multi_match" : {
             "query" : "song 2",
              "fields" : [ "Album", "Ignore2" ],
                "analyzer":"standard",
               "type":"cross_fields"
        },
}
```

This is on the musicbrainz dataset I shared where field "Album" is type string and "Ignore2" is type integer.
</comment><comment author="s1monw" created="2014-02-06T14:58:32Z" id="34330116">@markharwood can you confirm that last commit fixed your issues?
</comment><comment author="markharwood" created="2014-02-06T15:01:07Z" id="34330466">Following latest changes I can confirm that https://github.com/elasticsearch/elasticsearch/pull/5005#issuecomment-34310487 no longer throws exceptions and the logic for checking numeric fields works in my test
</comment><comment author="s1monw" created="2014-02-06T16:16:27Z" id="34339484">pushed to 1.x and master
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/MatchQuery.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Add slop to prefix phrase query after parsing query string</comment></comments></commit></commits></item><item><title>ScriptBytesValues.currentValueHash is wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5004</link><project id="" key="" /><description>`ScriptBytesValues.currentValueHash` doesn't return the hash value of the last returned term. The reason is that it has its own `BytesRef` (`ScriptDocValues.scratch`) to store the term while the hash code is computed on the parent's term (`BytesValues.scratch`).
</description><key id="26873074">5004</key><summary>ScriptBytesValues.currentValueHash is wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T10:49:43Z</created><updated>2014-02-04T13:22:13Z</updated><resolved>2014-02-04T13:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-04T10:50:31Z" id="34048540">Damned! good catch
</comment><comment author="jpountz" created="2014-02-04T11:11:51Z" id="34049924">`FieldDataSource.WithScript.BytesValues` has the same issue.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/FilterBytesValues.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/bytes/ScriptBytesValues.java</file><file>src/test/java/org/elasticsearch/search/aggregations/support/FieldDataSourceTests.java</file></files><comments><comment>Fix hashCode values of aggregations' BytesValues.</comment></comments></commit></commits></item><item><title>Run REST tests against multiple nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5003</link><project id="" key="" /><description>Multiple nodes are now started when running REST tests against the `TestCluster` (default randomized settings are now used instead of the hardcoded `1`)

Added also randomized round-robin based on all available nodes, and ability to provide multiple addresses when running tests against an external cluster to have the same behaviour.

Also sped up a few warmer tests and fixed get_source realtime test which might fail in some cases due to automatic refresh.
</description><key id="26872994">5003</key><summary>Run REST tests against multiple nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T10:48:23Z</created><updated>2014-06-13T16:08:57Z</updated><resolved>2014-02-07T12:56:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-04T12:01:45Z" id="34052973">left one comment otherwise LGTM maybe somebody with more Rest Test readability should look at it?
</comment><comment author="javanna" created="2014-02-06T15:30:49Z" id="34334206">Hey @s1monw I just pushed a new update according to your comments ;)
</comment><comment author="s1monw" created="2014-02-07T12:50:25Z" id="34433863">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add redis transport plugin to plugins page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5002</link><project id="" key="" /><description>Add redis transport link to plugins page

https://github.com/kzwang/elasticsearch-transport-redis
</description><key id="26871775">5002</key><summary>add redis transport plugin to plugins page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-02-04T10:25:13Z</created><updated>2014-06-14T21:04:45Z</updated><resolved>2014-02-06T17:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-06T17:22:29Z" id="34347262">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>value_count aggregations should support scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5001</link><project id="" key="" /><description>See https://groups.google.com/forum/#!topic/elasticsearch/sZ4l7-pkdTw and #4999 for reference.
</description><key id="26868672">5001</key><summary>value_count aggregations should support scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-04T09:24:46Z</created><updated>2014-02-04T16:01:15Z</updated><resolved>2014-02-04T16:01:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountTests.java</file></files><comments><comment>Add script support to value_count aggregations.</comment></comments></commit></commits></item><item><title>Very strange behavior of index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/5000</link><project id="" key="" /><description>1. Have an index template which matches everything and defines under mappings a "_default_" type with some "dynamic_templates", e.g.
   &lt;pre&gt;
   {
   "template_parent": {
     "mapping": {
       "include_in_all": false,
       "index": "not_analyzed"
     },
   "match": "parent_*"
   }
   }
   &lt;/pre&gt;
   Indices, types and their mappings are supposed to be generated dynamically, considering the template.
2. Index items in parallel from multiple threads on multiple indices (one type per index).

Observed : Indices are dynamically generated but the mapping lack from some of them.
e.g Items of type 'A' indexed on index 'a' led to the creation of index 'a', the creation of type 'A' on this index, and properties that starts with 'parent_' not being analyzed.
On the other hand, Item Type 'B' which must have been generated on index 'b' isn't there at all, the documents themselves are there (/B/_count returns the correct number, /B/b/_count returns 0). The index metadata returned(b/_mapping) also reveals that there is no type 'B' on the index, only the '_default_' type. The documents listed via /b/_search have their _type_ attribute equals to 'B'.

Everything is done from Java Client.
Start node, put template, insert items in parallel.
</description><key id="26867135">5000</key><summary>Very strange behavior of index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abaxanean</reporter><labels /><created>2014-02-04T08:50:39Z</created><updated>2014-12-24T19:17:10Z</updated><resolved>2014-12-24T19:17:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abaxanean" created="2014-02-05T14:41:49Z" id="34180350">I switched to using default-mapping.json (having the '_default_' type with 'dynamic_templates' within it).
Anyway, It happens sometimes that, having documents in an index, the index mapping is empty (I mean empty at all  "mappings": {})Performing search on such an index leads to error logs with debug level (e.g. sort by a nonexistent column).
</comment><comment author="javanna" created="2014-02-11T12:53:04Z" id="34751223">Hi @bax1989, can you reproduce this issue? It would be great if you can attach a complete recreation to the issue so that we can reproduce it and understand what causes it.
</comment><comment author="abaxanean" created="2014-03-24T11:25:34Z" id="38433521">I can't give you a concrete scenario since it's a concurrency issue as it seems and does not get reproduced constantly.
</comment><comment author="clintongormley" created="2014-12-24T19:17:10Z" id="68070407">Hi @bax1989 

If you're still seeing this issue, then it should be fixed by https://github.com/elasticsearch/elasticsearch/issues/8688
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOC] Does not support "script" in value_count aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4999</link><project id="" key="" /><description>Currently, value_count aggregation does not support script.
Please delete script section in value_count document.

See: https://groups.google.com/forum/#!topic/elasticsearch/sZ4l7-pkdTw
</description><key id="26854693">4999</key><summary>[DOC] Does not support "script" in value_count aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels /><created>2014-02-04T02:07:10Z</created><updated>2014-07-03T12:54:24Z</updated><resolved>2014-02-04T09:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-04T09:28:40Z" id="34042803">Merged, thanks for fixing the documentation! I also opened #5001 since `value_count` aggregations should support scripts.
</comment><comment author="jpountz" created="2014-02-04T16:02:54Z" id="34074531">@johtani FYI script support will be available in version 1.1
</comment><comment author="johtani" created="2014-02-05T02:42:28Z" id="34132652">Thanks for closing and opening new issue!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to set Index recovery priority</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4998</link><project id="" key="" /><description>When recovering all are part of a cluster it would be nice to have the ability to prioritize which indexes are recovered first.  For example, following the common Index per day pattern, one could envision setting today's index to take priority over all others.   
</description><key id="26846294">4998</key><summary>Allow to set Index recovery priority</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smayzak</reporter><labels /><created>2014-02-03T23:12:09Z</created><updated>2014-12-24T19:15:09Z</updated><resolved>2014-12-24T19:15:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2014-02-10T18:00:19Z" id="34661287">Would love to be able to set this as part of the template too, maybe (although I'm guessing that might be hard to balance against index per day pattern?).
</comment><comment author="robbydyer" created="2014-02-26T21:59:15Z" id="36182689">+1
A large number of indices, along with slow recovery times would make this nice. 
</comment><comment author="faxm0dem" created="2014-07-30T06:01:24Z" id="50576731">this is a duplicate of #6522
</comment><comment author="clintongormley" created="2014-12-24T19:15:09Z" id="68070324">Closing as a duplicate of #6522
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>GeohashGrid aggregation documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4997</link><project id="" key="" /><description>The type string used to identify the GeohashGrid aggregation was [changed](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java#L47) from `geohashgrid` to `geohash_grid` for the release of 1.0.0.RC2, but the [documentation](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/search/aggregations/bucket/geohashgrid-aggregation.asciidoc) has not yet been updated to reflect this change.
</description><key id="26829403">4997</key><summary>GeohashGrid aggregation documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlinn</reporter><labels><label>docs</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-03T19:20:20Z</created><updated>2014-03-21T10:05:38Z</updated><resolved>2014-02-04T08:42:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Rename `geohashgrid` to `geohash_grid` in documentation.</comment></comments></commit></commits></item><item><title>Old node rejoins cluster with really out of date state and revives deleted indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4996</link><project id="" key="" /><description>I have a node crash a few months ago that we left until we could fix the hardware today.  When it rejoined the cluster today it brought with it state for many indexes that had been deleted.  I had to manually re-delete those indexes.  It'd be nice if deleted indexes stayed deleted even if an old node rejoins the cluster.
</description><key id="26822534">4996</key><summary>Old node rejoins cluster with really out of date state and revives deleted indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-02-03T17:57:52Z</created><updated>2014-03-15T01:52:05Z</updated><resolved>2014-02-04T14:34:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-02-03T17:58:44Z" id="33981155">The old node was the cluster master at the time.
The old node was Elasticsearch 0.90.7 and tried to rejoin before we updated it to 0.90.10 which is the version the rest of the cluster is running.
</comment><comment author="kimchy" created="2014-02-03T22:57:08Z" id="34011005">thats intentional behavior, we call it importing of dangled indices. Its there to protect from mistakenly bringing new nodes to the cluster, and having them have a clean state and overriding other nodes joining with existing data. (this can also be protected with recover_after settings). Its better for us to err on the safe side and import those indices.

You do have control over how dangled indices will be handled. You can set `gateway.local.auto_import_dangled` to `yes`, `no`, and `close` (to import them in closed state).
</comment><comment author="nik9000" created="2014-02-04T13:40:43Z" id="34059677">Intended or not, it isn't good behavior to bring indexes back from the dead.  I don't know how to fix it but combining this with the advice in http://www.elasticsearch.org/blog/changing-mapping-with-zero-downtime/ will lead to some weird, weird stuff.  It really doesn't help make a cluster with tons of different indexes easy to maintain.
</comment><comment author="kimchy" created="2014-02-04T13:45:21Z" id="34060046">I strongly disagree. This feature came from users starting a fresh master node (mostly by mistake), and elasticsearch would end up deleting their data because the "empty" node would be elected and assume there is no data in the cluster, and nodes joining will delete their local data. 

Even though elasticsearch can be configured to make sure this doesn't happen (like the recover_after settings), we should be _very_ careful with user data with out of the box settings.

if you set your gateway.recover_after_xxx, and feel comfortable that you will not get to a state where fresh (master) nodes will be provisioned, you can just set `gateway.local.auto_import_dangled` to `no`.
</comment><comment author="nik9000" created="2014-02-04T14:34:51Z" id="34064289">So here is what I saw:
1.  Node dies.
2.  I delete indexes.
3.  Node revives.
4.  Deleted indexes come back in UNASSIGNED state.

That isn't right no matter how you slice it.  Deleted indexes should only come back if I recreate them.  If that is something I have to live with to prevent more destructive surprises then I can but I'll restate that it weakens any argument about Elasticsearch being easy to maintain.

I'm weary to do anything with `gateway.local.auto_import_dangled` because I can't find documentation on it and I'm not really sure what actions would cause it to take effect.  If I have to bring the cluster back up from a power loss or something and the master nodes don't come back in order would that cause the the setting to kick in?
</comment><comment author="kimchy" created="2014-02-04T22:57:34Z" id="34118029">there is no way to protect from erroneous usage of the cluster with out of the box settings, except for importing what we call dangled indices. Or at least, we couldn't come up with one. We have seen it happen with misconfigured clusters. Think of a 10 node cluster, using all defaults, all brought down, then another fresh node is started, elects itself as master, and the other 10 nodes join and delete their local data since there are no indices in the state. This can obviously be mitigated by configuring gateway.recover_after_nodes (or even minimum_master_nodes), but out of the box, we should protect users from it.

You can safely use `gateway.local.auto_import_dangled`, its officially supported, if not documented, then we should, @clintongormley where do you think would be the best place for it?
</comment><comment author="HenleyChiu" created="2014-03-15T01:52:05Z" id="37712435">I have a similar scenario that resulted in ES wiping out my ENTIRE node.

What happened:
1) Node A and Node B are initially connected and each are in sync, and have 10 million documents each.
2) Node B disconnects from the cluster due to some exception, like network timeout or whatever. I worry about split brain, so I delete all the data in this node, and then reconnect it with Node A.
3) Node B thinks Node A is the master or something, and gets the data from Node A, but Node A has NO data.. so Node B is completely wiped out.

Maybe I'm missing something, but this is a nightmare scenario that just happened to me. I now have to reimport everything which is not trivial. This is just 1 of many nightmare scenarios that has occurred since I've used ES.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Documented gateway.local.auto_import_dangled</comment></comments></commit></commits></item><item><title>Ensure that index specific failures do not affect whole request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4995</link><project id="" key="" /><description>Before a bulk request is executed, missing indices are being created by default.
If this fails, the whole request is failed.

This patch changes the behaviour to not fail the whole request, but rather all
requests using the index where the creation has failed.

TODO: Need someone to review if this kind of exception handling is correct, or if there are accidentally too much exceptions being ignored.
TODO: Need someone to review if never failing a bulk request in the `onFailure()` method is ok. I cant think of exceptions, but I think they might exist :-)

Closes #4987
</description><key id="26811221">4995</key><summary>Ensure that index specific failures do not affect whole request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-03T15:32:23Z</created><updated>2015-06-07T23:41:10Z</updated><resolved>2014-03-31T12:40:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-02-03T16:50:31Z" id="33974037">LGTM (left some minor comments). Not sure I understand what you mean by

"TODO: Need someone to review if never failing a bulk request in the onFailure() method is ok. I cant think of exceptions, but I think they might exist :-)"
</comment><comment author="spinscale" created="2014-02-10T11:55:22Z" id="34623444">@bleskes thanks for your input, took most of it and improved the PR

Regarding your understanding question: The question is, if there are special cases, where it is ok the fail the whole BulkRequest, however I do not think, that this is the case, in case we fail, when creating the index...
</comment><comment author="bleskes" created="2014-02-14T09:50:01Z" id="35069875">This line : https://github.com/elasticsearch/elasticsearch/pull/4995/files#diff-636fe548dfdb10121629d49667c6a4b8R188   can cause an IndexMissingRequest and fail the whole request (happens when auto index creation is disabled, or on concurrent bulk/delete index operation). This is not related to the issue mention in the pull request, but does match the spirit of the pull request. I think we want to fix it as well, potentially in another issue.
</comment><comment author="spinscale" created="2014-03-07T09:55:19Z" id="37010012">updated the PR. also throwing an exception now (at two occasions), if we read a different request than Index/Delete/Update...
</comment><comment author="s1monw" created="2014-03-31T12:05:59Z" id="39080685">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove the in memory buffer Lucene store/directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4994</link><project id="" key="" /><description>The in memory byte buffer implementation should be pushed through Lucene to get it working. In any case, we don't recommend using it, and for testing, the ram directory one is good enough.
</description><key id="26806269">4994</key><summary>Remove the in memory buffer Lucene store/directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Store</label><label>breaking</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-03T14:33:15Z</created><updated>2015-06-06T17:07:07Z</updated><resolved>2014-02-03T14:52:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferAllocator.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferDirectory.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferFile.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferFileOutput.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferIndexInput.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/ByteBufferIndexOutput.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/CachingByteBufferAllocator.java</file><file>src/main/java/org/apache/lucene/store/bytebuffer/PlainByteBufferAllocator.java</file><file>src/main/java/org/elasticsearch/cache/NodeCache.java</file><file>src/main/java/org/elasticsearch/cache/NodeCacheModule.java</file><file>src/main/java/org/elasticsearch/cache/memory/ByteBufferCache.java</file><file>src/main/java/org/elasticsearch/index/store/IndexStoreModule.java</file><file>src/main/java/org/elasticsearch/index/store/memory/ByteBufferDirectoryService.java</file><file>src/main/java/org/elasticsearch/index/store/memory/ByteBufferIndexStore.java</file><file>src/main/java/org/elasticsearch/index/store/memory/MemoryIndexStoreModule.java</file><file>src/main/java/org/elasticsearch/index/store/ram/RamDirectoryService.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/test/java/org/elasticsearch/index/store/memory/SimpleByteBufferStoreTests.java</file><file>src/test/java/org/elasticsearch/indices/store/SimpleDistributorTests.java</file><file>src/test/java/org/elasticsearch/test/store/MockDirectoryHelper.java</file><file>src/test/java/org/elasticsearch/test/store/MockRamDirectoryService.java</file></files><comments><comment>Remove the in memory buffer Lucene store/directory</comment><comment>closes #4994</comment></comments></commit></commits></item><item><title>Introducing VersionType.FORCE &amp; VersionType.EXTERNAL_GTE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4993</link><project id="" key="" /><description>Also added "external_gt" as an alias name for VersionType.EXTERNAL , accessible for the rest layer.

Closes #4213 , Closes #2946
</description><key id="26792715">4993</key><summary>Introducing VersionType.FORCE &amp; VersionType.EXTERNAL_GTE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:CRUD</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-03T10:36:01Z</created><updated>2015-06-06T18:46:30Z</updated><resolved>2014-03-10T20:12:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-06T14:10:34Z" id="34325777">I think this looks good - I still would want @kimchy to look at it before we push
</comment><comment author="kimchy" created="2014-03-09T16:54:52Z" id="37131662">LGTM!
</comment><comment author="bleskes" created="2014-03-10T20:12:35Z" id="37228316">Incorporated Simons feedback and pushed to 1.x ( https://github.com/elasticsearch/elasticsearch/commit/4e0e40644d53c701c2a7525dda7179829ac75d6c ) and master (https://github.com/elasticsearch/elasticsearch/commit/b7a95d11a78092696aa9f91793e405bbe397b566)
</comment><comment author="mrkamel" created="2015-03-24T19:43:22Z" id="85664897">I don't find this feature in the docs, especially not within the "optimistic concurrency control" documentation where i'd expect it ... is this still alive?
</comment><comment author="bleskes" created="2015-03-24T21:09:15Z" id="85691335">@mrkamel very much alive and described here: http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>EnhancedPatternLayout for logging.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4992</link><project id="" key="" /><description>For my Logstash Elasticsearch cluster, I use the EnhancedPatternLayout to generate one-liner stack traces from log4j. 

Here is an example log4j.properties (only relevant details included):

```
log4j.appender.SYSLOG=org.apache.log4j.net.SyslogAppender
log4j.appender.SYSLOG.layout=org.apache.log4j.EnhancedPatternLayout
log4j.appender.SYSLOG.layout.ConversionPattern=PROGRAM: [%p] %t/%c{1} - %m %throwable{short}
log4j.appender.SYSLOG.Facility=USER
log4j.appender.SYSLOG.syslogHost=127.0.0.1
```

This is what I would like to do with logging.yml in elasticsearch to retain this functionality:

```
appender:
  syslog:
    type: syslog
    header: true
    syslogHost: 127.0.0.1
    Facility: USER
    layout:
      type: enhancedPattern
      conversionPattern: "elasticsearch: [%p] %t/%c{1} - %m"
```

Reluctantly, this is not currently an option, but I have submitted a pull request to fix this.

https://github.com/elasticsearch/elasticsearch/pull/4991
</description><key id="26782656">4992</key><summary>EnhancedPatternLayout for logging.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">randywallace</reporter><labels /><created>2014-02-03T06:06:04Z</created><updated>2014-12-24T19:14:23Z</updated><resolved>2014-12-24T19:14:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T19:14:22Z" id="68070290">Looks like #4991 has been merged, so I'm closing this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add EnhancedPatternLayout to logging.yml options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4991</link><project id="" key="" /><description /><key id="26782648">4991</key><summary>Add EnhancedPatternLayout to logging.yml options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">randywallace</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-03T06:05:42Z</created><updated>2015-06-07T10:49:27Z</updated><resolved>2014-10-23T11:06:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-03-27T23:53:40Z" id="38875456">LGTM, @aleph-zero can you have a look as well, and pull it in?
</comment><comment author="clintongormley" created="2014-10-20T13:01:04Z" id="59745991">Hi @randywallace 

Sorry it has taken a while to get to this.  Please could I ask you to sign the CLA.
http://www.elasticsearch.org/contributor-agreement/

@colings86 could you review please?
</comment><comment author="randywallace" created="2014-10-20T16:28:41Z" id="59793268">I signed the CLA.
</comment><comment author="clintongormley" created="2014-10-20T16:29:27Z" id="59793377">thanks @randywallace 
</comment><comment author="colings86" created="2014-10-23T11:06:18Z" id="60223252">@randywallace Thanks for the PR. Its now been merged into the 1.x and master branches.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Marvel: wrong URL to ElasticSearch node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4990</link><project id="" key="" /><description>My ES is running behind NGINX in path /es/ over HTTPS. 
If I call /es/_plugin/marvel user interface shows up, but can't get data. 
See screenshot. BTW. ElasticHQ has no issues with that.

The error says to configure the proxy correctly, but where to set the base URL in Marvel?

![bildschirmfoto 2014-02-03 um 01 35 10](https://f.cloud.github.com/assets/2937597/2061806/1c95b408-8c6b-11e3-9961-55605523b355.png)
</description><key id="26775773">4990</key><summary>Marvel: wrong URL to ElasticSearch node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seti123</reporter><labels /><created>2014-02-03T00:34:02Z</created><updated>2014-02-03T11:42:08Z</updated><resolved>2014-02-03T11:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-02-03T11:02:10Z" id="33941862">hi @seti123 ,

We already have a fix for this but it is not released yet. You can work it by changing the following line in `_site/kibana/config.js`:

```
elasticsearch: "http://"+window.location.hostname+(window.location.port !== '' ? ':'+window.location.port : ''),
```

to 

```
elasticsearch: window.location.protocol+"//"+window.location.hostname+(window.location.port !== '' ? ':'+window.location.port : ''),
```

Make sure to do so on all the nodes which which you serve the _site part of marvel.
</comment><comment author="seti123" created="2014-02-03T11:42:08Z" id="33945225">Thx.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multicast discovery is broken when network.host is set an IPv6 address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4989</link><project id="" key="" /><description>At FOSDEM, I was fiddling around with ES 1.0RC1 on my laptop. When you bind explicitly to an IPv6 address, multicast discovery is broken.

When I start the instances, I get the following logs:

```
lieter $ ./bin/elasticsearch
[2014-02-02 16:19:18,501][INFO ][node                     ] [Lorvex] version[1.0.0.RC1], pid[4386], build[c6155c5/2014-01-15T17:02:32Z]
[2014-02-02 16:19:18,506][INFO ][node                     ] [Lorvex] initializing ...
[2014-02-02 16:19:18,507][DEBUG][node                     ] [Lorvex] using home [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1], config [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/config], data [[/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/data]], logs [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/logs], work [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/work], plugins [/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/plugins]
[2014-02-02 16:19:18,523][INFO ][plugins                  ] [Lorvex] loaded [], sites []
[2014-02-02 16:19:18,567][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder
[2014-02-02 16:19:18,592][DEBUG][env                      ] [Lorvex] using node location [[/home/lieter/workspace/elasticsearch/elasticsearch-1.0.0.RC1/data/elasticsearch/nodes/0]], local_node_id [0]
[2014-02-02 16:19:21,168][DEBUG][threadpool               ] [Lorvex] creating thread_pool [generic], type [cached], keep_alive [30s]
[2014-02-02 16:19:21,190][DEBUG][threadpool               ] [Lorvex] creating thread_pool [index], type [fixed], size [2], queue_size [200]
[2014-02-02 16:19:21,197][DEBUG][threadpool               ] [Lorvex] creating thread_pool [bulk], type [fixed], size [2], queue_size [50]
[2014-02-02 16:19:21,198][DEBUG][threadpool               ] [Lorvex] creating thread_pool [get], type [fixed], size [2], queue_size [1k]
[2014-02-02 16:19:21,198][DEBUG][threadpool               ] [Lorvex] creating thread_pool [search], type [fixed], size [6], queue_size [1k]
[2014-02-02 16:19:21,198][DEBUG][threadpool               ] [Lorvex] creating thread_pool [suggest], type [fixed], size [2], queue_size [1k]
[2014-02-02 16:19:21,199][DEBUG][threadpool               ] [Lorvex] creating thread_pool [percolate], type [fixed], size [2], queue_size [1k]
[2014-02-02 16:19:21,199][DEBUG][threadpool               ] [Lorvex] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2014-02-02 16:19:21,201][DEBUG][threadpool               ] [Lorvex] creating thread_pool [flush], type [scaling], min [1], size [1], keep_alive [5m]
[2014-02-02 16:19:21,204][DEBUG][threadpool               ] [Lorvex] creating thread_pool [merge], type [scaling], min [1], size [1], keep_alive [5m]
[2014-02-02 16:19:21,204][DEBUG][threadpool               ] [Lorvex] creating thread_pool [refresh], type [scaling], min [1], size [1], keep_alive [5m]
[2014-02-02 16:19:21,205][DEBUG][threadpool               ] [Lorvex] creating thread_pool [warmer], type [scaling], min [1], size [1], keep_alive [5m]
[2014-02-02 16:19:21,205][DEBUG][threadpool               ] [Lorvex] creating thread_pool [snapshot], type [scaling], min [1], size [1], keep_alive [5m]
[2014-02-02 16:19:21,205][DEBUG][threadpool               ] [Lorvex] creating thread_pool [optimize], type [fixed], size [1], queue_size [null]
[2014-02-02 16:19:21,267][DEBUG][transport.netty          ] [Lorvex] using worker_count[4], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/3/6/1/1], receive_predictor[512kb-&gt;512kb]
[2014-02-02 16:19:21,290][DEBUG][discovery.zen.ping.multicast] [Lorvex] using group [224.2.2.4], with port [54328], ttl [3], and address [null]
[2014-02-02 16:19:21,299][DEBUG][discovery.zen.ping.unicast] [Lorvex] using initial hosts [], with concurrent_connects [10]
[2014-02-02 16:19:21,301][DEBUG][discovery.zen            ] [Lorvex] using ping.timeout [3s], master_election.filter_client [true], master_election.filter_data [false]
[2014-02-02 16:19:21,310][DEBUG][discovery.zen.elect      ] [Lorvex] using minimum_master_nodes [-1]
[2014-02-02 16:19:21,315][DEBUG][discovery.zen.fd         ] [Lorvex] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2014-02-02 16:19:21,343][DEBUG][discovery.zen.fd         ] [Lorvex] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2014-02-02 16:19:21,465][DEBUG][monitor.jvm              ] [Lorvex] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{old=GcThreshold{name='old', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, young=GcThreshold{name='young', warnThreshold=1000, infoThreshold=700, debugThreshold=400}}]
[2014-02-02 16:19:21,992][DEBUG][monitor.os               ] [Lorvex] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@1d3302a5] with refresh_interval [1s]
[2014-02-02 16:19:22,003][DEBUG][monitor.process          ] [Lorvex] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@7dc2445f] with refresh_interval [1s]
[2014-02-02 16:19:22,013][DEBUG][monitor.jvm              ] [Lorvex] Using refresh_interval [1s]
[2014-02-02 16:19:22,021][DEBUG][monitor.network          ] [Lorvex] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@8a97164] with refresh_interval [5s]
[2014-02-02 16:19:22,041][DEBUG][monitor.network          ] [Lorvex] net_info
host [lieters-klaptop]
wlan0   display_name [wlan0]
        address [/2001:67c:1810:f051:222:fbff:fec9:1314%3] [/fe80:0:0:0:222:fbff:fec9:1314%3] 
        mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
lo  display_name [lo]
        address [/0:0:0:0:0:0:0:1%1] [/127.0.0.1] 
        mtu [65536] multicast [false] ptp [false] loopback [true] up [true] virtual [false]

[2014-02-02 16:19:22,075][DEBUG][monitor.fs               ] [Lorvex] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@16c0e304] with refresh_interval [1s]
[2014-02-02 16:19:22,946][DEBUG][indices.store            ] [Lorvex] using indices.store.throttle.type [MERGE], with index.store.throttle.max_bytes_per_sec [20mb]
[2014-02-02 16:19:22,969][DEBUG][cache.memory             ] [Lorvex] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]
[2014-02-02 16:19:23,005][DEBUG][script                   ] [Lorvex] using script cache with max_size [500], expire [null]
[2014-02-02 16:19:23,026][DEBUG][cluster.routing.allocation.decider] [Lorvex] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2014-02-02 16:19:23,027][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2014-02-02 16:19:23,035][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster_concurrent_rebalance] with [2]
[2014-02-02 16:19:23,047][DEBUG][gateway.local            ] [Lorvex] using initial_shards [quorum], list_timeout [30s]
[2014-02-02 16:19:23,353][DEBUG][indices.recovery         ] [Lorvex] using max_bytes_per_sec[20mb], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2014-02-02 16:19:23,420][DEBUG][http.netty               ] [Lorvex] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb-&gt;512kb]
[2014-02-02 16:19:23,437][DEBUG][indices.memory           ] [Lorvex] using index_buffer_size [100.7mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]
[2014-02-02 16:19:23,444][DEBUG][indices.cache.filter     ] [Lorvex] using [node] weighted filter cache with size [20%], actual_size [201.4mb], expire [null], clean_interval [1m]
[2014-02-02 16:19:23,453][DEBUG][indices.fielddata.cache  ] [Lorvex] using size [-1] [-1b], expire [null]
[2014-02-02 16:19:23,492][DEBUG][gateway.local.state.meta ] [Lorvex] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
[2014-02-02 16:19:23,512][DEBUG][gateway.local.state.meta ] [Lorvex] took 19ms to load state
[2014-02-02 16:19:23,513][DEBUG][gateway.local.state.shards] [Lorvex] took 0s to load started shards state
[2014-02-02 16:19:23,519][DEBUG][bulk.udp                 ] [Lorvex] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]
[2014-02-02 16:19:23,539][DEBUG][cluster.routing.allocation.decider] [Lorvex] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2014-02-02 16:19:23,542][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2014-02-02 16:19:23,542][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster_concurrent_rebalance] with [2]
[2014-02-02 16:19:23,544][DEBUG][cluster.routing.allocation.decider] [Lorvex] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2014-02-02 16:19:23,544][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2014-02-02 16:19:23,545][DEBUG][cluster.routing.allocation.decider] [Lorvex] using [cluster_concurrent_rebalance] with [2]
[2014-02-02 16:19:23,578][INFO ][node                     ] [Lorvex] initialized
[2014-02-02 16:19:23,579][INFO ][node                     ] [Lorvex] starting ...
[2014-02-02 16:19:23,612][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500
[2014-02-02 16:19:23,612][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false
[2014-02-02 16:19:23,853][DEBUG][transport.netty          ] [Lorvex] Bound to address [/2001:67c:1810:f051:222:fbff:fec9:1314:9300]
[2014-02-02 16:19:23,854][INFO ][transport                ] [Lorvex] bound_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]}, publish_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]}
[2014-02-02 16:19:23,932][DEBUG][discovery.zen.ping.multicast] [Lorvex] failed to send multicast ping request
java.io.IOException: Network is unreachable
    at java.net.PlainDatagramSocketImpl.send(Native Method)
    at java.net.DatagramSocket.send(DatagramSocket.java:676)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.sendPingRequest(MulticastZenPing.java:274)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.ping(MulticastZenPing.java:237)
    at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:149)
    at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:127)
    at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:677)
    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:297)
    at org.elasticsearch.discovery.zen.ZenDiscovery.access$500(ZenDiscovery.java:75)
    at org.elasticsearch.discovery.zen.ZenDiscovery$1.run(ZenDiscovery.java:282)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-02-02 16:19:25,446][DEBUG][discovery.zen.ping.multicast] [Lorvex] failed to send multicast ping request
java.io.IOException: Network is unreachable
    at java.net.PlainDatagramSocketImpl.send(Native Method)
    at java.net.DatagramSocket.send(DatagramSocket.java:676)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.sendPingRequest(MulticastZenPing.java:274)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing.access$100(MulticastZenPing.java:61)
    at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$3.run(MulticastZenPing.java:244)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-02-02 16:19:26,958][DEBUG][discovery.zen            ] [Lorvex] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2014-02-02 16:19:26,970][DEBUG][cluster.service          ] [Lorvex] processing [zen-disco-join (elected_as_master)]: execute
[2014-02-02 16:19:26,972][DEBUG][cluster.service          ] [Lorvex] cluster state updated, version [1], source [zen-disco-join (elected_as_master)]
[2014-02-02 16:19:26,974][INFO ][cluster.service          ] [Lorvex] new_master [Lorvex][m6rOwyS2QKeIhpQNLptJcA][lieters-klaptop][inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]], reason: zen-disco-join (elected_as_master)
[2014-02-02 16:19:27,043][DEBUG][transport.netty          ] [Lorvex] connected to node [[Lorvex][m6rOwyS2QKeIhpQNLptJcA][lieters-klaptop][inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9300]]]
[2014-02-02 16:19:27,044][DEBUG][cluster.service          ] [Lorvex] publishing cluster state version 1
[2014-02-02 16:19:27,044][DEBUG][cluster.service          ] [Lorvex] set local cluster state to version 1
[2014-02-02 16:19:27,047][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: execute
[2014-02-02 16:19:27,047][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: no change in cluster_state
[2014-02-02 16:19:27,048][DEBUG][cluster.service          ] [Lorvex] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state (version: 1)
[2014-02-02 16:19:27,048][INFO ][discovery                ] [Lorvex] elasticsearch/m6rOwyS2QKeIhpQNLptJcA
[2014-02-02 16:19:27,066][DEBUG][cluster.service          ] [Lorvex] processing [local-gateway-elected-state]: execute
[2014-02-02 16:19:27,078][DEBUG][cluster.service          ] [Lorvex] cluster state updated, version [2], source [local-gateway-elected-state]
[2014-02-02 16:19:27,078][DEBUG][cluster.service          ] [Lorvex] publishing cluster state version 2
[2014-02-02 16:19:27,079][INFO ][http                     ] [Lorvex] bound_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9200]}, publish_address {inet[/2001:67c:1810:f051:222:fbff:fec9:1314:9200]}
[2014-02-02 16:19:27,083][DEBUG][cluster.service          ] [Lorvex] set local cluster state to version 2
[2014-02-02 16:19:27,090][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: execute
[2014-02-02 16:19:27,091][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: no change in cluster_state
[2014-02-02 16:19:27,102][INFO ][gateway                  ] [Lorvex] recovered [0] indices into cluster_state
[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] processing [local-gateway-elected-state]: done applying updated cluster_state (version: 2)
[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] processing [updating local node id]: execute
[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] cluster state updated, version [3], source [updating local node id]
[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] publishing cluster state version 3
[2014-02-02 16:19:27,103][DEBUG][cluster.service          ] [Lorvex] set local cluster state to version 3
[2014-02-02 16:19:27,104][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: execute
[2014-02-02 16:19:27,104][DEBUG][river.cluster            ] [Lorvex] processing [reroute_rivers_node_changed]: no change in cluster_state
[2014-02-02 16:19:27,104][DEBUG][cluster.service          ] [Lorvex] processing [updating local node id]: done applying updated cluster_state (version: 3)
[2014-02-02 16:19:27,104][INFO ][node                     ] [Lorvex] started
[2014-02-02 16:19:37,047][DEBUG][cluster.service          ] [Lorvex] processing [routing-table-updater]: execute
[2014-02-02 16:19:37,048][DEBUG][cluster.service          ] [Lorvex] processing [routing-table-updater]: no change in cluster_state
^C[2014-02-02 16:19:43,925][INFO ][node                     ] [Lorvex] stopping ...
[2014-02-02 16:19:43,961][INFO ][node                     ] [Lorvex] stopped
[2014-02-02 16:19:43,961][INFO ][node                     ] [Lorvex] closing ...
[2014-02-02 16:19:43,971][INFO ][node                     ] [Lorvex] closed
```

The configuration is as follows:

```
lieter $ egrep -v '^#|^$' config/elasticsearch.yml
network.host: 2001:67c:1810:f051:222:fbff:fec9:1314
```

But it also fails if you set `network.host` to `::1`.
</description><key id="26762868">4989</key><summary>Multicast discovery is broken when network.host is set an IPv6 address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pieterlexis</reporter><labels><label>discuss</label></labels><created>2014-02-02T15:28:05Z</created><updated>2015-08-18T19:36:41Z</updated><resolved>2015-08-18T19:36:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-03T14:33:35Z" id="33958920">I just tried on my local machine and this worked fine. According to https://bugs.launchpad.net/ubuntu/+source/sun-java6/+bug/486215 this may be due to a bug in Sun's Java 6 and can be worked around by setting `net.ipv6.bindv6only` to `0`. Could you check if that fixes the issue?
</comment><comment author="pieterlexis" created="2014-02-03T15:09:55Z" id="33963344">Hi,

I'm running OpenJDK JRE 7:

```
$ java -version                  
java version "1.7.0_51"
OpenJDK Runtime Environment (IcedTea 2.4.5) (7u51-2.4.5-1)
OpenJDK 64-Bit Server VM (build 24.51-b03, mixed mode)
```

And that var is already set to 0:

```
$ sudo sysctl net.ipv6.bindv6only
net.ipv6.bindv6only = 0
```

What OS/Java are you running?
</comment><comment author="dakrone" created="2015-08-18T19:36:41Z" id="132327951">I believe this is fixed by https://github.com/elastic/elasticsearch/pull/12942
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix confusing sentence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4988</link><project id="" key="" /><description>The original sentence didn't make much sense. I hope this is a bit better. Taken heavy inspiration from https://github.com/elasticsearch/elasticsearch/commit/c63d8c4fb566882bd4655ddc7dec13b4e8719244
</description><key id="26762635">4988</key><summary>Fix confusing sentence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lfrancke</reporter><labels /><created>2014-02-02T15:14:46Z</created><updated>2014-07-16T21:48:55Z</updated><resolved>2014-02-03T16:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-03T16:29:46Z" id="33971609">Merged.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Bulk operation throws exception on invalid index name </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4987</link><project id="" key="" /><description>When doing a bulk operation, if one of the items holds an invalid index name, the operation returns a top level error (HTTP error in the _bulk API or an exception in case of the Java API).
It's expected it will return the error as part of the bulk result, as done for other types of errors. The error should be returned for the specific item and not fail the entire operation. 

Example:   

```
    curl -XPOST "http://localhost:9200/_bulk" -d'
    { "index" : { "_index" : "INVALID.NAME", "_type" : "type1", "_id" : "1"} }
    { "field1" : "value1" }
    '
```

Returns:

```
{
   "error": "InvalidIndexNameException[[INVALID.NAME] Invalid index name [INVALID.NAME], must be lowercase]",
   "status": 400
}
```
</description><key id="26761754">4987</key><summary>Bulk operation throws exception on invalid index name </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rore</reporter><labels><label>bug</label><label>v1.1.1</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-02T14:24:37Z</created><updated>2014-06-12T17:00:13Z</updated><resolved>2014-03-31T12:39:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file></files><comments><comment>Bulk API: Ensure that specific failures do not affect whole request</comment></comments></commit></commits></item><item><title>Support externalValue() in mappers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4986</link><project id="" key="" /><description>Some mappers do not support externalValue() to be set. So plugin developers can't use it while building their own mappers.

Support added in this PR for:
- `BinaryFieldMapper`
- `BooleanFieldMapper`
- `GeoPointFieldMapper`
- `GeoShapeFieldMapper`
</description><key id="26757384">4986</key><summary>Support externalValue() in mappers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-02T08:47:31Z</created><updated>2015-06-07T15:44:20Z</updated><resolved>2014-03-14T15:28:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-06T14:07:22Z" id="34325510">@s1monw PR updated based on your comments. Let me know.
</comment><comment author="s1monw" created="2014-02-06T17:30:58Z" id="34348156">LGTM thanks
</comment><comment author="dadoonet" created="2014-02-06T18:32:42Z" id="34354506">Ha sorry @simonw! That explains a lot why @s1monw never get my `alerts` :-) Thanks!
@s1monw I did add some tests. Could you review it?

I think I should perhaps add more random and may be test for other externalValue (String, Numbers, Date...). WDYT?
</comment><comment author="s1monw" created="2014-02-28T09:46:47Z" id="36335315">left some comments but other than that looks good
</comment><comment author="kzwang" created="2014-03-13T22:32:34Z" id="37595169">Hi @dadoonet @s1monw , do you have an ETA for this? Currently I have to add field to doc manually for binary field in plugin as a work around, it would be good to be able to use externalValue() for binary field as well
</comment><comment author="dadoonet" created="2014-03-14T15:29:10Z" id="37659611">@kzwang Pushed. Will be in elasticsearch 1.1.0.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalIndexModule.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalMapperPlugin.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/ExternalValuesMapperIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/externalvalues/RegisterExternalTypes.java</file></files><comments><comment>Support externalValue() in mappers</comment></comments></commit></commits></item><item><title>Add fuzzy/slop support to `simple_query_string`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4985</link><project id="" key="" /><description>Ports the change from https://issues.apache.org/jira/browse/LUCENE-5410, which allows `~N` after terms for fuzzy queries and `~N` after phrases for queries with slop.
</description><key id="26755561">4985</key><summary>Add fuzzy/slop support to `simple_query_string`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-02T06:03:00Z</created><updated>2015-06-06T18:43:19Z</updated><resolved>2014-02-06T17:30:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-02-06T08:10:59Z" id="34300801">LGTM - just to make sure this goes into `1.1` &amp;  `2.0` only right? And is this different to what is in lucene right now, ie. can we just drop the X classes once `4.7` is released?
</comment><comment author="dakrone" created="2014-02-06T16:24:43Z" id="34340778">This is an exact copy of what will be in Lucene 4.7, so once that's released we can just drop the X like you said.
</comment><comment author="dakrone" created="2014-02-06T17:30:41Z" id="34348113">Merged to master in d2078a5e287c56cdf37ea4f2183290359a65b0da and 1.x in 113b9a15
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Check plugin Lucene version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4984</link><project id="" key="" /><description>Check that a plugin is Lucene compatible with the current running node using `lucene` property in `es-plugin.properties` file.
- If plugin does not provide `lucene` property, we consider that the plugin is compatible.
- If plugin provides `lucene` property, we try to load related Enum org.apache.lucene.util.Version. If this fails, it means that the node is too "old" comparing to the Lucene version the plugin was built for.
- We compare then two first digits of current node lucene version against two first digits of plugin Lucene version. If not equal, it means that the plugin is too "old" for the current node.

Plugin developers who wants to launch plugin check only have to add a `lucene` property in `es-plugin.properties` file. If you are using maven to build your plugin, you can do it like this:

In `pom.xml`:

``` xml
    &lt;properties&gt;
        &lt;lucene.version&gt;4.6.0&lt;/lucene.version&gt;
    &lt;/properties&gt;

    &lt;build&gt;
        &lt;resources&gt;
            &lt;resource&gt;
                &lt;directory&gt;src/main/resources&lt;/directory&gt;
                &lt;filtering&gt;true&lt;/filtering&gt;
            &lt;/resource&gt;
        &lt;/resources&gt;
    &lt;/build&gt;
```

In `es-plugin.properties`, add:

``` properties
lucene=${lucene.version}
```

BTW, if you don't already have it, you can add the plugin version as well:

``` properties
version=${project.version}
```

You can disable that check using `plugins.check_lucene: false`.

Related to https://github.com/elasticsearch/elasticsearch-analysis-smartcn/pull/13.
</description><key id="26747399">4984</key><summary>Check plugin Lucene version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-02-01T21:16:45Z</created><updated>2015-06-07T15:44:28Z</updated><resolved>2014-05-16T12:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-01T21:23:28Z" id="33883823">@s1monw I moved the check lucene code from https://github.com/elasticsearch/elasticsearch-analysis-smartcn/pull/13 to elasticsearch core code.

It's now applied as soon as `es-plugin.properties` contains a `lucene` property.

WDYT?
</comment><comment author="dadoonet" created="2014-03-17T08:00:10Z" id="37791846">@s1monw I did rebase all my work on master as we had some changes in plugin isolation. WDYT?
cc @costin 
</comment><comment author="s1monw" created="2014-04-14T20:44:44Z" id="40415792">This looks good to me @costin do you have objections?
</comment><comment author="costin" created="2014-04-14T20:50:09Z" id="40416363">The `PluginUtils` has been removed (as with the entire functionality regarding plugin isolation). Unless we want to bring back the class, the PR needs updating.
</comment><comment author="dadoonet" created="2014-04-14T21:52:51Z" id="40422945">Ha! Sure... The first time I did this PR, `PluginUtils` did not exist. Then it was created so I updated the PR and now I guess I need to rollback! :) 

Thanks @costin and @s1monw for the review. Will try to update and merge it this week.
</comment><comment author="kimchy" created="2014-05-07T16:21:01Z" id="42448852">I left some comments, this check will mean that we are strict, and the same Lucene version needs to be defined, right? 
</comment><comment author="dadoonet" created="2014-05-07T16:48:16Z" id="42452212">@kimchy Almost true. Actually, we check only for the first two digits of Lucene version.

So a node running Lucene 4.8.1 will allow a plugin built with Lucene 4.8.0. Opposite is true as well.
But a node running Lucene 4.7.1 will refuse to load a plugin built with Lucene 4.8.0. A node running Lucene 4.9.0 will refuse to load a plugin built with Lucene 4.8.0.
</comment><comment author="s1monw" created="2014-05-13T11:08:24Z" id="42942268">can we have a test for this? It seems like there is no test in this feature 
</comment><comment author="s1monw" created="2014-05-14T19:11:43Z" id="43124186">@dadoonet any news on this or should we push it out to `1.3`?
</comment><comment author="dadoonet" created="2014-05-15T11:15:14Z" id="43197533">@s1monw You are absolutely right. While adding tests I found an issue in my code!
Going to update the PR with fix and tests.
</comment><comment author="s1monw" created="2014-05-15T11:44:28Z" id="43199679">I removed the review label.. put it back once you have an update
</comment><comment author="dadoonet" created="2014-05-15T16:18:59Z" id="43231497">@s1monw I added tests. Still need to run the full test suite though but you can start reviewing any time.
</comment><comment author="dadoonet" created="2014-05-15T19:03:48Z" id="43251472">Tests fail in PluginManagerTests. Need to check and fix.
</comment><comment author="s1monw" created="2014-05-16T10:03:49Z" id="43315964">left a minor comment here nad there - other than that LGTM
</comment><comment author="s1monw" created="2014-05-16T11:07:39Z" id="43320208">LGTM +1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: Add Child Aggregators to limit parent's bucket meta-data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4983</link><project id="" key="" /><description>Having the ability for a sub-aggregate to work off of buckets from the parent aggregate would be helpful in limiting the amount of information that is returned in results.  Currently, the only way to filter a result set by a count of aggregation is to use a has_child query with a score_type of "sum" and apply a custom scoring routine which filters scores of a certain amount.

I propose adding a child-aggregate, which can be placed anywhere a sub-aggregate can be placed, but the child only has access to the parent aggregate's buckets instead of access to the documents of the query.

Example:  given a flat log of recorded user actions, where each log document is parented to a user document, find users who had 3 or more failed logins (in a given time period).

```
{
   "min_score": 0,
   "query": {
      "function_score": {
         "boost_mode": "multiply",
         "functions": [
            {
               "script_score": {
                  "params": {
                     "cutoff": 2
                  },
                  "script": "_score &lt; cutoff ? -1 : 1"
               }
            }
         ],
         "query": {
            "has_child": {
               "type": "user_actions",
               "score_type": "sum",
               "query": {
                  "term": {"log_action": "login_failed"  }
                  }
               }
            }
         }
}}
```

gives...

```
{
  "hits" : {
    "total" : 2,
    "max_score" : 4,
    "hits" : [ {
      "_index" : "test",
      "_type" : "user",
      "_id" : "100",
      "_score" : 3
    }, {
      "_index" : "test",
      "_type" : "user",
      "_id" : "150",
      "_score" : 2
    } ]
  }
}
```

whereas an aggregate like this

```
{
    "query": { 
        "match_all": {}
    },
    "aggs": { 
        "unique_users": { 
            "terms": {"field":"email"},
            "aggs": { 
                "action_totals": { 
                    "terms": { 
                        "field": "log_action"
                    }
                } 
            } 
        } 
    } 
}
```

would yield too much information in the results...

```
  "aggregations" : {
    "unique_users" : {
      "buckets" : [ {
        "key" : "user1@example.org",
        "doc_count" : 24,
        "action_totals" : {
          "buckets" : [ {
            "key" : "login",
            "doc_count" : 18
          }, {
            "key" : "failed_login",
            "doc_count" : 5
          }, {
            "key" : "change_password",
            "doc_count" : 1
          } ]
        }
      }, {
        "key" : "user2@example.org",
        "doc_count" : 14,
        "action_totals" : {
          "buckets" : [ {
            "key" : "login",
            "doc_count" : 11
          }, {
            "key" : "failed_login",
            "doc_count" : 3
          } ]
        }
      },
```

Adding the ability for child-aggs to work off their parent meta-data

```
{
    "query": { 
        "match_all": {}
    },
    "aggs": { 
        "unique_users": { 
            "terms": {"field":"email"},
            "aggs": { 
                "action_totals": { 
                    "terms": { 
                        "field": "log_action"
                    },
                    "child-aggs": {
                        "limiting_range": {
                            "range": {
                                "field": "doc_count",
                                "range": [
                                {"from":3}
                                ]
                            }   
                        },
                        "limiting_action": {
                            "term": {
                                "key": "failed_login"
                            }   
                        }
                    }
                } 
            } 
        } 
    } 
}
```

Would limit the results to only the interested data

```
  "aggregations" : {
    "unique_users" : {
      "buckets" : [ {
        "key" : "user1@example.org",
        "doc_count" : 5,
        "action_totals" : {
          "buckets" : [ {
            "key" : "failed_login",
            "doc_count" : 5
          } ]
        }
      }, {
        "key" : "user2@example.org",
        "doc_count" : 3,
        "action_totals" : {
          "buckets" : [ {
            "key" : "failed_login",
            "doc_count" : 3
          } ]
        }
      },
```

This new way of aggregating and limiting does not require a parent-child relationship between the user and the log document types and only requires searching the log type documents to produce the desired results.

Note that the original top-level aggregations have new doc_count values.  Child-aggregates can be thought to be only limiting actions - never creating new buckets (as that is the role of sub-aggregates already).  Their output can replace the parent aggregate's entire result.

Limitations:  child-aggregates must only be run after all information in the parent aggregate has been collected.  Any aggregate that has a limiting child-aggregate would not be allowed to have sub-aggregates, because changing the parent's meta-information would cause discontinuity with the sub-aggregation buckets.
</description><key id="26741817">4983</key><summary>Aggregations: Add Child Aggregators to limit parent's bucket meta-data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">markkimsal</reporter><labels /><created>2014-02-01T16:27:13Z</created><updated>2014-10-17T06:36:50Z</updated><resolved>2014-10-17T06:36:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-03T15:57:17Z" id="33968197">I was trying to understand your use-case, and I think the following query would help?

``` javascript
{
    "aggs": {
        "failed_logins": {
            "filter": {
                "term": {
                    "log_action": "failed_login"
                }
            },
            "aggs": {
                "unique_users": {
                    "terms": {
                        "field": "email",
                        "min_doc_count": 3
                    }
                }
            }
        }
    }
}
```

More generally, `limiting_range` looks similar to the `min_doc_count` option of the `terms` and `histogram` aggregations while `limiting_action` looks similar to the `include/exclude` option of terms aggregations?
</comment><comment author="markkimsal" created="2014-02-03T20:13:12Z" id="33994822">Yes, you are correct, the limiting_action agg can be rewritten as a term filter, and the min_doc_count would satisfy the basic requirements of this use case.  I was unaware of min_doc_count, thanks for pointing that out. (Is there a max_doc_count?)  I was trying to construct an example that would show the flexibility of having "child aggregators" and their ability to work off of parent buckets, but perhaps the concept is not needed. (?)

I think the problem would come in when you want to find 3 failed_login actions with no password_change action.  It doesn't seem possible without first aggregating all actions by a userid/email and then analyzing the buckets.
</comment><comment author="jpountz" created="2014-02-04T09:15:30Z" id="34041965">&gt;  I was unaware of min_doc_count, thanks for pointing that out. (Is there a max_doc_count?)

This option is pretty new, Its main purpose is to replace the old `all_terms` option of terms facets with something more generic (`all_terms=true` with facets is equivalent to `min_doc_count=0` with aggregations). However, there is no `max_doc_count`.

There is one option of terms aggregations that allows to work on sub-aggregations, which is [sorting by sub-aggregation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-terms-aggregation.html#_order), maybe this could partially solve your problem?

&gt; 3 failed_login actions with no password_change action

This one is actually very hard because document granularity doesn't work: aggregations do filtering based on documents, not field values. We would somehow need a relationship between users and log_actions to do that. `nested` documents could work but are probably too costly indexing-wise since you would need to update a user document every-time a new event comes in. parent/child relationships on the other hand are not supported by aggregations (yet?).
</comment><comment author="amarkanth1984" created="2014-03-18T06:24:02Z" id="37902937">First of all, great job on the new 1.0.1 release. I am trying to use the "Aggregation" framework to solve an issue similar to http://stackoverflow.com/questions/22029796/how-do-i-use-doc-count-in-an-aggregations-range-query-in-elasticsearch-1-0

Using the aggregation framework, i can get the counts per user (but) didn't find a way to use these aggregated counts in nested aggregation(s) inorder to classify into "low/medium/high".

Reading through the docs, i found "min_doc_count" and was wishing that there was something like "max_doc_count" which would then let me filter my aggregated results.  How easy/hard is to support this ?
Also, can you please recommend any other alternative approaches.
</comment><comment author="bobstir" created="2014-04-06T10:33:59Z" id="39664161">Here's a use-case where doc_count / min_doc_count would not suffice.

I have a bunch of "users" as parent docs who have currency "transactions" as child documents.

I need to know the top 10 terms (on users.tags) who have spent the most money (a sum on transactions.amount_paid). Note, there are different products at different prices so doc_count is not adequate (100 txns at $1 &lt; 1 txn at $1000)

If there is currently a way to do this w/ the existing aggregations, please please do tell.

Thanks!

[EDIT] Aggregations do rock though!
</comment><comment author="avleen" created="2014-06-11T01:00:42Z" id="45690905">We're really interested in something like this too.
We want to run a terms aggregation, to select all of the terms in an index (the result will be a few tens of thousands of terms).
Then we want to know the frequency with which they occur, just as a histogram would tell us.
So we want to take the doc_count of the terms, and then histogram how they're distributed.

We can fake it for now, by working out the histogram function in our code, but having ES be able to do this for us would be cool
</comment><comment author="jpountz" created="2014-06-12T10:33:42Z" id="45870962">@avleen Are you just interested in the distribution of frequencies or would you also want to be able to have sub aggregations or know the matching terms on each bucket of the histogram?

The reason why I am asking is that aggregations have been designed to perform everything in a streaming way: there is a single pass on the data, no matter how many levels of aggregations you have. This is important for two reasons: speed and memory usage since there is nothing to buffer but what is needed to build the final result. So as long as your buckets are based on properties of documents, everything is ok, but if buckets start being the result of another aggregation, then it will require buffering information and running another aggregation on the buffered data. Although building buckets based on term frequencies would raise this issue, I think we could have a metric aggregation that would provide information about frequencies of terms using eg. a count-min sketch. Would that address your issue?
</comment><comment author="avleen" created="2014-06-23T16:31:29Z" id="46868534">Hi Adrien,

In this case we really needed to know the frequency distribution. Could
this be treated as another aggregation?
Build the final result and then run an aggregation on that?

Sounds like it would still end up buffering data but I suspect it's a
useful feature to have. Frequency distribution is a pretty common analytics
tool :)
On Jun 12, 2014 4:33 AM, "Adrien Grand" notifications@github.com wrote:

&gt; @avleen https://github.com/avleen Are you just interested in the
&gt; distribution of frequencies or would you also want to be able to have sub
&gt; aggregations or know the matching terms on each bucket of the histogram?
&gt; 
&gt; The reason why I am asking is that aggregations have been designed to
&gt; perform everything in a streaming way: there is a single pass on the data,
&gt; no matter how many levels of aggregations you have. This is important for
&gt; two reasons: speed and memory usage since there is nothing to buffer but
&gt; what is needed to build the final result. So as long as your buckets are
&gt; based on properties of documents, everything is ok, but if buckets start
&gt; being the result of another aggregation, then it will require buffering
&gt; information and running another aggregation on the buffered data. Although
&gt; building buckets based on term frequencies would raise this issue, I think
&gt; we could have a metric aggregation that would provide information about
&gt; frequencies of terms using eg. a count-min sketch. Would that address your
&gt; issue?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/4983#issuecomment-45870962
&gt; .
</comment><comment author="jimhorng" created="2014-06-25T03:44:34Z" id="47057680">Want the so-called "child aggregation" feature +1
My scenario state as below: http://stackoverflow.com/questions/24380588/elasticsearch-aggregation-over-metric-aggregation-result-values
</comment><comment author="avleen" created="2014-07-09T18:29:49Z" id="48514894">I found another use case for passing forward the doc_count:
I want to use `date_histogram` to count the number of events per second in a given time range.
Then I want to get stats on the doc_count of the matches. Sum, avg, max, min, etc.
</comment><comment author="jsnod" created="2014-07-15T22:51:26Z" id="49103458">I've just come across the exact scenario described by @avleen above.  I'm trying to get the average # of occurrences of a certain event for each user per month using the following: 

``` json
"aggs": {
    "user": {
      "terms": {
        "field": "identity"
      },
      "aggs": {
        "months": {
          "date_histogram": {
            "field": "time",
            "interval": "month"
          }
        }
      }
    }
  }
```

This works great to get doc counts per user/month, but what I'd like to do next is get the average # of docs across each user's `months` buckets.  This doesn't appear possible at the moment since we can't aggregate the `doc_count` values for each monthly bucket.  Ideally I'd be able to add a final `avg` agg using `user.months.doc_count` as the field like so:

``` json
"aggs": {
    "user": {
      "terms": {
        "field": "identity"
      },
      "aggs": {
        "months": {
          "date_histogram": {
            "field": "time",
            "interval": "month"
          }
        },
        "aggs": {
          "average_docs": {
            "avg": {
              "field": "doc_count"
            }
          }
        }
      }
    }
  }
```

Is there some other way?

I understand the reasoning for this not being available as outlined by @jpountz above but this seems like such a fundamental need for analysis that it may be worth considering.
</comment><comment author="jimhorng" created="2014-07-16T00:21:57Z" id="49109895">@afx114 
My very workaround for getting average of "doc_count" is produce intermediate documents which have "doc_count" as it's field and generate result from it.
In your scenario, I'l do below steps. 

Step 1: get aggregation from query

``` javascript
"aggs": {
    "user": {
      "terms": {
        "field": "identity"
      },
      "aggs": {
        "months": {
          "date_histogram": {
            "field": "time",
            "interval": "month"
          }
        }
      }
    }
}
```

Step 2: Transform to intermediate result as document and post back a temporarily index type called: "identity histogram"

``` javascript
{
    "identity": "user1",
    "identity_count": 12,
    "time": "2014-07-16"
}
{
    "identity": "user1",
    "identity_count": 10,
    "time": "2014-06-16"
}
{
    "identity": "user2",
    "identity_count": 15,
    "time": "2014-06-16"
}
...
```

Step 3: Query from type:"identity histogram" and get average

``` javascript
"aggs": {
    "months": {
        "term": {
            "field": "time"
        },
        "aggs": {
            "average_docs": {
                "avg": {
                  "field": "identity_count"
                }
            }
        }
    }
}
```

Hope this helps.
</comment><comment author="rore" created="2014-07-27T14:08:07Z" id="50265237">+1 on this. We have the exact case as @avleen describes. We want to aggregate events to get histogram of events/sec, and then perform further aggregations on the results of the histogram.
This seems to be a very basic need to anyone who collects events and needs some kind of BI on that.
</comment><comment author="benqua" created="2014-08-14T11:58:06Z" id="52174401">+1, same need as @rore here!
</comment><comment author="clintongormley" created="2014-10-17T06:36:50Z" id="59472022">Closing in favour of #8110
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add elasticsearch-osem to integrations page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4980</link><project id="" key="" /><description /><key id="26733318">4980</key><summary>add elasticsearch-osem to integrations page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kzwang</reporter><labels /><created>2014-02-01T06:46:57Z</created><updated>2014-06-18T07:45:16Z</updated><resolved>2014-02-13T10:04:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T10:04:26Z" id="34963916">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Parent / child queries should work with non-default similarities</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4979</link><project id="" key="" /><description>Closes #4977
</description><key id="26723206">4979</key><summary>Parent / child queries should work with non-default similarities</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gpstathis</reporter><labels><label>:Parent/Child</label><label>bug</label><label>v0.90.11</label><label>v1.0.0</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T23:19:39Z</created><updated>2015-06-07T23:42:07Z</updated><resolved>2014-02-02T08:29:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-02-01T00:02:34Z" id="33855250">@gpstathis Thanks for fixing this! I'll pull this change in soon.
</comment><comment author="martijnvg" created="2014-02-02T08:29:50Z" id="33895119">Pushed to master, 1.x, 1.0 and 0.90. Thanks @gpstathis! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>sysctl: permission denied on key 'vm.max_map_count'  - OpenVZ Elasticsearch 0.90.9 compatibility issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4978</link><project id="" key="" /><description>Hello,

Elasticsearch 0.90.9 and higher do not work with openvz (tested with debian wheezy i386) with the following error showing on startup:

Starting ElasticSearch Server:sysctl: permission denied on key 'vm.max_map_count'  
</description><key id="26721868">4978</key><summary>sysctl: permission denied on key 'vm.max_map_count'  - OpenVZ Elasticsearch 0.90.9 compatibility issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">bline79</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2014-01-31T22:52:05Z</created><updated>2016-12-29T14:56:37Z</updated><resolved>2016-11-06T11:53:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-03T09:47:17Z" id="33936392">Hey,

can you please check, if the process started anyway? It should have been. The error is just a permission problem due to your VM, but should not prevent elasticsearch from starting usually.
</comment><comment author="dmikhaylov" created="2014-02-24T13:57:18Z" id="35888002">I've got the same issue, and on my VM it's not starting.
</comment><comment author="spinscale" created="2014-02-24T14:01:22Z" id="35888336">can you add `set -x` to the elasticsearch init script and paste the output from `/etc/init.d/elasticsearch start` here?
</comment><comment author="dmikhaylov" created="2014-02-24T14:13:35Z" id="35889330">do you mean `/etc/init.d/elasticsearch start set -x`?
</comment><comment author="spinscale" created="2014-02-24T14:55:26Z" id="35893476">sorry for not being clear. You can do the following: Open `/etc/init.d/elasticsearch` in your favourite editor and change the current setup

```
#!/bin/sh
#
```

to

```
#!/bin/sh
set -x
#
```

Then run `/etc/init.d/elasticsearch start` and copy paste all that output into this ticket. You might want to comment out or remove that line again after that, so you do not get his verbose output all the time.

Thanks a lot for helping!
</comment><comment author="dmikhaylov" created="2014-02-24T16:49:29Z" id="35906856">- PATH=/bin:/usr/bin:/sbin:/usr/sbin
- NAME=elasticsearch
- DESC='ElasticSearch Server'
- DEFAULT=/etc/default/elasticsearch
  ++ id -u
- '[' 0 -ne 0 ']'
- . /lib/lsb/init-functions
  ++ FANCYTTY=
  ++ '[' -e /etc/lsb-base-logging.sh ']'
  ++ . /etc/lsb-base-logging.sh
  +++ LOG_DAEMON_MSG=
- '[' -r /etc/default/rcS ']'
- . /etc/default/rcS
  ++ TMPTIME=0
  ++ SULOGIN=no
  ++ DELAYLOGIN=no
  ++ UTC=yes
  ++ VERBOSE=no
  ++ FSCKFIX=no
- ES_USER=elasticsearch
- ES_GROUP=elasticsearch
- JDK_DIRS='/usr/lib/jvm/java-7-oracle /usr/lib/jvm/java-7-openjdk /usr/lib/jvm/java-7-openjdk-amd64/ /usr/lib/jvm/java-7-openjdk-armhf /usr/lib/jvm/java-7-openjdk-i386/ /usr/lib/jvm/java-6-sun /usr/lib/jvm/java-6-openjdk /usr/lib/jvm/java-6-openjdk-amd64 /usr/lib/jvm/java-6-openjdk-armhf /usr/lib/jvm/java-6-openjdk-i386 /usr/lib/jvm/default-java'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-7-oracle/bin/java -a -z '' ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-7-openjdk/bin/java -a -z '' ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-7-openjdk-amd64//bin/java -a -z '' ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-7-openjdk-armhf/bin/java -a -z '' ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-7-openjdk-i386//bin/java -a -z '' ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-6-sun/bin/java -a -z '' ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-6-openjdk/bin/java -a -z '' ']'
- JAVA_HOME=/usr/lib/jvm/java-6-openjdk
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-6-openjdk-amd64/bin/java -a -z /usr/lib/jvm/java-6-openjdk ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-6-openjdk-armhf/bin/java -a -z /usr/lib/jvm/java-6-openjdk ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/java-6-openjdk-i386/bin/java -a -z /usr/lib/jvm/java-6-openjdk ']'
- for jdir in '$JDK_DIRS'
- '[' -r /usr/lib/jvm/default-java/bin/java -a -z /usr/lib/jvm/java-6-openjdk ']'
- export JAVA_HOME
- ES_HOME=/usr/share/elasticsearch
- MAX_OPEN_FILES=65535
- LOG_DIR=/var/log/elasticsearch
- DATA_DIR=/var/lib/elasticsearch
- WORK_DIR=/tmp/elasticsearch
- CONF_DIR=/etc/elasticsearch
- CONF_FILE=/etc/elasticsearch/elasticsearch.yml
- MAX_MAP_COUNT=65535
- '[' -f /etc/default/elasticsearch ']'
- . /etc/default/elasticsearch
- PID_FILE=/var/run/elasticsearch.pid
- DAEMON=/usr/share/elasticsearch/bin/elasticsearch
- DAEMON_OPTS='-p /var/run/elasticsearch.pid -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch'
- export ES_HEAP_SIZE
- export ES_HEAP_NEWSIZE
- export ES_DIRECT_SIZE
- export ES_JAVA_OPTS
- test -x /usr/share/elasticsearch/bin/elasticsearch
- case "$1" in
- checkJava
- '[' -x /usr/lib/jvm/java-6-openjdk/bin/java ']'
- JAVA=/usr/lib/jvm/java-6-openjdk/bin/java
- '[' '!' -x /usr/lib/jvm/java-6-openjdk/bin/java ']'
- '[' -n '' -a -z '' ']'
- log_daemon_msg 'Starting ElasticSearch Server'
- '[' -z 'Starting ElasticSearch Server' ']'
- log_use_fancy_output
- TPUT=/usr/bin/tput
- EXPR=/usr/bin/expr
- '[' -t 1 ']'
- '[' xxterm '!=' x ']'
- '[' xxterm '!=' xdumb ']'
- '[' -x /usr/bin/tput ']'
- '[' -x /usr/bin/expr ']'
- /usr/bin/tput hpa 60
- /usr/bin/tput setaf 1
- '[' -z ']'
- FANCYTTY=1
- case "$FANCYTTY" in
- true
- /usr/bin/tput xenl
  ++ /usr/bin/tput cols
- COLS=80
- '[' 80 ']'
- '[' 80 -gt 6 ']'
  ++ /usr/bin/expr 80 - 7
- COL=73
- log_use_plymouth
- '[' n = y ']'
- plymouth --ping
- printf ' \* Starting ElasticSearch Server       '
  - Starting ElasticSearch Server       ++ /usr/bin/expr 80 - 1
- /usr/bin/tput hpa 79
                                                                             + printf ' '
  ++ pidofproc -p /var/run/elasticsearch.pid elasticsearch
  ++ local pidfile line i pids= status specified pid
  ++ pidfile=
  ++ specified=
  ++ OPTIND=1
  ++ getopts p: opt
  ++ case "$opt" in
  ++ pidfile=/var/run/elasticsearch.pid
  ++ specified=1
  ++ getopts p: opt
  ++ shift 2
  ++ base=elasticsearch
  ++ '[' '!' 1 ']'
  ++ '[' -n /var/run/elasticsearch.pid -a -r /var/run/elasticsearch.pid ']'
  ++ read pid
  ++ '[' -n 16987 ']'
  +++ kill -0 16987
  ++ ps 16987
  ++ return 1
- pid=
- '[' -n '' ']'
- mkdir -p /var/log/elasticsearch /var/lib/elasticsearch /tmp/elasticsearch
- chown elasticsearch:elasticsearch /var/log/elasticsearch /var/lib/elasticsearch /tmp/elasticsearch
- touch /var/run/elasticsearch.pid
- chown elasticsearch:elasticsearch /var/run/elasticsearch.pid
- '[' -n 65535 ']'
- ulimit -n 65535
- '[' -n '' ']'
- '[' -n 65535 ']'
- sysctl -q -w vm.max_map_count=65535
  error: permission denied on key 'vm.max_map_count'
- start-stop-daemon --start -b --user elasticsearch -c elasticsearch --pidfile /var/run/elasticsearch.pid --exec /usr/share/elasticsearch/bin/elasticsearch -- -p /var/run/elasticsearch.pid -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch
- log_end_msg 0
- '[' -z 0 ']'
- '[' 73 ']'
- '[' -x /usr/bin/tput ']'
- log_use_plymouth
- '[' n = y ']'
- plymouth --ping
- printf '\r'
- /usr/bin/tput hpa 73
                                                                       + '[' 0 -eq 0 ']'
- echo '[ OK ]'
  [ OK ]
- return 0
- exit 0
</comment><comment author="spinscale" created="2014-02-24T17:21:49Z" id="35910784">can you run `ps p $(cat /var/run/elasticsearch.pid)` - this actually looks as if elasticsearch was started...
</comment><comment author="dmikhaylov" created="2014-02-25T13:22:36Z" id="36006301">Returns just header, i've tried `service elasticsearch status`  and it says `* elasticsearch is not running`.
</comment><comment author="spinscale" created="2014-02-25T14:29:27Z" id="36012223">Can you make sure elasticsearch does not run, and try this on the commandline (as root!):

```
touch /var/run/elasticsearch.pid
chown elasticsearch:elasticsearch /var/run/elasticsearch.pid
start-stop-daemon -v --start --user elasticsearch -c elasticsearch --pidfile /var/run/elasticsearch.pid --exec /usr/share/elasticsearch/bin/elasticsearch -- -p /var/run/elasticsearch.pid -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch
```

and paste the output here?
</comment><comment author="dmikhaylov" created="2014-02-25T17:24:38Z" id="36033656">{0.90.8}: Initialization Failed ...
- ElasticSearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [/var/data/elasticsearch/hostname]]
  IOException[failed to obtain lock on /var/data/elasticsearch/hostname/nodes/49]
      IOException[Cannot create directory: /var/data/elasticsearch/hostname/nodes/49]
  Turns out I just forgot to create `/var/data` directory. Now everything works. Thank you, @spinscale.
</comment><comment author="spinscale" created="2014-02-26T07:32:07Z" id="36098526">@empirik One last question: Did this message also occur in the log file at `/var/log/elasticsearch` or was it just printed to stdout? Would be awesome if you could check it!
</comment><comment author="dmikhaylov" created="2014-02-26T18:16:14Z" id="36157256">@spinscale I don't see this message in logs.
</comment><comment author="derEremit" created="2014-03-16T23:53:59Z" id="37776012">Just for the record. Getting same error on startup in openvz, but elasticsearch runs without problems for now. A bit irritating as normaly a service start error means the service won't run. As opposed to a warning.
I expect without correct vm.max_map_count I should expect "only" performace problems?
</comment><comment author="spinscale" created="2014-04-25T19:51:32Z" id="41432718">I dont know openvz enough, but not setting this setting can also result in lucene exceptions and indexing problems, it is not only about performance here - I dont how openvz is handling this. Can you get us any insight here @derEremit?
</comment><comment author="13h3r" created="2014-05-12T02:07:15Z" id="42791055">Got the same on openvz
</comment><comment author="kritik" created="2014-05-14T08:53:35Z" id="43057018">I see the same message on big index but after allocating more memmory elasticsearch starts at least :) Using OpenVZ too and elasticsearch 1.1.1.
</comment><comment author="idanov" created="2014-06-03T13:00:04Z" id="44960900">Got the same on OpenVZ and elasticsearch 1.2.0.
</comment><comment author="r0mdau" created="2014-06-10T21:59:03Z" id="45677593">Got the same on OpenVZ and elasticsearch 1.2.0
Template : [debian-7.0-x86_64.tar.gz](http://download.openvz.org/template/precreated/debian-7.0-x86_64.tar.gz)
Host Server : Proxmox 3.1-21
</comment><comment author="derEremit" created="2014-06-11T14:58:31Z" id="45752902">Basically openvz does not allow modifications of kernel parameters as these would affect the host and every other guest machine.
I think that setting should be removed from init scripts and put in a README.
The init script could check for the correct setting and output a warning but not set these kernel parameters itself!
</comment><comment author="joke2k" created="2014-06-26T13:23:59Z" id="47224544">:+1: 
</comment><comment author="scamianbas" created="2014-07-02T17:51:44Z" id="47811482">Hi all,
kinda newbie here.
I'm using Proxmox, Ubuntu OpenVZ container, Elasticsearch 1.2.1.
I had the same service starting issues and I fixed that by removing the "-d" option in DAEMON_OPTS (a directory path shoud have been provided here)
Also I changed the line 
if [ -n "$MAX_MAP_COUNT" ]
with
if [ -n "$MAX_MAP_COUNT" ] &amp;&amp; [ ! -f /proc/sys/vm/max_map_count ]
to address the unwanted sysctl issue.
Hope it helps.
</comment><comment author="adeleglise" created="2014-08-10T13:58:01Z" id="51715612">Hi all,

Thanks to scamianbas for his tips, I tested this on proxmox 3.2-4, debian7-x64 template, works like a charm !

Thanks to the elasticsearch guys too for this great piece of software ;)
</comment><comment author="andykillen" created="2014-08-12T10:28:36Z" id="51897495">FYI:  fresh install of Unbuntu 12.4 with elastic search 1.3.1 over the top, following this Gist (easy way) for install https://gist.github.com/wingdspur/2026107

I get the error
- Starting Elasticsearch Server  
                                                                                                                                                                  error: permission denied on key 'vm.max_map_count'

when I do a ps check, I get

ps p $(cat /var/run/elasticsearch.pid)
  PID TTY      STAT   TIME COMMAND
 8246 ?        Sl     0:08 /usr/lib/jvm/java-7-openjdk-i386//bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX

and finally when doing "service elasticsearch status"

it says it is running. 

So the error has a slightly different prefix (error: and not sysctl:) but as with other information above it is not stopping the server.   If needed I have the screen output with set -x in the elasticsearch config file. 
</comment><comment author="grepwood" created="2014-09-02T14:29:43Z" id="54159603">I'm having the same issue on CentOS 6.5 with elasticsearch 1.3.2 in OpenVZ.
ps p $(cat /var/run/elasticsearch/elasticsearch.pid)
  PID TTY      STAT   TIME COMMAND
 1132 ?        Sl     0:05 /usr/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Delastics
Just like andykillen, the service is running after all.
</comment><comment author="deric" created="2014-09-30T12:38:16Z" id="57306679">Same issue with LXC and Ubuntu 14.04

```
sysctl: permission denied on key 'vm.max_map_count'
```

though elasticsearch is running, is it a huge problem for production environment?
</comment><comment author="karmux" created="2014-10-29T16:09:29Z" id="60952460">I also get this error on VPS running Ubuntu 14.04 when I (re)start Elasticsearch but is running at least.

```
sysctl: permission denied on key 'vm.max_map_count'
```
</comment><comment author="sts" created="2014-11-13T15:05:03Z" id="62902850">Can someone please merge this? This works! :+1: :shipit: 

`if [ -n "$MAX_MAP_COUNT" ]`
with
`if [ -n "$MAX_MAP_COUNT" ] &amp;&amp; [ ! -f /proc/sys/vm/max_map_count ]`
</comment><comment author="spinscale" created="2014-11-13T15:10:39Z" id="62903785">@sts, shouldnt the check be vice versa and only done if the file exists and thus without the negation? I am confused or maybe misunderstanding the intent
</comment><comment author="alexgarel" created="2014-11-17T15:57:46Z" id="63325903">@sts to avoid the warning you can also edit `/etc/default/elasticsearch` to set `MAX_MAP_COUNT` with an empty value, no need to fix the startup script.
</comment><comment author="grepwood" created="2014-11-18T10:01:58Z" id="63446979">@alexgarel I think it'd be very nice of ES to construct `/etc/default/elasticsearch` that has `MAX_MAP_COUNT` set to a null value when an offending environment like OpenVZ is detected as the host of ES, or otherwise, whatever it is meant to be set to.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Packaging: Check if proc file exists before calling sysctl</comment></comments></commit></commits></item><item><title>Parent / child queries force default similarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4977</link><project id="" key="" /><description>@martijnvg it seems that #3822 may have introduced a regression with custom similarities as well as the non-default bm25 and drf similarities. For instance, if I were to use bm25 for a child doc field, the Top Children query will not use it. It reverts back to the default similarity. 

Here is a set of curl statements for reproducing:

```
# delete index
curl -XDELETE 'http://localhost:9200/top_children_similarity_test/?pretty=true'

# create index with proper parent/child mappings
curl -XPUT 'http://localhost:9200/top_children_similarity_test/?pretty=true' -d '{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0
    }
  },
  "mappings": {
    "author": {
      "properties": {
        "name": { "type": "string" }
      }
    },
    "post": {
      "_parent": { "type": "author" },
      "properties": {
        "content": { "type": "string" }
      }
    }
  }
}'

# add data
curl -XPUT 'http://localhost:9200/top_children_similarity_test/author/1?pretty=true' -d '{
   "name": "George P. Stathis"
}'
curl -XPUT 'http://localhost:9200/top_children_similarity_test/post/1?parent=1&amp;pretty=true' -d '{
  "post" : {
    "content": "Lorem ipsum dolor sit amet."
  }
}'
curl -XPUT 'http://localhost:9200/top_children_similarity_test/post/2?parent=1&amp;pretty=true' -d '{
  "post" : {
    "content": "Lorem ipsum dolor sit amet again!"
  }
}'

echo " "
echo "Sleep for two secs to allow for indexing"
sleep 2

# Search posts directly
echo " "
echo "Run query against child docs"
curl 'http://localhost:9200/top_children_similarity_test/post/_search?pretty=1' -d '{
  "query" : {
    "query_string" : {
      "default_field": "content",
      "query" : "Lorem ipsum"
    }
  }
}' | grep '_score'

echo " "
echo "Two docs should have matched with scores 0.61871845 and 0.53033006"

# Search with top_children
echo " "
echo "Run same query as top_children query in 'sum' mode"
curl 'http://localhost:9200/top_children_similarity_test/_search?pretty=1' -d '{
  "query": {
    "top_children": {
      "type": "post",
      "query": {
        "query_string": {
          "query": "Lorem ipsum"
        }
      },
      "score": "sum"
    }
  }
}' | grep '_score'

echo " "
echo "One parent doc should have matched with score 1.1490486 (i.e. 0.61871845 + 0.53033006)"
sleep 5

# delete index
echo " "
echo "Start over and re-index posts using BM25 similarity"
curl -XDELETE 'http://localhost:9200/top_children_similarity_test/?pretty=true'

# create index with proper parent/child mappings
curl -XPUT 'http://localhost:9200/top_children_similarity_test/?pretty=true' -d '{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0
    }
  },
  "mappings": {
    "author": {
      "properties": {
        "name": { "type": "string" }
      }
    },
    "post": {
      "_parent": { "type": "author" },
      "properties": {
        "content": { "type": "string", "similarity" : "BM25" }
      }
    }
  }
}'

# add data
curl -XPUT 'http://localhost:9200/top_children_similarity_test/author/1?pretty=true' -d '{
   "name": "George P. Stathis"
}'
curl -XPUT 'http://localhost:9200/top_children_similarity_test/post/1?parent=1&amp;pretty=true' -d '{
  "post" : {
    "content": "Lorem ipsum dolor sit amet."
  }
}'
curl -XPUT 'http://localhost:9200/top_children_similarity_test/post/2?parent=1&amp;pretty=true' -d '{
  "post" : {
    "content": "Lorem ipsum dolor sit amet again!"
  }
}'

echo " "
echo "Sleep for another two secs to allow for indexing"
sleep 2

# Search posts directly
echo " "
echo "Run query against child docs"
curl 'http://localhost:9200/top_children_similarity_test/post/_search?pretty=1' -d '{
  "query" : {
    "query_string" : {
      "default_field": "content",
      "query" : "Lorem ipsum"
    }
  }
}' | grep '_score'

echo " "
echo "NOTE!!! Two docs should now have matched with scores 0.80081946 and 0.67905 because we are using BM25"

# Search with top_children
echo " "
echo "Run same query as top_children query in 'sum' mode"
curl 'http://localhost:9200/top_children_similarity_test/_search?pretty=1' -d '{
  "query": {
    "top_children": {
      "type": "post",
      "query": {
        "query_string": {
          "query": "Lorem ipsum"
        }
      },
      "score": "sum"
    }
  }
}' | grep '_score'

echo " "
echo "NOTE!!! One parent doc matched but with with score 1.1490486 which is the sum of the child doc scores as computed by the default similarity (i.e. 0.61871845 + 0.53033006) not BM25. With BM25, the expected parent doc score should have been 0.80081946 + 0.67905 = 1.47986946"
```

This affects every version from 0.90.6 all the way to 0.90.11-SNAPSHOT.

A proposed fix may be to carry over the similarity configured in the IndexSearcher passed  to the createWeight() methods. See https://github.com/gpstathis/elasticsearch/commit/21d4a766dcf6b6d576cf123ea95c3dde428dbe88 for an example. All `org.elasticsearch.index.search.child` package tests pass with these modifications.
</description><key id="26719997">4977</key><summary>Parent / child queries force default similarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gpstathis</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T22:19:02Z</created><updated>2014-02-06T16:59:53Z</updated><resolved>2014-02-06T16:59:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-31T22:32:01Z" id="33849310">hey @gpstathis this makes a lot of sense to me though! Would you be able to open a PullRequest for this and singe the CLA so we can pull it in? It would be great to have that in the next release which is coming very soon
</comment><comment author="gpstathis" created="2014-01-31T23:20:42Z" id="33852829">CLA is signed. Pull request #4979 is open.
</comment><comment author="gpstathis" created="2014-02-03T15:43:44Z" id="33966655">Hey @s1monw, @martijnvg, thanks for the lightning fast turnaround! I see most 0.90.11 tickets are closed. The release must be really close? ;-)
</comment><comment author="kimchy" created="2014-02-03T15:44:17Z" id="33966729">@gpstathis _really_ close ;)
</comment><comment author="gpstathis" created="2014-02-03T15:49:47Z" id="33967311">:+1: 
</comment><comment author="s1monw" created="2014-02-03T15:50:00Z" id="33967331">yeah I guess you had a good timing @gpstathis ;)
</comment><comment author="gpstathis" created="2014-02-03T16:33:25Z" id="33972029">https://twitter.com/elasticsearch/status/430368980122222593. Hehe. Nice!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cleanup for the internal plugin infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4976</link><project id="" key="" /><description>- Introduced the `JvmPlugin` wrapper (to add additional internal functionality to the plugin)
- fixed `hashcode()` on the `PluginInfo`
- extracted the `PluginsInfo` caching logic to `CachableReference` construct in utils
- cleaned up the `PluginsService` class to make the logic there more structured/understandable
</description><key id="26718575">4976</key><summary>cleanup for the internal plugin infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">uboness</reporter><labels><label>:Plugins</label><label>enhancement</label></labels><created>2014-01-31T21:55:57Z</created><updated>2016-03-08T13:06:36Z</updated><resolved>2016-03-08T13:06:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-01T07:07:39Z" id="33865452">I like it! Very much easier to understand.
I left a small comment.

LGTM.
</comment><comment author="clintongormley" created="2014-07-11T09:52:07Z" id="48712936">@uboness what's happening with this PR?
</comment><comment author="javanna" created="2015-03-21T10:19:12Z" id="84301243">Hey @tlrx what's the status here? Is it replaced by the newer refactoring of the `PluginManager` that you just sent?
</comment><comment author="tlrx" created="2015-04-27T12:29:18Z" id="96631996">@javanna this is a clean up from @uboness and it is not replaced by #9998 but I looked quickly at the code and I think this clean up is welcome.

I'll review this - hopefully quicky - since it has already one year old.
</comment><comment author="jpountz" created="2015-08-10T13:16:19Z" id="129441692">@uboness Plugins management having changed a lot recently, do some of the changes in your PR still apply or should we just close it?
</comment><comment author="uboness" created="2015-08-10T13:20:31Z" id="129444783">some things here still apply (e.g. fixing the `PluginInfo#hashcode()`), but obviously this PR needs to be rebased first
</comment><comment author="clintongormley" created="2016-03-08T13:06:36Z" id="193778402">This PR is very out of date.  Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add RamUsageEstimator#sizeOf(Object) to forbidden APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4975</link><project id="" key="" /><description>This method can be a performance trap since it traverse the
entire object tree that is referenced by the provided object.
See LUCENE-5373
</description><key id="26713945">4975</key><summary>Add RamUsageEstimator#sizeOf(Object) to forbidden APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v0.90.12</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T20:41:02Z</created><updated>2015-06-07T15:44:36Z</updated><resolved>2014-01-31T20:51:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-31T20:44:12Z" id="33840574">LGTM
</comment><comment author="martijnvg" created="2014-01-31T20:46:34Z" id="33840774">:+1: push!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problem with parameters for Odata-like URI in REST plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4974</link><project id="" key="" /><description>Hello,

I can't get correct values of parameters type and id with the following URI pattern:

```
controller.registerHandler(Method.GET,
            "/odata/v4/{index}/odata.svc/{type}({id})", this);
```

For example, if I use this URI /odata/v4/myindex/odata.svc/mytype(1), the values of parameters are:
- type : mytype(1)
- id : 

When looking at source code in methods PathTrie#insert and Strings#splitStringToArray, it appears that paths are splitted using the / separator and each corresponding token can only contain one param name enclosed by { and }.
</description><key id="26712008">4974</key><summary>Problem with parameters for Odata-like URI in REST plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">templth</reporter><labels /><created>2014-01-31T20:10:07Z</created><updated>2014-12-24T19:11:56Z</updated><resolved>2014-12-24T19:11:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T19:11:56Z" id="68070204">Hi @templth 

Sorry it has taken a while to look at this. I'm going to close this issue in favour of #5074
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Two bugfixes for the completion format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4973</link><project id="" key="" /><description /><key id="26709739">4973</key><summary>Two bugfixes for the completion format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Suggesters</label><label>bug</label><label>v0.90.12</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T19:40:40Z</created><updated>2015-06-07T23:42:19Z</updated><resolved>2014-01-31T20:17:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-31T20:17:27Z" id="33838426">pushed 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove support for boost in copy_to field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4972</link><project id="" key="" /><description>Currently, boosting on `copy_to` is misleading and does not work as originally specified in #4520. Instead of boosting just the terms from the origin field, it boosts the whole destination field.  If two fields copy_to a third field, one with a boost of 2 and another with a boost of 3, all the terms in the third field end up with a boost of 6.  This was not the intention.

  The alternative: to store the boost in a payload for every term, results in poor performance and inflexibility. Instead, users should either (1) query the common field AND the field that requires boosting, or (2) the multi_match query will soon be able to perform term-centric cross-field matching that will allow per-field boosting at query time (coming in 1.1).
</description><key id="26704861">4972</key><summary>Remove support for boost in copy_to field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-01-31T18:23:56Z</created><updated>2014-06-13T00:03:36Z</updated><resolved>2014-01-31T20:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-31T19:21:30Z" id="33833413">I had one small comment otherwise LGTM and push to all branches
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added exception to match and multi-match queries if passed an invalid type param</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4971</link><project id="" key="" /><description>Closes #4964
</description><key id="26704809">4971</key><summary>Added exception to match and multi-match queries if passed an invalid type param</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Query DSL</label><label>bug</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T18:22:55Z</created><updated>2015-06-07T23:48:45Z</updated><resolved>2014-03-07T12:37:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-31T19:08:14Z" id="33832130">LGTM +1 to push to all branches!
</comment><comment author="spinscale" created="2014-03-07T12:37:09Z" id="37020285">closing, has been pushed https://github.com/elasticsearch/elasticsearch/commit/5f8dbeaa8e6dade54bf488f9fe7875546a094d4c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>marvel.agent Background thread had an uncaught exception: java.lang.NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4970</link><project id="" key="" /><description>```
[2014-01-31 11:13:42,911][ERROR][marvel.agent             ] [mls-es-stag-01-mls-tigerstage] Background thread had an uncaught exception: java.lang.NullPointerException
at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$CompletionTerms.stats(Completion090PostingsFormat.java:288)
at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat.completionStats(Completion090PostingsFormat.java:351)
at org.elasticsearch.index.shard.service.InternalIndexShard.completionStats(InternalIndexShard.java:545)
at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:151)
at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:211)
at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:175)
at org.elasticsearch.node.service.NodeService.stats(NodeService.java:149)
at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:269)
at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:174)
at java.lang.Thread.run(Thread.java:744)
```

This is on ES 0.90.10. Possibly related to #4788. 
</description><key id="26701265">4970</key><summary>marvel.agent Background thread had an uncaught exception: java.lang.NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timbunce</reporter><labels><label>:Suggesters</label><label>bug</label><label>v0.90.12</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T17:25:57Z</created><updated>2015-06-07T23:46:08Z</updated><resolved>2014-01-31T20:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-31T18:21:39Z" id="33827958">tim do you have any custom configuration / code running on your nodes or any plugins that are not standard? You should never see a terms instance that has a `null` lookup factory.
</comment><comment author="timbunce" created="2014-01-31T21:53:29Z" id="33846290">Config is very plain.
Plugins: bigdesk  geocluster-facet  geohash-facet  head  inquisitor  marvel  paramedic  segmentspy.
Of those only marvel and head were being used.
Does that help?
</comment><comment author="s1monw" created="2014-01-31T21:54:30Z" id="33846368">not really I really don't see how we can pull a MERGE reader but I will try to investigate that I for now just made sure we don't get a NPE anymore.

Thanks so much for reporting this!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java</file></files><comments><comment>Don't load CompetionTerms if lookupFactory is null</comment></comments></commit></commits></item><item><title>Getting the list of templates that are in the directory config/templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4969</link><project id="" key="" /><description>Hi,
There is a strange behavior about how to get the list of the templates. According to the documentation (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html), we have to type this :
$ curl -XGET localhost:9200/_template/temp*
But we have only the template put via this command :
$ curl -XPUT localhost:9200/_template/template_1 -d[...]

But if we have template files in /etc/elasticsearch/templates and there are loaded at the start step, we don't see them if we type the first command line above.

I use the deb elasticsearch 0.90.9.

Can you confirm that or not ?

Thank you !
</description><key id="26700849">4969</key><summary>Getting the list of templates that are in the directory config/templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rsareth</reporter><labels /><created>2014-01-31T17:19:49Z</created><updated>2014-12-24T19:08:55Z</updated><resolved>2014-12-24T19:08:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T19:08:54Z" id="68070067">Hi @rsareth 

Sorry it has taken a while to get to this ticket.  This is correct. The GET-template API only returns templates that have been created via APIs, not ones sitting in the config directories (which may vary by node anyway).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve scroll search by using IndexSearcher#searchAfter(...)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4968</link><project id="" key="" /><description>PR for #4940
</description><key id="26698093">4968</key><summary>Improve scroll search by using IndexSearcher#searchAfter(...)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T16:41:59Z</created><updated>2015-06-07T15:46:50Z</updated><resolved>2014-03-21T06:53:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-02-21T10:43:32Z" id="35719576">Updated PR that includes @jpountz and @spinscale feedback points.
</comment><comment author="martijnvg" created="2014-02-25T14:35:47Z" id="36012923">@jpountz Your comments make sense, I updated the PR with another commit that addresses your comments.
</comment><comment author="jpountz" created="2014-03-18T18:07:40Z" id="37967260">This pull request looks good in general:
- I like the duel tests
- The comparator logic for `compareTop` looks good to me (even the tricky ordinal-based comparator).

However, there are a few places where the `useClassicScroll` assignment looks weird (used before being set)? I'm also wondering if the old scroll mode should be given a more despicable name than `classic`? Otherwise it isn't clear that performance is much worse when this parameter is false?
</comment><comment author="jpountz" created="2014-03-20T14:33:12Z" id="38173613">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>field mappings for elasticsearch v. 0.90.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4967</link><project id="" key="" /><description>I need to retrieve the field mappings for a type under elasticsearch 0.90.0. 

ES is running as an embedded local node, and here is how I connect to it and retrieve mappings (the result contains an empty map):
..
//creates index and mappings
...
  Client client = NodeBuilder.nodeBuilder().clusterName(CLUSTER_NAME).local(true).client(true).node().client();
  GetFieldMappingsRequest getMappingsRequest =
                new GetFieldMappingsRequest().indices(INDEX_NAME).types(TYPE_NAME).local(true).includeDefaults(true);
        GetFieldMappingsResponse response = client.admin().indices().getFieldMappings(getMappingsRequest).actionGet();

The problem is that the mappings returned are empty
</description><key id="26692889">4967</key><summary>field mappings for elasticsearch v. 0.90.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abennet</reporter><labels /><created>2014-01-31T16:32:31Z</created><updated>2014-01-31T16:34:39Z</updated><resolved>2014-01-31T16:34:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-31T16:34:39Z" id="33809630">Hi @abennet 

Please ask questions in the forum not on the issues list.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing filter not working for field name 'location' but works if prefixed by type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4966</link><project id="" key="" /><description>We ran into a very strange issue where a missing filter would not work for a specific field name 'location'. We filled this field with names of cities like Amsterdam, Londen, etc. Here is an example query:

```
{
  "from": 0,
  "size": 100,
  "sort": {
    "order": "asc"
  },
  "explain": true,
  "query": {
    "constant_score": {
      "filter": {
        "and": [
          {
            "missing": {
              "field": "location"
            }
          },
          {
            "term": {
              "type": "VacancySearchSeo"
            }
          }
        ]
      }
    }
  }
}
```

The problem is that the missing filter is completely ignored. You can see that in the result

```
ConstantScore(+*:* +cache(type:VacancySearchSeo))
```

If we replace the location field with any other field in the index the missing filter works. If we prefix the location field with the name of the type it also works.

```
ConstantScore(++cache(_type:vacancysearchseo) +cache(NotFilter(cache(BooleanFilter(location:[* TO *])))) +cache(type:VacancySearchSeo))
```

It seems like the problem is the name of the field. We have also a type with the same name.
The funny thing is that it only happens with the missing filter. A normal term filter on this field does work correctly.
</description><key id="26689361">4966</key><summary>Missing filter not working for field name 'location' but works if prefixed by type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gtwijma</reporter><labels /><created>2014-01-31T15:44:48Z</created><updated>2014-12-24T19:03:27Z</updated><resolved>2014-12-24T19:03:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T19:03:27Z" id="68069832">Hi @gtwijma 

Sorry it has taken a while to get to this issue.  I'm guessing that you had a type called `location` too?  This issue will be fixed by #8870.  

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[TEST] Added ability to skip REST test suites/sections based on their required features</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4965</link><project id="" key="" /><description>As we have different runners for the REST tests we need a mechanism that allows to add features to any of them without breaking all other runners builds.
The idea is to name features and temporarily use skip sections that mention the required new features, so that runners that don't support it yet will skip the test.

Added support for `features` field in skip section.
Added `Features` class that contains a static list of the features supported by the runner. If a feature mentioned in a skip section is not listed here, the test will be skipped.
</description><key id="26683668">4965</key><summary>[TEST] Added ability to skip REST test suites/sections based on their required features</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T14:18:10Z</created><updated>2014-06-30T11:39:44Z</updated><resolved>2014-01-31T16:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-31T15:48:30Z" id="33805031">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`multi_match` doesn't complain it `type` is not recognized</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4964</link><project id="" key="" /><description>if you run something like this today:

``` json
"multi_match" : {
   "query" : "mastodon aragon",
   "fields" : [ "Artist", "Album" ],
    "type":"something_that_does_not_exists"
}
```

the `multi_match` query just defaults to `bool` instead of throwing an exception which it should.
</description><key id="26682473">4964</key><summary>`multi_match` doesn't complain it `type` is not recognized</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.12</label><label>v1.0.0</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T13:57:25Z</created><updated>2014-02-10T10:00:23Z</updated><resolved>2014-02-03T10:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/MatchQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/MultiMatchQueryParser.java</file></files><comments><comment>Added exception if passed an invalid type param</comment><comment>Closes #4964</comment></comments></commit></commits></item><item><title>Added `fields` support to `geo_point` and `completion` field type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4963</link><project id="" key="" /><description>Upgrading 0.90.x `multi_field` type that has a `geo_point` or `completion` field type as default field would otherwise fail.
Also it make sense to support these field types, because both support specifying the actual values as string.
</description><key id="26676361">4963</key><summary>Added `fields` support to `geo_point` and `completion` field type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-31T11:51:08Z</created><updated>2015-06-07T23:50:06Z</updated><resolved>2014-02-02T08:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-31T23:08:55Z" id="33852055">LGTM
</comment><comment author="martijnvg" created="2014-02-02T08:22:09Z" id="33895003">pushed to master, 1.x and 1.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search results inconsistency with nested properties of the same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4962</link><project id="" key="" /><description>I am seeing what seems to be inconsistent behavior with the following type of data:

```
POST /naming_issue/1
{
    "name" : "foo",
    "properties": {
        "name" : "foo"
    }
}

POST /naming_issue/2
{
    "name" : "foo"
}

POST /naming_issue/3
{
    "properties": {
        "name" : "foo"
    }
}

POST /naming_issue/4
{
    "properties": {
        "name2" : "foo"
    }
}
```

If I issue the following search:

```
POST /naming_issue/_search
{
    "query": {
        "query_string": {
           "query": "name2:foo"
        }
    }
}
```

I see document 4, which is expected, without supplying the 'properties' in front of 'name2'. Likewise, the following search:

```
POST /naming_issue/_search
{
    "query": {
        "query_string": {
           "query": "properties.name:foo"
        }
    }
}
```

Shows documents 1 and 3, as expected. However if I execute:

```
POST /naming_issue/_search
{
    "query": {
        "query_string": {
           "query": "name:foo"
        }
    }
}
```

Then I see only documents 1 and 2. My expectation would be to see document 3 as well. 
</description><key id="26646003">4962</key><summary>Search results inconsistency with nested properties of the same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hglkrijger</reporter><labels /><created>2014-01-31T00:01:07Z</created><updated>2014-01-31T10:12:04Z</updated><resolved>2014-01-31T10:12:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-31T10:12:04Z" id="33774408">This is correct. If you don't use full pathnames to refer to your fields, Elasticsearch finds the first matching fieldname and uses that.  If you search on `name` then you may end up with the top level `name` field or the `properties.name` field.

To distinguish between them, prepend the `type` name as well:
- `issue.name` 
- `issue.properties.name`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Date format MapperParsingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4961</link><project id="" key="" /><description>&lt;pre lang="java"&gt;&lt;code&gt;
[error:MapperParsingException[failed to parse [@dateCreated]]; nested: MapperParsingException[failed to parse date field [2013-11-14 20:09:18.968 UTC], tried both date format [yyyy-MM-dd HH:mm:ss.SSS z], and timestamp number]; nested: IllegalArgumentException[Invalid format: "2013-11-14 20:09:18.968 UTC" is malformed at "UTC"]; , status:400]
&lt;/code&gt;&lt;/pre&gt;


Basically, this is a valid ISO 8601 pattern and elasticsearch returns an error while trying to index it. 

You can test the pattern here:
http://www.fileformat.info/tip/java/simpledateformat.htm
</description><key id="26635333">4961</key><summary>Date format MapperParsingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">moskiteau</reporter><labels /><created>2014-01-30T21:26:08Z</created><updated>2014-01-31T14:13:24Z</updated><resolved>2014-01-31T14:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-30T21:43:11Z" id="33737077">Time zone name `z` cannot be parsed. It's limitation of [joda time](http://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html) library that elasticsearch is using.
</comment><comment author="uboness" created="2014-01-30T21:50:21Z" id="33737768">this is the format that es will be able to parse by default (i.e. without mapping config):

```
date-opt-time     = date-element ['T' [time-element] [offset]]
date-element      = std-date-element | ord-date-element | week-date-element
std-date-element  = yyyy ['-' MM ['-' dd]]
ord-date-element  = yyyy ['-' DDD]
week-date-element = xxxx '-W' ww ['-' e]
time-element      = HH [minute-element] | [fraction]
minute-element    = ':' mm [second-element] | [fraction]
second-element    = ':' ss [fraction]
fraction          = ('.' | ',') digit+
offset            = 'Z' | (('+' | '-') HH [':' mm [':' ss [('.' | ',') SSS]]])
```

so based on the above you'll have to send the following: `2013-11-14T20:09:18.968Z`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filtered query parses _name incorrectly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4960</link><project id="" key="" /><description /><key id="26629592">4960</key><summary>Filtered query parses _name incorrectly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Query DSL</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T20:24:38Z</created><updated>2015-06-07T23:50:36Z</updated><resolved>2014-01-30T20:32:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java</file></files><comments><comment>filtered query parses _name incorrectly</comment><comment>fixes #4960</comment></comments></commit></commits></item><item><title>Document FilteredQuery strategy options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4959</link><project id="" key="" /><description>Relevant docs that are currently under-documented:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filtered-query.html

Relevant code with options:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/FilteredQueryParser.java#L79
</description><key id="26629117">4959</key><summary>Document FilteredQuery strategy options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dsmiley</reporter><labels /><created>2014-01-30T20:20:27Z</created><updated>2014-12-24T19:02:06Z</updated><resolved>2014-12-24T19:02:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T19:02:06Z" id="68069786">Hi @dsmiley 

Sorry it has taken a while to look at this ticket. In the meantime, it has already been resolved.  

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Startup: Add ES_HOME to ES_INCLUDE search path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4958</link><project id="" key="" /><description>With this change, the elasticsearch script can be linked to another path without having to set ES_INCLUDE to match the installation path. Previously, the elasticsearch would find ES_HOME correctly even if linked but could not find the include script, and finding it would be expected behavior to me based on its current search path.

I'm going straight to a pull request on this, as it's a minor improvement to the executable and solutions to ES_INCLUDES are generally workarounds listed on closed issues.
</description><key id="26620394">4958</key><summary>Startup: Add ES_HOME to ES_INCLUDE search path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rduplain</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T18:29:12Z</created><updated>2015-03-19T09:11:10Z</updated><resolved>2014-10-22T16:45:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T12:53:41Z" id="59745145">@spinscale please could you take a look at this
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Startup: Add ES_HOME to ES_INCLUDE search path</comment></comments></commit></commits></item><item><title>The binary field shouldn't be stored by default, because it is already available in the _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4957</link><project id="" key="" /><description /><key id="26613219">4957</key><summary>The binary field shouldn't be stored by default, because it is already available in the _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>breaking</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T16:53:37Z</created><updated>2015-06-06T17:07:17Z</updated><resolved>2014-01-31T09:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-30T19:16:16Z" id="33722278">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Handle "true"/"false" in snapshot api for "include_global_state"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4956</link><project id="" key="" /><description>closes #4949 : bug fix : handle "true"/"false" in snapshot api for "include_global_state"

[see issue #4949](https://github.com/elasticsearch/elasticsearch/issues/4949)
</description><key id="26600385">4956</key><summary>Handle "true"/"false" in snapshot api for "include_global_state"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">meconlin</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T14:11:03Z</created><updated>2015-06-07T15:46:01Z</updated><resolved>2014-01-30T18:33:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-30T18:33:21Z" id="33717813">Pushed! Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Range filter no cache behaviour for `now` with rounding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4955</link><project id="" key="" /><description>The forceful no cache behaviour for range filter with now date match expression should only be active if no rounding has been specified for `now` in the date range range expression (for example: `now/d`).

 Closes #4947
 Relates to #4846
</description><key id="26593763">4955</key><summary>Range filter no cache behaviour for `now` with rounding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T12:08:46Z</created><updated>2015-06-07T16:12:35Z</updated><resolved>2014-01-30T14:52:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-30T12:31:20Z" id="33683477">LGTM this should go on all branches?
</comment><comment author="martijnvg" created="2014-01-30T13:22:02Z" id="33686798">Yes: 0.90, 1.0, 1.x and master
</comment><comment author="s1monw" created="2014-01-30T14:27:53Z" id="33691970">LGTM! 
</comment><comment author="martijnvg" created="2014-01-30T14:52:14Z" id="33694127">pushed to 0.90, 1.0, 1.x and master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster State API: Remove index template filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4954</link><project id="" key="" /><description>As the get index template API does a pretty good job of selecting and showing specific index templates to the user, we could potentially remove this functionality, when using the cluster state API - as this functionality is no more needed at that place.

Up for discussion.
</description><key id="26593626">4954</key><summary>Cluster State API: Remove index template filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T12:05:57Z</created><updated>2014-05-06T07:55:35Z</updated><resolved>2014-05-05T13:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-20T09:42:03Z" id="38148874">@spinscale I pushed this to 1.2 lets discuss soon
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/state/RestClusterStateAction.java</file><file>src/test/java/org/elasticsearch/cluster/SimpleClusterStateTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Cluster State API: Remove index template filtering</comment></comments></commit></commits></item><item><title>REST Get Field Mapping API: Fix NPE if field not existent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4953</link><project id="" key="" /><description>When fixing #4738, a small issue leaked into the implementation.
The equals check in the RestAction only applied when the master node
returned the rest request, otherwise the object equality would not hold
due to being transferred over the wire and being deserialized into
another object (from and an equality point of view) than the
FieldMappingMetaData.NULL object - this could result in serialization
exceptions as an empty length bytes reference is used in toXContent.

This is not covered by our tests due to running all nodes in one JVM.
</description><key id="26587419">4953</key><summary>REST Get Field Mapping API: Fix NPE if field not existent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-01-30T10:06:00Z</created><updated>2014-06-18T11:51:40Z</updated><resolved>2014-01-30T11:04:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-30T10:53:38Z" id="33677541">fix LGTM we need to make sure we catch that!!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Marvel: index column sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4952</link><project id="" key="" /><description>When the rows are sorted using a column that has numerical values e.g. Documents then the rows are sorted alphabetically and not numerically
</description><key id="26586059">4952</key><summary>Marvel: index column sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">michaelsalmon</reporter><labels /><created>2014-01-30T09:39:08Z</created><updated>2014-01-30T13:08:45Z</updated><resolved>2014-01-30T12:37:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-01-30T12:37:04Z" id="33683857">Hi michael, I tried it out and works correctly for me. If you have any questions/more info I'd be more then happy to answer/look into if you open a thread on our google group: https://groups.google.com/forum/#!forum/elasticsearch

Cheers,
Boaz
</comment><comment author="michaelsalmon" created="2014-01-30T13:08:45Z" id="33685923">When I first played with marvel it behaved as I described but not now. If I find out how I broke it the first time I’ll start a thread.

On 30 Jan 2014, at 13:37:37, Boaz Leskes notifications@github.com wrote:

&gt; Hi michael, I tried it out and works correctly for me. If you have any questions/more info I'd be more then happy to answer/look into if you open a thread on our google group: https://groups.google.com/forum/#!forum/elasticsearch
&gt; 
&gt; Cheers,
&gt; Boaz
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.

/Michael Salmon
Efficiency IT Expert
Inovia AB
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sorting terms agg by sub-aggegation doesn't respect asc/desc when executing on a single shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4951</link><project id="" key="" /><description>As reported on #4643, when trying to sort `terms` aggregation based on sub metric aggregation, the `asc`/`desc` order is not respected. For example:

``` json
{
  "aggs": {
    "fieldValues": {
      "terms": {
        "field": "name.field",
        "order": {
          "totalAmount": "desc"
        }
      },
      "aggs": {
        "totalAmount": {
          "sum": {
            "field": "amount"
          }
        }
      }
    }
  },
  "size": 0
}
```

In the example above, when executing on a single shard, the terms are always sorted in an ascending order.
</description><key id="26575071">4951</key><summary>Sorting terms agg by sub-aggegation doesn't respect asc/desc when executing on a single shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T04:57:23Z</created><updated>2015-08-13T15:30:34Z</updated><resolved>2014-01-30T05:59:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-30T06:04:41Z" id="33662623">@benatwork99 fyi, thx for your patience
</comment><comment author="benatwork99" created="2014-01-31T05:22:03Z" id="33762576">@uboness Thank you for the quick fix! Looking forward to RC2 :)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file></files><comments><comment>fixed a bug where when executing on a single shard and sorting terms agg based on sub metric agg, the order (asc/desc) is not respected</comment><comment> - fixed tests for terms order</comment></comments></commit></commits></item><item><title>closes #4949 : bug fix : handle "true"/"false" in snapshot api for "include_global_state"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4950</link><project id="" key="" /><description>closes #4949 : bug fix : handle "true"/"false" in snapshot api for "include_global_state"

[see issue #4949](https://github.com/elasticsearch/elasticsearch/issues/4949)
</description><key id="26572728">4950</key><summary>closes #4949 : bug fix : handle "true"/"false" in snapshot api for "include_global_state"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meconlin</reporter><labels /><created>2014-01-30T03:25:49Z</created><updated>2014-06-14T13:27:45Z</updated><resolved>2014-01-30T14:10:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshot API : Why is "include_global_state" boolean handled differently from "ignore_unavailable" in the API?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4949</link><project id="" key="" /><description>Why is "include_global_state" boolean handled differently from "ignore_unavailable" in the API?

One handles string "true"/"false" one does not?

From `CreateSnapshotRequest`

```
} else if (name.equals("ignore_unavailable") ||  name.equals("ignoreUnavailable")) {
                ignoreUnavailable = nodeBooleanValue(entry.getValue());
}
```

vs.

```
if (!(entry.getValue() instanceof Boolean)) {
                    throw new ElasticsearchIllegalArgumentException("malformed include_global_state, should be boolean");
}
```
</description><key id="26569908">4949</key><summary>Snapshot API : Why is "include_global_state" boolean handled differently from "ignore_unavailable" in the API?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">meconlin</reporter><labels><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T01:57:21Z</created><updated>2014-02-03T07:29:45Z</updated><resolved>2014-01-30T18:30:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-30T02:06:03Z" id="33653956">Because I missed it. I will fix. Thanks for catching it.
</comment><comment author="meconlin" created="2014-01-30T02:24:45Z" id="33654804">oh fun! can I fix it? love es by the way, thanks for cranking on it. The snapshot api is awesome. 
</comment><comment author="imotov" created="2014-01-30T02:45:49Z" id="33655707">Sure. By the way, `RestoreSnapshotRequest` has the same issue.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file></files><comments><comment>Improve boolean handling in snapshot/restore request parsing</comment></comments></commit></commits></item><item><title>Mapping update on stored field requires a restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4948</link><project id="" key="" /><description>Hello,

Elasticsearch is not working as I expect when I change the mapping of a stored field to a multi_field.

I create an index called test, then assign a mapping to it. The mapping contains a field called _title_ which is stored string.

I add a document with a _title_ to the index.

I change the mapping of _title_ to multi_field. The multi_field has two subfields: title and simple_title. Both strings. Neither stored.

I add a new document with a _title_ to the index.

A query which specifies that only the field _title_ should be shown returns two documents but no fields. Restarting elasticsearch makes the same query display the _title_ field correctly. Is that expected behaviour?

Below are the same actions written in ES-friendly way:

```
PUT test
{
    "index" : {
        "number_of_shards": 5,
        "number_of_replicas" : 0
    }
}

PUT test/test/_mapping
{
    "test": {
        "properties": {
            "title": {
                "type": "string",
                "store": "yes"
            }
        }
    }
}

PUT test/test/before_mapping_update
{
   "title": "hello oi oi"
}

PUT test/test/_mapping
{
    "test": {
        "properties": {
            "title": {
                "type": "multi_field",
                "fields": {
                    "title": {"type":"string"},
                    "title_simple": {"type":"string"}
                }
            }
        }
    }
}

PUT test/test/after_mapping_update
{
   "title": "hello oi oi"
}

POST test/test/_search
{
    "fields":["title"],
    "query":{
        "match_all": {}    
    }
}
```

Cheers,
Rafael
</description><key id="26569174">4948</key><summary>Mapping update on stored field requires a restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aflag</reporter><labels /><created>2014-01-30T01:37:18Z</created><updated>2014-12-24T19:01:30Z</updated><resolved>2014-12-24T19:01:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aflag" created="2014-02-01T21:04:40Z" id="33883390">I found out that on 1.0.0.RC1 the bug is fixed. That's because 1.0.0.RC1 does not allow for 

```
PUT test/test/_mapping
{
    "test": {
        "properties": {
            "title": {
                "type": "multi_field",
                "fields": {
                    "title": {"type":"string"},
                    "title_simple": {"type":"string"}
                }
            }
        }
    }
}
```

to happen. The result looks like this:

``` json
{
  "error": "MergeMappingException[Merge failed with failures {[mapper [title] has different store values]}]",
  "status": 400
}
```

Instead, it requires the user to specify

```
PUT test/test/_mapping
{
    "test": {
        "properties": {
            "title": {
                "type": "multi_field",
                "fields": {
                    "title": {"type":"string", "store": "yes"},
                    "title_simple": {"type":"string", "store": "yes"}
                }
            }
        }
    }
}
```

which works both in 0.90.10 and 1.0.0.RC1. Investigating the issue a little further, I found out the patch which changed the behaviour is 943b62634c1ca798a0a8b47918f2b23f707d8b06. It introduces a new way to make multi-fields. So I suppose it can't be backported entirely to 0.90.10. However, disallowing the first mapping change is probably a good idea. I still don't know how to make that change in the code, though.
</comment><comment author="clintongormley" created="2014-12-24T19:01:30Z" id="68069755">Sorry for taking a while to get to this. Looks like the issue is resolved, so I'm going to close
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cache date range filters that use `now` with rounding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4947</link><project id="" key="" /><description>In #4846, caching of date range filters which use `now` was disabled.  However, that only makes sense as long as there is no rounding happening.

For instance, this filter does make sense to cache:

```
{
  "range": {
      "date": {
          "gt":  "now/d"
       }
   }
}
```
</description><key id="26569122">4947</key><summary>Cache date range filters that use `now` with rounding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T01:35:49Z</created><updated>2014-02-03T07:09:55Z</updated><resolved>2014-01-30T14:51:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-30T03:46:20Z" id="33658031">good one :)
</comment><comment author="bleskes" created="2014-01-30T07:01:33Z" id="33664658">It depends. Wouldn't it be just as confusing around midnight? Where just after midnight you might not see data? I agree it's much less of an issue but in certain scenarios you may still get wrong results.

On Thu, Jan 30, 2014 at 4:46 AM, uboness notifications@github.com wrote:

&gt; ## good one :)
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/4947#issuecomment-33658031
</comment><comment author="clintongormley" created="2014-01-30T11:05:07Z" id="33678258">@bleskes This isn't about wrong results - it's about cache churn. The filter is cached with whatever `now` evaluates to.  Typically filtering on eg `now - 1h` makes sense not to cache, because you'll never reuse that value.

But a filter with `now/d` likely will be reused. Think about this pattern when retrieving logs for the last hour in an index containing logs for the whole month:

```
"bool": {
    "must": [
        { "range" : {
            "timestamp" : {
                "gt" : "now/d" 
            }
        },
        { "range" : {
            "timestamp" : {
                "gt" : "now-1h" 
            }
        }
    ]
}
```

You want the first filter to be cached, and the second filter not to be cached.
</comment><comment author="imotov" created="2014-01-30T11:26:36Z" id="33679602">Could we resolve the expression with `now` in it using the current time and make it part of the cache key?
</comment><comment author="martijnvg" created="2014-01-30T11:28:35Z" id="33679712">That is what used to happen, but since now is different every time, the next time `now` is used the previous cache key wouldn't be used, this results in thrashing the filter cache (adding entries that never end up being used).
</comment><comment author="bleskes" created="2014-01-30T11:29:00Z" id="33679739">@clintongormley misunderstood then (and learned something about how our internal caching works: @imotov - the caching logic compares the term the expression resolves to)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterCachingTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>The forceful no cache behaviour for range filter with now date match expression should only be active if no rounding has been specified for `now` in the date range range expression (for example: `now/d`).</comment></comments></commit></commits></item><item><title>Add stats for running snapshot to get snapshot API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4946</link><project id="" key="" /><description /><key id="26567846">4946</key><summary>Add stats for running snapshot to get snapshot API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-30T01:04:43Z</created><updated>2014-03-19T15:58:14Z</updated><resolved>2014-03-18T01:08:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStage.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexShardStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotIndexStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotShardsStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportNodesSnapshotsStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/TransportSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/meta/TransportNodesListGatewayMetaState.java</file><file>src/main/java/org/elasticsearch/gateway/local/state/shards/TransportNodesListGatewayStartedShards.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotStatus.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardSnapshot.java</file><file>src/main/java/org/elasticsearch/indices/store/TransportNodesListShardStoreMetaData.java</file><file>src/main/java/org/elasticsearch/repositories/RepositoriesModule.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/snapshots/status/RestSnapshotsStatusAction.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Add ability to get snapshot status for running snapshots</comment></comments></commit></commits></item><item><title>`null_value` should not be analyzed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4945</link><project id="" key="" /><description>When setting a `null_value` on an analyzed field, that value should not be analyzed before indexing:

```
curl -XPUT "http://localhost:9200/my_index " -d'
{
   "mappings": {
      "posts": {
         "properties": {
            "tags": {
               "type": "string",
               "null_value": "NULL"
            }
         }
      }
   }
}'

curl -XPUT "http://localhost:9200/my_index/posts/1" -d'
{ "tags": null }'
```

Sorting shows that the `null_value` has been analyzed:

```
curl -XGET "http://localhost:9200/_search?sort=tags"

{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": null,
      "hits": [
         {
            "_index": "my_index",
            "_type": "posts",
            "_id": "1",
            "_score": null,
            "_source": {
               "tags": null
            },
            "sort": [
               "null"
            ]
         }
      ]
   }
}
```

And using the `missing` filter with `null_value` set to `true` fails to work because it is looking for `NULL` instead of `null`:

```
GET /_search?sort=tags
{
   "query": {
      "filtered": {
         "filter": {
            "missing": {
               "field": "tags",
               "null_value": true
            }
         }
      }
   }
}
```
</description><key id="26560899">4945</key><summary>`null_value` should not be analyzed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2014-01-29T22:53:03Z</created><updated>2015-08-27T09:33:32Z</updated><resolved>2015-08-27T09:33:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T19:00:02Z" id="68069679">@jpountz do you think it is possible to fix this, or should we just close this and move on?
</comment><comment author="rjernst" created="2015-04-10T16:09:42Z" id="91603693">Not analyzing the null value would mean you couldn't find documents with the null value from anything _but_ the `missing` filter.

I don't think this is a problem with null values, but simply with the missing filter.  When using `null_value: true` in `missing`, it should analyze the value before running the filter. 
</comment><comment author="clintongormley" created="2015-04-10T16:13:10Z" id="91604928">True, hadn't considered that. I was thinking more of the case where you want to be sure that the null value is a concrete value that couldn't possibly appear in real values, eg `NULL` when all the real values are lowercased.
</comment><comment author="jpountz" created="2015-08-26T14:43:29Z" id="135044920">I see `null_value` as something that is quite specific to structured data. Not sure how much sense it makes on analyzed string fields?
</comment><comment author="clintongormley" created="2015-08-27T09:33:32Z" id="135364125">@jpountz agreed - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Histogram aggregation documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4944</link><project id="" key="" /><description>The `empty_buckets` parameter was [removed](https://github.com/elasticsearch/elasticsearch/commit/5c237fe834625cdaa7f8850f4d565733ce78e687) in favor of setting `min_doc_count` to 0, but `empty_buckets` still appears in the [documentation](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/search/aggregations/bucket/histogram-aggregation.asciidoc).
</description><key id="26546979">4944</key><summary>Histogram aggregation documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlinn</reporter><labels /><created>2014-01-29T19:38:06Z</created><updated>2014-01-29T19:58:45Z</updated><resolved>2014-01-29T19:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-29T19:44:53Z" id="33622277">indeed, thx!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fixed date_/histogram aggregation documentation - added documentation for the `min_doc_count` setting</comment></comments></commit></commits></item><item><title>[TEST] Added REST test assertion that allows to test apis which don't return json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4943</link><project id="" key="" /><description>The new match_re assertion is based on a regular expression that needs to match the response body returned by an api call.
Added also example tests for cat count api, here is what a test would look like for instance:

```
"Test cat count output":

   - do:
       cat.count: {}

   - match_re: "^[0-9]+ +[0-9]{2}:[0-9]{2}:[0-9]{2} 0 $"

```
</description><key id="26529313">4943</key><summary>[TEST] Added REST test assertion that allows to test apis which don't return json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-29T15:48:06Z</created><updated>2014-06-28T20:43:23Z</updated><resolved>2014-02-03T10:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-31T17:07:39Z" id="33821215">Hi @javanna 

This passes for me with one change: The query string params should be specified as:

```
cat.count:
    help: true
```

rather than:

```
cat.count:
    help: ''
```

as the latter form gets sent as:

```
curl -XGET 'http://localhost:9200/_cat/count?&amp;v=
```

which actually means `false`
</comment><comment author="javanna" created="2014-01-31T17:08:32Z" id="33821307">makes sense, will change that!
</comment><comment author="s1monw" created="2014-02-03T10:34:41Z" id="33939746">LGTM 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>scroll REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4942</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in `/_search/scroll` REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"
curl -XPOST "http://localhost:9200/test/type/1" -d'
{
    "foo": "bar"
}'

# This one works
curl -XPOST "http://localhost:9200/_search/scroll" -d "FAKESCROLLID"

# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}
curl -XGET "http://localhost:9200/_search/scroll/?source=FAKESCROLLID"
```

Closes #4941.
</description><key id="26520599">4942</key><summary>scroll REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-29T13:45:23Z</created><updated>2015-06-07T23:54:08Z</updated><resolved>2014-01-29T14:01:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-29T13:55:05Z" id="33585827">:+1: lgtm!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>scroll REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4941</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in `/_search/scroll` REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"
curl -XPOST "http://localhost:9200/test/type/1" -d'
{
    "foo": "bar"
}'

# This one works
curl -XPOST "http://localhost:9200/_search/scroll" -d "FAKESCROLLID"

# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}
curl -XGET "http://localhost:9200/_search/scroll/?source=FAKESCROLLID"
```
</description><key id="26520102">4941</key><summary>scroll REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-29T13:36:53Z</created><updated>2014-01-29T14:01:46Z</updated><resolved>2014-01-29T14:01:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchScrollAction.java</file></files><comments><comment>scroll REST API should support source parameter</comment></comments></commit></commits></item><item><title>Improve scroll search by using Lucene's IndexSearcher#searchAfter(...)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4940</link><project id="" key="" /><description>Improve the regular scroll search by using Lucene's searchAfter, which allows subsequent scroll request to always have a priority queue size equal to the specified `size` in the first search request. (priority queue is used to collect the competitive hits that match with a query)

Currently the priority queue size grows with each subsequent scroll request with what has been specified in `from` of the first search request.

Note: scan scroll is unaffected by this issue, which already is a highly optimized search to fetch a large part or all docs from a cluster. Scan scroll forcefully sort the hits always by the Lucene docids, while with the regular scroll can now support any sort efficiently.
</description><key id="26518771">4940</key><summary>Improve scroll search by using Lucene's IndexSearcher#searchAfter(...)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-29T13:12:52Z</created><updated>2014-09-30T17:36:55Z</updated><resolved>2014-03-21T06:53:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-29T13:30:58Z" id="33584168">Is the idea to be able to `scroll` a non-`scan` search?
</comment><comment author="martijnvg" created="2014-01-29T13:59:15Z" id="33586144">This is already possible, the `scroll` parameter can also be used on non scan search requests.
</comment><comment author="nik9000" created="2014-01-29T14:15:00Z" id="33587436">Hey, neat.  That is _kinda_ documented on the scroll page but it is implied that scroll is a scan thing.  So this enhancement will make it more efficient to scroll without scan?
</comment><comment author="martijnvg" created="2014-01-29T14:36:13Z" id="33589270">Yes, this enhancement will make scroll without scan more efficient. 

The memory usage will be improved from `O(from+size)` to `O(size)` and also collecting the competitive hits for a query will be improved from `O(numHits + log(from+size))` to `O(numHits + (log(size))`. This improvement becomes really noticeable when scrolling deep into a result set.
</comment><comment author="tlrx" created="2014-03-21T13:17:36Z" id="38274355">nice, thanks for this optimisation!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/ParsedScrollId.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchDfsQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryAndFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchScrollQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/main/java/org/elasticsearch/common/lucene/Lucene.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefOrdValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/BytesRefValComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleScriptDataComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/DoubleValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/GeoDistanceComparator.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/LongValuesComparatorBase.java</file><file>src/main/java/org/elasticsearch/index/fielddata/fieldcomparator/NestedWrappableComparator.java</file><file>src/main/java/org/elasticsearch/index/search/nested/NestedFieldComparatorSource.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/controller/SearchPhaseController.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchSearchRequest.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>src/main/java/org/elasticsearch/search/sort/SortBuilder.java</file><file>src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>src/test/java/org/elasticsearch/search/scroll/DuelScrollTests.java</file><file>src/test/java/org/elasticsearch/search/scroll/SlowDuelScrollTests.java</file><file>src/test/java/org/elasticsearch/search/scroll/SlowSearchScrollTests.java</file></files><comments><comment>Improved regular scroll api by using IndexSearch#searchAfter instead of regular search methods which rely on `from` for pagination.</comment></comments></commit></commits></item><item><title>Field Updation in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4939</link><project id="" key="" /><description>Hi,

i'm facing problem to delete the field values like
"myfiled":["value1","value2","data1","data2"]

i want to delete the values based on values*

expected result like "myfiled":["data1","data2"]

but i'm not able to do 

please help me
</description><key id="26517839">4939</key><summary>Field Updation in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahee127</reporter><labels /><created>2014-01-29T12:54:15Z</created><updated>2014-01-29T13:06:47Z</updated><resolved>2014-01-29T13:06:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-29T13:06:47Z" id="33582559">Please use the mailing list for this sort of questions, we keep github issues for concrete bug/feature reports/requests only.

https://groups.google.com/forum/?fromgroups#!forum/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RemoteTransportException when calling /cluster/pending_tasks during cluster startup/shard recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4938</link><project id="" key="" /><description>Yesterday we upgraded our Elasticsearch production cluster to 0.9.10 (from 0.9.7). One of the endpoints we monitor during this process is the `/cluster/pending_tasks` endpoint to see which tasks are waiting to be executed, and how big the queue is.
However during the whole recovery of the cluster (from the moment all nodes were up and joined the cluster, until the moment all indices were again assigned), we received the following error:

```
RemoteTransportException[
    Failed to deserialize response of type [
        org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksResponse
    ]
]; nested: TransportSerializationException[
    Failed to deserialize response of type [
        org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksResponse
    ]
]; nested: ElasticSearchIllegalArgumentException[
    can't find priority for [
        -61
    ]
];
```

```
ArgumentException[can't find priority for [-61]];
```

Some time after the cluster state went back to green, the endpoint stopped giving that error. (Returning zero pending tasks.)
</description><key id="26516943">4938</key><summary>RemoteTransportException when calling /cluster/pending_tasks during cluster startup/shard recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oemebamo</reporter><labels /><created>2014-01-29T12:36:54Z</created><updated>2015-02-27T20:32:51Z</updated><resolved>2014-07-23T13:52:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T13:52:44Z" id="49876092">Hi @oemebamo 

This appears to be fixed in recent versions.  Please could you reopen if it is still an issue?
</comment><comment author="nirmalc" created="2015-02-17T21:33:12Z" id="74758558">got one few minutes back  with 1.3.4

```
curl -XGET 'http://localhost:9200/_cat/pending_tasks'
{"error":"RemoteTransportException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksResponse]]; nested: TransportSerializationException[Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.tasks.PendingClusterTasksResponse]]; nested: ElasticsearchIllegalArgumentException[can't find priority for [-33]]; ","status":500}

curl localhost:9200
{
  "status" : 200,
  "name" : "stuff-elasticsearch-XXXXXX",
  "version" : {
    "number" : "1.3.4",
    "build_hash" : "a70f3ccb52200f8f2c87e9c370c6597448eb3e45",
    "build_timestamp" : "2014-09-30T09:07:17Z",
    "build_snapshot" : false,
    "lucene_version" : "4.9"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="clintongormley" created="2015-02-27T20:30:04Z" id="76466184">@bleskes is this the same as the issue we saw where task removal tasks were assigned negative priorities?
</comment><comment author="bleskes" created="2015-02-27T20:32:51Z" id="76466621">@clintongormley looks very much like it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Impact of the “_id” field in the search method (0.90.10)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4937</link><project id="" key="" /><description>I have some trouble with elasticsearch... I managed to create a reproducible example on my machine, the code is at the end of the post.

I just create 6 users, "Roger Sand", "Roger Gilbert", "Cindy Sand", "Cindy Gilbert", "Jean-Roger Sands", "Sand Roger", and index it by its names.

Then I run a query to match "Roger Sand", and display the associated score.

Here is the executions of the same script, with 2 set of differents ids : 84046 to 84051 and 84047 to 84052 (just shifted by 1).

The results are not in the same order, and have not the same score :

Execution with 84046...84051

Sand Roger =&gt; 0.8838835
Roger Sand =&gt; 0.2712221
Cindy Sand =&gt; 0.22097087
Jean-Roger Sands =&gt; 0.17677669
Roger Gilbert =&gt; 0.028130025
Execution with 84047..84052

Roger Sand =&gt; 0.2712221
Sand Roger =&gt; 0.2712221
Cindy Sand =&gt; 0.22097087
Jean-Roger Sands =&gt; 0.17677669
Roger Gilbert =&gt; 0.15891947

Here is the code

curl -XDELETE 'http://localhost:9200/test' 
curl -XPUT 'http://localhost:9200/test' 
curl -XPOST 'http://localhost:9200/test/_refresh' 
curl -XPUT 'http://localhost:9200/test/user/84047?op_type=create' -d '{"full_name":"Roger Sand"}'
curl -XPUT 'http://localhost:9200/test/user/84048?op_type=create' -d '{"full_name":"Roger Gilbert"}'
curl -XPUT 'http://localhost:9200/test/user/84049?op_type=create' -d '{"full_name":"Cindy Sand"}'
curl -XPUT 'http://localhost:9200/test/user/84050?op_type=create' -d '{"full_name":"Cindy Gilbert"}'
curl -XPUT 'http://localhost:9200/test/user/84051?op_type=create' -d '{"full_name":"Jean-Roger Sands"}'
curl -XPUT 'http://localhost:9200/test/user/84052?op_type=create' -d '{"full_name":"Sand Roger"}'
curl -XPOST 'http://localhost:9200/test/_refresh' 
curl -XPOST 'http://localhost:9200/test/user/_search?pretty' -d '{"query":{"match":{"full_name":"Roger Sand"}}}'

curl -XDELETE 'http://localhost:9200/test'
curl -XPUT 'http://localhost:9200/test'
curl -XPOST 'http://localhost:9200/test/_refresh'
curl -XPUT 'http://localhost:9200/test/user/84046?op_type=create' -d '{"full_name":"Roger Sand"}'
curl -XPUT 'http://localhost:9200/test/user/84047?op_type=create' -d '{"full_name":"Roger Gilbert"}'
curl -XPUT 'http://localhost:9200/test/user/84048?op_type=create' -d '{"full_name":"Cindy Sand"}'
curl -XPUT 'http://localhost:9200/test/user/84049?op_type=create' -d '{"full_name":"Cindy Gilbert"}'
curl -XPUT 'http://localhost:9200/test/user/84050?op_type=create' -d '{"full_name":"Jean-Roger Sands"}'
curl -XPUT 'http://localhost:9200/test/user/84051?op_type=create' -d '{"full_name":"Sand Roger"}'
curl -XPOST 'http://localhost:9200/test/_refresh'
curl -XPOST 'http://localhost:9200/test/user/_search?pretty' -d '{"query":{"match":{"full_name":"Roger Sand"}}}'

Link to Stackoverflow : http://stackoverflow.com/questions/21426166/impact-of-the-id-field-in-elasticsearch-in-the-search-method
</description><key id="26507237">4937</key><summary>Impact of the “_id” field in the search method (0.90.10)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierallard</reporter><labels /><created>2014-01-29T09:22:32Z</created><updated>2014-01-29T10:32:39Z</updated><resolved>2014-01-29T10:32:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2014-01-29T10:00:02Z" id="33570498">The scoring will always wary with a small data set and the default Elasticsearch index settings of 5 shards.

Use an index with a single shard for a test like this, or use a much bigger data set, so the distribution of the corpus across shards is more balanced.
</comment><comment author="pierallard" created="2014-01-29T10:28:22Z" id="33572350">Even you! Thanks for this quick reply. So this is a configuration problem on my part.
Don't hesitate to close this "fake" issue.
</comment><comment author="karmi" created="2014-01-29T10:32:38Z" id="33572648">@pierallard Cool, closing!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Problems upgrading from elasticsearch 0.90.5 to 0.90.10 with elasticsearch-analysis-stempel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4936</link><project id="" key="" /><description>When trying to upgrade from elasticsearch 0.90.5 with elasticsearch-analysis-stempel 1.7.0 to elasticsearch 0.90.10 with elasticsearch-analysis-stempel 1.9.0 on a multi node cluster I receive the following error:

org.elasticsearch.indices.IndexCreationException: [apps] failed to create index [1]

java version info:
java -version
java version "1.7.0_51"
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode

Let me know if I can provide you with any additional information.

[1] https://gist.github.com/jasonthomas/516f17371d32f64d2565
</description><key id="26492469">4936</key><summary>Problems upgrading from elasticsearch 0.90.5 to 0.90.10 with elasticsearch-analysis-stempel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">jasonthomas</reporter><labels /><created>2014-01-29T01:40:37Z</created><updated>2014-02-25T20:45:40Z</updated><resolved>2014-02-05T22:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="solarce" created="2014-01-31T01:26:05Z" id="33754150">+1, we're seeing this on other clusters we're trying to upgrade to 0.90.10 within Mozilla now too
</comment><comment author="dadoonet" created="2014-02-05T19:53:14Z" id="34230352">Is this happening when you have an existing index created with 0.90.5?
Or is this happening when you create a new index in 0.90.10?

What is your `icu` plugin version BTW?
</comment><comment author="solarce" created="2014-02-05T20:04:05Z" id="34231547">For us, we're trying to upgrade from 0.20.5, as we're having issues with our 0.20.5 clusters having members go into GC when hitting memory limits and becoming unresponsive.

It seems to happen because we have existing indexes created with the stempel plugin, they fail to initialize and the error in https://gist.github.com/jasonthomas/516f17371d32f64d2565 appears in the log numerous times. 

https://bugzilla.mozilla.org/show_bug.cgi?id=960172#c10 is tracking this work

The cluster in the bug above doesn't have the ICU plugin

$ sudo rpm -qa | grep ela                                                                                                                          [12:03:01]
elasticsearch-plugin-site-bigdesk-2012072700-114752cc5b.noarch
elasticsearch-plugin-analysis-stempel-1.2.0-1.el6.noarch
elasticsearch-0.20.5-2.el6.x86_64
elasticsearch-plugin-site-paramedic-2012072302-fbfd358f7f.noarch
elasticsearch-plugin-site-head-2012072400-abdf1ef083.noarch

bburton@elasticsearch1: /usr/share/java/elasticsearch/plugins
$ ls -lah                                                                                                                                          [12:03:25]
total 24K
drwxr-xr-x 6 root root 4.0K Mar 11  2013 .
drwxr-xr-x 5 root root 4.0K Mar 11  2013 ..
drwxr-xr-x 2 root root 4.0K Jan 30 16:24 analysis-stempel
drwxr-xr-x 3 root root 4.0K Oct 30  2012 bigdesk
drwxr-xr-x 3 root root 4.0K Oct 30  2012 head
drwxr-xr-x 3 root root 4.0K Oct 30  2012 paramedic
</comment><comment author="solarce" created="2014-02-05T20:20:56Z" id="34234421">Just wanted to note, I'm idling in #elasticsearch if you want to talk in more real time
</comment><comment author="kimchy" created="2014-02-05T20:22:21Z" id="34234794">is there anything in the logs correlating to the index creation failure?
</comment><comment author="cyl-moz" created="2014-02-05T20:46:46Z" id="34240376">The only errors I see are that, after the initial "failed to create index" errors occurs, are messages like:

```
[2014-01-30 14:54:32,937][DEBUG][action.search.type       ] [elasticsearch1_dev] All shards failed for phase: [query]
```
</comment><comment author="dadoonet" created="2014-02-05T20:48:01Z" id="34240666">I can reproduce it:

``` sh
wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.5.tar.gz; tar xzf elasticsearch-0.90.5.tar.gz; cd elasticsearch-0.90.5
bin/plugin -install elasticsearch/elasticsearch-analysis-stempel/1.7.0
bin/elasticsearch -f
```

``` sh
curl -XPUT localhost:9200/test
```

``` sh
wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.11.tar.gz; tar xzf elasticsearch-0.90.11.tar.gz; cd elasticsearch-0.90.11
bin/plugin -install elasticsearch/elasticsearch-analysis-stempel/1.9.0
cp -r elasticsearch-0.90.5/data elasticsearch-0.90.11
bin/elasticsearch -f
```

It gives the error mentioned before.

This issue comes from: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactory.java#L44
</comment><comment author="dadoonet" created="2014-02-05T22:09:38Z" id="34258594">I opened issue #5030 to track it.
It could happen with other analysis plugins. For example Kuromoji is concerned as well.

Note that we will release very soon patched analysis plugins to fix that without forcing to upgrade elasticsearch.

Closing this issue for now.

Thanks for reporting and for all details you all provided!
</comment><comment author="dadoonet" created="2014-02-05T22:36:34Z" id="34264862">Heya!

Could you please check if this new version works for you?

```
bin/plugin -install elasticsearch/elasticsearch-analysis-stempel/1.10.0
```

Thanks!
</comment><comment author="cyl-moz" created="2014-02-25T20:45:40Z" id="36055845">The new version seems to be working fine.  (Upgraded without issue and there have been no complaints for the past two weeks.)  Thanks for the quick response.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerProviderFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltCharFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenFilterFactoryFactory.java</file><file>src/main/java/org/elasticsearch/index/analysis/PreBuiltTokenizerFactoryFactory.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltCharFilters.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenFilters.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltTokenizers.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalysisBinderProcessor.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalysisPlugin.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalyzer.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyAnalyzerProvider.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyCharFilterFactory.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyIndicesAnalysis.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyIndicesAnalysisModule.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyTokenFilterFactory.java</file><file>src/test/java/org/elasticsearch/indices/analysis/DummyTokenizerFactory.java</file><file>src/test/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzerIntegrationTests.java</file></files><comments><comment>Upgrading analysis plugins fails</comment></comments></commit></commits></item><item><title>Nested aggregation documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4935</link><project id="" key="" /><description>The example response in the [nested aggregation documentation](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/search/aggregations/bucket/nested-aggregation.asciidoc) should read:

``` javascript
{
    "aggregations": {
        "resellers": {
            "min_price": {
               "value": 350
            }
        }
    }
}
```
</description><key id="26487438">4935</key><summary>Nested aggregation documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlinn</reporter><labels /><created>2014-01-28T23:50:57Z</created><updated>2014-01-29T12:10:48Z</updated><resolved>2014-01-29T12:10:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-29T12:01:43Z" id="33578575">indeed, thx!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>fixed nested example response in docs</comment></comments></commit></commits></item><item><title>Add explanations for all AllocationDeciders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4934</link><project id="" key="" /><description>This adds explanations for all of the allocation deciders for their `yes` and `no` answers. It should help when using the reroute API to explain why a shard can or cannot be moved to a different node.

I would like to move to a full explain-like API for shard allocation, but I wanted to submit this as a separate PR since it can easily be backported to all branches to be useful without any breaking changes.

I tried to keep the explanations short but distinct.

Related to #4380 and #2483
</description><key id="26487107">4934</key><summary>Add explanations for all AllocationDeciders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v0.90.11</label><label>v1.0.0</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T23:44:28Z</created><updated>2015-06-07T15:47:27Z</updated><resolved>2014-01-31T17:47:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-29T08:40:58Z" id="33565540">@dakrone great usability feature

Might make more sense to use static fields for all the strings being used?
</comment><comment author="kimchy" created="2014-01-29T08:52:41Z" id="33566206">If we have this explanation, where will it be used?

sync with @uboness, he tried to tackle this a bit, and should have a branch with many more explanations. I wonder how / where would you want to use that info? We found logging to be close to useless.

An idea was to allow to run reroute API in "debug" mode, and gather the decisions made, and return them as part of the reroute response. But note that with the way the balanced shard allocator works, its going to be _very_ verbose. I believe @uboness tried it, and it ended being so verbose that again became useless.

Another option is to allow in reroute to give a shard, and a node, and return why this shard is not allocated on a node.
</comment><comment author="kimchy" created="2014-01-29T09:04:00Z" id="33566844">ahh, I see when it will be used, in the reroute when we return the list of decisions of why we can't move one shard to another for example.

It would be nice to have those explanations only enabled we we want them. This run will create a lot of garbage during normal operation that ends up calling the deciders a _lot_.

Also, for example, canAllocate does an early break on NO, where its used in move command in reroute as an example, I would not want to do an early break on NO in the move command case, so the full explanation on why a shard can't be moved to a node will be provided.

I am thinking of a decision debug flag on `RoutingAllocation`, that when enabled, we will not shortcut on NO decisions, and create Decision.single instead of using the enum Decision.NO (this can be abstracted in a method like "RoutingAllocation#decision(Enum, String, params)", that returns the full decision only when debug is enabled.

@spinscale I don't think we need static vars for strings, we only have them once? its cruft?
</comment><comment author="synhershko" created="2014-01-29T09:15:45Z" id="33567580">@kimchy yes, see #4380 

Thanks for this guys, it looks great

IMO it should always be on when called from the reroute API, otherwise it probably is easier to just return a YES/NO value, though it may be worthwhile to have some decisions logged (low hard disk space is one example that comes to mind where you want this logged).
</comment><comment author="kimchy" created="2014-01-29T09:17:04Z" id="33567663">@synhershko agreed, having this debug flag turned on for the explicit reroute API call makes sense.
</comment><comment author="dakrone" created="2014-01-29T18:44:59Z" id="33615956">Added changes that delegate to RoutingAllocation.decision() that only includes the reason if a debug flag is set to true. The debug flag defaults to false, being set only in the case where the reroute API is used.

Also made the decisions not short-circuit if the debug flag is true.
</comment><comment author="kimchy" created="2014-01-30T19:30:25Z" id="33723793">looks great!. I am missing one more thing, when a no or throttle decision is made (or even YES...), a lot of times is because a some sort of threshold matched or not. I would love to see those values in the message we associate when in debug mode.

I would add `Object... args` to the decision method, and call `String#format` on the text with args when in debug mode. Then, in all the places where we provide a debug message, add more info on relevant values that caused that decision.
</comment><comment author="dakrone" created="2014-01-30T19:31:46Z" id="33723941">That's a good idea, I will make that change.
</comment><comment author="dakrone" created="2014-01-30T22:11:30Z" id="33739890">Added the parameter passing and constraints for the Deciders where it makes sense. Also added a `.toString()` method for the DiscoverNodeFilters so they're human readable now.
</comment><comment author="kimchy" created="2014-01-30T22:16:09Z" id="33740366">LGTM, this is great!.
</comment><comment author="s1monw" created="2014-01-31T20:24:04Z" id="33838959">very cool stuff I think we should backport this to `0.90.12` as well as `1.0.0.RC2`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node fails to shutdown cleanly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4933</link><project id="" key="" /><description>Trying to shutdown two data nodes (search-05-a and -b) of 12 within our cluster resulted in the cluster going into a strange state.  The node stats api wouldn't work and nothing was being indexed despite the cluster status being yellow.  We are running 0.90.7.  Shutting down the cluster fixed the problem but I had to kill -9 search-05-a.  A and B nodes are on the same host but we have rack awareness set up so ES does not place both primary and replica shards on the same host. 

-drew

From the master log:
[2014-01-28 13:35:58,333][INFO ][action.admin.cluster.node.shutdown] [search-m00] [partial_cluster_shutdown]: requested, shutting down [[YBVVeR6MQIOlLVqr8B9rQg]] in [1s]
[2014-01-28 13:35:59,338][INFO ][action.admin.cluster.node.shutdown] [search-m00] [partial_cluster_shutdown]: done shutting down [[YBVVeR6MQIOlLVqr8B9rQg]]
[2014-01-28 13:36:01,967][ERROR][discovery.zen            ] [search-m00] unexpected failure during [zen-disco-node_left([search-05-a][YBVVeR6MQIOlLVqr8B9rQg][inet[/10.2.34.50:9300]]{host=search-05, master=
false})]
java.lang.UnsupportedOperationException
        at org.elasticsearch.common.hppc.AbstractIterator.remove(AbstractIterator.java:58)
        at org.elasticsearch.gateway.local.LocalGatewayAllocator.buildShardStates(LocalGatewayAllocator.java:373)
        at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:124)
        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:73)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:204)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:156)
        at org.elasticsearch.discovery.zen.ZenDiscovery$3.execute(ZenDiscovery.java:382)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:298)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2014-01-28 13:36:01,990][ERROR][discovery.zen            ] [search-m00] unexpected failure during [zen-disco-node_failed([search-05-a][YBVVeR6MQIOlLVqr8B9rQg][inet[10.2.34.50/10.2.34.50:9300]]{host=search-05, master=false}), reason transport disconnected (with verified connect)]
java.lang.UnsupportedOperationException
        at org.elasticsearch.common.hppc.AbstractIterator.remove(AbstractIterator.java:58)
        at org.elasticsearch.gateway.local.LocalGatewayAllocator.buildShardStates(LocalGatewayAllocator.java:373)
        at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:124)
        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:73)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:204)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:156)
        at org.elasticsearch.discovery.zen.ZenDiscovery$4.execute(ZenDiscovery.java:417)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:298)

From the search-05 log:

[2014-01-28 14:36:17,556][WARN ][indices.cluster          ] [search-05-a] [messages_20140125][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [messages_20140125][2] failed to fetch index version after copying it over
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:136)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [messages_20140125][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_44f.fdt, _49p_es090_0.doc, _3b9_es090_0.pay, _1vb_es090_0.pay, _3uz_es090_0.pos, _48l_8.del, _3eo.nvm, _3b9.nvd, _4bz.fnm, _41d.fnm, _4c7_es090_0.pos, _41d.fdx, _3rc.nvd, _3uz.nvm, _3uz.fdt, _3eo_es090_0.tim, segments_ah, _4aa.nvm, _48l.nvd, _44f_es090_0.doc, _3eo.fnm, _4c3.cfs, _44f_es090_0.pos, _3b9.fdt, _4bd.fdx, _44f_7.del, _3uz.nvd, _3o9.nvm, _3rc.fnm, _462_3.del, _4bd_es090_0.blm, _48l_es090_0.blm, _3eo_es090_0.pay, _44f.fdx, _4aa_es090_0.tim, _4bz_1.del, _462.cfs, _3uz_es090_0.tim, _4bz.nvd, _4bd_es090_0.doc, _4bh.cfe, _49p.fnm, _41d_es090_0.pos, _3rc.nvm, _4bh.si, _3o9_es090_0.doc, _3rc_es090_0.blm, _49p.nvd, _3b9_1d.del, _44f.nvm, segments.gen, _4bd_3.del, _4c7.nvd, _4b7.cfs, _3l5.si, _44f_es090_0.tip, _3b9_es090_0.tim, _3b9.fdx, _4c7_es090_0.pay, _3rc.fdx, _4c3.cfe, _3rc_es090_0.pay, _44f.fnm, _4c0_2.del, _4bz_es090_0.pay, _3l5_es090_0.doc, _3b9.fnm, _49p.nvm, _49p_es090_0.blm, _4c0.si, _41d.fdt, _1vb.nvm, _4bz_es090_0.tim, _4b7_2.del, _48l.si, _48l_es090_0.tim, _3l5_es090_0.pos, _4bz_es090_0.blm, _3l5.nvd, _4bd.nvm, _4au.si, _4au_3.del, _4aa_es090_0.doc, _4bh.cfs, _49p_2.del, _1vb_es090_0.blm, _3rc.si, _4bz_es090_0.tip, _4bz_es090_0.doc, _41d.nvm, _3b9_es090_0.tip, _3uz_es090_0.blm, _3l5.fdx, _4c7.nvm, _4ax_4.del, _1vb_es090_0.pos, _4ad.cfs, _4bd.fnm, _49p_es090_0.tim, _3l5_es090_0.tim, _48l.fdx, _3rc_es090_0.doc, _41d_es090_0.tip, _3uz_es090_0.tip, _4aa_es090_0.tip, _3rc_es090_0.tip, _4c7_es090_0.doc, _3o9_l.del, _4aa.si, _3eo.fdx, _3o9.fdx, _3eo_es090_0.doc, _48l.fdt, _49p.si, _46i.cfe, _4b7.si, _3o9_es090_0.pay, _4ad_4.del, _3rc_es090_0.pos, _41d_i.del, _44f.nvd, _1vb_21.del, _49p_es090_0.pos, _4bz.fdx, _4c0.cfe, _3uz_es090_0.pay, _3l5_es090_0.pay, _41d_es090_0.blm, _41d_es090_0.tim, _4aa.fdx, _4bz.nvm, _4bd_es090_0.tim, _4ad.si, _4bd_es090_0.pay, _3b9_es090_0.blm, _3l5.fnm, _4aa.nvd, _48l.fnm, _3rc.fdt, _48l_es090_0.pos, _3eo_es090_0.blm, _4bz.fdt, _3b9.nvm, _3b9_es090_0.doc, _4b7.cfe, _4aa_es090_0.pay, _4ax.cfs, _1vb.nvd, _4aa_5.del, _41d_es090_0.doc, _48l_es090_0.doc, _3o9_es090_0.blm, _3b9.si, _3rc_r.del, _41d.nvd, _3uz_c.del, _44f_es090_0.blm, _3uz.fdx, _41d.si, _4bd_es090_0.pos, _3eo_es090_0.pos, _3o9.fdt, _4aa.fdt, _46i.cfs, _4bz_es090_0.pos, _3eo.si, _48l.nvm, _44f_es090_0.pay, _3uz.si, _49p.fdt, _48l_es090_0.tip, _4c3.si, _46i.si, _3eo_es090_0.tip, _4aa.fnm, _4c7_es090_0.tip, _44f_es090_0.tim, _4bd.si, _4c0.cfs, _3eo_z.del, _1vb.fdt, _4c7.fdx, _3l5_es090_0.tip, _3o9.nvd, _3eo.nvd, _49p.fdx]
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:115)
        ... 4 more
Caused by: java.io.FileNotFoundException: _1vb.si
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:456)
        at org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoReader.read(Lucene40SegmentInfoReader.java:50)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:334)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:380)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:812)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:663)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:376)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:111)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:106)
        ... 4 more
[2014-01-28 14:36:17,910][WARN ][cluster.action.shard     ] [search-05-a] [messages_20140125][2] sending failed shard for [messages_20140125][2], node[N-3a4USES9GRcRVtH1tTOA], [P], s[INITIALIZING], indexUUID [wR7FwxEFSnCc_75FfGa39Q], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[messages_20140125][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[messages_20140125][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_44f.fdt, _49p_es090_0.doc, _3b9_es090_0.pay, _1vb_es090_0.pay, _3uz_es090_0.pos, _48l_8.del, _3eo.nvm, _3b9.nvd, _4bz.fnm, _41d.fnm, _4c7_es090_0.pos, _41d.fdx, _3rc.nvd, _3uz.nvm, _3uz.fdt, _3eo_es090_0.tim, segments_ah, _4aa.nvm, _48l.nvd, _44f_es090_0.doc, _3eo.fnm, _4c3.cfs, _44f_es090_0.pos, _3b9.fdt, _4bd.fdx, _44f_7.del, _3uz.nvd, _3o9.nvm, _3rc.fnm, _462_3.del, _4bd_es090_0.blm, _48l_es090_0.blm, _3eo_es090_0.pay, _44f.fdx, _4aa_es090_0.tim, _4bz_1.del, _462.cfs, _3uz_es090_0.tim, _4bz.nvd, _4bd_es090_0.doc, _4bh.cfe, _49p.fnm, _41d_es090_0.pos, _3rc.nvm, _4bh.si, _3o9_es090_0.doc, _3rc_es090_0.blm, _49p.nvd, _3b9_1d.del, _44f.nvm, segments.gen, _4bd_3.del, _4c7.nvd, _4b7.cfs, _3l5.si, _44f_es090_0.tip, _3b9_es090_0.tim, _3b9.fdx, _4c7_es090_0.pay, _3rc.fdx, _4c3.cfe, _3rc_es090_0.pay, _44f.fnm, _4c0_2.del, _4bz_es090_0.pay, _3l5_es090_0.doc, _3b9.fnm, _49p.nvm, _49p_es090_0.blm, _4c0.si, _41d.fdt, _1vb.nvm, _4bz_es090_0.tim, _4b7_2.del, _48l.si, _48l_es090_0.tim, _3l5_es090_0.pos, _4bz_es090_0.blm, _3l5.nvd, _4bd.nvm, _4au.si, _4au_3.del, _4aa_es090_0.doc, _4bh.cfs, _49p_2.del, _1vb_es090_0.blm, _3rc.si, _4bz_es090_0.tip, _4bz_es090_0.doc, _41d.nvm, _3b9_es090_0.tip, _3uz_es090_0.blm, _3l5.fdx, _4c7.nvm, _4ax_4.del, _1vb_es090_0.pos, _4ad.cfs, _4bd.fnm, _49p_es090_0.tim, _3l5_es090_0.tim, _48l.fdx, _3rc_es090_0.doc, _41d_es090_0.tip, _3uz_es090_0.tip, _4aa_es090_0.tip, _3rc_es090_0.tip, _4c7_es090_0.doc, _3o9_l.del, _4aa.si, _3eo.fdx, _3o9.fdx, _3eo_es090_0.doc, _48l.fdt, _49p.si, _46i.cfe, _4b7.si, _3o9_es090_0.pay, _4ad_4.del, _3rc_es090_0.pos, _41d_i.del, _44f.nvd, _1vb_21.del, _49p_es090_0.pos, _4bz.fdx, _4c0.cfe, _3uz_es090_0.pay, _3l5_es090_0.pay, _41d_es090_0.blm, _41d_es090_0.tim, _4aa.fdx, _4bz.nvm, _4bd_es090_0.tim, _4ad.si, _4bd_es090_0.pay, _3b9_es090_0.blm, _3l5.fnm, _4aa.nvd, _48l.fnm, _3rc.fdt, _48l_es090_0.pos, _3eo_es090_0.blm, _4bz.fdt, _3b9.nvm, _3b9_es090_0.doc, _4b7.cfe, _4aa_es090_0.pay, _4ax.cfs, _1vb.nvd, _4aa_5.del, _41d_es090_0.doc, _48l_es090_0.doc, _3o9_es090_0.blm, _3b9.si, _3rc_r.del, _41d.nvd, _3uz_c.del, _44f_es090_0.blm, _3uz.fdx, _41d.si, _4bd_es090_0.pos, _3eo_es090_0.pos, _3o9.fdt, _4aa.fdt, _46i.cfs, _4bz_es090_0.pos, _3eo.si, _48l.nvm, _44f_es090_0.pay, _3uz.si, _49p.fdt, _48l_es090_0.tip, _4c3.si, _46i.si, _3eo_es090_0.tip, _4aa.fnm, _4c7_es090_0.tip, _44f_es090_0.tim, _4bd.si, _4c0.cfs, _3eo_z.del, _1vb.fdt, _4c7.fdx, _3l5_es090_0.tip, _3o9.nvd, _3eo.nvd, _49p.fdx]]; nested: FileNotFoundException[_1vb.si]; ]]
</description><key id="26481714">4933</key><summary>Node fails to shutdown cleanly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djdaugherty612</reporter><labels /><created>2014-01-28T22:20:38Z</created><updated>2014-02-03T17:29:25Z</updated><resolved>2014-02-03T17:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-29T01:58:33Z" id="33549470">I think this issue was fixed in 0.90.8 by d1d93c5
</comment><comment author="djdaugherty612" created="2014-01-29T22:39:22Z" id="33639956">Thaks Igor, but what is the best way to recover from this?  In my case I restarted the cluster.  Is there a better way?  Can I just kill the offending node?
</comment><comment author="martijnvg" created="2014-01-30T09:23:38Z" id="33671661">Unfortunately the only way to recover safely is to bring down all your nodes and upgrade them. If only offending 0.90.7 nodes are brought down and upgraded it is possible that other 0.90.7 nodes may still remove shards unintentionally.
</comment><comment author="djdaugherty612" created="2014-01-30T23:00:08Z" id="33744351">Thanks Martijn.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix potential NPE when no source and no body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4932</link><project id="" key="" /><description>In recent changes, we added missing support for `source` parameter in some REST APIs:
- #4892 : mget
- #4900 : mpercolate
- #4901 : msearch
- #4902 : mtermvectors
- #4903 : percolate

``` java
        BytesReference content = null;
        if (request.hasContent()) {
            content = request.content();
        } else {
            String source = request.param("source");
            if (source != null) {
                content = new BytesArray(source);
            }
        }
```

It's definitely better to have:

``` java
        BytesReference content = request.content();
        if (!request.hasContent()) {
            String source = request.param("source");
            if (source != null) {
                content = new BytesArray(source);
            }
        }
```

That said, it could be nice to have a single method to manage it for various REST actions.

Closes #4924.
</description><key id="26457607">4932</key><summary>Fix potential NPE when no source and no body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T16:59:37Z</created><updated>2015-06-07T23:55:38Z</updated><resolved>2014-01-28T17:30:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-28T17:08:30Z" id="33499597">aside of the comment this LGTM and is good to go
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Asciifolding filter emit original token</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4931</link><project id="" key="" /><description>I'm looking to make asciifolding optional in my (English) index.  If the user searches without any high ascii characters then I want to match against the folded tokens.  If the user searches with high ascii characters then I only want to match the unfolded tokens.

I think the right way to implement this would be to get the `asciifolding` filter to spit out both the folded and unfolded tokens during indexing with 0 position increment between them and to not use it at all during searching.  Is that possible right now?  If not would Elasticsearch like me to make one?  If so, does it make sense to add a subclass of the `asciifoling` filter that supports the `keyword` parameter as though it were a stemmer?  
</description><key id="26452455">4931</key><summary>Asciifolding filter emit original token</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-28T15:58:58Z</created><updated>2014-02-14T18:46:14Z</updated><resolved>2014-02-14T18:46:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-28T16:00:20Z" id="33492016">Sorry for filing a question as an issue.  I didn't quite think this through.  If a code change is required I'll reopen.
</comment><comment author="nik9000" created="2014-02-06T22:07:16Z" id="34377942">It looks like there is no way to do this now so I filed [an issue with Lucene](https://issues.apache.org/jira/browse/LUCENE-5437) and sent a patch.  I'll port it over to Elasticsearch and get it registered in the analysis infrastructure once it gets merged into Lucene.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/analysis/miscellaneous/XASCIIFoldingFilter.java</file><file>src/main/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactory.java</file><file>src/test/java/org/elasticsearch/index/analysis/ASCIIFoldingTokenFilterFactoryTests.java</file></files><comments><comment>Add preserve original token option to ASCIIFolding</comment></comments></commit></commits></item><item><title>Move parent/child over from id cache to field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4930</link><project id="" key="" /><description>Move all parent/child queries (has_child, has_parent, top_children) from id cache to field data. This has a number of advantages:
- Parent/child memory footprint will get reduced by using field data, compared to what it now takes with id cache. The id cache use concrete object arrays to store the parent ids which is wasteful in terms of memory usage compared the field data which uses native byte arrays to store the parent ids (via Lucene's PagedBytes). Initial benchmarks have shown that the memory usage can be reduced up to half with parent/child using field data.
- Parent child can use paged data structures because field data uses paged data structures under the hood as well. This will result in a better stability because on the jvm level, because of less garbage collection, which boils down to the fact that the storage behind paged data structures is reused between requests and paged data structures taking less memory in general compared to the concrete object arrays in id cache. 
- By reusing the field data parent/child can reuse its infrastructure For example using the CircuitBreaker to fail search requests if too much memory is being spent on parent/child rather then going out of memory.
- The id cache is similar to field data in a sense that represents field values into memory by removing the id cache a lot of duplicate logic / code will be removed.

These advantages come at a cost of a small performance loss of up to 10% in query time execution, but the advantages outweigh the performance loss in terms of stability, predictability (less sudden gc collections) and less memory usage. 

The id cache can be removed, since nothing inside ES is using it. For backward compatibility reason in 1.x releases the id cache statistics will be reported as was before, but it will be based on the `_parent` field in field data and the `_parent` field will not be reported in field data statistics.
</description><key id="26439374">4930</key><summary>Move parent/child over from id cache to field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T13:41:45Z</created><updated>2014-02-26T19:47:17Z</updated><resolved>2014-02-26T19:02:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java</file><file>src/main/java/org/elasticsearch/common/util/AbstractBigArray.java</file><file>src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>src/main/java/org/elasticsearch/common/util/BigFloatArray.java</file><file>src/main/java/org/elasticsearch/common/util/FloatArray.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCache.java</file><file>src/main/java/org/elasticsearch/index/cache/IndexCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/id/IdCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/IdCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/id/IdCacheStats.java</file><file>src/main/java/org/elasticsearch/index/cache/id/IdReaderCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/ShardIdCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/ShardIdCacheModule.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdReaderCache.java</file><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdReaderTypeCache.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ShardFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointBinaryDVIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildFilteredTermsEnum.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/ParentChildIntersectTermsEnum.java</file><file>src/main/java/org/elasticsearch/index/mapper/Uid.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentConstantScoreQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentIdCollector.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentIdsFilter.java</file><file>src/main/java/org/elasticsearch/index/search/child/ParentQuery.java</file><file>src/main/java/org/elasticsearch/index/search/child/TopChildrenQuery.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/IndexShard.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/test/java/org/elasticsearch/index/cache/id/SimpleIdCacheTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/ParentChildFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/plain/ParentChildFilteredTermsEnumTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/UidTests.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterCachingTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/index/search/FieldDataTermsFilterTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ParentQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file><file>src/test/java/org/elasticsearch/test/index/service/StubIndexService.java</file></files><comments><comment>Migrated p/c queries from id cache to field data. Changed p/c queries to use paging data structures (BytesRefHash, BigFloatArray, BigIntArray) instead of hppc maps / sets.</comment><comment>Also removed the id cache.</comment></comments></commit></commits></item><item><title>Move Aggregations reduce phase to use Paged recycler enabled structures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4929</link><project id="" key="" /><description>We moved the shard level computation to use paged recycler based data structures, it would be great to also use it in the reduce phase.
</description><key id="26437251">4929</key><summary>Move Aggregations reduce phase to use Paged recycler enabled structures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T13:05:00Z</created><updated>2014-03-13T16:58:39Z</updated><resolved>2014-03-13T15:53:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/AbstractHash.java</file><file>src/main/java/org/elasticsearch/common/util/AbstractPagedHashMap.java</file><file>src/main/java/org/elasticsearch/common/util/BytesRefHash.java</file><file>src/main/java/org/elasticsearch/common/util/DoubleObjectPagedHashMap.java</file><file>src/main/java/org/elasticsearch/common/util/LongHash.java</file><file>src/main/java/org/elasticsearch/common/util/LongObjectPagedHashMap.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/InternalAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/InternalSingleBucketAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/controller/SearchPhaseController.java</file><file>src/test/java/org/elasticsearch/common/util/DoubleObjectHashMapTests.java</file><file>src/test/java/org/elasticsearch/common/util/LongObjectHashMapTests.java</file></files><comments><comment>Make aggregations CacheRecycler-free.</comment></comments></commit></commits></item><item><title>Use num of actual threads if busiestThreads is larger</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4928</link><project id="" key="" /><description>We currently use the number of hot threads that we are
interested in as the value for iterating over the actual
hot threads which can lead to AIOOB is the actual number
of threads is less than the given number.

Closes #4927
</description><key id="26437067">4928</key><summary>Use num of actual threads if busiestThreads is larger</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Stats</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T13:01:02Z</created><updated>2015-06-07T23:54:57Z</updated><resolved>2014-01-28T13:33:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>HotThreads fail with AIOOB if busiestThreads &gt; actual threads </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4927</link><project id="" key="" /><description>we currently use the number of hot threads that we are interested in as the value for iterating over the actual hot threads which can lead to AIOOB is the actual number of threads is less than the given number.

which will result in an exception like this:

```
Caused by: java.lang.IndexOutOfBoundsException: Index: 93, Size: 93
    at java.util.ArrayList.RangeCheck(ArrayList.java:547)
    at java.util.ArrayList.get(ArrayList.java:322)
    at org.elasticsearch.monitor.jvm.HotThreads.innerDetect(HotThreads.java:149)
    at org.elasticsearch.monitor.jvm.HotThreads.detect(HotThreads.java:75)
    at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:101)
    ... 5 more
```
</description><key id="26435998">4927</key><summary>HotThreads fail with AIOOB if busiestThreads &gt; actual threads </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T12:38:58Z</created><updated>2014-01-28T13:32:59Z</updated><resolved>2014-01-28T13:32:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/jvm/HotThreads.java</file><file>src/test/java/org/elasticsearch/action/admin/HotThreadsTest.java</file></files><comments><comment>Use num of actual threads if busiestThreads is larger</comment></comments></commit></commits></item><item><title>Rest API needs to be consistent across all multi-bucket aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4926</link><project id="" key="" /><description>currently some multi-bucket aggs (e.g. `histogram`) return an array, others (e.g. `terms`) returns an object that holds a `buckets` array.

The preferred format is that of the `terms` agg, as it leaves room for future extensions (where we might want to add top level info with the agg response unrelated to a particular bucket). 
</description><key id="26435853">4926</key><summary>Rest API needs to be consistent across all multi-bucket aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T12:36:15Z</created><updated>2015-06-07T15:52:21Z</updated><resolved>2014-01-28T16:57:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java</file></files><comments><comment>Made all multi-bucket aggs return consistent response format</comment></comments></commit></commits></item><item><title>Script fields disable _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4925</link><project id="" key="" /><description>```
GET /_search
{
    "script_fields": {
          "foo": {  "script": "some script"}
    }
}
```

returns `foo` in the `fields` block, but disables the `_source` field.  I can understand why this happens if you specify a `fields` parameter, but `script_fields` are generated and so you may well want the script field PLUS the source.

Is the right default to disable returning `_source` as it is today? Or should we default to returning `_source`?
</description><key id="26430465">4925</key><summary>Script fields disable _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-01-28T10:52:05Z</created><updated>2014-12-24T18:59:13Z</updated><resolved>2014-12-24T18:59:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-28T11:42:09Z" id="33471241">the reasoning behind it is that when someone picks and choose what they want to get with script, then typically, there is no need for _source, and it should be an opt in aspect.
</comment><comment author="clintongormley" created="2014-12-24T18:59:13Z" id="68069640">Agreed - closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix potential NPE when no source and no body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4924</link><project id="" key="" /><description>In recent changes, we added missing support for `source` parameter in some REST APIs:
- #4892 : mget
- #4900 : mpercolate 
- #4901 : msearch
- #4902 : mtermvectors
- #4903 : percolate

``` java
        BytesReference content = null;
        if (request.hasContent()) {
            content = request.content();
        } else {
            String source = request.param("source");
            if (source != null) {
                content = new BytesArray(source);
            }
        }
```

It's definitely better to have:

``` java
        BytesReference content = request.content();
        if (!request.hasContent()) {
            String source = request.param("source");
            if (source != null) {
                content = new BytesArray(source);
            }
        }
```

That said, it could be nice to have a single method to manage it for various REST actions.
Where should we put it in?
</description><key id="26430307">4924</key><summary>Fix potential NPE when no source and no body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T10:49:16Z</created><updated>2014-01-28T17:30:01Z</updated><resolved>2014-01-28T17:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>Fix potential NPE when no source and no body</comment></comments></commit></commits></item><item><title>Added support for aliases to create index api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4923</link><project id="" key="" /><description>It is now possible to specify aliases during index creation:

```
curl -XPUT 'http://localhost:9200/test' -d '
{
    "aliases" : {
        "alias1" : {},
        "alias2" : {
            "filter" : { "term" : {"field":"value"}}
        }
    }
}'
```

Closes #4920
</description><key id="26427966">4923</key><summary>Added support for aliases to create index api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label></labels><created>2014-01-28T10:07:29Z</created><updated>2014-06-13T17:42:41Z</updated><resolved>2014-02-17T14:17:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-29T08:47:38Z" id="33565900">looks good
</comment><comment author="javanna" created="2014-01-29T09:32:51Z" id="33568717">Thanks for the review @spinscale! I realized I missed a few bits though when it comes to validating the aliases and their filter, I've been working on that and will update the PR soon.
</comment><comment author="javanna" created="2014-02-05T14:53:48Z" id="34182156">PR updated with aliases (and filters) validation and previously missing bits :)
</comment><comment author="s1monw" created="2014-02-06T14:57:30Z" id="34330011">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cleanup aggs java api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4922</link><project id="" key="" /><description>- add javadocs
- remove Iterable from all multi-bucket aggregations
- all single-bucket aggregations should have getDocCount() and getAggregations()
- all multi-bucket aggregations should have getBuckets() that returns Collection
- every multi-bucket aggregation should have these methods:
  - getBuckets() : Collection
  - getBucketByKey(String) : Bucket
  - getBucketByKey(Number) : Bucket (only for numeric buckets)
  - getBucketByKey(DateTime) : Bucket (only for date buckets)
  - getBucketByKey(GeoPoint) : Bucket (only for geohash buckets)
- every bucket in all multi-bucket aggregations should have these methods:
  - getKey() : String
  - getKeyAsText() : Text
  - getKeyAsNumber() : Number (if the key can be numeric value, eg. range &amp; histograms)
  - getKeyAsGeoPoint() : GeoPoint (in case of the geohash_grid agg)
</description><key id="26426557">4922</key><summary>Cleanup aggs java api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T09:41:33Z</created><updated>2014-01-29T09:18:06Z</updated><resolved>2014-01-28T12:28:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/Bucket.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/InternalSingleBucketAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/MultiBucketsAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/SingleBucketAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/SingleBucketAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/Filter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/InternalFilter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGrid.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/global/Global.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/global/InternalGlobal.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/Histogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/missing/InternalMissing.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/missing/Missing.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/InternalNested.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/Nested.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/InternalRange.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/Range.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRange.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/InternalDateRange.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistance.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/InternalGeoDistance.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4Range.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IPv4RangeBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/InternalIPv4Range.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java</file><file>src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperIntegrationTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorFacetsAndAggregationsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/RandomTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FilterTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MinDocCountTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MissingTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/NestedTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java</file></files><comments><comment>cleanup of aggregations api</comment><comment>- add javadocs</comment><comment>- remove Iterable from all multi-bucket aggregations</comment><comment>- all single-bucket aggregations should have getDocCount() and getAggregations()</comment><comment>- all multi-bucket aggregations should have getBuckets() that returns Collection</comment><comment>- every multi-bucket aggregation should have these methods:</comment><comment> - getBuckets() : Collection</comment><comment> - getBucketByKey(String) : Bucket</comment><comment> - getBucketByKey(Number) : Bucket (only for numeric buckets)</comment><comment> - getBucketByKey(DateTime) : Bucket (only for date buckets)</comment><comment> - getBucketByKey(GeoPoint) : Bucket (only for geohash_grid)</comment><comment>- every bucket in all multi-bucket aggregations should have these methods:</comment><comment> - getKey() : String</comment><comment> - getKeyAsText() : Text</comment><comment> - getKeyAsNumber() : Number (if the key can be numeric value, eg. range &amp; histograms)</comment><comment> - getKeyAsGeoPoint() : GeoPoint (in case of the geohash_grid agg)</comment></comments></commit></commits></item><item><title>foreground mode is now the default, s/lets/let's/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4921</link><project id="" key="" /><description /><key id="26426178">4921</key><summary>foreground mode is now the default, s/lets/let's/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2014-01-28T09:34:10Z</created><updated>2014-07-16T21:49:05Z</updated><resolved>2014-01-28T10:11:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-28T10:11:06Z" id="33465483">merged to all branches thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create index to support aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4920</link><project id="" key="" /><description>The create index api currently supports providing mappings, settings and warmers. It would be nice to be able to provide aliases as well in the same request, during index creation.
</description><key id="26426036">4920</key><summary>Create index to support aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Index APIs</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-28T09:31:17Z</created><updated>2015-06-06T18:43:28Z</updated><resolved>2014-02-17T14:17:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/ToAndFromJsonMetaDataTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file><file>src/test/java/org/elasticsearch/indices/template/SimpleIndexTemplateTests.java</file></files><comments><comment>Added support for aliases to index templates</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/alias/Alias.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasValidator.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file></files><comments><comment>Added support for aliases to create index api</comment></comments></commit></commits></item><item><title>Can't return empty HTTP within REST plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4919</link><project id="" key="" /><description>Within a REST plugin, I try to an HTTP response with no content (for example with a DELETE method) and I get a NullPointerException. This feature doesn't seem to be supported whereas we can only use the HTTP status code to notify the client about the result of a request (200 -&gt; OK, 400 -&gt; bad request, and so on).

I tried to implement an EmptyRestResponse class, as described below, but I have exception when using it.

```
public class EmptyRestResponse extends AbstractRestResponse {
    private RestStatus status;

    public EmptyRestResponse(RestStatus status) {
        this.status = status;
    }

    @Override
    public byte[] content() throws IOException {
        return new byte[0];
    }

    @Override
    public int contentLength() throws IOException {
        return 0;
    }

    @Override
    public int contentOffset() throws IOException {
        return 0;
    }

    @Override
    public boolean contentThreadSafe() {
        return false;
    }

    @Override
    public String contentType() {
        return null;
    }

    @Override
    public RestStatus status() {
        return status;
    }
}
```

To support such feature, we need to patch the NettyHttpChannel class to avoid null pointer exceptions and setting media type in all case. With empty result, we don't need the corresponding header. First of all, we could add a dedicated method to check is the response contains content or not. See the isEmptryResponse method below:

```
private boolean isEmptyResponse(RestResponse response) {
    if (response instanceof XContentRestResponse) {
        return false;
    } else {
        try {
            return (response.content()==null
                || response.content().length==0);
        } catch(IOException e) {
            throw new HttpException(
                "Failed to convert response to bytes", e);
        }
    }
}
```

Then we could use this method within the sendResponse method of the NettyHttpChannel class:

```
@Override
public void sendResponse(RestResponse response) {
    (...)
    if (!isEmptyResponse(response)) {
        // Convert the response content to a ChannelBuffer.
        ChannelBuffer buf;
        (...)
        resp.setContent(buf);
        resp.headers().add(HttpHeaders.Names.CONTENT_TYPE,
                           response.contentType());

        resp.headers().add(HttpHeaders.Names.CONTENT_LENGTH,
                           String.valueOf(buf.readableBytes()));
    }
    (...)
}
```
</description><key id="26422375">4919</key><summary>Can't return empty HTTP within REST plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">templth</reporter><labels><label>adoptme</label></labels><created>2014-01-28T08:09:58Z</created><updated>2014-12-24T18:58:53Z</updated><resolved>2014-12-24T18:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-28T08:22:52Z" id="33458916">Hey

might have misread your question, but does this command send back too much data too you?

```
channel.sendResponse(new XContentRestResponse(request, RestStatus.OK, jsonBuilder()));
```
</comment><comment author="templth" created="2014-01-28T10:36:55Z" id="33467194">Hi Alexander,

Thanks for answering so quickly!

In fact, you will indeed define an empty content for the response with your solution but you will still have Content-type (with value application/json) and Content-length (with value 0). From my point of view, these two headers don't apply in this context.

In fact, I think that there is a difference between an empty json content for the response and no content in the response. ElasticSearch only implements the first approach and this issue is about the second one.

Hoping I'm clear in my explanations.
</comment><comment author="kimchy" created="2014-01-28T10:40:48Z" id="33467439">another option is `new StringRestResponse(NOT_FOUND)` (for example), shorter. As to if we need to return content type or not, I am not sure I agree that we shouldn't return content type if the response is empty, does it hurt? We do need to return the Content-Length for sure, it tremendously helps with parsing the response on the client side.
</comment><comment author="templth" created="2014-01-29T08:08:37Z" id="33563868">Hi Shay,

Thanks very much for your answer and the workaround.

However I think that it would be great to give hand on these two headers but I understand your point of view for the content-length header ;-) As far as I can see in code, it's only a test to add in order to check if the contentType property of the provided REST response is null or not. In addition adding this test would prevent from null pointer exceptions when the contentType property is null. My 2 cents ;-)

Otherwise to take a sample of what is done within the Restlet framework for the delete HTTP method, these headers (content-type and content-length) are only added to the response if we specify a representation / entity.

Hope it helps.
Thierry
</comment><comment author="clintongormley" created="2014-12-24T18:58:53Z" id="68069623">Hi @templth 

Given that there has been no movement on this issue in almost a year, I think it is unlikely we will ever get around to supporting it.  If you still need something like this, I suggest you submit a PR for us to review.

sorry about that
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>closes #4917 - export ES_MIN_MEM and ES_MAX_MEM in redhat init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4918</link><project id="" key="" /><description /><key id="26410213">4918</key><summary>closes #4917 - export ES_MIN_MEM and ES_MAX_MEM in redhat init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">teancom</reporter><labels /><created>2014-01-28T01:30:48Z</created><updated>2014-07-16T21:49:05Z</updated><resolved>2014-04-25T19:50:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-04-25T19:50:07Z" id="41432598">I'll close this one. I think ES_HEAP_SIZE is sufficient to be exposed. More options often lead to more confusion and I dont consider it any gain, when those are exposed. Any objections on your side? Happy to get different feedback to discuss.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>redhat init script does not export ES_MIN_MEM and ES_MAX_MEM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4917</link><project id="" key="" /><description>By not doing so, setting the variables in /etc/sysconfig/elasticsearch does not actually do what is necessary - override the Xms and Xmx values on startup. 
</description><key id="26410159">4917</key><summary>redhat init script does not export ES_MIN_MEM and ES_MAX_MEM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">teancom</reporter><labels /><created>2014-01-28T01:29:40Z</created><updated>2014-11-20T20:05:37Z</updated><resolved>2014-04-25T19:49:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-28T08:15:57Z" id="33458573">Hey,

adding this is easy. Wondering what made you opt for `ES_MIN_MEM` and `ES_MAX_MEM` instead of `ES_HEAP_SIZE`? Any specific use case you are covering?
</comment><comment author="teancom" created="2014-01-28T15:32:04Z" id="33488890">Well, I hadn’t looked at the ES_HEAP_SIZE variable before I started - I’m used to tuning the Xms and Xmx values with other java projects I’m the sysad on. And then when I did the thing I was told (via comments/doc for the /etc/sysconfig/elasticsearch file) , it didn’t work. Knowing about ES_HEAP_SIZE, though, I’ll probably go back to my developers and ask if they’re okay with using that instead.

—  
David Bishop

On Jan 28, 2014, 3:16:36 AM, Alexander Reelsen notifications@github.com wrote:  

Hey,

adding this is easy. Wondering what made you opt for ES_MIN_MEM and ES_MAX_MEM instead of ES_HEAP_SIZE? Any specific use case you are covering?

—
Reply to this email directly or view it on GitHub(https://github.com/elasticsearch/elasticsearch/issues/4917#issuecomment-33458573).
</comment><comment author="spinscale" created="2014-04-25T19:49:59Z" id="41432584">I'll close this one. I think ES_HEAP_SIZE is sufficient to be exposed. More options often lead to more confusion and I dont consider it any gain, when those are exposed. Any objections on your side? Happy to get different feedback to discuss.
</comment><comment author="JPvRiel" created="2014-11-13T16:35:24Z" id="62921892">Hi there. I suggest this be re-opened, or the issue be re-created. Essentially the export issue is in debian too. One of two things need to be done in my view:
- Update the documentation and remove the  ES_MIN_MEM and ES_MAX_MEM setting all together, instead suggesting that the JAVA_OPTS be used directly instead should someone really want to control this.
- Or Fix the .deb/.rpm packages and `bin/elasticsearch.in.sh` to properly support ES_MIN_MEM and ES_MAX_MEM if it's not a deprecated option.

Note, in my VM use case, I very much did want use ES_MIN_MEM and ES_MAX_MEM and thought it was useful (but it didn't work).

Setup  documentation says the following

&gt; ES_HEAP_SIZE : The heap size to start with
&gt; https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/setup/as-a-service.asciidoc

I've just seen my 1.1.1 cluster die because ES_HEAP_SIZE actually means the min and max heap size, not just the start size

And this

&gt; The ES_HEAP_SIZE environment variable allows to set the heap memory that will be allocated to elasticsearch java process. It will allocate the same value to both min and max values, though those can be set explicitly (not recommended) by setting ES_MIN_MEM (defaults to 256m), and ES_MAX_MEM (defaults to 1g).
&gt; https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/setup/configuration.asciidoc

Apart from fixing the missing exports for ES_MIN_MEM and ES_MAX_MEM, nowhere does it explain that ES_HEAP_SIZE will supersede the ES_MIN_MEM and ES_MAX_MEM settings.

I've read that being greedy and setting ES_HEAP_SIZE to grab 50% of system RAM is advised for better performance. However, if like me, one hast to run elasticsearch in a shared VM environment (yes, not ideal, but that's what we've got available for now), then I certainly don't want to entertain this premature memory hog mentality of ES_HEAP_SIZE=50% and rather, I want ES_MIN_MEM and ES_MAX_MEM to work as advertised and supersede the ES_HEAP_SIZE setting.

I've looked at
https://github.com/elasticsearch/elasticsearch/blob/master/bin/elasticsearch.in.sh
and at the init script /etc/init.d/elasticsearch as well as /etc/default/elasticsearch shipped with .deb download for ES v1.1.1 and ES v1.4.0
Observations
- neither `ES_MIN_MEM` and `ES_MAX_MEM` seem to be supported/suggested and in practice look deprecated
  - `/etc/default/elasticsearch` doesn't provide examples 
  - `/etc/init.d/elasticsearch` doesn't export those variables
- `bin/elasticsearch.in.sh` does still have the old logic to pass onto the java arguments if  `ES_MIN_MEM` and `ES_MAX_MEM` are set, but this is overwritten by `ES_HEAP_SIZE`.

I've patched all of this in my install such that ES_MIN_MEM and ES_MAX_MEM supersede ES_HEAP_SIZE and work with debian `/etc/default` and `/etc/init.d`. If ES_MIN_MEM and ES_MAX_MEM are still supposed to be supported options, then I'm happy to try submit and commit this patch, but I've not yet tested how having ES_MIN_MEM and ES_MAX_MEM set will mess with the option of MAX_LOCKED_MEMORY and `bootstrap.mlockall: true` in `elasticsearch.yml`.
</comment><comment author="clintongormley" created="2014-11-13T17:35:57Z" id="62932797">Hi @JPvRiel 

MIN/MAX are indeed deprecated options, and if you have nodes dieing because they try to allocate all the heap initially,  then you'll just have them dieing later on when the JVM tries to allocate it's MAX memory setting later on.

&gt; I've not yet tested how having ES_MIN_MEM and ES_MAX_MEM set will mess with the option of MAX_LOCKED_MEMORY and bootstrap.mlockall: true in elasticsearch.yml.

It won't play well at all.  mlockall needs to lock all the memory into RAM at startup.

i suggest that a better approach is just to set ES_HEAP_SIZE to the maximum amount that you can safely allocate on that box, otherwise you're letting yourself in for a world of pain later on.  Also, remember that Elasticsearch and Lucene need generous amounts of file system cache for them to function with decent performance.  It doesn't sound like you're leaving any space for file system cache, which means that you're going to have to hit disk all the time.  Performance will not be good in this case.
</comment><comment author="JPvRiel" created="2014-11-20T20:02:27Z" id="63869968">Hey @Clintongormley, appreciate the reply. Sounds like I should find the time to submit a [configuration](https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/setup/configuration.asciidoc) documention patch which updates and clarifies it a bit?

Suppose I'm flogging a dead horse here (given 'closed' issue). I totally buy the recommendations made for ES_HEAP_SIZE in a production and dedicated hardware context - no disagreement there.

That said, in testing and dev workspaces with virtualized/shared infrastructure, I'm sticking to my preference of wanting memory only allocated as needed, and should probably drop my use of bootstrap.mlockall, MAX_LOCKED_MEMORY and ES_HEAP_SIZE. Instead I can use JAVA_OPTS with -Xms and -Xmx to have my way.

Some more interesting notes after a bit of reading
- man page for Linux mlockall() mentions a MCL_FUTURE in addition to MCL_CURRENT flag. In theory, it suggests one doesn't have to grab and lock all the ram right away and can instead lock new heap space as and when it's allocated
- [Java Mlockall Agent README](https://github.com/LucidWorks/mlockall-agent/blob/master/README.txt) suggests differently, stating in the FAQ section that Java won't lock new pages when it grows the heap

Anyhow, plan to look into this more out of interest, but for now, think I've figured out what to do for my use case (shared VM environment).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] List of reserved characters for query_string isn't complete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4916</link><project id="" key="" /><description>The list of reserved characters on the `query_string` page doesn't include `&gt;`, `&lt;`, or `=` even though they all have meaning.
</description><key id="26398096">4916</key><summary>[docs] List of reserved characters for query_string isn't complete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2014-01-27T21:55:32Z</created><updated>2015-02-04T16:59:48Z</updated><resolved>2015-02-04T16:59:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-02-04T16:59:48Z" id="72892986">Closed by #9518
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Paging support for aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4915</link><project id="" key="" /><description>Terms aggregation  does not support a way to page through the buckets returned.
To work around this, I've been trying to set 'min_doc_count'  to limit the buckets returned and using a 'exclude' filter, to exclude already 'seen' pages.

Will this result in better running time performance on the ES cluster as compared to getting all the buckets and then doing my own paging logic client side ?
</description><key id="26393327">4915</key><summary>Paging support for aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aaneja</reporter><labels><label>:Aggregations</label></labels><created>2014-01-27T20:48:34Z</created><updated>2016-12-16T13:36:49Z</updated><resolved>2016-11-24T16:23:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-03T15:07:08Z" id="33963055">Paging is tricky to implement because document counts for terms aggregations are not exact when `shard_size` is less than the field cardinality and sorting on `count desc`. So weird things may happen like the first term of the 2nd page having a higher count than the last element of the first page, etc.

Regarding your question, terms aggregations run in two phases on the shard-level: first they compute counts for every possible term, and then they pick the top `shard_size` ones. Increasing `size` (or `shard_size`) only makes the 2nd step more costly. Given that the runtime of the first step is linear with the number of matched documents and that the runtime of the 2nd step is `O(#unique_values * log(shard_size))`, if you only have a limited number of unique values compared to the number of matched documents, doing the paging on client-side would be more efficient. On the other hand, on high-cardinality-fields, your first approach based on an exclude would probably be better.

As a side-note, `min_doc_count` has no effect on runtime performance when it is greater than or equal to 1. Only `min_doc_count=0` is more costly given that it requires Elasticsearch to also fetch terms that are not contained in any match.
</comment><comment author="haschrisg" created="2014-03-12T03:35:35Z" id="37372551">@jpountz would storing the results of an aggregation in a new index be feasible?  In general, it'd be great to have a way of dealing with both aggregations with high cardinality, and nested aggregations that produce a large number (millions) of results -- even if the cost of that is that they're not sorted properly when paging.
</comment><comment author="jpountz" created="2014-03-12T07:32:43Z" id="37382008">If it makes sense for your use-case, this is something that you could consider implementing on client-side, by running hourly/daily these costly aggregations, storing the result in an index and using this index between two runs to explore the results of the aggregation?
</comment><comment author="apatrida" created="2014-09-02T20:31:27Z" id="54213019">When sorting by term instead of count, why would paging then not be possible?  For example, having a terms aggregation, with top hits aggregation which could produce an overly large result set without having paging on the terms aggregation.  Not all aggregations wants want to sort by count.
</comment><comment author="tugberkugurlu" created="2014-09-24T16:46:54Z" id="56700228">I can see that this may not be possible but for a top_hits aggregation, I really need this functionality. I have the below aggregation query:

```
POST sport/_search
{
  "size": 0,
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "bool": {
          "must": [
            {
              "range": {
                "defense_strength": {
                  "lte": 83.43
                }
              }
            },
            {
              "range": {
                "forward_strength": {
                  "gte": 91
                }
              }
            }
          ]
        }
      }
    }
  }, 
  "aggs": {
    "top_teams": {
        "terms": {
          "field": "primaryId"
        },
        "aggs": {
          "top_team_hits": {
            "top_hits": {
              "sort": [
                {
                    "forward_strength": {
                        "order": "desc"
                    }
                }
              ],
              "_source": {
                  "include": [
                      "name"
                  ]
              },
              "from": 0,
              "size" : 1
            }
          }
        }
      }
    }
  }
}
```

This produces the below result for an insanely cheap index (with low number of docs):

```
    {
         "took": 2,
         "timed_out": false,
         "_shards": {
                "total": 5,
                "successful": 5,
                "failed": 0
         },
         "hits": {
                "total": 5,
                "max_score": 0,
                "hits": []
         },
         "aggregations": {
                "top_teams": {
                     "buckets": [
                            {
                                 "key": "541afdfc532aec0f305c2c48",
                                 "doc_count": 2,
                                 "top_team_hits": {
                                        "hits": {
                                             "total": 2,
                                             "max_score": null,
                                             "hits": [
                                                    {
                                                         "_index": "sport",
                                                         "_type": "football_team",
                                                         "_id": "y6jZ31xoQMCXaK23rPQgjA",
                                                         "_score": null,
                                                         "_source": {
                                                                "name": "Barcelona"
                                                         },
                                                         "sort": [
                                                                98.32
                                                         ]
                                                    }
                                             ]
                                        }
                                 }
                            },
                            {
                                 "key": "541afe08532aec0f305c5f28",
                                 "doc_count": 2,
                                 "top_team_hits": {
                                        "hits": {
                                             "total": 2,
                                             "max_score": null,
                                             "hits": [
                                                    {
                                                         "_index": "sport",
                                                         "_type": "football_team",
                                                         "_id": "hewWI0ZpTki4OgOeneLn1Q",
                                                         "_score": null,
                                                         "_source": {
                                                                "name": "Arsenal"
                                                         },
                                                         "sort": [
                                                                94.3
                                                         ]
                                                    }
                                             ]
                                        }
                                 }
                            },
                            {
                                 "key": "541afe09532aec0f305c5f2b",
                                 "doc_count": 1,
                                 "top_team_hits": {
                                        "hits": {
                                             "total": 1,
                                             "max_score": null,
                                             "hits": [
                                                    {
                                                         "_index": "sport",
                                                         "_type": "football_team",
                                                         "_id": "x-_YBX5jSba8qsEuB8guTQ",
                                                         "_score": null,
                                                         "_source": {
                                                                "name": "Real Madrid"
                                                         },
                                                         "sort": [
                                                                91.34
                                                         ]
                                                    }
                                             ]
                                        }
                                 }
                            }
                     ]
                }
         }
    }
```

What I need here is the ability to get first 2 aggregation result and get the other 2 (in this case, only 1) in other request.
</comment><comment author="missingpixel" created="2014-10-31T12:26:21Z" id="61253054">If paging aggregations is not possible, how do we use ES for online stores where products of different colours are grouped together? Or, what if there are five million authors in the example at: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/top-hits.html ? Aggregate them and perform pagination in-memory?

If that's not possible, what else can be done in place of grouping in Solr?

Thank you
</comment><comment author="adrienbrault" created="2014-12-11T13:56:11Z" id="66621633">A parameter allowing to hide the first X buckets from the response would be nice.
</comment><comment author="adrienbrault" created="2014-12-11T14:01:49Z" id="66622350">@clintongormley Why was this issue closed ?
</comment><comment author="bobbyhubbard" created="2014-12-12T00:39:32Z" id="66715346">Reopen please?
</comment><comment author="mikelrob" created="2015-01-02T14:16:09Z" id="68528947">+1 for pagination while sorted on term not doc count
</comment><comment author="android-programmer" created="2015-01-06T08:37:16Z" id="68839253">+1
</comment><comment author="daniel1028" created="2015-01-12T14:04:13Z" id="69573596">Can you re-open this please?

I understand that aggregation pagination will create performance issue in larger numbers of records. But it will not affect smaller numbers of records right?

The performance issue will be happen only if have more records.Why don't we have this support at least for smaller set of records.

Why do we have to hesitate to add this support considering larger amount of data? If we have this support , it would be very helpful for us to paginate smaller amount data.

May be we can inform users, this support will be efficient only for smaller amount data. Whenever the amount for data increases ,the performance will hit highly.
</comment><comment author="clintongormley" created="2015-01-14T09:05:49Z" id="69887477">We have been against adding pagination support to the terms (and related) aggregations because it hides the cost of generating the aggregations from the user.  Not only that, it can produce incorrect ordering because term based aggregations are approximate.

That said, we support pagination on search requests, which are similarly costly (although accurate).

While some users will definitely shoot themselves in the foot with pagination (eg https://github.com/elasticsearch/elasticsearch/issues/4915#issuecomment-61253054), not supporting pagination does limit some legitimate use cases.

I'll reopen this ticket for further discussion.
</comment><comment author="byronvoorbach" created="2015-01-14T16:23:01Z" id="69942916">I would love to see this feature being added to ES, but I understand the cost at which it would come.
I'm currently working for a client which needed such a feature, but since it didn't exist yet we solved it with doing 2 queries:

The first query has a terms aggregation on our field on which we want grouping and orders the aggregation based on the doc.score. We set the size of the aggregation to 0, so that we get all buckets for that query. 
We then parse the result and get the keys from the buckets corresponding to the given size and offset. ( eg bucket 30-40 for page 3).

We then perform a new query, filtering all results based on the keys from the first query. Next to the query is a term aggregation (on the same field as before), and we add a top_hits aggregation to get the results for those (10) buckets.

This way we don't have to load all 40 buckets and get the top_hits for those buckets, which increases performance.

Loading all buckets and top 10 hits per bucket took around 20 seconds for a certain query. With the above change we managed to bring it back to 100ms.

Info:
- +-60 million records
- around 1500 buckets for average query
- around 300 documents per bucket

This might help someone out as a workaround till such a feature exists within Elasticsearch
</comment><comment author="davidvgalbraith" created="2015-03-17T18:42:22Z" id="82518186">Hey! I too would like paging for aggregations. That's all. 
</comment><comment author="bauneroni" created="2015-03-19T11:07:35Z" id="83509702">I'd also love to see this someday but I do understand the burden (haven't used that word in a long time) and costs to implement this. This feature would be quite handy for my client's application which is operating on ~250GB+ of data. 

Well, yeah.. what he^ said :+1:  
</comment><comment author="vinusebastian" created="2015-04-01T10:14:40Z" id="88426565">@aaneja  with respect to  "Terms aggregation does not support a way to page through the buckets returned.
To work around this, I've been trying to set 'min_doc_count' to limit the buckets returned and using a 'exclude' filter, to exclude already 'seen' pages.

Will this result in better running time performance on the ES cluster as compared to getting all the buckets and then doing my own paging logic client side ?"

How did you exclude already seen pages? Or how did you keep track of seen pages? Also what did you learn about performance issues with such an approach? 
</comment><comment author="dakrone" created="2015-04-10T16:03:33Z" id="91601921">We discussed this and one potential idea is to add the ability to specify a `start_term` for aggregations, that would allow the aggregation to skip all of the preceding terms, then the client could implement the paging by retrieving the first page of aggregations, then sending the same request with the `start_term` being the last term of the previous results. Otherwise the aggregation will still incur the overhead of computing the results and sub-aggregations for each of the "skipped" buckets.

To better understand this, it would be _extremely_ useful to get more use-cases out of why people need this and how they would use it, so please do add those to this ticket.
</comment><comment author="2e3s" created="2015-04-10T20:13:27Z" id="91671239">+1 for that. There may be tens of thousands unique terms by which we group, and gather statistics by subaggregations. It can be sorted by any of these subaggregations, so it's gonna be very costly anyway, but its speed with ES is currently more that bearable as well as its precision, and if not sending such big json data between servers and holding it with PHP (which isn't good at all as for now), it would be fine. I even think of some plugin which would do this simple job. But this still will require computing a sorting subaggregation if used.
</comment><comment author="a0s" created="2015-04-29T15:44:18Z" id="97475286">+1
</comment><comment author="benneq" created="2015-05-06T13:44:28Z" id="99464482">+1
</comment><comment author="pauleil" created="2015-05-06T17:59:00Z" id="99553336">+1
</comment><comment author="jaynblue" created="2015-05-06T18:03:04Z" id="99554279">+1
</comment><comment author="dragonkid" created="2015-05-07T05:45:06Z" id="99728049">+1
</comment><comment author="genme" created="2015-05-07T12:59:37Z" id="99856235">+1
</comment><comment author="aznamier" created="2015-05-07T17:17:30Z" id="99942501">+1
</comment><comment author="bobbyhubbard" created="2015-05-07T17:20:06Z" id="99942982">+1
</comment><comment author="robinmitra" created="2015-05-08T11:18:21Z" id="100197602">+1
</comment><comment author="GregorSondermeier" created="2015-05-08T15:53:48Z" id="100278954">+1
</comment><comment author="mfischbo" created="2015-05-08T19:39:31Z" id="100336670">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>has_parent doesn't set the default type to the parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4914</link><project id="" key="" /><description>Trying to reference a field in the parent document using a script like `doc["field"]` fails because the field doesn't exist in the _child_ mapping.  Prepending the parent type ( `doc["parentType.field"]` finds the field correctly.

The default type within a `has_parent` or `has_child` query or filter should be the parent/child type respectively.

```
curl -XPUT "http://localhost:9200/t" -d'
{
  "mappings": {
    "myParent": {
      "properties": {
        "weight": {
          "type": "double"
        }
      }
    },
    "myChild": {
      "_parent": {
        "type": "myParent"
      },
      "_routing": {
        "required": true
      }
    }
  }
}'


curl -XPUT "http://localhost:9200/t/myParent/1" -d'
{
  "weight": 2
}'

curl -XPUT "http://localhost:9200/t/myChild/3?parent=1" -d'
{}'

curl -XGET "http://localhost:9200/t/myChild/_search" -d'
{
  "query": {
    "has_parent": {
      "query": {
        "function_score": {
          "script_score": {
            "script": "_score * doc[\"myParent.weight\"].value"
          }
        }
      },
      "parent_type": "myParent",
      "score_type": "score"
    }
  }
}'
```

This query:

```
curl -XGET "http://localhost:9200/t/myChild/_search" -d'
{
  "query": {
    "has_parent": {
      "query": {
        "function_score": {
          "script_score": {
            "script": "_score * doc[\"weight\"].value"
          }
        }
      },
      "parent_type": "myParent",
      "score_type": "score"
    }
  }
}'
```

Fails with:

```
"QueryPhaseExecutionException[[t][2]: query[filtered(ParentQuery[myParent](filtered(function score (ConstantScore(*:*),function=script[_score * doc['weight'].value], params [null]))-&gt;cache(_type:myParent)))-&gt;cache(_type:myChild)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: RuntimeException[[Error: No field found for [weight] in mapping with types [myChild]]
[Near : {... _score * doc['weight'].value ....}]
             ^
[Line: 1, Column: 1]]; nested: CompileException[[Error: No field found for [weight] in mapping with types [myChild]]
[Near : {... _score * doc['weight'].value ....}]
             ^
[Line: 1, Column: 1]]; nested: ElasticsearchIllegalArgumentException[No field found for [weight] in mapping with types [myChild]];
```
</description><key id="26381019">4914</key><summary>has_parent doesn't set the default type to the parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2014-01-27T18:49:27Z</created><updated>2014-12-24T18:55:27Z</updated><resolved>2014-12-24T18:55:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-30T16:45:41Z" id="33706328">Fixing this in the parser isn't going to help, because the field in the script is evaluated at query execution time and not at query parsing time.
</comment><comment author="clintongormley" created="2014-12-24T18:55:27Z" id="68069462">Closing in favour of #8870.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw exception if an additional field was placed inside the "query" body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4913</link><project id="" key="" /><description>Currently the parser accepts queries like

```
"query" : {
     "any_query": {
         ...
     },
     "any_field_name":...
}
```

The "any_field_name" is silently ignored. However, this also causes the parser
not to move to the next closing bracket which in turn can lead to additional query
paremters being ignored such as "fields", "highlight",...
This was the case in issue #4895

closes issue #4895
</description><key id="26376117">4913</key><summary>Throw exception if an additional field was placed inside the "query" body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T17:40:32Z</created><updated>2015-06-07T15:57:05Z</updated><resolved>2014-04-25T07:04:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-04-23T15:40:50Z" id="41176806">I think we should push this - it's a good fix
</comment><comment author="brwe" created="2014-04-25T07:04:28Z" id="41364282">Pushed to master and 1.x. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[ 0.90.9] Completion - updated properties are not reflected </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4912</link><project id="" key="" /><description>I'm using the completion feature on 0.90.9. I've a document which I index with multiple inputs and suggest based search works OK. Then I deleted that doc and re-create it with different inputs, and it still being returned with the first search I did, although I examined the doc (head plugin) and the input field is indeed updated, which does not match that search.
Next thing I re-created the doc with different output field, and again - although seen when I examine the doc (head plugin), the api ( REST and JAVA concreteOption.getText().toString() ) returns the old output.
</description><key id="26359215">4912</key><summary>[ 0.90.9] Completion - updated properties are not reflected </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">mastershifu</reporter><labels /><created>2014-01-27T15:17:59Z</created><updated>2014-04-25T19:38:19Z</updated><resolved>2014-04-25T19:38:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-22T16:32:06Z" id="35806885">The suggest data structure does not reflect deletes immediately see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters-completion.html#indexing

Can you provide a small example to reproduce (maybe I am misunderstanding you somewhere).

Thanks a lot!
</comment><comment author="spinscale" created="2014-04-25T19:38:19Z" id="41431506">closing.. happy to reopen, if this is a bug (need an example though). Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>simple_query_string flags Does not seem to support "PHRASE" flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4911</link><project id="" key="" /><description>using Elastic search "0.90.10", "lucene_version" : "4.6"

Related to https://groups.google.com/d/topic/elasticsearch/TYVqdYQNZVQ/discussion

I am trying to use the new "simple_query_string" when I add to the flags "PHRASE" I get "Unknown simple_query_string flag [PHRASE]]"

However If I add the "ALL" flag, it seems to work.

Example:

``` sh
curl -XPOST localhost:9200/test/_search -d '{
    "query": {
        "filtered": {
            "query": {
                "simple_query_string": {
                    "query": "horse",
                    "fields": [
                        "sreferenceNumber^20",
                        "sTitle^2",
                        "sDescription"
                    ],
                    "flags":"PHRASE",
                    "default_operator":"AND"

                }
            }
        }
    }
}'
```

If I try that same query with the Flags set to all I get results.

Error I am getting, is:

&gt; ElasticSearchIllegalArgumentException[Unknown simple_query_string flag [PHRASE]];

From http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html
It seems that it should be possible to send in the "PHRASE" flag.
</description><key id="26354699">4911</key><summary>simple_query_string flags Does not seem to support "PHRASE" flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">loneparadox</reporter><labels /><created>2014-01-27T14:08:37Z</created><updated>2014-01-27T14:17:04Z</updated><resolved>2014-01-27T14:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SimpleQueryStringFlag.java</file></files><comments><comment>Add missing PHRASE flag for simple_query_string</comment></comments></commit></commits></item><item><title>mtermvectors REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4910</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in msearch REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPOST "http://localhost:9200/test/type/1?refresh" -d'{
    "foo": "bar"
}'

# This one works
curl -XPOST "http://localhost:9200/test/type/_mtermvectors" -d'
{
    "ids" : ["1"]
}'

# This one gives: "ActionRequestValidationException[Validation Failed: 1: multi term vectors: no documents requested;]"
curl -XGET "http://localhost:9200/test/type/_mtermvectors?source=%7B%22ids%22%3A%5B%221%22%5D%7D"
```

Closes #4902.
</description><key id="26352261">4910</key><summary>mtermvectors REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T13:28:21Z</created><updated>2015-06-07T23:57:53Z</updated><resolved>2014-01-28T08:58:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-28T08:58:21Z" id="33460803">Merged in:
- 1.0
- 1.x
- master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>percolate REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4909</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in percolate REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPUT "http://localhost:9200/test/.percolator/1" -d'
{
    "query" : {
        "match" : {
            "foo" : "bar"
        }
    }
}'

# This one works
curl -XPOST "http://localhost:9200/test/message/_percolate" -d '{
  "doc" : {
    "foo" : "bar is in foo"
  }
}'

# This one gives: BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate];
curl -XGET "http://localhost:9200/test/message/_percolate?source=%7B%22doc%22%3A%7B%22foo%22%3A%22bar%20is%20in%20foo%22%7D%7D"
```

Closes #4903.
</description><key id="26351679">4909</key><summary>percolate REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T13:16:03Z</created><updated>2015-06-07T23:57:18Z</updated><resolved>2014-01-28T16:42:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>mpercolate REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4908</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in mpercolate REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPUT "http://localhost:9200/test/.percolator/1" -d'
{
    "query" : {
        "match" : {
            "foo" : "bar"
        }
    }
}'

# This one works
curl -XPOST "http://localhost:9200/test/message/_mpercolate" -d '
{"percolate" : {}}
{"doc" : {"foo" : "bar is in foo"}}
'

# This one gives: BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate];
curl -XGET "http://localhost:9200/test/message/_mpercolate?source=%7B%22percolate%22%3A%7B%7D%7D%0A%7B%22doc%22%3A%7B%22foo%22%3A%22bar is in foo%22%7D%7D%0A"
```

Closes #4900.
</description><key id="26351168">4908</key><summary>mpercolate REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T13:05:26Z</created><updated>2015-06-07T23:58:57Z</updated><resolved>2014-01-28T08:58:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-28T08:58:50Z" id="33460833">Merged in:
- 1.0
- 1.x
- master
</comment><comment author="RaazTripathi" created="2014-01-28T11:29:08Z" id="33470472">Great update
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add thread pool cat api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4907</link><project id="" key="" /><description>Add dedicated thread pool cat api, that can show all thread pool related statistic (size, rejected, queue etc.) for all thread pools (get

By default active, rejected and queue thread statistics should be included for the index, bulk and search thread pool.

```
$curl 'localhost:9200/_cat/thread_pool?v'
host      ip            bulk.active bulk.queue bulk.rejected index.active index.queue index.rejected search.active search.queue search.rejected 
mvg.local 10.20.100.174           0          0             0            0           0              0             0            0               0
```

Other thread statistics of other thread pools can be included via the `h` query string parameter.

```
$curl 'localhost:9200/_cat/thread_pool?v&amp;h=id,host,index.completed'
id   host      index.completed 
SHLd mvg.local               1 
```
</description><key id="26348217">4907</key><summary>Add thread pool cat api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T12:03:28Z</created><updated>2014-01-29T12:27:00Z</updated><resolved>2014-01-29T12:27:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestThreadPoolAction.java</file></files><comments><comment>Added dedicated thread pool cat api, that can show all thread pool related statistic (size, rejected, queue etc.) for all thread pools (get, search, index etc.)</comment><comment>By default active, rejected and queue thread statistics are included for the index, bulk and search thread pool.</comment><comment>Other thread statistics of other thread pools can be included via the `h` query string parameter.</comment></comments></commit></commits></item><item><title>Added thread pool cat api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4906</link><project id="" key="" /><description>Added dedicated thread pool cat api, that can show all thread pool related statistic (size, rejected, queue etc.) for all thread pools (get, search, index etc.)

By default active, rejected and queue thread statistics are included for the index, bulk and search thread pool.
Other thread statistics of other thread pools can be included via the `h` query string parameter.

Closes #4907
</description><key id="26347700">4906</key><summary>Added thread pool cat api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T11:52:56Z</created><updated>2015-06-07T15:58:30Z</updated><resolved>2014-01-29T12:27:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-27T11:54:07Z" id="33361370">LGTM, old license header is used...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>msearch REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4905</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in msearch REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPOST "http://localhost:9200/test/type/1?refresh" -d'{
    "foo": "bar"
}'

cat requests
{}
{"query" : {"match_all" : {}}}

# This one works
curl -XGET localhost:9200/_msearch --data-binary @requests

# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}
curl -XGET "http://localhost:9200/test/type/_mget?source=%7B%7D%0A%7B%22query%22%3A%7B%22match_all%22%3A%7B%7D%7D%7D%0A"
```

Closes #4901.
</description><key id="26346260">4905</key><summary>msearch REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T11:22:15Z</created><updated>2015-06-07T23:58:23Z</updated><resolved>2014-01-28T08:56:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-28T08:56:51Z" id="33460713">Merged in:
- 0.90
- 1.0
- 1.x
- master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A bit of extra javadoc for updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4904</link><project id="" key="" /><description>Adding javadoc to UpdateRequestBuilder for a couple of details it took me a while to find.
</description><key id="26345542">4904</key><summary>A bit of extra javadoc for updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">winterstein</reporter><labels /><created>2014-01-27T11:07:50Z</created><updated>2014-06-19T22:28:39Z</updated><resolved>2014-04-04T15:54:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T14:20:48Z" id="34981792">I left a small comment, would you mind squashing this into a single commit and signing our [CLA](http://www.elasticsearch.org/contributor-agreement/) so that we can get this in?
</comment><comment author="winterstein" created="2014-02-23T08:33:30Z" id="35826728">Hello @javanna. I have signed the CLA. I'm going to get some help on squashing the commits into a single commit...
</comment><comment author="javanna" created="2014-04-04T15:54:29Z" id="39580550">Closing in favor of #5676 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>percolate REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4903</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in percolate REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPUT "http://localhost:9200/test/.percolator/1" -d'
{
    "query" : {
        "match" : {
            "foo" : "bar"
        }
    }
}'

# This one works
curl -XPOST "http://localhost:9200/test/message/_percolate" -d '{
  "doc" : {
    "foo" : "bar is in foo"
  }
}'

# This one gives: BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate];
curl -XGET "http://localhost:9200/test/message/_percolate?source=%7B%22doc%22%3A%7B%22foo%22%3A%22bar%20is%20in%20foo%22%7D%7D"
```
</description><key id="26344173">4903</key><summary>percolate REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T10:43:22Z</created><updated>2014-01-28T16:59:37Z</updated><resolved>2014-01-28T08:46:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>Fix potential NPE when no source and no body</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file></files><comments><comment>percolate REST API should support source parameter</comment></comments></commit></commits></item><item><title>mtermvectors REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4902</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in msearch REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPOST "http://localhost:9200/test/type/1?refresh" -d'{
    "foo": "bar"
}'

# This one works
curl -XPOST "http://localhost:9200/test/type/_mtermvectors" -d'
{
    "ids" : ["1"]
}'

# This one gives: "ActionRequestValidationException[Validation Failed: 1: multi term vectors: no documents requested;]"
curl -XGET "http://localhost:9200/test/type/_mtermvectors?source=%7B%22ids%22%3A%5B%221%22%5D%7D"
```
</description><key id="26344141">4902</key><summary>mtermvectors REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T10:42:57Z</created><updated>2014-01-28T16:59:37Z</updated><resolved>2014-01-28T08:42:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>Fix potential NPE when no source and no body</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>Fix for #4902</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>mtermvectors REST API should support source parameter</comment></comments></commit></commits></item><item><title>msearch REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4901</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in msearch REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPOST "http://localhost:9200/test/type/1?refresh" -d'{
    "foo": "bar"
}'

cat requests
{}
{"query" : {"match_all" : {}}}

# This one works
curl -XGET localhost:9200/_msearch --data-binary @requests

# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}
curl -XGET "http://localhost:9200/test/type/_mget?source=%7B%7D%0A%7B%22query%22%3A%7B%22match_all%22%3A%7B%7D%7D%7D%0A"
```
</description><key id="26344123">4901</key><summary>msearch REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T10:42:32Z</created><updated>2014-01-28T16:59:37Z</updated><resolved>2014-01-28T08:42:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>Fix potential NPE when no source and no body</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file></files><comments><comment>msearch REST API should support source parameter</comment></comments></commit></commits></item><item><title>mpercolate REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4900</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in mpercolate REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPUT "http://localhost:9200/test/.percolator/1" -d'
{
    "query" : {
        "match" : {
            "foo" : "bar"
        }
    }
}'

# This one works
curl -XPOST "http://localhost:9200/test/message/_mpercolate" -d '
{"percolate" : {}}
{"doc" : {"foo" : "bar is in foo"}}
'

# This one gives: BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate];
curl -XGET "http://localhost:9200/test/message/_mpercolate?source=%7B%22percolate%22%3A%7B%7D%7D%0A%7B%22doc%22%3A%7B%22foo%22%3A%22bar is in foo%22%7D%7D%0A"
```
</description><key id="26344095">4900</key><summary>mpercolate REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T10:42:06Z</created><updated>2014-01-28T16:59:37Z</updated><resolved>2014-01-28T08:42:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>Fix potential NPE when no source and no body</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file></files><comments><comment>mpercolate REST API should support source parameter</comment></comments></commit></commits></item><item><title>Ignore internal errors if JVM can't find the memory pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4899</link><project id="" key="" /><description /><key id="26343485">4899</key><summary>Ignore internal errors if JVM can't find the memory pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-27T10:30:31Z</created><updated>2014-07-16T21:49:10Z</updated><resolved>2014-01-28T10:18:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-28T10:07:40Z" id="33465270">LGTM!, weird jvm :)
</comment><comment author="s1monw" created="2014-01-28T10:18:27Z" id="33466015">pushed...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 4.6.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4898</link><project id="" key="" /><description>This upgrade includes a fix for RAM estimation on IndexReader
that allows to expose the amount of used bytes per segment now
as a setting in Elasticsearch. (LUCENE-5373)

Additionally this bugfix release contained a small fix for highlighting
that was already ported to Elasticsearch when reported (LUCENE-5361)

Closes #4897
</description><key id="26342582">4898</key><summary>Upgrade to Lucene 4.6.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>upgrade</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T10:14:29Z</created><updated>2015-08-25T13:26:01Z</updated><resolved>2014-01-28T09:54:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-27T10:33:34Z" id="33356527">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Upgrade to Lucene 4.6.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4897</link><project id="" key="" /><description>The Lucene 4.6.1 vote passed so once it's available on the maven mirrors we should upgrade all our branches
</description><key id="26342524">4897</key><summary>Upgrade to Lucene 4.6.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T10:13:23Z</created><updated>2014-01-28T18:22:52Z</updated><resolved>2014-01-28T09:54:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-27T10:38:50Z" id="33356865">@s1monw I suppose we need to update all lang plugins as well?
</comment><comment author="s1monw" created="2014-01-27T10:41:25Z" id="33357014">well ideally yes! @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java</file></files><comments><comment>Upgrade to Lucene 4.6.1</comment></comments></commit></commits></item><item><title>Queries with preference local use different JSON parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4896</link><project id="" key="" /><description>When querying with "preference": "_local" a different JSON parser is used.

`$ curl -XPUT 'http://ks398280.kimsufi.com:9200/test/test/1' -d '{"user": "foo", "amount": 35, "data": "some more data"}'`

``` json
$ curl -XPOST 'http://ks398280.kimsufi.com:9200/test/test/_search?pretty' -d  '{
    "query": {
        "match": {
            "data": {
                "query": "some"
             }
        }
    },
    "fields": ["user", "amount"],
}'

{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[p5CLZtRvSC-3spyvNBNVng][test][4]: SearchParseException[[test][4]: query[data:some],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\": {\n        \"match\": {\n            \"data\": {\n                \"query\": \"some\"\n             }\n        }\n    },\n    \"fields\": [\"user\", \"amount\"],\n}]]]; nested: JsonParseException[Unexpected character ('}' (code 125)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@7a04355; line: 10, column: 2]]; }{[p5CLZtRvSC-3spyvNBNVng][test][3]: SearchParseException[[test][3]: query[data:some],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\": {\n        \"match\": {\n            \"data\": {\n                \"query\": \"some\"\n             }\n        }\n    },\n    \"fields\": [\"user\", \"amount\"],\n}]]]; nested: JsonParseException[Unexpected character ('}' (code 125)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@7a04355; line: 10, column: 2]]; }{[p5CLZtRvSC-3spyvNBNVng][test][2]: SearchParseException[[test][2]: query[data:some],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\": {\n        \"match\": {\n            \"data\": {\n                \"query\": \"some\"\n             }\n        }\n    },\n    \"fields\": [\"user\", \"amount\"],\n}]]]; nested: JsonParseException[Unexpected character ('}' (code 125)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@7a04355; line: 10, column: 2]]; }{[p5CLZtRvSC-3spyvNBNVng][test][0]: SearchParseException[[test][0]: query[data:some],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\": {\n        \"match\": {\n            \"data\": {\n                \"query\": \"some\"\n             }\n        }\n    },\n    \"fields\": [\"user\", \"amount\"],\n}]]]; nested: JsonParseException[Unexpected character ('}' (code 125)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@7a04355; line: 10, column: 2]]; }{[p5CLZtRvSC-3spyvNBNVng][test][1]: SearchParseException[[test][1]: query[data:some],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\": {\n        \"match\": {\n            \"data\": {\n                \"query\": \"some\"\n             }\n        }\n    },\n    \"fields\": [\"user\", \"amount\"],\n}]]]; nested: JsonParseException[Unexpected character ('}' (code 125)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@7a04355; line: 10, column: 2]]; }]",
  "status" : 400
}
```

But when using preference _local, the error disappears. Note that the error is the extra comma after the fields.

``` json
$ curl -XPOST 'http://ks398280.kimsufi.com:9200/test/test/_search?pretty' -d  '{
    "query": {
        "match": {
            "data": {
                "query": "some"
             }
        },
        "preference": "_local"
    },
    "fields": ["user", "amount"],
}'

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.15342641,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.15342641, "_source" : {"user": "foo", "amount": 35, "data": "some more data"}
    } ]
  }
}
```
</description><key id="26340420">4896</key><summary>Queries with preference local use different JSON parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sk-</reporter><labels /><created>2014-01-27T09:34:31Z</created><updated>2014-04-25T07:05:59Z</updated><resolved>2014-04-25T07:05:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-04-25T07:05:59Z" id="41364381">Closing this, duplicate of #4895
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Queries with preference local do not respect requested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4895</link><project id="" key="" /><description>When querying with "preference": "_local", the _source is returned instead of the requested fields.

`$ curl -XPUT 'http://ks398280.kimsufi.com:9200/test/test/1' -d '{"user": "foo", "amount": 35, "data": "some more data"}'`

``` json
$ curl -XPOST 'http://ks398280.kimsufi.com:9200/test/test/_search?pretty' -d  '{
    "query": {
        "match": {
            "data": {
                "query": "some"
             }
        }
    },
    "fields": ["user", "amount"]
}'

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.15342641,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.15342641,
      "fields" : {
        "amount" : 35,
        "user" : "foo"
      }
    } ]
  }
}
```

``` json
$ curl -XPOST 'http://ks398280.kimsufi.com:9200/test/test/_search?pretty' -d  '{
    "query": {
        "match": {
            "data": {
                "query": "some"
             }
        },
        "preference": "_local"
    },
    "fields": ["user", "amount"],
}'

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.15342641,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.15342641, "_source" : {"user": "foo", "amount": 35, "data": "some more data"}
    } ]
  }
}
```
</description><key id="26340245">4895</key><summary>Queries with preference local do not respect requested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">sk-</reporter><labels /><created>2014-01-27T09:31:13Z</created><updated>2014-04-25T09:31:35Z</updated><resolved>2014-04-25T07:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-01-27T17:42:33Z" id="33400189">The parameter `"preference": "_local"` can only be set as parameter in the uri. The reason is that this information is needed to decide on which shard the search request should be executed that is, before the request body is parsed. 

The reason why the fields were not returned is that the query parser did not check if there is an additional field defined after the actual query and this breaks the whole parsing process. Hence my pull request.
</comment><comment author="spinscale" created="2014-01-28T07:51:19Z" id="33457502">I think #4896 is a duplicate and can be closed, but please verify
</comment><comment author="brwe" created="2014-04-25T07:05:25Z" id="41364344">Closing this, fixed in #4913
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Throw exception if an additional field was placed inside the "query" body</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/search/SearchService.java</file></files><comments><comment>Throw exception if an additional field was placed inside the "query" body</comment></comments></commit></commits></item><item><title>cleanup of aggregations api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4894</link><project id="" key="" /><description>- add javadocs
- remove Iterable from all multi-bucket aggregations
- all single-bucket aggregations should have getDocCount() and getAggregations()
- all multi-bucket aggregations should have getBuckets() that returns Collection
- every bucket in all multi-bucket aggregations should have these methods:
  - getKey() : String
  - getKeyAsText() : Text
  - getKeyAsNumber() : Number (if the key can be numeric value, eg. range &amp; histograms)
  - getKeyAsGeoPoint() : GeoPoint (in case of the geohash_grid agg)

Closes #4922
</description><key id="26339649">4894</key><summary>cleanup of aggregations api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T09:18:50Z</created><updated>2015-06-07T15:53:23Z</updated><resolved>2014-01-28T12:31:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-27T20:39:25Z" id="33419898">A couple of minor late evening notes, which are not directly linked with this PR, but might be valid for a clean up:

### Testing code

Instead of casting test values, maybe it makes sense to have own assertions which do this? This is hard to read

```
// casting kills readability

// not readable
assertThat(bucket.getFrom().doubleValue(), equalTo((double) date(3, 15).getMillis()));
assertThat(key(bucket), equalTo("" + (double) i));
```

while we are talking about assertions :-)

```
// shorter
assertThat(bucket.getFrom().doubleValue(), equalTo(500.0));
assertThat(bucket.getTo().doubleValue(), equalTo(1000.0));
// to be
assertBucketBoundaries(bucket, 500.0, 1000.0)
```

and another one making test code maybe more readable

```
assertThat(histo.getBuckets().size(), equalTo(4));
assertBucketCount(histo, 4);
```

### Generics use

One minor thing struck me (I am not a generics guy, so maybe this commit is right, quite a few generics at work here). When checking `InternalGlobal`, `InternalFilter` `InternalMissing` and `InternalNesting` (all extends `InternalSingleBucketAggregation`), they all have a class declaration like this:

```
public class Foo extends InternalSingleBucketAggregation&lt;Foo&gt; ...
```

However the generic version used in `InternalSingleBucketAggregation` is never really needed. I think you can from

```
public abstract class InternalSingleBucketAggregation&lt;B extends InternalSingleBucketAggregation&lt;B&gt;&gt; extends InternalAggregation implements SingleBucketAggregation {
```

to

```
public abstract class InternalSingleBucketAggregation extends InternalAggregation implements SingleBucketAggregation
```

and make it more readable and remove `@SuppressWarnings("unchecked")` - as the generic value is never used
</comment><comment author="kimchy" created="2014-01-27T20:48:59Z" id="33420996">LGTM
</comment><comment author="uboness" created="2014-01-27T20:49:27Z" id="33421039">++1 on `assertBucketBoundaries(bucket, 500.0, 1000.0)` except the numbers.. I don't like numbers when it comes to dates - it's not readable, and when reading you're left to do the math... `date(3,15)` read `15th March`.

re the generics use... you're absolutely right! it's a left over from an old version of the api where the `B` was used in the public interface of the code... no need for it anymore and indeed will be much cleaner without.

Thx, for the awesome review (always good to have extra pair of eyes on things ;))
</comment><comment author="uboness" created="2014-01-28T12:31:38Z" id="33474096">closed by #4922 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mget REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4893</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in mget REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPOST "http://localhost:9200/test/type/1?refresh" -d'{
    "foo": "bar"
}'

# This one works
curl -XPOST "http://localhost:9200/test/type/_mget" -d'{
    "ids": ["1"]
}'

# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}
curl -XGET "http://localhost:9200/test/type/_mget?source=%7B%22ids%22%3A%20%5B%221%22%5D%7D"
```

Closes #4892.
</description><key id="26337564">4893</key><summary>mget REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:REST</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T08:31:31Z</created><updated>2015-06-07T23:59:35Z</updated><resolved>2014-01-27T10:30:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-27T09:23:55Z" id="33351789">@dadoonet  can we have a REST spec / test for this as well? Would be great!

otherwise looks great
</comment><comment author="s1monw" created="2014-01-27T10:00:42Z" id="33354223">awesome LGTM lets fix that on all applicable branches 
</comment><comment author="dadoonet" created="2014-01-27T10:30:45Z" id="33356336">Fixed in 0.90, 1.0, 1.x and master branch (2.0)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>mget REST API should support source parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4892</link><project id="" key="" /><description>As stated in documentation, we should support `?source=` parameter in mget REST operations.

This is how to reproduce it:

``` sh
curl -XDELETE "http://localhost:9200/test"

curl -XPOST "http://localhost:9200/test/type/1?refresh" -d'{
    "foo": "bar"
}'

# This one works
curl -XPOST "http://localhost:9200/test/type/_mget" -d'{
    "ids": ["1"]
}'

# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}
curl -XGET "http://localhost:9200/test/type/_mget?source=%7B%22ids%22%3A%20%5B%221%22%5D%7D"
```
</description><key id="26337346">4892</key><summary>mget REST API should support source parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-27T08:25:28Z</created><updated>2014-10-27T02:43:27Z</updated><resolved>2014-01-27T10:16:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rb2k" created="2014-10-27T02:43:27Z" id="60544830">I just ran into this:

```
Elasticsearch::Transport::Transport::Errors::BadRequest: [400] {"error":"ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@1]","status":400}
gems/elasticsearch-transport-1.0.6/lib/elasticsearch/transport/transport/base.rb:132 __raise_transport_error
gems/elasticsearch-transport-1.0.6/lib/elasticsearch/transport/transport/base.rb:224 perform_request
gems/elasticsearch-transport-1.0.6/lib/elasticsearch/transport/transport/http/manticore.rb:33 perform_request
gems/elasticsearch-transport-1.0.6/lib/elasticsearch/transport/client.rb:111 perform_request
gems/elasticsearch-api-1.0.6/lib/elasticsearch/api/actions/mget.rb:70 mget
```

I tried using:

``` ruby
      :transport_class =&gt; Elasticsearch::Transport::Transport::HTTP::Manticore
```

Is this a possible regression?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestActions.java</file><file>src/main/java/org/elasticsearch/rest/action/termvector/RestMultiTermVectorsAction.java</file></files><comments><comment>Fix potential NPE when no source and no body</comment></comments></commit><commit><files /><comments><comment>Revert mget yaml test changes</comment></comments></commit><commit><files /><comments><comment>Revert mget yaml test changes</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file></files><comments><comment>mget REST API should support source parameter</comment></comments></commit></commits></item><item><title>Add throttling to snaphost and restore operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4891</link><project id="" key="" /><description>Closes #4855
</description><key id="26319863">4891</key><summary>Add throttling to snaphost and restore operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-26T18:59:29Z</created><updated>2015-06-07T16:07:02Z</updated><resolved>2014-01-29T15:45:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-28T21:04:36Z" id="33525160">+1, tested this locally and it works great.
</comment><comment author="s1monw" created="2014-01-28T21:09:57Z" id="33525686">left some minor comments otherwise LGTM
</comment><comment author="s1monw" created="2014-01-29T15:04:24Z" id="33591984">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't throttle the translog stage of recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4890</link><project id="" key="" /><description>After copying the index files (which are throttled), we currently throttle the translog as well. The translog phase3 part is performed under a lock, so its better not to throttle it at all, and move it as fast as possible.
</description><key id="26294038">4890</key><summary>Don't throttle the translog stage of recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Translog</label><label>enhancement</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-25T14:47:29Z</created><updated>2015-06-07T16:00:16Z</updated><resolved>2014-01-25T14:52:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-25T14:52:36Z" id="33290557">pushed.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster state toXContent serialization only returns needed data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4889</link><project id="" key="" /><description>In order to make sure, that only the requested data is returned to the client,
a couple of fixes have been applied in the ClusterState.toXContent() method.
Also some tests were added to the yaml test suite

Closes #4885
</description><key id="26291962">4889</key><summary>Cluster state toXContent serialization only returns needed data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:REST</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-25T12:22:08Z</created><updated>2015-06-08T00:00:04Z</updated><resolved>2014-01-27T11:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-27T09:52:22Z" id="33353656">LGTM
</comment><comment author="s1monw" created="2014-01-27T10:19:38Z" id="33355564">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>non-leaf fields in search request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4888</link><project id="" key="" /><description>Hi,

I'm trying to adapt an app that uses `0.90.7` to `1.0.0.RC1` and I have an issue with what appears to be an undocumented breaking change.

I want to fetch only certain fields in a search query. I've been using the [fields](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-fields.html) to limit the fields I want to receive.

Some of the desired fields are non-leaf fields. With `1.0.0.RC1`, I get an error `ElasticsearchIllegalArgumentException[field [price] isn't a leaf field]`.

Here is a simplified mapping : 

``` json
{
   "partner_results": {
      "mappings": {
         "result": {
            "properties": {
               "name": { "type": "string", "index": "not_analyzed" },
               "price": {
                  "properties": {
                     "avg_cents": { "type": "long" },
                     "currency": { "type": "string" },
                     "total_cents": { "type": "long" }
}}}}}}}
```

The request : 

``` json
{
  "fields":["name", "price"],
    "query" : { "term" : { "name" : "kimchy" }
}}
```

The response : 

``` json
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 1,
    "failures": [{
      "index": "partner_results",
      "shard": 4,
      "status": 400,
      "reason": "ElasticsearchIllegalArgumentException[field [price] isn't a leaf field]"
    }]
  },
  "hits": {
    "total": 34,
    "max_score": 1.0,
    "hits": []
  }
}
```

I haven't found anything in the 1.0 documentation nor in the "breaking changes" about that.
Is there a solution?

Thanks
</description><key id="26290878">4888</key><summary>non-leaf fields in search request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlecour</reporter><labels /><created>2014-01-25T10:58:11Z</created><updated>2014-09-15T21:14:25Z</updated><resolved>2014-01-25T11:34:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-25T11:35:46Z" id="33286950">Hi @jlecour 

Yes, use the `_source` parameter instead.  http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-request-source-filtering.html

I've updated the breaking changes doc to add this info.

thanks
</comment><comment author="vizanto" created="2014-04-15T08:44:31Z" id="40458328">The doc says "wildcard pattern" but not what kind of pattern this is. Regular expression? Or should I just end everything with .\* ?
</comment><comment author="bleskes" created="2014-04-15T08:52:53Z" id="40459001">It’s expressions with potentially `*` them: `"xxx*”` `"*xxx”`, `"*xxx*”` and `"xxx*yyy”`

On 15 Apr 2014, at 10:44, Danny Wilson notifications@github.com wrote:

&gt; The doc says "wildcard pattern" but not what kind of pattern this is. Regular expression? Or should I just end everything with .\* ?
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Updated the breaking changes for the fields param</comment></comments></commit></commits></item><item><title>MultiSearch hangs forever + EsRejectedExecutionException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4887</link><project id="" key="" /><description>im getting this error logged after executing many multisearch requests concurrently (snippet below for single multisearch):

``` java
MultiSearchRequestBuilder builder = _client.prepareMultiSearch();
for (final String index: indexes)
{
    final SearchRequestBuilder request = _client.prepareSearch(index).setPreference("_local");
    request.setQuery(QueryBuilders.termQuery("group_id", id)).setSize(100).setTimeout("10s");
    builder.add(request);
}

final MultiSearchResponse response = builder.get();
```

exception call stack:

```
org.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution (queue capacity 1000) on org.elasticsearch.acti
        at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.ja
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.ja
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.ja
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.start(TransportSearchTypeAction.java:190)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:59
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:49
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:108)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:63)
        at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:39)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
        at org.elasticsearch.client.support.AbstractClient.multiSearch(AbstractClient.java:242)
        at org.elasticsearch.action.search.MultiSearchRequestBuilder.doExecute(MultiSearchRequestBuilder.java:79)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
        at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67)
...
```

the worst thing about this issue is that it hangs 'forever' on **MultiSearchRequestBuilder .get()** method (call stack of hanging thread below):

```
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67)
...
```

context:
- es version 1.0.0.RC1
- 3 node cluster
- 20 indexes (around 2,000,000 documents per index)
- 12 shards per index
- 2 replicas
</description><key id="26285410">4887</key><summary>MultiSearch hangs forever + EsRejectedExecutionException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-25T03:01:10Z</created><updated>2014-12-29T10:30:33Z</updated><resolved>2014-05-05T07:25:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-25T03:05:58Z" id="33279583">which version are you running?
</comment><comment author="kimchy" created="2014-01-25T03:07:05Z" id="33279598">also, can you paste the full log of the failure, it is cut off.
</comment><comment author="kimchy" created="2014-01-25T03:14:19Z" id="33279738">side note, the rejections are expected, we have a limit on the search thread pool (3x cores), with a limited queue size (1000), so if you will overload (3x cores + 1000), requests will start to get rejected. This is a good thing, since it will make sure the servers are not being overloaded. The fact that its gets stuck, thats weird (And the logs + version would help).
</comment><comment author="karol-gwaj" created="2014-01-25T03:29:38Z" id="33280008">full log:

```
[2014-01-25 02:07:39,613][DEBUG][action.search.type       ] [&lt;node name&gt;] [&lt;index name&gt;][2], node[WTCscW1_R7uA7juvJ1lacg], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@46d6608e] lastShard [true]
org.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution (queue capacity 1000) on org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$4@3bdb7cad
        at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:289)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:296)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:296)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.start(TransportSearchTypeAction.java:190)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:59)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:49)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:108)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:63)
        at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:39)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
        at org.elasticsearch.client.support.AbstractClient.multiSearch(AbstractClient.java:242)
        at org.elasticsearch.action.search.MultiSearchRequestBuilder.doExecute(MultiSearchRequestBuilder.java:79)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
        at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67)
        ... concealed ...
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

```

and im on version 1.0.0.RC1

also i started seeing this problem after switching my code from using post filter to using query, so the code:

```
MultiSearchRequestBuilder builder = _client.prepareMultiSearch();
for (final String index: indexes)
{
    final SearchRequestBuilder request = _client.prepareSearch(index).setPreference("_local");
    request.setPostFilter(FilterBuilders.termFilter("group_id", id)).setSize(100).setTimeout("10s");
    builder.add(request);
}
```

was working fine
</comment><comment author="orenorgad" created="2014-02-11T14:20:15Z" id="34758065">Hi Shay, I'm running 2 bulk requests (each with 420 requests, but on ES queue that translate into more than 1000 requests) and getting the same exception for the second bulk. That worked fine when running on version 0.90.2, but now that I've upgraded to 0.90.10 I'm getting the exception. Was there a change in the queue size or reject behavior since  0.90.2?

note: the request doesn't hang.

Thanks in advanced,
</comment><comment author="tvinod" created="2014-05-01T22:30:02Z" id="41964428">I am facing the exact same issue.. so what is the solution or workaround??
</comment><comment author="kimchy" created="2014-05-01T22:33:29Z" id="41964708">@orenorg yes, the defaults were added post 0.90.2 to set the queue size to make sure the server doesn't get overloaded

@tvinod if you get rejected failures, then you need to add more capacity to your cluster if you expect it to handle such load. Overloading it without protection will just cause it to fall over.

@karol-gwaj sorry to get back late, but did you try to upgrade to latest version and see if it got solved?
</comment><comment author="tvinod" created="2014-05-01T22:36:40Z" id="41964980">Thanks @kimchy . I understand the rejected failures exception. My issue is that my java call on the client side hangs forever. that shouldn't happen. it should either return an error or throw an exception. im on the latest 1.1.0 version.
</comment><comment author="kimchy" created="2014-05-01T22:37:35Z" id="41965054">@tvinod if it hangs then its a problem, is there a chance that you can write a repro for this? we will try and repro it as well again...
</comment><comment author="tvinod" created="2014-05-01T22:43:45Z" id="41965524">i have a 100% repro but i think its because of my setup, amount of data i
have in ES and the client pattern.. i can try my best to see if i can write
a repro for you.
but if there is any instrumentation that you want me to do, either on
client side or ES config side, i can do it.

let me know.

On Thu, May 1, 2014 at 3:38 PM, Shay Banon notifications@github.com wrote:

&gt; @tvinod https://github.com/tvinod if it hangs then its a problem, is
&gt; there a chance that you can write a repro for this? we will try and repro
&gt; it as well again...
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4887#issuecomment-41965054
&gt; .
</comment><comment author="kimchy" created="2014-05-01T22:46:07Z" id="41965689">@tvinod its tricky with instrumentation, because of the async nature of ES..., I wrote a very simple program that continuously simulates rejections and it doesn't seem to get stuck, so a repro (you can mail me privately) would go a long way to help solve this.
</comment><comment author="tvinod" created="2014-05-01T23:11:43Z" id="41967580">ok, ill let you know when i have something for you..

but if it helps - in my case, it hangs when the number of requests in the
multisearch is 26. its not a magic number, it just happens to be in my case.

thanks

On Thu, May 1, 2014 at 3:46 PM, Shay Banon notifications@github.com wrote:

&gt; @tvinod https://github.com/tvinod its tricky with instrumentation,
&gt; because of the async nature of ES..., I wrote a very simple program that
&gt; continuously simulates rejections and it doesn't seem to get stuck, so a
&gt; repro (you can mail me privately) would go a long way to help solve this.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4887#issuecomment-41965689
&gt; .
</comment><comment author="kimchy" created="2014-05-01T23:35:35Z" id="41969132">I believe I managed to recreate it, tricky... . Hold on the repro for now, will continue to work on it.
</comment><comment author="kimchy" created="2014-05-01T23:51:28Z" id="41970085">I managed to recreate it under certain conditions (very tricky), this is similar in nature to #4526, and it happens because the rejection exception happens on the calling thread, so its not on a forked thread. I will think about how this can be solved, we should fix this case cleanly in ES, but thats a biggish refactoring potentially, will update this issue...
</comment><comment author="kimchy" created="2014-05-04T01:30:41Z" id="42121255">I finally found the problem (see pull request above), it didn't relate to #4526 at the end, but wrong management of how we iterate over the shard iterator between copies of the same shard.
</comment><comment author="tvinod" created="2014-05-05T06:46:45Z" id="42162736">great, any ETA on the fix? whats the best way to get it..
thanks

On Sat, May 3, 2014 at 6:31 PM, Shay Banon notifications@github.com wrote:

&gt; I finally found the problem (see pull request above), it didn't relate to
&gt; #4526 https://github.com/elasticsearch/elasticsearch/issues/4526 at the
&gt; end, but wrong management of how we iterate over the shard iterator between
&gt; copies of the same shard.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4887#issuecomment-42121255
&gt; .
</comment><comment author="billynewport" created="2014-05-13T19:45:20Z" id="43002990">I'm stuck on this now also. I'm running 1.1.1 ES server and 1.0.2 java client. It just hangs in the client on actionGet(). Is this a server or client bug? It was working but when I increased the amount I'm bulking, the problem started. Any ETA?
</comment><comment author="situ2011" created="2014-12-25T05:26:33Z" id="68087946">I got this too, any solution? I'm running es 1.4.0 and java 1.7
</comment><comment author="clintongormley" created="2014-12-29T10:30:33Z" id="68246694">@situ2011 this was fixed in 1.1.2.  If you're seeing something similar please open a new issue with all necessary details.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchQueryThenFetchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/test/java/org/elasticsearch/action/RejectionActionTests.java</file></files><comments><comment>Search might not return on thread pool rejection</comment><comment>When a thread pool rejects the execution on the local node, the search might not return.</comment><comment>This happens due to the fact that we move to the next shard only *within* the execution on the thread pool in the start method. If it fails to submit the task to the thread pool, it will go through the fail shard logic, but without "counting" the current shard itself. When this happens, the relevant shard will then execute more times than intended, causing the total opes counter to skew, and for example, if on another shard the search is successful, the total ops will be incremented *beyond* the expectedTotalOps, causing the check on == as the exit condition to never happen.</comment><comment>The fix here makes sure that the shard iterator properly progresses even in the case of rejections, and also includes improvement to when cleaning a context is sent in case of failures (which were exposed by the test).</comment><comment>Though the change fixes the problem, we should work on simplifying the code path considerably, the first suggestion as a followup is to remove the support for operation threading (also in broadcast), and move the local optimization execution to SearchService, this will simplify the code in different search action considerably, and will allow to remove the problematic #firstOrNull method on the shard iterator.</comment><comment>The second suggestion is to move the optimization of local execution to the TransportService, so all actions will not have to explicitly do the mentioned optimization.</comment><comment>fixes #4887</comment></comments></commit></commits></item><item><title>Install SecurityManager inside ElasticsearchTestCase for easier randomization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4886</link><project id="" key="" /><description>We currently run always with SecurityManager installed. To make sure we work also without we should randomly swap it out ie. run without the security manager.
</description><key id="26271737">4886</key><summary>Install SecurityManager inside ElasticsearchTestCase for easier randomization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-24T21:17:53Z</created><updated>2014-06-26T14:54:13Z</updated><resolved>2014-01-24T21:53:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-24T21:20:32Z" id="33262346">LGTM, ++
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filtered cluster state still returns stub elements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4885</link><project id="" key="" /><description>This request:

```
GET /_cluster/state/nodes
```

should only return the `nodes` element, but it still returns `metadata`, `routing_table` etc.  They're empty, but still present.
</description><key id="26251496">4885</key><summary>Filtered cluster state still returns stub elements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-24T16:18:11Z</created><updated>2014-01-27T11:32:33Z</updated><resolved>2014-01-27T11:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-24T19:13:47Z" id="33251877">Good catch, the toXContent serialization still checks for the old filter_ parameters.. will fix and add tests
</comment><comment author="spinscale" created="2014-01-25T12:33:26Z" id="33287928">fixed, also added a bunch of yaml tests, maybe you can take a look
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file></files><comments><comment>Cluster state toXContent serialization only returns needed data</comment></comments></commit></commits></item><item><title>Add resetSort() methods to SearchSourceBuilder?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4884</link><project id="" key="" /><description>Greetings,
In order to use JavaAPI for search request building, it would be really handy to modify or reset contents of `sorts`, `facets` etc. What do you think?
</description><key id="26251379">4884</key><summary>Add resetSort() methods to SearchSourceBuilder?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nfx</reporter><labels /><created>2014-01-24T16:16:21Z</created><updated>2014-02-15T14:41:34Z</updated><resolved>2014-02-15T14:41:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-13T19:29:05Z" id="35015778">I would probably just create a new `SearchSourceBuilder` instead or reusing the same and resetting its content, does that make sense @nfx ?
</comment><comment author="nfx" created="2014-02-13T21:42:03Z" id="35029499">makes sense. i actually did almost the way you suggested. but sometimes it could be useful to have a method to create one SearchSourceBuilder from another and i didn't find method for that
</comment><comment author="javanna" created="2014-02-14T18:37:58Z" id="35111567">Hey @nfx sorry but I'm not sure I follow... can you elaborate on what you are trying to achieve? You have an existing `SearchSourceBuilder` and you'd like to create a new one with same content?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Inner objects fields aren't indexed in _all field with analyzers from mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4883</link><project id="" key="" /><description>I have model with the following mapping:

``` json
"companies": {
    "properties": {
        "taxId": {
            "type": "string",
            "analyzer": "remove_special_characters_analyzer",
            "boost": 3.0
        },

        "contactInfos": {
            "properties": {
                "number": {
                    "type": "string",
                    "analyzer": "remove_special_characters_analyzer"
                },
                "zipCode": {
                    "type": "string",
                    "analyzer": "remove_special_characters_analyzer"
                }
            }
        }
    }
}
```

When I search for company using match query on contactInfos.zipCode field, I recieve correct results (it seems that field was indexed using remove_special_characters_analyzer). But when I search using match query on _all field it seems that the same field was saved there using default analyzer. I have checked it using skywalker and it confirmed my suspicions. For taxId field everything seems to be fine.

For sure: I use remove_special_characters_analyzer for both queries.
</description><key id="26251293">4883</key><summary>Inner objects fields aren't indexed in _all field with analyzers from mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsikorski</reporter><labels /><created>2014-01-24T16:15:09Z</created><updated>2014-01-25T13:41:59Z</updated><resolved>2014-01-25T13:41:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-25T13:41:59Z" id="33289177">HI @jsikorski 

This is not a bug - it is the way it is intended to be.  The `_all` field is just a string field like any other string field.   It just takes the string values of other fields and analyzes them, it doesn't use the terms from other fields. It defaults to using the `standard` analyzer, and you can choose to use a different analyzer on it. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolator response always returns the `matches` key.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4882</link><project id="" key="" /><description>Relates to  #4881
</description><key id="26248241">4882</key><summary>Percolator response always returns the `matches` key.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-24T15:35:56Z</created><updated>2015-06-08T00:00:32Z</updated><resolved>2014-01-27T15:39:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-27T15:39:35Z" id="33377995">pushed to master, 1.x and 1.0 branches.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolator response should always return the `matches` key</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4881</link><project id="" key="" /><description>Only including the `matches` key can be cumbersome for anyone parsing the percolator response. Also it is inconsistent with the search api, where the `hits` key is always returned regardless if there are any hits.

So we should always include the `matches` key in the percolator response.
</description><key id="26245706">4881</key><summary>Percolator response should always return the `matches` key</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-24T14:59:39Z</created><updated>2014-01-27T15:38:36Z</updated><resolved>2014-01-27T15:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateResponse.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Percolator response now always returns the `matches` key.</comment></comments></commit></commits></item><item><title>Check if term exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4880</link><project id="" key="" /><description>For example i have two different analyzers. And i made two "_suggest" requests:

&lt;pre&gt;
{
  "PRODUCT": {
    "text" : "hammer",
    "term" : {
      "field" : "name",
      "suggest_mode": "missing",
    }
  }
}
&lt;/pre&gt;

If both requests returns me some options: [...], then i will choose option with max score (similarity) and send to user did you mean "..."?
But what will be my decision if one of requests returns options, and second is not? Does it mean, that term exists and user input is correct and i don't need to bother user with "did you mean" from options from first request. Or it means that there is no such term in index with second analyzer, so i ought to fix misprint with options from first request?

Can you add such functionality, or maybe it already exists?
</description><key id="26243323">4880</key><summary>Check if term exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kamapcuc</reporter><labels><label>:Suggesters</label><label>feedback_needed</label></labels><created>2014-01-24T14:23:37Z</created><updated>2015-02-28T05:02:33Z</updated><resolved>2015-02-28T05:02:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T18:54:26Z" id="68069427">Hi @Kamapcuc 

Sorry it has taken a while to look at this one.  I'm afraid I don't understand the description. Could you explain with more examples please?
</comment><comment author="clintongormley" created="2015-02-28T05:02:33Z" id="76510930">No more info provided. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow for executing queries based on pre-defined templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4879</link><project id="" key="" /><description>It would be nice to be able to store pre-defined query templates that can be referenced and filled with parameter values at query time. This can be in particular useful to be able to quickly replay queries with slightly different templates but identical parameter values e.g. in order to compare slightly different ways to formulate Elasticsearch queries wrt. runtime performance and actual results returned.

For developing and testing the template, both, template_string and template_vars can be submitted as part of the search request:

``` json
GET _search
{
    "query": {
        "template": {
            "template_string": "{\"match_{{template}}\": {}}\"",
            "template_vars" : {
                "template" : "all"
            }
        }
    }
}
```

You register a template by storing it in the conf/scripts directory of
elasticsearch. In order to execute the stored template reference it in the query parameters:

``` json
GET _search
{
    "query": {
        "template": {
            "template_string": "storedTemplate",
            "template_vars" : {
                "template" : "all"
            }
        }
    }
}
```
## Template language

Templating is based on Mustache. Substitution of tokens works as follows:

``` json
            "template_string": "{\"match_{{template}}\": {}}\"",
            "template_vars" : {
                "template" : "all"
```
</description><key id="26234514">4879</key><summary>Allow for executing queries based on pre-defined templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>:Query DSL</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-24T11:25:34Z</created><updated>2015-06-06T18:43:46Z</updated><resolved>2014-02-20T13:20:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-24T11:38:42Z" id="33216189">can we also support 

```
{
    "query": {
        "template": {
            "query": {
                "match_{{template}}" : {}
            }
            "params" : {
                "template" : "all"
            }
        }
    }
}
```

this is much more readable? we can also make it super fast by using a new `XContentBuilder` and calling `copyCurrentStructure`, and running the template on it.

UPDATE:

Also, based on above, I would go with 

```
{
    "query": {
        "template": {
            "query": "{\"match_{{template}}\": {}}\"",
            "params" : {
                "template" : "all"
            }
        }
    }
}
```

when in "string/bytes" mode, we can detect that, and its more readable.
</comment><comment author="uboness" created="2014-01-24T11:43:56Z" id="33216495">or 

```
{
    "query": {
        "template": {
            "template": {
                "match_{{template}}" : {}
            }
            "vars" : {
                "template" : "all"
            }
        }
    }
}
```
</comment><comment author="uboness" created="2014-01-24T11:49:56Z" id="33216828">and

```
{
    "query": {
        "template": {
            "name": "storedTemplate",
            "vars" : {
                "template" : "all"
            }
        }
    }
}
```
</comment><comment author="MaineC" created="2014-01-24T11:53:21Z" id="33216989">Makes sense - makes it also easier to simply copy/paste existing queries into templates.
</comment><comment author="gedl" created="2014-07-10T20:34:29Z" id="48660267">Is it possible to create templates at runtime as opposed to pre-package them in the config folder ?
</comment><comment author="clintongormley" created="2014-07-11T11:52:14Z" id="48721697">@gedl yes - see [template query](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-template-query.html#query-dsl-template-query) and [search template](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file></files><comments><comment>Add some more documentation to TemplateQueryParser</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/TemplateQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/TemplateQueryParser.java</file><file>src/main/java/org/elasticsearch/indices/query/IndicesQueriesModule.java</file><file>src/main/java/org/elasticsearch/script/ScriptModule.java</file><file>src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryBuilderTest.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryParserTest.java</file><file>src/test/java/org/elasticsearch/index/query/TemplateQueryTest.java</file><file>src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTest.java</file><file>src/test/java/org/elasticsearch/script/mustache/MustacheTest.java</file></files><comments><comment>Add mustache templating to query execution.</comment></comments></commit></commits></item><item><title>Migrated p/c queries from id cache to field data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4878</link><project id="" key="" /><description>PR for #4930
</description><key id="26231337">4878</key><summary>Migrated p/c queries from id cache to field data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-24T10:23:47Z</created><updated>2015-06-07T15:49:52Z</updated><resolved>2014-02-26T19:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-02-25T16:31:21Z" id="36026619">Hey Martijn, I think this is a very clean change. I left comments, but that is mostly me being picky, merging is very close.
</comment><comment author="martijnvg" created="2014-02-26T00:22:43Z" id="36076582">@jpountz I addressed your feedback (except the BytesRefHash#clear()) 

Also I removed the IdCache and its related classes, but the `_parent` field data stats remain to be reported under the id cache in node stats and related apis for bwc and are omitted from the regular field data part on node stats api and friends.
</comment><comment author="jpountz" created="2014-02-26T09:02:16Z" id="36104474">I left a few minor comments, but once they are addressed, +1 to push. Regarding the backward compatibility layer that you added for the `_stats` API, the plan is to only have it on 1.x (not master), correct?
</comment><comment author="martijnvg" created="2014-02-26T09:22:06Z" id="36105899">Yes, the backward comp. layer will only exist in 1.x
</comment><comment author="martijnvg" created="2014-02-26T11:22:19Z" id="36115277">I updated the PR, the ParentChildAtomicFieldData#getBytesValues also now properly supports 'needHashes' and sorts values if a doc is both parent and child.
</comment><comment author="martijnvg" created="2014-02-26T13:57:47Z" id="36126760">Added another commit that fixed an unnecessary performance drop for ParentQuery and ParentConstantQuery. I think this PR is very close to get merged in! 
</comment><comment author="martijnvg" created="2014-02-26T15:41:54Z" id="36138478">Your comments make sense, I updated the PR.
</comment><comment author="jpountz" created="2014-02-26T16:08:31Z" id="36142076">+1, I think it is ready now!
</comment><comment author="martijnvg" created="2014-02-26T16:31:05Z" id="36144855">woohoo :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Analysis:  protecting tokens based on their length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4877</link><project id="" key="" /><description>I would like to be able to tell the keyword marker to protect tokens 1-4 characters in length, or tell the minimal english stemmer to ignore tokens shorter than 5 characters.

Perhaps the more generic thing to have would be a Minimum Length Keyword Marker that could go in front of the other filters.

Based on discussion at https://groups.google.com/forum/#!msg/elasticsearch/uFlKWq2HvQk/mM8KjaItPH0J
</description><key id="26202108">4877</key><summary>Analysis:  protecting tokens based on their length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">loren</reporter><labels><label>:Analysis</label><label>adoptme</label><label>feature</label></labels><created>2014-01-23T22:00:25Z</created><updated>2017-03-28T15:28:53Z</updated><resolved>2017-03-28T15:13:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ilanrivers" created="2014-12-03T12:18:43Z" id="65398944">Is there an expected date when this will be implemented?
</comment><comment author="clintongormley" created="2016-12-23T10:41:33Z" id="268971366">This should be done in Lucene - nice easy one to adopt</comment><comment author="mikemccand" created="2016-12-23T19:22:35Z" id="269034527">&gt; This should be done in Lucene - nice easy one to adopt

+1</comment><comment author="abeyad" created="2017-03-28T15:26:09Z" id="289806937">@loren we have exposed the pattern keyword marker token filter from Lucene in ES: https://github.com/elastic/elasticsearch/issues/23600

With this, you could specify your minimum token length as a regular expression pattern</comment><comment author="loren" created="2017-03-28T15:28:53Z" id="289807786">Excellent! And a much more widely useful solution than I had expected. Thanks!</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/index/analysis/KeywordMarkerTokenFilterFactory.java</file><file>core/src/test/java/org/elasticsearch/index/analysis/KeywordMarkerFilterFactoryTests.java</file></files><comments><comment>Adds pattern keyword marker filter support (#23600)</comment></comments></commit></commits></item><item><title>No index mapper found for field: [&lt;field name&gt;] returning default posting format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4876</link><project id="" key="" /><description>i have index with multiple parent/child related document types
when i bulk index parent documents im getting this warning logged for one of the **child documents**
why child documents are 'touched' when parent document is indexed ?
also im getting hundreds of this warnings per minute so any suggestion how to fix it will be really appriciated

I know that this issue was already raised before (#3088), but it was not exactly fixed (more  like hacked by failing over to default posting format, so i still hope for real fix). 

context:
- es version 1.0.0.RC1 (updated from 0.90.10)
- 3 node cluster
- 12 shards per index
- 2 replicas
</description><key id="26192326">4876</key><summary>No index mapper found for field: [&lt;field name&gt;] returning default posting format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2014-01-23T19:42:06Z</created><updated>2014-07-04T10:20:42Z</updated><resolved>2014-07-04T10:20:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karol-gwaj" created="2014-01-23T19:48:12Z" id="33161071">also i started getting this warning after upgrading from 0.90.10 to 1.0.0.RC1
</comment><comment author="sallgeud" created="2014-02-04T17:11:52Z" id="34082573">We're getting the same thing in version 0.90.5.  Will try upgrading to 0.90.10 to see if it's been fixed and regressed.  Would love to know the cause. It only happens when we're attempting to index thousands of documents a second.  Not all of them fail.
</comment><comment author="clintongormley" created="2014-02-04T17:14:28Z" id="34082871">@karol-gwaj you say you get "this" warning, but you haven't included the warning.  More info please?
</comment><comment author="clintongormley" created="2014-02-04T17:14:42Z" id="34082900">Doh - just seen it in the title :)
</comment><comment author="karol-gwaj" created="2014-02-04T17:43:06Z" id="34086102">i want to add few more pointers to the issue that could help to solve it:
- the field in question was originally of type 'multi_field' (migrated to different mapping in 1.0.0.RC1: http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/_multi_fields.html)
- i was getting this warning during bulk reindexing of parent documents, and im not seeing it so often anymore (looks like over time, as document are reindexed, it is cleaning itself up)
</comment><comment author="sallgeud" created="2014-02-04T19:40:50Z" id="34098302">We have never used multi_field 
</comment><comment author="karol-gwaj" created="2014-02-06T12:56:04Z" id="34320148">also i got a lot of this warnings after deleting child document type from index 
and then calling _optimize on it

the amount of warning was lower than the amount of documents in index, but more or less similar to amount of deleted documents in this index
could it be that elasticsearch is attempting to find mappers for deleted documents ?
</comment><comment author="jpountz" created="2014-07-04T10:20:42Z" id="48028650">Fixed through https://github.com/elasticsearch/elasticsearch/issues/3088
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo-hash grid aggregations: make size==0 return all hashes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4875</link><project id="" key="" /><description>Likewise terms aggregations (#4837), setting `size=0` on geo-hash grid aggregations should return all buckets.
</description><key id="26183735">4875</key><summary>Geo-hash grid aggregations: make size==0 return all hashes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v1.1.0</label></labels><created>2014-01-23T17:36:07Z</created><updated>2014-02-07T09:02:36Z</updated><resolved>2014-02-07T09:02:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/InternalAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BucketUtils.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file></files><comments><comment>Make size=0 return all buckets for the geohash_grid aggregation.</comment></comments></commit></commits></item><item><title>Can't set http.max_initial_line_length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4874</link><project id="" key="" /><description>I am getting this error when my Kibana time frame is more than about 6 months:

"org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: An HTTP line is larger than 4096 bytes." 

I have changed the http.max_initial_line_length to 8kb in elasticsearch.yml as described in #3210 but keep getting exactly the same error. 

Am I doing something wrong?

Thanks in advance :)
</description><key id="26179021">4874</key><summary>Can't set http.max_initial_line_length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">slavlosevs</reporter><labels /><created>2014-01-23T16:32:33Z</created><updated>2014-01-27T11:13:14Z</updated><resolved>2014-01-24T18:49:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="slavlosevs" created="2014-01-24T13:39:58Z" id="33223017">Found a workaround by changing ES_JAVA_OPTS in /etc/init.d/elasticsearch

ES_JAVA_OPTS="-Des.http.netty.maxInitialLineLength=8192"
</comment><comment author="spinscale" created="2014-01-24T18:49:06Z" id="33249802">Note that the option in elasticsearch.yml also has to be `http.netty.maxInitialLineLength: 8192` (note the netty keyword in between) in order to work.
</comment><comment author="slavlosevs" created="2014-01-27T10:56:55Z" id="33357984">I've tried your suggestion and couldn't make it work. I am not sure you can use Java syntax directly in elasticsearch.yml. 

Note that both #1174 and Elasticsearch reference guide refer to 

```
http.max_initial_line_length
```

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-http.html
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Convert common terms in query string to OR</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4873</link><project id="" key="" /><description>So far this is just a test for what I expect to work when I finish:
Add support for converting the boolean clauses built by the query_string
query for chunks of query without any special syntax into common_terms
queries.  These should keep the query string's default operator.

Also add a configuration to the query string to switch the operator for
the low frequency terms in generated common terms queries to OR even if
the default query operator is AND.

Closes #4839
</description><key id="26178074">4873</key><summary>Convert common terms in query string to OR</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-23T16:21:04Z</created><updated>2014-06-18T09:54:04Z</updated><resolved>2014-03-05T16:50:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-23T16:21:48Z" id="33139546">I figured it'd be worth uploading this now even though I've barely really started so folks can get a sense of what my goal is.
</comment><comment author="roytmana" created="2014-01-26T22:07:48Z" id="33332133">Hi Nik,

If I undersand correctly you are trying to build an intelligent adaptive query which rewrites itself based on the data queried. Not exsvtly related to this but what do you think of query relaxa technique where a query is an array of queries whichbaee executed one after another and their results are combined. The transittion from one to the next query (relaxation) controled by specifying if it should always be done or only if prior query returned less than certain number of documents (or even via a script expression) and how to combine results for example boost for each query and whether results should be combined based on scores or appended (unique) to the prior query results or rescored, max sizes for each query ....

I think it may be pretty useful.

Any thought?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Transliterate token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4872</link><project id="" key="" /><description>I have two cases for adding transliterate token filter :
1) There are still persons who trying writing messages on some language using letters of other language. For example 'dobroe utro' that means 'доброе утро' and translates 'good morning'. 
2) Some illiterate people write english words as they hear them. For example 'гугл мапс' - it's transliteration of 'google maps'.
It will be nice to improve elasticsearch with transliterate token filter. Something like that:

&lt;pre&gt;
{
    index : {
        analysis : {
            analyzer : {
                my_analyzer : {
                    type : "custom",
                    tokenizer : "standard",
                    filter : ["standard", "lowercase", "my_filter"]
                }
            },
            filter : {
                my_filter : {
                    type : "transliterate",
                    ruleSet : "ICAO",
                    direction : "backward"
                }
            }
        }
    }
}
&lt;/pre&gt;

Sorry for bad english - it's not my first language.
</description><key id="26177959">4872</key><summary>Transliterate token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kamapcuc</reporter><labels /><created>2014-01-23T16:19:32Z</created><updated>2014-07-04T15:27:42Z</updated><resolved>2014-07-04T15:27:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-24T18:22:34Z" id="33247560">I am not entirely sure if it is sufficient but maybe the [ICU Plugin](https://github.com/elasticsearch/elasticsearch-analysis-icu) could help here. It has a transform filter that supports transliterations from cyrillic to latin etc. 
see also https://github.com/elasticsearch/elasticsearch-analysis-icu/blob/master/src/main/java/org/elasticsearch/index/analysis/IcuTransformTokenFilterFactory.java
</comment><comment author="rmuir" created="2014-07-04T11:11:42Z" id="48032008">I agree. The only possible enhancement would be to allow users to supply custom rules files to the ICU plugin if they are unhappy with what ICU provides. This is kind of tricky though, and super-advanced...
</comment><comment author="clintongormley" created="2014-07-04T15:27:42Z" id="48056158">Makes sense.  Closing.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RestTable.renderValue() doesn't know about tera and peta</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4871</link><project id="" key="" /><description /><key id="26177313">4871</key><summary>RestTable.renderValue() doesn't know about tera and peta</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T16:11:22Z</created><updated>2015-06-07T16:03:46Z</updated><resolved>2014-01-24T17:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/support/RestTable.java</file></files><comments><comment>Add tera and peta to RestTable.renderValue()</comment></comments></commit></commits></item><item><title>Wrong keyboard layout token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4870</link><project id="" key="" /><description>I'm wishing new token filter. For example, user forget change keyboard layout and starts typing query "кув ырщуы" wich means "red shoes". It will be nice to have filter, that transforms token "ырщуы" into two tokens "ырщуы", " shoes". Of couse, i can make it by myself, but it will be more kindly for all users of elasticsearch if those code will be unified and incapsulated into search engine. Example:

&lt;pre&gt;
{
    index : {
        analysis : {
            analyzer : {
                my_analyzer : {
                    type : "custom",
                    tokenizer : "standard",
                    filter : ["standard", "lowercase", "my_filter"]
                }
            },
            filter : {
                my_filter : {
                    type : "wrongLayout",
                    sourceLayout : "JCUKEN",
                    targetLayout : "QWERTY",
                    leaveSourceTokens : true
                }
            }
        }
    }
}
&lt;/pre&gt;

Sorry for bad english - it's not my first language.
</description><key id="26174929">4870</key><summary>Wrong keyboard layout token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Kamapcuc</reporter><labels /><created>2014-01-23T15:43:11Z</created><updated>2014-12-24T18:51:43Z</updated><resolved>2014-12-24T18:51:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-23T23:15:14Z" id="33180373">@Kamapcuc wouldn't multi_field with [mapping char filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-mapping-charfilter.html) work better for that?
</comment><comment author="Kamapcuc" created="2014-01-24T09:50:22Z" id="33209835">@imotov mapping char filter will leave only transformed token. So, i need to make term suggest request for "кув ырщуы". And oly after suggest request returns nothing, i can make search request "red shoes" and send to user «did you mean "red shoes"» and search results.

And with mapping char filter all elasticsearch users need to implement those again and again. It will be cool, if you incapsulate that. From the other side, incapsulation will prevent users from mystyping in mappings.
</comment><comment author="Kamapcuc" created="2014-01-24T10:06:52Z" id="33210854">@imotov but you are right - it's be better to make Wrong keyboard layout char filter rather than token filter :)
</comment><comment author="clintongormley" created="2014-12-24T18:51:43Z" id="68069311">Looks like there is nothing to do with this one. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixed an issue where there are sub aggregations executing on a single shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4869</link><project id="" key="" /><description>...ard, the reduce call was not propagated properly down the agg hierarchy.

Closed: #4843
</description><key id="26173481">4869</key><summary>Fixed an issue where there are sub aggregations executing on a single shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T15:24:15Z</created><updated>2015-06-07T23:52:34Z</updated><resolved>2014-01-23T18:25:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-23T17:02:56Z" id="33144171">Looks good to me.
</comment><comment author="uboness" created="2014-01-23T18:25:10Z" id="33152755">fixed in https://github.com/elasticsearch/elasticsearch/commit/da953700f47924f4948ec3775eb1f42f3109aac7
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation for score_mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4868</link><project id="" key="" /><description>Closes #4742
</description><key id="26173413">4868</key><summary>Documentation for score_mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-23T15:23:16Z</created><updated>2014-07-16T21:49:16Z</updated><resolved>2014-01-23T15:36:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>BalancedShardAllocator makes non-deterministic rebalance decisions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4867</link><project id="" key="" /><description>NOTE: this is not a problem in production! It happens that the allocator iterates over the keys of a set which might be different across runs. This makes unittests non-reproducible. We should tie-break on the shard ID in that case.
</description><key id="26171423">4867</key><summary>BalancedShardAllocator makes non-deterministic rebalance decisions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T14:53:58Z</created><updated>2015-08-13T15:30:38Z</updated><resolved>2014-01-23T15:16:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file></files><comments><comment>Make shard balancing deterministic if weights are identical</comment></comments></commit></commits></item><item><title>Make shard balancing deterministic if weights are identical</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4866</link><project id="" key="" /><description>It happens to be the case that the iteration order of a HashMaps
keyset might be different across runs. This can cause undeterministic
results in shard balancing if weights are identical and multiple shards
of the same index are eligable for relocation. This commit adds
a tie-breaker based on the shard ID to prioritise the lowest shard
ID. This also makes `AddIncrementallyTests#testAddNodesAndIndices`
reproducible.
</description><key id="26171203">4866</key><summary>Make shard balancing deterministic if weights are identical</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T14:50:52Z</created><updated>2015-06-08T00:01:42Z</updated><resolved>2014-01-23T15:16:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-23T14:53:28Z" id="33129986">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Implemented update by paths in order to overwrite document fields instead of merge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4865</link><project id="" key="" /><description>We've implemented a new feature for overwriting doc fields instead of merging values in.
This is done by defining the paths to sub-fields instead of submitting an updated doc.
It gives us the opportunity to overwrite a complete object by a new one, and so deleting unwanted keys.
This is maybe most useful when using dynamic mapping on object fields.
## Demonstration Example (Documentation):
### Create an index

``` shell
curl -XPUT 'http://localhost:9200/twitter'
```
### Create a document mapping with a field of type `object`

``` shell
curl -XPUT 'http://localhost:9200/twitter/tweet/_mapping' -d '{
    "tweet" : {
        "properties" : {
            "person" : {
                "type" : "object",
                "properties" : {
                    "name" : {"type" : "object"},
                    "sid" : {"type" : "string", "index" : "not_analyzed"}
                }
            },
            "message" : {"type" : "string"}
        }
    }
}'
```
### Index a document

``` shell
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "person" : {
        "name" : {
            "first_name" : "Shay",
            "last_name" : "Banon",
            "nick_name": "kimchy"
        },
        "sid" : "12345"
    },
    "message" : "This is a tweet!"
}'
```
### Update the document in order to delete the `person.name.nick_name` field
#### Can be done by submitting the paths by using the `paths` key

``` shell
curl -XPOST 'http://localhost:9200/twitter/tweet/1/_update' -d '{
    "paths" : {
        "person.name" : {
            "first_name" : "Shay",
            "last_name" : "Banon"
        }
    }
}'
```
#### Validate the updated document (`nick_name` removed)

``` shell
 curl -XGET 'http://localhost:9200/twitter/tweet/1' 
{
    "_index" : "twitter",
    "_type" : "tweet",
    "_id" : "1",
    "_version" : 2,
    "exists" : true,
    "_source" : {
        "person" : {
            "name" : {
                "first_name" : "Shay",
                "last_name" : "Banon"
            },
            "sid" : "12345"
        },
        "message" : "This is a tweet!"
    }
} 
```
#### Same can be achieved by using `doc` in conjunction with `doc_as_paths=true`

``` shell
curl -XPOST 'http://localhost:9200/twitter/tweet/1/_update' -d '{
    "doc" : {
        "person.name" : {
            "first_name" : "Shay"
        }
    },
    "doc_as_paths" : true
}'
```
#### Validate the updated document (now `last_name` is removed)

``` shell
 curl -XGET 'http://localhost:9200/twitter/tweet/1' 
{
    "_index" : "twitter",
    "_type" : "tweet",
    "_id" : "1",
    "_version" : 3,
    "exists" : true,
    "_source" : {
        "person" : {
            "name" : {
                "first_name" : "Shay"
            },
            "sid" : "12345"
        },
        "message" : "This is a tweet!"
    }
} 
```

Of course `paths` can also be used to update non-objects.
</description><key id="26168253">4865</key><summary>Implemented update by paths in order to overwrite document fields instead of merge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seut</reporter><labels /><created>2014-01-23T14:06:59Z</created><updated>2014-06-26T12:41:55Z</updated><resolved>2014-02-28T10:48:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Rivers might not get started due to missing _meta document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4864</link><project id="" key="" /><description>When a new river is registered by indexing its `_meta` document, its type gets created via dynamic mappings, which triggers a cluster state update task on the master node. The update triggers a cluster state listener (`RiversRouter`) that executes only on the master. The master node looks for the river `_meta` document (get with `preference=_primary`) and schedules a retry in case it is not found (since #4089).

Once the master node has found the `_meta` document it decides where to allocate the river and  publishes the new river cluster state containing that information. At that point each node receives the new river cluster state and the node where the river is supposed to be allocated on will start the river locally (`RiversService.ApplyRivers`).

In order for the river to be properly allocated, the `_meta` document has to be found through get api. There is a retry mechanism in case the get fails, but not in case the `_meta` document is not found, which can currently happen as this second get doesn't set `preference` to `_primary`, thus the `_meta` document could be found by the master node on the primary shard, but not on the second get call done by the node that is trying to start the river locally. This happens when the document replication hasn't been completed yet.

Long story short: no retry needed, we just need to add `preference=_primary` to the second get call.
</description><key id="26166562">4864</key><summary>Rivers might not get started due to missing _meta document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T13:38:15Z</created><updated>2014-07-28T09:39:39Z</updated><resolved>2014-01-23T14:05:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/river/RiversService.java</file></files><comments><comment>Made sure rivers _meta documents are retrieved via get with preference _primary</comment></comments></commit></commits></item><item><title>Improve Aggregations documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4863</link><project id="" key="" /><description>- Mostly minor things like typos and grammar stuff
- Some clarifications
- The note on the deprecation was ambiguous. I've removed the problematic part so that it now definitely says it's deprecated
</description><key id="26164352">4863</key><summary>Improve Aggregations documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lfrancke</reporter><labels /><created>2014-01-23T12:57:18Z</created><updated>2014-07-16T21:49:17Z</updated><resolved>2014-02-03T16:28:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-23T14:06:57Z" id="33126057">The changes are good! Could you please sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) so that I can merge these changes? Thanks!
</comment><comment author="uboness" created="2014-01-23T14:42:47Z" id="33129044">Since we didn't officially deprecate facets (in the codebase), I'd stick with `facets should be considered deprecated`
</comment><comment author="lfrancke" created="2014-01-23T15:00:05Z" id="33130631">CLA is signed, thanks.

Thanks @uboness. The fact that it's not deprecated is not clear from the current text. The text said: "_facets are_ and should be considered _deprecated_ and will likely be removed in one of the future major releases." Had at least me confused/convinced of the deprecation status.

If it is not deprecated it should say so. I suggest something like "Facets are not officially deprecated yet but are likely to be in the future." (I'm not a native speaker though so I'm happy for suggestions).
</comment><comment author="uboness" created="2014-01-23T15:09:30Z" id="33131501">&gt; "Facets are not officially deprecated yet but are likely to be in the future."

+1
</comment><comment author="jpountz" created="2014-02-03T16:28:59Z" id="33971524">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Uninstall elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4862</link><project id="" key="" /><description>I want to reinstall Elasticsearch but there is no documentation on the matter!
how should I do it ? I'm using nginx if that could add some specification
</description><key id="26156675">4862</key><summary>Uninstall elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Sshuichi</reporter><labels /><created>2014-01-23T10:25:45Z</created><updated>2014-01-24T18:50:28Z</updated><resolved>2014-01-24T18:50:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-24T18:50:28Z" id="33249913">Please ask questions like this on the mailinglist/google group, as we try to use github issues for bugs only. Thanks a lot!

Also please try to tell people what you already tried out, so it is easier to follow and give better hints. Have a nice weekend!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Mark 'lucene-expression' as 'provided' in pom.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4861</link><project id="" key="" /><description>We currently pull in the lucene-expression module that is referenced
by lucene-suggest. Yet, we don't make use of this dependency at all
and it pulls in a bunch of unshaded libs like `antlr` and `asm` which
are pretty common in other projects. We should exclude this
dependency since we don't use it at all and it causes problems
when Elasticsearch is used as a node client. (see #4858)

If we mark the dependency as provided it won't be included in the
distribution.

Closes #4859
</description><key id="26154835">4861</key><summary>Mark 'lucene-expression' as 'provided' in pom.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T09:52:12Z</created><updated>2015-06-07T16:06:21Z</updated><resolved>2014-01-23T13:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Support CBOR data format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4860</link><project id="" key="" /><description>See https://github.com/FasterXML/jackson-dataformat-cbor, would be great to support it as another format, it sounds promising. Seems like the format can also be auto detected, which is great (see `CBORParserBootstrapper`), which means we can plug it into our auto detection code in `XContentFactory`.
</description><key id="26154731">4860</key><summary>Support CBOR data format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T09:50:23Z</created><updated>2014-03-30T14:51:08Z</updated><resolved>2014-03-28T09:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Mark 'lucene-expression' as 'provided' in pom.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4859</link><project id="" key="" /><description>We currently pull in the `lucene-expression` module that is referenced by `lucene-suggest`. Yet, we don't make use of this dependency at all and it pulls in a bunch of unshaded libs like `antlr` and `asm` which are pretty common in other projects. We should exclude this dependency since we don't use it at all and it causes problems when Elasticsearch is used as a node client. (see https://github.com/elasticsearch/elasticsearch/issues/4858 ) 

If we mark the dependency as provided it won't be included in the distribution.
</description><key id="26154134">4859</key><summary>Mark 'lucene-expression' as 'provided' in pom.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Packaging</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T09:38:36Z</created><updated>2015-06-07T16:05:51Z</updated><resolved>2014-01-23T13:33:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-23T09:48:41Z" id="33109717">I think the main problem is when using elasticsearch as a embedded client jar, and then I agree, we should make sure its optional. I think that we will end up integrating with it though (as a script engine?), so it will be part of our full  distribution eventually. I think provided will work then as well.
</comment><comment author="s1monw" created="2014-01-23T09:51:58Z" id="33109931">agreed
</comment><comment author="CodingFabian" created="2014-01-27T14:55:05Z" id="33373949">Just an idea: Why not make a elasticsearch-client artifact which does only contain dependencies needed for client only nodes? Would that still require all those lucene dependencies?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Mark 'lucene-expression' as 'provided' in pom.xml</comment></comments></commit></commits></item><item><title>1.0.0.RC1 Cannot use ES client within Jersey Servlet, due to ASM incompatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4858</link><project id="" key="" /><description>ElasticSearch 1.0.0.RC1 uses ASM 4
Jersey is incompatible with ASM 4, requires 3

If you use ES as a client in a Jersey application you get an exception:

```
java.lang.IncompatibleClassChangeError: Implementing class
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at org.apache.catalina.loader.WebappClassLoader.findClass(WebappClassLoader.java:1191)
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1669)
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1547)
    at org.glassfish.jersey.server.ResourceConfig.scanClasses(ResourceConfig.java:875)
    at org.glassfish.jersey.server.ResourceConfig._getClasses(ResourceConfig.java:840)
    at org.glassfish.jersey.server.ResourceConfig.getClasses(ResourceConfig.java:755)
    at org.glassfish.jersey.server.ResourceConfig$WrappingResourceConfig._getClasses(ResourceConfig.java:1113)
    at org.glassfish.jersey.server.ResourceConfig.getClasses(ResourceConfig.java:755)
    at org.glassfish.jersey.server.ResourceConfig$RuntimeConfig.&lt;init&gt;(ResourceConfig.java:1171)
    at org.glassfish.jersey.server.ResourceConfig$RuntimeConfig.&lt;init&gt;(ResourceConfig.java:1144)
    at org.glassfish.jersey.server.ResourceConfig.createRuntimeConfig(ResourceConfig.java:1140)
    at org.glassfish.jersey.server.ApplicationHandler.&lt;init&gt;(ApplicationHandler.java:299)
    at org.glassfish.jersey.servlet.WebComponent.&lt;init&gt;(WebComponent.java:311)
    at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:169)
    at org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:359)
...
```

I am not sure if it is safe to revert the version of ASM that is within ES to 3.
</description><key id="26152424">4858</key><summary>1.0.0.RC1 Cannot use ES client within Jersey Servlet, due to ASM incompatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels /><created>2014-01-23T09:04:22Z</created><updated>2014-10-02T11:00:36Z</updated><resolved>2014-01-23T13:33:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-23T09:32:55Z" id="33108741">this is due to a lucene dependency that is pulled in from the `lucene-expression` module. We don't use this module but it's referenced through the `suggest` module that we use. It's not needed at runtime for ES so it can safely excluded. I wonder if we should mark it as provided via maven ie. using this:

``` XML
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
  &lt;artifactId&gt;lucene-expressions&lt;/artifactId&gt;
  &lt;version&gt;${lucene.version}&lt;/version&gt;
  &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
```

I will run tests to see if that helps. Can you check if that would work for you?
</comment><comment author="apatrida" created="2014-01-23T09:41:24Z" id="33109259">I excluded the module and I no longer receive the error message.  It was blocking Jersey at startup so it was quite evil and noticeable.
</comment><comment author="s1monw" created="2014-01-23T12:37:41Z" id="33120178">perfect. I will make sure that this goes in the next release `0.90` as well as `1.0RC2`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Mark 'lucene-expression' as 'provided' in pom.xml</comment></comments></commit><commit><files /><comments><comment>Mark 'lucene-expression' as 'provided' in pom.xml</comment></comments></commit></commits></item><item><title>Null Pointer Exception updating default mapping to multi_field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4857</link><project id="" key="" /><description>With 0.90.10, if I index a document with the default mapping, and then try to update the mapping to change a field type to multi_field, the call returns with a 500 error reporting a null pointer exception.

``` javascript
// curl -XPUT 'http://localhost:9200/i/t/1' -d '{"version":"1"}'
{
  "_version": 1,
  "_id": "1",
  "_type": "t",
  "_index": "i",
  "ok": true
}
// curl -XGET 'http://localhost:9200/i/t/_mapping'
{
  "t": {
    "properties": {
      "version": {
        "type": "string"
      }
    }
  }
}
// curl -XPUT 'http://localhost:9200/i/t/_mapping' -d '{"t":{"properties":{"version":{"type":"multi_field"}}}}'
{
  "status": 500,
  "error": "RemoteTransportException[[hostname.domainname][inet[/192.168.1.1:9300]][indices/mapping/put]]; nested: NullPointerException; "
}
```
</description><key id="26151999">4857</key><summary>Null Pointer Exception updating default mapping to multi_field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bwmeier</reporter><labels /><created>2014-01-23T08:53:08Z</created><updated>2014-07-23T13:53:40Z</updated><resolved>2014-07-23T13:53:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-23T10:04:54Z" id="33110834">If you upgrade a field to a multi_field type you should always add the original field as default field, like is done here:

``` bash
curl -XPUT "http://localhost:9200/i/t/_mapping" -d'
{
    "t": {
        "properties": {
            "version": {
                "type":"multi_field",
                "fields" : {
                    "version" : {
                        "type" : "string"
                    }
                }
            }
        }
    }
}'
```

In this case the default field is the field inside `fields` that has the same name as the multi_field typed field.

I think the error message (NPE) should be changed to a more descriptive error. 
</comment><comment author="clintongormley" created="2014-07-23T13:53:40Z" id="49876219">This is no longer an issue has multi-fields have changed format.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Facets with large value counts (&gt; 2 billion) return a negative number</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4856</link><project id="" key="" /><description>I have a production setup of ElasticSearch where a certain facet may return very large counts, e.g. a field value might be present more that 2 billion times. In such cases ElasticSearch will return a negative number.
</description><key id="26149124">4856</key><summary>Facets with large value counts (&gt; 2 billion) return a negative number</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eriky</reporter><labels /><created>2014-01-23T07:32:16Z</created><updated>2014-11-18T18:08:06Z</updated><resolved>2014-11-18T18:08:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-23T10:07:07Z" id="33111001">Some terms facets indeed use integers to aggregate counts, and may overflow in case of terms that match lots of documents. However, this issue should be fixed in [`aggregations`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-aggregations.html) that use longs instead.
</comment><comment author="eriky" created="2014-01-23T10:45:40Z" id="33113455">Adrien, thank you for you quick reply. Even though aggregations will fix this, can this be fixed in the current facets implementation too? It sounds like a small fix, e.g.  by using longs instead of ints? (i know.. this could be one of those "famous last words" remarks)
</comment><comment author="jpountz" created="2014-01-23T14:31:21Z" id="33128032">Yeah, that is my concern as well. :) I'll check how much change this needs.
</comment><comment author="jpountz" created="2014-11-18T18:08:06Z" id="63516501">Closing as facets are now deprecated and even removed in master.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add throttling to snaphost and restore operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4855</link><project id="" key="" /><description /><key id="26143469">4855</key><summary>Add throttling to snaphost and restore operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-23T04:09:28Z</created><updated>2014-02-03T07:26:28Z</updated><resolved>2014-01-29T15:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/RateLimitingInputStream.java</file><file>src/main/java/org/elasticsearch/repositories/Repository.java</file><file>src/main/java/org/elasticsearch/repositories/blobstore/BlobStoreRepository.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file></files><comments><comment>Add throttling to snaphost and restore operations</comment></comments></commit></commits></item><item><title>deprecate index status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4854</link><project id="" key="" /><description>With the addition of the recovery API in issue #4637 the index status API call will be redundant.  We could then deprecate and eventually eliminate the call.
</description><key id="26133995">4854</key><summary>deprecate index status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kevinkluge</reporter><labels><label>v1.2.0</label></labels><created>2014-01-22T23:58:37Z</created><updated>2014-05-07T15:39:31Z</updated><resolved>2014-05-07T15:39:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-28T08:42:25Z" id="41535180">I've deprecated the indices status API in the 1.x branch d12418eff1af73798ebe6b14f8ee6ff31f341b34 , and removed it in the master branch 4b9f1d261d244192a5409434a427bf319a461c2f

The code also needs to be deprecated/removed.
</comment><comment author="kevinkluge" created="2014-05-05T19:06:54Z" id="42225320">Alex, can you remove this code from master (master only)?  

Adding breaking label for master / 2.0.  No breaking change for 1.x ; only deprecation.
</comment><comment author="spinscale" created="2014-05-06T12:55:26Z" id="42298170">Created the above PR for master only, which completely removes the index status API (also on java level). I think it makes more sense to create an additional PR for the 1.x branch, where everything related to this API is marked as deprecated.
</comment><comment author="spinscale" created="2014-05-07T08:27:24Z" id="42401432">Referenced a commit of the 1.x branch (cant create a PR from it, as it somehow goes against master instead of 1.x and shows tons of commits), which adds `@Deprecated` annotations to all of the index status classes, quick review is appreciated and then I will get in both. See https://github.com/spinscale/elasticsearch/commit/d6edde7d20c064726c3bf8553d9a75bb13317ed2
</comment><comment author="spinscale" created="2014-05-07T15:39:30Z" id="42443326">closed by https://github.com/elasticsearch/elasticsearch/commit/f0d73a8476f4f239fd7b8b149bc3d11db7e8933e
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/DocsStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/GatewayRecoveryStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/GatewaySnapshotStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndexShardStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndexStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/IndicesStatusResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/PeerRecoveryStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/ShardStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/status/RestIndicesStatusAction.java</file><file>src/test/java/org/elasticsearch/gateway/local/SimpleRecoveryLocalGatewayTests.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/indices/state/SimpleIndexStateTests.java</file><file>src/test/java/org/elasticsearch/nested/SimpleNestedTests.java</file><file>src/test/java/org/elasticsearch/stresstest/rollingrestart/RollingRestartStressTest.java</file></files><comments><comment>Removed Index Status API</comment></comments></commit><commit><files /><comments><comment>Removed spec and YAML tests for indices.status</comment></comments></commit><commit><files /><comments><comment>Removed indices-status docs.</comment></comments></commit></commits></item><item><title>Clean up cat headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4853</link><project id="" key="" /><description>Remove duplicate cache columns, add aliases, normalize header names across actions.

For example:

```
==== nodes ====
id                       | id,nodeId                 | unique node id
pid                      | p                         | process id
host                     | h                         | host name
ip                       | i                         | ip address
port                     | po                        | bound transport port
version                  | v                         | es version
build                    | b                         | es build hash
jdk                      | j                         | jdk version
disk.avail               | d,disk,diskAvail          | available disk space
heap.percent             | hp,heapPercent            | used heap ratio
heap.max                 | hm,heapMax                | max configured heap
ram.percent              | rp,ramPercent             | used machine memory ratio
ram.max                  | rm,ramMax                 | total machine memory
load                     | l                         | most recent load avg
uptime                   | u                         | node uptime
node.type                | type,dc,nodeType          | d:data node, c:client node
master                   | m                         | m:master-eligible, *:current master
name                     | n                         | node name
completion.size          | cs,completionSize         | size of completion
fielddata.memory_size    | fm,fielddataMemory        | used fielddata cache
fielddata.evictions      | fe,fielddataEvictions     | fielddata evictions
filter_cache.memory_size | fcm,filterCacheMemory     | used filter cache
```

Closes #4852
</description><key id="26133212">4853</key><summary>Clean up cat headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-22T23:43:26Z</created><updated>2015-06-07T16:08:07Z</updated><resolved>2014-01-24T22:27:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-01-24T22:27:14Z" id="33268073">Merged 90b443492
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cat headers are inconsistent, incomplete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4852</link><project id="" key="" /><description>There are some dups and need more aliases...
</description><key id="26132914">4852</key><summary>cat headers are inconsistent, incomplete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>bug</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-22T23:36:49Z</created><updated>2014-01-28T12:10:03Z</updated><resolved>2014-01-24T16:45:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestAliasAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestHealthAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestMasterAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file></files><comments><comment>Normalize cat headers, add aliases</comment></comments></commit></commits></item><item><title>ClearScrollRequest should set a type parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4851</link><project id="" key="" /><description>Since ActionRequest requires a bounded type parameter.
</description><key id="26131517">4851</key><summary>ClearScrollRequest should set a type parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">knutwalker</reporter><labels /><created>2014-01-22T23:12:06Z</created><updated>2014-07-16T21:49:18Z</updated><resolved>2014-01-23T09:50:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-23T09:50:06Z" id="33109789">Thanks! I've pushed it to master and the 1.x branch.
</comment><comment author="s1monw" created="2014-01-23T12:36:48Z" id="33120110">@martijnvg this seems like a fix that should go to 1.0 and 0.90 as well?
</comment><comment author="martijnvg" created="2014-01-23T17:41:39Z" id="33148240">@s1monw done
</comment><comment author="knutwalker" created="2014-01-24T06:58:34Z" id="33201563">Thanks guys, see you at bbuzz ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Serving _site plugins do not pick up on index.html for sub directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4850</link><project id="" key="" /><description>If one asks for `http://es:9200/_plugin/PLUGIN_NAME/` and the the plugin's _site directory contains an index.html file, it will be correctly served.

This is not the case for sub directories: a _site/folder/index.html is not served when requesting  `http://es:9200/_plugin/PLUGIN_NAME/folder/` but one gets a 403 Forbidden response as if trying to browse the folder.

Closes #4845.
</description><key id="26107491">4850</key><summary>Serving _site plugins do not pick up on index.html for sub directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-22T17:35:46Z</created><updated>2015-06-07T16:14:10Z</updated><resolved>2014-01-23T19:42:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-01-23T08:50:51Z" id="33106370">Thanks david! looking good. left some comments
</comment><comment author="dadoonet" created="2014-01-23T09:26:59Z" id="33108407">Thanks @bleskes! I applied changes. WDYT?
BTW, in which branches should I push changes? 
</comment><comment author="bleskes" created="2014-01-23T12:18:55Z" id="33119025">left one really minor comment. O.w. LGTM . I'd vote for pushing to 1.x and 0.90.x .
</comment><comment author="dadoonet" created="2014-01-23T19:42:06Z" id="33160438">Closed by https://github.com/elasticsearch/elasticsearch/commit/f4411e697eae1ab8159208272b5253f1075b0ca2
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>WARNING "failed to prepare/warm" after upgrading from 0.90.3 to 0.90.10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4849</link><project id="" key="" /><description>I have check release note from 0.90.3 to 0.90.10 to see if warmer need some change, but nothing.

Initially reported on ML : https://groups.google.com/d/msg/elasticsearch/xkX5RVII-Gk/BRd28_zbZUIJ

Two samples stack trace :

```
[2014-01-16 17:27:00,669][WARN ][index.engine.robin       ] [integration] [m112][0] failed to prepare/warm
java.lang.IllegalMonitorStateException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.elasticsearch.search.SearchService$SearchWarmer$2.awaitTermination(SearchService.java:822)
        at org.elasticsearch.indices.warmer.InternalIndicesWarmer.warm(InternalIndicesWarmer.java:99)
        at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearchFactory.newSearcher(RobinEngine.java:1652)
        at org.apache.lucene.search.SearcherManager.getSearcher(SearcherManager.java:155)
        at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
        at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1530)
        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:277)
        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:660)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
```

```
[2014-01-16 11:17:36,836][WARN ][index.engine.robin       ] [sissor2-pp] [m105][0] failed to prepare/warm
java.lang.IllegalMonitorStateException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:503)
        at org.elasticsearch.search.SearchService$SearchWarmer$2.awaitTermination(SearchService.java:822)
        at org.elasticsearch.indices.warmer.InternalIndicesWarmer.warm(InternalIndicesWarmer.java:99)
        at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearchFactory.newSearcher(RobinEngine.java:1652)
        at org.apache.lucene.search.SearcherManager.getSearcher(SearcherManager.java:155)
        at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
        at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1530)
        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:277)
        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:660)
        at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:389)
        at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:363)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
```
</description><key id="26099153">4849</key><summary>WARNING "failed to prepare/warm" after upgrading from 0.90.3 to 0.90.10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">benoit-intrw</reporter><labels><label>bug</label><label>v0.90.11</label></labels><created>2014-01-22T15:54:18Z</created><updated>2014-01-23T10:17:49Z</updated><resolved>2014-01-23T10:17:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-23T10:14:54Z" id="33111538">Thanks for reporting this, I found the issue and will fix shortly.
</comment><comment author="jpountz" created="2014-01-23T10:17:38Z" id="33111708">Closed via c70afd2385d69c0a7ce87dba8aa45f12aa14e76d
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>DateHistogramBuilder is missing support for min_doc_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4848</link><project id="" key="" /><description>`min_doc_count` support in `date_histogram` aggregations is not supported by the DateHistogramBuilder
</description><key id="26086036">4848</key><summary>DateHistogramBuilder is missing support for min_doc_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>bug</label><label>v1.0.0</label></labels><created>2014-01-22T12:31:43Z</created><updated>2014-01-22T13:31:19Z</updated><resolved>2014-01-22T13:31:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-22T13:31:19Z" id="33021433">fixed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MinDocCountTests.java</file></files><comments><comment>Added missing support for min_doc_count in DateHistogramBuilder</comment></comments></commit></commits></item><item><title>Histogram aggregations: support decimal intervals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4847</link><project id="" key="" /><description>Fork of #3810 and #3799. Decimal intervals would be useful for certain data types such as currencies, temperatures, lengths ...
</description><key id="26085283">4847</key><summary>Histogram aggregations: support decimal intervals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label></labels><created>2014-01-22T12:18:08Z</created><updated>2016-12-23T19:09:00Z</updated><resolved>2016-08-03T06:43:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hudsonb" created="2014-09-17T15:05:34Z" id="55907618">+1
</comment><comment author="jakegibson" created="2015-02-18T20:58:25Z" id="74946447">+1
</comment><comment author="marcosg3" created="2015-04-01T22:16:08Z" id="88650779">+1
We came across this limitation on one of our projects, has there been any traction on this one?
</comment><comment author="ignaciovazquez" created="2015-04-04T17:03:56Z" id="89614640">+1
</comment><comment author="roblovelock" created="2015-04-07T13:22:45Z" id="90550635">+1
I am currently working around this issue by running a script (probably not the most efficient)

"aggs" : {
                            "decimalValueHistogram" : { 
                                "terms": {
                                    "field": "value",
                                    "script" : "_value % interval &lt; 0 ? _value -((_value % interval) + interval) : (_value -  (_value % interval))",
                                    "lang" : "expression" ,
                                    "params": {
                                        "interval" : 0.05
                                    }
                                }
                            }  
                        } 
</comment><comment author="ESamir" created="2015-05-19T17:12:57Z" id="103597492">+1
</comment><comment author="sylvinus" created="2015-05-22T22:51:05Z" id="104793369">+1. Would be incredibly useful and avoid storing lots of duplicate columns, which is our only workaround for now :/
</comment><comment author="ghost" created="2015-06-11T11:01:23Z" id="111085042">If you're not going to do this, please at least put a massive warning in the documentation! It's not mentioned at all, and I think most people's assumption would be that continuous values are supported, not just integers.

As it stands, I've spent time and money gathering data I now can't use.

+1
</comment><comment author="clintongormley" created="2015-06-12T16:16:43Z" id="111540770">@pjcardy want to send a pull request?
</comment><comment author="ejain" created="2015-06-12T16:21:35Z" id="111542916">For a facet that supports decimal intervals (and offsets), see https://github.com/zenobase/decimal-histogram-facet. I'd be happy to update it to use aggregations and contribute it to elasticsearch.
</comment><comment author="clintongormley" created="2015-06-12T17:37:21Z" id="111569861">@ejain thanks for the offer. I don't know whether or not there is some missing infrastructure that would prevent this happening.  @jpountz what do you think?
</comment><comment author="ghost" created="2015-06-15T07:28:16Z" id="111965854">@clintongormley That's a good point, sorry I should have done that in the first place. Please see above.
</comment><comment author="jpountz" created="2015-06-15T07:50:46Z" id="111970019">@clintongormley I don't think there's anything missing. I think the two main challenges will be to make sure it works fine with pipeline aggregations and that empty bucket generation (`min_doc_count:0`) also works fine if there are floating-point rounding errors.
</comment><comment author="clintongormley" created="2015-06-15T08:16:50Z" id="111974354">@ejain you've heard [the man](https://github.com/elastic/elasticsearch/issues/4847#issuecomment-111970019) :)  A PR for decimal support in aggs wouldl be greatly appreciated 
</comment><comment author="sbyim" created="2015-07-03T07:33:26Z" id="118265086">+1
</comment><comment author="alecklandgraf" created="2015-08-11T21:58:07Z" id="130090357">+1
</comment><comment author="jeffsteinmetz" created="2015-10-19T17:30:43Z" id="149290136">There is actually a typo in that last merge btw `themsevles`
</comment><comment author="nicktgr15" created="2016-01-30T23:32:01Z" id="177336761">+1
</comment><comment author="troglotit" created="2016-02-04T09:08:12Z" id="179721155">+1
</comment><comment author="GregHubs" created="2016-02-16T09:32:23Z" id="184596812">+1
</comment><comment author="tom-christie" created="2016-04-28T19:15:26Z" id="215533010">+1
</comment><comment author="DimitryDushkin" created="2016-05-06T10:16:24Z" id="217404444">+1
</comment><comment author="yossicahen" created="2016-05-19T11:02:10Z" id="220292977">+1
</comment><comment author="alicia-c" created="2016-06-09T18:01:20Z" id="224977069">+1
</comment><comment author="Hossrod" created="2016-07-19T16:59:49Z" id="233698050">+1

https://discuss.elastic.co/t/less-than-1-interval-values/55201
</comment><comment author="StianOvrevage" created="2016-08-01T23:22:18Z" id="236738541">+1!
</comment><comment author="slava-vishnyakov" created="2016-08-31T10:53:21Z" id="243729035">So, is this available only in `v5.0.0-alpha5` ? No backport?
</comment><comment author="jpountz" created="2016-08-31T13:12:56Z" id="243759294">This required a very large refactoring (see diif stats on the PR) so I am leaning towards not backporting in order to not destabilize the 2.x series so close to the release of 5.0.
</comment><comment author="slava-vishnyakov" created="2016-08-31T13:16:35Z" id="243760253">@jpountz, got it, thanks! Very useful feature indeed!
</comment><comment author="Hossrod" created="2016-08-31T13:56:12Z" id="243771969">Woot, thanks!!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>core/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>core/src/main/java/org/elasticsearch/search/SearchModule.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/Histogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregatorFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramFactory.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/BucketHelpers.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/cumulativesum/CumulativeSumPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/derivative/DerivativePipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregationBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/movavg/MovAvgPipelineAggregator.java</file><file>core/src/main/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffPipelineAggregator.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/RoundingTests.java</file><file>core/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/MissingValueIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramOffsetIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBoundsTests.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/AvgBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketScriptIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/BucketSelectorIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/CumulativeSumIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DateDerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/DerivativeIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/ExtendedStatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MaxBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/MinBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/PercentilesBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/StatsBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/SumBucketIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/moving/avg/MovAvgIT.java</file><file>core/src/test/java/org/elasticsearch/search/aggregations/pipeline/serialdiff/SerialDiffIT.java</file><file>core/src/test/java/org/elasticsearch/search/profile/aggregation/AggregationProfilerIT.java</file><file>modules/lang-expression/src/test/java/org/elasticsearch/script/expression/MoreExpressionTests.java</file></files><comments><comment>Split regular histograms from date histograms. #19551</comment></comments></commit></commits></item><item><title>Never cache a range filter that uses the now date math expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4846</link><project id="" key="" /><description>Never cache a range filter that uses the now date math expressions or compound filters that wrap this kind of filters.
</description><key id="26077734">4846</key><summary>Never cache a range filter that uses the now date math expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.11</label><label>v1.0.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-22T09:54:01Z</created><updated>2014-02-20T21:45:20Z</updated><resolved>2014-01-22T12:30:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/RangeFilterParser.java</file><file>src/test/java/org/elasticsearch/index/query/IndexQueryParserFilterCachingTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>The forceful no cache behaviour for range filter with now date match expression should only be active if no rounding has been specified for `now` in the date range range expression (for example: `now/d`).</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Never cache a range filter that uses the `now` date expression.</comment></comments></commit></commits></item><item><title>Serving _site plugins do not pick up on index.html for sub directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4845</link><project id="" key="" /><description>If one asks for `http://es:9200/_plugin/PLUGIN_NAME/` and the the plugin's _site directory contains an index.html file, it will be correctly served. 

This is not the case for sub directories: a _site/folder/index.html is not served when requesting  `http://es:9200/_plugin/PLUGIN_NAME/folder/` but one gets a 403 Forbidden response as if trying to browse the folder.
</description><key id="26077407">4845</key><summary>Serving _site plugins do not pick up on index.html for sub directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-22T09:48:17Z</created><updated>2014-01-28T17:39:51Z</updated><resolved>2014-01-28T17:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/http/HttpServer.java</file><file>src/test/java/org/elasticsearch/plugin/SitePluginTests.java</file></files><comments><comment>Serving _site plugins do not pick up on index.html for sub directories</comment></comments></commit></commits></item><item><title>[DOCS] Various small documentation fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4844</link><project id="" key="" /><description>Not sure if this PR should contain multiple commits so that you can cherry-pick or shall I squash the commits?

Most changes are tiny, but the documented transport.tcp.connect_timeout should be incorrect. It perhaps was the main cause of instability in my cluster since I "raised" it to 5s. :)
</description><key id="26062577">4844</key><summary>[DOCS] Various small documentation fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">brusic</reporter><labels /><created>2014-01-22T02:33:32Z</created><updated>2014-07-16T21:49:20Z</updated><resolved>2014-01-23T09:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-22T08:50:46Z" id="33002828">looks all good to me
</comment><comment author="javanna" created="2014-01-22T20:38:17Z" id="33065144">Hey @brusic thanks a lot for the PR, we left a couple of comments, could you fix those two things so that we can merge this in?
</comment><comment author="brusic" created="2014-01-23T05:32:54Z" id="33098415">If I understand @clintongormley's issue correctly, I have changed the wording slightly regarding segment merges. I removed the part regarding "low number of segments" and combined the two paragraphs. Please let me know if I misunderstood the intent.
</comment><comment author="javanna" created="2014-01-23T09:57:05Z" id="33110290">Squashed and merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>min_doc_count=0 doesn't work with a date_histogram with a filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4843</link><project id="" key="" /><description>I'm trying to create a date_histogram for recent events, where days where no events happen are still shown.

```
{
  "aggs": {
    "events_last_week": {
      "filter": {
        "range": {
          "@timestamp": {
            "from": "2014-01-10"
          }
        }
      },
      "aggs": {
        "events_last_week_histogram": {
          "date_histogram": {
            "min_doc_count": 0,
            "field": "@timestamp",
            "format": "yyyy-MM-dd",
            "interval": "1d"
          }
        }
      }
    }
  }
}
```

I get a response like this

```
"aggregations":  {
  "events_last_week": {
    "doc_count": 33861,
    "events_last_week_histogram": [
      {
        "key_as_string": "2014-01-10",
        "key": 1389744000000,
        "doc_count": 2120
      }, {
        "key_as_string": "2014-01-16",
        "key": 1389830400000,
        "doc_count": 3823
      }, {
        "key_as_string": "2014-01-17",
        "key": 1389916800000,
        "doc_count": 27918
      }
    ]
  }
}
```

The empty days are not returned. If I construct the query without the filter, the empty days are returned correctly.

There is also an issue even when the empty days are returned correctly without the filter. If, for example, today is "2014-01-22", and the latest timestamp in my data is "2014-01-17", then the 5 days between these two dates are not returned as empty buckets, though all the empty buckets prior to "2014-01-17" are returned correctly.
</description><key id="26061428">4843</key><summary>min_doc_count=0 doesn't work with a date_histogram with a filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cmaitchison</reporter><labels><label>bug</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-22T02:00:14Z</created><updated>2015-06-18T19:49:24Z</updated><resolved>2014-01-23T18:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-22T10:27:13Z" id="33009257">@cmaitchison

I can't really reproduce it, I ran the same queries as you and I get the right responses. What es version are you working with? we introduced `min_doc_count` on `1.0.0.RC1`

&gt; There is also an issue even when the empty days are returned correctly without the filter. If, for example, today is "2014-01-22", and the latest timestamp in my data is "2014-01-17", then the 5 days between these two dates are not returned as empty buckets, though all the empty buckets prior to "2014-01-17" are returned correctly.

the gaps that are filled are based on the dates in the documents you're aggregating... so the first histogram bucket will be  based on the earliest date in the document set and the last bucket will be based on the latest date in the set... then we fill in all gaps between these two buckets.

we can consider adding a "range" settings to the histograms which will enable to define the value range (or date range in case of `date_histogram`) on which the buckets will be created. In your case, that'll mean that if you define a range of the form `"range": { "to" : "now" }` along with `"min_doc_count" : 0` we'll return all the empty buckets until `now` (beyond the dates in the document set)
</comment><comment author="uboness" created="2014-01-22T10:31:54Z" id="33009587">@cmaitchison scratch that... I finally managed to reproduce it (it happens when you have a single shard)... will work on a fix
</comment><comment author="cmaitchison" created="2014-01-22T10:53:55Z" id="33011343">Wow, nice find! I would never have thought to have mentioned that.

&gt; On 22 Jan 2014, at 21:32, uboness notifications@github.com wrote:
&gt; 
&gt; @cmaitchison scratch that... I finally managed to reproduce it (it happens when you have a single shard)... will work on a fix
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="cmaitchison" created="2014-01-23T05:17:41Z" id="33097873">Also related to this title, I've found that `min_doc_count=0` does not work if _all_ of the buckets would be empty after applying the filter. I can reproduce this issue on an index with 2 shards.

```
{
  "aggs": {
    "filtered_events": {
      "filter": {
        "and": [
          {
            "range": {
              "@timestamp": {
                "from": 1390267500000,
                "to":   1390267560000
              }
            }
          }
        ]
      },
      "aggs": {
        "filtered_events_histogram": {
          "date_histogram": {
            "min_doc_count": 0,
            "field": "@timestamp",
            "interval": "1s"
          }
        }
      }
    }
  }
}
```

The above query should return 60 results, 1 for each second in the minute. If any events are found in that minute then 60 results are returned. If no events are found in that minute then 0 results are returned, when you would expect 60 empty buckets.

My use case is zooming in on a series on a chart. The zero value results are very helpful to know where to plot the zeros on the x-axis.
</comment><comment author="cmaitchison" created="2014-01-23T05:59:27Z" id="33099319">Another related issue I am finding is that sometimes the intervals do not go back far enough.

```
{
  "aggs": {
    "events_last_week": {
      "filter": {
        "and": [
          {
            "range": {
              "@timestamp": {
                "from": 1390267432894,
                "to": 1390267547037
              }
            }
          }
        ]
      },
      "aggs": {
        "events_last_week_histogram": {
          "date_histogram": {
            "min_doc_count": 0,
            "field": "@timestamp",
            "interval": "second"
          }
        }
      }
    }
  }
}
```

returns exactly

```
{
  "aggregations": {
    "events_last_week": {
      "doc_count": 1099,
      "events_last_week_histogram": [
        {
          "key": 1390267526000,
          "doc_count": 12
        },
        {
          "key": 1390267527000,
          "doc_count": 0
        },
        {
          "key": 1390267528000,
          "doc_count": 29
        },
        {
          "key": 1390267529000,
          "doc_count": 32
        },
        {
          "key": 1390267530000,
          "doc_count": 58
        },
        {
          "key": 1390267531000,
          "doc_count": 64
        },
        {
          "key": 1390267532000,
          "doc_count": 35
        },
        {
          "key": 1390267533000,
          "doc_count": 36
        },
        {
          "key": 1390267534000,
          "doc_count": 43
        },
        {
          "key": 1390267535000,
          "doc_count": 52
        },
        {
          "key": 1390267536000,
          "doc_count": 58
        },
        {
          "key": 1390267537000,
          "doc_count": 62
        },
        {
          "key": 1390267538000,
          "doc_count": 76
        },
        {
          "key": 1390267539000,
          "doc_count": 70
        },
        {
          "key": 1390267540000,
          "doc_count": 53
        },
        {
          "key": 1390267541000,
          "doc_count": 72
        },
        {
          "key": 1390267542000,
          "doc_count": 81
        },
        {
          "key": 1390267543000,
          "doc_count": 48
        },
        {
          "key": 1390267544000,
          "doc_count": 88
        },
        {
          "key": 1390267545000,
          "doc_count": 45
        },
        {
          "key": 1390267546000,
          "doc_count": 83
        },
        {
          "key": 1390267547000,
          "doc_count": 2
        }
      ]
    }
  }
}
```

But it is missing all of the empty buckets between 1390267432894 and 1390267526000. Again, this is with a 2 shard index on 1.0.0RC1.
</comment><comment author="uboness" created="2014-01-23T15:41:03Z" id="33134557">@cmaitchison as I mentioned above, the histogram operates on the dataset and extracts the min/max of the histogram from the documents (the earliest/latest). There is no direct relations between the filter aggregation and the histogram aggregations (aggregations are unaware of other aggregations in their hierarchy). We could potentially add a `range` feature to histogram, but if we do it'll have to be post 1.0.

In the first example you gave, there are no documents in that minute, there are no buckets (as we can't determine the min/max values). For the second example, it might be that the first document in the doc set has a later timestamp than the `from` one in the filter.
</comment><comment author="cmaitchison" created="2014-01-23T21:13:08Z" id="33169088">Thanks, @uboness, for your help and excellent explanation. `range` on histogram is definitely a feature I would use. For now I can fill in the gaps on the client-side. Thanks again.
</comment><comment author="uboness" created="2014-01-23T21:18:54Z" id="33169638">@cmaitchison no worries... thank you for the bug report! important one!
</comment><comment author="erikvanzijst" created="2014-01-28T18:22:17Z" id="33507524">I'm interested in hard range boundaries (returning empty buckets to fill gaps between from and to in the case of missing documents) as well. Is there an issue tracking this, or shall I raise one?
</comment><comment author="deanchen" created="2015-04-28T02:13:54Z" id="96878284">For anyone who arrived to this thread via Google, hard ranges is supported via the extended_bounds param. http://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html
</comment><comment author="taf2" created="2015-06-18T14:40:27Z" id="113177682">I'm now experiencing the same issue as reported running es 1.6.0 

```
histogram = {
  invervals: {
    date_histogram: {
      field: 'called_at',
      interval: 'day',
      order: { _key: "asc" },
      min_doc_count: 0 # doesn't appear to have any impact on the final result.
    },
    aggs: stats
  }
}
```
</comment><comment author="taf2" created="2015-06-18T15:47:14Z" id="113198062">it looks like when nesting a date_histogram within a term aggregation there is no way for the min_doc_count to auto fill the zero results.

```
aggs: {
   groups: {
     terms: {
       min_doc_count: 0
       script: '...'
    },
   aggs: {
   invervals: {
    date_histogram: {
      field: 'called_at',
      interval: 'day',
      order: { _key: "asc" },
      min_doc_count: 0 # doesn't appear to have any impact on the final result.
    },
    aggs: stats
  }
  }
}
```
</comment><comment author="clintongormley" created="2015-06-18T19:49:24Z" id="113270250">@taf2 please could you open an issue with a complete recreation which explains the problem?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/InternalAggregations.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/SingleBucketAggregation.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/InternalGeoHashGrid.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/AbstractRangeBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoHashGridTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardReduceTests.java</file></files><comments><comment>Fixed an issue where there are sug aggregations executing on a single shard, the reduce call was not propagated properly down the agg hierarchy.</comment></comments></commit></commits></item><item><title>Make all aggregators reader-context-aware.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4842</link><project id="" key="" /><description>This removes the overhead of polling a Bytes/Double/Long-Values instance in
every call to collect.

Additionally, the AggregationsCollector has been changed to wrap a simple array
instead of an ArrayList.

HistogramAggregationSearchBenchmark reports response times that are ~10%
better.

Close #4841
</description><key id="26058317">4842</key><summary>Make all aggregators reader-context-aware.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-22T00:53:43Z</created><updated>2014-06-18T10:58:27Z</updated><resolved>2014-01-22T10:16:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-22T09:39:55Z" id="33006039">+1 to push to 1.x
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove some abstraction in aggregations collection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4841</link><project id="" key="" /><description>I have been running some basic aggregations benchmarks (the ones in src/test) and it appears that some of the abstraction in aggregations is causing a slight performance hit. The thing is that `collect` is called in a very tight loop so even stuff that usually doesn't matter seems to have an impact here.

For instance I tried to change some aggregators to implement ReaderContextAware instead of polling for Bytes/Long/DoubleValues in every call to collect and this improved the response times by a few percents.
</description><key id="26058095">4841</key><summary>Remove some abstraction in aggregations collection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.1.0</label></labels><created>2014-01-22T00:48:50Z</created><updated>2014-01-22T10:16:16Z</updated><resolved>2014-01-22T10:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/Aggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/filter/FilterAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/geogrid/GeoHashGridAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/global/GlobalAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/UnmappedTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggegator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java</file></files><comments><comment>Make all aggregators reader-context-aware.</comment></comments></commit></commits></item><item><title>Full snapshot API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4840</link><project id="" key="" /><description>Incremental snapshots are great, but sometimes full snapshots are more desirable. With the current snapshot api, creating a full snapshot requires creating a new repository, then a new snapshot.

```
$ curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d '{
  "type": "fs",
  "settings": {
    "location": "/mount/backups/my_backup",
    "compress": true
  }
}'

$ curl -XPUT "localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true"
```

It would be great if it was possible to force a full backup on snapshot create with an `{ "incremental": false }` or similar option. Or a `{ "snapshot": true }` option to repository create to immediately create a snapshot in the new repository.

Allowing multiple full snapshots in a repository would be ideal. If that's not possible, then multiple repositories need to be supported for all repository types. It's not clear whether the S3 plugin supports multiple or dynamically defined repositories: https://github.com/elasticsearch/elasticsearch-cloud-aws#s3-repository

/cc @imotov
</description><key id="26057780">4840</key><summary>Full snapshot API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels /><created>2014-01-22T00:41:22Z</created><updated>2014-10-29T16:39:19Z</updated><resolved>2014-10-29T16:39:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-22T01:27:18Z" id="32985108">@grantr could you elaborate a bit on why full snapshots are more desirable? I would like to better understand the your use case for this feature. I am not sure I understand why having the same file copied several times to the same drive is better than having a full separate copy in another drive, bucket or maybe even region. Could you help me better understand the rationale behind this request? 

You should be able to create multiple S3 repositories. I will update documentation to make it clearer. 
</comment><comment author="grantr" created="2014-01-22T02:41:36Z" id="32988654">@imotov The use case I'm thinking of is off-site backup of an index that is rotated every month; that is, every month has totally new data in it. Imagine you want to take a final snapshot of the index that was rotated out at the end of each month.

In this case there is no advantage to incremental backups. A single repository stores all snapshots as a single unit even if they share no data. Having self-contained snapshots is more convenient because older snapshots can be relocated, verified, and deleted as needed by external tools without affecting other snapshots and without knowledge of the internal snapshot structure.

It seems like this can be accomplished today by creating a new repository for every snapshot. If this is the recommendation going forward, then I'd like a `?snapshot=true` option to create a new snapshot right away when a repository is created. Used this way, a repository is equivalent to a snapshot and we only need the repository create and snapshot restore APIs.
</comment><comment author="imotov" created="2014-01-23T03:19:37Z" id="33093838">@grantr I think I am starting to understand your use case. So, basically, a full snapshot would mean "create a new snapshot and delete all previous snapshots" or maybe other way around "delete all snapshots from the repository and create a new one". Did I understand it correctly? 
</comment><comment author="grantr" created="2014-03-21T18:01:23Z" id="38304690">Yes, I think that's the idea. Probably would be preferable to create a new snapshot first, then delete to avoid any time when there are zero valid snapshots.

I think such functionality is probably too complicated. Currently single-snapshot repositories can be used as full snapshots. That use case should be officially supported.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add cutoff_frequency to more query types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4839</link><project id="" key="" /><description>`cutoff_frequency` seems really useful for eliminating stopword filters and increasing relevance, but it's only available in the `common_terms` and `match` queries right now. If you're using a query string query, you still need stopword filters.

Can `cutoff_frequency` be supported in other query types like `query_string` or `simple_query_string`?

/cc @dakrone
</description><key id="26056143">4839</key><summary>Add cutoff_frequency to more query types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label></labels><created>2014-01-22T00:05:00Z</created><updated>2015-09-10T00:07:24Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-22T00:44:32Z" id="32981672">I planned on working on a few common terms things on query_string pretty soon. Things like converting common terms from and to or and, lower boosting, and of course the common term lookup disable. 
</comment><comment author="s1monw" created="2014-01-22T09:44:23Z" id="33006339">+1 this makes lots of sense to me. We just need to make sure we are making things clear that this will only really work with certain operators!
</comment><comment author="nik9000" created="2014-01-23T16:27:06Z" id="33140161">I've gotten a small start on this - mostly just the tests of what I imagine it should work like.  Also, just for `query_string` at the moment.

The two things I hope to do are:
Add `common_terms_cutoff_frequency` to the `query_string` (and eventually `simple_query_string`) that converts common terms that aren't explicitly already in some syntax into a common terms query.
Add `switch_common_terms_to_or` to the `query_string` (and eventually `simple_query_string`) that switches the low frequency operator from the default operator to `OR`.

I think that is actually enough to have stop words like behaviour.  The default similarity should handle making sure that the stopwords have very little effect on the score.
</comment><comment author="dan-blanchard" created="2015-09-03T20:37:03Z" id="137567677">I was very surprised to see that `more_like_this` doesn't support `cutoff_frequency`, since that seems like it would be one of the places where this could be very helpful.
</comment><comment author="clintongormley" created="2015-09-06T12:25:16Z" id="138078742">@dan-blanchard MLT already has `min_term_freq` and `max_query_terms`.  Not sure what benefit cutoff freq would bring?
</comment><comment author="dan-blanchard" created="2015-09-10T00:07:24Z" id="139077232">@clintongormley `cutoff_frequency` supports relative document frequency, which is really nice feature.

I'm using analyzers that include stopwords, because that is the current recommendation.  To have faster searches it seems like `max_doc_freq` would be closer to `cutoff_frequency`, but again, that is just an integer rather than a relative frequency.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms aggregations: make size=0 return all terms.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4838</link><project id="" key="" /><description>Terms aggregations return up to `size` terms, so up to now, the way to get all
matching terms back was to set `size` to an arbitrary high number that would be
larger than the number of unique terms.

Terms aggregators already made sure to not allocate memory based on the `size`
parameter so this commit mostly consists in making `0` an alias for the
maximum integer value in the TermsParser.

Close #4837
</description><key id="26049026">4838</key><summary>Terms aggregations: make size=0 return all terms.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-21T22:08:05Z</created><updated>2014-06-24T14:03:34Z</updated><resolved>2014-01-22T10:16:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms aggregations: make size=0 return all terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4837</link><project id="" key="" /><description>This is a fork of #1776 for terms aggregations.

Setting `size: 0` should make the number of terms returned by terms aggregations unlimited.
</description><key id="26047464">4837</key><summary>Terms aggregations: make size=0 return all terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v1.1.0</label></labels><created>2014-01-21T21:45:51Z</created><updated>2014-01-23T17:36:07Z</updated><resolved>2014-01-22T10:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-21T22:15:38Z" id="32970158">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/UnmappedTerms.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file></files><comments><comment>Terms aggregations: make size=0 return all terms.</comment></comments></commit></commits></item><item><title>Documentation search putting java and groovy APIs over json APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4836</link><project id="" key="" /><description>I normally receive these kinds of bug reports so it is fun filing one.  I think it is important that the documentation present the json api first because it is more generally useful then the language specific api.

So if you:
1.  Go to the documentation, say http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search.html
2.  Type "search api" into the search box.

The first result is for the java api and second is for the groovy api.  The json api is ninth.  It really ought to be first.

I'm not sure if the right solution is to name the json search api page "search api" instead of "search apis" or if it'd be better to lower the score of the non-json apis with a document boost or something and expect people to type "search api java" if they want the java api which should bring it back to the top.
</description><key id="26042676">4836</key><summary>Documentation search putting java and groovy APIs over json APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-21T20:35:32Z</created><updated>2015-04-04T17:41:11Z</updated><resolved>2015-04-04T17:41:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-21T21:47:16Z" id="32967217">I suggest we don't even have them (Java and Groovy) as part of the search box
</comment><comment author="dadoonet" created="2014-01-21T22:34:26Z" id="32971984">+1 for keeping it simple as possible for now and removing everything which is not REST API.
In the future, I would love to have faceted (aggregated? :-) ) navigation which would allow users to filter results from current, master, Java, js, ... Other clients or other parts of the .org website.

For example, searching for snapshot and restore is not possible right now as we only index 0.90 (current) branch.

My 0.05 cents.
</comment><comment author="javanna" created="2015-03-20T07:39:35Z" id="83943770">This seems to be solved with the new website... unless I am missing something :)

Thoughts @clintongormley ?
</comment><comment author="clintongormley" created="2015-04-04T17:41:11Z" id="89625256">This is definitely better on the new site, and more improvements will follow.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_cat/allocation doesn't show disk percentage when 100% of disk is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4835</link><project id="" key="" /><description>Example:

```
∴ get 'localhost:9200/_cat/allocation?v'
shards diskUsed diskAvail diskTotal diskPercent host         ip           node
    17   55.4mb   117.4mb   172.8mb          32 Xanadu.local 192.168.0.30 Jonathan Richards
    16     39mb        0b      39mb             Xanadu.local 192.168.0.30 Amiko Kobayashi
    17   53.7mb    43.8mb    97.6mb          55 Xanadu.local 192.168.0.30 Shadow King
```

The `diskPercent` field is missing.
</description><key id="26013179">4835</key><summary>_cat/allocation doesn't show disk percentage when 100% of disk is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>bug</label></labels><created>2014-01-21T16:29:46Z</created><updated>2014-01-24T22:25:04Z</updated><resolved>2014-01-24T22:25:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file></files><comments><comment>_cat/allocation: disk.avail can be zero, if so, want to show 100%</comment></comments></commit></commits></item><item><title>Throw an ElasticsearchIllegalArgumentException when allocating on a non-data node.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4834</link><project id="" key="" /><description>Today, it would fail with a NullPointerException.

Close #4833
</description><key id="26011674">4834</key><summary>Throw an ElasticsearchIllegalArgumentException when allocating on a non-data node.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-21T16:10:51Z</created><updated>2014-07-04T21:52:34Z</updated><resolved>2014-01-21T16:37:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-21T16:15:56Z" id="32900384">LGTM :+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allocation on a non-data node causes a NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4833</link><project id="" key="" /><description>I mistakenly tried to allocate a shard on a node which is not a data-node and this resulted in a cryptical `NullPointerException`. We should throw a meaningful exception instead.
</description><key id="26010536">4833</key><summary>Allocation on a non-data node causes a NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-21T15:55:31Z</created><updated>2014-01-21T16:37:30Z</updated><resolved>2014-01-21T16:31:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-21T16:11:58Z" id="32899934">hey this should go into the 1.0 release branch as well right?
</comment><comment author="jpountz" created="2014-01-21T16:12:24Z" id="32899994">I was just going to ask about it. :-)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file></files><comments><comment>Throw an ElasticsearchIllegalArgumentException when allocating on a non-data node.</comment></comments></commit></commits></item><item><title>Document `index.shard.check_on_startup`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4832</link><project id="" key="" /><description>This option has existed for years and is convenient to check corrupted indices.
</description><key id="26005249">4832</key><summary>Document `index.shard.check_on_startup`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-21T14:45:39Z</created><updated>2014-06-27T15:41:50Z</updated><resolved>2014-01-21T14:58:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-21T14:49:35Z" id="32891037">LGTM, great to finally have this documented.
</comment><comment author="jpountz" created="2014-01-21T14:58:31Z" id="32891886">Pushed, thanks for the review @dakrone 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Nodes don't reattach to cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4831</link><project id="" key="" /><description>We are currently running a 2 node cluster in Azure. 

We have set up unicast configuration with addresses for both servers.

The cluster runs fine but periodically for no reason connectivity is lost. This disconnects the nodes from each other (They don't crash) but bizarrely they both then get set up as master. They don't however see each other afterwards. 

The only way we get them to talk to each other again is by restarting 1 of the nodes.

Is there a setting I'm missing ?

It's like it only connects the servers on startup, polls to see if they're still connected but when they're disconnected they never check it that node returns.
</description><key id="26002110">4831</key><summary>Nodes don't reattach to cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vinnytheviking</reporter><labels /><created>2014-01-21T13:56:48Z</created><updated>2014-02-21T15:02:14Z</updated><resolved>2014-02-21T15:02:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bitonp" created="2014-01-25T09:03:45Z" id="33284589">Two issues there... I have found a similar thing, but it was down to me..
1. Are all nodes exactly the same version of ES? I found that if a node was 0.90.5 and the rest were 0.90.9 then I got this issue.
1. Server times... are your servers set at the same time, or within the time allowed in your config? We have installed NTP to ensure that all servers in our cluster(s) are within 30s of each other. Times on machines do change, and need to be synched.

hth 
</comment><comment author="vinnytheviking" created="2014-01-25T09:53:08Z" id="33285333">Thanks Peter.

The versions are the same. I will however check the server times.

Thanks 

Vinny

-----Original Message-----
From: "Peter Colclough" notifications@github.com
Sent: ‎25/‎01/‎2014 09:04
To: "elasticsearch/elasticsearch" elasticsearch@noreply.github.com
Cc: "vinnytheviking" vincent.vanderwalt@gmail.com
Subject: Re: [elasticsearch] Nodes don't reattach to cluster (#4831)

Two issues there... I have found a similar thing, but it was down to me..
1. Are all nodes exactly the same version of ES? I found that if a node was 0.90.5 and the rest were 0.90.9 then I got this issue.
Server times... are your servers set at the same time, or within the time allowed in your config? We have installed NTP to ensure that all servers in our cluster(s) are within 30s of each other. Times on machines do change, and need to be synched. 
hth 
—
Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2014-01-25T12:01:13Z" id="33287383">Hi vinny

You are suffering a "split brain". The nodes lose communication and each
one assumes that the other has died, which is why they both become master
and don't rejoin.

You can prevent this behaviour by setting the `minimum_master_nodes`
parameter to a quorum (majority) of the nodes that you have, which in your
case would be 2.  This setting says "how many master eligible nodes must
each node be able to see in order to be sure that it is part of the bigger
half of the cluster"

A setup with two nodes is problematic, because a quorum is... two nodes.
 So if either of the goes down, the other node won't be able to see enough
masters and will stop functioning as well. If you had three nodes, a quorum
would still be 2, but you could afford to lose one node and the other two
would continue functioning normally

clint
</comment><comment author="dadoonet" created="2014-02-21T15:02:14Z" id="35737614">I think we could close this issue. Was a `split-brain` issue.

BTW, you could now try [elasticsearch-cloud-azure plugin](https://github.com/elasticsearch/elasticsearch-cloud-azure). It won't solve your split brain issue unless you follow clint's advices but it could help to manage discovery.

Feel free to reopen the issue if I was wrong closing it.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>jsr305 exclusion is no longer needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4830</link><project id="" key="" /><description>Starting from release 13.0, the Guava library made the dependency to the jsr305 library `provided` and starting from release 15.0 it has become `optional` (see https://code.google.com/p/guava-libraries/wiki/Release13#Non-API_changes)

As a result, the following exclusion is no longer needed in the pom.xml :

```
        &lt;dependency&gt;
            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;
            &lt;artifactId&gt;guava&lt;/artifactId&gt;
            &lt;version&gt;15.0&lt;/version&gt;
            &lt;scope&gt;compile&lt;/scope&gt;
            &lt;exclusions&gt;
                &lt;exclusion&gt;
                    &lt;groupId&gt;com.google.code.findbugs&lt;/groupId&gt;
                    &lt;artifactId&gt;jsr305&lt;/artifactId&gt;
                &lt;/exclusion&gt;
            &lt;/exclusions&gt;
        &lt;/dependency&gt;
```

You may take this opportunity to update Guava version to 16.0 as well.
</description><key id="26001376">4830</key><summary>jsr305 exclusion is no longer needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spydesk</reporter><labels /><created>2014-01-21T13:44:10Z</created><updated>2014-01-21T14:26:14Z</updated><resolved>2014-01-21T14:26:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>upgrade to guava 16.0</comment><comment>also fixes #4830</comment></comments></commit></commits></item><item><title>Query rescorers don't use distributed frequencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4829</link><project id="" key="" /><description>Query rescorers always use local term frequencies, even if the search type is `DFS_QUERY_(THEN|AND)_FETCH`.
</description><key id="25999733">4829</key><summary>Query rescorers don't use distributed frequencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label></labels><created>2014-01-21T13:16:02Z</created><updated>2016-08-24T15:17:57Z</updated><resolved>2016-08-24T15:17:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-08-24T15:17:57Z" id="242101899">Fixed by @jimferenczi via #19972
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Never cache a range filter that uses the `now` date expression.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4828</link><project id="" key="" /><description>Closes #4846
</description><key id="25999731">4828</key><summary>Never cache a range filter that uses the `now` date expression.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Query DSL</label><label>bug</label><label>v0.90.11</label><label>v1.0.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-21T13:16:00Z</created><updated>2015-06-07T16:10:40Z</updated><resolved>2014-01-29T10:00:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-21T13:16:50Z" id="32883746">LGTM
</comment><comment author="jpountz" created="2014-01-21T13:18:04Z" id="32883815">Nice!
</comment><comment author="jpountz" created="2014-01-21T13:19:37Z" id="32883917">I think there should also be an issue for this change? Even if it's simple, I think it's interesting for quite a number of users?
</comment><comment author="martijnvg" created="2014-01-29T10:00:04Z" id="33570503">Already pushed a while ago: https://github.com/elasticsearch/elasticsearch/issues/4846
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>id_cache stats returns negative values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4827</link><project id="" key="" /><description>after executing **_cache/clear** action _cluster/stats started returning negative values for id_cache

``` JSON
{
"id_cache": {
         "memory_size": "-1E10b",
         "memory_size_in_bytes": -10313508276
      }
}
```

also i started seeing warning below quite often in my logs:

```
[2014-01-21 11:19:28,706][WARN ][transport.netty          ] [&lt;node name&gt;] Message not fully read (response) for [9917139] handler org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4@37970778, error [false], resetting

```

context:
- es version 0.90.10
- java version: 1.7.0_51 (oracle)
- os: ubuntu 12.04
- 3 node cluster
- 12 shards per index
- 2 replicas
- around 100,000,000 parent/child related documents (below 1kb each)
</description><key id="25988887">4827</key><summary>id_cache stats returns negative values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC1</label></labels><created>2014-01-21T12:06:14Z</created><updated>2014-01-22T13:34:19Z</updated><resolved>2014-01-22T13:34:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-21T14:48:59Z" id="32890984">I think this happened in your case, because the normal cleanup mechanism (via merges) and the clear cache mechanism can accidentally remove simple id cache entries twice. This has been fixed in the master branch: https://github.com/elasticsearch/elasticsearch/commit/d5c440cd2e80ac7d19d494b0adfbbfb358afcc32 but has not been ported back yet. I think it make sense to port this back to the 0.90 branch.
</comment><comment author="s1monw" created="2014-01-21T19:48:54Z" id="32955823">+1 to port to `0.90`
</comment><comment author="martijnvg" created="2014-01-22T13:34:19Z" id="33021726">Pushed to 0.90: https://github.com/elasticsearch/elasticsearch/commit/24222540fa42386704edd77a0721c0eb3c1798f3
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Are there any facets that can be used to co-relate log events ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4826</link><project id="" key="" /><description>Hi

I am  looking for way to co-relate multiple log events and then calculate the time duration between those events?

e.g: Request log event &amp; response log event - to calculate the difference in timestamps to assess the performance of the application.
</description><key id="25977963">4826</key><summary>Are there any facets that can be used to co-relate log events ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adityapavan18</reporter><labels /><created>2014-01-21T10:34:01Z</created><updated>2014-01-21T16:14:12Z</updated><resolved>2014-01-21T16:14:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-21T16:14:12Z" id="32900210">Can you ask this kind of question on the googlegroup/mailinglist, as we try to use this for tracking issues and bugs.

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add _cat/versions endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4825</link><project id="" key="" /><description>If we want to have a full picture of versions running in a cluster, we need to add a `_cat/versions` endpoint.

Response could look like:

``` sh
% curl es2:9200/_cat/versions?v
node component                        version
es1  jvm                              1.6.0_65-b14
es1  elasticsearch                    0.90.10
es1  elasticsearch-mapper-attachment  1.7.0
es1  elasticsearch-lang-js            1.7.0
es1  elasticsearch-analysis-smartcn   1.9.0
es2  jvm                              1.6.0_65-b14
es2  elasticsearch                    1.0.0.RC1
es2  elasticsearch-mapper-attachment  2.0.0.RC1
es2  elasticsearch-lang-js            2.0.0.RC1
es2  elasticsearch-analysis-smartcn   2.0.0.RC1
```

Closes #4824.

**Note** that this PR requires first #4378 to be merged.
</description><key id="25974455">4825</key><summary>Add _cat/versions endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-01-21T09:30:20Z</created><updated>2014-06-12T22:14:22Z</updated><resolved>2014-03-17T08:03:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-01-21T15:02:38Z" id="32892414">Looks ok to me!
</comment><comment author="dadoonet" created="2014-01-30T15:21:35Z" id="33697284">I did rebase this PR on master which now have the plugin version in.
The end point is now `_cat/versions`.

I'm still adding information about jvm and elasticsearch here but they are not really plugins...

Wondering if it makes sense?
</comment><comment author="uboness" created="2014-01-30T15:39:35Z" id="33699181">I would still want to get the jvm &amp; es versions in the `_cat/nodes` endpoint, it's something you care quite often about when looking at nodes... and resolving problems and switching between "views" doesn't help here (you'll to correlated between two different outputs)
</comment><comment author="s1monw" created="2014-02-06T10:07:00Z" id="34308357">Looks cool can you also add a REST test for this feature?
</comment><comment author="dadoonet" created="2014-03-17T08:03:30Z" id="37792054">Closed with f54e9246c15ea5bd1302451af4738f96160c38d8
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add _cat/plugins endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4824</link><project id="" key="" /><description>If we want to have a full picture of plugins running in a cluster, we need to add a `_cat/plugins` endpoint.

Response could look like:

``` sh
% curl es2:9200/_cat/plugins?v
node component                        version   type url                                   desc
es1  mapper-attachments               1.7.0       j                                        Adds the attachment type allowing to parse difference attachment formats
es1  lang-javascript                  1.4.0       j                                        JavaScript plugin allowing to add javascript scripting support
es1  analysis-smartcn                 1.9.0       j                                        Smart Chinese analysis support
es1  marvel                           1.1.0      j/s http://localhost:9200/_plugins/marvel Elasticsearch Management &amp; Monitoring
es1  kopf                             0.5.3       s  http://localhost:9200/_plugins/kopf   kopf - simple web administration tool for ElasticSearch
es2  mapper-attachments               2.0.0.RC1   j                                        Adds the attachment type allowing to parse difference attachment formats
es2  lang-javascript                  2.0.0.RC1   j                                        JavaScript plugin allowing to add javascript scripting support
es2  analysis-smartcn                 2.0.0.RC1   j                                        Smart Chinese analysis support
```
</description><key id="25974253">4824</key><summary>Add _cat/plugins endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:CAT API</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-21T09:26:51Z</created><updated>2015-06-06T18:43:52Z</updated><resolved>2014-03-16T11:18:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-21T12:13:50Z" id="32867382">personally I'd much rather have this as part of the `_cat/nodes` api... a la:

```
% curl es2:9200/_cat/nodes?v&amp;h=jvmVersion,esVersion,pluginVersion
name        ip              jvmVersion      esVersion   pluginVersion
Kurt Wagner 10.20.100.185   1.6.0_65-b14    1.0.0.RC1   elasticsearch-mapper-attachment:0.90.1
```

the `pluginVersion` column holds a comma separated list of plugins and their versions
</comment><comment author="karmi" created="2014-01-21T14:02:41Z" id="32887108">@uboness But what if you have many plugins on the node? That would be a very long line... The `component | version` table would work better in this case I think...
</comment><comment author="uboness" created="2014-01-21T14:13:17Z" id="32887899">The more I think about it the more I realize that it would make sense to separate the plugin info and the node info... Also discussed it with Boaz and the general feeling is that the es/jvm versions belong to the nodes API, and for the plugins we can have a dedicated API... (Which can potential return the mete data info as well.. Eg description)

On Tue, Jan 21, 2014 at 3:02 PM, Karel Minarik notifications@github.com
wrote:

&gt; ## @uboness But what if you have many plugins on the node? That would be a very long line... The `component | version` table would work better in this case I think...
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/4824#issuecomment-32887108
</comment><comment author="drewr" created="2014-01-21T14:28:30Z" id="32889211">:+1: for `_cat/versions` since it will require multiple lines per node...
</comment><comment author="uboness" created="2014-01-21T14:51:29Z" id="32891228">I'm more for `_cat/plugins`
</comment><comment author="dadoonet" created="2014-01-21T14:57:31Z" id="32891783">I think it could be nice to have a full overview of all versions (nodes, jvm and plugins).
Naming it `plugins` could appear as restrictive. But to be honest, both are fine to me.

If we restrict on plugins, I can add more columns, such as:
- `site` (boolean): is a site plugin?
- `jvm` (boolean): is a JVM plugin?
- `url` (String): if site, endpoint

WDYT?
</comment><comment author="uboness" created="2014-01-21T15:02:13Z" id="32892310">I don't think, from a user perspective, that the user wants to know information about version.. .they want to know information about nodes (and jvm/es versions are part of that info)... if they want to know versions of plugins.. .it comes from the perspective of knowing information about plugins... hence the dedicated `_cat/plugins` api... this IMO feels more natural and fits nicely with the rest of the `_cat` APIs

and yeah... having a `_cat/plugins` endpoint enables you to provide the different metadata over the plugins, like site, desc, url, etc...
</comment><comment author="dadoonet" created="2014-03-14T17:50:44Z" id="37676420">@drewr Could you have a look please at my PR. I did rebase on master, added some new columns and update doc.

@javanna How can I test it as I don't know how I can add a plugin to the REST test runner. May be we can not really test it automatically? 
</comment><comment author="drewr" created="2014-03-14T21:04:33Z" id="37695500">Looks good!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestPluginsAction.java</file></files><comments><comment>Add _cat/plugins endpoint</comment></comments></commit></commits></item><item><title>Allow to configure indices.fielddata.breaker.limit with a ratio of the heap size.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4823</link><project id="" key="" /><description>Close #4616
</description><key id="25973685">4823</key><summary>Allow to configure indices.fielddata.breaker.limit with a ratio of the heap size.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-21T09:14:39Z</created><updated>2014-07-03T04:23:57Z</updated><resolved>2014-01-21T13:40:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-21T09:31:24Z" id="32832742">LGTM - left one small comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>REST API: Consistent get field mapping response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4822</link><project id="" key="" /><description>If a get field mapping request is issued, and all but the field can be
found, the response should return an empty JSON object instead of a 404.

Closes #4738
</description><key id="25971203">4822</key><summary>REST API: Consistent get field mapping response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:REST</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-21T08:16:48Z</created><updated>2015-06-07T16:17:50Z</updated><resolved>2014-01-28T07:39:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-27T21:20:50Z" id="33424161">left one small comment! really good solution - thanks for the extra iteration!

LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Node can't join cluster after another node restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4821</link><project id="" key="" /><description>We are running ES 0.90.10 on 4 node cluster. One our node was restarted (killed by OOM killer), another node lost its connection to cluster and stuck in that state until we restart it.

Here is log from node that lost its membership in cluster (10.31.10.154):

```
[2014-01-20 20:40:18,550][WARN ][transport.netty          ] [main] exception caught on transport layer [[id: 0xd9dc9013, /10.31.10.15
4:46234 :&gt; /10.31.10.150:9201]], closing connection
java.io.IOException: Соединение сброшено другой стороной
  at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
  at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
  at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:218)
  at sun.nio.ch.IOUtil.read(IOUtil.java:186)
  at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
  at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
  at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
  at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
  at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
  at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
  at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
  at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
  at java.lang.Thread.run(Thread.java:722)
[2014-01-20 20:40:18,554][WARN ][discovery.zen            ] [main] not enough master nodes, current nodes: {[main][fnZZN_rgTEyczC7H_hX6sA][inet[/10.31.10.154:9201]]{max_local_storage_nodes=1},[97c87465-6ec2-429a-8c17-b06ea8739748][Ngb-aDzPQ7KTaivrb10OKA][inet[/10.31.10.151:9201]]{max_local_storage_nodes=1},}
[2014-01-20 20:40:18,554][INFO ][cluster.service          ] [main] removed {[daeb5096-5cd1-4c9c-80c2-354031206176][b-LgzNuiR7uc1gGAjUS_YA][inet[/10.31.10.150:9201]]{max_local_storage_nodes=1},[97c87465-6ec2-429a-8c17-b06ea8739748][Ngb-aDzPQ7KTaivrb10OKA][inet[/10.31.10.151:9201]]{max_local_storage_nodes=1},}, reason: zen-disco-node_failed([daeb5096-5cd1-4c9c-80c2-354031206176][b-LgzNuiR7uc1gGAjUS_YA][inet[/10.31.10.150:9201]]{max_local_storage_nodes=1}), reason transport disconnected (with verified connect)
[2014-01-20 20:40:22,641][INFO ][discovery.zen            ] [main] master_left [[2fe7ca69-61af-49b8-8390-6c565ab185e2][3ZdVtdrZQlScQKoiC7Dj2g][inet[/10.31.10.152:9201]]{max_local_storage_nodes=1}], reason [do not exists on master, act as master failure]
[2014-01-21 11:06:32,462][INFO ][node                     ] [main] stopping ...
```

(IOException message in stack trace is 'Connection reset by peer')

We manually restarted that node at 2014-01-21 11:06, and after restart it successfully joined the cluster. Before starts node returned only MasterNotDiscoveredException.

ES configuration:

```
discovery.zen.minimum_master_nodes: 3
cluster.name: jet
node.name: main
node.max_local_storage_nodes: 1
index.number_of_shards: 6
index.number_of_replicas: 0
network.host: 0.0.0.0
discovery.zen.ping.unicast.hosts: 
  - 10.31.10.154:9201
  - 10.31.10.150:9201
  - 10.31.10.151:9201
  - 10.31.10.152:9201
discovery.zen.ping.multicast.enabled: false
http.max_content_length: 500mb
http.port: 9200
transport.tcp.port: 9201
gateway.local.auto_import_dangled: no
```
</description><key id="25970556">4821</key><summary>Node can't join cluster after another node restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maxcom</reporter><labels /><created>2014-01-21T08:00:34Z</created><updated>2014-12-24T19:50:26Z</updated><resolved>2014-12-24T18:46:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-21T08:31:51Z" id="32829310">Can you unset your locale and test again? I see some nasty checks for exception message content in that code region, which is locale dependent, as it checks for english messages... not sure if this is the root cause though
</comment><comment author="maxcom" created="2014-01-21T10:34:09Z" id="32836866">Thanx. I'm not sure that I can reproduce this problem (I've seen it only once).

Is it safe to change locale on production cluster? Does current locale settings affect on analysis, index format or sorting?
</comment><comment author="spinscale" created="2014-01-21T15:43:58Z" id="32896855">Hey,

elasticsearch tries to be locale independent. I know that the locale might be used (depending on your configuration) in Hunspell and ICU. In addition the date field mapping can have a locale configured, but that is independent from your configured one.
</comment><comment author="maxcom" created="2014-01-24T10:04:36Z" id="33210707">We see the same problem with US locale. Node left cluster due to network error. It stays isolated from cluster for a few hours until we restart it. We use unicast discovery, all hosts are listed in configuration file.
</comment><comment author="rolyv" created="2014-02-19T15:51:16Z" id="35512854">I think we're having the same problem on a cluster with 3 nodes. We see sporadic network interruptions that last multiple minutes at times, and then end up having to restart node(s). We're also using unicast discovery. 
</comment><comment author="clintongormley" created="2014-12-24T18:46:19Z" id="68069089">Hi @maxcom 

Sorry it has been a while since looking at this issue, which now refers to an old version.  This code has received a huge amount of attention in recent release, so I'm not sure it is worth chasing this old bug down.  Please open another ticket if you see anything similar in version 1.4 or above.

thanks
</comment><comment author="maxcom" created="2014-12-24T19:50:26Z" id="68071659">We updated to 1.3.x and this problem did not happen. Now we use 1.4.x and it works fine too. Thanx.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Feature Request: Option for Index Cmd to return existing source on failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4820</link><project id="" key="" /><description>Can we possible add a feature to the Index command that gives the option of returning the source of the existing item on failure to update/insert. The failure would probably be caused by a version conflict. I have scenarios were multiple (external) nodes need to update the same key with different values and possibly the same value. If I get the source back it would remove the need for me to have to query for it on failure to figure out what state I am in.
</description><key id="25935551">4820</key><summary>Feature Request: Option for Index Cmd to return existing source on failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jm4games</reporter><labels /><created>2014-01-20T17:56:02Z</created><updated>2014-12-24T18:42:45Z</updated><resolved>2014-12-24T18:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T18:42:44Z" id="68068939">Closing as duplicate of #4420
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[SPEC,TEST,FIX] add spec and tests for termvector api and fix inconsiste...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4819</link><project id="" key="" /><description>...ncies
- index and type were not read from the uri with the _mtermvectors api
- ids were not read from the uri parameters with the _mtermvectors api
</description><key id="25933719">4819</key><summary>[SPEC,TEST,FIX] add spec and tests for termvector api and fix inconsiste...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-01-20T17:25:27Z</created><updated>2014-07-16T21:49:24Z</updated><resolved>2014-01-21T09:03:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-21T08:54:19Z" id="32830503">LGTM +1 to push
</comment><comment author="brwe" created="2014-01-21T09:03:12Z" id="32830968">pushed to master (cae5eb4)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecated ToXContent.Params#paramAsBooleanOptional in favour of paramAsBoolean</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4818</link><project id="" key="" /><description>Deprecated ToXContent.Params#paramAsBooleanOptional in favour of paramAsBoolean

Closes #4817
</description><key id="25929242">4818</key><summary>Deprecated ToXContent.Params#paramAsBooleanOptional in favour of paramAsBoolean</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2014-01-20T16:15:49Z</created><updated>2014-06-22T16:32:38Z</updated><resolved>2014-01-21T10:06:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-21T09:48:31Z" id="32833844">can you point out when this was deprecated ie. which version so it's easier to track? otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unify RestRequest paramAsBoolean and paramAsBooleanOptional</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4817</link><project id="" key="" /><description>This is a follow-up of #4808: since `paramAsBoolean` and `paramAsBooleanOptional` do the same, with the only difference being their return type (`boolean` vs `Boolean`), their names should be the same. The fact that their names currently differ can be misleading.

The proposal is to rename `paramAsBooleanOptional` to `paramAsBoolean`. The original method will be kept around and deprecated though to keep backwards compatibility.
</description><key id="25928835">4817</key><summary>Unify RestRequest paramAsBoolean and paramAsBooleanOptional</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v1.0.0</label></labels><created>2014-01-20T16:09:53Z</created><updated>2014-01-21T10:06:56Z</updated><resolved>2014-01-21T10:06:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/common/xcontent/ToXContent.java</file><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/explain/RestExplainAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestHeadAction.java</file><file>src/main/java/org/elasticsearch/rest/action/get/RestMultiGetAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file></files><comments><comment>Deprecated ToXContent.Params#paramAsBooleanOptional in favour of paramAsBoolean</comment></comments></commit></commits></item><item><title>Deprecated use of partial fields in Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4816</link><project id="" key="" /><description>Deprecated use of partial fields in Java API, was already deprecated in the docs for the search API

Closes #4118
</description><key id="25925434">4816</key><summary>Deprecated use of partial fields in Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2014-01-20T15:21:12Z</created><updated>2014-06-16T08:07:37Z</updated><resolved>2014-01-21T09:37:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add page tracking to MockPageCacheRecycler.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4815</link><project id="" key="" /><description>This found an issue in BytesRefHash that forgot to release the start offsets.

Close #4814
</description><key id="25924061">4815</key><summary>Add page tracking to MockPageCacheRecycler.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-20T14:59:51Z</created><updated>2014-07-01T17:21:14Z</updated><resolved>2014-01-21T13:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T15:22:58Z" id="32767997">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add tracking of pages to MockPageCacheRecycler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4814</link><project id="" key="" /><description>Page tracking would help make sure that we never forget to release pages when we don't need them anymore.
</description><key id="25923804">4814</key><summary>Add tracking of pages to MockPageCacheRecycler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>test</label><label>v1.0.0</label></labels><created>2014-01-20T14:55:47Z</created><updated>2014-01-20T15:29:58Z</updated><resolved>2014-01-20T15:29:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BytesRefHash.java</file><file>src/test/java/org/elasticsearch/cache/recycler/MockPageCacheRecycler.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/BytesRefHashTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file></files><comments><comment>Add page tracking to MockPageCacheRecycler.</comment></comments></commit></commits></item><item><title>Norms disabling on existing fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4813</link><project id="" key="" /><description>We should allow for disabling norms on existing fields via the update mappings API. Implementation-wise, we would only have to set `omitNorms` to false in the `FieldType` and Lucene would automatically ignore norms on the next fields that would be added to the index and remove data from the index upon merges.

However, the reverse operation cannot be supported, so disabling norms would be a destructive operation.
</description><key id="25919720">4813</key><summary>Norms disabling on existing fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-20T13:46:03Z</created><updated>2015-06-08T15:22:16Z</updated><resolved>2014-03-25T13:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-21T15:13:13Z" id="32893558">How would scoring work if some of the documents have norms and some do not?  Would it just ignore them on all documents because they are false on the `FieldType`?
</comment><comment author="jpountz" created="2014-01-21T15:47:38Z" id="32897262">By default, here is what Lucene would do: Let's assume that we already have 2 segments A and B that have norms. We are now writing segment C and the first `n` documents have been added with norms enabled while the last `maxDoc-n` documents have been added with norms disabled because of a mapping update.

On the next refresh, segment C will be written, and no document of segment C will have norms (even though the first documents were added with norms -- disabling norms is a destructuive operation). If you run a query on this index, norms will be taken into account on A and B, and norms will be assumed to be all equal to 1 on C since it doesn't have norms.

Then, as background merges happen, A and B are going to be merged with segments that don't have norms and the resulting segment won't have norms either (even for documents that come from A or B).

This is it for the default behavior. Alternatively, if we want to, something we could do as well would be to wrap the IndexReader to make all segments pretend that they don't have norms as soon as norms get disabled via a mapping update.
</comment><comment author="nik9000" created="2014-01-21T16:06:23Z" id="32899332">&gt; Then, as background merges happen, A and B are going to be merged with segments that don't have norms and the resulting segment won't have norms either (even for documents that come from A or B).

Would that make the results from segment C score more highly then segment A and B until the merge unless you did something like wrap the IndexReader?  If you aren't using index time boosts a value of 1 represents a single term, right?
</comment><comment author="jpountz" created="2014-01-21T16:44:07Z" id="32903444">Yes, very likely, segment C would score higher.

I have to admit I hadn't thought too much about scoring, I mostly thought about users who would realize they were using a particular field solely for matching (without scoring), sorting or aggregations and would like to stop paying the price for norms. So maybe it makes sense to do some wrapping to avoid surprises with scores.
</comment><comment author="nik9000" created="2014-01-21T16:48:30Z" id="32903863">You could get away with just documenting it very well I suppose.
</comment><comment author="jpountz" created="2014-01-21T16:49:45Z" id="32904000">Right. I need to think more about the consequences of each option... :-)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file></files><comments><comment>Allow to disable norms on an existing field.</comment></comments></commit></commits></item><item><title>Fix logging on immediate exit on start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4812</link><project id="" key="" /><description>If elasticsearch was started in the foreground an immediate exit on startup
led to logging in the logfile, where as when starting in the background,
an immediate exit logged to stdout.

Closes #4805
</description><key id="25918336">4812</key><summary>Fix logging on immediate exit on start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-01-20T13:18:49Z</created><updated>2014-06-16T20:28:48Z</updated><resolved>2014-01-20T14:33:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T13:20:16Z" id="32758546">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Return a conflict when trying to enable/disable norms.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4811</link><project id="" key="" /><description>Close #4761
</description><key id="25916661">4811</key><summary>Return a conflict when trying to enable/disable norms.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-20T12:45:57Z</created><updated>2014-07-16T21:49:27Z</updated><resolved>2014-01-21T13:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T13:50:51Z" id="32760493">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make StringFieldMapper.toXContent aware of defaults for not_analyzed fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4810</link><project id="" key="" /><description>StringFieldMapper.toXContent uses the defaults for analyzed fields in order to
know which options to add to the builder. This means that if the field is not
analyzed and has norms enabled, it will omit to emit `norms.enabled: true`.
Parsing the mapping again will result in a StringFieldMapper that has norms
disabled.

The same fix applies to index options.

Close #4760
</description><key id="25915538">4810</key><summary>Make StringFieldMapper.toXContent aware of defaults for not_analyzed fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-20T12:21:34Z</created><updated>2014-06-12T19:23:04Z</updated><resolved>2014-01-21T13:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T13:49:12Z" id="32760376">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Accept the same boolean values in RestRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4809</link><project id="" key="" /><description>Added `no` as a possible value to `paramBooleanAsOptional`, also reused existing code from `Booleans.parseBoolean`

Closes #4808
</description><key id="25915077">4809</key><summary>Accept the same boolean values in RestRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2014-01-20T12:12:51Z</created><updated>2014-06-16T03:39:40Z</updated><resolved>2014-01-20T13:28:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-01-20T12:15:00Z" id="32754737">I would also consider renaming one of the two methods (in a separate issue): why `paramAsBoolean` and `paramAsBooleanOptional`? They do the same, the only difference is the return type: primitive `boolean` or `Boolean`.
</comment><comment author="s1monw" created="2014-01-20T13:24:25Z" id="32758784">LGTM +1 to the suggestion of unifying!
</comment><comment author="javanna" created="2014-01-20T13:27:00Z" id="32758946">Cool will push this and open another issue as well to discuss renaming one of the two methods.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>RestRequest boolean methods should always accept the same values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4808</link><project id="" key="" /><description>We currently have two methods that accept boolean values in `RestRequest`: one for the primitive type (`paramAsBoolean`) and one for the `Boolean` object (`paramAsBooleanOptional`).

They work pretty much the same way with a small difference: the latter doesn't accept `no` as a boolean value while the the first one does. I think both methods should accept exactly the same values consistently.
</description><key id="25914976">4808</key><summary>RestRequest boolean methods should always accept the same values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-20T12:10:29Z</created><updated>2014-01-20T16:09:53Z</updated><resolved>2014-01-20T13:28:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-20T12:11:16Z" id="32754527">++
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/ToXContent.java</file></files><comments><comment>Follow-up of #4808, same fix applied to ToXContent.MapParams</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file></files><comments><comment>Accept the same boolean values in RestRequest</comment></comments></commit></commits></item><item><title>Return `MatchNoDocsQuery` if query string is emtpy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4807</link><project id="" key="" /><description>Closes #3952
</description><key id="25914331">4807</key><summary>Return `MatchNoDocsQuery` if query string is emtpy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-20T11:56:14Z</created><updated>2014-06-27T05:26:43Z</updated><resolved>2014-01-20T15:09:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-20T14:20:14Z" id="32762643">This makes sense to me. If you search for nothing you get nothing. But not the same as a search which doesn't specify a query, which defaults to `match_all`, so:

```
GET /_search               # all docs
GET /_search?q=            # no docs
GET /_search?q=*           # all docs
```

++
</comment><comment author="s1monw" created="2014-01-20T15:09:51Z" id="32766887">pushed to `master` and `0.90`
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster pending tasks always uses default master_timeout parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4806</link><project id="" key="" /><description>The cluster pending tasks doesn't read the input `master_timeout` parameter from the REST layer, although it allows to set the `masterNodeTimeout` through Java API.

As a result, when using the REST layer the default `master_timeout` (30 seconds) is always used.
</description><key id="25913996">4806</key><summary>Cluster pending tasks always uses default master_timeout parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v1.0.0</label></labels><created>2014-01-20T11:49:49Z</created><updated>2014-01-20T11:54:13Z</updated><resolved>2014-01-20T11:54:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/tasks/RestPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestPendingClusterTasksAction.java</file></files><comments><comment>master_timeout parameter is now taken into account when calling cluster pending tasks api through the REST layer</comment></comments></commit></commits></item><item><title>BindException not occuring in elasticserch.log when started as a service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4805</link><project id="" key="" /><description>Steps to reproduce:
1. Fix the transport.port and http.port to fixed values.
2. Start another process that is using port 9300
3. Register ElasticSearch service
4. Try and start ElasticSearch using services

Observed behavior:
There should be a bindException in elasticsearch.log file.

Actual behavior:
There is no error in elasticsearch.log file, but elasticsearch process does not start

More notes:
In Bootstrap class, this seems incorrect check.
            if (foreground) {
                logger.error(errorMessage);
            } else {
                System.err.println(errorMessage);
                System.err.flush();
            }
</description><key id="25911816">4805</key><summary>BindException not occuring in elasticserch.log when started as a service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">sameerpokarna</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-20T11:04:00Z</created><updated>2014-01-22T14:22:10Z</updated><resolved>2014-01-20T14:33:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-20T11:23:51Z" id="32751814">Hey,

just to make sure I get this right: What do you mean with register elasticsearch service? Where and how do you reigster it? Which elasticsearch version are you using and which operating system?

Seems the if-else order should be reversed, but want to make sure I get all information first.
</comment><comment author="sameerpokarna" created="2014-01-20T16:54:10Z" id="32776669">Hi Alex,

That is what I thought too that the if-else should be reversed.
When I said register, I mean install the service. Install is currently
supported (0.90.9) only on Windows OOTB, and I was trying on a Windows 7
laptop (64-bit).

Regards,
Sameer

On Mon, Jan 20, 2014 at 4:54 PM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; Hey,
&gt; 
&gt; just to make sure I get this right: What do you mean with register
&gt; elasticsearch service? Where and how do you reigster it? Which
&gt; elasticsearch version are you using and which operating system?
&gt; 
&gt; Seems the if-else order should be reversed, but want to make sure I get
&gt; all information first.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4805#issuecomment-32751814
&gt; .
</comment><comment author="spinscale" created="2014-01-21T08:24:13Z" id="32828902">Hey,

can you see, if the above patch fixes the issue for you? Would be great! You can either use the master or the 0.90 branch to test.

Thanks a lot!
</comment><comment author="sameerpokarna" created="2014-01-21T09:44:35Z" id="32833590">Sure, I have yet to use the ES build process to deploy, but I can test on
0.90 branch.

Thanks and regards,
Sameer

On Tue, Jan 21, 2014 at 1:54 PM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; Hey,
&gt; 
&gt; can you see, if the above patch fixes the issue for you? Would be great!
&gt; You can either use the master or the 0.90 branch to test.
&gt; 
&gt; Thanks a lot!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4805#issuecomment-32828902
&gt; .
</comment><comment author="sameerpokarna" created="2014-01-21T11:54:05Z" id="32864153">I tried on 0.90 branch, and it is now updating the error in
elasticsearch.log file, but service console does not give me an error
during startup. I have to refresh the list to see that the ES process did
not start. I used to see the error earlier that the ES process did not
start.

Regards,
Sameer

On Tue, Jan 21, 2014 at 1:54 PM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; Hey,
&gt; 
&gt; can you see, if the above patch fixes the issue for you? Would be great!
&gt; You can either use the master or the 0.90 branch to test.
&gt; 
&gt; Thanks a lot!
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4805#issuecomment-32828902
&gt; .
</comment><comment author="costin" created="2014-01-21T16:53:49Z" id="32904453">Hi Sameer,

I've tried reproducing the issue and discovered the following -  In Windows, under Administrative Tasks &gt; Services:
1.starting a service gives an error only if ES is configured incorrectly - namely if the service wrapper x86 is configured to use a JVM on 64 bits.
2. in all other cases, when the service wrapper is properly configured (namely elasticsearch-service-x64 is using a 64-bits JVM and service-32 a 32-bits JVM) there is no error message if the service fails. That is because the service invocation starts and the failure occurs in a background thread.

I've tried this on 0.90.5 to 0.90.10 and in all cases, both 1 and 2 apply.
Tried on Windows 7 64 bit SP1.
</comment><comment author="sameerpokarna" created="2014-01-22T03:23:35Z" id="32990302">Yes, that does make sense that there is no error because the error is in
another thread. Maybe, a status check wrapper is required which will be
used instead of starting ES directly, so that the user gets feedback of
when the service fails to start up.

Thanks and regards,
Sameer

On Tue, Jan 21, 2014 at 10:24 PM, Costin Leau notifications@github.comwrote:

&gt; Hi Sameer,
&gt; 
&gt; I've tried reproducing the issue and discovered the following - In
&gt; Windows, under Administrative Tasks &gt; Services:
&gt; 1.starting a service gives an error only if ES is configured incorrectly -
&gt; namely if the service wrapper x86 is configured to use a JVM on 64 bits.
&gt; 2. in all other cases, when the service wrapper is properly configured
&gt; (namely elasticsearch-service-x64 is using a 64-bits JVM and service-32 a
&gt; 32-bits JVM) there is no error message if the service fails. That is
&gt; because the service invocation starts and the failure occurs in a
&gt; background thread.
&gt; 
&gt; I've tried this on 0.90.5 to 0.90.10 and in all cases, both 1 and 2 apply.
&gt; Tried on Windows 7 64 bit SP1.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4805#issuecomment-32904453
&gt; .
</comment><comment author="costin" created="2014-01-22T14:22:10Z" id="33026573">I agree that would be nice but I must say I'm unsure where there the problem lies.
If you start the service from the command-line using `service.bat start`, you'll
get the failure message right away.
However, the same service using the same infrastructure behaves differently -
it seems to be a difference in behavior of commons-daemon rather then ES itself.

I'll dig deeper to figure out what the issue but so far nothing jumped out.
In the meantime I recommend using service.bat start|stop.

Hope this helps...

On 22/01/2014 5:24 AM, sameerpokarna wrote:

&gt; Yes, that does make sense that there is no error because the error is in
&gt; another thread. Maybe, a status check wrapper is required which will be
&gt; used instead of starting ES directly, so that the user gets feedback of
&gt; when the service fails to start up.
&gt; 
&gt; Thanks and regards,
&gt; Sameer
&gt; 
&gt; On Tue, Jan 21, 2014 at 10:24 PM, Costin Leau notifications@github.comwrote:
&gt; 
&gt; &gt; Hi Sameer,
&gt; &gt; 
&gt; &gt; I've tried reproducing the issue and discovered the following - In
&gt; &gt; Windows, under Administrative Tasks &gt; Services:
&gt; &gt; 1.starting a service gives an error only if ES is configured incorrectly -
&gt; &gt; namely if the service wrapper x86 is configured to use a JVM on 64 bits.
&gt; &gt; 2. in all other cases, when the service wrapper is properly configured
&gt; &gt; (namely elasticsearch-service-x64 is using a 64-bits JVM and service-32 a
&gt; &gt; 32-bits JVM) there is no error message if the service fails. That is
&gt; &gt; because the service invocation starts and the failure occurs in a
&gt; &gt; background thread.
&gt; &gt; 
&gt; &gt; I've tried this on 0.90.5 to 0.90.10 and in all cases, both 1 and 2 apply.
&gt; &gt; Tried on Windows 7 64 bit SP1.
&gt; &gt; 
&gt; &gt; —
&gt; &gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4805#issuecomment-32904453
&gt; &gt; .
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/4805#issuecomment-32990302.

## 

Costin
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/bootstrap/Bootstrap.java</file></files><comments><comment>Fix logging on immediate exit on start</comment></comments></commit></commits></item><item><title>Documentation typo.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4804</link><project id="" key="" /><description /><key id="25909511">4804</key><summary>Documentation typo.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels /><created>2014-01-20T10:19:19Z</created><updated>2014-06-20T03:55:42Z</updated><resolved>2014-01-20T10:51:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T10:51:24Z" id="32749852">pushed thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>avoid IndexOutOfBoundsException on all field with no tokens and keywordanalyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4803</link><project id="" key="" /><description>should fix #4771 
</description><key id="25907406">4803</key><summary>avoid IndexOutOfBoundsException on all field with no tokens and keywordanalyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mfelsche</reporter><labels /><created>2014-01-20T09:36:38Z</created><updated>2014-06-14T21:48:30Z</updated><resolved>2014-01-20T10:47:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T09:50:18Z" id="32745995">Hey, thanks for the PR this looks pretty cool tough. Can you maybe add another test that tries to simply index an empty document through our API it can be as simple as this:

``` Java
  public void testSearchEmptyDoc() {
        prepareCreate("test").setSettings("{\"index.analysis.analyzer.default.type\":\"keyword\"}").get();
        client().prepareIndex("test", "type1", "1").setSource("{}").get();
        refresh();
        assertHitCount(client().prepareSearch().setQuery(matchAllQuery()).get(), 1l);
    }

```

I guess `org.elasticsearch.search.query.SimpleQueryTests` is a good place for it or any other test that inherits `ElasticsearchIntegrationTest`
</comment><comment author="jpountz" created="2014-01-20T09:55:58Z" id="32746370">Thanks for opening a PR @mfelsche! Can you please sign the CLA http://www.elasticsearch.org/contributor-agreement/ so that we can get it in?
</comment><comment author="s1monw" created="2014-01-20T09:57:10Z" id="32746445">LGTM thanks for the quick iteration! I guess we can squash and push once the CLA is in
</comment><comment author="jpountz" created="2014-01-20T10:01:17Z" id="32746729">This PR looks good to me too!
</comment><comment author="mfelsche" created="2014-01-20T10:04:47Z" id="32746981">squashed, CLA signed.

thanks for your quick feedback!
</comment><comment author="s1monw" created="2014-01-20T10:11:40Z" id="32747422">May I ask you for one more thing, We try to keep commit messages descriptive and like this"

```
Description 80 chars

some more information for people interested in details...

Closes #issuesid
```

this makes it easier to associate the commit with an issue, would you mind changing that?
</comment><comment author="s1monw" created="2014-01-20T10:47:23Z" id="32749593">pushed to `0.90` via https://github.com/elasticsearch/elasticsearch/commit/bd91a67479eca5293a2034196c2ca9b0b1ce5e1a
and `master` via https://github.com/elasticsearch/elasticsearch/commit/c42f7708be601cf195661d7e0c13570e0223fee8

thanks for fixing this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add Recovery API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4802</link><project id="" key="" /><description>Adds a new API endpoint at /_recovery which can be used to report on the
status of recovering index shards and replicas. All types of recoveries
will be reported: gateway, peer, and snapshot. Pertinent details such as
percent recovered, bytes recovered, and recovery type will be reported
along with recovery status.

Closes #4637
</description><key id="25892083">4802</key><summary>Add Recovery API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>:Index APIs</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-19T23:46:55Z</created><updated>2015-06-06T18:45:27Z</updated><resolved>2014-03-07T00:14:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-20T09:00:19Z" id="32743064">this is a great start!. Since we want to remove the status API completely, I think we should have a transport level implementation of a dedicated recovery API as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>HistogramAggregator: Finer-grained rounding.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4801</link><project id="" key="" /><description>The way `HistogramAggregator` works is that for every value, it is going to
compute a rounded value, that basically looks like
`(value / interval) * interval` and use it as a key in a hash table to
aggregate counts.

However, the exact rounded value is not needed yet at that stage, all we need
is a value that uniquely identifies the bucket, such as `(value / interval)`.
We could only multiply with `interval` again when building the bucket: this way
the second step is only performed once per bucket instead of once per value.

Although this looks like a micro optimization for the case that was just
decribed, it makes more sense with the date rounding implementations that we
have that are more CPU-intensive.

Close #4800
</description><key id="25891641">4801</key><summary>HistogramAggregator: Finer-grained rounding.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-19T23:24:20Z</created><updated>2014-07-13T01:03:50Z</updated><resolved>2014-01-21T16:50:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Histogram aggregations: finer-grained rounding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4800</link><project id="" key="" /><description>The way `HistogramAggregator` works is that for every value, it is going to
compute a rounded value, that basically looks like `(value / interval) * interval` (using integer arithmetic) and use it as a key in a hash table to aggregate counts.

However, the exact rounded value is not needed yet at that stage, all we need
is a value that uniquely identifies the bucket, such as `(value / interval)`.
We could only multiply with `interval` again when building the bucket: this way
the second step is only performed once per bucket instead of once per value.

Although this looks like a micro optimization for the case that was just
decribed, it makes more sense with the date rounding implementations that we
have that are more CPU-intensive.
</description><key id="25891404">4800</key><summary>Histogram aggregations: finer-grained rounding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.1.0</label></labels><created>2014-01-19T23:13:58Z</created><updated>2014-01-21T16:48:39Z</updated><resolved>2014-01-21T16:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file></files><comments><comment>HistogramAggregator: Finer-grained rounding.</comment></comments></commit></commits></item><item><title>[DOCS] Various small documentation fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4799</link><project id="" key="" /><description>This pull request is both a code change and a call for help. :) 

I have been stuck in Perforce hell for a few years now and my git skills have been suffering (my private repos are in BitBucket and I only contribute to them, no forks). I rebased my local master branch to avoid having a merge commit, but I might have made things worse. My intent was to squash my small commits into one and send a pull request on that one commit. Can someone provide any pointers to send a better pull request for my last commits? https://github.com/brusic/elasticsearch/commits/docs Please contact me privately (or here if you don't mind the noise).
</description><key id="25889587">4799</key><summary>[DOCS] Various small documentation fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">brusic</reporter><labels /><created>2014-01-19T21:56:19Z</created><updated>2014-07-16T21:49:30Z</updated><resolved>2014-01-21T16:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2014-01-21T16:57:22Z" id="32904825">Someone my rebase went awry. Will close this PR and submit an updated one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] failed to fetch index version after copying it over</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4798</link><project id="" key="" /><description>I started getting this warning/error after full cluster restart:

```
[2014-01-19 19:15:15,239][WARN ][cluster.action.shard     ] [&lt;node name&gt;] [&lt;index name&gt;][4] sending failed shard for [&lt;index name&gt;][4], node[FgQg7A4HRdSuCKK-EHjNdQ], [P], s[INITIALIZING], indexUUID [gRKAbB7AQYGhgZRfi6pgzQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[&lt;index name&gt;][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[&lt;index name&gt;][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_7cn.fdx, _agr_es090_0.tim, _agr.nvm, _b4g.nvd, _b4g.fdt, _7cn.nvd, _7cn_es090_0.pos, _b4g.fdx, _agr.nvd, _b4g_es090_0.pay, _b4g_es090_0.pos, _b4g.nvm, _b4g.si, _checksums-1390157207161, _91s_es090_0.tim, _b4g_es090_0.doc, _7cn.nvm, _7cn_es090_0.tip, _agr_es090_0.pos, _7cn.si, _7cn_es090_0.blm, _7cn.fnm, _agr_es090_0.pay, _7cn_12.del, _b4g_es090_0.tip, _91s.si, _91s.nvm, _agr.fnm, _agr_es090_0.doc, _7cn_es090_0.pay, _91s_es090_0.tip, _agr.fdt, _91s_es090_0.blm, _agr.fdx, _agr_es090_0.tip, _7cn_es090_0.doc, _91s.fdt, segments_7i, _91s_d.del, _91s_es090_0.doc, _7cn_es090_0.tim, segments.gen, _91s_es090_0.pay, _agr.si, _7cn.fdt, _91s.fdx, _91s.fnm, _b4g_es090_0.tim, _b4g.fnm, _agr_d.del, _b4g_es090_0.blm, _91s.nvd, _agr_es090_0.blm, _91s_es090_0.pos]]; nested: FileNotFoundException[segments_7k]; ]]
[2014-01-19 19:15:15,437][WARN ][indices.cluster          ] [&lt;node name&gt;] [&lt;index name&gt;][4] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] failed to fetch index version after copying it over
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:136)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_7cn.fdx, _agr_es090_0.tim, _agr.nvm, _b4g.nvd, _b4g.fdt, _7cn.nvd, _7cn_es090_0.pos, _b4g.fdx, _agr.nvd, _b4g_es090_0.pay, _b4g_es090_0.pos, _b4g.nvm, _b4g.si, _checksums-1390157207161, _91s_es090_0.tim, _b4g_es090_0.doc, _7cn.nvm, _7cn_es090_0.tip, _agr_es090_0.pos, _7cn.si, _7cn_es090_0.blm, _7cn.fnm, _agr_es090_0.pay, _7cn_12.del, _b4g_es090_0.tip, _91s.si, _91s.nvm, _agr.fnm, _agr_es090_0.doc, _7cn_es090_0.pay, _91s_es090_0.tip, _agr.fdt, _91s_es090_0.blm, _agr.fdx, _agr_es090_0.tip, _7cn_es090_0.doc, _91s.fdt, segments_7i, _91s_d.del, _91s_es090_0.doc, _7cn_es090_0.tim, segments.gen, _91s_es090_0.pay, _agr.si, _7cn.fdt, _91s.fdx, _91s.fnm, _b4g_es090_0.tim, _b4g.fnm, _agr_d.del, _b4g_es090_0.blm, _91s.nvd, _agr_es090_0.blm, _91s_es090_0.pos]
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:115)
        ... 4 more
Caused by: java.io.FileNotFoundException: segments_7k
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:469)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:324)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:404)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:843)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:694)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:400)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:114)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:106)
        ... 4 more
[2014-01-19 19:15:15,438][WARN ][cluster.action.shard     ] [&lt;node name&gt;] [&lt;index name&gt;][4] sending failed shard for [&lt;index name&gt;][4], node[FgQg7A4HRdSuCKK-EHjNdQ], [P], s[INITIALIZING], indexUUID [gRKAbB7AQYGhgZRfi6pgzQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[&lt;index name&gt;][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[&lt;index name&gt;][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_7cn.fdx, _agr_es090_0.tim, _agr.nvm, _b4g.nvd, _b4g.fdt, _7cn.nvd, _7cn_es090_0.pos, _b4g.fdx, _agr.nvd, _b4g_es090_0.pay, _b4g_es090_0.pos, _b4g.nvm, _b4g.si, _checksums-1390157207161, _91s_es090_0.tim, _b4g_es090_0.doc, _7cn.nvm, _7cn_es090_0.tip, _agr_es090_0.pos, _7cn.si, _7cn_es090_0.blm, _7cn.fnm, _agr_es090_0.pay, _7cn_12.del, _b4g_es090_0.tip, _91s.si, _91s.nvm, _agr.fnm, _agr_es090_0.doc, _7cn_es090_0.pay, _91s_es090_0.tip, _agr.fdt, _91s_es090_0.blm, _agr.fdx, _agr_es090_0.tip, _7cn_es090_0.doc, _91s.fdt, segments_7i, _91s_d.del, _91s_es090_0.doc, _7cn_es090_0.tim, segments.gen, _91s_es090_0.pay, _agr.si, _7cn.fdt, _91s.fdx, _91s.fnm, _b4g_es090_0.tim, _b4g.fnm, _agr_d.del, _b4g_es090_0.blm, _91s.nvd, _agr_es090_0.blm, _91s_es090_0.pos]]; nested: FileNotFoundException[segments_7k]; ]]
[2014-01-19 19:15:15,512][WARN ][indices.cluster          ] [&lt;node name&gt;] [&lt;index name&gt;][4] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] failed to fetch index version after copying it over
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:136)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_7cn.fdx, _agr_es090_0.tim, _agr.nvm, _b4g.nvd, _b4g.fdt, _7cn.nvd, _7cn_es090_0.pos, _b4g.fdx, _agr.nvd, _b4g_es090_0.pay, _b4g_es090_0.pos, _b4g.nvm, _b4g.si, _checksums-1390157207161, _91s_es090_0.tim, _b4g_es090_0.doc, _7cn.nvm, _7cn_es090_0.tip, _agr_es090_0.pos, _7cn.si, _7cn_es090_0.blm, _7cn.fnm, _agr_es090_0.pay, _7cn_12.del, _b4g_es090_0.tip, _91s.si, _91s.nvm, _agr.fnm, _agr_es090_0.doc, _7cn_es090_0.pay, _91s_es090_0.tip, _agr.fdt, _91s_es090_0.blm, _agr.fdx, _agr_es090_0.tip, _7cn_es090_0.doc, _91s.fdt, segments_7i, _91s_d.del, _91s_es090_0.doc, _7cn_es090_0.tim, segments.gen, _91s_es090_0.pay, _agr.si, _7cn.fdt, _91s.fdx, _91s.fnm, _b4g_es090_0.tim, _b4g.fnm, _agr_d.del, _b4g_es090_0.blm, _91s.nvd, _agr_es090_0.blm, _91s_es090_0.pos]
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:115)
        ... 4 more
Caused by: java.io.FileNotFoundException: segments_7k
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:469)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:324)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:404)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:843)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:694)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:400)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:114)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:106)
        ... 4 more
[2014-01-19 19:15:15,515][WARN ][cluster.action.shard     ] [&lt;node name&gt;] [&lt;index name&gt;][4] sending failed shard for [&lt;index name&gt;][4], node[FgQg7A4HRdSuCKK-EHjNdQ], [P], s[INITIALIZING], indexUUID [gRKAbB7AQYGhgZRfi6pgzQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[&lt;index name&gt;][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[&lt;index name&gt;][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_7cn.fdx, _agr_es090_0.tim, _agr.nvm, _b4g.nvd, _b4g.fdt, _7cn.nvd, _7cn_es090_0.pos, _b4g.fdx, _agr.nvd, _b4g_es090_0.pay, _b4g_es090_0.pos, _b4g.nvm, _b4g.si, _checksums-1390157207161, _91s_es090_0.tim, _b4g_es090_0.doc, _7cn.nvm, _7cn_es090_0.tip, _agr_es090_0.pos, _7cn.si, _7cn_es090_0.blm, _7cn.fnm, _agr_es090_0.pay, _7cn_12.del, _b4g_es090_0.tip, _91s.si, _91s.nvm, _agr.fnm, _agr_es090_0.doc, _7cn_es090_0.pay, _91s_es090_0.tip, _agr.fdt, _91s_es090_0.blm, _agr.fdx, _agr_es090_0.tip, _7cn_es090_0.doc, _91s.fdt, segments_7i, _91s_d.del, _91s_es090_0.doc, _7cn_es090_0.tim, segments.gen, _91s_es090_0.pay, _agr.si, _7cn.fdt, _91s.fdx, _91s.fnm, _b4g_es090_0.tim, _b4g.fnm, _agr_d.del, _b4g_es090_0.blm, _91s.nvd, _agr_es090_0.blm, _91s_es090_0.pos]]; nested: FileNotFoundException[segments_7k]; ]]
[2014-01-19 19:15:15,549][WARN ][indices.cluster          ] [&lt;node name&gt;] [&lt;index name&gt;][4] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] failed to fetch index version after copying it over
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:136)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_7cn.fdx, _agr_es090_0.tim, _agr.nvm, _b4g.nvd, _b4g.fdt, _7cn.nvd, _7cn_es090_0.pos, _b4g.fdx, _agr.nvd, _b4g_es090_0.pay, _b4g_es090_0.pos, _b4g.nvm, _b4g.si, _checksums-1390157207161, _91s_es090_0.tim, _b4g_es090_0.doc, _7cn.nvm, _7cn_es090_0.tip, _agr_es090_0.pos, _7cn.si, _7cn_es090_0.blm, _7cn.fnm, _agr_es090_0.pay, _7cn_12.del, _b4g_es090_0.tip, _91s.si, _91s.nvm, _agr.fnm, _agr_es090_0.doc, _7cn_es090_0.pay, _91s_es090_0.tip, _agr.fdt, _91s_es090_0.blm, _agr.fdx, _agr_es090_0.tip, _7cn_es090_0.doc, _91s.fdt, segments_7i, _91s_d.del, _91s_es090_0.doc, _7cn_es090_0.tim, segments.gen, _91s_es090_0.pay, _agr.si, _7cn.fdt, _91s.fdx, _91s.fnm, _b4g_es090_0.tim, _b4g.fnm, _agr_d.del, _b4g_es090_0.blm, _91s.nvd, _agr_es090_0.blm, _91s_es090_0.pos]
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:115)
        ... 4 more
Caused by: java.io.FileNotFoundException: segments_7k
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:469)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:324)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:404)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:843)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:694)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:400)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:114)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:106)
        ... 4 more

```

the problem i have with this warning is, that it **never stops** (obviously it goes into some infinite loop trying to recover broken shard and it cant)
i have logging level set to WARN, so my log files are growing very fast (this warning is logged like 10 times per second on every node)

context:
- es version 0.90.9
- 3 nodes cluster
- 10 shards per index with 2 replicas
- local gateway
</description><key id="25886500">4798</key><summary>IndexShardGatewayRecoveryException: [&lt;index name&gt;][4] failed to fetch index version after copying it over</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2014-01-19T19:31:29Z</created><updated>2014-12-17T12:34:33Z</updated><resolved>2014-01-19T19:53:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-19T19:33:32Z" id="32717395">the shard might be retried in order to recover. Are you using multiple data path locations by any chance? there is a bug fixed in 0.90.10 in regards to it #4674 
</comment><comment author="karol-gwaj" created="2014-01-19T19:44:20Z" id="32717713">yep, im using multiple data path's
as for now i fixed this error manually (by deleting broken shard)
but just in case i will upgrade my cluster to 0.90.10 and we will see if it will happen again 
thx
</comment><comment author="kimchy" created="2014-01-19T19:53:09Z" id="32717936">Side note, you didn't have to delete the shard, you could have just deleted the `segments.gen` files in it
</comment><comment author="dieend" created="2014-10-20T05:31:22Z" id="59685403">I deleted all `segments.gen` files, and it worked. Thanks.
</comment><comment author="Bowrna" created="2014-12-17T10:16:10Z" id="67302391">Hi 

I get this error even after deleting the segments.gen files and restarting. 

This is the exception found in my log.

```
17-12-2014 15:30:31" "WARNING" "30" "[ES_NODE_NAME] [812843][0] failed to start shard" "org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [812843][0] failed to fetch index version after copying it over
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:136)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [812843][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_k.si, segments_u, _m.si, segments.gen, _m.cfe, _checksums-1418381938786, _k.cfs, _m.cfs, _k.cfe, write.lock]
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:115)
    ... 4 more
Caused by: org.apache.lucene.index.IndexFormatTooNewException: Format version is not supported (resource: MMapIndexInput(path=""/home/likewise-open/ZOHOCORP/bowrna-1819/elasticsearch/elasticSearchData/alarmcentral/nodes/0/indices/812843/0/index/segments.gen"")): -3 (needs to be between -2 and -2)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:782)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:694)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:400)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:114)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:106)
    ... 4 more
```
</comment><comment author="clintongormley" created="2014-12-17T12:34:33Z" id="67316069">@Bowrna 

```
IndexFormatTooNewException
```

Looks like you're trying to read a newer index with an older version of Elasticsearch.  This is not supported
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>_nodes endpoint response contains JSON field which has dot(.) </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4797</link><project id="" key="" /><description>_nodes endpoint response contains JSON field which has dot(.), which makes it difficult to deserialize into class because dots(.) are not allowed in variable name.

"settings" : {
        "path.home" : "/home/anand/logmanagementtools/elasticsearch/elasticsearch-0.90.9",
        "name" : "Fagin",
        "cluster.name" : "elasticsearch",
        "path.logs" : "/home/anand/logmanagementtools/elasticsearch/elasticsearch-0.90.9/logs"
      }
</description><key id="25877251">4797</key><summary>_nodes endpoint response contains JSON field which has dot(.) </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anandaverma</reporter><labels /><created>2014-01-19T09:09:07Z</created><updated>2014-01-19T09:29:22Z</updated><resolved>2014-01-19T09:28:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-19T09:28:48Z" id="32704288">This is not really an issue with es as it shouldn't be responsible for the variable naming in the client's application objects (btw, any decent un/marshalling framework should support custom mappings of json keys to object properties).

In `1.0` we changed the defaults where whenever es returns settings, we now return it as a structured json. Once can still get the settings flattened (using the dot-notation) by specifying `flat_settings` request parameter.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Initial implementation of custom _all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4796</link><project id="" key="" /><description>Closes #4520
</description><key id="25868207">4796</key><summary>Initial implementation of custom _all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-01-18T20:59:35Z</created><updated>2014-06-19T11:54:36Z</updated><resolved>2014-01-20T16:05:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-18T21:26:23Z" id="32693613">a hand full of minor comments but to me this looks pretty good though!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>SearchType randomization in integration tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4795</link><project id="" key="" /><description>Issue #4754 showed that using DFS_QUERY_THEN_FETCH instead of QUERY_THEN_FETCH
might expose interesting bugs.

Close #4793
</description><key id="25838096">4795</key><summary>SearchType randomization in integration tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-17T21:24:44Z</created><updated>2014-06-30T13:27:24Z</updated><resolved>2014-01-21T13:41:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T12:23:35Z" id="32755248">LGTM +1 to push after fixing the small issues I commented at
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add transport.publish_port setting Edit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4794</link><project id="" key="" /><description>Add transport.publish_port setting to allow users to specify the port
other cluster members should use when connecting to an instance. This
is needed for systems such as OpenShift, where cluster communication
needs to use a publicly accessibly proxy port, because the normal port
(9300) is bound to a private loopback IP address.

see https://github.com/elasticsearch/elasticsearch/pull/4359
</description><key id="25837669">4794</key><summary>Add transport.publish_port setting Edit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-17T21:19:22Z</created><updated>2014-01-17T21:19:53Z</updated><resolved>2014-01-17T21:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-17T21:19:53Z" id="32648213">pushed to master and 0.90 --&gt; https://github.com/elasticsearch/elasticsearch/commit/fff2f32f4fbc0eb0e9f1b073ea436fb839ab7956 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add SearchType randomization to ElasticsearchIntegrationTest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4793</link><project id="" key="" /><description>#4754 showed that executing queries with SearchType=DFS_QUERY_THEN_FETCH instead of just QUERY_THEN_FETCH might expose interesting bugs. We should randomize on the search type in our integration tests in order to find those bugs.
</description><key id="25837396">4793</key><summary>Add SearchType randomization to ElasticsearchIntegrationTest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v1.0.0</label></labels><created>2014-01-17T21:15:29Z</created><updated>2014-01-21T13:39:44Z</updated><resolved>2014-01-21T13:39:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-17T21:15:42Z" id="32647466">w00t +1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/indices/fielddata/breaker/CircuitBreakerServiceTests.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/client/RandomizingClient.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>SearchType randomization in integration tests.</comment></comments></commit></commits></item><item><title>build randomization, modify how local mode is run</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4792</link><project id="" key="" /><description>1) create dummy directory structure with JDK6 and JDK7 when running
local mode
2) move prop.txt from ‘/var/tmp’ to ENV[‘PWD’] when running local mode
</description><key id="25835991">4792</key><summary>build randomization, modify how local mode is run</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-01-17T20:54:04Z</created><updated>2014-06-12T16:07:20Z</updated><resolved>2014-01-17T20:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #4792 from mrsolo/master</comment></comments></commit></commits></item><item><title>Possibility to disable the version increment on some updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4791</link><project id="" key="" /><description>I'd like the version number of a doc to remain unchanged on some update operations.

Imagine you have a simple counter field, which is incremented whenever a user requests a document. (For example a video-views-counter in a video application)
When a user wants to update other (maybe more critical) fields of the doc the update might fail, only due to the increment of the counter field.

Therefore it would be nice to have the possibility to pass a param to the update operation to not change the version number. 

See also: https://groups.google.com/forum/#!topic/elasticsearch/C3faafCP4To
</description><key id="25833323">4791</key><summary>Possibility to disable the version increment on some updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joafeldmann</reporter><labels /><created>2014-01-17T20:09:29Z</created><updated>2014-12-24T18:41:21Z</updated><resolved>2014-12-24T18:41:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T18:41:20Z" id="68068879">Hi @joafeldmann 

Sorry it has taken a while to get to this ticket.  The functionality that you describe is quite dangerous.  Elasticsearch uses the version numbers for its own internal tracking. If you force the version numbers not to change, you're very likely to lose data.

Really the only options are to retry on conflict, to set a high external version number, or to just reindex the document ignoring version numbers.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>If lots of large shards fill up the disk on one node then the cluster won't properly rebalance shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4790</link><project id="" key="" /><description>If lots of large shards fill up the disk on one node then the cluster won't properly rebalance shards.  It won't move large shards from the full node to less full nodes - it'll grab shards on the less full nodes and swap them back and forth.
</description><key id="25828388">4790</key><summary>If lots of large shards fill up the disk on one node then the cluster won't properly rebalance shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2014-01-17T18:50:28Z</created><updated>2015-02-28T05:01:23Z</updated><resolved>2015-02-28T05:01:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-17T20:51:46Z" id="32645148">Hi @nik9000, are you using the Disk Threshold Decider or any custom shard allocation weights? If so, what are your settings?
</comment><comment author="nik9000" created="2014-01-17T21:32:44Z" id="32650410">Yeah,  Disk Threshold Decider:

``` js
{
    "transient" : {
        "cluster.routing.allocation.disk.threshold_enabled": true,
        "cluster.routing.allocation.disk.watermark.low": "0.85",
        "cluster.routing.allocation.disk.watermark.high": "0.95",
        "cluster.info.update.interval": "60s"
    }
}
```

I'm not using any awareness attributes right now.

By "fill up the disk" I mean end up between the low and high watermark.  I get two nodes with maybe 150 big shards and the rest end up with twice that many shards which are mostly smaller.  The two nodes sit just above the low watermark while the other shards have plenty of space and start trading shards back and forth between each other.  By back and forth I mean they really send the same shard back and forth.

I was able to dig my way out of the situation through a combination of manually moving large shards from the nodes with fewer shards to nodes with more shards.  That mostly worked.  Most of the time I could one large shard for a hand full of smaller ones.

I think the allocation heuristics must break down when you get that unbalanced.  The cluster might be able to work itself out of the state if nodes above the low water mark that have some % fewer shards then the average decided moved away their largest shards.  Or something.
</comment><comment author="xstevens" created="2014-03-20T16:09:24Z" id="38186441">For what it's worth I am seeing this same behavior without disk threshold enabled. We recently expanded our cluster and so now we have more nodes than shards on the older indices. On some of those indices we're seeing shards get bounced back and forth between a nodes that have no shards. For example: We have 14 data nodes and the old indices had 6 shards (w/ 1 replica).
</comment><comment author="clintongormley" created="2014-12-24T18:36:37Z" id="68068697">Hi @nik9000 

Are you still seeing these same dynamics with the disk allocator in 1.4.2?
</comment><comment author="nik9000" created="2014-12-24T18:45:16Z" id="68069048">We're no longer really a good candidate for reproducing this because we've doubled our capacity since I filed this.  We're much less likely to get to that point.  We also have yet to upgrade to 1.4.  That is probably coming early next year.

I imagine you could reproduce this with simulations similar to what I was doing to test #7171.
</comment><comment author="clintongormley" created="2015-02-28T05:01:23Z" id="76510892">This issue appears to be stale. Please feel free to open a new ticket if you see issues with the disk decider again.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reference docs fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4789</link><project id="" key="" /><description>I was reading through the new aggregations docs and found a few
minor quibbles that I ironed out in the process:
- Make it clearer that `aggs` is an allowed synomym
  for the `aggregations` key
- Fix broken example in for datehistogram, `1.5M` is
  not an allowed interval
- Make use of colon before examples consistent
- Fix typos
</description><key id="25822605">4789</key><summary>Reference docs fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2014-01-17T17:25:57Z</created><updated>2014-07-16T21:49:32Z</updated><resolved>2014-01-20T11:14:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-17T20:53:38Z" id="32645281">Hiya @skade 

Many thanks for all of the corrections. One question: why do you think `1.5M` isn't supported? `M` should be the unit for month. Have you seen a problem with using it?
</comment><comment author="clintongormley" created="2014-01-17T20:56:05Z" id="32645469">Please could you sign our CLA http://www.elasticsearch.org/contributor-agreement/ so that we can get this merged in.

thanks again
</comment><comment author="skade" created="2014-01-19T01:06:37Z" id="32698264">@clintongormley `1.5M`: I tried it and it didn't work. It also doesn't make a lot of sense, as month lengths differ and "half a month" is not a well defined interval. I'll check the CLA later today.
</comment><comment author="uboness" created="2014-01-19T09:14:17Z" id="32704116">it's correct, `1.5M` is not supported, we only support the following "franctional" time units:
- `ms` | `S` - milliseconds
- `s` - seconds
- `m` - minutes
- `h` | `H` - hours
- `d` - days
- `w` - weeks

btw... when no time unit is specified, we assume milliseconds
</comment><comment author="skade" created="2014-01-20T11:09:58Z" id="32750977">@clintongormley I signed the agreement through my company (asquera).
</comment><comment author="clintongormley" created="2014-01-20T11:14:48Z" id="32751274">Many thanks @skade - merged!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException (NPE) in completion suggester requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4788</link><project id="" key="" /><description>I've been getting sporadic NullPointerException's when executing completion suggester requests for some time and with several recent versions of ES (0.90.{8,9,10}). They're hard to reproduce. Here's an example:

```
[2014-01-17 11:08:22,577][DEBUG][action.suggest           ] [tbunce-mls-tim] [mls_fts_140117_094546][1], node[IKnJF62HSKe71A0Rr5s1tw], [P], s[STARTED]: Failed to execute [[[mls_fts]], suggestSource[{"GROUP":{"text":"0 health-check","completion":{"field":"mls_area_group_name_suggest"}},"ZIP":{"text":"0 health-check","completion":{"field":"address_zip_suggest"}},"AREA":{"text":"0 health-check","completion":{"field":"mls_area_name_suggest"}},"MLS#":{"text":"0 health-check","completion":{"field":"mls_num_suggest"}}}]]
    org.elasticsearch.ElasticSearchException: failed to execute suggest
    at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:167)
    at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:60)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$2.run(TransportBroadcastOperationAction.java:225)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.NullPointerException
```

They seem to only occur if data is being loaded at the same time.
</description><key id="25821737">4788</key><summary>NullPointerException (NPE) in completion suggester requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timbunce</reporter><labels><label>:Suggesters</label><label>bug</label><label>v0.90.12</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-17T17:13:38Z</created><updated>2015-06-07T23:46:14Z</updated><resolved>2014-01-31T20:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-17T17:14:36Z" id="32624671">do you have a full stacktrace where the excception occurs?
</comment><comment author="timbunce" created="2014-01-17T17:46:12Z" id="32627928">How could I get a full stacktrace where the excception occurs?
</comment><comment author="clintongormley" created="2014-01-31T18:12:17Z" id="33827189">@timbunce the stacktrace should be available in the elasticsearch logs
</comment><comment author="clintongormley" created="2014-01-31T18:13:45Z" id="33827313">Actually, what you originally pasted looks like it came from the logs already. ignore me
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java</file></files><comments><comment>Don't load CompetionTerms if lookupFactory is null</comment></comments></commit></commits></item><item><title>Added support for local flag to all cluster state read operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4787</link><project id="" key="" /><description>Added base Request class for read operations that usually happen on the master but can be executed locally.

Added base TransportAction class for master read operations that execute locally or not depending on the request class (local flag).

Added support for local flag where missing, in a backwards compatible manner:
     - IndicesExistsRequest
     - GetAliasesRequest (get alias api, aliases exist api)
     - TypesExistsRequest
     - GetIndexTemplatesRequest (get template, template exists)
     - GetSettingsRequest
     - GetRepositoriesRequest
     - PendingClusterTasks

 Added parsing of the local flag where missing in Rest*Action.

 Updated SPEC adding local flag param where missing and added REST tests that contain use of the local flag where it was just added.

Closes #3345
</description><key id="25816208">4787</key><summary>Added support for local flag to all cluster state read operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2014-01-17T15:53:59Z</created><updated>2014-07-01T04:22:04Z</updated><resolved>2014-01-20T11:36:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-20T09:04:25Z" id="32743277">LGTM
</comment><comment author="s1monw" created="2014-01-20T09:37:11Z" id="32745179">one small comment otherwise LGTM
</comment><comment author="javanna" created="2014-01-20T11:36:24Z" id="32752492">Merged into master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fix for #4785</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4786</link><project id="" key="" /><description>fix for #4785
</description><key id="25812680">4786</key><summary>fix for #4785</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels /><created>2014-01-17T15:02:03Z</created><updated>2014-07-16T21:49:33Z</updated><resolved>2014-01-17T21:01:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-01-17T21:01:24Z" id="32645873">Merged into master and 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>BulkRequestTests fails on Windows due to line ending differences</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4785</link><project id="" key="" /><description>Since windows uses different line endings `\r\n` vs *nixes `\n`, the tests fails as an extra char `\r` is returned on windows.
</description><key id="25812506">4785</key><summary>BulkRequestTests fails on Windows due to line ending differences</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-17T14:59:38Z</created><updated>2014-01-17T21:00:58Z</updated><resolved>2014-01-17T21:00:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-17T16:18:34Z" id="32619550">@costin can we make this fix depending on a constant. ie only replace if we are only windows?
</comment><comment author="costin" created="2014-01-17T16:20:40Z" id="32619743">@s1monw done.
</comment><comment author="s1monw" created="2014-01-17T17:05:15Z" id="32623888">LGTM
</comment><comment author="s1monw" created="2014-01-17T20:18:11Z" id="32642632">@costin +1 to push
</comment><comment author="costin" created="2014-01-17T21:00:58Z" id="32645835">Done.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file></files><comments><comment>apply line ending fix only on Windows</comment><comment>fix for #4785</comment></comments></commit></commits></item><item><title>Completion suggester interaction with synonyms could be improved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4784</link><project id="" key="" /><description>The current synonym token filter behaviour of _replacing_ the token with the synonym doesn't work well with the completion suggester.

An option to keep the original token would be helpful.

Background:

For synonyms to work well with the completion suggester it's important to define synonyms that map to longer tokens, i.e. `ne =&gt; north east`. (If the synonym mapping was an equivalence instead, such as `ne, north east`, then someone entering 'north east' would get suggestions for all completions that begin with `ne` such as `neck`.)

However, when a user enters "ne" they no longer get suggestions that start with "ne". They only get suggestions that start with "north east".

A workaround is to explicitly include the short form in the synonym mapping: `ne =&gt; north east, ne`.

However, that becomes painful to setup and maintain because a single mapping like `exp, expr, express, expw, expy =&gt; expressway` now has to be written out as five separate mappings!

I think a simple fix would be to add an option to make the synonym token filter _add_ synonyms instead of _replacing_ the original. Need only apply to the last token in the stream.
</description><key id="25812456">4784</key><summary>Completion suggester interaction with synonyms could be improved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timbunce</reporter><labels /><created>2014-01-17T14:58:58Z</created><updated>2014-12-24T18:32:37Z</updated><resolved>2014-12-24T18:32:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timbunce" created="2014-01-19T16:31:20Z" id="32712306">Here's an alternative approach to improve behaviour of synonyms with the completion suggester: add an option so synonyms only match complete terms in the FST, e.g, are followed by a space or the end of the FST.

Synonyms are terms by definition and only matched when there's a (presumed) complete term, such as typing the "t" at the end of "west", so it's reasonable that the synonyms of "west", eg "w", are also treated as complete terms and not as potential prefixes of longer terms.

This seems like a very natural approach. It would avoid the need for using explicit mappings (`=&gt;`), avoid the need to map shorter to longer terms, and avoid the need to duplicate the input in the output of the mapping.
</comment><comment author="clintongormley" created="2014-07-11T10:13:56Z" id="48714692">@timbunce you know that if you specify synonyms like `ne,north east` instead of using the `=&gt;`then all synonyms are output?
</comment><comment author="clintongormley" created="2014-07-11T10:16:07Z" id="48714863">i think this solves your issue and this ticket can be closed?
</comment><comment author="clintongormley" created="2014-12-24T18:32:37Z" id="68068512">No more info after 6 months - closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use millis for joda DateTimeFormatter.print() in cat timestamps</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4783</link><project id="" key="" /><description>Fixes #4782.
</description><key id="25809896">4783</key><summary>Use millis for joda DateTimeFormatter.print() in cat timestamps</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-17T14:15:35Z</created><updated>2014-06-14T22:28:26Z</updated><resolved>2014-01-17T14:39:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-17T14:35:17Z" id="32610267">LGTM +1 to push
</comment><comment author="drewr" created="2014-01-17T14:39:47Z" id="32610653">Merged in 3f14725.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Timestamp column regression in cat/health and cat/count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4782</link><project id="" key="" /><description>In #4696 we made the epoch time more Unix-compatible without realizing it was used in the `DateTimeFormatter` which outputs the HMS `timestamp` string.  Now that column shows the wrong time.

Also take this opportunity to use `TimeUnit` instead of literal math.
</description><key id="25808014">4782</key><summary>Timestamp column regression in cat/health and cat/count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>bug</label><label>regression</label><label>v1.0.0</label></labels><created>2014-01-17T13:38:44Z</created><updated>2014-01-17T14:38:07Z</updated><resolved>2014-01-17T14:38:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-17T14:33:41Z" id="32610131">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestHealthAction.java</file></files><comments><comment>Use millis for joda DateTimeFormatter.print()</comment></comments></commit></commits></item><item><title>Failed preparsing does not fail whole bulk request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4781</link><project id="" key="" /><description>If a preparsing of the source is needed (due to mapping configuration,
which extracts the routing/id value from the source) and the source is not
valid JSON, then the whole bulk request is failed instead of a single
BulkRequest.

This commit ensures, that a broken JSON request is not forwarded to the
destination shard and creates an appropriate BulkItemResponse, which
includes a failure.

This also implied changing the BulkItemResponse serialization, because one
cannot be sure anymore, if a response includes an ID, in case it was not
specified and could not be extracted from the JSON.

Closes #4745
</description><key id="25805933">4781</key><summary>Failed preparsing does not fail whole bulk request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Bulk</label><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-17T12:56:17Z</created><updated>2015-06-08T00:05:36Z</updated><resolved>2014-01-27T10:35:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T10:09:07Z" id="32747232">LGTM - added a small comment
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add SecurityManger / policy when running tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4780</link><project id="" key="" /><description>This commit adds a security manager to the test JVMs
that prevents mainly writing files outside of the JVMs
current test directory.
</description><key id="25803258">4780</key><summary>Add SecurityManger / policy when running tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-17T11:57:16Z</created><updated>2014-07-08T01:22:16Z</updated><resolved>2014-01-17T14:22:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-17T13:02:49Z" id="32603763">thx @markharwood 
</comment><comment author="spinscale" created="2014-01-17T13:20:32Z" id="32604860">passed for me as well on osx 10.9
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter array type by value ["a"] doesn't return any results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4779</link><project id="" key="" /><description>Hi, I'm new to the elasticsearch and I'm trying to understand how to implement the tag search.
I've created a simple document containing the tags: ['a', 'b', 'c'] and I observe a weird result. It finds by 'b' and 'c', but never by 'a'.

I wrote a simple py.test test file.
you can reproduce it by running elasticsearch 0.90.10 on default port.

install pytest and elasticsearch python clients: pip install pytest, pip install elasticsearch

then pytest test_es_a.py

It passes for all the options, but not for the ['a'] one.
What am I missing?

test_es_a.py:

https://gist.github.com/olegpidsadnyi/8471944
</description><key id="25801896">4779</key><summary>Filter array type by value ["a"] doesn't return any results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">olegpidsadnyi</reporter><labels /><created>2014-01-17T11:26:59Z</created><updated>2014-01-17T11:51:17Z</updated><resolved>2014-01-17T11:30:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-17T11:30:52Z" id="32598617">Please ask your questions on the mailing list.

BTW I answered somehow the same question some hours ago: https://groups.google.com/d/msgid/elasticsearch/0A41FF2C-E8D6-4893-B7C0-C40B55528978%40pilato.fr
</comment><comment author="olegpidsadnyi" created="2014-01-17T11:51:17Z" id="32599756">Many thanks. BTW the project is awesome
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove the "-f" script argument from the documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4778</link><project id="" key="" /><description>In the document docs/reference/setup.asciidoc, the following example is given :

```
bin/elasticsearch -f -Xmx2g -Xms2g -Des.index.store.type=memory --node.name=my-node
```

The `-f` argument looks to be no longer needed.
</description><key id="25799057">4778</key><summary>Remove the "-f" script argument from the documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">spydesk</reporter><labels><label>docs</label></labels><created>2014-01-17T10:33:02Z</created><updated>2014-01-17T10:45:00Z</updated><resolved>2014-01-17T10:44:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-17T10:39:02Z" id="32595474">It's already up to date in master doc. Where did you see it?

See http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/setup.html

Feel free to reopen if needed.
</comment><comment author="dadoonet" created="2014-01-17T10:39:26Z" id="32595503">Sorry! Got it!
</comment><comment author="dadoonet" created="2014-01-17T10:45:00Z" id="32595849">Fixed. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Remove the "-f" script argument from the documentation</comment></comments></commit></commits></item><item><title>Check ThreadInfo[] for null element if thread are not alive.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4777</link><project id="" key="" /><description>If a thread is not alive getting ThreadMXBean#getThreadInfo(long[], int)
places null elemnents in the returned array which are not repected
in the HotTheards API.

Closes #4775
</description><key id="25796667">4777</key><summary>Check ThreadInfo[] for null element if thread are not alive.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-17T09:45:43Z</created><updated>2014-07-16T21:49:35Z</updated><resolved>2014-01-17T16:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-17T10:08:26Z" id="32593411">I'm not an expert in this area, but it looks good to me!
</comment><comment author="kimchy" created="2014-01-17T16:04:44Z" id="32618255">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Added documentation for CAT Aliases API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4776</link><project id="" key="" /><description>Added asciidoc. Added new lines in java class.
</description><key id="25795525">4776</key><summary>[DOCS] Added documentation for CAT Aliases API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-01-17T09:19:49Z</created><updated>2014-07-13T06:01:54Z</updated><resolved>2014-01-20T08:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-01-17T13:33:38Z" id="32605743">:+1: 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to detect hot threads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4775</link><project id="" key="" /><description>I'm not really sure what caused this but I'm pretty sure I didn't get hot threads from the node it mentions:

```
[2014-01-17 01:36:11,648][DEBUG][action.admin.cluster.node.hotthreads] [elastic1008] failed to execute on node [Orfebp5QSN2iIag5IKTrXg]
org.elasticsearch.transport.RemoteTransportException: [elastic1001][inet[/10.64.0.108:9300]][cluster/nodes/hot_threads/n]
Caused by: org.elasticsearch.ElasticSearchException: failed to detect hot threads
        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:103)
        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:43)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:281)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:272)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.monitor.jvm.HotThreads.similarity(HotThreads.java:216)
        at org.elasticsearch.monitor.jvm.HotThreads.innerDetect(HotThreads.java:177)
        at org.elasticsearch.monitor.jvm.HotThreads.detect(HotThreads.java:75)
        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:101)
        ... 7 more
```
</description><key id="25782553">4775</key><summary>Failed to detect hot threads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-17T01:59:21Z</created><updated>2014-01-17T16:38:03Z</updated><resolved>2014-01-17T16:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-17T16:33:23Z" id="32620969">Thanks!
</comment><comment author="s1monw" created="2014-01-17T16:38:03Z" id="32621367">thank you!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/jvm/HotThreads.java</file><file>src/test/java/org/elasticsearch/action/admin/HotThreadsTest.java</file></files><comments><comment>Check ThreadInfo[] for null element if thread are not alive.</comment></comments></commit></commits></item><item><title>p/c filters should never cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4774</link><project id="" key="" /><description>Made sure that any filter that wraps a p/c filter (has_child &amp; has_parent) either directly or indirectly will never be cached by making CustomQueryWrappingFilter extend from NoCacheFilter.

This PR relies on #4768 otherwise the fix doesn't work.
</description><key id="25766065">4774</key><summary>p/c filters should never cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-16T21:16:49Z</created><updated>2015-05-18T23:33:04Z</updated><resolved>2014-01-20T10:12:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-17T09:40:24Z" id="32591590">LGTM in conjunction with #4768 
</comment><comment author="jpountz" created="2014-01-17T18:17:18Z" id="32630791">+1
</comment><comment author="martijnvg" created="2014-01-20T10:12:34Z" id="32747470">Pushed to master and 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix for p/c filters in scan api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4773</link><project id="" key="" /><description>Consume the entire weight and pre compute the DocIdSets for all segments instead of keeping the weight around and build a DocIdSet when a segment is being processed. This fixes issues where the has_child / has_parent filter produce no results or errors on subsequent scan requests.

Also made CustomQueryWrappingFilter implement Releasable in order to cleanup the pre-computed DocIdSets.

Closes #4703
</description><key id="25765444">4773</key><summary>Fix for p/c filters in scan api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-16T21:07:21Z</created><updated>2015-05-18T23:33:04Z</updated><resolved>2014-01-20T10:12:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-17T18:20:18Z" id="32631013">+1, this is an elegant fix
</comment><comment author="s1monw" created="2014-01-17T19:31:05Z" id="32638724">+1 LGTM especially given that we don't cache those fitlers anymore.
</comment><comment author="martijnvg" created="2014-01-20T10:12:45Z" id="32747478">Pushed to master and 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Updated test for `_source` exclusion changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4772</link><project id="" key="" /><description>I have updated this test to ensure that we always

&gt; maintain document structure above the point of exclusion
</description><key id="25761082">4772</key><summary>Updated test for `_source` exclusion changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">RobCherry</reporter><labels /><created>2014-01-16T20:21:08Z</created><updated>2014-08-15T12:39:50Z</updated><resolved>2014-08-15T12:39:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-08-15T12:39:50Z" id="52301255">superseded by #4715 . See discussion in #4491
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>IndexOutOfBoundsException on indexing empty JSON document in 0.90.10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4771</link><project id="" key="" /><description>In 0.90.10 on indexing an empty json document `{}` into an index with `_all` enabled and a default analyzer of type `keyword` and IndexOutOfBoundsException is thrown.

This Bug was introduced with commit 0ef6ed98945d9698f0703283086883554a28dfb7

curl-script to reproduce this issue:

``` bash
# delete index
curl -XDELETE http://localhost:9200/all_field_bug

# create index with default analyzer for every field
# without analyzer the index query below works fine
curl -XPUT http://localhost:9200/all_field_bug -d '{"index.analysis.analyzer.default.type":"keyword"}'

# create some fields, type doesn't matter here
# index query below works fine with "_all": {"enabled": false}
curl -XPOST http://localhost:9200/all_field_bug/default -d '{"properties":{"date":{"type":"date"}}}'

# should index this empty json document
# but returns {"error":"IndexOutOfBoundsException[Index: 0, Size: 0]","status":500}
curl -XPUT http://localhost:9200/all_field_bug/default/1 -d '{}'
```

More detailed stacktrace:

``` java
Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
    at java.util.ArrayList.rangeCheck(ArrayList.java:635)
    at java.util.ArrayList.get(ArrayList.java:411)
    at org.elasticsearch.common.lucene.all.AllEntries.boost(AllEntries.java:159)
    at org.elasticsearch.common.lucene.all.AllTokenStream.incrementToken(AllTokenStream.java:65)
    at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:102)
    at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:248)
    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:253)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:453)
    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1520)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1190)
    at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)
    at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:492)
    at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:386)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:212)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    ... 1 more
```

Will try to come up with a fix, if time is on my side.
</description><key id="25757937">4771</key><summary>IndexOutOfBoundsException on indexing empty JSON document in 0.90.10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mfelsche</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-16T19:54:38Z</created><updated>2014-01-20T10:48:22Z</updated><resolved>2014-01-20T10:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-17T13:04:39Z" id="32603866">Good catch!

&gt; Will try to come up with a fix, if time is on my side.

Cool, don't hesitate to ask for help if you need some!
</comment><comment author="jpountz" created="2014-01-17T16:16:28Z" id="32619348">@mfelsche Do you plan to work on this soon? I'm asking because it would be nice to have a fix for this issue in 1.0 GA.
</comment><comment author="mfelsche" created="2014-01-20T08:48:30Z" id="32742430">@jpountz I'm about to create a pull request. against which branch do you prefer it? v0.90.10 or master or something else?
</comment><comment author="jpountz" created="2014-01-20T09:17:04Z" id="32743994">@mfelsche Against master would be perfect!
</comment><comment author="s1monw" created="2014-01-20T10:48:22Z" id="32749657">fixed on 0.90 via bd91a67
and master via c42f770
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/all/AllEntries.java</file><file>src/test/java/org/elasticsearch/common/lucene/all/SimpleAllTests.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>fix IndexOutOfBoundsException on _all field without tokens and keyword analyzer</comment></comments></commit></commits></item><item><title>_cache:true leads to cache evictions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4770</link><project id="" key="" /><description>i dont know if this is wanted behavior but i think its quite weird to me.

When im doint a query with an "and" filter and force it to cache with:
"_cache":true then he average query time is about 10 times slower then without _cache.

Also the _stats show a rate of filter cache evictions with the same ratio has search query raises.

Removing the _cache:true the cache evictions drops to none.

Using: 0.90.10
Doing all the time the same query with ab (apache bench).

Do i misunderstand the concept behind _cache or does it always evict the complete cache before writing a new one?

So is this basically used like:
- one query with cache (so its cached)
- every other query fter that without cache parameter
  ??
</description><key id="25753914">4770</key><summary>_cache:true leads to cache evictions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">julianhille</reporter><labels /><created>2014-01-16T19:16:10Z</created><updated>2014-12-24T18:31:53Z</updated><resolved>2014-12-24T18:31:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-16T20:12:54Z" id="32530497">@julianhille Can you share the complete and filter (with all filters it encapsulates)?
</comment><comment author="julianhille" created="2014-01-16T20:20:22Z" id="32537021">i try creating a little gist
but it was a exists and and "term": {"fieldname": true} as second.
</comment><comment author="julianhille" created="2014-01-16T21:20:43Z" id="32548196">i tried to recreate that with a gist. The only thing i now can see is that the cache file size is raising, raising and raising.
</comment><comment author="clintongormley" created="2014-12-24T18:31:52Z" id="68068483">Hi @julianhille 

I'm assuming that you never managed to produce a gist demonstrating the issue? This refers to code which is quite old now, so I'm going to close this issue.  If you're still seeing similar problems, please open with a new issue and a recreation.

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[SPEC] Created snapshot.* and nodes.* namespaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4769</link><project id="" key="" /><description>Moved node_info, node_stats, shutdown and hot_threads into nodes.*
Moved snapshot and repository APIs into snapshot.*

Fixes https://github.com/elasticsearch/dev/issues/91
</description><key id="25749243">4769</key><summary>[SPEC] Created snapshot.* and nodes.* namespaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-01-16T18:50:14Z</created><updated>2014-06-13T22:32:48Z</updated><resolved>2014-01-17T11:23:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-17T11:23:06Z" id="32598177">Merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added no-cache infrastucture the the filter cache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4768</link><project id="" key="" /><description>During query parsing if a filter is encountered that extends from NoCacheFilter then the filter will not be given to the filter cache (also not wrapped in FilterCacheFilterWrapper). Also if a filter directly or indirectly wraps a NoCacheFilter then that filter will also not be cached.

This addition is useful for p/c filters, date range filters that use `NOW` in the range expressions and perhaps other filters.

Relates to #4757
</description><key id="25748944">4768</key><summary>Added no-cache infrastucture the the filter cache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-16T18:47:45Z</created><updated>2015-05-18T23:33:05Z</updated><resolved>2014-01-20T09:52:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-16T19:00:06Z" id="32506129">+1 LGTM
</comment><comment author="jpountz" created="2014-01-17T18:15:36Z" id="32630664">+1, nice idea
</comment><comment author="martijnvg" created="2014-01-20T09:52:12Z" id="32746121">Pushed to master and 0.90.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Logstash init script fails to record PID when ARGS contain asterisks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4767</link><project id="" key="" /><description>I am using the logstash 1.3.2 RPM found in this repository:
http://packages.elasticsearch.org/logstash/1.3/centos

I have set CONF_DIR to /etc/logstash/conf.d/*_/_.conf.
The asterisks seem to be messing with the regular expression used by pgrep to find the PID of the process that was started.

The process starts fine, but then the process cannot be killed when using the stop command on the init script. When issuing a restart, there will be duplicate processes.
Escaping the asterisks for pgrep allowed me to get the pid.
Either the asterisks need to be escaped or a different value besides the full ARGS list should be used to find the PID.

Also, in a somewhat related issue. It seems that in the init script for logstash, reads the ${DAEMON} variable in a few places, but it is never set. I think ${DAEMON} should probably be the ${JAVA} variable.
</description><key id="25743476">4767</key><summary>Logstash init script fails to record PID when ARGS contain asterisks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kpears201</reporter><labels /><created>2014-01-16T17:50:01Z</created><updated>2014-01-17T15:37:22Z</updated><resolved>2014-01-17T09:39:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-17T09:39:16Z" id="32591508">Hey,

I guess it makes more sense to add this to the logstash JIRA bug tracker at https://logstash.jira.com/ - as this might be an issue with logstash, which is just hosted on elasticsearch.org

Maybe @electrical can help debugging here as well...
</comment><comment author="kpears201" created="2014-01-17T15:37:22Z" id="32615777">Thanks. Issue created here:
https://logstash.jira.com/browse/LOGSTASH-1806
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better mem reporting in _cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4766</link><project id="" key="" /><description>Currently `ramPercent` is based on `Sigar.getMem().getUsedPercent()`, which includes file cache.  We need a `ramActualPercent` that does not take into account file cache.

It would be nice if there was a `getActualUsedPercent()` in `OsStats.Mem` so we could expose it directly, but failing that we can do the math in `RestNodesAction`.
</description><key id="25742997">4766</key><summary>Better mem reporting in _cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>non-issue</label><label>v1.0.0</label></labels><created>2014-01-16T17:43:06Z</created><updated>2014-01-16T19:59:55Z</updated><resolved>2014-01-16T19:59:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-01-16T19:59:55Z" id="32529315">Actually I think this is wrong.  Trying to reproduce what I saw (rather consistently) before, but I'm pretty sure this was pebkac.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>securing elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4765</link><project id="" key="" /><description>Hey there.

Iam new to elasticsearch. What is the best Way to securing elasticsearch for production sytem?

Thanks
</description><key id="25742109">4765</key><summary>securing elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">FabianKoestring</reporter><labels /><created>2014-01-16T17:30:44Z</created><updated>2014-01-16T17:32:51Z</updated><resolved>2014-01-16T17:32:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-16T17:32:51Z" id="32491463">Welcome! 

Ask question on the mailing list please. See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Search query type "scan" results are different compared to  QUERY_THEN_FETCH</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4764</link><project id="" key="" /><description>The scan type searches results in more results compared to other types.

We also have very frequent deletes to ES store. Its seems that scan includes the deleted documents and as well &amp; hence the docs returned inflated from scan type are inflated and includes the deleted docs.
We use scan as main type of all our search requirement as we need to fetch all the matched the results and enrich the results with different data sources building custom sort/facet mechanisms.

Returning of extra documents is having huge impact and we cannot afford to have full refresh every time.

we are using 0.90.5 version.
</description><key id="25739440">4764</key><summary>Search query type "scan" results are different compared to  QUERY_THEN_FETCH</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pmanvi</reporter><labels /><created>2014-01-16T16:57:03Z</created><updated>2014-01-16T17:03:47Z</updated><resolved>2014-01-16T17:03:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-16T17:03:47Z" id="32488609">Scan returns results without ordering, with a max of `size * no_of_shards` results at a time.  The results in a scan/scroll reflect the state of the index at the time that the scan started.

This is all exactly how it is supposed to work and is not a bug.  I suggest that you ask your question on the forum instead.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch startup script should set good ulimit/sysctl defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4763</link><project id="" key="" /><description>In order to have a good out-of-the-box experience, the `bin/elasticsearch` shell script should try to set 
- the number of open file descriptors (to 65536)
- since the switch to mmapfs the number of max mapped pages to 262144
- optionally: setting memory locked memory unlimited (not sure if it makes sense, since `bootstrap.mlockall` is disabled by default
</description><key id="25731799">4763</key><summary>Elasticsearch startup script should set good ulimit/sysctl defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label></labels><created>2014-01-16T15:21:28Z</created><updated>2014-03-13T08:13:23Z</updated><resolved>2014-03-13T08:13:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="damm" created="2014-01-23T23:50:35Z" id="33182891">make it easy to set the memory locked to unlimited if mlockall is enabled? Doubt that's easy without it looking at the config file.
</comment><comment author="s1monw" created="2014-03-12T19:50:10Z" id="37455522">@spinscale what's the status of this?
</comment><comment author="drewr" created="2014-03-12T19:53:27Z" id="37455878">@s1monw We were leaning -1 on this if I remember correctly.
</comment><comment author="spinscale" created="2014-03-13T08:13:23Z" id="37508521">we need to rethink this. A couple of reasons speak against this. Most likely you are not the admin user starting elasticsearch, so probabilities are quite low that you are allowed to change these settings at all. Closing it for now until we come up with a better idea.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[DOCS] Wording inconsistency between AND- and OR-filter in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4762</link><project id="" key="" /><description>The explanation of the AND- and OR-filters, especially the first sentence, needs to be revised. It currently reads like that:

_"A filter that matches documents using [and/or] boolean operator on other [TARGETS]"_

where [TARGETS] is either 'filters' or 'queries'

Despite the already difficult to parse/understand construction, this leaves me puzzled, which of the two versions for [TARGETS] is intended? I tend to say, both should read _"... on other filters"_, but I don't know. 

Also I wish, that sentence would be easier to understand. Maybe add an article, as in _"using_ **the** _[and/or] boolean operator"_?
</description><key id="25731677">4762</key><summary>[DOCS] Wording inconsistency between AND- and OR-filter in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels /><created>2014-01-16T15:19:46Z</created><updated>2014-08-05T15:17:44Z</updated><resolved>2014-08-05T15:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="konradkonrad" created="2014-01-16T15:22:58Z" id="32477923">Let me add the two files in question:

https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/query-dsl/filters/and-filter.asciidoc
and
https://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/query-dsl/filters/or-filter.asciidoc
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Resolve wording inconsistency</comment></comments></commit></commits></item><item><title>AbstractFieldMapper.merge doesn't return conflicts when trying to enable or disable norms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4761</link><project id="" key="" /><description>AbstractFieldMapper.merge should return a conflict when trying to enable or disable norms on an existing mapping.
</description><key id="25726556">4761</key><summary>AbstractFieldMapper.merge doesn't return conflicts when trying to enable or disable norms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0</label></labels><created>2014-01-16T14:03:01Z</created><updated>2014-01-20T15:14:45Z</updated><resolved>2014-01-20T15:14:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>Return a conflict when trying to enable/disable norms.</comment></comments></commit></commits></item><item><title>norms.enabled/omit_norms serialization and parsing are inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4760</link><project id="" key="" /><description>We compare the field type against the default one in `toXContent` in order to only serialize changes from the default field type, which has `norms.enabled: true`.

However we also have some logic to omit norms in case norms have not been configured and the field is `not_analyzed`.

This means that if you configure a field to be `not_analyzed` and have norms enabled:

``` javascript
{
  "type": string,
  "index": "not_analyzed",
  "norms": {
    "enabled": true
  }
}
```

it will be parsed correctly, but if you serialize it with toXContent, you will get:

``` javascript
{
  "type": "string",
  "index": "not_analyzed"
}
```

`norms.enabled` are missing because they are the same as in the default field type. So parsing it again would return a field which has norms disabled.

The same is true for `index_options` (docs, positions and offsets by default, docs_only in case of `not_analyzed` fields) but this is less of an issue given that it doesn't make sense to index offsets on a not_analyzed field.
</description><key id="25726209">4760</key><summary>norms.enabled/omit_norms serialization and parsing are inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0</label></labels><created>2014-01-16T13:57:54Z</created><updated>2014-01-20T15:22:00Z</updated><resolved>2014-01-20T15:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file></files><comments><comment>Internal: extend refresh-mapping logic to the _default_ type</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file></files><comments><comment>Make StringFieldMapper.toXContent aware of defaults for not_analyzed fields.</comment></comments></commit></commits></item><item><title>Improve result ordering logic for completion suggester to account for quality of match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4759</link><project id="" key="" /><description>We're using shingles for the inputs and we have many inputs per suggestion, plus synonyms (for [postal address abbreviations](https://www.usps.com/send/official-abbreviations.htm)) and we'd like to use fuzzy searching as well.

What we're finding is that short inputs will match many suggestions which are then ordered only by `weight`, regardless of the 'quality' of the input match. So the top `size` results _often don't include items that are much closer to the actual input_, and sometimes don't include the item that's an exact match! This understandably causes confusion, especially when users know that an exact match exists.

We're working around it by requesting suggestions with a much larger `size` value, then reordering the results with extra code before returning the 'top N' to the client.

I'm still working out the best logic to use for the reordering but we're limited in what we can achieve on the client-side. We'll probably put prefix matches first, then substring matches, then everything else.

Obviously we'd rather ES did this for us because it would do a better job (it has more information, such as synonyms and fuzzy edit distance), would apply to all matching results not just the top `size`, and would be faster.

Possibly related to #3791.
</description><key id="25725946">4759</key><summary>Improve result ordering logic for completion suggester to account for quality of match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">timbunce</reporter><labels /><created>2014-01-16T13:53:38Z</created><updated>2014-12-11T20:37:11Z</updated><resolved>2014-12-11T20:37:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-12-11T20:37:11Z" id="66684374">closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8909
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Get repository should support wildcards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4758</link><project id="" key="" /><description>These work:

```
GET /_snapshot/
GET /_snapshot/_all
GET /_snapshot/name
GET /_snapshot/name,name
```

But wildcards don't:

```
GET /_snapshot/*
GET /_snapshot/prefi*
```
</description><key id="25721759">4758</key><summary>Get repository should support wildcards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-01-16T12:33:23Z</created><updated>2016-11-06T07:40:40Z</updated><resolved>2016-11-06T07:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-16T15:05:23Z" id="32476145">Wondering if we should also support regex?

We were talking about it about templates with @kimchy to support wildcards (simpleMatch) or regex when using notation like `regex:YOUR_REGEX_HERE`. WDYT? Does it worth it for snapshots or useless?

Note: it's not implemented yet for templates.
</comment><comment author="jsnod" created="2014-07-30T17:15:16Z" id="50648450">I'd like this for delete operations as well, eg:

```
curl -XDELETE 'http://localhost:9200/_snapshot/myrepo/snapshot_201405*'
```
</comment><comment author="clintongormley" created="2016-11-06T07:40:36Z" id="258665519">Closed by https://github.com/elastic/elasticsearch/pull/15151
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Forcefully never cache any filter that wraps a p/c filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4757</link><project id="" key="" /><description>Any filter that wraps a p/c filter (has_child &amp; has_parent) either directly or indirectly must never be cached. 

The reason behind this is that the filter-cache caches per segment reader and the p/c filters rely on executing with a top level reader. The p/c filters execute in a two phase search. The first phase collects the parent ids of any document that matches with the wrapped filter or query. The second phase iterates over all parent or child documents and checks if the parent id of each document (for parent docs this the _uid field value and child docs the _parent field value) is in the set of ids collected in the first phase. The second phase executes per segment, but the first phase executed top level. 

Note: p/c filters on their own can't already be cached, since the cache options are a no-op in the filter parsers.
</description><key id="25719218">4757</key><summary>Forcefully never cache any filter that wraps a p/c filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-16T11:39:47Z</created><updated>2014-02-05T05:16:18Z</updated><resolved>2014-01-20T10:02:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/HasChildFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterBuilder.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Added test that verifies that p/c filters never cache.</comment><comment>Made the has_parent &amp; has_child filterbuilder's cache options a noop as well, like it is in the related parsers.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/search/child/CustomQueryWrappingFilter.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Made sure that any filter that wraps a p/c filter (has_child &amp; has_parent) either directly or indirectly will never be cached by making CustomQueryWrappingFilter extend from NoCacheFilter.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/NoCacheFilter.java</file><file>src/main/java/org/elasticsearch/index/query/QueryParseContext.java</file></files><comments><comment>Added no-cache infrastucture the the filter cache.</comment></comments></commit></commits></item><item><title>indices_boost ignore aliasing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4756</link><project id="" key="" /><description>It looks that the top level boosting based on the indices (_indices_boost_) requires that we specify the exact index name.  We cannot substitue an alias pointing to a given index.  This is very inconvenient since it defeats the purpose of having aliases to hide internal organization of the indices.
</description><key id="25718953">4756</key><summary>indices_boost ignore aliasing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/masaruh/following{/other_user}', u'events_url': u'https://api.github.com/users/masaruh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/masaruh/orgs', u'url': u'https://api.github.com/users/masaruh', u'gists_url': u'https://api.github.com/users/masaruh/gists{/gist_id}', u'html_url': u'https://github.com/masaruh', u'subscriptions_url': u'https://api.github.com/users/masaruh/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/572174?v=4', u'repos_url': u'https://api.github.com/users/masaruh/repos', u'received_events_url': u'https://api.github.com/users/masaruh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/masaruh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'masaruh', u'type': u'User', u'id': 572174, u'followers_url': u'https://api.github.com/users/masaruh/followers'}</assignee><reporter username="">fdejaeger</reporter><labels><label>:Search</label><label>bug</label></labels><created>2014-01-16T11:33:38Z</created><updated>2016-12-16T06:07:52Z</updated><resolved>2016-12-16T06:07:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajhalani" created="2014-04-07T18:05:43Z" id="39763047">+1 .. 
</comment><comment author="exAspArk" created="2014-09-18T09:38:40Z" id="56016470">I have the same problem
</comment><comment author="sschwell" created="2015-02-06T17:10:46Z" id="73273939">+1
</comment><comment author="tdoman" created="2015-08-18T17:39:11Z" id="132291483">+1, has this been addressed yet?  i must be able to use the alias.
</comment><comment author="EgorZhuk" created="2015-08-28T08:04:23Z" id="135666634">+1
</comment><comment author="dabit" created="2015-10-21T20:14:50Z" id="150012232">Has this been addressed? Please don't tell me this has not been fixed. Just bumped into it in production.
</comment><comment author="splittingfield" created="2015-10-23T19:20:46Z" id="150668316">+1 as we have just hit this surprising behavior in production as well.
</comment><comment author="masaruh" created="2015-11-12T07:24:49Z" id="156022382">Things to consider:
- should wildcard expansion be supported as well? (probably yes)
- what if an index belongs to more than one alias in indices_boost parameter? For example, `"indices_boost": {"alias1": 3,"alias2": 2}` and both aliases include `index1`. (sum boost? max? min?)
</comment><comment author="thiagolocatelli" created="2015-11-26T03:11:35Z" id="159791109">It seems the option to add indices_boost with the java client is not available. Is this true? I am using spring-data-elasticsearch and I cant seem to find a way to add indices_boots to my query. Thank you.
</comment><comment author="davibe" created="2016-01-13T15:06:59Z" id="171323020">+1
</comment><comment author="ramseydsilva" created="2016-01-14T12:48:55Z" id="171635709">I would like support for wildcard expansion as well.

:+1: 
</comment><comment author="LoooooKe" created="2016-04-26T11:45:32Z" id="214714487">+1
</comment><comment author="aurimasniekis" created="2016-05-03T15:01:56Z" id="216556819">Any updates on these? @clintongormley ?
</comment><comment author="bjorn-ali-goransson" created="2016-05-04T14:44:37Z" id="216887338">Calling all devs. @clintongormley @kimchy Is this for real?
</comment><comment author="thehybridtechnician" created="2016-10-19T15:55:47Z" id="254856880">+1
</comment><comment author="clintongormley" created="2016-11-06T11:37:57Z" id="258675341">This change stalled because of the problem of figuring out what to do when an index is boosted more than once via an alias or wildcard.  From https://github.com/elastic/elasticsearch/pull/8811#issuecomment-258675219 : 

@masaruh just reread this thread and i think the correct answer is here:

&gt; Make indices_boost take list of index name and boost pair. We may need to do this if we want to have full control. But I somewhat hesitate to do this because it's breaking change.

Then the logic would be that we use the boost from the first time we see the index in the list, so eg:

```
[  
  { "foo" : 2 },  # alias foo points to bar &amp; baz
  { "bar": 1.5 }, # this boost is ignored because we've already seen bar
  { "*": 1.2 }      # bar and baz are ignored because already seen, but index xyz gets this boost
]
```

This could be implemented in a bwc way. In fact, the old syntax doesn't need to be removed. We could just add this new syntax as an expert way of controlling boosts.

What do you think?
</comment><comment author="masaruh" created="2016-11-07T05:00:47Z" id="258747770">Ooh, true. that should work! Thanks @clintongormley.
I'll see what I can do.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/search/AbstractSearchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchDfsQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryAndFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/SearchQueryThenFetchAsyncAction.java</file><file>core/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>core/src/main/java/org/elasticsearch/search/DefaultSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/SearchService.java</file><file>core/src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>core/src/main/java/org/elasticsearch/search/internal/FilteredSearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchLocalRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/ShardSearchTransportRequest.java</file><file>core/src/main/java/org/elasticsearch/search/internal/SubSearchContext.java</file><file>core/src/test/java/org/elasticsearch/action/search/SearchAsyncActionTests.java</file><file>core/src/test/java/org/elasticsearch/index/SearchSlowLogTests.java</file><file>core/src/test/java/org/elasticsearch/search/SearchServiceTests.java</file><file>core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java</file><file>core/src/test/java/org/elasticsearch/search/indicesboost/SimpleIndicesBoostSearchIT.java</file><file>core/src/test/java/org/elasticsearch/search/internal/ShardSearchTransportRequestTests.java</file><file>test/framework/src/main/java/org/elasticsearch/test/TestSearchContext.java</file></files><comments><comment>Resolve index names in indices_boost</comment></comments></commit></commits></item><item><title>Provide better indication of the cluster state - is it writable (has quorum)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4755</link><project id="" key="" /><description>When using Elasticsearch as a critical application system you need to be able to monitor the cluster health. 
The current health indication is not enough since the "yellow" state doesn't have a singular meaning. 
It means that all primaries are up but some replica shards are not allocated. But this, on an operational level, has two possible implications - 
It can be that some shards are missing but there's a quorum to all indexes so the cluster is "writable".
It can also mean that some indexes don't have a quorum in which case those writes will fail. 

This is a huge difference on an operational level. 

We need a way to know (and monitor) the real state of the cluster - knowing not only that all primaries are up, but also if there's a quorum.

A possible way can be to add another color, for instance - 
yellow - some replicas are missing but you have a quorum
orange - primary up but no quorum

Or use another indicator altogether.
</description><key id="25716057">4755</key><summary>Provide better indication of the cluster state - is it writable (has quorum)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rore</reporter><labels><label>:Cluster</label><label>discuss</label><label>enhancement</label></labels><created>2014-01-16T10:36:22Z</created><updated>2016-09-27T22:20:14Z</updated><resolved>2016-09-27T12:18:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T15:50:23Z" id="91598471">I think we can add something like this to the indices level of the cluster health API, so something like:

``` json
{
  "active_primary_shards": 1,
  "active_shards": 1,
  "cluster_name": "elasticsearch",
  "indices": {
    "test": {
      "active_primary_shards": 1,
      "active_shards": 1,
      "initializing_shards": 0,
      "number_of_replicas": 1,
      "number_of_shards": 1,
      "relocating_shards": 0,
      "status": "yellow",
      "unassigned_shards": 1,
      "has_quorum": true
    }
  },
  "initializing_shards": 0,
  "number_of_data_nodes": 1,
  "number_of_nodes": 1,
  "number_of_pending_tasks": 0,
  "relocating_shards": 0,
  "status": "yellow",
  "timed_out": false,
  "unassigned_shards": 1
}
```

(see the `has_quorum` key)
</comment><comment author="rore" created="2015-04-11T19:15:57Z" id="91908531">Having it per index is still problematic to monitor. The idea here was that the global cluster health indicator will reflect the state. Like today, if you have only one index that isn't fully allocated the cluster health indicates it. But there's a big difference between a "yellow" that is caused by some not allocated replicates but still allows writes, and a "yellow" when there's no quorum and writes will fail. The cluster health currently doesn't differentiate between those states.
</comment><comment author="bleskes" created="2015-04-12T19:04:39Z" id="92100722">To me, yellow means “cluster is fully functional albeit not conforming to the required replication factor”. Red means that some operations can not be performed. With this definition in mind, I think we should signal an shard (and thus index &amp; cluster) as red if one can not index into it due to the `write_consistency` settings (which also be `all` , and not `quorum`)

&gt; On 11 Apr 2015, at 21:16, Rotem Hermon notifications@github.com wrote:
&gt; 
&gt; Having it per index is still problematic to monitor. The idea here was that the global cluster health indicator will reflect the state. Like today, if you have only one index that isn't fully allocated the cluster health indicates it. But there's a big difference between a "yellow" that is caused by some not allocated replicates but still allows writes, and a "yellow" when there's no quorum and writes will fail. The cluster health currently doesn't differentiate between those states.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="javanna" created="2015-04-15T13:18:56Z" id="93392641">I tend to agree with @rore here. We could improve the colors that we use depending on the cluster level `action.write_consistency` setting (note that it can be overridden per request though). I am not a big fan of returning `red` for an index that would be yellow but cannot be written into. Maybe a new color would help. I don't think this is ready to be an adoptme though, given that we are still discussing it? Maybe we should mark for discussion instead? :)
</comment><comment author="jpountz" created="2015-11-13T10:34:59Z" id="156392577">We just discussed this in Fixit Friday. Having indices that don't have a quorum reported RED or YELLOW seems problematic, so it appears we would either need to:
- add a new color (ORANGE?) (@javanna's suggestion)
- add a new flag (`has_quorum`?) on the index and cluster levels (@dakrone's suggestion)
</comment><comment author="dakrone" created="2016-09-27T11:04:47Z" id="249834184">@abeyad I think this is no longer applicable with the quorum changes you added at index creation?
</comment><comment author="abeyad" created="2016-09-27T12:16:07Z" id="249847756">The default as of 5.0 is to only require the primary shard to be active before indexing, which is the same as `!= RED`.  The only time we can't in the `YELLOW` cluster state is if `wait_for_active_shards` has been explicitly changed to a different value. In this situation, someone can just pass `wait_for_active_shards` as a request parameter to the indexing operation, and the indexing operation will wait for the requisite number of shards to be active before proceeding with the write (or it will timeout).  So I don't believe there is anything more to do for this.
</comment><comment author="dakrone" created="2016-09-27T12:18:22Z" id="249848206">Great! I'm going to close this then, we can always re-open if needed
</comment><comment author="javanna" created="2016-09-27T21:45:32Z" id="250008591">This issue is about getting info through cluster health about whether indexing would pass our "consistency" checks. We have now renamed write consistency to wait for active shards and made things much better, but do we have that piece of info now in cluster health? Or maybe we don't need to add it anymore? Sorry if I misunderstood or I am missing anything ;)
</comment><comment author="abeyad" created="2016-09-27T22:20:13Z" id="250016275">@javanna in 5.0, by default, if the cluster health is YELLOW, it means the "consistency" check will pass for write operations. If a user wants to increase the number of active shards to wait on before indexing proceeds, they can do so by setting the `wait_for_active_shards` parameter on the indexing operation, so at that point, they are in control of the value they give.  If a user wants to make sure that the value they give for the indexing operation would proceed, they can always do a health check first e.g. `/_cluster/health/{index_name}?wait_for_active_shards={n}`.  Not sure if that answers your question.. feel free to ask again if I wasn't clear :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Faceting using DFS_QUERY_THEN_FETCH fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4754</link><project id="" key="" /><description>Hey,

this came via the ML (there are also some prerequisites listed): https://groups.google.com/d/msg/elasticsearch/ySHfbIW4sBQ/8ozOkBcbWTMJ

Works on master, fails on 0.90

To reproduce (sometimes, not always)

```
public class BrokenFacetTest extends ElasticsearchIntegrationTest {

    @Test
    @TestLogging("_root:DEBUG")
    public void foo() throws Exception {
        client().prepareIndex("test-index", "test-type")
                .setSource("{ \"id\": 123, \"test-value\": 321 }")
                .setRefresh(true)
                .execute()
                .actionGet();

        SearchResponse searchResponse = client()
                .prepareSearch("test-index")
                .setQuery(new MatchAllQueryBuilder())
                .addFacet(new TermsFacetBuilder("test-facet").field("test-value"))
                .setSearchType(SearchType.DFS_QUERY_AND_FETCH)
                .execute()
                .actionGet();

        assertEquals(searchResponse.getFailedShards(), 0); // Failing!
    }

}
```

Exception being logged

```
[2014-01-16 10:13:16,061][DEBUG][org.elasticsearch.action.search.type] [node_2] [2] Failed to execute query phase
org.elasticsearch.ElasticSearchException
    at org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:37)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:360)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:338)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction$AsyncAction.executeSecondPhase(TransportSearchDfsQueryAndFetchAction.java:139)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction$AsyncAction$2.run(TransportSearchDfsQueryAndFetchAction.java:123)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.AssertionError
    at org.elasticsearch.common.recycler.ThreadLocalRecycler$TV.release(ThreadLocalRecycler.java:84)
    at org.elasticsearch.search.facet.terms.longs.TermsLongFacetExecutor.buildFacet(TermsLongFacetExecutor.java:122)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:200)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:129)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:357)
    ... 6 more
```
</description><key id="25712100">4754</key><summary>Faceting using DFS_QUERY_THEN_FETCH fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>v0.90.13</label></labels><created>2014-01-16T09:22:54Z</created><updated>2014-04-24T10:03:28Z</updated><resolved>2014-03-20T09:20:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-16T09:31:00Z" id="32453512">There is indeed an issue in the case of `DFS_QUERY_AND_FETCH` because the facet executor is initialized and requests a recycled object in executeDfsPhase while facets are built (and recycled objects released) in executeFetchPhase. For reference, here are a stack trace of allocation and release:

```
at org.elasticsearch.common.recycler.ThreadLocalRecycler$TV.&lt;init&gt;(ThreadLocalRecycler.java:67)
    at org.elasticsearch.common.recycler.ThreadLocalRecycler.obtain(ThreadLocalRecycler.java:57)
    at org.elasticsearch.common.recycler.Recycler$Sizing.obtain(Recycler.java:47)
    at org.elasticsearch.cache.recycler.CacheRecycler.longIntMap(CacheRecycler.java:236)
    at org.elasticsearch.search.facet.terms.longs.TermsLongFacetExecutor.&lt;init&gt;(TermsLongFacetExecutor.java:71)
    at org.elasticsearch.search.facet.terms.TermsFacetParser.parse(TermsFacetParser.java:214)
    at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:94)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:569)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:484)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:469)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:462)
    at org.elasticsearch.search.SearchService.executeDfsPhase(SearchService.java:168)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteDfs(SearchServiceTransportAction.java:168)
at org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchDfsQueryAndFetchAction.java:77)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```

```
at org.elasticsearch.common.recycler.ThreadLocalRecycler$TV.release(ThreadLocalRecycler.java:88)
    at org.elasticsearch.search.facet.terms.longs.TermsLongFacetExecutor.buildFacet(TermsLongFacetExecutor.java:104)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:200)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:129)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:357)
```
</comment><comment author="jpountz" created="2014-01-17T16:13:26Z" id="32619092">Even though this assertion failure shows that the recycler is not used optimally (the reference is acquired and used in different phases), I think this doesn't break anything. Since facets are going to be progressively replaced by aggregations, I'm wondering if we should just remove the assertion instead of trying to fix the issue?
</comment><comment author="s1monw" created="2014-01-20T15:53:12Z" id="32770955">so I don't want to loose the ability to fail if it's not released in the same thread to be honest. can we somehow only special case the broken facet impls?
</comment><comment author="s1monw" created="2014-03-12T20:26:53Z" id="37459982">@jpountz can we actually close this?
</comment><comment author="jpountz" created="2014-03-20T09:20:17Z" id="38147377">Closed via f7e887b5576fdadb729b608491b0ba55fd30dc86
</comment><comment author="jpountz" created="2014-04-24T10:03:28Z" id="41263106">For reference, the root cause of this issue has been fixed in #5821
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/indices/fielddata/breaker/CircuitBreakerServiceTests.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/client/RandomizingClient.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>SearchType randomization in integration tests.</comment></comments></commit></commits></item><item><title>[Failed to execute phase [query_fetch], all shards failed]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4753</link><project id="" key="" /><description>I was working on elastic search and it was working perfectly. Today I just restarted my remote server (Ubuntu). Now I am searching in my indexes, it is giving me this error.

{"error":"SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed]","status":503}

I also checked the health. The status is red. 
</description><key id="25711226">4753</key><summary>[Failed to execute phase [query_fetch], all shards failed]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iqbalmalik89</reporter><labels /><created>2014-01-16T09:04:35Z</created><updated>2014-01-16T21:52:23Z</updated><resolved>2014-01-16T21:52:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-16T21:52:23Z" id="32551010">Please use the mailing lists for such questions/discussions

https://groups.google.com/forum/?fromgroups#!forum/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Plugins not installed in correct directory in RC1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4752</link><project id="" key="" /><description>In elasticsearch.yml, I have the following

path.plugins: ${ES_PLUGINS}

where ES_PLUGINS environmental variable is pointing to a valid directory .../elasticsearch

and the RC1 is installed under that.  

In Beta2, new plugins were installed in the $ES_PLUGINS directory as expected but with RC1, the plugins are being installed in the plugin directory in the RC1 directory instead.

Installing river-jdbc with
./bin/plugin -install river-jdbc -url http://bit.ly/1ctvKka

Starting RC1 with 
./bin/elasticsearch --config=$ES_CONFIG/elasticsearch.yml --node.name=node0

where ES_CONFIG points to the ..../elasticsearch directory

Running 
Java 1.7.045 
OSX 10.9.1
</description><key id="25709319">4752</key><summary>Plugins not installed in correct directory in RC1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">sybsteve</reporter><labels /><created>2014-01-16T08:21:47Z</created><updated>2014-03-17T11:24:00Z</updated><resolved>2014-03-17T11:24:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-16T11:07:49Z" id="32459861">Hey,

can you help me reproducing this one, seems it works for me, but maybe I forgot something

Configured the conf file and then installed the plugin, which got installed to the right directory into `/tmp`

```
elasticsearch-installations/elasticsearch-1.0.0.RC1 » vi config/elasticsearch.yml
elasticsearch-installations/elasticsearch-1.0.0.RC1 » grep PLUGINS config/elasticsearch.yml
path.plugins: ${ES_PLUGINS}
elasticsearch-installations/elasticsearch-1.0.0.RC1 » export ES_PLUGINS=/tmp
elasticsearch-installations/elasticsearch-1.0.0.RC1 » bin/plugin -install mobz/elasticsearch-head
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/mobz/elasticsearch-head/archive/master.zip...
Downloading ......................................................DONE
Installed mobz/elasticsearch-head into /tmp/head
Identified as a _site plugin, moving to _site structure ...
elasticsearch-installations/elasticsearch-1.0.0.RC1 » ./bin/elasticsearch --node.name=node0
[2014-01-16 12:06:12,393][INFO ][node                     ] [node0] version[1.0.0.RC1], pid[96762], build[c6155c5/2014-01-15T17:02:32Z]
[2014-01-16 12:06:12,394][INFO ][node                     ] [node0] initializing ...
[2014-01-16 12:06:12,463][INFO ][plugins                  ] [node0] loaded [], sites [head]
[2014-01-16 12:06:15,374][INFO ][node                     ] [node0] initialized
[2014-01-16 12:06:15,374][INFO ][node                     ] [node0] starting ...
```
</comment><comment author="jprante" created="2014-01-20T18:49:40Z" id="32786156">As a side node, JDBC river 2.3.1 is not compatible to 1.0.0.RC1
</comment><comment author="spinscale" created="2014-03-17T11:24:00Z" id="37805523">closing due to lack of feedback, please reopen if new information are available. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed to load previously installed river-jdbc after upgrading to RC1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4751</link><project id="" key="" /><description>Migrated installation from 0.90.7 to 0.90.9 to 1.0.0.Beta2 with no problems but going to RC1 cause river-jdbc to fail to load.  Removing plugin by deleting the river's directory and then re-installing the river seemed to fix the problem.

Running 
Java 1.7.045 
OSX 10.9.1

Error:
failed to create river [jdbc][es_river]
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.util.ServiceConfigurationError: org.xbib.elasticsearch.river.jdbc.RiverMouth: Provider org.xbib.elasticsearch.river.jdbc.strategy.simple.SimpleRiverMouth could not be instantiated: java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchTimeoutException
  at org.xbib.elasticsearch.river.jdbc.JDBCRiver.&lt;init&gt;(Unknown Source)
  while locating org.xbib.elasticsearch.river.jdbc.JDBCRiver
  while locating org.elasticsearch.river.River

1 error
    at org.elasticsearch.common.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:344)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:178)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.river.RiversService.createRiver(RiversService.java:139)
    at org.elasticsearch.river.RiversService$ApplyRivers$2.onResponse(RiversService.java:271)
    at org.elasticsearch.river.RiversService$ApplyRivers$2.onResponse(RiversService.java:265)
    at org.elasticsearch.action.support.TransportAction$ThreadedActionListener$1.run(TransportAction.java:93)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.util.ServiceConfigurationError: org.xbib.elasticsearch.river.jdbc.RiverMouth: Provider org.xbib.elasticsearch.river.jdbc.strategy.simple.SimpleRiverMouth could not be instantiated: java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchTimeoutException
    at java.util.ServiceLoader.fail(ServiceLoader.java:224)
    at java.util.ServiceLoader.access$100(ServiceLoader.java:181)
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:377)
    at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
    at org.xbib.elasticsearch.river.jdbc.support.RiverServiceLoader.findRiverMouth(RiverServiceLoader.java:50)
    at org.xbib.elasticsearch.river.jdbc.JDBCRiver.&lt;init&gt;(JDBCRiver.java:119)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
</description><key id="25708662">4751</key><summary>Failed to load previously installed river-jdbc after upgrading to RC1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sybsteve</reporter><labels /><created>2014-01-16T08:05:07Z</created><updated>2014-01-16T09:40:45Z</updated><resolved>2014-01-16T08:40:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-16T08:40:49Z" id="32450626">Hey,

the jdbc river plugins needs to be adapted to support elasticsearch 1.0.0, as we changed many class names. So please file a bug report over there. Thanks!
</comment><comment author="sybsteve" created="2014-01-16T09:40:45Z" id="32454164">Sorry.  There is an update for the jdbc river which I just noticed

https://github.com/jprante/elasticsearch-river-jdbc
./bin/plugin -install river-jdbc -url http://bit.ly/1fzvNCy

After removing the old and installing the new, the problem was resolved.  Thanks for the quick response.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Strtup failure for Elastic search </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4750</link><project id="" key="" /><description>Whenever I try to start the elastic search I get this error.

JAVA_HOME points to an invalid Java installation (no java.exe found in "C:\Program Files\Java\jdk1.6.0_31\bin").

The JAVA_HOME is pointing to JDK 6 but still the same issue
</description><key id="25706970">4750</key><summary>Strtup failure for Elastic search </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">azam7861</reporter><labels /><created>2014-01-16T07:11:21Z</created><updated>2014-01-16T09:29:08Z</updated><resolved>2014-01-16T08:41:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-16T08:41:26Z" id="32450667">Please use the mailing list for those questions.
That said, you probably need to define a proper JAVA_HOME in your windows control panel (general settings)
</comment><comment author="azam7861" created="2014-01-16T09:28:34Z" id="32453361">the JAVA_HOME is pointing to JDK 6 but still the same issue
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support multiple rescores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4749</link><project id="" key="" /><description>Support multiple rescores

Detects if rescores arrive as an array instead of a plain object.  If so
then parse each element of the array as a separate rescore to be executed
one after another.  It looks like this:

``` js
   "rescore" : [ {
      "window_size" : 100,
      "query" : {
         "rescore_query" : {
            "match" : {
               "field1" : {
                  "query" : "the quick brown",
                  "type" : "phrase",
                  "slop" : 2
               }
            }
         },
         "query_weight" : 0.7,
         "rescore_query_weight" : 1.2
      }
   }, {
      "window_size" : 10,
      "query" : {
         "score_mode": "multiply",
         "rescore_query" : {
            "function_score" : {
               "script_score": {
                  "script": "log10(doc['numeric'].value + 2)"
               }
            }
         }
      }
   } ]
```

Rescores as a single object are still supported.

Also add documentation on score_mode when adding documentation about multiple
rescores.

Closes #4748
Closes #4742
</description><key id="25696898">4749</key><summary>Support multiple rescores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-16T01:30:16Z</created><updated>2014-06-25T15:35:27Z</updated><resolved>2014-01-23T15:35:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-16T01:31:02Z" id="32434575">This isn't quite ready but it is worth reviewing I think.  TODO:
1.  Test the explanation of multiple rescores.
2.  Documentation.
3.  Rest testing?
</comment><comment author="nik9000" created="2014-01-16T13:50:30Z" id="32469861">Added test for explanation.  It caught that I was building the explanations backwards so the first rescore looked like it processed output from the second when in fact the opposite is true.
</comment><comment author="nik9000" created="2014-01-16T13:53:04Z" id="32470069">Wait! I had it backwards!  The code was right and the test was wrong.  Fixed.
</comment><comment author="nik9000" created="2014-01-16T14:24:12Z" id="32472593">Added documentation and while I was in there documentation for `score_mode`.
</comment><comment author="nik9000" created="2014-01-16T14:39:40Z" id="32473824">I don't think I have anything to do for the rest testing/rest spec because it just describes the body of the search request as "The search definition using the Query DSL" which I think the asciidoc covers.
</comment><comment author="jpountz" created="2014-01-16T15:59:20Z" id="32481836">This looks good in general. However it seems to me that the second rescorer would be applied to all top docs but not only the first 10? (QueryRescorer.rescore rescores all top docs which was probably ok when there could be a single rescorer but now that there should be several ones, I think the TopDocsFilter should take the window size into account?)
</comment><comment author="s1monw" created="2014-01-16T16:33:54Z" id="32485457">cool stuff I like the feature!
</comment><comment author="nik9000" created="2014-01-16T18:46:49Z" id="32504490">@jpountz I'll have a look at that in a bit.  I thought I had a test that checked that if the second window is smaller then the first _and_ the first doesn't pull the match into the window then the second one doesn't see it.
</comment><comment author="nik9000" created="2014-01-16T22:28:27Z" id="32554094">@jpountz you were right of course.  My test was actually backwards.  It was making sure that the second rescore took effect when I wanted the opposite.

I've pushed a fix.  I also did some reworking on QueryRescorer#rescore because it was a little twisted.  The only real change is that TopDocsFilter now takes a maximum number of docs to filter and I set it to the rescore window.  I also set the maximum number of docs returned by the searcher to the rescore window rather than the size of topdocs.
</comment><comment author="nik9000" created="2014-01-23T02:20:41Z" id="33091462">Rebased.  Is there anything else I should change in this?
</comment><comment author="jpountz" created="2014-01-23T09:58:39Z" id="33110389">This looks very good . My only concern right now is about the client API. Now that it is possible to have several rescorers per request, it feels wrong to me to have the `setRescoreWindow` method on SearchRequestBuilder. @s1monw what do you think?

Something that would be nice also would be to validate that rescore window sizes are in strictly descending order. Otherwise applying a rescorer that is followed by a rescorer with a greater window size would be useless I think?
</comment><comment author="s1monw" created="2014-01-23T12:35:51Z" id="33120049">IMO the rescore window should be `max(default_window_size, max_rescorer_window` where we can `setRescoreWindow` as a default so we don't need to specify it everywhere? and I agree we should sort the rescorer by windows size!
</comment><comment author="nik9000" created="2014-01-23T14:02:42Z" id="33125761">&gt; Something that would be nice also would be to validate that rescore window sizes are in strictly descending order. Otherwise applying a rescorer that is followed by a rescorer with a greater window size would be useless I think?

I was thinking it might be nice to have a multiply rescore with a big window after a total rescore with a smaller window.  The multiply must come after so you multiply the totalled score.  The total has a smaller window because it is more expensive then the multiply.

&gt; `setRescoreWindow` as a default

I'll make this change and see what it looks like.
</comment><comment author="jpountz" created="2014-01-23T14:15:22Z" id="33126749">&gt; I was thinking it might be nice to have a multiply rescore with a big window after a total rescore with a smaller window. The multiply must come after so you multiply the totalled score. The total has a smaller window because it is more expensive then the multiply.

Agreed, let's not check the window sizes in order to allow for this kind of usage.
</comment><comment author="nik9000" created="2014-01-23T14:35:07Z" id="33128341">Added another commit to make `setRescoreWindow` set a default and to use set/add instead of set/next.  If you like it I'll squash the changes together.  I didn't want to have to do git backflips to get back to the old api in case this one doesn't make sense.
</comment><comment author="jpountz" created="2014-01-23T15:00:57Z" id="33130726">Looks good to me. I'm going to merge this PR if there are no objections.
</comment><comment author="nik9000" created="2014-01-23T15:06:45Z" id="33131255">Cool.  Want me to squash the commits or will you handle it?
</comment><comment author="s1monw" created="2014-01-23T15:09:21Z" id="33131488">LGTM - would be awesome if we could have an issue for this as well to mark the versions etc. otherwise +1 to the feature thanks nik!
</comment><comment author="nik9000" created="2014-01-23T15:13:48Z" id="33131937">&gt;  awesome if we could have an issue for this as well to mark the versions

Is #4748 what you need?
</comment><comment author="s1monw" created="2014-01-23T15:14:25Z" id="33131982">yeah @jpountz made see it too :) sorry for the noise! ;)
</comment><comment author="jpountz" created="2014-01-23T15:15:56Z" id="33132135">&gt; Want me to squash the commits or will you handle it?

Actually what would be nice would be to split the change into one commit for documentation of score mode that I'll merge into 1.0,1.x and master and the rest that I'll merge into 1.x and master.
</comment><comment author="nik9000" created="2014-01-23T15:23:31Z" id="33132816">Done.
</comment><comment author="jpountz" created="2014-01-23T15:35:59Z" id="33134033">Merged, thanks again Nik!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sequential rescores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4748</link><project id="" key="" /><description>Why stop at one rescore when you can execute a bunch of them in a sequence?  That way you can have different window sizes and the first rescore can drag interesting results up into subsequent rescores with smaller windows.
</description><key id="25696869">4748</key><summary>Sequential rescores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-16T01:29:23Z</created><updated>2015-06-06T18:43:58Z</updated><resolved>2014-01-23T15:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/dfs/DfsPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/explain/ExplainFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/main/java/org/elasticsearch/search/query/QueryPhase.java</file><file>src/main/java/org/elasticsearch/search/rescore/QueryRescorer.java</file><file>src/main/java/org/elasticsearch/search/rescore/RescoreBuilder.java</file><file>src/main/java/org/elasticsearch/search/rescore/RescoreParseElement.java</file><file>src/main/java/org/elasticsearch/search/rescore/RescorePhase.java</file><file>src/main/java/org/elasticsearch/search/rescore/Rescorer.java</file><file>src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/search/rescore/QueryRescorerTests.java</file></files><comments><comment>Support multiple rescores</comment></comments></commit></commits></item><item><title>test randomization assert</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4747</link><project id="" key="" /><description>Add 10 percent chance to add tests.assertion.disabled=org.elasticsearch 
</description><key id="25694807">4747</key><summary>test randomization assert</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-01-16T00:41:49Z</created><updated>2014-07-08T03:21:10Z</updated><resolved>2014-01-16T17:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #4747 from mrsolo/features/test_randomization</comment></comments></commit></commits></item><item><title>[SPEC] updated urls in the 0.90 rest-spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4746</link><project id="" key="" /><description>Ffollowed the current redirects to update the url's in the rest-spec so that they can be included directly in the docs.
</description><key id="25693687">4746</key><summary>[SPEC] updated urls in the 0.90 rest-spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels /><created>2014-01-16T00:15:43Z</created><updated>2014-07-02T06:53:10Z</updated><resolved>2014-01-16T18:14:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2014-01-16T09:27:27Z" id="32453288">Can we prepend the commit with `[SPEC]` so it is more parseable in the Git log?

Also, given the imminent release of 1.0 final, can we change the URLs to point to docs for 0.90 on the `0.90` branch? @clintongormley, can it be done like this?
</comment><comment author="clintongormley" created="2014-01-16T10:45:55Z" id="32458491">@karmi yes it can, eg:

```
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html
```

in the 0.90 branch would change to:

```
"http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/docs-bulk.html
```
</comment><comment author="spalger" created="2014-01-16T18:10:30Z" id="32500904">Now all the urls reference the 0.90 docs specifically. Good catch @karmi 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Enabling "_timestamp" can cause bulk API to fail entire request instead of single operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4745</link><project id="" key="" /><description>As I understand it, the intention of the bulk API is that individual operations may fail, but a failure in an individual operation should generally not cause the failure of all operations in the request.

First let's verify that this is generally how it works. I am testing here with a simple case of malformed JSON (though I originally saw the problem with a subtler JSON-parsing issue of unexpected non-printable ASCII characters in JSON data).

```
$ curl -XPUT http://localhost:9200/testing/
{"ok":true,"acknowledged":true}

$ curl -XPUT http://localhost:9200/testing/person/_mapping -d '{"person": {"dynamic": "strict", "properties": {"last_modified": {"type": "date", "format": "dateOptionalTime"},"name": {"type": "string"}}}}'
{"ok":true,"acknowledged":true}

$ cat baddata.txt
{"index": {"_id": "1"}}
{"name": "Malformed}
{"index": {"_id": "2"}}
{"name": "Good"}

$ curl -XPOST http://localhost:9200/testing/person/_bulk --data-binary @baddata.txt
{"took":72,"items":[{"index":{"_index":"testing","_type":"person","_id":"1","error":"MapperParsingException[failed to parse [name]]; nested: JsonParseException[Unexpected end-of-input in VALUE_STRING\n at [Source: [B@27beb7ec; line: 1, column: 65]]; "}},{"index":{"_index":"testing","_type":"person","_id":"2","_version":1,"ok":true}}]}
```

This worked correctly - one item failed with an error, the other succeeded, and we do indeed find one item indexed in a subsequent search.

Now let's re-create that index and enable the magic "_timestamp" field this time:

```
$ curl -XDELETE http://localhost:9200/testing/
{"ok":true,"acknowledged":true}

$ curl -XPUT http://localhost:9200/testing/
{"ok":true,"acknowledged":true}

$ curl -XPUT $LOCAL/testing/person/_mapping -d '{"person": {"_timestamp": {"enabled": true, "path": "last_modified"}, "dynamic": "strict", "properties": {"last_modified": {"type": "date", "format": "dateOptionalTime"},"name": {"type": "string"}}}}'
{"ok":true,"acknowledged":true}

$ curl -XPOST $LOCAL/testing/person/_bulk --data-binary @baddata.txt
{"error":"ElasticSearchParseException[failed to parse doc to extract routing/timestamp]; nested: JsonParseException[Unexpected end-of-input in VALUE_STRING\n at [Source: [B@68f55ff2; line: 1, column: 65]]; ","status":400}
```

This time the entire request errors out and returns a 400 response code, and no items are successfully indexed.

Since the malformed JSON is limited to a single action in the bulk request, I would expect only that action to fail, regardless of whether the "_timestamp" magic field is enabled or not.

Tested against latest ElasticSearch release:

```
$ curl http://localhost:9200
{
  "ok" : true,
  "status" : 200,
  "name" : "Pip the Troll",
  "version" : {
    "number" : "0.90.10",
    "build_hash" : "0a5781f44876e8d1c30b6360628d59cb2a7a2bbb",
    "build_timestamp" : "2014-01-10T10:18:37Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="25679253">4745</key><summary>Enabling "_timestamp" can cause bulk API to fail entire request instead of single operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">carljm</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-15T20:29:06Z</created><updated>2014-01-27T11:08:36Z</updated><resolved>2014-01-27T10:35:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkItemResponse.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Bulk: Failed preparsing does not fail whole bulk request</comment></comments></commit></commits></item><item><title>Plain highlighter is crazy slow for fuzzy queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4744</link><project id="" key="" /><description>I don't have consistent reproduction steps but could probably put something together if you need it.  Essentially a query like "fuzzythin~" will match and properly highlight "fuzzything" but the highlighter will take ~100ms chewing on it.  I'm not sure if this is a bug or maybe just something that should be documented.
</description><key id="25678727">4744</key><summary>Plain highlighter is crazy slow for fuzzy queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label></labels><created>2014-01-15T20:20:22Z</created><updated>2016-11-06T11:23:08Z</updated><resolved>2016-11-06T11:23:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="NoelKennedy" created="2016-05-12T12:58:14Z" id="218749036">This problem is affecting us as well.  It does seem to be the '~1' Levenshtein edit distance matches that slows highlighting down.  Prefix searches are unaffected.  This problem is a bit odd because string edit distance searches are several orders of magnitude faster on 120m documents (in 203ms) than they are when the document highlighter runs on 50 docs (in 26 seconds).
</comment><comment author="clintongormley" created="2016-11-06T11:23:08Z" id="258674646">Closing in favour of https://github.com/elastic/elasticsearch/issues/11442
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consistent indices APIs, part 2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4743</link><project id="" key="" /><description>Issue #4071 completed a lot of the work to make the various indices APIs consistent.  This issue details the work that remains:
## PUT (singular) COMPLETED

```
Request                                 Status
-----------------------------------------------------------------------------------
PUT /{index}/_mapping/{type}            Done
{ single defn }

PUT /{index}/_warmer/{name}             Done
{ single defn }

PUT /{index}/_alias/{name}              Done
{ single defn }

PUT /{index}/_setting/{name}            Will not support
{ single defn }
```

Also support:

```
 PUT /{index}/{type}/_mapping           Done
 PUT &amp; POST                             Done
```

Where: 
- `{index}` = `blank | * | _all | prefix* | name,name`
- `{name}`  = `name`
## POST (plural) WORK NEEDED

```
Request                                 Status
-----------------------------------------------------------------------------------
POST /{index}/_mappings                 Not done
{ multi defn }

POST /{index}/_warmers                  Not done
{ multi defn }

POST /_aliases                          Already supported
{ actions }

POST /{index}/_settings                 Already supported
{ multi defn }
```

Also support:

```
 PUT &amp; POST                             Add URLs
```

Where: 
- `{index}` = `blank | * | _all | prefix* | name,name` 
## GET (singular or plural) WORK NEEDED

```
Request                                 Status
-----------------------------------------------------------------------------------
GET /{index}/_mappings/{type}           Add URL
GET /{index}/_mapping/{type}            Done

GET /{index}/_warmers/{name}            Add URL
GET /{index}/_warmer/{name}             Done

GET /{index}/_aliases/{name}            See note below
GET /{index}/_alias/{name}              Done

GET /{index}/_settings/{name}           Done
GET /{index}/_setting/{name}            Add URL
```

Also support:

```
GET /{index}/{type}/_mapping            Done
```

 Where: 
- `{index}` = `blank | * | _all | prefix* | name,name`
- `{name}`  = `blank | * | _all | prefix* | name,name`

Missing types/warmers/aliases/settings should return empty objects, not throw 404s

Note: The `GET /_aliases` response is different from `GET /_alias` currently, as it returns all matching indices, regardless of whether they have aliases or not.  This functionality is required by Kibana (et al) and should be implemented via the proposed functionality in #4614 or #4609.
## DELETE (singular or plural) WORK NEEDED

```
Request                                 Status
-----------------------------------------------------------------------------------
DELETE /{index}/_mappings/{type}        Add URL
DELETE /{index}/_mapping/{type}         Done

DELETE /{index}/_warmers/{name}         Add URL
DELETE /{index}/_warmer/{name}          Done

DELETE /{index}/_aliases/{name}         Add URL
DELETE /{index}/_alias/{name}           Done

DELETE /{index}/_settings/{name}        Will not be supported
DELETE /{index}/_setting/{name}         Will not be supported
```

Also support:

```
DELETE /{index}/{type}/_mapping         Done
```

 Where: 
- `{index}` = `* | _all | prefix* | name,name` BLANK NOT ALLOWED
- `{name}`  = `* | _all | prefix* | name,name` BLANK NOT ALLOWED

If no matching types/warmers/aliases/settings found, throw 404.
## HEAD (singular or plural) WORK NEEDED

```
Request                                 Status
-----------------------------------------------------------------------------------
HEAD /{index}/_mappings/{type}           Not done
HEAD /{index}/_mapping/{type}            Not done

HEAD /{index}/_warmers/{name}            Not done
HEAD /{index}/_warmer/{name}             Not done

HEAD /{index}/_aliases/{name}            Add URL
HEAD /{index}/_alias/{name}              Done

HEAD /{index}/_settings/{name}           Not done
HEAD /{index}/_setting/{name}            Not done
```

Also support:

```
HEAD /{index}/{type}/_mapping            Add URL
```

 Where: 
- `{index}` = `blank | * | _all | prefix* | name,name`
- `{name}`  = `blank | * | _all | prefix* | name,name`

If no matching types/warmers/aliases/settings found, throw 404.
</description><key id="25672241">4743</key><summary>Consistent indices APIs, part 2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>enhancement</label></labels><created>2014-01-15T19:09:20Z</created><updated>2016-10-06T15:50:45Z</updated><resolved>2016-10-06T15:50:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kevinkluge" created="2014-05-05T19:04:14Z" id="42224981">Pushed to 1.3.  Looks like a big change to start now...
</comment><comment author="dakrone" created="2016-09-27T11:00:06Z" id="249833270">@clintongormley is this something we still want to do? I think it would be better to standardize on a single API endpoint instead of trying to handle singular/plural for all the APIs
</comment><comment author="clintongormley" created="2016-10-06T15:50:45Z" id="252004829">@dakrone I agree.  i think we're good where we are today
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[docs] Rescore's score_mode not documented</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4742</link><project id="" key="" /><description>Looks like this was implemented in #3258 but never documented.
</description><key id="25654320">4742</key><summary>[docs] Rescore's score_mode not documented</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v1.0.0</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-15T15:19:04Z</created><updated>2014-03-21T10:05:53Z</updated><resolved>2014-01-23T15:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Documentation for score_mode</comment></comments></commit></commits></item><item><title>Storing queries in ElasticSearch and use it like a view</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4741</link><project id="" key="" /><description>It is easy to query ElasticSearch but exchanging with others is not as easy as I would like to have it.

I would like to have the possibillity to save queries to ElasticSearch in a way similar to the percolator API:

```
url -XPUT localhost:9200/_query/myqueryname -d '{
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    }
}'
```

Later I would like to execute the query via

```
curl -X GET localhost:9200/_query/myqueryname
```

I think such a functionality is not available in ElasticSearch at the moment. Or? 
</description><key id="25652091">4741</key><summary>Storing queries in ElasticSearch and use it like a view</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">obfischer</reporter><labels /><created>2014-01-15T14:47:59Z</created><updated>2014-05-02T08:22:49Z</updated><resolved>2014-04-23T15:18:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2014-01-15T15:29:45Z" id="32371363">Hi Oliver,

What you describe sounds pretty similar to a plugin I'm currently working on: My goal goes a bit further in that I would like to be able to store a query template that at query time can then be filled with actual parameters. For a first stab at this functionality see here: https://github.com/MaineC/elasticsearch-query-templates 

Storing and later loading such templates is what I am looking into right now.

Isabel
</comment><comment author="uboness" created="2014-01-15T15:47:32Z" id="32373243">+1 on Isabel's comment... Plus we currently support attaching filters to aliases... These are not queries (so no scoring) but they can serve as a filtered views into your data

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-aliases.html#filtered

On Wed, Jan 15, 2014 at 9:29 AM, Isabel Drost-Fromm
notifications@github.com wrote:

&gt; Hi Oliver,
&gt; What you describe sounds pretty similar to a plugin I'm currently working on: My goal goes a bit further in that I would like to be able to store a query template that at query time can then be filled with actual parameters. For a first stab at this functionality see here: https://github.com/MaineC/elasticsearch-query-templates 
&gt; Storing and later loading such templates is what I am looking into right now.
&gt; 
&gt; ## Isabel
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/4741#issuecomment-32371363
</comment><comment author="javanna" created="2014-04-23T15:18:25Z" id="41173806">Hi @obfischer , did you have a look at the [search template](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html) feature that got released with `1.1`? That should be what you were looking for, have a look at #5637 as well for further improvements on it.

Closing this issue for now, feel free to reopen though if you think search templates don't address it.
</comment><comment author="obfischer" created="2014-05-02T08:22:23Z" id="42002355">Hi @javanna, yes this is what I suggested. Great to see this available.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reverse sort order for date_/histogram facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4740</link><project id="" key="" /><description>The entries of the terms/terms_stats facets can be ordered on different fields (count, term, total, etc) and each of those fields also supports reverse ordering. 

How difficult would it be to also provide reverse ordering for the histogram and date_histogram facets? Both of them can be ordered by count and total (+key resp. time) but the reverse_\* counterpart for both count and total is not supported, even though it could be useful in many situations.

The thinking behind this is to enable one to quickly find the bucket with either the minimum or maximum count/total (of whatever numeric field) without having to retrieve/parse and look at the end of the facet entries array.

Taking this one step further, if those two facets would support a "size" parameter, one could specify size=1 and sort=reverse_total in order to find the one bucket with the highest total value.

One real-world use case for this could be to find the day for which the highest/lowest turnover was achieved (or whatever numeric figure you can think about). If the business has been going on for 10 years, we have 3650 buckets to return/parse/digest and we're only interested in the last one.
</description><key id="25644519">4740</key><summary>Reverse sort order for date_/histogram facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">consulthys</reporter><labels /><created>2014-01-15T12:39:18Z</created><updated>2014-02-22T16:28:51Z</updated><resolved>2014-02-22T16:28:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-15T15:06:06Z" id="32369040">Histogram aggregations, which are going to replace histogram facets in the long term, are going to support ordering in ascending order: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-histogram-aggregation.html#_order_2

&gt; Taking this one step further, if those two facets would support a "size" parameter, one could specify size=1 and sort=reverse_total in order to find the one bucket with the highest total value.

Selection problems are typically solved by terms aggregations instead of histograms. For example, I think you could use a script (`field_value / interval`) to find the bucket that has the largest number of buckets?
</comment><comment author="consulthys" created="2014-01-15T15:18:21Z" id="32370199">Adrien, thank you for your answer and suggestion.

I'm eager to migrate to aggregations, indeed, yet for various reasons, I'll be more or less stuck with ES 0.90.\* (more precisely the 0.90.7 release) for a couple more months. Apologies for not having mentioned that earlier.

I'm not sure the terms facet supports the "interval" parameter. Could you point out what you mean in your last paragraph, please? Thanks much for your help and support.
</comment><comment author="jpountz" created="2014-01-15T17:00:26Z" id="32381118">&gt; I'll be more or less stuck with ES 0.90.*

That is perfectly fine. However, we don't plan to do any change in facets in the future and to stay focused on aggregations.

&gt; I'm not sure the terms facet supports the "interval" parameter.

They don't support it natively indeed, but it is possible to use the scripting support of terms facets to do that. See
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_term_scripts

For example, if your field is `price` and you want to find the top bucket with an interval of 10, you could run a terms facet using `(int) (price / 10)` as a script.
</comment><comment author="consulthys" created="2014-01-15T17:27:20Z" id="32388018">Ok, I understand, then you might close this non-issue as I can get around it for now, yet in a non-performant way, but I still get the result I want.

To better illustrate what I'm after, below is the facet I'm using. If there's a better way of achieving the same result in a more performant manner, I'm all ears.

```
  "facets": {
    "best_day": {
      "date_histogram": {
        "interval": "day",
        "order": "total",
        "key_field": "paidDate",
        "value_field": "paidAmount"
      }
    }
  }
```

Basically, I want to bucket the whole time spectrum into daily buckets and for each I want to know the total of the paidAmount field. I'm then ordering the result entries on the "total" and I know that the last bucket in the list will be the one I'm looking for. Hence why if I could reverse-order and specify size=1 I'd only get the bucket I'm interested in.

Thanks for your time.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow changing write consistency timeout on a cluster level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4739</link><project id="" key="" /><description>The write consistency timeout (which is 1 minute by default) can be changed only with the timeout parameter on each individual indexing request.

It will be very useful to be able to change it globally on a cluster level (via configuration / settings), so you can be sure that the cluster doesn't hang when there are recovering shards. 

In the current situation after a cluster restart, when some indexes don't have a quorum yet, indexing requests hang and timeout. This can cause a cascading failure on an application level if you have a high throughput of indexing. In many cases it's better to fail the request fast rather than timeout.  
</description><key id="25641963">4739</key><summary>Allow changing write consistency timeout on a cluster level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rore</reporter><labels><label>:Settings</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-01-15T11:47:20Z</created><updated>2016-11-07T13:05:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-04-10T15:46:59Z" id="91597857">@rore when you are talking about the write consistency timeout, are you talking about the `?timeout=1m` parameter when sending the indexing request? Or are you talking about a different timeout entirely?
</comment><comment author="rore" created="2015-04-11T19:11:33Z" id="91908186">Wow, this was opened a long time ago. 

Yes, it's the timeout parameter on the index API. 
I know you can pass it with the indexing requests, the point was to enable changing the default at the cluster level so you won't have to specify it on each request. 
1m timeout can be disastrous at some scenarios as described.
</comment><comment author="clintongormley" created="2016-11-06T11:22:08Z" id="258674595">@bleskes what do you think about this, esp with the changes around write consistency?
</comment><comment author="bleskes" created="2016-11-06T22:23:37Z" id="258715752">This is implemented in 5.0 with #19454 where an index settings  `index.write.wait_for_active_shards` was added. Also note that the original issue is now solved because our default is to only wait for the primary. 
</comment><comment author="bleskes" created="2016-11-07T13:05:43Z" id="258830379">It seems I misread and the request was for changing the timeout, not the actual wait criteria. I'm reopening the issue. Sorry for the noise.

PS I think this should be an index level setting rather than a cluster one
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consistent REST Get Field Mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4738</link><project id="" key="" /><description>As of #4071, this commit https://github.com/elasticsearch/elasticsearch/commit/a3abcdc93a004bea53ba2d4cbab585f8820b660d prepared the Get Field Mapping API. However, if index and type exist, but the field does not, it should return an empty JSON response instead of a 404 in order to be consistent.

Right now, you cannot now, if the index, type or field was the one not being found in in the rest action
</description><key id="25634245">4738</key><summary>Consistent REST Get Field Mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-15T09:13:53Z</created><updated>2014-01-30T10:06:00Z</updated><resolved>2014-01-28T07:39:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java</file></files><comments><comment>REST Get Field Mapping API: Fix NPE if field not existent</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java</file></files><comments><comment>REST API: Consistent get field mapping response</comment></comments></commit></commits></item><item><title>update rest spec to be consistent with recent changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4737</link><project id="" key="" /><description>see issue #4071
</description><key id="25614892">4737</key><summary>update rest spec to be consistent with recent changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-01-14T23:34:20Z</created><updated>2014-07-08T03:05:48Z</updated><resolved>2014-01-14T23:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-01-14T23:57:46Z" id="32321486">@spenceralger thanks! 

pushed to master (2f115b8)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>how to change index: analyzer on existing type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4736</link><project id="" key="" /><description>I am using logstash with the collectd input, it's indexing data with a default host: type as follows.

```
#"host" : {
#        "type" : "multi_field",
#        "fields" : {
#          "host" : {
#            "type" : "string",
#            "omit_norms" : true
#          },
#          "raw" : {
#            "type" : "string",
#            "index" : "not_analyzed",
#            "omit_norms" : true,
#            "index_options" : "docs",
#            "include_in_all" : false,
#            "ignore_above" : 256
#          }
#        }
#      },
#


I'd like to change the host: to include index:not_analyzed
I have tried this multiple ways and just dont seem to be able to figure it out.

Here's a basic example of the curl command

curl -XPOST "http://localhost:9200/logstash-2014.01.14/logs/_mapping" -d '
{
 "properties":{
  "host":{
   "type":{
     "host":{
      "index":"not_analyzed"
     }
   }
  }
 }
}
'
```
</description><key id="25611258">4736</key><summary>how to change index: analyzer on existing type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zbuckholz</reporter><labels /><created>2014-01-14T22:31:52Z</created><updated>2014-02-22T16:28:19Z</updated><resolved>2014-02-22T16:28:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-22T16:28:19Z" id="35806765">please ask questions like this on the logstash or elasticsearch mailinglist or irc channels, we try to use github for issues only.

Minor help: `host.raw` already looks like an not analyzed field, so you might just want to use that field for further things.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Routing to a group of shards (nodes) ? </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4735</link><project id="" key="" /><description>Is it possible to index/search documents within a group of shards ? 
While indexing, if I provide the routing parameter, the document should be indexed on a specific shard, but on which one? what happens if we continue to index with the same routing param? the shard is always getting bigger? 
Is it likely to configure routing for a group of shards and split the data on multiple selective shards? or nodes ?
</description><key id="25610359">4735</key><summary>Routing to a group of shards (nodes) ? </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bagdemir</reporter><labels /><created>2014-01-14T22:17:39Z</created><updated>2014-01-15T10:28:34Z</updated><resolved>2014-01-15T10:28:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-15T10:28:34Z" id="32349827">Hey, can you please use the google group/mailing list at https://groups.google.com/forum/#!forum/elasticsearch for this kind of questions, as we try to use github issues for bugs only.

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Consistent APIs: Get field mapping API includes 'mappings'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4734</link><project id="" key="" /><description>The get field mapping API now includes a mappings element after the index in its JSON

Added more consistent endpoint /{index}/_mapping/{type}/field/{fields}
and added endpoint /_mapping/{type}/field/{fields}
which are also used in tests

Added rest spec tests for wildcards and _all

Relates #4071

NOTE: This is not yet complete for 1.0. We need to return an empty JSON document instead
of a 404 if the field of an existing index and type is not found. However this is not
possible with the current data structure being returned. Needs to be finished for 1.0.
</description><key id="25609076">4734</key><summary>Consistent APIs: Get field mapping API includes 'mappings'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-01-14T21:57:53Z</created><updated>2014-07-08T01:17:18Z</updated><resolved>2014-01-14T22:03:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-14T21:59:26Z" id="32312117">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Previously, the cluster state before flushing was used to check which</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4733</link><project id="" key="" /><description>types map the given types pattern. However, this state might not be
up to date. Instead use the more recent cluster state from clusterState.state()
This fixes a test failure of PercolatorTests.testDeletePercolatorType

Other changes:
- use BoolFilter instead of OrFilter, because it is faster
- throw exception immediately when no type matching the given patterns
  was found in cluster state
</description><key id="25607605">4733</key><summary>Previously, the cluster state before flushing was used to check which</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-01-14T21:37:41Z</created><updated>2014-07-08T03:19:05Z</updated><resolved>2014-01-14T22:06:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-14T21:44:00Z" id="32310663">LGTM +1 to push
</comment><comment author="martijnvg" created="2014-01-14T21:45:59Z" id="32310840">+1 looks good to me as well!
</comment><comment author="brwe" created="2014-01-14T22:06:34Z" id="32312750">pushed 6389432 to master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add publish port</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4732</link><project id="" key="" /><description>```
Add transport.publish_port setting to allow users to specify the port
other cluster members should use when connecting to an instance. This
is needed for systems such as OpenShift, where cluster communication
needs to use a publicly accessibly proxy port, because the normal port
(9300) is bound to a private loopback IP address.

This commit is a backport from https://github.com/elasticsearch/elasticsearch/pull/4359
```
</description><key id="25606934">4732</key><summary>Add publish port</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">caruccio</reporter><labels /><created>2014-01-14T21:27:49Z</created><updated>2014-07-16T21:49:40Z</updated><resolved>2014-01-18T16:47:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="caruccio" created="2014-01-18T16:47:38Z" id="32686309">Already done by #4359.
Thanks guys!
</comment><comment author="s1monw" created="2014-01-18T20:25:24Z" id="32692069">ah sry @caruccio It was easier to just cherry-pick it. thanks for opening this!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove redundant version checks in transport serialisation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4731</link><project id="" key="" /><description>Since 1.0 breaks transport serialisation support with 0.90.x, it doesn't make sense to do version checks in the serialisation code. This will clean up the code (writeTo() and readFrom()) and allows ES to start fresh.
</description><key id="25605021">4731</key><summary>Remove redundant version checks in transport serialisation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-14T21:01:27Z</created><updated>2015-06-07T16:18:37Z</updated><resolved>2014-01-24T14:51:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-14T21:18:20Z" id="32308395">this looks great - we can pull this in post RC1
</comment><comment author="javanna" created="2014-01-24T11:25:56Z" id="33215531">I think we should be able to remove the read/writeTimeout methods in `AcknowledgedRequest` as well? If you want I can look into this!
</comment><comment author="martijnvg" created="2014-01-24T12:43:14Z" id="33219684">@javanna Make sense, I'll remove these methods.
</comment><comment author="martijnvg" created="2014-01-24T13:43:43Z" id="33223245">Updated PR to implement Luca's feedback.
</comment><comment author="javanna" created="2014-01-24T13:59:39Z" id="33224221">I left a small comment, but looks great @martijnvg , thanks!
</comment><comment author="martijnvg" created="2014-01-24T14:51:27Z" id="33228296">@javanna Thanks for the feedback.

Pushed to master, 1.x and 1.0 branches.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedResponse.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file></files><comments><comment>Removed redundant version checks in transport serialization (#4731 leftover)</comment></comments></commit></commits></item><item><title>Spelling corrections in aggregations docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4730</link><project id="" key="" /><description /><key id="25604662">4730</key><summary>Spelling corrections in aggregations docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awentzonline</reporter><labels /><created>2014-01-14T20:55:52Z</created><updated>2014-07-16T21:49:41Z</updated><resolved>2014-02-06T17:31:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-06T17:31:33Z" id="34348215">Thanks for the PR! This has been fixed in the meantime, closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove the `path` setting in objects </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4729</link><project id="" key="" /><description>Once #4520 is completed, there is no need for users to be able to set the `path` setting in object and multi-field mappings.

This should be removed.

(We discussed adding an `index_path` parameter to allow setting the absolute name for a field's inverted index, but I don't think this is needed any more.)
</description><key id="25604590">4729</key><summary>Remove the `path` setting in objects </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>breaking</label></labels><created>2014-01-14T20:54:30Z</created><updated>2015-06-06T16:00:09Z</updated><resolved>2014-11-27T11:40:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-02-05T13:35:30Z" id="34167970">Should we remove the `index_name` setting as well? Is there any use case for this now that we have `copy_to` fields?
</comment><comment author="clintongormley" created="2014-11-27T11:40:53Z" id="64780021">Closing in favour of #6677
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Deprecated the path setting in favour of copy_to</comment></comments></commit></commits></item><item><title>Add the ability to retrieve fields from field data using `fielddata_fields`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4728</link><project id="" key="" /><description>Adds a new FetchSubPhase, FieldDataFieldsFetchSubPhase, which loads the
field data cache for a field and returns an array of values for the
field.

Also removes a `doc['&lt;field&gt;']` workaround no longer needed in field
name resolving.

Closes #4492
</description><key id="25598546">4728</key><summary>Add the ability to retrieve fields from field data using `fielddata_fields`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2014-01-14T19:29:17Z</created><updated>2014-06-20T14:44:15Z</updated><resolved>2014-01-21T16:24:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-21T14:24:40Z" id="32888877">LGTM
</comment><comment author="dakrone" created="2014-01-21T16:13:34Z" id="32900141">Merged to 1.x and master (2.0)
</comment><comment author="s1monw" created="2014-01-21T19:53:42Z" id="32956325">I think we should merge this into `1.0` as well WDYT?
</comment><comment author="kimchy" created="2014-01-21T20:05:26Z" id="32957525">++ on 1.0
</comment><comment author="s1monw" created="2014-01-21T20:05:44Z" id="32957559">let's do it then...
</comment><comment author="dakrone" created="2014-01-21T20:11:20Z" id="32958097">Pushed to 1.0 in b3060afa also.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>PluginService loads Eclipse's plugins jar into classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4727</link><project id="" key="" /><description>Hi, I've tried to configure an ES client node on a REST project based on Spring but I'm always getting the following error when I try to query any endpoint:

```
java.lang.LinkageError: loader constraint violation: when resolving method "com.sun.jersey.spi.container.servlet.ServletContainer.getServletConfig()Ljavax/servlet/ServletConfig;" the class loader (instance of org/apache/catalina/loader/WebappClassLoader) of the current class, com/sun/jersey/spi/container/servlet/WebServletConfig, and the class loader (instance of org/apache/catalina/loader/StandardClassLoader) for resolved class, com/sun/jersey/spi/container/servlet/ServletContainer, have different Class objects for the type ainer.getServletConfig()Ljavax/servlet/ServletConfig; used in the signature
```

It took me so long to find the root cause of this, and it was that org.elasticsearch.plugins.PluginsService.loadPluginsIntoClassLoader() is loading Eclipse's pluings jar (in this case Groovy's pluing) into the Tomcat's context classloader, causing the LinkageError.

I only need the ES dependency on my project for use it as a client, is there any configuration to avoid this?
</description><key id="25597716">4727</key><summary>PluginService loads Eclipse's plugins jar into classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">gmazzo</reporter><labels /><created>2014-01-14T19:17:11Z</created><updated>2014-04-25T19:43:05Z</updated><resolved>2014-04-25T19:43:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-22T16:25:26Z" id="35806674">Hey

can you elaborate why the groovy plugin is loaded when elasticsearch tries to load plugins, this sounds strange. Can you elaborate about your setup?
</comment><comment author="gmazzo" created="2014-02-23T05:02:22Z" id="35824072">Sorry, I've forget to update. The issue was the PluginManager, which it was loading my Eclipse's classpath (that's why I've got the Groovy's pluing in). I've solved it by specifing a fake plugin's directory on the configuration.
</comment><comment author="spinscale" created="2014-04-25T19:43:05Z" id="41431930">I'll close this, if you can spot something, where we can improve to create a better default behaviour for eclipse or add some documentation, please reopen or maybe even create a PR! Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Pretend there aren't query norms even when they exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4726</link><project id="" key="" /><description>It'd be nice to be able to pretend that query norms don't exist in some fields even though they do.  Rather, it'd make investigating whether or not they are appropriate for certain fields easier because you could compare the results side by side without reindexing.  Also it'd help if you had a class of queries for which they don't make sense and a class for which they do then you could disable them in the first class.

I have no idea if this is possible but it'd be useful.
</description><key id="25588515">4726</key><summary>Pretend there aren't query norms even when they exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2014-01-14T17:04:09Z</created><updated>2014-07-25T09:43:01Z</updated><resolved>2014-07-25T09:43:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-14T17:14:37Z" id="32285251">I guess we could switch something off / on via similarity or so but that is a per-field option and valid for all sub queries that use that field. It can certainly be a switch per query though
</comment><comment author="rmuir" created="2014-07-04T11:09:43Z" id="48031878">Note that query-norm isnt a per-field thing in lucene. Also there is a proposal to remove it...
</comment><comment author="clintongormley" created="2014-07-25T09:43:01Z" id="50128688">Lucene are discussing removing query norms.  Elasticsearch will follow what Lucene does here.  
See https://issues.apache.org/jira/browse/LUCENE-5712
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Trouble running ElasticSearch in java test with numeric facets and multiple shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4725</link><project id="" key="" /><description>Hi,
We seem to have stumbled on some issue while running queries against a local elasticsearch node under a JUnit test.

We've been able to create a small test case which demonstrates this.

The necessary prerequisites for the failure:
- 2 or more shards
- Any of the DFS search types
- A terms facet on a field with a "long" type in the mapping

We have tested it with elasticsearch 0.90.x, and different versions of java.

It doesn't always fail, e.g. we have one machine with an old java version (1.7.0_07) where it works in IntelliJ, but not in maven. 

When we run the test against the master-branch the test works, but not on the 0.90-branch.

```
public class MyTest {
    private Client client;

    @Before
    public void createNode() throws Exception {
        Settings nodeSettings = ImmutableSettings.settingsBuilder()
                .put("node.local", false)
                .put("path.data", "./target/elasticsearch-test/data")
                .put("path.work", "./target/elasticsearch-test/work")
                .put("path.logs", "./target/elasticsearch-test/logs")
                .put("index.number_of_shards", "2")
                .build();

        Node node = NodeBuilder.nodeBuilder().settings(nodeSettings).node();
        client = node.client();
    }

    @After
    public void cleanup() throws Exception {
        FileSystemUtils.deleteRecursively(new File("./target/elasticsearch-test/"), true);
    }

    @Test
    public void demonstrateFailure() throws Exception {

        IndexResponse indexResponse = client
                .prepareIndex("test-index", "test-type")
                .setSource("{ \"id\": 123, \"test-value\": 321 }")
                .setRefresh(true)
                .execute()
                .actionGet();

        SearchResponse searchResponse = client
                .prepareSearch("test-index")
                .setQuery(new MatchAllQueryBuilder())
                .addFacet(new TermsFacetBuilder("test-facet").field("test-value"))
                .setSearchType(SearchType.DFS_QUERY_AND_FETCH)
                .execute()
                .actionGet();

        System.out.println(searchResponse);

        assertEquals(searchResponse.getFailedShards(), 0); // Failing!
    }
}
```

Output from a failed run:

```
{
  "took" : 68,
  "timed_out" : false,
  "_shards" : {
    "total" : 2,
    "successful" : 0,
    "failed" : 2,
    "failures" : [ {
      "index" : "test-index",
      "shard" : 0,
      "status" : 500,
      "reason" : "ElasticSearchException; nested: AssertionError; "
    }, {
      "index" : "test-index",
      "shard" : 1,
      "status" : 500,
      "reason" : "ElasticSearchException; nested: AssertionError; "
    } ]
  },
  "hits" : {
    "total" : 0,
    "max_score" : 0.0,
    "hits" : [ ]
  }
}
```
</description><key id="25579581">4725</key><summary>Trouble running ElasticSearch in java test with numeric facets and multiple shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">henrikno</reporter><labels /><created>2014-01-14T15:07:50Z</created><updated>2014-01-21T13:29:35Z</updated><resolved>2014-01-14T15:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-14T15:30:35Z" id="32274040">can you please use the mailing list for issues like this. We also have a test framework that might help

http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/testing-framework.html
</comment><comment author="henrikno" created="2014-01-21T13:29:35Z" id="32884690">For reference, this resulted in: #4754
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster routing allocation exclude by node name as well as ip?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4724</link><project id="" key="" /><description>Currently exclusion for routing occurs by IP.

What if I wish to decommission only one node on a box and each box has more than one node?

Can exclusion by node name by added?

```
curl -XPUT localhost:9200/_cluster/settings -d '{
"transient" : {
    "cluster.routing.allocation.exclude._node_name" : "my_node_1"
   }
}
```
</description><key id="25579081">4724</key><summary>Cluster routing allocation exclude by node name as well as ip?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">meconlin</reporter><labels><label>v0.90.11</label><label>v1.0.0.RC1</label></labels><created>2014-01-14T15:00:08Z</created><updated>2014-01-14T21:32:51Z</updated><resolved>2014-01-14T21:31:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-14T21:31:47Z" id="32309576">Node name (`_name`) and node id (`_id`) are supported but were not documented. Added documentation for them in 5eeb702.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `omit_term_freq_and_positions` for new indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4723</link><project id="" key="" /><description>`omit_term_freq_and_positions` was deprecated in `0.20` and
is not documented anymore. We should reject indices that are
created with this option in the future.

Closes #4722
</description><key id="25571090">4723</key><summary>Remove `omit_term_freq_and_positions` for new indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-14T12:41:45Z</created><updated>2014-06-17T16:36:29Z</updated><resolved>2014-01-17T14:03:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-14T14:19:48Z" id="32267716">+1
</comment><comment author="s1monw" created="2014-01-17T10:11:19Z" id="32593624">@jpountz I rebased the branch and added some more asserts can you take a quick look again? I also moved he `onAfterVersion` check to 1.0 instead of RC1
</comment><comment author="s1monw" created="2014-01-17T14:03:13Z" id="32607802">@jpountz  I reverted that file since it's not needed and push to master!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Deprecated &amp; remove 'omit_term_freq_and_positions' </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4722</link><project id="" key="" /><description>We deprecated `omit_term_freq_and_positions` in `0.20` already and I think for GA we should drop the support entirely for newly created indices.
</description><key id="25570856">4722</key><summary>Deprecated &amp; remove 'omit_term_freq_and_positions' </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v1.0.0</label></labels><created>2014-01-14T12:36:34Z</created><updated>2014-01-17T14:02:33Z</updated><resolved>2014-01-17T14:02:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/Mapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/test/java/org/elasticsearch/count/query/SimpleQueryTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Remove `omit_term_freq_and_positions` for new indices</comment></comments></commit></commits></item><item><title>RPMs: Add timeout to shutdown with KILL signal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4721</link><project id="" key="" /><description>If the thread pools of an elasticsearch node cannot be shutdown
immediately, a wait of 10 seconds is added. This clashes with the
RPM scripts, as by default the init functions wait for 3 seconds
for a service to shutdown before a KILL signal is sent, resulting
in an unclean shutdown - not from an elasticsearch point of view,
but from init system point of view, as some lock files are left
around.

In order to prevent this the init script as well as the systemd
configuration now feature the same timeout than the debian package,
which is 20 seconds.

The await statement, which causes the 10 second delay can be found in
InternalNode.close()

Closes #5020
</description><key id="25562480">4721</key><summary>RPMs: Add timeout to shutdown with KILL signal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>bug</label><label>v0.90.11</label><label>v1.0.0</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-14T09:42:27Z</created><updated>2015-06-07T23:37:43Z</updated><resolved>2014-01-16T15:15:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-16T14:24:23Z" id="32472608">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix disk percent used calc in cat/allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4720</link><project id="" key="" /><description>Closes #4670.
</description><key id="25550161">4720</key><summary>Fix disk percent used calc in cat/allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-14T03:01:29Z</created><updated>2014-07-08T03:17:39Z</updated><resolved>2014-01-14T21:35:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-14T16:57:14Z" id="32283320">LGTM, +1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can't modify mapping if it has a parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4719</link><project id="" key="" /><description>If a type mapping with a parent is already exist, it can't be updated even if we are not modify the parent.

Seems related to https://github.com/elasticsearch/elasticsearch/issues/3849

I think that issue should be not allow to update the parent field in the mapping, however it will throw this exception even if we are updating other fields for mapping with parent

``` sh
org.elasticsearch.index.mapper.MergeMappingException: Merge failed with failures {[The _parent field can't be added or updated]}" 
```

Example:

``` sh

# create mapping for tweet
curl -XPUT localhost:9200/test/tweet/_mapping -d '{
    "tweet" : {
        "properties" : {
            "message" : {"type" : "string", "store" : "yes"}
        }
    }
}'

# create mapping for tweet2 with parent tweet
curl -XPUT localhost:9200/test/tweet2/_mapping -d '{
    "tweet2": {
        "_parent": {"type": "tweet"},
        "properties": {
            "message": {"type": "string", "store": "yes"}
        }
    }
}'

# add a new property to tweet2 mapping, it's not modifying the parent but still got exception
curl -XPUT localhost:9200/test/tweet2/_mapping -d '{
    "tweet2" : {
        "properties" : {
            "message2" : {"type" : "string", "store" : "yes"}
        }
    }
}'
```
</description><key id="25549051">4719</key><summary>Can't modify mapping if it has a parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kzwang</reporter><labels /><created>2014-01-14T02:27:29Z</created><updated>2014-02-01T03:20:18Z</updated><resolved>2014-02-01T03:20:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kzwang" created="2014-02-01T03:20:18Z" id="33862226">If the updated mapping has a "_parent" with same value it can be successfully merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>generate timestamp when path is null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4718</link><project id="" key="" /><description>Shouldn't timestamp be generated when value of path is null ?

Mapping definition:

``` bash
curl -X PUT  http://localhost:9200/twitter/ -d '{
    "mappings": {
        "_default_": {
            "_timestamp" : {
                "enabled" : "yes",
                "store": "yes",
                "path" : "post_date"
            },
            "properties": {
                "message": {
                    "type": "string"
                }
            }
        }
    }
}'
```

Get error when:

``` bash
curl -X PUT http://127.0.0.1:9200/twitter/tweet/123 -d '{
  message: "bam bam"
}'

=&gt;  {"error":"ElasticSearchParseException[failed to parse doc to extract routing/timestamp]; nested: TimestampParsingException[failed to parse timestamp [null]]; ","status":400}

curl -X PUT http://127.0.0.1:9200/twitter/tweet/123 -d '{
  message: "bam bam",
  post_date: "2009-11-15T14:12:12Z"
}'

=&gt; {"ok":true,"_index":"twitter","_type":"tweet","_id":"123","_version":1}
```
</description><key id="25548130">4718</key><summary>generate timestamp when path is null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">khoan</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-01-14T02:01:54Z</created><updated>2015-05-01T07:24:48Z</updated><resolved>2014-07-31T18:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T09:41:03Z" id="50128488">This should probably be an option that can be turned on, so that bad data isn't silently ignored.
</comment><comment author="dadoonet" created="2014-07-25T11:01:55Z" id="50135270">@clintongormley Where should we put that new option? Index settings? Mapping? Other?
Which name should we use: `ignore_missing_timestamp` or so?
</comment><comment author="dadoonet" created="2014-07-25T11:15:44Z" id="50136388">The more I think about it, the more I think we should define for `_timestamp` field a new option named `default`. `default` could be: `now` (by default) or a date which respect the `format` format or `null`.

I remember a use case on the mailing list where the user does not have a value for every document so I'd like to set it to `01/01/1970`.

The new `_timestamp` field would look like this:

```
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "path" : "post_date",
            "format" : "YYYY-MM-dd",
            "default" : "1970-01-01"
        }
    }
}
```

Or

```
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "path" : "post_date",
            "format" : "YYYY-MM-dd",
            "default" : "now"
        }
    }
}
```

Or 

```
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "path" : "post_date",
            "format" : "YYYY-MM-dd",
            "default" : null
        }
    }
}
```

WDYT?
</comment><comment author="clintongormley" created="2014-07-25T11:30:22Z" id="50137562">Sounds reasonable to me
</comment><comment author="dadoonet" created="2014-07-25T13:25:24Z" id="50148266">PR Opened #7036. 
</comment><comment author="alfasin" created="2015-05-01T00:21:25Z" id="98007223">Can you do:
"enabled" : "yes",   ???

I thought that the only valid values of "enabled"  are true/false...
</comment><comment author="dadoonet" created="2015-05-01T07:24:48Z" id="98064811">Actually "enabled":"whateveryouwant" might be considered as true. But you're right, true is definitely better! 
This boolean parsing might change in the future to be more strict.
More info http://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html#boolean
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MappingMetaDataParserTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/timestamp/TimestampMappingTests.java</file></files><comments><comment>Generate timestamp when path is null</comment></comments></commit></commits></item><item><title>excluding all fields of an object should not remove parent.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4717</link><project id="" key="" /><description>When excluding '*.f1' from `{ "obj": { "f1": 1, "f2": 2 } }` XContentMapValues.filter returns `{ "obj": { "f2": 2}}`. When run on `{ "obj": { "f1" : 1 }}` we should return `{ "obj": { }}` to maintain object structure. People currently need to always check whether `obj` is there or not.

Closes #4715
Closes #4047
Related to #4491
</description><key id="25547606">4717</key><summary>excluding all fields of an object should not remove parent.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-01-14T01:48:41Z</created><updated>2014-06-15T12:45:43Z</updated><resolved>2014-01-14T16:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-01-14T15:47:47Z" id="32275851">@imotov thx for the review. Made the change you suggested it reads clearer.
</comment><comment author="s1monw" created="2014-01-14T15:57:34Z" id="32276987">LGTM
</comment><comment author="bleskes" created="2014-01-14T16:06:20Z" id="32277950">Pushed to: f2710c16ebd918f646be9d0ab64b4871c25be4c2
Thx.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Shorten cat epoch time to second precision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4716</link><project id="" key="" /><description>Closes #4696.
</description><key id="25547569">4716</key><summary>Shorten cat epoch time to second precision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-14T01:47:32Z</created><updated>2014-06-16T23:14:54Z</updated><resolved>2014-01-14T21:34:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-14T16:58:40Z" id="32283455">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>`_source` filtering shouldn't remove objects unless explicitly excluded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4715</link><project id="" key="" /><description>We currently support `_source` filtering both on indexing time and read time using include/exclude terms. Currently, the logic is aggressively removing objects where all the fields has been removed, trying to make things as compact as possible. For example, consider the following document:

```
{
    obj: { f: 1 }
}
```

using `exclude=['obj.f']`, will return `{}` because the `obj` now becomes empty. However, the same `exclude=['obj.f']` will return `{ obj : { d : 1 }` if the original document is:

```
{
    obj: { f: 1, d :1 }
}
```

This causes client code interested in `obj.d` to need to always have to test whether `obj` is there before checking if `obj.d` exists. This can be cumbersome but also is not expected as the `obj.f` was excluded but not `obj`

This should change and only remove objects if they explicitly match an exclude (like `exclude=['obj']`), maintaining the object structure.
</description><key id="25547189">4715</key><summary>`_source` filtering shouldn't remove objects unless explicitly excluded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-14T01:37:26Z</created><updated>2014-08-15T12:39:50Z</updated><resolved>2014-01-14T16:04:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>excluding all fields of an object should not remove parent.</comment></comments></commit></commits></item><item><title>third party license extractor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4714</link><project id="" key="" /><description>extract dependency license information.

the script currently is limited to extract information out of logstash project.

example of the output
# 
# i18n,0.6.9,MIT
# multi_json,1.8.2,MIT
# activesupport,3.2.16,MIT
# addressable,2.3.5,Apache License 2.0
# atomic,1.1.14,Apache-2.0

extlib,0.9.16
Copyright (c) 2010 Dan Kubb

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

---

---

Some portions of blank.rb and mash.rb are verbatim copies of software
licensed under the MIT license. That license is included below:

Copyright (c) 2005-2008 David Heinemeier Hansson

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
</description><key id="25540444">4714</key><summary>third party license extractor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-01-14T00:11:00Z</created><updated>2014-07-08T03:11:54Z</updated><resolved>2014-01-16T17:38:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #4714 from mrsolo/features/thirdparty</comment></comments></commit></commits></item><item><title>Remove `ElasticsearchInterruptedException` and handle interrupt state correctly.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4713</link><project id="" key="" /><description>InterruptedExceptions should be handled by either rethrowing or
restoring the interrupt state (i.e. calling
`Thread.currentThread().interrupt()`). This is important since the
caller of the is method or subequent method calls might also be
interested in this exception. If we ignore the interrupt state the
caller might be left unaware of the exception and blocks again on
a subsequent method.

Closes #4712
</description><key id="25527694">4713</key><summary>Remove `ElasticsearchInterruptedException` and handle interrupt state correctly.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-13T20:59:25Z</created><updated>2014-07-08T03:00:11Z</updated><resolved>2014-01-13T21:35:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-13T21:07:53Z" id="32211494">+1!
</comment><comment author="s1monw" created="2014-01-13T21:35:22Z" id="32213961">pushed thx for the review
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove `ElasticsearchInterruptedException` and handle interrupt state  correctly.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4712</link><project id="" key="" /><description>InterruptedExceptions should be handled by either rethrowing or restoring the interrupt state (i.e. calling `Thread.currentThread().interrupt()`). This is important since the caller of the is method or subequent method calls might also be interested in this exception. If we ignore the interrupt state the caller might be left unaware of the exception and blocks again on a subsequent method.
</description><key id="25527167">4712</key><summary>Remove `ElasticsearchInterruptedException` and handle interrupt state  correctly.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-13T20:51:18Z</created><updated>2014-01-13T21:34:49Z</updated><resolved>2014-01-13T21:34:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/ElasticsearchInterruptedException.java</file><file>src/main/java/org/elasticsearch/action/ActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsAbortPolicy.java</file><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/transport/PlainTransportFuture.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file></files><comments><comment>Remove `ElasticsearchInterruptedException` and handle interrupt state</comment><comment>correctly.</comment></comments></commit></commits></item><item><title>Add _cat/segments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4711</link><project id="" key="" /><description>`/_cat/segments?v`:

```
index   shard prirep ip        state   segment deleted_docs num_docs search size_in_bytes
twitter     0 p      127.0.0.1 STARTED _6y                0     2272 t              6.2mb
twitter     0 p      127.0.0.1 STARTED _6z                0       12 t               84kb
```

Other columns:
- committed (comm)
- compound (comp)
- generation (gen)
- memory_in_bytes (mem)
- version (ver)
</description><key id="25523373">4711</key><summary>Add _cat/segments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>:CAT API</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-13T19:54:17Z</created><updated>2015-06-06T18:44:04Z</updated><resolved>2014-03-20T03:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T16:23:39Z" id="32773822">pushed to `1.1`
</comment><comment author="colings86" created="2014-02-13T21:36:42Z" id="35028965">Having not really looked a lot into how to extend the ElasticSearch Rest API before I thought this would be a good introduction issue to tackle, especially as I can see this being very useful.  I have created a pull request for my solution.  It currently mirrors the _segments API information.  Let me know if there are any issues with the change, I would be happy to correct them
</comment><comment author="colings86" created="2014-02-13T21:43:47Z" id="35029670">Note:  I am sure some of the descriptions of the headers could be improved so suggestions on that are welcome
</comment><comment author="drewr" created="2014-02-14T20:16:10Z" id="35120546">Thanks Colin! Left comments inline.
</comment><comment author="colings86" created="2014-02-15T11:29:32Z" id="35153546">Thanks for the feedback.  I have made the changes you suggested in your comments (I should have checked the aliases before committing the first time). The isCommitted and isCompound aliases were still going to be the same so I made isCompound 'ico' instead.  I am now wondering is the num.committed and num.searchable actually belong here as they are more attributes of the shard rather than the segments?
</comment><comment author="colings86" created="2014-02-15T11:30:35Z" id="35153565">Updated the pull request with the changes
</comment><comment author="drewr" created="2014-03-18T14:41:29Z" id="37940203">I haven't forgotten about this! :smile_cat:  Hoping to get it into 1.1.0 in the next couple days.
</comment><comment author="colings86" created="2014-03-20T07:03:05Z" id="38139963">Thanks Drew, much appreciated :)
</comment><comment author="javanna" created="2014-04-03T09:33:01Z" id="39431703">This got merged right @drewr? I think we are missing some docs for it though, then we could also close #5118.
</comment><comment author="drewr" created="2014-04-07T17:43:03Z" id="39760289">@javanna That and a rest test! :8ball: 
</comment><comment author="javanna" created="2014-04-07T18:06:51Z" id="39763206">@drewr ++++++ ;)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestSegmentsAction.java</file></files><comments><comment>Remove shard-level info from _cat/segments and add pri/rep and ip address to segments</comment></comments></commit></commits></item><item><title>Consistent apis: GET part</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4710</link><project id="" key="" /><description>In order to make some our APIs consistent, ticket #4071 has been created.

This PR (not yet rebase, will do after review) takes care of streamlining the GET APIs
</description><key id="25519515">4710</key><summary>Consistent apis: GET part</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-01-13T18:59:56Z</created><updated>2014-07-07T16:43:12Z</updated><resolved>2014-01-14T21:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-13T23:45:52Z" id="32224999">I think this is close - @clintongormley can you look at it please!
</comment><comment author="dakrone" created="2014-01-14T21:32:17Z" id="32309627">This looks good to me, +1
</comment><comment author="s1monw" created="2014-01-14T21:32:18Z" id="32309628">left some minor comments
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Specify an empty stopwords list with an empty array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4709</link><project id="" key="" /><description>To set the list of stopwords to the empty list, currently you have to do:

```
{ "stopwords": ["_none"] }
```

It should be possible to just write:

```
{ "stopwords": [] }
```
</description><key id="25517004">4709</key><summary>Specify an empty stopwords list with an empty array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/medcl/following{/other_user}', u'events_url': u'https://api.github.com/users/medcl/events{/privacy}', u'organizations_url': u'https://api.github.com/users/medcl/orgs', u'url': u'https://api.github.com/users/medcl', u'gists_url': u'https://api.github.com/users/medcl/gists{/gist_id}', u'html_url': u'https://github.com/medcl', u'subscriptions_url': u'https://api.github.com/users/medcl/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/64487?v=4', u'repos_url': u'https://api.github.com/users/medcl/repos', u'received_events_url': u'https://api.github.com/users/medcl/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/medcl/starred{/owner}{/repo}', u'site_admin': False, u'login': u'medcl', u'type': u'User', u'id': 64487, u'followers_url': u'https://api.github.com/users/medcl/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-01-13T18:26:09Z</created><updated>2016-11-06T11:19:15Z</updated><resolved>2016-11-06T11:19:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T13:44:18Z" id="148053741">The same goes for `stem_exclusion` in the language analyzers
</comment><comment author="clintongormley" created="2016-11-06T11:19:15Z" id="258674474">This doesn't seem to be an issue so closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Tribe Node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4708</link><project id="" key="" /><description>Note, this feature is experimental.

The tribes feature allowed to create a tribe node that can act as a federated client across multiple clusters.

The tribe node configuration looks something like this:

```
tribe.t1.cluster.name: cluster1
tribe.t2.cluster.name: cluster2
```

The configuration above configure connections to 2 clusters, named `t1`, `t2`. It creates a "node" client to each (so by default, above, multicast discovery is used). The settings for each node client is extracted from the `tribe.[tribe_name]` prefix.

The way the tribe node works is by merging the cluster state from each cluster, and creating a merged view of all clusters. This means all operations work the same, distributed search, suggest, percolation, indexing, ... .

The merged view drops conflicted indices and picks one of them if there are 2 indices with the same name across multiple clusters.

By default, read and write operations are allowed. Master level read operations (cluster state for example), require setting the local flag to true (since there is no elected master). Master level write operations are not allowed (create index, ...).

The tribe node can be configured to block write operations `tribe.blocks.write` to `true`, and metadata operations by setting `tribe.blocks.metadata` to `true`.
</description><key id="25514828">4708</key><summary>Tribe Node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v1.0.0.RC1</label></labels><created>2014-01-13T17:51:31Z</created><updated>2017-01-24T06:26:28Z</updated><resolved>2014-01-13T19:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gseng" created="2014-01-13T23:34:59Z" id="32224266">Just to confirm, is this how it's supposed to work?

## Steps
- We have 2 clusters, `boston_cluster` and `seattle_cluster`.
- We leave their setting as is.
- We create a new tribe node with the following setting:

```
tribe.t1.cluster.name: boston_cluster
tribe.t2.cluster.name: seattle_cluster
```
- When the tribe node starts, it will connect to both clusters and aggregate their information.
- We can query this tribe node to retrieve results across both clusters.
  - For example, if `boston_cluster` had an index `docs-bos` and `seattle_cluster` had an index `docs-se`, a `&lt;tribe_node&gt;/_all/_search` would return documents from both indices.

## Questions

I tried the above steps but wasn't able to get the aggregated results. I think I'm missing something.
- What cluster should the tribe node have? It joins the `elasticsearch` by default.
- Are the `node.master` and `node.data` settings important for the tribe node?
</comment><comment author="kimchy" created="2014-01-13T23:38:05Z" id="32224462">yea, this is how ti will work. You need to make sure that those internal clients opened against boston and seattle can actually connect. Make sure they were started each with the respective cluster name, and by default, it will do multicast discovery, so potentially you need to configure unicast discovery? Something like:

```
tribe.t1.cluster.name: boston_cluster
tribe.t1.discovery.zen.ping.unicast.hosts: ["host1", "host2"]

tribe.t2.cluster.name: seattle_cluster
tribe.t2.discovery.zen.ping.unicast.hosts: ["host3", "host4"]
```

UPDATE: to answer your other questions, the cluster name or the nodes.master, node.data are not important for the tribe node.
</comment><comment author="jensihnow" created="2014-02-10T15:24:45Z" id="34643601">Hi,
what about Tribe Nodes on AWS?

How to configure three cluster in different regions (US,EU,AP) for tribe nodes?
In this scenario we would like to use the AWS EC2 discovery, but it looks like this doesn't work with Tribe nodes.

Please enhance  the tribe nodes docs. We know it's brand new, but it's too less detailed. 

Thanks
</comment><comment author="Kamapcuc" created="2014-12-23T18:36:25Z" id="67983565">Is it possible to have the same data in all clusters?
For example: 
We have the some servers in data centers in several countries to decrease ping and latency for clients. For some reasons, sometimes there is no connection between those data centers. So we want tribe to merge data in clusters after connection restore.
</comment><comment author="rparkhunovsky" created="2015-02-12T16:27:11Z" id="74101371">Would you please update the docs with a wider life-applicable examples? What is there now is quite unclear and non-obvious how to set the tribe node in AWS at all. Thank you.
</comment><comment author="NehaSood" created="2016-01-14T04:06:59Z" id="171524232">Do tribe nodes support AWS EC2 discovery? I wouldn't want to hard-code IP addresses to support unicast discovery.
</comment><comment author="jianchen2580" created="2016-11-28T02:35:48Z" id="263170501">Do tribe nodes support AWS EC2 discovery? +1 </comment><comment author="bleskes" created="2016-11-28T21:57:52Z" id="263407560">&gt; Do tribe nodes support AWS EC2 discovery?

They should yes.</comment><comment author="dshweta" created="2016-12-07T06:16:08Z" id="265365310">I am still unable to find any documentation/instructions for how to set the tribe node in AWS. We are trying to have unified   kibana dashboard for multiple clusters in different regions in AWS. Any pointers/input would be helpful</comment><comment author="jianchen2580" created="2016-12-07T06:39:33Z" id="265368556">@dshweta, which discovery in your clusters, EC2 discovery or Zen discovery? </comment><comment author="dshweta" created="2016-12-07T07:22:03Z" id="265375048">@jianchen2580, I am new to AWS and not sure about it. The AWS Elastic Search clusters are created using the AWS console in different regions. Tried to find the cluster discovery but no luck. </comment><comment author="jianchen2580" created="2016-12-07T08:02:20Z" id="265381503">@dshweta - 
For Zen discovery - 
make sure your tribe node can reach clusters (9200/tcp, 9300-9310/tcp)

```
tribe:
  eu:
    cluster.name: eu-cluster
    discovery.zen.ping.unicast.hosts: ["tribe-node-ip", "eu-cluster-node1-ip", "eu-cluster-node2-ip", ...]
    network.bind_host: 0.0.0.0
    network.publish_host: tribe_ip
  us:
    cluster.name: us-cluster
    discovery.zen.ping.unicast.hosts: ["tribe-node-ip", "us-cluster-node1-ip", "us-cluster-node2-ip", ...]
    network.bind_host: 0.0.0.0
    network.publish_host: tribe_ip</comment><comment author="bleskes" created="2016-12-07T17:38:49Z" id="265516217">@dshweta  AWS ES as a service doesn't support transport level access to the cluster (see [here](http://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/aes-limits.html) ) and thus also doesn't support tribe nodes. This is different for Elasticsearch that is hosted on your own instances on EC2.</comment><comment author="dshweta" created="2016-12-07T19:16:58Z" id="265543963">@bleskes Thanks for your response. </comment><comment author="lucyl" created="2017-01-08T05:50:57Z" id="271131839">Does tribe node use client clusters' coordinating nodes, or it acts as a coordinating node itself?</comment><comment author="amazinganshul" created="2017-01-23T19:51:04Z" id="274597380">Hi,
What are the scaling concerns related to a tribe node? We have 2 nodes added to the tribe node environment but I don't see them connecting to each. They independently connect to the federated clusters but not to themselves. How will they sync the state among themselves?</comment><comment author="jianchen2580" created="2017-01-24T05:50:05Z" id="274716084">You only need one tribe node, and this tribe node will join two clusters and become the norm node of the cluster.</comment><comment author="amazinganshul" created="2017-01-24T06:26:28Z" id="274720672">Hi, won't one tribe node become bottle neck? We have a use case where this tribe node will be used both for reads and writes. Also our write tps will be around 1500 writes per second for both clusters via tribe nodes and read tps of around 200 tps from kibana via tribe node.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Added docs for tribe node</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/ClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoveryService.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/main/java/org/elasticsearch/tribe/TribeModule.java</file><file>src/main/java/org/elasticsearch/tribe/TribeService.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/TestCluster.java</file><file>src/test/java/org/elasticsearch/tribe/TribeTests.java</file></files><comments><comment>Tribe Node</comment><comment>The tribes feature allowed to create a tribe node that can act as a federated client across multiple clusters.</comment></comments></commit></commits></item><item><title>simple_query_string `-` operator not working with default_operator `OR`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4707</link><project id="" key="" /><description>I tried to negates a keyword at query but it already comes in result

simple query is 
"query": {
            "simple_query_string": {
               "query": "\"This repository\" -removed",
               "fields": [
                  "content",
                  "headline"
               ]
            }
         },
</description><key id="25504610">4707</key><summary>simple_query_string `-` operator not working with default_operator `OR`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ksaritek</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2014-01-13T15:32:52Z</created><updated>2017-01-10T19:53:01Z</updated><resolved>2017-01-10T19:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-01-13T21:41:42Z" id="32214671">Could you provide a complete reproduction that would help us to reproduce the issue? See http://www.elasticsearch.org/help/ for an example.
</comment><comment author="clintongormley" created="2014-12-24T18:14:50Z" id="68067696">Recreation:

```
PUT /t/t/1
{
  "content": "This repository has been removed"
}


GET /t/_search
{
  "query": {
    "simple_query_string": {
      "query": "+this -removed",
      "fields": [
        "content",
        "headline"
      ]
    }
  }
}
```

The negated term is ignored, because with `default_operator: "OR"`, it is optional.
</comment><comment author="dakrone" created="2014-12-24T18:59:57Z" id="68069676">@clintongormley not sure how this is a bug though? This is like saying "contains the term **this** OR does not contain **removed**", which the document does (it contains the term "this")
</comment><comment author="clintongormley" created="2014-12-29T10:19:50Z" id="68246019">I think that a user would expect `foo bar -baz` (with default operator `OR`) to match as if they had written: `+(foo bar) -baz`.  Otherwise, a negated clause with default operator `OR` is meaningless.
</comment><comment author="dakrone" created="2015-02-23T23:20:12Z" id="75659921">@clintongormley I'm still not sure that this is actually a bug, the negated clause with the default `OR` operation is not meaningless (it has meaning, it just means" anything that does not contain this"). I think what is desired is setting `default_operator` to `AND`, which is a user-changeable setting?
</comment><comment author="ppf2" created="2015-08-19T04:55:28Z" id="132445550">@dakrone @clintongormley Here is another [use case](https://gist.github.com/ppf2/e56c34151113115c1266) from the field.  Is this a bug or not currently supported (a feature)?  Don't think default_operator:AND will help here since the use case is to return both documents in the example. thx
</comment><comment author="clintongormley" created="2015-08-25T08:25:49Z" id="134521519">@ppf2 Actually, your example works as expected if you use the `simple_query_string` (you used the `query_string`query instead).  As a query string query, you would need to change:

```
"(name:Android) OR NOT (status:approved)"
```

to:

```
"(name:Android) OR (NOT status:approved)"
```

The latter, when run through validate-query, shows the following:

```
"name:android (-status:approved +*:*)"
```
</comment><comment author="ppf2" created="2015-08-25T17:03:24Z" id="134671530">Ah got it, thx @clintongormley for the tip!
</comment><comment author="jdconrad" created="2015-10-14T15:32:36Z" id="148087381">Taking a look.
</comment><comment author="jdconrad" created="2015-10-14T16:54:37Z" id="148115044">This is working as expected as @dakrone has inferred.  The SimpleQueryParser should be thought of as only using AND and OR as operators.  There is no concept of SHOULD and MUST other than internally to create the AND and OR queries.  So when doing the query "+this -removed" the AND (+) is actually ignored as it is not thought of as a MUST.  Using SimpleQueryParser this will always be the case where the query ends up being documents that either have 'this' OR not 'removed'  ... Also note, that while this will return all the documents, the not 'removed' still affects scoring so it's not meaningless.  Going to leave this open for now for further discussion if necessary.
</comment><comment author="clintongormley" created="2015-10-15T11:51:52Z" id="148364757">While it may be working as designed, I'd argue that the syntax is surprising to most users.  For example:

```
POST t/t/_bulk
{"index":{"_id":1}}
{"foo":"one"}
{"index":{"_id":2}}
{"foo":"two"}
{"index":{"_id":3}}
{"foo":"one two"}
{"index":{"_id":4}}
{"foo":"three"}
```

I would expect the following:
- `"one"`:  Return docs 1 &amp; 3 (works)
- `"-two"`: Return docs 1 &amp; 3 (works)
- `"one -two"`: Return doc 1 (returns 1, 3, &amp; 4)
- `"one three -two"`:  Return docs 1 &amp; 4 (returns 1, 3 &amp; 4)

To get what I want (ie "Give me docs with one or three, but exclude anything with two") I need to write it as `"one three +-two"`.  That is not at all intuitive.  If I typed `"windows -microsoft"` into google, I wouldn't expect Google to return all of the documents on the internet which don't contain the word microsoft.

At the very least it should be well documented but, given that this query is intended to be exposed to users who will not read documentation, I would say that the syntax could be improved.
</comment><comment author="imotov" created="2015-10-15T14:24:32Z" id="148401074">&gt; If I typed "windows -microsoft" into google, I wouldn't expect Google to return all of the documents on the internet which don't contain the word microsoft.

@clintongormley that's because google (as well as most other search engines in 21st century) is using `AND` instead of `OR` as a default operator, which should be the default behavior for elasticsearch as well IMO. Having `OR` as a default operator is causing all sort of confusion for many new users.
</comment><comment author="rmuir" created="2015-10-15T14:32:44Z" id="148404542">Not really. if you query https://www.google.com/?gws_rd=ssl#q=elasticsearch+reference+query+dsl+oracle it gladly returns high ranking hits and just tells you: Missing: oracle

Switching to AND breaks many analysis chains such as n-grams. With a good ranking algo its also not necessary, its just that DefaultSimilarity is really weak here.
</comment><comment author="jdconrad" created="2015-10-15T15:46:28Z" id="148428747">I agree that this syntax is ugly -- "one three +-two" ; however, I am reluctant to special case the not operator because right now you have one OR three OR NOT two which while may be unexpected is predictable, but if I change this it becomes one OR three AND NOT two which is no longer predictable because it ignores the default operator and it loses its consistency.  It is also very difficult to predict proper sub queries outside of this simple case.  Take for example "one -three two" -- is this one AND not three OR two?  Do I need to reorder this?  I think this would end up being more confusing because of the way operator precedence works in that it's always first come first serve.
</comment><comment author="imotov" created="2015-10-15T15:51:13Z" id="148429969">What google does, is some weird "fuzzy" AND (or something like `should` with large minimum should match) search that google turns on a long tail queries with a large number of terms. But the basic behavior with 2-3 term queries resembles AND more than OR, would you agree?

n-grams is an advanced feature, I think if a user can figure out how to enable n-gram (or configure any other custom analysis chain) they should be able to figure out how to switch from AND to OR in the query.

Anyway, I shouldn't hijack this discussion. I apologize for that. Back to the original topic. I think that my expectation would be that `foo bar +baz -qux` should be translated into something like this:

```
{
   "bool": {
        "should" : [
            {
                "term" : { "_all" : "foo" }
            }, {
                "term" : { "_all" : "bar" }
            }
        ],
        "must" : {
            "term" : { "_all" : "baz" }
        },
        "must_not" : {
            "term" : { "_all" : "qux" }
        }
   }
}
```
</comment><comment author="jdconrad" created="2015-10-15T16:00:30Z" id="148434035">I should explain further what happens right now, each time an AND is switched to an OR or vice versa a new boolean query branch is created.  So if you have a b c  +d +e  f the tree ends up looking like

bq( should bq( should bq( should a should b should c ) must d must e ) should f)

so changing the not operator to always use must will have an inconsistent change in boolean query branches since operator precedence is always left to right.

We could change it to be something like @imotov suggests (maybe this should be a different parser altogether in Lucene?), but then you have should, must, and must not... if you're truly a basic user I think and/or is easier to understand than should/must/must not.
</comment><comment author="imotov" created="2015-10-15T16:13:24Z" id="148441151">&gt; I should explain further what happens right now, each time an AND is switched to an OR or vice versa a new boolean query branch is created. 

Yes, and this is where it breaks my expectation. To me order of elements in the query shouldn't make any difference because "+" and "-" feel like unary operators but they behave in strange ways.
</comment><comment author="jdconrad" created="2015-10-15T20:08:26Z" id="148506768">@imotov What you're saying makes sense to me from the point of view of someone that regularly deals with search, but for someone less technical I think and/or make more sense.  Honestly, the default to OR is a bit odd to me too because if someone, say my mother, types "dog food" into the google she expects it to be anded together there at least through decent scoring (as you and @rmuir mentioned earlier).  I think making a new parser with the behavior of must/should/must not makes sense depending on what our target audience wants.  SimpleQueryParser2 or something.
</comment><comment author="jdconrad" created="2015-10-15T21:53:34Z" id="148532454">All right after a bit more thought and discussion, I've come to agree with everyone in this issue that this behavior is unexpected for everyone.  I'll work on making a Lucene patch for the SimpleQueryParser using the behavior describe by @imotov and @rmuir where the structure will be a single bq per subquery.
</comment><comment author="clintongormley" created="2016-11-06T11:18:51Z" id="258674464">@jdconrad did anything ever come of this?  Did you open any issue in Lucene that we can track?
</comment><comment author="jdconrad" created="2016-11-07T17:29:42Z" id="258903012">@clintongormley Sorry, I must've gotten distracted by other issues before I had anytime to address this.  I'll have to take a bit of time to remember what we had discussed.
</comment><comment author="clintongormley" created="2016-12-23T10:23:37Z" id="268969186">Let's document and close</comment><comment author="dakrone" created="2017-01-06T23:48:48Z" id="271041077">Okay, opened a PR to document this, and then it can be closed.</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Document simple_query_string negation with default_operator of OR</comment></comments></commit></commits></item><item><title>remove `engine.robin.refresh_interval`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4706</link><project id="" key="" /><description>`InternalIndexShard.java` still accepts `engine.robin.refresh_interval` which seems super old. I think we should drop the support in 1.0 thoughts?
</description><key id="25499067">4706</key><summary>remove `engine.robin.refresh_interval`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2014-01-13T14:10:32Z</created><updated>2014-01-13T15:30:22Z</updated><resolved>2014-01-13T15:30:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-13T14:28:47Z" id="32173295">++
</comment><comment author="s1monw" created="2014-01-13T14:31:47Z" id="32173582">yeah I think we shoudl dropp it is seems very old
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file></files><comments><comment>Remove ancient `engine.robin.refresh_interval`</comment></comments></commit></commits></item><item><title>Default stopwords list should be `_none_` for all but language-specific analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4705</link><project id="" key="" /><description>`standard_html_strip` and `pattern` analyzer support stopwords which are
set to the default `english` stopwords by default. Those analyzers
should not use stopwords by default since they are language neutral

Closes #4699
</description><key id="25496527">4705</key><summary>Default stopwords list should be `_none_` for all but language-specific analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-13T13:24:59Z</created><updated>2014-06-13T23:46:57Z</updated><resolved>2014-01-13T13:44:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-13T13:43:28Z" id="32169864">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove StandardHtmlAnalyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4704</link><project id="" key="" /><description>The StandardHtmlAnalyzer has never been documented and has been implemented with limited support for configuration.

Its functionality is easy to replicate (in a more flexible way) using the HTMLStripCharFilter, so we should remove this analyzer.
</description><key id="25488993">4704</key><summary>Remove StandardHtmlAnalyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label></labels><created>2014-01-13T10:43:25Z</created><updated>2016-11-06T11:15:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-13T10:44:29Z" id="32158991">References #4704 and #4699 
</comment><comment author="s1monw" created="2014-01-13T10:45:06Z" id="32159015">++
</comment><comment author="s1monw" created="2014-01-13T13:16:53Z" id="32168121">actually I think we can't easily do that and we need to think more about how we handle deprecated analyzers in the future. For now I think we should just fix #4699  WDYT @clintongormley 
</comment><comment author="clintongormley" created="2016-11-06T11:15:57Z" id="258674322">I think we can deprecate this now (5.x).  We'll need to support it for existing indices in 6.0, but not allow new indices to be created with it, and we can remove in 7.0.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Scrolling with has_child filter returns no hits on 2nd request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4703</link><project id="" key="" /><description>When using scroll with a has_child filter, the initial request returns the correct total number of hits, but subsequent requests return no hits.

It looks like this problem was introduced in 0.90.6, and still occurs in 0.90.10.  0.90.5 works as expected.

The number of documents seems to play a part - in my initial test cases with only 2 parent documents, I couldn't reproduce the issue.  However, creating 100 parents does reliably reproduce it.  In my testing, 8 parent documents worked fine, but 9 did not.

It sounds very similar to the issue mentioned here: http://elasticsearch-users.115913.n3.nabble.com/No-hit-using-scan-scroll-with-has-parent-filter-td4047236.html

Here's a test script (requires jq(1) to grab the scroll ID from the first JSON result):

``` bash
#!/bin/sh

HOST='localhost:9200'
INDEX='test_scroll_jj'
CURL="curl -q --ipv4 --silent --show-error --fail"

$CURL -XDELETE "$HOST/${INDEX}?pretty=true" &gt;/dev/null
$CURL -XPOST "$HOST/${INDEX}/?pretty=true" -d '
{
    "mappings": {
        "homes":{
            "_parent":{
                "type" : "person"
            }
        }
    }
}' &gt;/dev/null

for x in {1..100}; do # in my testing, 8 docs works, 9 fails
    $CURL -XPUT "$HOST/${INDEX}/person/$x/?pretty=true" -d '{}' &gt;/dev/null
    $CURL -XPOST "$HOST/${INDEX}/homes?parent=$x&amp;pretty=true" -d '{}' &gt;/dev/null
done

$CURL -XPOST "$HOST/${INDEX}/_refresh?pretty=true" &gt;/dev/null

echo "REQUEST ONE:"
SCROLL_RESULT=$($CURL -v -XPOST "http://$HOST/${INDEX}/person/_search?pretty=true&amp;scroll=30s" -d'
{
    "size" : 1,
    "fields" : ["_id"],
    "query" : {
        "filtered" : {
            "filter" : {
                "has_child" : {
                    "type" : "homes",
                    "query" : {
                        "match_all" : {}
                    }
                }
            }
        }
    }
}')
echo $SCROLL_RESULT

scroll_id=$(echo $SCROLL_RESULT | jq -r '.["_scroll_id"]')

echo
echo "REQUEST TWO:"
$CURL -v "http://$HOST/_search/scroll?scroll=30s&amp;scroll_id=$scroll_id&amp;pretty=true"
```

The failing output on 0.90.10:

```
/tmp|⇒  /tmp/scrollbug.sh
REQUEST ONE:
* About to connect() to localhost port 9200 (#0)
*   Trying 127.0.0.1...
* Adding handle: conn: 0x7fb832006e00
* Adding handle: send: 0
* Adding handle: recv: 0
* Curl_addHandleToPipeline: length: 1
* - Conn 0 (0x7fb832006e00) send_pipe: 1, recv_pipe: 0
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; POST /test_scroll_jj/person/_search?pretty=true&amp;scroll=30s HTTP/1.1
&gt; User-Agent: curl/7.32.0
&gt; Host: localhost:9200
&gt; Accept: */*
&gt; Content-Length: 321
&gt; Content-Type: application/x-www-form-urlencoded
&gt;
} [data not shown]
* upload completely sent off: 321 out of 321 bytes
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 520
&lt;
{ [data not shown]
* Connection #0 to host localhost left intact
{ "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs2OkM5SXlBenNyU0lXR21uX3JsN25XcHc7NzpDOUl5QXpzclNJV0dtbl9ybDduV3B3Ozg6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzs5OkM5SXlBenNyU0lXR21uX3JsN25XcHc7MTA6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzswOw==", "took" : 5, "timed_out" : false, "_shards" : { "total" : 5, "successful" : 5, "failed" : 0 }, "hits" : { "total" : 100, "max_score" : 1.0, "hits" : [ { "_index" : "test_scroll_jj", "_type" : "person", "_id" : "2", "_score" : 1.0 } ] } }

REQUEST TWO:
* About to connect() to localhost port 9200 (#0)
*   Trying 127.0.0.1...
* Adding handle: conn: 0x7f8589806e00
* Adding handle: send: 0
* Adding handle: recv: 0
* Curl_addHandleToPipeline: length: 1
* - Conn 0 (0x7f8589806e00) send_pipe: 1, recv_pipe: 0
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; GET /_search/scroll?scroll=30s&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs2OkM5SXlBenNyU0lXR21uX3JsN25XcHc7NzpDOUl5QXpzclNJV0dtbl9ybDduV3B3Ozg6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzs5OkM5SXlBenNyU0lXR21uX3JsN25XcHc7MTA6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzswOw==&amp;pretty=true HTTP/1.1
&gt; User-Agent: curl/7.32.0
&gt; Host: localhost:9200
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 410
&lt;
{
  "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs2OkM5SXlBenNyU0lXR21uX3JsN25XcHc7NzpDOUl5QXpzclNJV0dtbl9ybDduV3B3Ozg6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzs5OkM5SXlBenNyU0lXR21uX3JsN25XcHc7MTA6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzswOw==",
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

And the expected output as per 0.90.5:

```
/tmp|⇒  /tmp/scrollbug.sh
REQUEST ONE:
* About to connect() to localhost port 9200 (#0)
*   Trying 127.0.0.1...
* Adding handle: conn: 0x7fd04a006e00
* Adding handle: send: 0
* Adding handle: recv: 0
* Curl_addHandleToPipeline: length: 1
* - Conn 0 (0x7fd04a006e00) send_pipe: 1, recv_pipe: 0
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; POST /test_scroll_jj/person/_search?pretty=true&amp;scroll=30s HTTP/1.1
&gt; User-Agent: curl/7.32.0
&gt; Host: localhost:9200
&gt; Accept: */*
&gt; Content-Length: 321
&gt; Content-Type: application/x-www-form-urlencoded
&gt;
} [data not shown]
* upload completely sent off: 321 out of 321 bytes
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 523
&lt;
{ [data not shown]
* Connection #0 to host localhost left intact
{ "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsyMTpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzIzOkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MjI6SC1fSFFlNjJUa1M0MndiUWM2S0t3UTsyNDpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzI1OkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MDs=", "took" : 9, "timed_out" : false, "_shards" : { "total" : 5, "successful" : 5, "failed" : 0 }, "hits" : { "total" : 100, "max_score" : 1.0, "hits" : [ { "_index" : "test_scroll_jj", "_type" : "person", "_id" : "2", "_score" : 1.0 } ] } }

REQUEST TWO:
* About to connect() to localhost port 9200 (#0)
*   Trying 127.0.0.1...
* Adding handle: conn: 0x7f8a92006e00
* Adding handle: send: 0
* Adding handle: recv: 0
* Curl_addHandleToPipeline: length: 1
* - Conn 0 (0x7f8a92006e00) send_pipe: 1, recv_pipe: 0
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; GET /_search/scroll?scroll=30s&amp;scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsyMTpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzIzOkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MjI6SC1fSFFlNjJUa1M0MndiUWM2S0t3UTsyNDpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzI1OkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MDs=&amp;pretty=true HTTP/1.1
&gt; User-Agent: curl/7.32.0
&gt; Host: localhost:9200
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 523
&lt;
{
  "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsyMTpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzIzOkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MjI6SC1fSFFlNjJUa1M0MndiUWM2S0t3UTsyNDpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzI1OkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MDs=",
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 100,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test_scroll_jj",
      "_type" : "person",
      "_id" : "7",
      "_score" : 1.0
    } ]
  }
}
```
</description><key id="25479957">4703</key><summary>Scrolling with has_child filter returns no hits on 2nd request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">joedj</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-13T06:33:43Z</created><updated>2014-02-27T14:22:01Z</updated><resolved>2014-01-20T10:12:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="deverton" created="2014-01-13T23:12:16Z" id="32222609">I've written this up as a an integration test and used git bisect to try and track down where this broke between 0.90.5 and 0.90.6. It looks like this commit 9950e4440aeea76fc71b5ee534e15d4bfb1d73ed seems to be the culprit.

This is the test method I'm using.

``` java
    @Test
    public void simpleScrolledHasChildFilteredQuery() throws Exception {
        client().admin().indices().prepareCreate("test")
                .setSettings(ImmutableSettings.settingsBuilder().put("index.number_of_shards", 1).put("index.number_of_replicas", 0))
                .execute().actionGet();
        client().admin().cluster().prepareHealth().setWaitForEvents(Priority.LANGUID).setWaitForGreenStatus().execute().actionGet();
        client().admin()
                .indices()
                .preparePutMapping("test")
                .setType("child")
                .setSource(
                        jsonBuilder().startObject().startObject("child").startObject("_parent").field("type", "parent").endObject()
                                .endObject().endObject()).execute().actionGet();


        for (int i = 0; i &lt; 10; i++) {
            client().prepareIndex("test", "parent", "p" + i).setSource("{}").execute().actionGet();
            client().prepareIndex("test", "child", "c" + i).setSource("{}").setParent("p" + i).execute().actionGet();
        }

        client().admin().indices().prepareRefresh().execute().actionGet();

        final SearchResponse scrollResponse = client().prepareSearch("test")
                .setScroll(TimeValue.timeValueSeconds(30))
                .setSize(1)
                .addField("_id")
                .setTypes("parent")
                .setQuery(filteredQuery(matchAllQuery(), FilterBuilders.hasChildFilter("child", matchAllQuery())))
                .execute()
                .actionGet();

        final SearchResponse firstScroll = client().prepareSearchScroll(scrollResponse.getScrollId()).setScroll(TimeValue.timeValueSeconds(30)).execute().actionGet();
        final SearchResponse secondScroll = client().prepareSearchScroll(firstScroll.getScrollId()).setScroll(TimeValue.timeValueSeconds(30)).execute().actionGet();

        client().prepareClearScroll().addScrollId(secondScroll.getScrollId()).execute().actionGet();

        assertThat(scrollResponse.getFailedShards(), equalTo(0));
        assertThat(scrollResponse.getHits().totalHits(), equalTo(10l));

        assertThat(firstScroll.getFailedShards(), equalTo(0));
        assertThat(firstScroll.getHits().getHits().length, equalTo(1));

        assertThat(secondScroll.getFailedShards(), equalTo(0));
        assertThat(secondScroll.getHits().getHits().length, equalTo(1));
    }
```
</comment><comment author="martijnvg" created="2014-01-14T22:57:05Z" id="32317041">Nice catch! 

I further looked into this issue and this error only seems to occur with the has_child or has_parent filter, but not with the has_child / has_parent query.
</comment><comment author="govindm" created="2014-01-15T22:05:56Z" id="32420481">Thanks for fixing it.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/search/child/CustomQueryWrappingFilter.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Consume the entire weight and pre compute the DocIdSets for all segments instead of keeping the weight around and build a DocIdSet when a segment is being processed. This fixes issues where the has_child / has_parent filter produce no results or errors on subsequent scan requests.</comment></comments></commit></commits></item><item><title>Improve support for partial snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4702</link><project id="" key="" /><description>Fixes #4701. Changes behaviour of the snapshot operation. The operation now fails if not all primary shards are available at the beginning of the snapshot operation. The restore operation no longer tries to restore indices with shards that failed or were missing during snapshot operation.
</description><key id="25476792">4702</key><summary>Improve support for partial snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-01-13T03:43:11Z</created><updated>2014-07-08T03:16:46Z</updated><resolved>2014-01-13T22:37:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-13T21:21:03Z" id="32212679">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Snapshots with missing or failed shards hang on restore operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4701</link><project id="" key="" /><description>To reproduce snapshot an index with unavailable primary shards and then try to restore it - the restore operation will hang. Expected result: restore operation should fail with an error. 
</description><key id="25474578">4701</key><summary>Snapshots with missing or failed shards hang on restore operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2014-01-13T01:45:09Z</created><updated>2014-01-13T22:37:21Z</updated><resolved>2014-01-13T22:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/SnapshotMetaData.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotInfo.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/snapshots/AbstractSnapshotTests.java</file><file>src/test/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/snapshots/RepositoriesTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SharedClusterSnapshotRestoreTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Improve support for partial snapshots</comment></comments></commit></commits></item><item><title>Fix RandomExceptionCircuitBreakerTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4700</link><project id="" key="" /><description>RandomExceptionCircuitBreakerTests never actually installed
the FilteredAtomicReader that should throw random exceptions
when field data is loaded. There were also bug in the way field data invalidates caches when a testing FilteredAtomicReader was used.
</description><key id="25470366">4700</key><summary>Fix RandomExceptionCircuitBreakerTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-12T21:33:22Z</created><updated>2014-07-08T03:07:23Z</updated><resolved>2014-01-13T16:25:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-13T15:33:47Z" id="32179409">Left minor comments, otherwise it looks good. I like the new SegmentReaderUtils class! +1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default stopwords list should be `_none_` for all but language-specific analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4699</link><project id="" key="" /><description>/cc @s1monw 

In #4092 the `standard` analyzer's default stopwords list was changed from `english` to `_none_`. The reasons for this are:
1. Removing stopwords on any string field by default can have surprising results, eg `country_code: "NO"` would be indexed with no value,  `title: "To be or not to be"` would similarly have all words removed.
2. Stopwords do add value to search, and with tools like the `common` query, we can take stopwords into account while still keeping queries performing well.
3. Choosing the English stopwords by default can be surprising for users whose primary language isn't English. 

However, there are two other non-language-specific analyzers which should have a similar treatment, specifically:
- `pattern` analyzer 
- `standard_html` analyzer

Also, the change to the `standard` analyzer has not been documented, and the `standard_html` analyzer is not documented at all.
</description><key id="25464221">4699</key><summary>Default stopwords list should be `_none_` for all but language-specific analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2014-01-12T15:37:58Z</created><updated>2014-01-13T13:44:44Z</updated><resolved>2014-01-13T13:44:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-12T15:43:06Z" id="32125204">Correction: the `standard` analyzer docs do reflect the change to stopwords in 1.0
</comment><comment author="s1monw" created="2014-01-13T10:32:17Z" id="32158300">+1 I agree we should fix `pattern` and `standard_html` as well
</comment><comment author="s1monw" created="2014-01-13T10:39:38Z" id="32158705">actually I think we should drop the `standard_html_strip` analyzer entirely and let folks configure it themself
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/analysis/PatternAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzer.java</file><file>src/main/java/org/elasticsearch/index/analysis/StandardHtmlStripAnalyzerProvider.java</file><file>src/main/java/org/elasticsearch/indices/analysis/PreBuiltAnalyzers.java</file><file>src/test/java/org/elasticsearch/index/analysis/AnalyzerBackwardsCompatTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/PreBuiltAnalyzerTests.java</file><file>src/test/java/org/elasticsearch/index/analysis/synonyms/SynonymsAnalysisTest.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTokenStreamTestCase.java</file></files><comments><comment>Default stopwords list should be `_none_` for all but language-specific analyzers</comment></comments></commit></commits></item><item><title>Added stopwords: _none_ to the docs #329</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4698</link><project id="" key="" /><description>I'm missing `stopwords: _none_` in the docs, as well as a `stopwords: ...` section within the pattern analyzer docs.
</description><key id="25460081">4698</key><summary>Added stopwords: _none_ to the docs #329</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrkamel</reporter><labels /><created>2014-01-12T10:29:02Z</created><updated>2014-06-24T17:47:04Z</updated><resolved>2014-01-14T10:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-12T15:40:25Z" id="32125148">Hi @mrkamel 

Many thanks for this PR.  Reading through it made me realise that we have some inconsistent changes coming up.  In 1.0 we have set the default stopwords list on new indices for the `standard` analyzer to `_none_` (which apparently isn't documented yet).  

I've just opened #4699 to get the `pattern` and `standard_html` analyzers changed to default to `_none_`. We should probably wait for that before moving forward with this PR.

Also, please could you sign the CLA http://www.elasticsearch.org/contributor-agreement so that we can accept these changes.

many thanks
</comment><comment author="mrkamel" created="2014-01-12T16:48:27Z" id="32126807">a) The docs of master state

```
A list of stopwords to initialize the stop filter with.
Defaults to an 'empty' stopword list added[1.0.0.Beta1, Previously 
defaulted to the English stopwords list]
```

b) I agree, we should wait for the removal of inconsistencies
c) Signed
d) You're right, it's probably not the best page to add this section. However, i used it since 

```
        ...
        stopwords : [test1, test2, test3]
```

was already listed in the page's sample config. Thus, i assume stopwords should not be listed on this page at all and we should provide an extra page for them to have less duplication in the docs and then reference it where neccessary. Where do you want it to live? `analysis/analyzers/stopwords.asciidoc`?
</comment><comment author="mrkamel" created="2014-01-13T13:50:34Z" id="32170376">Great, #4699 seems to be fixed. Do you agree to my proposal in d) ?
I'd create another pull request then ... and close this one
</comment><comment author="clintongormley" created="2014-01-13T13:56:04Z" id="32170802">Heya

Re:
(a) - I missed that :)
(b) done, we won't document StandardHtml
(c) Great, thanks!
(d) I would:
- leave the `stopwords` example as it was originally on the analyzers page - it's just an example, doesn't need full documentation
- document `stopwords` fully on analysis/analyzers/stop-analyzer.asciidoc (note the default here is still english)
- refer to the above from standard and pattern analyzers, noting that the default for these is `_none_`

No need to close this PR, you can just update it and push again.

thanks for helping @mrkamel 
</comment><comment author="clintongormley" created="2014-01-13T13:57:46Z" id="32170930">also please include the `coming[1.0.0.RC1,Previously defaulted to the English stopwords]` on the pattern analyzer page, and please rebase your PR on master.

ta
</comment><comment author="mrkamel" created="2014-01-13T18:11:26Z" id="32194764">Before, may i finally ask why ES actually uses such kind of special keyword `_none_`? I'd at least expect `stopwords: []` to work while being much more self-explanatory and yaml-like, thus making this PR unnecessary. However, `stopwords: []` or `stopwords: ""` doesn't work in my tests.

Sorry for being pedantic, but i don't easily find an obvious hint within the sources.
</comment><comment author="clintongormley" created="2014-01-13T18:27:15Z" id="32196141">Good question, and I don't know either :)  Looks like an empty list isn't being considered currently. 

I've opened https://github.com/elasticsearch/elasticsearch/issues/4709 to get support added (but this may take a little while as we're getting ready for the next release now).
</comment><comment author="clintongormley" created="2014-01-14T10:54:23Z" id="32255172">Merged! Many thanks for the PR @mrkamel 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use of sigar 1.6.5 library in RHEL6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4697</link><project id="" key="" /><description>Is it possible to make use of default sigar library, present in CentOS/RHEL6?
Does your sigar-1.6.4.jar file needs to have the .so library present into same directory?

```
$ yum list sigar
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: centos.mirror.rafal.ca
 * extras: mirror.netflash.net
 * updates: mirror.netflash.net
Installed Packages
sigar.x86_64    1.6.5-0.4.git58097d9.el6    @base
$ ls -lah /usr/lib64/libsigar.so 
-rwxr-xr-x. 1 root root 144K Dec  7  2011 /usr/lib64/libsigar.so
```

Thank you.
</description><key id="25456419">4697</key><summary>Use of sigar 1.6.5 library in RHEL6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">yqed</reporter><labels /><created>2014-01-12T04:01:43Z</created><updated>2014-02-22T16:25:58Z</updated><resolved>2014-02-22T16:25:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-13T08:33:34Z" id="32151821">Hey,

elasticsearch tries to come with every dependency included by default, so you do not get to mix up versions, because you are using different distributions or operating systems (and to make sure you do not have to install a a ton of dependencies). Furthermore one cannot guarantee, that the next sigar version still works with that elasticsearch version (though it is quite likely).

Is there any specific reason you want exactly that library version working with elasticsearch?
</comment><comment author="spinscale" created="2014-02-22T16:25:58Z" id="35806691">closing due to inactivity for now. happy to reopen on more feedback..
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use seconds instead of millis for timestamps in cat api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4696</link><project id="" key="" /><description>Output of `_cat/health`:

```
time(ms)      timestamp cluster  status nodeTotal nodeData shards pri relo init unassign
1389481752742 17:09:12  debugger yellow         1        1      7   7    0    0        7
```

The precision of millis isn't necessary here, but more importantly:

```
% date -r 1389481752742
Sun Dec  3 11:32:22 CST 46000
% date -r 1389481752
Sat Jan 11 17:09:12 CST 2014
```
</description><key id="25453508">4696</key><summary>Use seconds instead of millis for timestamps in cat api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-11T23:45:40Z</created><updated>2014-01-17T13:38:44Z</updated><resolved>2014-01-14T21:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestHealthAction.java</file></files><comments><comment>Shorten epoch to second precision.</comment></comments></commit></commits></item><item><title>Add pri flag to cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4695</link><project id="" key="" /><description>```
% curl localhost:9200/_cat/indices\?h=health,index,rep,docs.count,ss,cs,fm\&amp;v\&amp;pri
health index   rep docs.count      ss  pri.ss cs pri.cs     fm pri.fm
green  wiki      1       6401 302.9mb 151.4mb 0b     0b 25.1mb     0b
green  twitter   1      11434    64mb    32mb 0b     0b     0b     0b
```
</description><key id="25453227">4695</key><summary>Add pri flag to cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-11T23:26:37Z</created><updated>2014-07-16T21:49:48Z</updated><resolved>2014-01-29T14:39:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-11T23:51:51Z" id="32110923">looks good. Lets push. wondering though what we should do with the docs..., its the only one that default to primaries, might be confusing? maybe we should show by default the (total) docs and pri.docs, so we maintain the convention, but also gives a notion of the number of docs in total, and on primaries?
</comment><comment author="drewr" created="2014-01-13T15:37:12Z" id="32179749">It's a good point, though I'm not sure consistency is the best policy here.  I can't think of a time when I've ever cared about the total docs including replicas.  I wonder if it's worth the space?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add MockPageCacheRecycler in test jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4694</link><project id="" key="" /><description>MockPageCacheRecycler is missing in test jar which makes failing tests when using
test jar in plugins:

```
1&gt; [2014-01-11 10:51:30,531][ERROR][test                     ] FAILURE  : testWikipediaRiver(org.elasticsearch.river.wikipedia.WikipediaRiverTest)
  1&gt; REPRODUCE WITH  : mvn test -Dtests.seed=5DAFD4FBAE587363 -Dtests.class=org.elasticsearch.river.wikipedia.WikipediaRiverTest -Dtests.method=testWikipediaRiver -Dtests.prefix=tests -Dtests.network=true -Dfile.encoding=MacRoman -Duser.timezone=Europe/Paris -Des.logger.level=INFO -Des.node.local=true -Dtests.cluster_seed=134842C2D806FFC0
  1&gt; Throwable:
  1&gt; java.lang.NoClassDefFoundError: org/elasticsearch/cache/recycler/MockPageCacheRecycler
  1&gt;     org.elasticsearch.test.cache.recycler.MockPageCacheRecyclerModule.configure(MockPageCacheRecyclerModule.java:30)
  1&gt;     org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
```
</description><key id="25445858">4694</key><summary>add MockPageCacheRecycler in test jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>test</label><label>v1.0.0.RC1</label></labels><created>2014-01-11T16:12:17Z</created><updated>2014-06-14T07:07:36Z</updated><resolved>2014-01-13T08:08:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-11T16:14:57Z" id="32099874">+1 LGTM thanks @dadoonet 
</comment><comment author="jpountz" created="2014-01-11T17:52:42Z" id="32102333">Looks good, thanks David!

Just for reference, the reason why it isn't in the test package is that it needs to be in the same package as PageCacheRecycler in order to pick the same components settings (otherwise randomization of the recycler type wouldn't work).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>add MockPageCacheRecycler in test jar</comment></comments></commit></commits></item><item><title>NPE when bulk indexing into an index in the process of being deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4693</link><project id="" key="" /><description>This is reproducible by deleting an index, not waiting for the response, then trying to bulk index into that index, (ie the requests were run in parallel):

```
[2014-01-11 14:34:30,541][INFO ][cluster.metadata         ] [Justin Hammer] [test_index] creating index, cause [auto(bulk api)], shards [5]/[1], mappings []
[2014-01-11 14:34:31,007][INFO ][cluster.metadata         ] [Justin Hammer] [test_index] update_mapping [test_type] (dynamic)
[2014-01-11 14:34:33,131][INFO ][cluster.metadata         ] [Justin Hammer] [test_index] deleting index
[2014-01-11 14:34:33,156][DEBUG][action.bulk              ] [Justin Hammer] [test_index][0], node[LthVQsuVRsyVgKZZr8_gpA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@cfd5855]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:136)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:73)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```
</description><key id="25443592">4693</key><summary>NPE when bulk indexing into an index in the process of being deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-01-11T13:44:43Z</created><updated>2014-01-13T08:29:56Z</updated><resolved>2014-01-13T08:29:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-13T08:29:56Z" id="32151662">duplicate of #4224
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>REST Update Settings API should not take timeout and master_timeout as index parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4692</link><project id="" key="" /><description /><key id="25433539">4692</key><summary>REST Update Settings API should not take timeout and master_timeout as index parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC1</label></labels><created>2014-01-11T00:55:38Z</created><updated>2014-01-11T01:05:33Z</updated><resolved>2014-01-11T01:05:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file></files><comments><comment>REST Update Settings API should not take timeout and master_timeout as index parameters</comment><comment>closes #4692</comment></comments></commit></commits></item><item><title>Highlighting should have an ordering where only the top scoring fragments are return but in the order they are in the document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4691</link><project id="" key="" /><description>Highlighting should have an ordering where only the top scoring fragments are return but in the order they are in the document.  At least where that is possible.
</description><key id="25426258">4691</key><summary>Highlighting should have an ordering where only the top scoring fragments are return but in the order they are in the document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-10T22:08:17Z</created><updated>2014-03-31T17:27:40Z</updated><resolved>2014-03-31T17:27:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-31T17:27:40Z" id="39116268">Abandoning in favor of getting this Elasticsearch plugin released which has this:  https://github.com/nik9000/expiremental-highlighter
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting should make an effort to return snippets that contain all terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4690</link><project id="" key="" /><description>I'm not really sure how this would/should work but I feel like it could dovetail into #3776 to let you get a handful of smaller snippets that contain all the terms or just one large one if possible.  I'm also not really sure how this should play in with terms with different scores.
</description><key id="25425891">4690</key><summary>Highlighting should make an effort to return snippets that contain all terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-10T22:02:09Z</created><updated>2014-04-24T14:16:30Z</updated><resolved>2014-04-24T14:16:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-24T14:16:30Z" id="41284989">Actually implemented this over here:  https://github.com/wikimedia/search-highlighter
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make cat/indices primary shard columns enabled with an http param</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4689</link><project id="" key="" /><description /><key id="25403407">4689</key><summary>Make cat/indices primary shard columns enabled with an http param</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-10T15:57:10Z</created><updated>2014-01-15T15:19:52Z</updated><resolved>2014-01-13T15:45:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/Table.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestTable.java</file></files><comments><comment>Support sibling columns, with _cat/indices?pri as first go.</comment></comments></commit></commits></item><item><title>Failed to create Shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4688</link><project id="" key="" /><description>We have the elasticsearch instance that doesn't have any production load yet. However, after indexing few records, we got following in logs:

```
[2014-01-09 15:14:30,583][WARN ][indices.cluster          ] [O'Hara, Miguel] [inbound][2] failed to create shard
org.elasticsearch.index.shard.IndexShardCreationException: [inbound][2] failed to create shard
        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:340)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:567)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:532)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:174)
        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:321)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:95)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.IOException: directory '/usr/local/elasticsearch/data/kuew-search/nodes/0/indices/inbound/2/index' exists and is a directory, but cannot be listed: list() returned null
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:230)
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:241)
        at org.apache.lucene.store.RateLimitedFSDirectory.listAll(RateLimitedFSDirectory.java:47)
        at org.elasticsearch.index.store.Store.readChecksums(Store.java:189)
        at org.elasticsearch.index.store.Store$StoreDirectory.&lt;init&gt;(Store.java:307)
        at org.elasticsearch.index.store.Store.&lt;init&gt;(Store.java:82)
        at sun.reflect.GeneratedConstructorAccessor13.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
```

While, in browser it says:

```
{
  "took": 41,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 1,
    "failed": 4,
    "failures": [
      {
        "index": "inbound",
        "shard": 3,
        "status": 500,
        "reason": "No active shards"
      },
      {
        "index": "inbound",
        "shard": 4,
        "status": 500,
        "reason": "No active shards"
      },
      {
        "index": "inbound",
        "shard": 1,
        "status": 500,
        "reason": "No active shards"
      },
      {
        "index": "inbound",
        "shard": 2,
        "status": 500,
        "reason": "No active shards"
      }
    ]
  },
  "hits": {
    "total": 49,
    "max_score": 1,
    "hits": []
  }
}
```

`PS:` we can see few records in `hits[]`.

Help appreciated.
</description><key id="25403101">4688</key><summary>Failed to create Shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ArkeologeN</reporter><labels><label>non-issue</label></labels><created>2014-01-10T15:52:50Z</created><updated>2014-01-11T16:57:15Z</updated><resolved>2014-01-10T19:29:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-10T19:29:01Z" id="32057366">Please use the mailing list for questions like this in the future! It seems like you are running out of file handles, is that possible? Do you see "Too many open files" in the logs somewhere? Also check this link: http://stackoverflow.com/questions/15903105/too-many-open-files-warning-from-elasticsearch
</comment><comment author="ArkeologeN" created="2014-01-11T08:55:58Z" id="32091160">Sorry, I'll take care of this to ask over mailing list! Bdw, it works! thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make PUT consistent for `_mapping`, `_alias` and `_warmer`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4687</link><project id="" key="" /><description>This pull request makes consistent the PUT for `_mapping`, `_alias` and `_warmer` as described in issue #4071.
### `_mapping`:

Single type can now be added with

`[PUT|POST] {index|_all|*|regex|blank}/[_mapping|_mappings]/type`

and

`[PUT|POST] {index|_all|*|regex|blank}/type/[_mapping|_mappings]`
### `_warmer`

A single warmer can now be put with

`[PUT|POST] {index|_all|*|prefix*|blank}/{type|_all|*|prefix*|blank}/[_warmer|_warmers]/warmer_name`
### `_alias`

A single alias can now be PUT with

`[PUT|POST] {index|_all|*|prefix*|blank}/[_alias|_aliases]/alias`
</description><key id="25398857">4687</key><summary>Make PUT consistent for `_mapping`, `_alias` and `_warmer`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-01-10T14:46:53Z</created><updated>2014-07-08T02:59:45Z</updated><resolved>2014-01-16T12:04:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-11T02:10:17Z" id="32083831">LGTM, would be great if @clintongormley can review the rest tests and the behavior as well...
</comment><comment author="brwe" created="2014-01-13T23:11:57Z" id="32222589">I updated the pull request and added also the changes for DELETE. I think it makes sense to squash the commits for DELETE and PUT _alias (7c9f814, 7188455) because they both touch the same files.
</comment><comment author="dakrone" created="2014-01-14T00:00:12Z" id="32225919">The output from putting a mapping with no type specified seem strange:

### pre-this-change:

```
curl -XDELETE 'localhost:9200/test4687'
echo
curl -XPOST 'localhost:9200/test4687'
echo
curl -XPOST 'localhost:9200/test4687/_mapping' -d'{
  "_source": {"enabled": false},
  "properties": {
    "body": {"type": "string"},
    "newtype": {"type": "integer"}
  }
}'
echo
curl 'localhost:9200/test4687/_mapping?pretty'
```

Output:

```
{"acknowledged":true}
{"acknowledged":true}
{"error":"ActionRequestValidationException[Validation Failed: 1: mapping type is missing;]","status":500}
{
  "test4687" : { }
}
```

### post-this-change:

```
curl -XDELETE 'localhost:9200/test4687'
echo
curl -XPOST 'localhost:9200/test4687'
echo
curl -XPOST 'localhost:9200/test4687/_mapping' -d'{
  "_source": {"enabled": false},
  "properties": {
    "body": {"type": "string"},
    "newtype": {"type": "integer"}
  }
}'
echo
curl 'localhost:9200/test4687/_mapping?pretty'
```

Output:

```
{"acknowledged":true}
{"acknowledged":true}
{"acknowledged":true}
{
  "test4687" : {
    "test4687" : {
      "_source" : {
        "enabled" : false
      },
      "properties" : {
        "body" : {
          "type" : "string"
        },
        "newtype" : {
          "type" : "integer"
        }
      }
    }
  }
}
```

Is this intended? The double-wrapping of types in the post-change seems incorrect to me.
</comment><comment author="brwe" created="2014-01-14T11:18:09Z" id="32256674">@dakrone  about the double wrapping: when you call 

`curl -XPOST 'localhost:9200/test4687'`

 this creates and index named `test4687`

When you call

 `curl -XPOST 'localhost:9200/test4687/_mapping'`

this creates the type `test4687` for all indices. The reason is that we would like to let people leave the index blank when putting a mapping. The original way to do this was 

`PUT /index/type`

but since we now can leave the index blank this means that when using the uri

`/something/_mapping`

`something` will be interpreted as `type`.

If this is too confusing maybe we should  remove the old uris

`/index/type/_mapping`

and 

`/index/_mapping?type=name`

?
</comment><comment author="clintongormley" created="2014-01-14T11:22:37Z" id="32256933">@brwe Agreed that is confusing.  Would it be possible to only support the old URLs by making `{index}` and `{type}` required in those cases?
</comment><comment author="clintongormley" created="2014-01-14T11:24:54Z" id="32257047">I'd like to keep the old URLs around, to make migration easier, but if we can't do the above, then I'd be leaning towards making `{index}` required for all PUT operations (incl `_settings`, unfortunately). It can still be `*` or `_all`, but just not blank.  (but if you can just make it required for the old URLs, then I'd prefer that)
</comment><comment author="brwe" created="2014-01-14T12:39:33Z" id="32261141">@clintongormley @dakrone ok, it is easy to remove the options and only keep the old functionality. Should we then remove `PUT /{index}/{type}/_mapping` from the documentation? It would be quite confusing to explain why for `PUT /{index}/_mapping/{type}` we can leave index blank but not for the old path.
</comment><comment author="brwe" created="2014-01-14T18:17:47Z" id="32291177">@imotov @dakrone @s1monw @clintongormley Thanks so much for the review!

I updated the pull request with the following changes:
- new commits with the changes requested in review
- commit 9072c89068 : 404 if aliases are reqested to be deleted but none of the aliases was found, this also affects `POST /_aliases { actions: [...]}` if the actions contains only deletes
- added all commits in @clintongormley pull request https://github.com/brwe/elasticsearch/pull/1 

Next, I will squash and push.
</comment><comment author="s1monw" created="2014-01-14T19:01:18Z" id="32295416">LGMT +1 to push Good Job!!
</comment><comment author="brwe" created="2014-01-16T12:04:36Z" id="32463274">pushed to master (411739f)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adds a new “coerce” flag for controlling numeric value field mappings </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4686</link><project id="" key="" /><description>Defaulted to true

When set to false a new strict mode of parsing is employed which
a) does not permit numbers to be passed as JSON strings in quotes
b) rejects numbers with fractions that are passed to integer, short or long fields.

Closes #4117
</description><key id="25398534">4686</key><summary>Adds a new “coerce” flag for controlling numeric value field mappings </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels /><created>2014-01-10T14:41:37Z</created><updated>2014-06-27T17:35:42Z</updated><resolved>2014-01-13T18:07:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-01-13T18:07:01Z" id="32194387">Pushed to master via https://github.com/elasticsearch/elasticsearch/commit/541059a4d13042819fcdf3e563b2ea60ba0bb604
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE in bool filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4685</link><project id="" key="" /><description>typo was cause of NPE
see my comment of commit
https://github.com/elasticsearch/elasticsearch/commit/8e0291823ab6e1d638b986d22690b805828c478a
</description><key id="25398485">4685</key><summary>NPE in bool filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">skingreek</reporter><labels><label>bug</label><label>v0.90.11</label><label>v1.0.0.RC1</label></labels><created>2014-01-10T14:40:50Z</created><updated>2014-01-11T14:46:40Z</updated><resolved>2014-01-10T21:53:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-10T21:17:05Z" id="32067024">@skingreek Thanks bringen this up. Just curious: what filter clauses where you using in the bool filter when the NPE occurred?
</comment><comment author="skingreek" created="2014-01-11T14:42:04Z" id="32097673">i use filed query. internally elastic call org.apache.lucene.search.TermQuery.scorer method.
strange but on other index this query was succesful. i suppose that NPE occured if at least one segment of lucen index not content requested term

example of failed query:

``` JSON
{

   "from":"0",
   "query":{
      "filtered":{
         "filter":{
            "bool":{
               "should":[
                  {
                     "query":{
                        "field":{
                           "type":"test"
                        }
                     }
                  }
               ],
               "must":[
                  {
                     "match_all":{

                     }
                  }
               ]
            }
         }
      }
   },
   "size":21
}
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/lucene/search/XBooleanFilter.java</file><file>src/test/java/org/elasticsearch/common/lucene/search/XBooleanFilterTests.java</file></files><comments><comment>Fixes NPE in bool filter, when an empty should filter clause returns a DocIdSet, but null as iterator.</comment></comments></commit></commits></item><item><title>Add new option `min_doc_count` to terms and histogram aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4684</link><project id="" key="" /><description>`min_doc_count` is the minimum number of hits that a term or histogram key
should match in order to appear in the response.

`min_doc_count=0` replaces `compute_empty_buckets` for histograms and will
behave exactly like facets' `all_terms=true` for terms aggregations.

Close #4662
</description><key id="25388979">4684</key><summary>Add new option `min_doc_count` to terms and histogram aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-10T11:16:28Z</created><updated>2014-06-22T17:27:04Z</updated><resolved>2014-01-13T09:16:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mkdynamic" created="2014-04-01T03:36:38Z" id="39167864">I'm using this with a terms aggregation with a script, like so:

``` json
{
  "terms": {
    "script": "doc[\"data.o_data.o_object.o_plan.s_name.raw\"].value + \": $\" + (doc[\"data.o_data.o_object.o_plan.i_amount\"].value / 100)",
    "min_doc_count": 0
  }
}
```

If I leave out `min_doc_count` it works fine. If I leave it out, the search fails with a 500, and this error message (expanded from JSON response for readability):

```
SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[BJgRogsaTASoE3KkhylFHA][quanta][3]: PropertyAccessException[[Error: could not access: value; in class: org.elasticsearch.index.fielddata.ScriptDocValues$Empty]
[Near : {... value ....}]
             ^
[Line: 1, Column: 1]]}{[BJgRogsaTASoE3KkhylFHA][quanta][2]: PropertyAccessException[[Error: could not access: value; in class: org.elasticsearch.index.fielddata.ScriptDocValues$Empty]
[Near : {... value ....}]
             ^
[Line: 1, Column: 1]]}{[BJgRogsaTASoE3KkhylFHA][quanta][4]: PropertyAccessException[[Error: could not access: value; in class: org.elasticsearch.index.fielddata.ScriptDocValues$Empty]
[Near : {... value ....}]
             ^
[Line: 1, Column: 1]]}{[BJgRogsaTASoE3KkhylFHA][quanta][1]: PropertyAccessException[[Error: could not access: value; in class: org.elasticsearch.index.fielddata.ScriptDocValues$Empty]
[Near : {... value ....}]
             ^
[Line: 1, Column: 1]]}{[BJgRogsaTASoE3KkhylFHA][quanta][0]: PropertyAccessException[[Error: could not access: value; in class: org.elasticsearch.index.fielddata.ScriptDocValues$Empty]
[Near : {... value ....}]
             ^
[Line: 1, Column: 1]]}]
```

Is it trying to evaluate the script with no document? Any ideas?
</comment><comment author="jpountz" created="2014-04-01T14:29:37Z" id="39211677">Thank you @mkdynamic . Although not related to `min_doc_count`, there is indeed a bug, I just opened an issue for it.
</comment><comment author="mkdynamic" created="2014-04-01T19:14:38Z" id="39246335">Thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move RestRequest to be an abstract class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4683</link><project id="" key="" /><description>Opening an issue for this change: https://github.com/elasticsearch/elasticsearch/commit/bc0909b2325edb97ccb7254ce956469e3d678920

Move RestRequest to be an abstract class, and expose local/remote address

This issue breaks bwc for plugins which were extending `AbstractRestRequest` class.

Like the https://github.com/elasticsearch/elasticsearch-transport-memcached plugin. 
</description><key id="25386753">4683</key><summary>Move RestRequest to be an abstract class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2014-01-10T10:33:06Z</created><updated>2014-01-28T06:03:10Z</updated><resolved>2014-01-10T10:33:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose min/max open file descriptors in Cluster Stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4682</link><project id="" key="" /><description>Also changes the response format of that section to:

```
 "open_file_descriptors": {
      "min": 200,
      "max": 346,
       "avg": 273
 }
```

Closes 4681
</description><key id="25383841">4682</key><summary>Expose min/max open file descriptors in Cluster Stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-01-10T09:32:11Z</created><updated>2014-07-07T04:04:55Z</updated><resolved>2014-01-10T10:14:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-10T09:38:49Z" id="32014049">LGTM
</comment><comment author="bobrik" created="2014-01-10T09:42:54Z" id="32014275">Why maxOpenFileDescriptors and minOpenFileDescriptors are not 0 by default?
</comment><comment author="s1monw" created="2014-01-10T09:44:42Z" id="32014370">well first I think the update logic would be much more complicated if we set it to 0. If it is not initialized the resulted values are `-1` anyways so it won't make a difference and `Math.min` / `Math.max` reads much better IMO. So I think stuff like this should always be initialized like this and not with 0 as the invariant @bobrik 
</comment><comment author="bobrik" created="2014-01-10T10:48:03Z" id="32018033">@s1monw update logic stays the same, since we can only have positive number of open files. Here is more general solution, I got this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose min/max open file descriptors in Cluster Stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4681</link><project id="" key="" /><description>Currently we only return the average number of open file descriptors of all nodes (under the `avg_open_file_descriptors` key). The min/max values are more interesting to spot problems. Also for consistency, we should use the following format:

```
 "open_file_descriptors": {
      "min": 200,
      "max": 346,
       "avg": 273
 }
```
</description><key id="25383681">4681</key><summary>Expose min/max open file descriptors in Cluster Stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>breaking</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-10T09:28:19Z</created><updated>2014-01-10T10:12:55Z</updated><resolved>2014-01-10T10:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-10T10:11:33Z" id="32015848">@bleskes can we close this?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java</file></files><comments><comment>Expose min/max open file descriptors in Cluster Stats API</comment></comments></commit></commits></item><item><title>Feature request: add more information returned when using update api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4680</link><project id="" key="" /><description>The present update API has option to upsert document when it does not exist. The result is something like:
{"ok":true,"_index":"metadata","_type":"load","_id":"ind1","_version":1}

It does not tell us whether the document existed, or was upserted or fields were updated. If in the result we can have the following tunable via the update api:
- Difference between old and new values
- Whether document was upserted or updated
- List of fields that were updated, or that existed before

I'm not sure if this feature could be implemented as a plugin or by some other way.
</description><key id="25381620">4680</key><summary>Feature request: add more information returned when using update api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rhtyd</reporter><labels><label>discuss</label></labels><created>2014-01-10T08:35:39Z</created><updated>2015-03-27T11:32:35Z</updated><resolved>2015-03-27T11:32:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-10T09:18:07Z" id="32012915">I think you can know if the document has been updated or inserted looking at `version` field, right?

If `version` &gt; 1 then it's an update. Overwise, it's an insert, right?
</comment><comment author="rhtyd" created="2014-01-10T10:06:02Z" id="32015538">@dadoonet thanks. Yes using that we can know if document has been updated or inserted. But I also want to know if fields exited before and possible diff of previous and new values or if the field existed before after?
</comment><comment author="Plasma" created="2014-01-10T21:55:18Z" id="32070301">Related to #4420 
</comment><comment author="bleskes" created="2015-03-27T11:32:35Z" id="86907035">I think this last suggestion on https://github.com/elastic/elasticsearch/issues/4420#issuecomment-86890958 , makes #4420 a complete duplicate of this one. I'm going to close this. If I missed something please re-open or comment on #4420 ?

PS. there is also the http response code (200 vs 201 ) to tell if an upsert happened. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE while doing haschild query, intermittently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4679</link><project id="" key="" /><description>)],from[0],size[1]: Query Failed [failed to execute context rewrite]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
^C

And once a while i see the following

&gt; cache(_type:movie)],from[0],size[1]: Query Failed [failed to execute context rewrite]
&gt; at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)
&gt; at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
&gt; at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)
&gt; at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)
&gt; at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)
&gt; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
&gt; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
&gt; at java.lang.Thread.run(Thread.java:722)
&gt; Caused by: java.lang.NullPointerException
&gt; at org.elasticsearch.index.cache.id.simple.SimpleIdCache.refresh(SimpleIdCache.java:210)
&gt; at org.elasticsearch.index.search.child.HasChildFilter.contextRewrite(HasChildFilter.java:118)
&gt; at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:96)
&gt; ... 7 more
</description><key id="25374068">4679</key><summary>NPE while doing haschild query, intermittently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">govindm</reporter><labels><label>bug</label></labels><created>2014-01-10T04:01:31Z</created><updated>2014-04-18T07:29:16Z</updated><resolved>2014-04-18T07:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-10T07:02:58Z" id="32006836">Which version are you using?
</comment><comment author="govindm" created="2014-01-10T19:28:45Z" id="32057342">version: {
number: "0.90.2",
snapshot_build: false,
lucene_version: "4.3.1"
},
</comment><comment author="s1monw" created="2014-01-10T19:35:03Z" id="32057927">that is a pretty old version given the fact that the has_child has been rewritten twice since then. I guess it's wise to upgrade to the latest version. 
</comment><comment author="govindm" created="2014-01-10T19:42:31Z" id="32058633">which is the stable latest version that I can move to?
</comment><comment author="s1monw" created="2014-01-10T21:26:54Z" id="32067828">http://www.elasticsearch.org/download/ `0.90.10`
</comment><comment author="govindm" created="2014-01-10T22:45:59Z" id="32074484">Thanks a lot.
</comment><comment author="govindm" created="2014-01-12T06:42:35Z" id="32116579">I upgraded from 0.90.2 to 0.90.10 and I still see the NPE.

RemoteTransportException[[i-72d6065c][inet[/10.150.0.70:7102]][search/phase/query]]; nested: QueryPhaseExecutionException[[mmd][9]: query[filtered(filtered(movieId:[\* TO 0])-&gt;+CustomQueryWrappingFilter(child_filter[flags/movie](filtered%28countryCode:US%29-&gt;cache%28_type:flags%29)))-&gt;cache(_type:movie)],from[0],size[1]: Query Failed [Failed to execute main query]]; nested: RuntimeException[java.lang.NullPointerException]; nested: NullPointerException; }]
</comment><comment author="govindm" created="2014-01-12T06:57:43Z" id="32116769">Looks like similar issue(https://github.com/elasticsearch/elasticsearch/issues/3965) reported earlier in 0.90.5 and the issue is closed stating it will be addressed in the next release (90.6) but this issue is still showing in 0.90.10
</comment><comment author="dadoonet" created="2014-01-12T09:05:02Z" id="32118261">Could you post the full stacktrace please?
</comment><comment author="martijnvg" created="2014-01-14T12:05:36Z" id="32259272">@govindm Does it reproduce easily? Also can you share your query that you execute?
</comment><comment author="govindm" created="2014-01-15T00:47:55Z" id="32324440">This problem occurs often, but I couldn't find a pattern triggers it. Also the exception is logged only in debug mode and not in info level. I will try to bounce the ES server in debug mode and capture the stack trace.
</comment><comment author="govindm" created="2014-01-15T01:24:43Z" id="32326529">[2014-01-15 01:22:12,194][DEBUG][org.elasticsearch.action.search.type] [i-88c9a8a8] [51754] Failed to execute query phase
org.elasticsearch.transport.RemoteTransportException: [i-b059f291][inet[/10.166.46.221:7102]][search/phase/scan/scroll]
Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [mmd][8]: query[filtered(filtered(+(countryCode:US) +(movieId:[70140365 TO 70140365] movieId:[60023248 TO 60023248]))-&gt;CustomQueryWr
appingFilter(parent_filter[movie](filtered%28movieTypeCode:show movieTypeCode:standalone%29-&gt;cache%28_type:movie%29)))-&gt;cache(_type:flags)],from[0],size[10000]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:121)
        at org.elasticsearch.search.SearchService.executeScan(SearchService.java:215)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:791)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:780)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
        at org.elasticsearch.index.search.child.ParentConstantScoreQuery$ChildrenWeight$ChildrenDocIdIterator.match(ParentConstantScoreQuery.java:176)
        at org.apache.lucene.search.FilteredDocIdSetIterator.advance(FilteredDocIdSetIterator.java:71)
        at org.elasticsearch.index.search.child.ConstantScorer.advance(ConstantScorer.java:70)
        at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.advanceToNextCommonDoc(FilteredQuery.java:268)
        at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.advance(FilteredQuery.java:292)
        at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.score(FilteredQuery.java:247)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
        at org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:50)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:108)
        ... 7 more
</comment><comment author="martijnvg" created="2014-01-15T09:24:26Z" id="32345621">The stacktrace contains valuable information and I think this bug is related to #4703. I can reproduce the same NPE if I use the has_parent filter in the scan api. I have a fix for it and hopefully get it in soon.
</comment><comment author="govindm" created="2014-01-15T22:22:59Z" id="32422019">Thanks a lot Martijn
</comment><comment author="govindm" created="2014-02-03T23:29:18Z" id="34013688">I am getting NPE in non scan search type as well.

[2014-02-03 23:11:34,466][DEBUG][org.elasticsearch.action.search.type] [i-45d2a865] [mmd][3], node[LoO5A_v4TEaJHGgWUkjfDA], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4c9c6877]
org.elasticsearch.transport.RemoteTransportException: [i-05ec6c24][inet[/10.166.31.158:7102]][search/phase/query]
Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [mmd][3]: query[filtered(ConstantScore(child_filter[character_asset/jfk_character](+%28%28spaceCode:creative-services%29 %28sharedSpaceCodes:creative-services-originals%29%29 -deleted:true)))-&gt;cache(_type:jfk_character)],from[0],size[100]: Query Failed [failed to execute context rewrite]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:243)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
</comment><comment author="govindm" created="2014-02-03T23:30:18Z" id="34013771">ES version 

version: {
number: "0.90.3",
build_hash: "5c38d6076448b899d758f29443329571e2522410",
build_timestamp: "2013-08-06T13:18:31Z",
build_snapshot: false,
lucene_version: "4.4"
}
</comment><comment author="martijnvg" created="2014-02-10T19:54:01Z" id="34674714">There were various bugs fixed in the last months. I don't think this error occurs when you upgrade to ES `0.90.11`.
</comment><comment author="martijnvg" created="2014-04-18T07:29:16Z" id="40792289">Fixed in recent versions.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Create dedicated fielddata implementation for boolean fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4678</link><project id="" key="" /><description>Just like we have for bytes &amp; numerics, we should have a dedicated field data implementation for boolean values. This will be much more optimized memory-wise and overall performance-wise than what we have today (today, we treat booleans as bytes - `T` &amp; `F`)

This will also solve the formatting issues in the different APIs (eg. facets &amp; aggregations) where instead of returning `T` &amp; `F` we'll be able to return `true` &amp; `false`.

related to: #2462 
</description><key id="25372141">4678</key><summary>Create dedicated fielddata implementation for boolean fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">uboness</reporter><labels><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-01-10T02:49:21Z</created><updated>2015-05-29T16:03:06Z</updated><resolved>2015-04-02T14:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T10:24:18Z" id="48028895">@uboness any news on this issue?
</comment><comment author="uboness" created="2014-07-04T11:32:23Z" id="48033413">I believe @jpountz did some relevant work 
</comment><comment author="jpountz" created="2014-07-04T15:33:12Z" id="48056539">I still need to figure out whether we actually need a custom boolean field data API, or if only a different implementation would be required (using eg. the numeric API) and customizing the formatting in aggregations like we already do eg. for ip addresses or dates. I would lean towards the latter option. This will also help integration with Lucene APIs and eg. allow to have doc values support without any adapter.
</comment><comment author="clintongormley" created="2014-09-29T17:17:26Z" id="57195213">Also see #7851 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexNumericFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormat.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueFormatterStreams.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/format/ValueParser.java</file><file>src/test/java/org/elasticsearch/bwcompat/OldIndexBackwardsCompatibilityTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/core/BooleanFieldMapperTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/BooleanTermsTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Add doc values support to boolean fields.</comment></comments></commit></commits></item><item><title>License checker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4677</link><project id="" key="" /><description>Hooking up license maven plug in 

http://code.mycila.com/license-maven-plugin/

do 

1) mvn license:check to check.
2) mvn license:help for help
3) mvn license:fomat for license header override and update
</description><key id="25360072">4677</key><summary>License checker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrsolo</reporter><labels /><created>2014-01-09T22:10:24Z</created><updated>2014-06-26T12:13:07Z</updated><resolved>2014-01-10T21:38:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrsolo" created="2014-01-10T21:38:04Z" id="32068794">Done

https://github.com/elasticsearch/elasticsearch/commit/2df42e4460fa5f0d87363672e305c88d9f30b093
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Multi data path config can cause a shard to be perceived as corrupted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4676</link><project id="" key="" /><description>Multi data path config support writes a file to a data location based on the available size (by default). There is a Lucene file called segments.gen that has the same name, and only in that case, we need to make sure we alway write it to the same data location, otherwise, the index will have multiple segments.gen files, and the shard can seem to be corrupted.

The message if this case happens is that segments_xxx file was not found, in which case, a find for segments.gen can yield multiple files. Deleting the segments.gen files will cause the shard to recover properly (as its an extra protection layer to resolve the segments header by Lucene)

Make sure the segments.gen file is writtne to the same directory every time
fixes #4674
</description><key id="25359219">4676</key><summary>Multi data path config can cause a shard to be perceived as corrupted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-01-09T21:58:04Z</created><updated>2014-06-12T08:30:24Z</updated><resolved>2014-01-09T22:16:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-09T22:03:32Z" id="31981828">+1 commit LGTM
</comment><comment author="kimchy" created="2014-01-09T22:16:59Z" id="31982988">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Updated the documentation urls in the rest api spec.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4675</link><project id="" key="" /><description /><key id="25358384">4675</key><summary>Updated the documentation urls in the rest api spec.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels /><created>2014-01-09T21:45:46Z</created><updated>2014-07-16T21:49:52Z</updated><resolved>2014-01-09T21:46:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #4675 from spenceralger/update_spec_doc_urls</comment></comments></commit></commits></item><item><title>Multi data path config can cause a shard to be perceived as corrupted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4674</link><project id="" key="" /><description>Multi data path config support writes a file to a data location based on the available size (by default). There is a Lucene file called `segments.gen` that has the same name, and only in that case, we need to make sure we alway write it to the same data location, otherwise, the index will have multiple segments.gen files, and the shard can seem to be corrupted.

The message if this case happens is that segments_xxx file was not found, in which case, a find for segments.gen can yield multiple files. Deleting the segments.gen files will cause the shard to recover properly (as its an extra protection layer to resolve the segments header by Lucene)
</description><key id="25357737">4674</key><summary>Multi data path config can cause a shard to be perceived as corrupted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-09T21:36:27Z</created><updated>2014-06-18T21:08:25Z</updated><resolved>2014-01-09T22:05:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sbarton" created="2014-01-15T18:05:41Z" id="32391666">Hi,

I was using ES 0.90.7 and my shards were after restart of the cluster going missing. Physically, I can see the files (in 6 cases out of 8) still on the disk, but the shards won't come up with the indication of a segments_X file missing. I have followed your advice of removing the segments.gen file in order to recover, but the shards are not coming up. Even after restart of the cluster, the shards are not coming up, the error is now:

[2014-01-15 18:46:00,504][DEBUG][cluster.service          ] [Synch] processing [shard-failed ([1millionnewv2][4], node[fWigetX5QNar2zIpvkEK_Q], [P], s[INITIALIZING]), reas
on [Failed to start shard, message [IndexShardGatewayRecoveryException[[1millionnewv2][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRe
coveryException[[1millionnewv2][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_s41.nvd, _twd_es090_0.tip, _um4.nvm,... ]]; nested: IndexNotF
oundException[no segments\* file found in store(least_used[rate_limited(niofs(/0/elasticsearch/elasticsearch/nodes/0/indices/1millionnewv2/4/index), type=MERGE, rate=20.0),
 rate_limited(niofs(/1/elasticsearch/elasticsearch/nodes/0/indices/1millionnewv2/4/index), type=MERGE, rate=20.0), rate_limited(niofs(/2/elasticsearch/elasticsearch/nodes/
0/indices/1millionnewv2/4/index), type=MERGE, rate=20.0), rate_limited(niofs(/3/elasticsearch/elasticsearch/nodes/0/indices/1millionnewv2/4/index), type=MERGE, rate=20.0)]
): files: [ .... ] ]]]: no change in cluster_state

What can I do if the usual strategy of the segments.gen file removal doesn't work? I have even updated the ES to the latest 0.90.10 version but the restart problems are still the same and the shards are not coming up. I have lost a considerable amount of data, in those 2 shards that were wiped out, but I could still save a lot of data in the 6 shards I am could recover.
</comment><comment author="kimchy" created="2014-01-20T15:57:20Z" id="32771392">@sbarton are you sure you deleted the segments.gen from all data directories for the relevant shard ([1millionnewv2][4]) (or, better yet, just delete it recursively across all data locations)? the failure suggests you potentially didn't.
</comment><comment author="sbarton" created="2014-02-06T09:35:48Z" id="34306121">@kimchy I have made sure I removed the segments.gen from all data directories (using find command) but still no luck. I at the end gave up (I had more shards in the same situation on other machines - tried the same approach but none of them came back) and re-indexed the whole thing once again. But I can say, that the 0.90.10 version is not loosing shards even after several harsh restarts.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/store/Store.java</file></files><comments><comment>Multi data path config can cause a shard to be perceived as corrupted</comment><comment>Multi data path config support writes a file to a data location based on the available size (by default). There is a Lucene file called segments.gen that has the same name, and only in that case, we need to make sure we alway write it to the same data location, otherwise, the index will have multiple segments.gen files, and the shard can seem to be corrupted.</comment></comments></commit></commits></item><item><title>Extend defaults for debian / rpm packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4673</link><project id="" key="" /><description>The defaults for the deb / rpm packages could contain a setting which controls the startup of the elasticseach node. This may be useful when packages are installed due to library depenencies but no running cluster node is needed.
</description><key id="25344203">4673</key><summary>Extend defaults for debian / rpm packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">p0lar</reporter><labels><label>:Packaging</label></labels><created>2014-01-09T19:14:58Z</created><updated>2014-07-18T11:23:54Z</updated><resolved>2014-07-18T11:23:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-13T08:18:46Z" id="32151203">Elasticsearch 1.0 will not start by default when used with the packages. You will manually add to to your startup system. Does that help?
</comment><comment author="p0lar" created="2014-01-20T21:18:00Z" id="32797868">Sounds good. However, the debian package does contain an init.d startup already, so the script could obey settings in the default file.
</comment><comment author="spinscale" created="2014-07-18T11:23:54Z" id="49420694">Closing this. I think using `update-rc.d` and similar tools are sufficient to disable/enable the elasticsearch init script, there is no need to recreate the same behaviour in the `default`/`sysconfig` files.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cache estimated sizes of per-segment per-field field data to short-circuit circuit breaker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4672</link><project id="" key="" /><description>It could be helpful to have something like a `Map&lt;SegmentReader.CoreCacheKey|FieldName, Long&gt;` cache so that multiple requests all attempting to load field data for the same field can first check to see whether the there is enough space for the data, and short-circuit if not.

This should help for multiple requests so we don't have to continually estimate if we know it's going to break regardless.
</description><key id="25336065">4672</key><summary>Cache estimated sizes of per-segment per-field field data to short-circuit circuit breaker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>enhancement</label></labels><created>2014-01-09T17:28:40Z</created><updated>2015-11-20T14:11:39Z</updated><resolved>2015-10-14T13:37:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T20:07:20Z" id="37457466">@dakrone any ETA for this? I am moving to `1.2` for now
</comment><comment author="dakrone" created="2014-03-13T00:09:44Z" id="37484824">I haven't started on this yet, so 1.2 is a good place for it.
</comment><comment author="s1monw" created="2014-07-04T12:25:11Z" id="48038082">@dakrone are we still working on this?
</comment><comment author="dakrone" created="2014-07-04T12:35:39Z" id="48039118">I haven't started on this, moving to 2.0.0 for now.
</comment><comment author="clintongormley" created="2015-10-14T13:37:10Z" id="148051294">In memory fielddata is going to be removed for all fields that support doc values (#14113).  I think we can close this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't require mapping if specified in the request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4671</link><project id="" key="" /><description>This fixes #4483 and is backwards-compatible with our current behavior.
</description><key id="25334325">4671</key><summary>Don't require mapping if specified in the request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2014-01-09T17:03:18Z</created><updated>2014-06-16T04:43:27Z</updated><resolved>2014-01-13T16:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-11T19:27:51Z" id="32104596">the main problem here is that a top level mapping can have more than just `properties` as top level object. Also run tests where, for example, the mapping is:

```
{
   "_routing" : { .... },
   "properties" : { ...}
}
```

you will see that a few lines above, there is  a check that checks for a single top level object, and fails with an exception that says it must be a type. if this change is in, this needs to be fixed as well.

I commented on this issue, but we _have_ to make this change backward comp. to support older indices.
</comment><comment author="dakrone" created="2014-01-13T04:11:31Z" id="32144540">@kimchy I've updated this PR so that it supports multiple top-level objects and is entirely backwards-compatible.
</comment><comment author="s1monw" created="2014-01-13T09:48:53Z" id="32155758">I think this patch looks good. The only thing that I think we should improve is the nameing of the commit, I had to go to the issues to figure out what this is about :)
</comment><comment author="dakrone" created="2014-01-13T16:35:19Z" id="32185768">Merged in https://github.com/elasticsearch/elasticsearch/commit/2341825358e740b0bea4c16d164c5acdf12fc6b3
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cat/allocation diskRatio calc wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4670</link><project id="" key="" /><description>```
shards diskUsed diskAvail diskRatio host       ip           node
     6    257gb     208gb    123.6% iota.local 192.168.56.1 Cat-Man
     6                                                      UNASSIGNED
```
</description><key id="25332091">4670</key><summary>cat/allocation diskRatio calc wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>bug</label><label>v1.0.0</label></labels><created>2014-01-09T16:34:16Z</created><updated>2014-01-14T21:24:56Z</updated><resolved>2014-01-14T21:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file></files><comments><comment>Fix disk percent used calc in cat/allocation</comment></comments></commit></commits></item><item><title>[docs] Span Or Query page looks funky on Firefox</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4669</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-span-or-query.html looks funky on Firefox:

![span_or_query](https://f.cloud.github.com/assets/215970/1879310/248fa398-7948-11e3-8dca-6c8811f08764.png)
</description><key id="25330167">4669</key><summary>[docs] Span Or Query page looks funky on Firefox</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-09T16:07:51Z</created><updated>2014-01-09T16:30:10Z</updated><resolved>2014-01-09T16:30:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-09T16:08:07Z" id="31947065">The menu bleeds into the footer.
</comment><comment author="clintongormley" created="2014-01-09T16:09:22Z" id="31947188">Gaah - what did I break...

/me goes on the hunt
</comment><comment author="clintongormley" created="2014-01-09T16:30:10Z" id="31949547">Fixed - many thanks @nik9000 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Disabling allow_explicit_index breaks bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4668</link><project id="" key="" /><description>The documentation on URL-based access control implies that bulk operations still work if you set `rest.action.multi.allow_explicit_index: false`, as long as you specify the index in the URL. However, it doesn't work:

```
POST /foo/bar/_bulk
{ "index": {} }
{ "_id": 1234, "baz": "foobar" }
```

returns 

```
explicit index in bulk is not allowed
```

See https://groups.google.com/forum/#!topic/elasticsearch/TLaTkaJjGYg for the discussion.
</description><key id="25327732">4668</key><summary>Disabling allow_explicit_index breaks bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">gabegorelick</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-09T15:38:13Z</created><updated>2014-10-22T18:46:13Z</updated><resolved>2014-01-09T16:11:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gabegorelick" created="2014-01-09T17:55:29Z" id="31958658">I can confirm this fixes it. Thanks for the quick resolution. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java</file></files><comments><comment>Disabling allow_explicit_index breaks bulk</comment><comment>fixes #4668</comment></comments></commit></commits></item><item><title>Rename RobinEngine and friends to InternalEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4667</link><project id="" key="" /><description>Closes #4633
</description><key id="25324672">4667</key><summary>Rename RobinEngine and friends to InternalEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-09T14:58:06Z</created><updated>2014-06-26T15:05:17Z</updated><resolved>2014-01-13T15:05:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-13T14:41:51Z" id="32174371">Looks like there are 2 leftovers in InternalIndexShard.java ("engine.robin.refresh_interval"). Otherwise +1!
</comment><comment author="s1monw" created="2014-01-13T14:43:24Z" id="32174499">@jpountz see https://github.com/elasticsearch/elasticsearch/issues/4706
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>improve buffered output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4666</link><project id="" key="" /><description>- use the same buffer size if wrapping a buffered output
- directly call flush on the checksum wrapper
</description><key id="25311738">4666</key><summary>improve buffered output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-01-09T11:18:35Z</created><updated>2014-07-16T21:49:53Z</updated><resolved>2014-01-09T11:36:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-09T11:26:29Z" id="31922148">LGTM
</comment><comment author="jpountz" created="2014-01-09T11:45:31Z" id="31923290">One question about flush, but otherwise this looks good to me too.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Put DestructiveOperations logic into a guice component or merge it into an existing component</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4665</link><project id="" key="" /><description>The `DestructiveOperations` class is a statefull helper, but initialized several times in the transport actions that rely on its logic. This should become a guice component or merge into an existing component, so it will initialized only once. The same applies for `AutoCreateIndex` class.

Perhaps both classes can be merged into a single component.
</description><key id="25310349">4665</key><summary>Put DestructiveOperations logic into a guice component or merge it into an existing component</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-01-09T10:59:53Z</created><updated>2015-08-31T19:22:55Z</updated><resolved>2015-08-31T19:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-29T09:01:13Z" id="87374900">We briefly discussed this, seems like a valuable improvement, made it an adoptme and low hanging fruit.
</comment><comment author="jimhooker2002" created="2015-08-19T21:35:43Z" id="132793807">This looks like the fix to me: https://github.com/jimhooker2002/elasticsearch/compare/master...jimhooker2002:issue-4665

I'll rebase and submit a pull request unless anyone has any comments before I do that.
</comment><comment author="jimhooker2002" created="2015-08-20T06:46:37Z" id="132909185">I've pinged https://github.com/s1monw over email (original submitter of the code) to take a quick look at my diff and help ensure some level of sanity.  I'm a little bit worried about the comment I removed, specifically the part about repeated log statements.  Hopefully Simon can verify I didn't just introduce a regression.
</comment><comment author="jimhooker2002" created="2015-08-21T07:05:15Z" id="133311749">OK, I've rebased to my repos master branch and this is now part of an existing pull request.

https://github.com/elastic/elasticsearch/pull/12968
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/action/ActionModule.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>core/src/main/java/org/elasticsearch/action/support/DestructiveOperations.java</file></files><comments><comment>Merge pull request #13046 from jimhooker2002/issue-4665-clean</comment></comments></commit></commits></item><item><title>Deprecate document boost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4664</link><project id="" key="" /><description>The document boost is a nice feature but since it was removed from lucene 4.0, the way it works in elasticsearch is by adding fields boosts to each field, multiplying it with the original field boost. Here is the interesting commit: https://github.com/elasticsearch/elasticsearch/commit/c60f20413b299e4d9ea0a5fa3e24381e90d914b8#diff-7117c679a1ca0d5002c0c9b9ef8bad16 . That is not exactly how the document boost should work, it has downsides and the same result can be obtained using function_score.

For the above reasons we are going to deprecate the document boost.
</description><key id="25307225">4664</key><summary>Deprecate document boost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2014-01-09T10:09:08Z</created><updated>2014-09-29T14:24:59Z</updated><resolved>2014-01-09T15:04:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Deprecate document _boost</comment></comments></commit></commits></item><item><title>Future may never complete for concurrent update and delete requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4663</link><project id="" key="" /><description>Suppose that two client threads make a concurrent attempt to update and delete the same document. Then, depending on the timing, the thread attempting the update may block forever, waiting on a future that never completes. This seems to be a bug, either on the server or in the client.

I have created a test program which you can find here to reproduce the problem: https://gist.github.com/peschlowp/8331596

The test program inserts 100 documents into an index, then starts two threads which concurrently try to update or delete these 100 documents. The updating thread regularly blocks on the future that completes the update request. Instead I would expect the future to complete with success or failure.

The behavior seems to be connected to the retryOnConflict parameter of the update request (the test program provides a constant to quickly toggle it):
- When retryOnConflict is set to a value larger than zero, the problem may occur.
- When retryOnConflict is set to zero, the respective call fails with a VersionConflictEngineException instead and does not block.

I'm using ElasticSearch 0.90.9 and haven't checked the behavior with previous versions.

Let me know if more explanation or input is needed.
</description><key id="25306706">4663</key><summary>Future may never complete for concurrent update and delete requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels /><created>2014-01-09T10:00:06Z</created><updated>2014-05-31T11:12:17Z</updated><resolved>2014-05-31T11:12:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="peschlowp" created="2014-05-31T11:12:17Z" id="44745498">Replaced by #6355.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregations: add a `min_doc_count` option to terms and histogram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4662</link><project id="" key="" /><description>Right now terms aggregations may return terms that match one hit or more. The purpose of the `min_doc_count` option is to make it configurable. For example, if

``` javascript
{
    "aggs" : {
        "tags" : {
            "terms" : {
                "field" : "tag"
            }
        }
    }
}
```

returns

``` javascript
{
    ...

    "aggregations" : {
        "tags" : {
            "buckets" : [
                {
                    "key" : "search",
                    "doc_count" : 115
                },
                {
                    "key" : "java",
                    "doc_count" : 50
                },
                {
                    "key" : "concurrency",
                    "doc_count" : 12
                }
            ]
        }
    }
}
```

then

``` javascript
{
    "aggs" : {
        "tags" : {
            "terms" : {
                "field" : "tag",
                "min_doc_count": 50
            }
        }
    }
}
```

would return

``` javascript
{
    ...

    "aggregations" : {
        "tags" : {
            "buckets" : [
                {
                    "key" : "search",
                    "doc_count" : 115
                },
                {
                    "key" : "java",
                    "doc_count" : 50
                }
            ]
        }
    }
}
```

The special case `min_doc_count: 0` will behave similarly to the [`all_terms` option of facets](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_all_terms) and also return terms that don't match any hit. For example, we could have the following response:

``` javascript
{
    ...

    "aggregations" : {
        "tags" : {
            "buckets" : [
                {
                    "key" : "search",
                    "doc_count" : 115
                },
                {
                    "key" : "java",
                    "doc_count" : 50
                },
                {
                    "key" : "concurrency",
                    "doc_count" : 12
                },
                {
                    "key" : "unit testing",
                    "doc_count" : 0
                },
                {
                    "key" : "performance",
                    "doc_count" : 0
                }
            ]
        }
    }
}
```

Histograms are going to support this option as well and the [`empty_bucket` option](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-histogram-aggregation.html) will be removed in favor of `min_doc_count: 0`.
</description><key id="25274162">4662</key><summary>Aggregations: add a `min_doc_count` option to terms and histogram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v1.0.0.RC1</label></labels><created>2014-01-08T22:12:17Z</created><updated>2014-01-22T12:01:28Z</updated><resolved>2014-01-13T09:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2014-01-08T22:48:20Z" id="31884735">If min_doc_count is going to be 0 by default it would be great.
</comment><comment author="uboness" created="2014-01-08T22:55:07Z" id="31885315">@bobrik currently the plan is to have `1` as the default as `0` comes with extra perf. costs and it's not necessarily the right default (really use case dependent)
</comment><comment author="bobrik" created="2014-01-08T22:57:53Z" id="31885556">Sorry, I actually meant 1.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/collect/Iterators2.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/AbstractHistogramBase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalDateHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalHistogram.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/UnmappedTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/UnmappedTermsAggregator.java</file><file>src/test/java/org/elasticsearch/common/collect/Iterators2Tests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DateRangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/FilterTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/IPv4RangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MinDocCountTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/MissingTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/NestedTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/AvgTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/ExtendedStatsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/MaxTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/MinTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/StatsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/metrics/SumTests.java</file></files><comments><comment>Add new option `min_doc_count` to terms and histogram aggregations.</comment></comments></commit></commits></item><item><title>Create standard gc and memory_pool names for Jvm stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4661</link><project id="" key="" /><description>Translate the Java names (for all the different types) for GC collectors and memory pool names in Jvm stats into `young`, `survivor`, `old`. This is a breaking change in the node stats API.

In order not to loose the original names, they should be added to the JVM info API.
</description><key id="25268383">4661</key><summary>Create standard gc and memory_pool names for Jvm stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-08T20:15:21Z</created><updated>2014-04-07T17:10:20Z</updated><resolved>2014-01-08T20:16:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/jvm/GcNames.java</file><file>src/main/java/org/elasticsearch/monitor/jvm/JvmInfo.java</file><file>src/main/java/org/elasticsearch/monitor/jvm/JvmMonitorService.java</file><file>src/main/java/org/elasticsearch/monitor/jvm/JvmStats.java</file></files><comments><comment>Create standard gc and memory_pool names for Jvm stats</comment><comment>fixes #4661</comment></comments></commit></commits></item><item><title>Beta2 Missing FromString annotation for joda/time/DateTime.class when compiling in Scala</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4660</link><project id="" key="" /><description>I'm using `"org.elasticsearch" % "elasticsearch" % "1.0.0.Beta2"` in my scala project and hitting the following error on compile:

```
[info] Compiling 183 Scala sources to /home/dev/projects/core/target/scala-2.10/classes...
[warn] Class org.elasticsearch.common.joda.convert.FromString not found - continuing with a stub.
[warn] Caught: java.lang.NullPointerException while parsing annotations in /home/.ivy2/cache/org.elasticsearch/elasticsearch/jars/elasticsearch-1.0.0.Beta2.jar(org/elasticsearch/common/joda/time/DateTime.class)
[error] error while loading DateTime, class file '/home/.ivy2/cache/org.elasticsearch/elasticsearch/jars/elasticsearch-1.0.0.Beta2.jar(org/elasticsearch/common/joda/time/DateTime.class)' is broken
[error] (class scala.MatchError/p (of class java.lang.Character))
```
</description><key id="25264968">4660</key><summary>Beta2 Missing FromString annotation for joda/time/DateTime.class when compiling in Scala</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dbachelder</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-08T19:24:03Z</created><updated>2014-08-19T13:23:00Z</updated><resolved>2014-01-08T21:14:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dbachelder" created="2014-01-08T19:45:41Z" id="31870516">Joda 2.3 has this in the pom

```
  &lt;dependency&gt;
      &lt;groupId&gt;org.joda&lt;/groupId&gt;
      &lt;artifactId&gt;joda-convert&lt;/artifactId&gt;
      &lt;version&gt;1.2&lt;/version&gt;
      &lt;scope&gt;compile&lt;/scope&gt;
      &lt;optional&gt;true&lt;/optional&gt;&lt;!-- mandatory in Scala --&gt;
    &lt;/dependency&gt;
```

and I can see that the DateTime object is depending on one of the annotations found therein. So, I think for the ES stuff to work correctly in scala land, the joda-convert library will also need to be shaded into the final jar.
</comment><comment author="dbachelder" created="2014-01-08T20:05:28Z" id="31872503">The easy temporary fix is to create the missing file (`org.elasticsearch.common.joda.convert.FromString`) in my own project structure with the same contents as in the joda-convert project:

```
package org.elasticsearch.common.joda.convert;


import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target({ElementType.METHOD, ElementType.CONSTRUCTOR })
@Retention(RetentionPolicy.RUNTIME)
public @interface FromString {
}
```
</comment><comment author="kimchy" created="2014-01-08T21:13:03Z" id="31878442">I will fix it, will shard joda convert as well....
</comment><comment author="splatch" created="2014-04-01T17:11:31Z" id="39231726">I think scala is broken then - if annotation is missing on classpath it should be skipped. That's part of Java specification..
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>add joda-convert so missing annotations in joda-time will not cause failures when used as dependency</comment><comment>fixes #4660</comment></comments></commit></commits></item><item><title>Add warning phrase suggester's max_errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4659</link><project id="" key="" /><description>Setting this too high can consume tons of time.  I'm not sure of the formal
run time but I'll wager max_errors factorial is in there somewhere.
</description><key id="25261810">4659</key><summary>Add warning phrase suggester's max_errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-08T18:35:56Z</created><updated>2014-06-16T03:22:47Z</updated><resolved>2014-01-08T22:10:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-08T20:11:09Z" id="31873027">+1 thanks
</comment><comment author="s1monw" created="2014-01-08T22:10:43Z" id="31881950">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>remove default `_all` for `type` and `index` if these are missing in RES...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4658</link><project id="" key="" /><description>...T tests

If a type or path is missing in the REST test yaml file, it is
automatically replaced with _all. This makes it hard to test changes
in the api, for example adding the possibility to leave the index
blank in addition to _all and \* in the uri.

closes #4657
</description><key id="25258466">4658</key><summary>remove default `_all` for `type` and `index` if these are missing in RES...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-01-08T17:45:57Z</created><updated>2014-07-16T21:49:54Z</updated><resolved>2014-01-09T09:18:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-01-08T19:46:10Z" id="31870568">Looks good!
</comment><comment author="brwe" created="2014-01-09T09:18:48Z" id="31914059">pushed e623f61 (0.90) and 216c814 (master)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>remove default `_all` for `type` and `index` if these are missing in REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4657</link><project id="" key="" /><description>If a `type` or `path` is missing in the REST test yaml file, it is automatically replaced with `_all`. This makes it hard to test changes in the api, for example adding the possibility to leave the index blank in addition to `_all` and `*` in the uri. 
</description><key id="25258113">4657</key><summary>remove default `_all` for `type` and `index` if these are missing in REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-08T17:41:00Z</created><updated>2014-01-09T09:18:14Z</updated><resolved>2014-01-09T09:17:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/spec/RestApi.java</file></files><comments><comment>remove default `_all` for `type` and `index` if these are missing in REST tests</comment></comments></commit></commits></item><item><title>River does not start when using config/templates files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4656</link><project id="" key="" /><description>From elasticsearch 0.90.6, when you have templates files defined in `config/templates` dir, rivers don't start anymore.

Steps to reproduce:

Create `config/templates/default.json`:

``` javascript
{
  default:
  {
    "template" : "*",
    "mappings" : {
      "_default_" : {
      }
    }
  }
}
```

Start a dummy river:

``` sh
curl -XPUT 'localhost:9200/_river/my_river/_meta' -d '{ "type" : "dummy" }'
```

It gives:

```
[2014-01-01 22:08:38,151][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings [_default_]
[2014-01-01 22:08:38,239][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:38,245][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)
[2014-01-01 22:08:38,250][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:39,244][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:39,252][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:40,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:40,254][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:41,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:41,255][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:42,249][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts
[2014-01-01 22:08:42,257][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts
```

With elasticsearch 0.90.2 or with no template file in `config/templates` dir, it gives:

```
[2014-01-01 22:22:32,096][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings []
[2014-01-01 22:22:32,221][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)
[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] create
[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] start
[2014-01-01 22:22:32,234][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)
```

Closes #4577.
</description><key id="25257714">4656</key><summary>River does not start when using config/templates files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-01-08T17:36:28Z</created><updated>2014-06-12T20:22:10Z</updated><resolved>2014-01-20T17:04:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-01-10T15:43:50Z" id="32037732">Beyond the above comment, I'm a bit worried about unnecessary scheduled retries (not caused by this PR, but something I realized looking at it), that would be caused by having at least a river and one or more index templates registered on `*`. Maybe we should just skip applying index templates to the `_river` index?
</comment><comment author="javanna" created="2014-01-17T16:43:15Z" id="32621834">Hey @dadoonet thanks for updating the PR, I left a few comments but we are on the right track!
</comment><comment author="javanna" created="2014-01-20T14:57:43Z" id="32765831">I left a minor comment, apart from that LGTM
</comment><comment author="kimchy" created="2014-01-20T15:37:12Z" id="32769334">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file><file>src/test/java/org/elasticsearch/river/RiverTests.java</file></files><comments><comment>River does not start when using config/templates files</comment></comments></commit></commits></item><item><title>Deduplicated names list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4655</link><project id="" key="" /><description>Removed 55 duplicate entries from the list
</description><key id="25243775">4655</key><summary>Deduplicated names list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">guerda</reporter><labels /><created>2014-01-08T14:39:54Z</created><updated>2014-07-16T21:49:55Z</updated><resolved>2014-01-08T18:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nicl" created="2014-01-08T17:42:25Z" id="31858402">Why did we lose Zebediah Killgrave?!!!
</comment><comment author="guerda" created="2014-01-08T17:51:33Z" id="31859330">nicl: We didn't lose him, he can be found in line 1381
</comment><comment author="nicl" created="2014-01-08T18:05:40Z" id="31860803">Ah sorry! Phew!
</comment><comment author="kimchy" created="2014-01-08T18:09:47Z" id="31861181">haha, I think there is one that goes a step further: #3263
</comment><comment author="HonzaKral" created="2014-01-08T18:26:18Z" id="31862751">I just merged #3263 via https://github.com/elasticsearch/elasticsearch/commit/b7a5537d8397e6bb525988569d05976702547a1e which also includes this change.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>multi_field on object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4654</link><project id="" key="" /><description>Using a mapping containing a multi_field with no "fields" field allows to send documents containing a json object for this field's value. 
The document is then stored but not indexed correctly, any query or filter made against a particular field will not work, but when using match_all or a direct get, the _source contains the field.

The documentation tells that a multi_field can only be used to map core types http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-multi-field-type.html#mapping-multi-field-type , so sending a document containing an object in the multi_field should not have succeeded.

Maybe creating a multi_field mapping could also check that the "field" property of the multi_field is present, if there is no particular use case for creating a multi_field with no "field".

See the shell script below to reproduce : 

``` bash
#!/bin/sh

ELASTICSEARCH=localhost:9200
INDEX=index

if [ -z "$INDEX" ]; then
    echo the $INDEX var should be set in the script
    exit 1
fi

cat &lt;&lt;EOF

delete existing index
EOF
curl -XDELETE $ELASTICSEARCH/$INDEX

cat &lt;&lt;EOF

create index
EOF
curl -XPUT $ELASTICSEARCH/$INDEX

cat &lt;&lt;EOF

put mapping with a multi_field with no "fields" declared
EOF
curl -XPOST $ELASTICSEARCH/$INDEX/type/_mapping -d '{
    "type": {
        "properties": {
            "correct_field": {
                "type" : "double"
            },
            "accessories": {
                "type" : "multi_field"
            }
        }
    }
}
'

cat &lt;&lt;EOF

insert a document with an object in the multi_field
EOF
curl -XPUT $ELASTICSEARCH/$INDEX/type/1 -d '{
  "accessories": {
    "bar":"baz"
  }
}'

cat &lt;&lt;EOF

refresh before search
EOF
curl -XPOST $ELASTICSEARCH/$INDEX/_refresh

cat &lt;&lt;EOF

search all, take a look a the _source
EOF
curl -XPOST $ELASTICSEARCH/$INDEX/type/_search?pretty=true -d '{
  "version": true,
  "query": {
      "filtered": {
         "query": {
            "match_all": {}
         }
      }
  }
}'

cat &lt;&lt;EOF

search with an exists filter on "accessories" 
the "accessories" field exists, see the _source in the previous response, 
why doesn t it pass through the exists filter ?
EOF
curl -XPOST $ELASTICSEARCH/$INDEX/type/_search?pretty=true -d '{
  "version": true,
  "query": {
      "filtered": {
         "query": {
            "match_all": {}
         }, "filter": {
             "exists": {
                "field": "accessories"
             }
         }
      }
  }
}'

cat &lt;&lt;EOF

update
EOF
curl -XPOST $ELASTICSEARCH/$INDEX/type/1/_update -d '{
    "doc": {
        "correct_field": "12"
    }
}'

cat &lt;&lt;EOF

refresh before search
EOF
curl -XPOST $ELASTICSEARCH/$INDEX/_refresh

cat &lt;&lt;EOF

the "correct_field" field is not missing, see the _source in the response, why does it pass through the missing filter ?
it seems that the document is not correctly index on other fields
EOF
curl -XPOST $ELASTICSEARCH/$INDEX/type/_search?pretty=true -d '{
    "version": true,
    "query": {
      "filtered": {
         "query": {
            "match_all": {}
         }, "filter": { 
             "missing": {
                "field": "correct_field"
             }
         }
      }
  }
}'


cat &lt;&lt;EOF

in get mapping we see that the accessories.bar field is at the root of the document, where we would expect it to be in accessories.bar
EOF
curl -XGET $ELASTICSEARCH/$INDEX/_mapping?pretty
```
</description><key id="25239128">4654</key><summary>multi_field on object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">obourgain</reporter><labels /><created>2014-01-08T13:30:05Z</created><updated>2014-12-24T18:04:57Z</updated><resolved>2014-12-24T18:04:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T18:04:57Z" id="68067254">Hi @obourgain 

Sorry it has taken a while to look at this.  The issue you describe has already been fixed, so I'm going to close this issue

thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>0.14</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4653</link><project id="" key="" /><description /><key id="25230047">4653</key><summary>0.14</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">juliedabbs</reporter><labels /><created>2014-01-08T10:13:44Z</created><updated>2014-07-16T21:49:56Z</updated><resolved>2014-01-08T10:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-08T10:20:10Z" id="31819653">kind of outdated I guess...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change the default recycler type for page-based recycling.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4652</link><project id="" key="" /><description>Page-based recycling is not thread-local anymore but instead there are several
pools of objects to recycle that threads may use depending on their hashCode.
Each pool is protected by its own lock so up to ${number of pools} threads may
recycle objects concurrently.

Recyclers have also been refactored for better composability, for example there
is a soft recycler that creates a recycler that wraps data around a
SoftReference and a thread-local recycler that can take any factory or recyclers
and instantiates a dedicated instance per thread.

RecyclerBenchmark has been added to try to figure out the overhead of object
recycling depending on the recycler type and the number of threads trying to
recycle objects concurrently.

Close #4647
</description><key id="25225793">4652</key><summary>Change the default recycler type for page-based recycling.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-01-08T08:45:19Z</created><updated>2014-06-19T15:36:07Z</updated><resolved>2014-01-09T15:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-08T16:20:32Z" id="31849409">New commit pushed:
- removed unused methods
- use jsr166y.ConcurrentLinkedDeque instead of Java7's (otherwise compilation fails under Javac 6)
- modified benchmark to not take into account thread start-up
- divided DequeRecycler into a thread-safe ConcurrentDequeRecycler that uses an AtomicInteger to track its size and a NOT thread-safe DequeRecycler that just uses the .size() method
- use Thread.getId instead of Thread.hashCode to compute the recycler to use in the concurrent recycler
- use SOFT_CONCURRENT as a default for CacheRecycler too (in addition to PageCacheRecycler)

And here are updated results of the benchmark (no change):
![pouet](https://f.cloud.github.com/assets/299848/1869934/7b063368-7880-11e3-8fca-41f56b30a396.png)
</comment><comment author="kimchy" created="2014-01-08T17:10:42Z" id="31854943">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add support for 'disable primary shard nodes option' in datacenter crossing case</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4651</link><project id="" key="" /><description>Sometimes we need to do write request in one datacenter for low latency and another datacenter we just provide read operateion.
On the other hand ,we just have one elasticsearch cluster because of we want to use elastisearch self replication strategy.
Because of these reasons, we want to specify primary shard nodes . 
For example ,we want some nodes  in west would not be assigned primary shards and the nodes in east assigned all primary shards.
Please ignore the "NOTICE.txt" changes. 
</description><key id="25219138">4651</key><summary>Add support for 'disable primary shard nodes option' in datacenter crossing case</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">3h-william</reporter><labels><label>discuss</label></labels><created>2014-01-08T06:23:13Z</created><updated>2014-08-08T08:58:51Z</updated><resolved>2014-08-08T08:58:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T08:58:51Z" id="51578376">Hi @3h-william 

Thanks for the PR, but we really don't recommend running a cluster across data centres, even only with replicas on one side.  Replication is synchronous and any latency will be problematic.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>isCloseConnectionException returns false on unexpected exceptions (i18n problem)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4650</link><project id="" key="" /><description>In non-English environment, org.elasticsearch.common.transport.NetworkExceptionHelper#isCloseConnectionException method fails to check an exception(so, returns false..). isCloseConnectionException method checks an exception by English message, but the message is translated in non-English environment. For example, "Connection reset by peer" is translated into Japanese if you runs elasticsearch on ja_JP.UTF-8 locale. Therefore, unexpected log messages are printed.
</description><key id="25212975">4650</key><summary>isCloseConnectionException returns false on unexpected exceptions (i18n problem)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marevol</reporter><labels /><created>2014-01-08T02:51:59Z</created><updated>2014-05-28T23:52:24Z</updated><resolved>2014-05-28T23:52:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marevol" created="2014-05-28T23:52:24Z" id="44479396">This problem will be fixed by #6047.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Conditional highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4649</link><project id="" key="" /><description>I highlight two fields but only use the highlighting from one of them if there aren't matches in the first field.  I think it'd be cool not to highlight the second field at all if there is a match in the first field.  Mostly I like this idea because that second field is typically much much larger than the first field and takes some time to highlight.

``` js
      "fields": {
         "summary": {
            "number_of_fragments": 1,
            "fragment_size": 100,
            "no_match_size": 100
         },
         "contents": {
            "number_of_fragments": 1,
            "fragment_size": 100,
            "unless": ["text"]
         }
     }
```

or

``` js
      "fields": {
         "summary": {
            "number_of_fragments": 1,
            "fragment_size": 100,
            "no_match_size": 100,
            "when_no_match": {
               "contents": {
                  "number_of_fragments": 1,
                  "fragment_size": 100,
                  "unless": ["summary"]
               }
            }
         }
     }
```

Note that I really only need that no match snippet from the first field if there aren't matches in either field but it felt simpler to declare it on the first field.

I'll happy to implement this but would some thoughts on my proposed api.
</description><key id="25210007">4649</key><summary>Conditional highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-08T01:17:17Z</created><updated>2014-05-22T14:52:34Z</updated><resolved>2014-05-22T14:52:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-04-19T02:26:46Z" id="40858514">I picked this back up after doing some performance testing.  I found out that highlighting if one of the heaviest consumers of CPU for my use case.  So I figured anything that I can do to lower it would be useful.  This could allow you to skip highlighting some fields entirely.

Implemented this with a different api.  It looks like:

``` js
{
    "highlight": {
        "fields": {
            "title": {
                "conditional": {
                    "match": {
                        "text": {
                            "no_match_size": 100,
                            "skip_matching":  true
                        }
                    }
                    "no_match": {
                        "text": {
                            "no_match_size": 100
                        }
                    },
                }
            }
        }
    }
}
```

Each field can contain a `conditional` field.  There no limit other then your imagination.  And memory.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/test/java/org/elasticsearch/search/highlight/CustomHighlighter.java</file><file>src/test/java/org/elasticsearch/search/highlight/CustomHighlighterSearchTests.java</file></files><comments><comment>Highlight fields in request order</comment></comments></commit></commits></item><item><title>Fix inet cat errors in local mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4648</link><project id="" key="" /><description>Not sure if there's a better, guicier way of handling the polymorphism in `DiscoveryNode.host()` and `port()`. Comments welcome.
</description><key id="25193320">4648</key><summary>Fix inet cat errors in local mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-07T20:05:23Z</created><updated>2014-06-13T22:09:49Z</updated><resolved>2014-01-09T17:01:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-07T20:34:30Z" id="31776465">I am not a fan of making DiscoveryNode aware of the transport address type. Thinking about it, I don't think we need to have different IP/Host and port columns, how about adding `toShortString` to `TransportAddress`, where in local address it will just retun the id, and in inet one, it will return the `address.toString` (which will include the host if available, ip, and port). Then, instead of host/port columns, have an address columns, and use that toShortString
</comment><comment author="drewr" created="2014-01-07T21:12:36Z" id="31779786">The verbosity of `InetSocketTransportAddress.toString()` is pretty costly in terms of screen real estate, especially in `nodes` when you have relocations, like `inet[localhost/127.0.0.1:9300] -&gt; inet[localhost/127.0.0.1:9301]`.  There's a functional issue as well, though, in that if you want to dump the IPs of the cluster it order to iterate over them in the shell (`for ...; do ssh $host $cmd; done`), you'd now have to parse `inet[..]` yourself.
</comment><comment author="drewr" created="2014-01-07T21:32:53Z" id="31781783">Left host/port separate but moved it to the proper place.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make PageCacheRecycler better reuse memory across threads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4647</link><project id="" key="" /><description>Although using a thread-local is nice to avoid contention, we have to divide the available amount of memory by the size of the search thread pool and each thread may end up with not much memory, especially on machines with high numbers of processors and small heaps (eg. if they use doc values). For instance, a machine with 12 processors and 4GB of heap would use 410MB of memory for the recycler but each thread would only have 11MB (given that the default size of the search thread pool is 3 \* ${processors}).

Moreover, I don't think there is a lot of contention on this recycler since it is only used for large objects.

I think it would be a better trade-off to use the queue recycler by default (which is based on a non-blocking queue)? Or maybe something in-between that would allow for sharing data between several threads (with a configurable concurrency level like ConcurrentHashMap)?
</description><key id="25190646">4647</key><summary>Make PageCacheRecycler better reuse memory across threads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-07T19:34:00Z</created><updated>2014-01-09T15:36:03Z</updated><resolved>2014-01-09T15:35:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-07T19:35:57Z" id="31770985">The queue recycler has proven to be quite expensive when it comes to CPU usage (non blocking, but that spinning is expensive)... . I think that the thread base one is the best since most times number of cores correlates to the amount of memory a machine has.
</comment><comment author="jpountz" created="2014-01-08T09:47:25Z" id="31817454">I wrote a benchmark (see PR) to try to better understand the overhead of the different recyclers we have. The y axis is the time to recycle 5000000 objects and the x-axis is the number of threads (the machine on which this benchmark has been run has 8 processors).

There are 3 recyclers in this benchmark which are not in master right now:
- `Locked` which protects calls to obtain and release with a lock (so the recycler may be accessed by a single thread at once)
- `Concurrent` which creates one locked recycler per processor and affects threads to them based on their hashCode,
- `SoftConcurrent` which is to `Concurrent` what `SoftThreadLocal` is to `ThreadLocal`

![pouet](https://f.cloud.github.com/assets/299848/1867179/757fabd6-7849-11e3-8c5c-02e3dfbb8c39.png)

This confirms what you said about the queue recycler, which seems to have more overhead than other recyclers. However, I tend to like the lines for the `Concurrent` and `SoftConcurrent` ones, especially given that they would give each pool more memory, give the ability to the warmer thread pool to recycle data of the search thread pool and play more nicely with cached thread pools?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cache/recycler/CacheRecycler.java</file><file>src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/AbstractRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/ConcurrentDequeRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/DequeRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/FilterRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/NoneRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/Recycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/Recyclers.java</file><file>src/main/java/org/elasticsearch/common/recycler/SoftThreadLocalRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/ThreadLocalRecycler.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/ConcurrentCollections.java</file><file>src/test/java/org/elasticsearch/benchmark/common/recycler/RecyclerBenchmark.java</file><file>src/test/java/org/elasticsearch/common/recycler/ConcurrentRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/LockedRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/NoneRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/QueueRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/SoftConcurrentRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/SoftThreadLocalRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/ThreadLocalRecyclerTests.java</file></files><comments><comment>Change the default recycler type.</comment></comments></commit></commits></item><item><title>Use FHV's phraseLimit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4646</link><project id="" key="" /><description>This prevents poisoning the FVH with documents that contain TONS of matches
which take tons of memory and time to highlight.

Closes #4645
</description><key id="25187356">4646</key><summary>Use FHV's phraseLimit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-07T18:43:22Z</created><updated>2014-06-14T15:44:35Z</updated><resolved>2014-01-08T10:50:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-07T18:43:45Z" id="31765850">I haven't yet run the full test suite with this.  I'll update the PR when I have.
</comment><comment author="jpountz" created="2014-01-07T19:07:56Z" id="31768297">PR looks good to me. I'll push tomorrow if nobody does it before me.
</comment><comment author="s1monw" created="2014-01-07T19:10:58Z" id="31768565">+1 LGTM
</comment><comment author="nik9000" created="2014-01-07T19:12:40Z" id="31768699">Before pushing, let me add a test that sets the phraseLimit just to prove that works.
</comment><comment author="s1monw" created="2014-01-07T19:14:16Z" id="31768840">@jpountz lets fix this highlighting stuff once 1.0 is out! ;)
</comment><comment author="jpountz" created="2014-01-07T19:15:30Z" id="31768958">+1
</comment><comment author="nik9000" created="2014-01-07T20:14:06Z" id="31774703">Pushed an updated version that lets you set the phrase limit in the HighlightBuilder and only uses one megabyte of text.  That is still enough to verify it.
</comment><comment author="jpountz" created="2014-01-08T10:50:39Z" id="31821685">Pushed. Thanks, Nik!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>FVH can take a long time highlighting documents with many matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4645</link><project id="" key="" /><description>It looks like the FVH has a thing called phraseLimit which we can use to stop highlighting after a certain number of matched phrases.  I'm not sure it is the best solution from a quality standpoint but it is a great solution from a "the highlighter doesn't run for minutes on end" standpoint.
</description><key id="25187308">4645</key><summary>FVH can take a long time highlighting documents with many matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-07T18:42:37Z</created><updated>2014-01-10T10:06:04Z</updated><resolved>2014-01-08T10:44:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Use FHV's phraseLimit</comment></comments></commit></commits></item><item><title>Faceting only works on one nested field when multiple nested fields are mapped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4644</link><project id="" key="" /><description>I'm attempting to create an index where documents have multiple nested fields of the form: `"parent": [{"k": ..., "v": 1"}]"` with the mapping seen below (actually a dynamic mapping in my app, but it doesn't seem to make a difference).

I want to facet on the nested fields and filter results down to only specific values of `k` which is covered in the [nested field docs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-nested-type.html).

However, I'm running into a problem (bug?) where only 1 nested field seems to work with the facet. All other nested fields always return an empty result set (empty `terms`).

Test case to reproduce (using [Sense](https://chrome.google.com/webstore/detail/sense/doinijnbnggojdlcjifpdckfokbbfpbo?hl=en)):

[Bash/CURL version](https://gist.github.com/schmichael/8303986)

``` json
DELETE /test
POST /test
{
   "mappings" : {
        "testtype" : {
            "_source" : { "enabled" : true },
            "properties" : {
                "mapA": {
                    "type": "nested", 
                    "index": "not_analyzed"
                    , "properties": {
                        "k": {
                            "type": "string",
                            "index": "not_analyzed"
                        },
                        "v": {
                            "type": "long",
                            "index": "not_analyzed"
                        }
                    }
                },
                "mapB": {
                    "type": "nested", 
                    "index": "not_analyzed"
                    , "properties": {
                        "k": {
                            "type": "string",
                            "index": "not_analyzed"
                        },
                        "v": {
                            "type": "long",
                            "index": "not_analyzed"
                        }
                    }
                }
            }
        }
    }
}

PUT /test/testtype/1
{"mapA": [{"k": "K", "v": 1}]}
PUT /test/testtype/2
{"mapB": [{"k": "K", "v": 1}]}

POST /test/testtype/_search
{
    "facets": {
        "mapfacet": {
            "facet_filter": {
                "term": {
                    "k": "K"
                }
            },
            "nested": "mapA",
            "terms": {
                "field": "v"
            }
        }
    },
    "size": 0
}
POST /test/testtype/_search
{
    "facets": {
        "mapfacet": {
            "facet_filter": {
                "term": {
                    "k": "K"
                }
            },
            "nested": "mapB",
            "terms": {
                "field": "v"
            }
        }
    },
    "size": 0
}
```

**Expected result:**

Both facets should return `"terms": [ { "term": 1, "count": 1 } ]`

**Actual result:**

The facet on nested field `mapA` returns no terms.

**Versions**

Tested on 0.90.3 and 0.90.9.
</description><key id="25185396">4644</key><summary>Faceting only works on one nested field when multiple nested fields are mapped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">schmichael</reporter><labels><label>bug</label></labels><created>2014-01-07T18:10:44Z</created><updated>2014-07-23T12:26:57Z</updated><resolved>2014-07-23T12:26:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="schmichael" created="2014-01-07T18:38:24Z" id="31765354">jprante on irc discovered that it seems to be a bug in the nested document's field lookup because changing the mapping for `mapB` from `k` &amp; `v` to `x` &amp; `y` produces the expected results: https://gist.github.com/jprante/8304172

Unfortunately my actual use case (not this minimal test case) uses dynamic mappings to apply nested indexing to an unknown number of fields, so I can't vary the nested documents' fields.
</comment><comment author="schmichael" created="2014-01-07T18:46:36Z" id="31766127">jprante further discovered that using fully qualified field names seems to fix the bug! (I was unaware fully qualified field names were even allowed in these contexts.)

So the query for `mapA` becomes:

``` json
{
    "facets": {
        "mapfacet": {
            "facet_filter": {
                "term": {
                    "mapA.k": "K"
                }
            },
            "nested": "mapA",
            "terms": {
                "field": "mapA.v"
            }
        }
    },
    "size": 0
}
```

So either the bug needs to be fixed to allow this to work without fully qualified names (as the `nested` key should allow the query parser to generate fully qualified names) and/or the docs should be updated to encourage the use of fully qualified key names.
</comment><comment author="martijnvg" created="2014-01-09T12:55:10Z" id="31928020">This kind of problems can be very confusing! Obviously we need to update the documentation were is needed. On op of this I think ES should throw an error if the fully qualified name isn't specified in the case accessing nested fields is involved.
</comment><comment author="clintongormley" created="2014-07-23T12:26:57Z" id="49866554">Closing in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Sorting by sub-aggregation is broken in terms agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4643</link><project id="" key="" /><description>Terms aggregations enables sorting by sub metric aggregation, a la:

``` json
{
    "aggregations": {
        "host": {
            "terms": {
                "field": "host",
                "order": {
                    "cpu_avg": "asc"
                }
            },
            "aggs": {
                "cpu_avg": {
                    "avg": {
                        "field": "cpu"
                    }
                }
            }
        }
    }
}
```

unfortunately, that got broken along the way.

The fix will be mostly based on https://github.com/elasticsearch/elasticsearch/pull/4472

thx @alexbrasetvik
</description><key id="25167784">4643</key><summary>Sorting by sub-aggregation is broken in terms agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">uboness</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2014-01-07T13:40:50Z</created><updated>2014-02-04T03:40:12Z</updated><resolved>2014-01-09T15:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-01-09T15:58:19Z" id="31945890">fixed
</comment><comment author="TrevRUNDSP" created="2014-01-22T19:19:21Z" id="33057079">Hi all,
I'm using the below code to try and grab the terms that have the greatest sums of field "impression":

{
"size":0,
"query":{
"filtered":{
"query":{"match_all":{}},
"filter":{
"and":{
"filters":[{"range":{"date_time":{"from":"2013-12-12T00:00:00Z","to":"2013-12-12T23:59:59Z"}}}]
} 
}
}
},
"aggs":{
"XXXXXXXXX" : {
"terms" : { 
"field" : "XXXXXXXXX.untouched",
"size" : 10,
"order" : { "Impression_summation" : "desc" }
},
"aggs" : {
"Impression_summation" : { "sum" : { "field" : "impression" } }
}
}
}
}}
'

While this code _does_ order by the sums of that field, it is omitting many terms that would have even greater sums. It is only after I increase the size parameter to something pretty high (10 --&gt; 1000 in this case), does it then 'catch' the terms with the largest summations of field "impression". Is there other code I should be using for this or is this a bug? Thanks!
- Trev
</comment><comment author="alexbrasetvik" created="2014-01-22T19:47:13Z" id="33059872">Hi Trev,

You probably want to increase the `shard_size`.

Here's the relevant section in the docs: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-terms-aggregation.html#_size_amp_shard_size

– Alex
</comment><comment author="TrevRUNDSP" created="2014-01-22T19:53:55Z" id="33060554">Excellent! I'll give that a whirl. Thanks for the help (and fast response)!
- Trev
</comment><comment author="benatwork99" created="2014-01-29T23:44:49Z" id="33645384">I'm using 1.0.0-RC1 and this doesn't appear to be working correctly for me for a metric sub-aggregation.

If I use the following query:

```
{
  "aggs": {
    "fieldValues": {
      "terms": {
        "field": "name.field",
        "order": {
          "totalAmount": "desc"
        }
      },
      "aggs": {
        "totalAmount": {
          "sum": {
            "field": "amount"
          }
        }
      }
    }
  },
  "size": 0
}
```

It appears to be sorted in 'asc' mode, and changing 'desc' to 'asc' produces the same results. Other than that the query does appear to work successfully and returns the expected sort of data in the buckets.

Changing the order key from 'totalAmount' to '_term' works as expected, and 'asc' and 'desc' show different results.

I've looked at the code for 1.0.0-RC1 and the above changes do appear to be in, was it working for you on RC1 Trev?
</comment><comment author="uboness" created="2014-01-30T00:52:41Z" id="33649899">@benatwork99 I just indexed a few documents that fit the aggs you have above, and it seem to work as expected. Are you sure you're running it on RC1? 

here's what I ran (save it to a file `test_aggs.sh`):

``` bash
curl -XDELETE 'http://localhost:9200/idx?pretty'; echo;

for i in {1..3}
do
  curl -XPOST 'http://localhost:9200/idx/type&amp;pretty' -d '{
    "name" : {
      "field" : "val1"
    },
    "amount" : '$i'
  }'; echo;
done

for i in {1..2}
do
  curl -XPOST 'http://localhost:9200/idx/type&amp;pretty' -d '{
    "name" : {
      "field" : "val2"
    },
    "amount" : '$i'
  }'; echo;
done


sleep 3s; ehco;

dir=$1

echo 'executing search ('$dir')';

curl -XGET 'http://localhost:9200/idx/_search?search_type=count&amp;pretty' -d '{
  "aggs": {
    "fieldValues": {
      "terms": {
        "field": "name.field",
        "order": {
          "totalAmount": "'$dir'"
        }
      },
      "aggs": {
        "totalAmount": {
          "sum": {
            "field": "amount"
          }
        }
      }
    }
  }
}'; echo;
```

desc order : `./test_aggs.sh desc`
asc order: `./test_aggs.sh asc`
</comment><comment author="benatwork99" created="2014-01-30T03:14:17Z" id="33656895">Thank you very much for the detailed reply uboness. If I download the vanilla RC1 distribution and run your query against it, I can confirm that it works as intended. Our version is part of a larger build and configuration process, it's definitely RC1 but there's obviously something else - distribution or configuration - affecting it.

I'm looking in to this now trying to understand what's different, will post back as soon as I can figure it out, and hopefully this won't have been just a waste of time for you!
</comment><comment author="uboness" created="2014-01-30T03:18:08Z" id="33657010">@benatwork99 ok, cool, keep us posted (and it's definitely not a waste of our time... we're here to make it work :))
</comment><comment author="benatwork99" created="2014-01-30T03:49:50Z" id="33658147">Aha! Ok turned out our test and dev setups use a single shard - setting

```
index.number_of_shards: 1
```

in elasticsearch.yml.

With this set to 1, I see ordering by the metric sub aggregate always sorting in ascending order! Leaving it blank (or setting it to 2 or higher) means that the descending order works as expected.

So, I think that's a bug. At least I wasn't doing something daft ;) Do you want me to create a separate issue on it?
</comment><comment author="uboness" created="2014-01-30T04:58:24Z" id="33660371">@benatwork99 yep.. indeed a bug... opened an issue for it, will be fixed shortly. Thanks! (see.. no waste of time :))
</comment><comment author="andrewreedy" created="2014-02-04T03:40:12Z" id="34028028">@uboness I'm seeing an issue with sorting by sub-agg when two of the values equal each other.. for example: Doc 1 and 2 work fine but with Doc 3 it will throw NPE.

#### Mapping

```
{
    "bid" : {
        "properties" : {
            "bid_price" : {
                "type" : "float"   
            }, 
           "company_id" : {
                "type" : "string",
                "index" : "not_analyzed"
            },
           "bid_id" : {
                "type" : "string",
                "index" : "not_analyzed"
            }
        }
    }
}
```

#### Doc 1

```
{
    "bid_price" : 7.0, 
    "company_id" : "7",
    "bid_id": "11"
}
```

#### Doc 2

```
{
    "bid_price" : 6.0, 
    "company_id" : "6",
    "bid_id": "11"
}
```

#### Doc 3

```
{
    "bid_price" : 7.0, 
    "company_id" : "6",
    "bid_id": "11"
}
```

#### Agg

```
  "aggs" : {
        "company_id" : {
            "terms" : { 
                "field" : "company_id",
                "order": {
                    "max_bid": "desc"
                }
            },
            "aggs" : {
                "bid_id" : {
                    "terms" : { 
                        "field" : "bid_id",
                        "order": {
                            "company_max_bid": "desc"
                        },
                        "size"  : 1
                    },
                    "aggs" : {
                        "company_max_bid": {
                            "max": {
                                "field": "bid_price"
                            }
                        }   
                    }
                },
                "max_bid": {
                    "max": {
                        "field": "bid_price"
                    }
                }
            }
        }
    }
```

#### Result

```
   "_shards": {
      "total": 5,
      "successful": 4,
      "failed": 1,
      "failures": [
         {
            "index": "bids_02-03-14",
            "shard": 0,
            "status": 500,
            "reason": "NullPointerException[null]"
         }
      ]
   }
```
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/Aggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/Bucket.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/InternalOrder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTerms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/Terms.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/MetricsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/InternalStats.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggegator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/InternalExtendedStats.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/DoubleTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/HistogramTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ShardSizeTermsTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file></files><comments><comment>- fixed terms aggs ordering by sub metric agg - introduced MetricsAggregator to enable access to metrics values without creating buckets</comment></comments></commit></commits></item><item><title>Add percentage for other stats (affects _stats and _cat)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4642</link><project id="" key="" /><description>Percentages are quite useful for doing quick diagnosis - there's such an indicator for ram/heap but we could/should extend them to other runtime stats as well such as disk used/free for example.
</description><key id="25165231">4642</key><summary>Add percentage for other stats (affects _stats and _cat)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-01-07T12:43:53Z</created><updated>2016-11-06T11:07:31Z</updated><resolved>2016-11-06T11:07:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-01-07T12:44:06Z" id="31734270">@bleskes @drewr thoughts?
</comment><comment author="drewr" created="2014-01-07T18:23:43Z" id="31763977">Totally agree. We're slowly getting ratios out of cat into stats proper. A ratio is the only sane at-a-glance number in a lot of cases.
</comment><comment author="bleskes" created="2014-01-07T19:45:36Z" id="31771924">Good with me. I just wonder about the disk example as % is not always what you need. Think about a 3TB disk were 90% used still means 300GB which is quite a lot. Doesn't mean we can't add it and let people choose what they want to monitor/see.
</comment><comment author="s1monw" created="2014-01-10T10:09:21Z" id="32015732">moving to 0.90.11
</comment><comment author="s1monw" created="2014-03-12T20:14:35Z" id="37458343">@costin will you have time for this in the near future? otherwise please push to `1.2`
</comment><comment author="costin" created="2014-03-12T20:16:08Z" id="37458526">Sure - if @drewr or @bleskes won't pick it up, I'll go for it for 1.2 (changing the label anyway).
</comment><comment author="s1monw" created="2014-03-12T20:16:27Z" id="37458570">@costin please do
</comment><comment author="clintongormley" created="2014-12-24T17:56:52Z" id="68066908">Is this still relevant?
</comment><comment author="bleskes" created="2015-03-27T10:35:38Z" id="86896439">@clintongormley the sentiment is that this is still a useful thing, but we need to judge on a metric by metric basis.  

@costin it's a long time ago- but was there some specific metrics you had in mind? (other then the disk usage)
</comment><comment author="clintongormley" created="2016-11-06T11:07:31Z" id="258673972">No feedback in 18 months. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add support for tera and peta units</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4641</link><project id="" key="" /><description>PR for issue #4640
</description><key id="25164544">4641</key><summary>add support for tera and peta units</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels /><created>2014-01-07T12:28:01Z</created><updated>2014-06-26T10:00:09Z</updated><resolved>2014-01-08T17:05:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-07T12:36:20Z" id="31733885">looks good!
</comment><comment author="costin" created="2014-01-08T17:05:08Z" id="31854320">committed into master and 0.90
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>add support for Tera (bytes) and Peta (bytes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4640</link><project id="" key="" /><description>Considering the amount of data stored in a ES cluster, the giga byte unit, as a max, seems unsuitable. Tera bytes and potentially peta bytes are more appropriate.
</description><key id="25164368">4640</key><summary>add support for Tera (bytes) and Peta (bytes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">costin</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-07T12:23:39Z</created><updated>2014-01-08T17:04:43Z</updated><resolved>2014-01-08T17:04:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Exception cause unwrapping ran for 10 levels</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4639</link><project id="" key="" /><description>Hi,

To summarize from https://groups.google.com/forum/#!msg/elasticsearch/WdMU4_vD8zk/SKjOiCGHvFgJ

running elasticsearch version 0.90.7 

gist file 
https://gist.github.com/jasonwee/8282477 
https://gist.github.com/jasonwee/8294514

server and client both running the same version the entire times.
$ java -version
java version "1.6.0_25"
Java(TM) SE Runtime Environment (build 1.6.0_25-b06)
Java HotSpot(TM) 64-Bit Server VM (build 20.0-b11, mixed mode)

According to Jörg Prante, this is worth raising an issue at the Elasticsearch github . 

node3 elasticsearch instance was restarted and then the exceptions is no longer exists in node1 and node4 no more.

If you need more information, please let me know what I can provide.

Thank you.

Jason
</description><key id="25155368">4639</key><summary>Exception cause unwrapping ran for 10 levels</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasonwee</reporter><labels><label>discuss</label></labels><created>2014-01-07T09:16:18Z</created><updated>2015-10-14T12:19:07Z</updated><resolved>2015-10-14T12:19:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:55:49Z" id="68066867">Hi @jasonwee 

Sorry it has taken so long to look at this.  That's the first time I've seen this particular exception.  Have you ever seen this problem again? The code has obviously changed a lot since back in 0.90.7. I think it is not worth investigating at this stage, but if you see something similar on a more recent version, please open another ticket.  

thanks
</comment><comment author="jasonwee" created="2014-12-29T10:46:40Z" id="68247686">No, I have no seen this exception anymore. Thank you for this response.

However, this one happened recently. Twice actually. https://groups.google.com/forum/#!topic/elasticsearch/1TlJDwuKXiA

Should I file a git issue?
</comment><comment author="clintongormley" created="2014-12-29T11:10:41Z" id="68249180">Hi @jasonwee 

That's on 0.90.7.  I suggest upgrading as we're no longer supporting the 0.90 branch
</comment><comment author="jasonwee" created="2014-12-29T13:28:56Z" id="68257103">sure sure, upgrade is good, could you please tell me which issue that fix the exception?
</comment><comment author="sacheendra" created="2015-05-11T00:21:11Z" id="100720765">This problem still exists. Noticed it while doing a benchmark. Some nodes had high CPU usage relative to other nodes and similar things were there in the logs of such nodes. 

Gist of the error: https://gist.github.com/sacheendra/ab00b6625c3367ed06c1

ES version: 1.4.0
JVM: Hotspot 1.7.0_67
Number of nodes: 15
</comment><comment author="Srinathc" created="2015-07-12T17:47:54Z" id="120749921">Found a similar exception on release 1.3.0.
https://discuss.elastic.co/t/stackoverflowerror-in-es/25278/2
</comment><comment author="clintongormley" created="2015-07-13T11:30:44Z" id="120896836">We have changed exception handling completely in master - would be interested to know if you're seeing something similar in 2.0 (once it is released)
</comment><comment author="Srinathc" created="2015-07-13T12:18:37Z" id="120907656">Are there any details on when and why this happens? Any remedies for 1.x releases will be great to know. Our instances are already on production.

Thanks.
</comment><comment author="clintongormley" created="2015-10-14T12:19:07Z" id="148031137">almost all of these exceptions are due to a change in serialization between java versions.  we no longer rely on java serialization.  also, stack traces are now trimmed of their guice content (see https://github.com/elastic/elasticsearch/pull/13782)

I think this is resolved in 2.0
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>how to config the random port range in ealsticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4638</link><project id="" key="" /><description>there are lots of random ports    "ESTABLISHED"  when i started elasticsearch 

how can i config it
 TCP    127.0.0.1:9300         127.0.0.1:59013        ESTABLISHED     8324
 TCP    127.0.0.1:58977        127.0.0.1:58978        ESTABLISHED     8324
 TCP    127.0.0.1:58978        127.0.0.1:58977        ESTABLISHED     8324
 TCP    127.0.0.1:58979        127.0.0.1:58980        ESTABLISHED     8324
 TCP    127.0.0.1:58980        127.0.0.1:58979        ESTABLISHED     8324
 TCP    127.0.0.1:58981        127.0.0.1:58982        ESTABLISHED     8324
 TCP    127.0.0.1:58982        127.0.0.1:58981        ESTABLISHED     8324
 TCP    127.0.0.1:58983        127.0.0.1:58984        ESTABLISHED     8324
 TCP    127.0.0.1:58984        127.0.0.1:58983        ESTABLISHED     8324
 TCP    127.0.0.1:58985        127.0.0.1:58986        ESTABLISHED     8324
 TCP    127.0.0.1:58986        127.0.0.1:58985        ESTABLISHED     8324
 TCP    127.0.0.1:58987        127.0.0.1:58988        ESTABLISHED     8324
 TCP    127.0.0.1:58988        127.0.0.1:58987        ESTABLISHED     8324
 TCP    127.0.0.1:58989        127.0.0.1:58990        ESTABLISHED     8324
 TCP    127.0.0.1:58990        127.0.0.1:58989        ESTABLISHED     8324
 TCP    127.0.0.1:58991        127.0.0.1:58992        ESTABLISHED     8324
 TCP    127.0.0.1:58992        127.0.0.1:58991        ESTABLISHED     8324
 TCP    127.0.0.1:58993        127.0.0.1:58994        ESTABLISHED     8324
 TCP    127.0.0.1:58994        127.0.0.1:58993        ESTABLISHED     8324
 TCP    127.0.0.1:58995        127.0.0.1:58996        ESTABLISHED     8324
 TCP    127.0.0.1:58996        127.0.0.1:58995        ESTABLISHED     8324
 TCP    127.0.0.1:58997        127.0.0.1:58998        ESTABLISHED     8324
 TCP    127.0.0.1:58998        127.0.0.1:58997        ESTABLISHED     8324
 TCP    127.0.0.1:58999        127.0.0.1:59000        ESTABLISHED     8324
 TCP    127.0.0.1:59000        127.0.0.1:58999        ESTABLISHED     8324
 TCP    127.0.0.1:59001        127.0.0.1:59002        ESTABLISHED     8324
 TCP    127.0.0.1:59002        127.0.0.1:59001        ESTABLISHED     8324
 TCP    127.0.0.1:59003        127.0.0.1:59004        ESTABLISHED     8324
 TCP    127.0.0.1:59004        127.0.0.1:59003        ESTABLISHED     8324
 TCP    127.0.0.1:59005        127.0.0.1:59006        ESTABLISHED     8324
 TCP    127.0.0.1:59006        127.0.0.1:59005        ESTABLISHED     8324
 TCP    127.0.0.1:59007        127.0.0.1:59008        ESTABLISHED     8324
 TCP    127.0.0.1:59008        127.0.0.1:59007        ESTABLISHED     8324
 TCP    127.0.0.1:59009        127.0.0.1:59010        ESTABLISHED     8324
 TCP    127.0.0.1:59010        127.0.0.1:59009        ESTABLISHED     8324
 TCP    127.0.0.1:59011        127.0.0.1:59012        ESTABLISHED     8324
 TCP    127.0.0.1:59012        127.0.0.1:59011        ESTABLISHED     8324
 TCP    127.0.0.1:59024        127.0.0.1:59025        ESTABLISHED     8324
 TCP    127.0.0.1:59025        127.0.0.1:59024        ESTABLISHED     8324
 TCP    127.0.0.1:59026        127.0.0.1:59027        ESTABLISHED     8324
 TCP    127.0.0.1:59027        127.0.0.1:59026        ESTABLISHED     8324
 TCP    127.0.0.1:59028        127.0.0.1:59029        ESTABLISHED     8324
 TCP    127.0.0.1:59029        127.0.0.1:59028        ESTABLISHED     8324
 TCP    127.0.0.1:59030        127.0.0.1:59031        ESTABLISHED     8324
 TCP    127.0.0.1:59031        127.0.0.1:59030        ESTABLISHED     8324
 TCP    127.0.0.1:59032        127.0.0.1:59033        ESTABLISHED     8324
 TCP    127.0.0.1:59033        127.0.0.1:59032        ESTABLISHED     8324
 TCP    127.0.0.1:59034        127.0.0.1:59035        ESTABLISHED     8324
 TCP    127.0.0.1:59035        127.0.0.1:59034        ESTABLISHED     8324
 TCP    127.0.0.1:59036        127.0.0.1:59037        ESTABLISHED     8324
 TCP    127.0.0.1:59037        127.0.0.1:59036        ESTABLISHED     8324
 TCP    127.0.0.1:59038        127.0.0.1:59039        ESTABLISHED     8324
 TCP    127.0.0.1:59039        127.0.0.1:59038        ESTABLISHED     8324
 TCP    127.0.0.1:59040        127.0.0.1:59041        ESTABLISHED     8324
 TCP    127.0.0.1:59041        127.0.0.1:59040        ESTABLISHED     8324
 UDP    0.0.0.0:54328          _:_                                    8324
 UDP    [::]:54328             _:_                                    8324
</description><key id="25154276">4638</key><summary>how to config the random port range in ealsticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhyj0121</reporter><labels /><created>2014-01-07T08:46:53Z</created><updated>2014-01-07T08:55:17Z</updated><resolved>2014-01-07T08:55:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-07T08:55:17Z" id="31721715">Please use the mailing list for questions.
Github issues are used for issues and feature requests.

See [help](http://www.elasticsearch.org/help) for details.
Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add recovery API endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4637</link><project id="" key="" /><description>Add a new API endpoint for retrieving the status of on-going recovery operations. This API will report the status of index recovery for the 1.0 snapshot/restore feature, as well as for gateway recoveries. 

The API endpoint will live at /_recovery. 
</description><key id="25141808">4637</key><summary>Add recovery API endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Index APIs</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-07T01:05:48Z</created><updated>2015-06-06T18:44:15Z</updated><resolved>2014-03-20T17:29:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-20T09:05:01Z" id="38146380">I looked at the PR and it seems like it's good to go. @aleph-zero @imotov @kimchy can we push it?
</comment><comment author="kimchy" created="2014-03-20T09:31:39Z" id="38148086">++, I reviewed it and it looks good!
</comment><comment author="aleph-zero" created="2014-03-20T16:36:55Z" id="38189918">@s1monw I was afraid to push it; not clear on which 1.x branches it needs to go in. @imotov was going to walk me through it but he had to travel this week. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/ShardRecoveryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/TransportRecoveryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/gateway/IndexShardGatewayService.java</file><file>src/main/java/org/elasticsearch/index/gateway/RecoveryStatus.java</file><file>src/main/java/org/elasticsearch/index/gateway/blobstore/BlobStoreIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/gateway/local/LocalIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/gateway/none/NoneIndexShardGateway.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardSnapshotAndRestoreService.java</file><file>src/main/java/org/elasticsearch/index/snapshots/blobstore/BlobStoreIndexShardRepository.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryStatus.java</file><file>src/main/java/org/elasticsearch/indices/recovery/RecoveryTarget.java</file><file>src/main/java/org/elasticsearch/indices/recovery/StartRecoveryRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/recovery/RestRecoveryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestRecoveryAction.java</file><file>src/test/java/org/elasticsearch/indices/recovery/IndexRecoveryTests.java</file></files><comments><comment>Recovery API</comment></comments></commit></commits></item><item><title>Fix Elasticsearch License Headers </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4636</link><project id="" key="" /><description>`s/ElasticSearch and Shay Banon/Elasticsearch`
</description><key id="25133329">4636</key><summary>Fix Elasticsearch License Headers </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2014-01-06T22:05:36Z</created><updated>2014-01-13T11:18:58Z</updated><resolved>2014-01-07T10:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/analysis/CustomAnalyzerWrapper.java</file><file>src/main/java/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.java</file><file>src/main/java/org/apache/lucene/analysis/miscellaneous/UniqueTokenFilter.java</file><file>src/main/java/org/apache/lucene/index/TrackingConcurrentMergeScheduler.java</file><file>src/main/java/org/apache/lucene/index/TrackingSerialMergeScheduler.java</file><file>src/main/java/org/apache/lucene/index/memory/ExtendedMemoryIndex.java</file><file>src/main/java/org/apache/lucene/queries/ExtendedCommonTermsQuery.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/ExistsFieldQueryExtension.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/FieldQueryExtension.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/MissingFieldQueryExtension.java</file><file>src/main/java/org/apache/lucene/queryparser/classic/QueryParserSettings.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPassageFormatter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/CustomPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/Snippet.java</file><file>src/main/java/org/apache/lucene/search/postingshighlight/XPostingsHighlighter.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XAnalyzingSuggester.java</file><file>src/main/java/org/apache/lucene/search/suggest/analyzing/XFuzzySuggester.java</file><file>src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>src/main/java/org/apache/lucene/store/RateLimitedFSDirectory.java</file><file>src/main/java/org/apache/lucene/store/StoreRateLimiting.java</file><file>src/main/java/org/apache/lucene/store/StoreUtils.java</file><file>src/main/java/org/elasticsearch/Build.java</file><file>src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchGenerationException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchIllegalArgumentException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchIllegalStateException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchInterruptedException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchNullPointerException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchParseException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchTimeoutException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchWrapperException.java</file><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/Version.java</file><file>src/main/java/org/elasticsearch/action/Action.java</file><file>src/main/java/org/elasticsearch/action/ActionFuture.java</file><file>src/main/java/org/elasticsearch/action/ActionListener.java</file><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/ActionRequest.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestValidationException.java</file><file>src/main/java/org/elasticsearch/action/ActionResponse.java</file><file>src/main/java/org/elasticsearch/action/FailedNodeException.java</file><file>src/main/java/org/elasticsearch/action/GenericAction.java</file><file>src/main/java/org/elasticsearch/action/ListenableActionFuture.java</file><file>src/main/java/org/elasticsearch/action/NoShardAvailableActionException.java</file><file>src/main/java/org/elasticsearch/action/NoSuchNodeException.java</file><file>src/main/java/org/elasticsearch/action/PrimaryMissingActionException.java</file><file>src/main/java/org/elasticsearch/action/RoutingMissingException.java</file><file>src/main/java/org/elasticsearch/action/ShardOperationFailedException.java</file><file>src/main/java/org/elasticsearch/action/ThreadingModel.java</file><file>src/main/java/org/elasticsearch/action/TimestampParsingException.java</file><file>src/main/java/org/elasticsearch/action/TransportActionNodeProxy.java</file><file>src/main/java/org/elasticsearch/action/UnavailableShardsException.java</file><file>src/main/java/org/elasticsearch/action/ValidateActions.java</file><file>src/main/java/org/elasticsearch/action/WriteConsistencyLevel.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ClusterAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterIndexHealth.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterShardHealth.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodeHotThreads.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodeInfo.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/NodesRestartAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/NodesRestartRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/NodesRestartRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/NodesRestartResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/TransportNodesRestartAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/NodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/NodesShutdownRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/NodesShutdownRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/NodesShutdownResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodeStats.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsGroup.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/PendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/PendingClusterTasksRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/PendingClusterTasksRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/PendingClusterTasksResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/IndicesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/AliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/AliasesExistRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/AliasesExistResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/BaseAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ShardClearIndicesCacheResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/ShardFlushResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/GatewaySnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/ShardGatewaySnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/ShardGatewaySnapshotResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/TransportGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetMappingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetMappingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetMappingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/ShardOptimizeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/ShardRefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/ShardRefreshResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/package-info.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndexSegments.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndexShardSegments.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/ShardSegments.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/TransportUpdateSettingsAction.java</file></files><comments><comment>Fix ASL Header in source files to reflect s/ElasticSearch/Elasticsearch</comment></comments></commit></commits></item><item><title>Rename "exists" to "found" in TermVector and Get responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4635</link><project id="" key="" /><description>Also:
- Adds the "created" field to the index action response
- Reverses Delete class' notFound to Found to avoid double negative

Fixes #4480
</description><key id="25131104">4635</key><summary>Rename "exists" to "found" in TermVector and Get responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2014-01-06T21:30:16Z</created><updated>2014-12-12T16:27:49Z</updated><resolved>2014-01-07T16:46:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-07T16:22:19Z" id="31752048">LGTM
</comment><comment author="dakrone" created="2014-01-07T16:46:29Z" id="31754562">Merged in 2cb40fcb
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename `ElasticSearch*` to `Elasticsearch*`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4634</link><project id="" key="" /><description>before we release 1.0 we should cleanup naming on the internal classes to use `Elasticsearch` rather than `ElasticSearch`
</description><key id="25127616">4634</key><summary>Rename `ElasticSearch*` to `Elasticsearch*`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2014-01-06T20:31:25Z</created><updated>2014-09-16T15:41:55Z</updated><resolved>2014-01-07T10:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-08T10:03:11Z" id="31818412">For the record this issue renamed the following classes:
- `ElasticSearchException.java` → `ElasticsearchException.java`
- `ElasticSearchGenerationException.java` → `ElasticsearchGenerationException.java`
- `ElasticSearchIllegalArgumentException.java` → `ElasticsearchIllegalArgumentException.java`
- `ElasticSearchIllegalStateException.java` → `ElasticsearchIllegalStateException.java`
- `ElasticSearchInterruptedException.java` → `ElasticsearchInterruptedException.java`
- `ElasticSearchNullPointerException.java` → `ElasticsearchNullPointerException.java`
- `ElasticSearchParseException.java` → `ElasticsearchParseException.java`
- `ElasticSearchTimeoutException.java` → `ElasticsearchTimeoutException.java`
- `ElasticSearchWrapperException.java` → `ElasticsearchWrapperException.java`
- `ElasticSearch.java` → `Elasticsearch.java`
- `ElasticSearchF.java` → `ElasticsearchF.java`

This change is not backwards compatible!
</comment><comment author="dadoonet" created="2014-01-09T07:05:12Z" id="31907181">With that change, all plugins are broken for 1.0.
Trying to keep plugins compatible with both 0.90 and 1.0 like we started to do in https://github.com/dadoonet/elasticsearch-cloud-aws/commit/72595560be16d78c335a110cb659c455a15fda33#diff-29637d427c53f19f7a083fd0a40da1b5R89 is not doable anymore.

That basically means that all plugins in their master branch are not compatible anymore with 0.90. I will update all README.

See also https://github.com/elasticsearch/elasticsearch-transport-memcached/issues/13
and https://github.com/elasticsearch/elasticsearch-transport-thrift/issues/12
</comment><comment author="s1monw" created="2014-01-09T09:00:30Z" id="31912960">@dadoonet yes that is correct we need to see how we deal with major version updates in the plugins. What keeps us from doing version branches there as well aligned with the ES versions?
</comment><comment author="dadoonet" created="2014-01-09T09:21:22Z" id="31914199">I did propose that but @kimchy would like to keep it simple.
I agree that we should create branch for plugins only when we need to fix an issue. 

So I guess we don't need to do it now.
</comment><comment author="apatrida" created="2014-01-20T11:11:03Z" id="32751038">Wow, ok, so all plugins that were already falling behind trying to keep up with previous broken changes are now one more step back.  For a cosmetic change, was it worth breaking most plugins and most client code?  (not just this change, but HPCC non Map implementing Map-like classes and others)

This would be nice change to also include pull requests to many plugins that are now unload-able.
</comment><comment author="s1monw" created="2014-01-20T11:13:50Z" id="32751223">@jaysonminard the plugins that we have access to are already fixed. If you have a plugin you will be able to fix that in a couple of minutes. The chance to fix stuff like this for consistency doesn't happen too often and I think it had enough time to bubble up before RC1 now that we have a RC1 you can easily upgrade and make your plugin loadable for the GA release. Which plugin doesn't load for you?
</comment><comment author="apatrida" created="2014-01-20T11:26:08Z" id="32751938">@s1monw there are plugins like IN/OUT plugin which are not even accepting pull requests (they didn't have time to keep up with breaking changes) that we have had to fork, and we'll bring it up to date along with our other forks.  But I'm talking generally and not just in this particular change.  Over the last 2 months it is more common to find plugins that are not currently load-able or run-able (even if loading) or sometimes just in an unknown and untested state with current ES versions.  Obviously breaking changes have to happen sometimes along the way, but the rate seems to increase towards 1.0 rather than reduce towards stability.  So while ES might be more stable, the ecosystem around it seems more in disarray.  

And even if a plugin works, the documentation doesn't make it clear if they will still load since they are not updated to reflect the top version supported.  Imagine a new ES user installing it, trying to use a plugin listed on the site but not seeing a matching version, and sometimes using one to find it doesn't work.  Does not lend credibility to the product.  I think this is my message, please keep that in mind (you likely are, but it feels less so from the "outside")

I think it is a case where the speed of change has outpaced the open-source plugins writers ability or desire to keep up with the pace of change.

(The previous collection class changes just introduced really odd feeling code to users of the Java client, that's a different story.  I think the example of listing indexes with 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/store/StoreRateLimiting.java</file><file>src/main/java/org/elasticsearch/ElasticsearchException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchGenerationException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchIllegalArgumentException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchIllegalStateException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchInterruptedException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchNullPointerException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchParseException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchTimeoutException.java</file><file>src/main/java/org/elasticsearch/ElasticsearchWrapperException.java</file><file>src/main/java/org/elasticsearch/ExceptionsHelper.java</file><file>src/main/java/org/elasticsearch/action/ActionFuture.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestValidationException.java</file><file>src/main/java/org/elasticsearch/action/FailedNodeException.java</file><file>src/main/java/org/elasticsearch/action/PrimaryMissingActionException.java</file><file>src/main/java/org/elasticsearch/action/RoutingMissingException.java</file><file>src/main/java/org/elasticsearch/action/ThreadingModel.java</file><file>src/main/java/org/elasticsearch/action/TimestampParsingException.java</file><file>src/main/java/org/elasticsearch/action/TransportActionNodeProxy.java</file><file>src/main/java/org/elasticsearch/action/UnavailableShardsException.java</file><file>src/main/java/org/elasticsearch/action/WriteConsistencyLevel.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/TransportClusterHealthAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/TransportNodesHotThreadsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/TransportNodesInfoAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/TransportNodesRestartAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/TransportNodesShutdownAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/TransportDeleteRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/TransportGetRepositoriesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/TransportPutRepositoryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/TransportClusterUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/TransportDeleteSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/TransportGetSnapshotsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/TransportClusterStateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/TransportPendingClusterTasksAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/TransportIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/TransportClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/TransportFlushAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/gateway/snapshot/TransportGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/TransportOptimizeAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/TransportRefreshAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/TransportIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/TransportIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/GatewayRecoveryStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/GatewaySnapshotStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/PeerRecoveryStatus.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/status/TransportIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/TransportDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/TransportGetIndexTemplatesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/TransportPutIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/get/TransportGetWarmersAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequest.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/delete/index/TransportShardDeleteAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/get/GetResponse.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetRequest.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportShardMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequest.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportShardMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchOperationThreading.java</file><file>src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchType.java</file><file>src/main/java/org/elasticsearch/action/search/ShardSearchFailure.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchScrollAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchHelper.java</file><file>src/main/java/org/elasticsearch/action/suggest/SuggestRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/suggest/TransportSuggestAction.java</file><file>src/main/java/org/elasticsearch/action/support/AbstractListenableActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/AdapterActionFuture.java</file><file>src/main/java/org/elasticsearch/action/support/DefaultShardOperationFailedException.java</file><file>src/main/java/org/elasticsearch/action/support/IndicesOptions.java</file><file>src/main/java/org/elasticsearch/action/support/TransportAction.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationThreading.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastShardOperationFailedException.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/TransportNodesOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ReplicationShardOperationFailedException.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ReplicationType.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndexReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndicesReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportShardReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/TransportSingleCustomOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/TransportShardSingleOperationAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorResponse.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportSingleShardMultiTermsVectorAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportSingleShardTermVectorAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java</file><file>src/main/java/org/elasticsearch/bootstrap/ElasticsearchF.java</file><file>src/main/java/org/elasticsearch/bulk/udp/BulkUdpService.java</file><file>src/main/java/org/elasticsearch/cache/recycler/CacheRecycler.java</file><file>src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java</file><file>src/main/java/org/elasticsearch/client/transport/NoNodeAvailableException.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterState.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeIndexDeletedAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeMappingRefreshAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlockException.java</file><file>src/main/java/org/elasticsearch/cluster/block/ClusterBlockLevel.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/AliasMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MappingMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexStateService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/ProcessClusterEventTimeoutException.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RepositoriesMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/RestoreMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/SnapshotMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNode.java</file><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNodes.java</file><file>src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingException.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/ShardRoutingState.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocationCommands.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file><file>src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java</file><file>src/main/java/org/elasticsearch/cluster/settings/Validator.java</file><file>src/main/java/org/elasticsearch/common/Preconditions.java</file><file>src/main/java/org/elasticsearch/common/Priority.java</file><file>src/main/java/org/elasticsearch/common/Strings.java</file><file>src/main/java/org/elasticsearch/common/Table.java</file><file>src/main/java/org/elasticsearch/common/blobstore/BlobStoreException.java</file><file>src/main/java/org/elasticsearch/common/blobstore/fs/FsImmutableBlobContainer.java</file><file>src/main/java/org/elasticsearch/common/breaker/CircuitBreakingException.java</file><file>src/main/java/org/elasticsearch/common/bytes/BytesArray.java</file><file>src/main/java/org/elasticsearch/common/bytes/HashedBytesArray.java</file><file>src/main/java/org/elasticsearch/common/collect/HppcMaps.java</file><file>src/main/java/org/elasticsearch/common/component/AbstractLifecycleComponent.java</file><file>src/main/java/org/elasticsearch/common/component/CloseableComponent.java</file><file>src/main/java/org/elasticsearch/common/component/Lifecycle.java</file><file>src/main/java/org/elasticsearch/common/component/LifecycleComponent.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/common/inject/Modules.java</file><file>src/main/java/org/elasticsearch/common/joda/DateMathParser.java</file><file>src/main/java/org/elasticsearch/common/lease/Releasable.java</file><file>src/main/java/org/elasticsearch/common/logging/log4j/ConsoleAppender.java</file><file>src/main/java/org/elasticsearch/common/lucene/all/AllEntries.java</file><file>src/main/java/org/elasticsearch/common/lucene/all/AllField.java</file><file>src/main/java/org/elasticsearch/common/network/NetworkUtils.java</file><file>src/main/java/org/elasticsearch/common/recycler/NoneRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/QueueRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/ThreadLocalRecycler.java</file><file>src/main/java/org/elasticsearch/common/regex/Regex.java</file><file>src/main/java/org/elasticsearch/common/rounding/DateTimeUnit.java</file><file>src/main/java/org/elasticsearch/common/rounding/Rounding.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/common/settings/SettingsException.java</file><file>src/main/java/org/elasticsearch/common/settings/loader/XContentSettingsLoader.java</file><file>src/main/java/org/elasticsearch/common/transport/TransportAddressSerializers.java</file><file>src/main/java/org/elasticsearch/common/unit/ByteSizeValue.java</file><file>src/main/java/org/elasticsearch/common/unit/DistanceUnit.java</file><file>src/main/java/org/elasticsearch/common/unit/SizeValue.java</file><file>src/main/java/org/elasticsearch/common/unit/TimeValue.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/AtomicArray.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/CountDown.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsAbortPolicy.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsRejectedExecutionException.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/EsThreadPoolExecutor.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/KeyedLock.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/SizeBlockingQueue.java</file><file>src/main/java/org/elasticsearch/common/util/concurrent/UncategorizedExecutionException.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentFactory.java</file><file>src/main/java/org/elasticsearch/common/xcontent/XContentHelper.java</file><file>src/main/java/org/elasticsearch/common/xcontent/json/JsonXContentParser.java</file><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/main/java/org/elasticsearch/common/xcontent/yaml/YamlXContent.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoveryException.java</file><file>src/main/java/org/elasticsearch/discovery/DiscoveryService.java</file><file>src/main/java/org/elasticsearch/discovery/MasterNotDiscoveredException.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java</file><file>src/main/java/org/elasticsearch/discovery/zen/fd/NodesFaultDetection.java</file></files><comments><comment>Cleanup comments and class names s/ElasticSearch/Elasticsearch</comment></comments></commit></commits></item><item><title>Rename `RobinEngine` to `LuceneEngine` or `DefaultEngine`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4633</link><project id="" key="" /><description>it seems like we settled on Lucene and I don't think we will have another `Engine` in the near future. I think the term `Robin` is misleading or rather confusing so I think we should move to a more appropriate name.
</description><key id="25127524">4633</key><summary>Rename `RobinEngine` to `LuceneEngine` or `DefaultEngine`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>v1.0.0.RC1</label></labels><created>2014-01-06T20:29:32Z</created><updated>2014-01-13T15:05:09Z</updated><resolved>2014-01-13T15:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequestBuilder.java</file><file>src/main/java/org/elasticsearch/index/deletionpolicy/SnapshotDeletionPolicy.java</file><file>src/main/java/org/elasticsearch/index/engine/IndexEngineModule.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalEngineModule.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalIndexEngine.java</file><file>src/main/java/org/elasticsearch/index/engine/internal/InternalIndexEngineModule.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/snapshots/IndexShardRepository.java</file><file>src/main/java/org/elasticsearch/search/internal/ContextIndexSearcher.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/HistogramAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchAndIndexingBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/child/ChildSearchShortCircuitBenchmark.java</file><file>src/test/java/org/elasticsearch/benchmark/search/nested/NestedSearchBenchMark.java</file><file>src/test/java/org/elasticsearch/benchmark/stress/NodesStressTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineIntegrationTest.java</file><file>src/test/java/org/elasticsearch/index/engine/internal/InternalEngineTests.java</file><file>src/test/java/org/elasticsearch/indices/fielddata/breaker/RandomExceptionCircuitBreakerTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/UpdateSettingsTests.java</file><file>src/test/java/org/elasticsearch/search/basic/SearchWithRandomExceptionsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchTestCase.java</file><file>src/test/java/org/elasticsearch/test/engine/MockEngineModule.java</file><file>src/test/java/org/elasticsearch/test/engine/MockInternalEngine.java</file></files><comments><comment>Rename RobinEngine and friends to InternalEngine</comment></comments></commit></commits></item><item><title>Add a message about import style</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4632</link><project id="" key="" /><description>We don't have a consistent style and we don't want new contributors to
get hung up on trying to figure out the style.
</description><key id="25110171">4632</key><summary>Add a message about import style</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-01-06T16:05:01Z</created><updated>2014-07-16T21:49:58Z</updated><resolved>2014-01-06T17:38:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-06T16:16:45Z" id="31660809">;) +1 /cc @uboness 
</comment><comment author="uboness" created="2014-01-06T17:27:23Z" id="31667452">:)
</comment><comment author="s1monw" created="2014-01-06T17:38:15Z" id="31668425">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Use a tolerance to decide if a value is less than the threshold</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4631</link><project id="" key="" /><description>Adding a small value to the threshold prevents weight deltas that are
very very close to the threshold to not trigger relocations. These
deltas can be rounding errors that lead to unnecessary relocations. In
practice this might only happen under very rare circumstances.
In general it's a good idea for the shard allocator to be a bit
more conservative in terms of rebalancing since in general relocation
costs are pretty high.

Closes #4630
</description><key id="25107120">4631</key><summary>Use a tolerance to decide if a value is less than the threshold</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-06T15:16:35Z</created><updated>2014-07-05T12:53:52Z</updated><resolved>2014-01-06T17:33:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-06T16:50:20Z" id="31664113">@jpountz pushed a new commit 
</comment><comment author="jpountz" created="2014-01-06T17:22:03Z" id="31666946">Looks good to me!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>BalancedShardAllocator might trigger unnecessary relocation under rare circumstances if deltas are very close to the threshold due to rounding issues.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4630</link><project id="" key="" /><description>Sometimes if there are very small number of nodes compared to large number of shards and indices deltas between nodes are very close to the default threshold `1.0` but due to the fact that we use floats we might end up with a weight of `1.000000001` which then in-turn triggers a relocation which is unnecessary and is kind of `undone` in the next iteration due to the same issue. 
</description><key id="25106510">4630</key><summary>BalancedShardAllocator might trigger unnecessary relocation under rare circumstances if deltas are very close to the threshold due to rounding issues.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-06T15:07:47Z</created><updated>2014-01-06T17:33:28Z</updated><resolved>2014-01-06T17:33:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file></files><comments><comment>Use a tolerance to decide if a value is less than the threshold</comment></comments></commit></commits></item><item><title>Change the `sort` boolean option in percolate api to the sort dsl available in search api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4629</link><project id="" key="" /><description>Relates to #4625
</description><key id="25102746">4629</key><summary>Change the `sort` boolean option in percolate api to the sort dsl available in search api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-06T13:56:22Z</created><updated>2015-05-18T23:33:15Z</updated><resolved>2014-01-09T09:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-09T09:00:14Z" id="31912946">Pushed to master: https://github.com/elasticsearch/elasticsearch/commit/7e341cefd0051163b6d0e6e539a95ff622f9df27
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allowed empty index parameter in open/close index api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4628</link><project id="" key="" /><description>Similarly to what happens with delete index api, curl -XPOST localhost:9200/_open opens now all indices &amp; curl -XPOST localhost:9200/_close closes now all indices.

Closes #4627
</description><key id="25102508">4628</key><summary>Allowed empty index parameter in open/close index api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2014-01-06T13:51:33Z</created><updated>2014-06-13T14:16:26Z</updated><resolved>2014-01-07T15:40:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-07T09:40:04Z" id="31724155">+1, looks good to me.
</comment><comment author="javanna" created="2014-01-07T15:40:58Z" id="31747933">Closing PR as this change is not needed anymore.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to open and close all indices without specifying any index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4627</link><project id="" key="" /><description>Currently, open/close index api require the index parameter, which can be a wildcard expression or even `_all`, but cannot be empty.

It is not possible to do `curl -XPOST localhost:9200/_close` as a validation error is thrown, while it is possible to do `curl -XDELETE localhost:9200/`.

With #4549 we are adding a settings that allows to control whether to allow or not potentially destructive operation on all indices. Those operations need to have the same behaviour, thus `curl -XPOST localhost:9200/_close` and `curl -XPOST localhost:9200/_open` should be possible as well, although disabled by default. 

This issue is about accepting the empty index parameter in open/close index api, so that we can either allow it or not depending on the `action.operate_all_indices` settings.
</description><key id="25100414">4627</key><summary>Allow to open and close all indices without specifying any index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>non-issue</label></labels><created>2014-01-06T13:03:38Z</created><updated>2014-01-07T15:40:24Z</updated><resolved>2014-01-07T15:40:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-01-07T15:40:24Z" id="31747872">Closing this issue as it's been decided to completely drop the `DELETE /` and `POST /_close` in favour of using wildcard expressions and _all, that can still be disabled through the new option introduced in #4549 .
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Rename `score` to `track_scores` in percolate api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4626</link><project id="" key="" /><description>Relates #4624
</description><key id="25100024">4626</key><summary>Rename `score` to `track_scores` in percolate api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-06T12:54:12Z</created><updated>2015-05-18T23:33:16Z</updated><resolved>2014-01-06T14:14:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-06T13:04:14Z" id="31646612">made a small comment, other than that, looks good.
</comment><comment author="martijnvg" created="2014-01-06T14:14:39Z" id="31650852">pushed to master via: https://github.com/elasticsearch/elasticsearch/commit/32c5471d330a62363dc5d1fe4703d211fd229129
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Change the `sort` boolean option in percolate api to the sort dsl available in search api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4625</link><project id="" key="" /><description>At the moment the `sort` boolean option allows to sort via percolate score in descending order. The full sort dsl should be supported, so that in the future if other sort options are available, the percolate api doesn't need to break backward compatibility. 

The following sort options will be the only supported option in percolate api:

```
{
...
"sort" : [
   "_score"
]
....
```
</description><key id="25099709">4625</key><summary>Change the `sort` boolean option in percolate api to the sort dsl available in search api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-06T12:46:15Z</created><updated>2014-01-09T08:59:37Z</updated><resolved>2014-01-09T08:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/sort/SortParseElement.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorFacetsAndAggregationsTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Change the `sort` boolean option in percolate api to the sort dsl available in search api.</comment></comments></commit></commits></item><item><title>Rename `score` to `track_scores` in percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4624</link><project id="" key="" /><description>The `score` option allows to include the score of a matched percolate query, without sorting by the score. This option should be renamed to `track_scores`, which is consistent with the same functionality in the search api (but then for matches documents).
</description><key id="25099407">4624</key><summary>Rename `score` to `track_scores` in percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-06T12:38:11Z</created><updated>2014-01-06T14:13:24Z</updated><resolved>2014-01-06T14:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file></files><comments><comment>Rename `score` to `track_scores` in percolate api.</comment></comments></commit></commits></item><item><title>Deprecated disable allocation decider in favour for the enable allocation decider.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4623</link><project id="" key="" /><description>Relates to #4488
</description><key id="25098949">4623</key><summary>Deprecated disable allocation decider in favour for the enable allocation decider.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-06T12:25:14Z</created><updated>2015-05-18T23:33:14Z</updated><resolved>2014-01-09T09:08:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-07T09:29:16Z" id="31723555">@s1monw I updated the PR with your comments.
</comment><comment author="s1monw" created="2014-01-07T11:23:16Z" id="31730105">left a small comment LGTM
</comment><comment author="martijnvg" created="2014-01-07T13:21:43Z" id="31736371">Thanks @s1monw for the review! I updated the PR and will push it soon (after updating the docs).
</comment><comment author="martijnvg" created="2014-01-09T09:08:55Z" id="31913481">Pushed to master: https://github.com/elasticsearch/elasticsearch/commit/e6f83248a2d0608b67baa6afa93ab4fd204eb101
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added `action.destructive_requires_name` that controls whether wildcard expressions are allowed for destructive operations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4622</link><project id="" key="" /><description>Added `action.destructive_requires_name` that controls whether wildcard expressions and `_all` is allowed to be used for destructive operations. Also the delete index api requires always an index to be specified (either concrete index, alias or wildcard expression)

Relates to  #4549 #4481
</description><key id="25096480">4622</key><summary>Added `action.destructive_requires_name` that controls whether wildcard expressions are allowed for destructive operations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-06T11:13:33Z</created><updated>2015-05-18T23:33:08Z</updated><resolved>2014-01-09T10:50:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-01-09T09:34:56Z" id="31915050">Looks good!
</comment><comment author="s1monw" created="2014-01-09T09:42:35Z" id="31915565">ah damned I commented on the commit not on the PR @martijnvg can you check? :)
</comment><comment author="martijnvg" created="2014-01-09T10:15:48Z" id="31917635">@s1monw I checked and I turned the DestructiveOperations into a statefull helper class!
</comment><comment author="s1monw" created="2014-01-09T10:29:36Z" id="31918532">LGTM thanks @martijnvg 
</comment><comment author="martijnvg" created="2014-01-09T10:50:57Z" id="31919841">Pushed to master: https://github.com/elasticsearch/elasticsearch/commit/eb63bb259d393354d4875c4e41dfed97edd142d1

Thanks for the review @s1monw 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Changed get index settings api to use new internal get index settings api instead of relying on the cluster state api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4621</link><project id="" key="" /><description>The new internal get index settings api is more efficient when it comes to sending the index settings from the master to the client via the node that received the initial request. Also the get index settings support now all the indices options and a new prefix option.

Relates to #4620
</description><key id="25095298">4621</key><summary>Changed get index settings api to use new internal get index settings api instead of relying on the cluster state api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-06T10:44:00Z</created><updated>2015-05-18T23:33:19Z</updated><resolved>2014-01-08T12:19:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-08T12:19:52Z" id="31826689">pushed to master: https://github.com/elasticsearch/elasticsearch/commit/6dc434822c848f6cdec515254983c83712c4ed58
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add internal get index settings api, that the GET /_settings api uses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4620</link><project id="" key="" /><description>The internal get index settings api can retrieve index settings more efficiently than the cluster state api does, which is what the rest get index settings api now uses.

On top of this the get settings api will now also fully support the indices options (`ignore_missing`, `allow_no_indices` and `expand_wildcards`) and the get settings api will also support a new `prefix` option, which allows to only include settings that start with a specific prefix.
</description><key id="25094033">4620</key><summary>Add internal get index settings api, that the GET /_settings api uses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-06T10:14:59Z</created><updated>2014-01-08T12:19:15Z</updated><resolved>2014-01-08T12:19:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/TransportGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsResponse.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataUpdateSettingsService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestGetSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>src/test/java/org/elasticsearch/blocks/SimpleBlocksTests.java</file><file>src/test/java/org/elasticsearch/cluster/BlockClusterStatsTests.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/UpdateSettingsTests.java</file></files><comments><comment>Changed get index settings api to use new internal get index settings api instead of relying on the cluster state api.</comment></comments></commit></commits></item><item><title>Cannot forcefully unlock a NativeFSLock which is held by another indexer component</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4619</link><project id="" key="" /><description>Hello,
I'm running ES as single node cluster, and I'm using logtash. I have ~400 indexes. And now nothing happens :-) only 
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /opt/kullm
ann/elasticsearch-0.90.9/data/elasticsearch/nodes/0/indices/ordered-items-2013.11.02/3/index/write.lock
        at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
        at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)
        at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1415)
        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:254)

I already tried sysctl -w vm.max_map_count=262144 described here https://github.com/elasticsearch/elasticsearch/issues/4547 but no effect. 
</description><key id="25077626">4619</key><summary>Cannot forcefully unlock a NativeFSLock which is held by another indexer component</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">areman</reporter><labels /><created>2014-01-05T21:05:14Z</created><updated>2014-07-23T14:01:51Z</updated><resolved>2014-07-23T14:01:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gregoryb" created="2014-03-13T13:54:26Z" id="37534902">Go the same issue:

[2014-03-13 14:28:09,047][WARN ][cluster.action.shard     ] [ES 8 - V101] [events][1] sending failed shard for [events][1], node[zT2Fk-DdQ4KQaH_HyIS5BA], [P], s[INITIALIZING], indexUUID [3st3bmkVRP6RcMeUXB5WdA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[events][1] failed recovery]; nested: EngineCreationFailureException[[events][1] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /data/elasticsearch-prod/nodes/0/indices/events/1/index/write.lock]; ]]
[2014-03-13 14:28:09,048][WARN ][cluster.action.shard     ] [ES 8 - V101] [events][1] received shard failed for [events][1], node[zT2Fk-DdQ4KQaH_HyIS5BA], [P], s[INITIALIZING], indexUUID [3st3bmkVRP6RcMeUXB5WdA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[events][1] failed recovery]; nested: EngineCreationFailureException[[events][1] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /data/elasticsearch-prod/nodes/0/indices/events/1/index/write.lock]; ]]
[2014-03-13 14:28:09,146][WARN ][cluster.action.shard     ] [ES 8 - V101] [events][1] received shard failed for [events][1], node[bxXlduIDQbK6AHuSXAodkg], [P], s[INITIALIZING], indexUUID [3st3bmkVRP6RcMeUXB5WdA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[events][1] failed recovery]; nested: EngineCreationFailureException[[events][1] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /data/elasticsearch-prod/nodes/0/indices/events/1/index/write.lock]; ]]
[2014-03-13 14:28:10,337][WARN ][index.engine.internal    ] [ES 8 - V101] [events][1] shard is locked, releasing lock
[2014-03-13 14:28:10,339][WARN ][indices.cluster          ] [ES 8 - V101] [events][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [events][1] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [events][1] failed to create engine
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:260)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:706)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /data/elasticsearch-prod/nodes/0/indices/events/1/index/write.lock
    at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
    at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4472)
    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1354)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:258)
    ... 6 more
[2014-03-13 14:28:10,395][WARN ][cluster.action.shard     ] [ES 8 - V101] [events][1] sending failed shard for [events][1], node[zT2Fk-DdQ4KQaH_HyIS5BA], [P], s[INITIALIZING], indexUUID [3st3bmkVRP6RcMeUXB5WdA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[events][1] failed recovery]; nested: EngineCreationFailureException[[events][1] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /data/elasticsearch-prod/nodes/0/indices/events/1/index/write.lock]; ]]
[2014-03-13 14:28:10,399][WARN ][cluster.action.shard     ] [ES 8 - V101] [events][1] received shard failed for [events][1], node[zT2Fk-DdQ4KQaH_HyIS5BA], [P], s[INITIALIZING], indexUUID [3st3bmkVRP6RcMeUXB5WdA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[events][1] failed recovery]; nested: EngineCreationFailureException[[events][1] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /data/elasticsearch-prod/nodes/0/indices/events/1/index/write.lock]; ]]
[2014-03-13 14:28:11,458][WARN ][cluster.action.shard     ] [ES 8 - V101] [events][1] received shard failed for [events][1], node[bxXlduIDQbK6AHuSXAodkg], [P], s[INITIALIZING], indexUUID [3st3bmkVRP6RcMeUXB5WdA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[events][1] failed recovery]; nested: EngineCreationFailureException[[events][1] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /data/elasticsearch-prod/nodes/0/indices/events/1/index/write.lock]; ]]
[2014-03-13 14:28:11,514][WARN ][index.engine.internal    ] [ES 8 - V101] [events][1] shard is locked, releasing lock
[2014-03-13 14:28:11,519][WARN ][indices.cluster          ] [ES 8 - V101] [events][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [events][1] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</comment><comment author="GuillaumeDievart" created="2014-03-18T16:01:25Z" id="37950717">Hello, 

I use es 1.0.1 and I have the same issue:

[2014-03-18 01:03:17,080][WARN ][cluster.action.shard     ] [] [][4] sending failed shard for [mdn][4], node[OgSk8g38S9WZfl9GE-D_Jg], [P], s[INITIALIZING], indexUUID [60FeoYqVR_W3Gg8jG8exew], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[][4] failed recovery]; nested: EngineCreationFailureException[[mdn][4] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /var/lib/elasticsearch/elasticsearch/nodes/0/indices/mdn/4/index/write.lock]; ]]
[2014-03-18 01:03:17,080][WARN ][cluster.action.shard     ] [master node] [mdn][4] received shard failed for [mdn][4], node[OgSk8g38S9WZfl9GE-D_Jg], [P], s[INITIALIZING], indexUUID [60FeoYqVR_W3Gg8jG8exew], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[mdn][4] failed recovery]; nested: EngineCreationFailureException[[mdn][4] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /var/lib/elasticsearch/elasticsearch/nodes/0/indices/mdn/4/index/write.lock]; ]]
[2014-03-18 01:03:17,084][WARN ][index.engine.internal    ] [master node] [mdn][2] shard is locked, releasing lock
[2014-03-18 01:03:17,084][WARN ][indices.cluster          ] [master node] [mdn][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [mdn][2] failed recovery
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [mdn][2] failed to create engine
        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:260)
        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:706)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
        ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /var/lib/elasticsearch/elasticsearch/nodes/0/indices/mdn/2/index/write.lock
        at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
        at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4472)
        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1354)
        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:258)
        ... 6 more

It happened after to have indexed 10 millions documents with jdbc-river.
</comment><comment author="iaindjackson" created="2014-03-19T11:52:45Z" id="38042258">I have seen this on my ES 1.0.1 setup when I was loading ~1 million documents.

Looking into it, its nothing to do with the vm.max_map_count as I started to hit issues with an index of about ~4Gb.  The number of open maps was only about 50,000.

I resolved it by increasing the virtual memory available to the ES process. It started to fail when the process size exceeded about 4.8Gb. My Linux distro (SUSE) imposes a maximum virtual memory size by default. I increased this to unlimited - 'ulimit -v unlimited' - before starting the elasticsearch daemon and it is now loading in all my documents.
</comment><comment author="clintongormley" created="2014-07-23T14:01:51Z" id="49877382">We _think_  this should be fixed in 1.3 with https://issues.apache.org/jira/browse/LUCENE-5544 and various other file locking bugs. If you are still seeing these issues with 1.3, please could you reopen
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose header names in RestRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4618</link><project id="" key="" /><description>closes #4609
</description><key id="25052516">4618</key><summary>Expose header names in RestRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels /><created>2014-01-04T11:59:21Z</created><updated>2014-07-02T09:02:33Z</updated><resolved>2014-01-08T22:14:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-06T23:01:57Z" id="31696974">apologies for stepping in too late, but I think that it makes sense to simplify this as was suggested originally. The reason for it is the fact that we have other implementation of the REST layer (like the thrift plugin, ...), and the more complex headers we have, the harder it is to implement it.

I believe it was raised by the need to have the ability to iterate over the headers, so I would suggest simply allowing to get back the list of headers would be a great and simple way to solve it. 

Thoughts?
</comment><comment author="lmenezes" created="2014-01-06T23:05:09Z" id="31697211">It's good enough for me. Originally all I needed was a way of retrieving the list of headers name. If it's ok for you, I will just roll back to the original PR.
</comment><comment author="kimchy" created="2014-01-06T23:06:03Z" id="31697277">@lmenezes ++, and sorry for not chipping in earlier
</comment><comment author="lmenezes" created="2014-01-06T23:07:17Z" id="31697386">no problems, it was a lot of boiler plate code anyway.
</comment><comment author="lmenezes" created="2014-01-07T00:01:16Z" id="31700929">@kimchy looks ok?
</comment><comment author="lmenezes" created="2014-01-08T20:52:25Z" id="31876604">@kimchy anything else I could/should do?
</comment><comment author="s1monw" created="2014-01-08T22:14:56Z" id="31882167">pushed thx
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>JODA Formats do not support Unix epoch in seconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4617</link><project id="" key="" /><description>Currently if a field is a unix timestamp in seconds, date will fail to process it properly. There are no semantics in JODA to say that part of a string is a unix time stamp (for formatting). Everything I'm coming across simply says to multiply the number by 1000.

Example date: 1386890902.829732
Example from bash: date +%s.%N

One possible way to address this is simply to have input filters for long fields allowing math or string manipulation on the input data prior to casting it to a Java Long.
</description><key id="25048840">4617</key><summary>JODA Formats do not support Unix epoch in seconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2014-01-04T05:36:14Z</created><updated>2017-06-18T16:35:37Z</updated><resolved>2014-12-24T17:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:49:50Z" id="68066610">Hi @Downchuck 

Sorry it has taken a while to look at this.  Yes, multiplying by 1000 is about the only thing to do here.  You could do this automatically with a [transform script](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-transform.html#mapping-transform)
</comment><comment author="Downchuck" created="2014-12-24T18:22:56Z" id="68068089">Thanks @clintongormley - the transform script functionality addresses the issue quite well.
</comment><comment author="OrangeDog" created="2015-09-22T16:04:53Z" id="142334255">Looks like now you can set `"numeric_resolution": "seconds"`?
</comment><comment author="HumanAlgo" created="2017-06-18T16:35:37Z" id="309288246">http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-transform.html#mapping-transform

This link  is no longer available. Please provide the solution here</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to configure circuit breaker with a percentage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4616</link><project id="" key="" /><description>It would be nice to be able to say "75%" instead of absolute values.
</description><key id="25040376">4616</key><summary>Add ability to configure circuit breaker with a percentage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.0.0</label></labels><created>2014-01-03T22:43:15Z</created><updated>2014-01-21T12:57:07Z</updated><resolved>2014-01-21T12:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-09T14:34:21Z" id="31936244">@dakrone that shouldn't be hard given the stuff @jpountz added no? Can you take a look at it?
</comment><comment author="dakrone" created="2014-01-09T16:38:37Z" id="31950522">@s1monw planning on it but finishing up #4483 first.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/cluster/settings/Validator.java</file><file>src/main/java/org/elasticsearch/common/unit/MemorySizeValue.java</file><file>src/main/java/org/elasticsearch/indices/fielddata/breaker/InternalCircuitBreakerService.java</file><file>src/test/java/org/elasticsearch/indices/fielddata/breaker/CircuitBreakerServiceTests.java</file></files><comments><comment>Allow to configure indices.fielddata.breaker.limit with a ratio of the heap size.</comment></comments></commit></commits></item><item><title>`cluster.routing.allocation.same_shard.host` should be documented...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4615</link><project id="" key="" /><description>something like this:

```
`cluster.routing.allocation.same_shard.host`::                                                                                                                  
      Allows to enable checks that prevent more than one replica of the                                                                                         
      same shard to be allocated on the same physical host. The checks                                                                                          
      are based on the nodes host name. nodes with the same host name are                                                                                       
      treated as running on the same physical host. Default is `false`. 
```
</description><key id="25037432">4615</key><summary>`cluster.routing.allocation.same_shard.host` should be documented...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>docs</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-03T21:39:07Z</created><updated>2014-01-09T10:25:09Z</updated><resolved>2014-01-09T10:25:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java</file></files><comments><comment>[DOCS] Added documentation for SameShardAllocationDecider</comment></comments></commit></commits></item><item><title>GET _indices API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4614</link><project id="" key="" /><description>With the removal of _aliases (#4539) we've lost an endpoint for efficiently listing available indices &amp; aliases, which Kibana relies on for computing timestamped indices.

It would be nice to have an _indices endpoint that could efficiently return a list of indices with a configurable amount of additional granularity, eg mappings, settings, aliases. 

Bonus points if we could support wildcard matching here for both index and alias names, eg

*\* Every index name **

```
/_indices
```

*\* Every index with its aliases and settings **

```
/_indices?aliases
```

*\* Every index named mydata-\* or with an alias matching mydata-_, along with its mapping *_

```
/mydata-*/_indices?mapping
```
</description><key id="25031991">4614</key><summary>GET _indices API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rashidkpc</reporter><labels><label>enhancement</label></labels><created>2014-01-03T19:46:18Z</created><updated>2014-04-28T11:19:23Z</updated><resolved>2014-04-28T11:19:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-04-28T11:19:09Z" id="41547078">Closing in favor of #4069
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Default the circuit breaker limit to 80% of the maximum JVM heap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4613</link><project id="" key="" /><description>Fixes #4604.
</description><key id="25027321">4613</key><summary>Default the circuit breaker limit to 80% of the maximum JVM heap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label></labels><created>2014-01-03T18:11:54Z</created><updated>2016-10-25T23:08:03Z</updated><resolved>2014-01-03T23:21:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-03T19:21:36Z" id="31546413">LGTM
</comment><comment author="dakrone" created="2014-01-03T23:21:35Z" id="31562415">Merged via 47607a6
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add rest of stats to various cat endpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4612</link><project id="" key="" /><description>With flexible header calling, we can add many more columns.  This pull request tracks their addition.
</description><key id="25026619">4612</key><summary>Add rest of stats to various cat endpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-03T17:56:23Z</created><updated>2014-06-16T20:28:55Z</updated><resolved>2014-01-06T22:18:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix potential infinite loop in double wildcard processing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4611</link><project id="" key="" /><description>Fixes #4610
</description><key id="25026154">4611</key><summary>Fix potential infinite loop in double wildcard processing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-01-03T17:46:39Z</created><updated>2014-06-13T13:06:52Z</updated><resolved>2014-01-03T19:14:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Double wildcards in the the index name can cause a request to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4610</link><project id="" key="" /><description>Double wildcards with non-matching index pattern can cause [Regex#simpleMatch](https://github.com/elasticsearch/elasticsearch/blob/6a04c169326ab99c1e5b4eef6f9fdbed222b5fa0/src/main/java/org/elasticsearch/common/regex/Regex.java#L81) to go into infinite loop. To reproduce, call `Regex.simpleMatch("**ddd", "fff")`.
</description><key id="25026018">4610</key><summary>Double wildcards in the the index name can cause a request to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-03T17:43:35Z</created><updated>2014-01-03T19:14:24Z</updated><resolved>2014-01-03T19:14:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-03T18:57:17Z" id="31544670">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/regex/Regex.java</file><file>src/test/java/org/elasticsearch/common/regex/RegexTests.java</file></files><comments><comment>Fix potential infinite loop in double wildcard processing</comment></comments></commit></commits></item><item><title>[Feature Request] - Expose headers on HttpRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4609</link><project id="" key="" /><description>There is a method available for retrieving the value of a header, but it's not possible to iterate over all headers. 

Is there a reason why this is not exposed? If not, would be nice having that.
</description><key id="25022779">4609</key><summary>[Feature Request] - Expose headers on HttpRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-03T16:40:16Z</created><updated>2014-01-15T19:09:20Z</updated><resolved>2014-01-08T22:18:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2014-01-03T16:45:55Z" id="31535009">btw, it would also be totally fine just exposing the list of header names.
</comment><comment author="s1monw" created="2014-01-03T19:22:22Z" id="31546469">do you wanna come up with a PR?
</comment><comment author="lmenezes" created="2014-01-03T19:28:47Z" id="31546980">sure. thanks :)
</comment><comment author="lmenezes" created="2014-01-06T13:46:50Z" id="31648995">@s1monw I just created a wrapper for the headers. When pushing -f We lost part of the comments that were there.
 I created the wrapper, deprecated the current getHeader and updated parts of the code that were using the deprecated method. Looks good now?

By the way, how do you usually work with updating a pull request like this? Just rebasing and push -f? 
</comment><comment author="s1monw" created="2014-01-06T17:01:44Z" id="31665162">I think it looks good. Would you mind adding some javadocs to the new interface? I usually try to just stack up commits until we are good to go and then squash once everybody is happy.
</comment><comment author="lmenezes" created="2014-01-06T22:55:32Z" id="31696513">Javadocs done. Let me know if there is anything else.
</comment><comment author="s1monw" created="2014-01-08T22:18:38Z" id="31882429">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/http/netty/NettyHttpRequest.java</file><file>src/main/java/org/elasticsearch/rest/RestRequest.java</file></files><comments><comment>Expose headers list in RestRequest</comment></comments></commit></commits></item><item><title>REST cluster state API: Improved consistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4608</link><project id="" key="" /><description>Instead of specifying what kind of data should be filtered, this commit
streamlines the API to actually specify, what kind of data should be displayed.

A small feature has been added as well: If you specify an index to select on, not
only the metadata, but also the routing tables are filtered by index in order
to prevent too big cluster states to be returned.

Also the CAT apis have been changed to only return the wanted data in order to keep
network traffic as small as needed.

Tests for the cluster state API filtering have been added as well.

Note: This change breaks backwards compatibility with 0.90!

Closes #4065
</description><key id="25022216">4608</key><summary>REST cluster state API: Improved consistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>v1.0.0.RC1</label></labels><created>2014-01-03T16:29:19Z</created><updated>2014-07-05T21:39:00Z</updated><resolved>2014-01-08T08:45:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-06T09:44:38Z" id="31636844">@kimchy improved the cluster state API to work like other requests (`NodeIndicesStatsRequest`) and include everything by default, and allowing to deselect everything and select it again instead of filtering it.
</comment><comment author="kimchy" created="2014-01-07T14:08:01Z" id="31739562">LGTM!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cat: Add rest of stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4607</link><project id="" key="" /><description>We want to add all of the index and node stats.
</description><key id="25022197">4607</key><summary>cat: Add rest of stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-03T16:29:02Z</created><updated>2014-01-06T23:24:56Z</updated><resolved>2014-01-06T23:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestShardsAction.java</file></files><comments><comment>Add rest of index stats to cat/nodes and cat/shards.</comment></comments></commit></commits></item><item><title>Fixed open/close index api when using wildcard only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4606</link><project id="" key="" /><description>Named wildcards were not always properly replaced with proper values by PathTrie.
Delete index (`curl -XDELETE localhost:9200/*`) worked anyway as the named wildcard is the last path element. 
When the named wildcard wasn't the last path element (e.g. `curl -XPOST localhost:29200/*/_close`), the variable didn't get replaced with the current '*' value, but with the empty string, which lead to an error as empty index is not allowed by open/close index.

Closes #4564
</description><key id="25021958">4606</key><summary>Fixed open/close index api when using wildcard only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2014-01-03T16:24:51Z</created><updated>2014-06-27T05:16:32Z</updated><resolved>2014-01-08T14:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-07T11:14:02Z" id="31729645">+1 I didn't see anything suspicious. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Highlighting - too long snippets for phrase queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4605</link><project id="" key="" /><description>I've stumbled upon a weird bug today - highlight snippets are much too long for some queries with phrases used.
After some searching I've found an excellent example of this behavior by @omarkhan
https://gist.github.com/omarkhan/2843912
(whole description here: http://elasticsearch-users.115913.n3.nabble.com/Very-long-highlighted-snippets-with-some-query-string-queries-td4018627.html )

I will only add that I'm having the same problem with new "simple_query_string" query.
</description><key id="25021488">4605</key><summary>Highlighting - too long snippets for phrase queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ocher</reporter><labels /><created>2014-01-03T16:16:06Z</created><updated>2014-10-01T07:59:23Z</updated><resolved>2014-07-09T12:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="omarkhan" created="2014-01-04T12:49:02Z" id="31577720">As a workaround I ended up truncating the snippets client-side and passing them through BeautifulSoup to clean up the html in case it truncated in the middle of a tag. Messy and slow, but effective.
</comment><comment author="clintongormley" created="2014-07-09T12:35:45Z" id="48463782">This appears to be fixed in master.  Please reopen this issue if you have other examples demonstrating otherwise.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Set default for circuit breaker to 80% of the maximum heap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4604</link><project id="" key="" /><description>The current default is -1 (no limit).
</description><key id="25021191">4604</key><summary>Set default for circuit breaker to 80% of the maximum heap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2014-01-03T16:10:06Z</created><updated>2014-01-03T23:21:52Z</updated><resolved>2014-01-03T23:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-03T23:21:52Z" id="31562428">Merged via 47607a6
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>plugin manager: new `timeout` option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4603</link><project id="" key="" /><description>When testing plugin manager with real downloads, it could happen that the test run forever. Fortunately, test suite will be interrupted after 20 minutes, but it could be useful not to fail the whole test suite but only warn in that case.

By default, plugin manager still wait indefinitely but it can be modified using new `--timeout` option:

``` sh
bin/plugin --install elasticsearch/kibana --timeout 30s

bin/plugin --install elasticsearch/kibana --timeout 1h
```
</description><key id="25020171">4603</key><summary>plugin manager: new `timeout` option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>test</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-03T15:49:37Z</created><updated>2014-07-25T11:41:34Z</updated><resolved>2014-01-03T15:51:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/test/java/org/elasticsearch/plugin/PluginManagerTests.java</file></files><comments><comment>plugin manager: new `timeout` option</comment></comments></commit></commits></item><item><title>Add NodeVersionAllocationDecider that prevent allocations that require forward compatibility.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4602</link><project id="" key="" /><description>Today during restart scenarios it is possible that we recover from a node that
has already been upgraded to version N+1. The node that we relocate to is
on version N and might not be able to read the index format from the node
we relocate from. This causes `IndexFormatToNewException` during
recovery but only after recovery has finished which can cause large
load spikes during the upgrade period.

Closes #4588
</description><key id="25013447">4602</key><summary>Add NodeVersionAllocationDecider that prevent allocations that require forward compatibility.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-03T12:58:08Z</created><updated>2014-07-11T07:58:00Z</updated><resolved>2014-01-03T14:58:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-03T13:42:28Z" id="31522843">+1, looks good!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES v0.20.6 significant faster then v0.90.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4601</link><project id="" key="" /><description>im having a weird performance issue, where a way more powerfull machine is much slower then the old one.

Old machine:
- 4 cpu cores
- 12 GB ra,
- ES 0.20.5
- java 7u21 (oracle)
- normal HDD

New machine:
- 12 cpu core (with more cache)
- 32 GB ram (higher mhz rate)
- ES 0.90.8 
- java (oracle 7u45)
- SSD with cascaded cache

Both in common:
- index size of 2 GB
- indexed documents (around 120.000)
- cache size of 40%
- mlock off
- SAME INDEX (same docs, same mapping)
- SAME QUERIES
- rest of the config settings untouched and as default from the deb package
- both debian 7 systems (updated)
- tested with curl requests on the bash

The more powerfull machine was about 50ms (from 40-50ms up to 100ms-120ms)
so half the speed as before.

What i tried so far:
- downgrade java version to the same 7u21 (little speed gain for about 7-10ms but that could be measure problems)
- looked at any changed default values in configs
  - found a change where thread pools changed from cached to fixed (reset that values back to the old default), nearly no change.
- downgraded (everytime with reindex) one by one the ES versions. which changed nothing until i reached 0.20.6 where i got the speed i got on 0.20.5
- upgraded again and have the same speed issue.
- store mode changed from niofs to mmapfs -&gt; slower
- store mode changed to memory -&gt; bit faster but still slower than 0.20.6 (verfified that its really in memory through restart and empty index)
- tested both with the _source compress set to false. cause this was also a change from 0.20.6 to 0.90.0

Im a bit lost and searching for a way to investigate this more. Is this a known information? Any one else saw this speed decrease? 
</description><key id="25010425">4601</key><summary>ES v0.20.6 significant faster then v0.90.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">julianhille</reporter><labels><label>discuss</label></labels><created>2014-01-03T11:17:29Z</created><updated>2014-07-25T09:37:17Z</updated><resolved>2014-07-25T09:37:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="avleen" created="2014-01-10T05:15:55Z" id="32003306">iirc, you can't actually store the data uncompressed. I think setting _source compression to false doesn't _really_ disable compression. The docs seem to concur, that the compression setting is for versions &lt; 0.90.
Have you tried increasing the size of the thread pools for indexing?
I found that to be helpful sometimes.
</comment><comment author="julianhille" created="2014-01-10T13:43:07Z" id="32027786">I understood the compression value like, it only changed the default value.
The compress setting is still in the 

```
    ./src/test/java/org/elasticsearch/test/unit/index/mapper/source/CompressSourceMappingTests.java
```

The test ran fine and seems to test the compress value.

Thread pool resize for indexing? Why should this be considered if nothing is indexed?
</comment><comment author="jpountz" created="2014-01-10T13:53:45Z" id="32028530">What queries did you run to compare both versions? It would also be interesting to know the size of your documents.
</comment><comment author="julianhille" created="2014-01-10T15:13:27Z" id="32034972">i added the index size and indexed documents to the "both in commin" list.
I did the same queries on botch machine. Its basically a 

```
{
  "filter": {....exists, bool: true (ie.) }
  "facets":{
    20x
    "0": {
      "facet_filter": {
        "and": [
          {
            .... exists, some boolean: true/false filter
        ]
      },
      "terms": {
        "field": "the.nested.field",
        "size": 100000 (getting all of them)
      }  
    }
  },
  "size": 100,
   "fields"; [10Fields]
}
```

What i saw is:
- every facet is now 1-2ms slower then with the old version
- with 20 facets this can sum up to 20-30ms

The facet size is in max 6000
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>plugin manager: new `timeout` option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4600</link><project id="" key="" /><description>When testing plugin manager with real downloads, it could happen that the test run forever. Fortunately, test suite will be interrupted after 20 minutes, but it could be useful not to fail the whole test suite but only warn in that case.

By default, plugin manager still wait indefinitely but it can be modified using new `--timeout` option:

``` sh
bin/plugin --install elasticsearch/kibana --timeout 30s

bin/plugin --install elasticsearch/kibana --timeout 1h
```
</description><key id="25010362">4600</key><summary>plugin manager: new `timeout` option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-01-03T11:15:11Z</created><updated>2014-06-14T04:34:01Z</updated><resolved>2014-01-03T15:51:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-03T14:56:08Z" id="31526895">+1
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/http/client/HttpDownloadHelper.java</file><file>src/main/java/org/elasticsearch/plugins/PluginManager.java</file><file>src/test/java/org/elasticsearch/plugin/PluginManagerTests.java</file></files><comments><comment>plugin manager: new `timeout` option</comment></comments></commit></commits></item><item><title>Fix for issue #4083-  Make 'length' parameters consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4599</link><project id="" key="" /><description>Java Builder apis drop old “len” methods in favour of new “length”
Rest APIs support both old “len: and new “length” forms
Documentation and rest specs only document the new “*length” forms with added “since 0.90.10” notice
</description><key id="25008714">4599</key><summary>Fix for issue #4083-  Make 'length' parameters consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels /><created>2014-01-03T10:21:34Z</created><updated>2014-06-16T20:32:17Z</updated><resolved>2014-01-13T16:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-03T10:25:43Z" id="31514897">+1 looks good to me
</comment><comment author="kimchy" created="2014-01-03T11:05:43Z" id="31516537">I think we should push this only to master (1.0), so the docs do not need to reflect "since 0.90.10"?
</comment><comment author="s1monw" created="2014-01-13T09:37:48Z" id="32155050">I only found spaces issues - can you please fix them? other than that LGTM. I think you should also squash the commits into one no?
</comment><comment author="markharwood" created="2014-01-13T16:06:38Z" id="32182765">Pushed to master via https://github.com/elasticsearch/elasticsearch/commit/2795f4e55d0cbda397ef29887d3c9dee35c467b4
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Check if node is still present when collecting attribute shard routings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4598</link><project id="" key="" /><description>The node we need to lookup for attribute colelction might not be part
of the `DiscoveryNodes` anymore due to node failure or shutdown. This
commit adds a check and removes the shard from the iteration.

Closes #4589
</description><key id="25006138">4598</key><summary>Check if node is still present when collecting attribute shard routings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-03T09:02:30Z</created><updated>2014-07-14T11:35:51Z</updated><resolved>2014-01-03T13:04:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-03T11:14:21Z" id="31516862">LGTM
</comment><comment author="s1monw" created="2014-01-03T13:04:45Z" id="31521144">merged 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Accurate Haversine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4597</link><project id="" key="" /><description>Setup an accurate version of Haversine formula for `GeoDistance.ARC`

Closes #4596
</description><key id="25004153">4597</key><summary>Accurate Haversine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2014-01-03T07:29:15Z</created><updated>2014-07-16T21:50:06Z</updated><resolved>2014-01-03T09:10:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-03T08:38:00Z" id="31510847">+1 to push
</comment><comment author="s1monw" created="2014-01-03T08:40:05Z" id="31510913">+1 squash and push!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Using Haversine for accurate distance measurement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4596</link><project id="" key="" /><description>The current implementation of an accurate distance calculation is not accurate for distances less than _1m_. Since the _haversine_ function is more robust against rounding error of floating point arithmetic, the great circle distance should be replaced by an accurate implementation of the haversine function.

Related to #4498
</description><key id="25003728">4596</key><summary>Using Haversine for accurate distance measurement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-03T07:06:09Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-01-03T09:10:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file></files><comments><comment>Setup an accurate version of Haversine closes #4596</comment></comments></commit></commits></item><item><title>Refresh the id_cache if a new child type with _parent field has been introduced</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4595</link><project id="" key="" /><description>Already loaded SimpleIdReaderCache should be reloaded when a new `_parent` has been introduced.

Relates #4568
</description><key id="24991891">4595</key><summary>Refresh the id_cache if a new child type with _parent field has been introduced</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T22:40:36Z</created><updated>2015-05-18T23:33:19Z</updated><resolved>2014-01-03T14:50:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-03T12:12:38Z" id="31519124">We should call clear when a new document is added, since effectively we do it anyhow. `clear` method can be improved by creating a copy of the map and iterating over it, so we properly call onRemoval only if it was actually removed. Other than that, LGTM.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentTypeListener.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/test/java/org/elasticsearch/index/cache/id/SimpleIdCacheTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Already loaded SimpleIdReaderCache should be reloaded when a new `_parent` has been introduced.</comment></comments></commit></commits></item><item><title>Change `fields` behaviour in get and search api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4594</link><project id="" key="" /><description>See #4542
</description><key id="24991844">4594</key><summary>Change `fields` behaviour in get and search api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-01-02T22:39:21Z</created><updated>2015-05-18T23:33:19Z</updated><resolved>2014-01-03T16:31:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-03T16:31:01Z" id="31533839">pushed to master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add /_cat/fielddata to display fielddata on a per-node per-field basis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4593</link><project id="" key="" /><description>This would be great for figuring out where all your memory is going with regard to field data:

```
$ curl localhost:9200/_cat/fielddata
node   body  timestamp   subject  myotherfield  total
node1  7gb       501mb     1.1gb          17mb   10gb
node2  1.3gb     100mb     981mb           2mb  3.1gb
node3  2.7gb      81mb     182mb         512kb  3.2gb
```

It would be a nice-to-have for 1.0 RC1, but not required.
</description><key id="24991451">4593</key><summary>Add /_cat/fielddata to display fielddata on a per-node per-field basis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>feature</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-01-02T22:30:09Z</created><updated>2014-05-12T09:13:02Z</updated><resolved>2014-05-09T11:33:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestFielddataAction.java</file></files><comments><comment>Add /_cat/fielddata to display fielddata usage</comment></comments></commit></commits></item><item><title>Add field data circuit breaker to stop field data loading from running out of memory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4592</link><project id="" key="" /><description>We should add a circuit breaker to prevent Elasticsearch from running out of memory when field data is loaded.
</description><key id="24990328">4592</key><summary>Add field data circuit breaker to stop field data loading from running out of memory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>feature</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T22:06:20Z</created><updated>2014-01-02T22:08:12Z</updated><resolved>2014-01-02T22:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-02T22:06:38Z" id="31489037">Added in https://github.com/elasticsearch/elasticsearch/commit/a7542247516d594f612044f33b95db39d27c5393
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>How to find the different types in an Index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4591</link><project id="" key="" /><description>I am new to Elastic Search and have come from a pure SQL background. I find the JSON queries a little confusing and have been trying to find a simple way of knowing what are the different Types in an Index. Can  someone pls help.
</description><key id="24988858">4591</key><summary>How to find the different types in an Index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">charshar</reporter><labels /><created>2014-01-02T21:37:33Z</created><updated>2014-01-03T01:30:31Z</updated><resolved>2014-01-03T01:30:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-03T01:30:31Z" id="31499812">Please use the mailing list for questions. See http://www.elasticsearch.org/help/
Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add cache stats to cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4590</link><project id="" key="" /><description>Closes #4543.
</description><key id="24981459">4590</key><summary>Add cache stats to cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-01-02T19:15:23Z</created><updated>2014-07-16T21:50:07Z</updated><resolved>2014-01-03T16:20:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-02T19:17:33Z" id="31476581">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NullPointerException in IndexShardRoutingTable.getActiveAttribute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4589</link><project id="" key="" /><description>im getting error below after killing one node in the cluster 
(exception is thrown on remaining nodes)

```
org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution
        at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:90)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:49)
        at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67)
        ...
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:288)
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:275)
        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
        ... 15 more
Caused by: java.lang.NullPointerException
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.getActiveAttribute(IndexShardRoutingTable.java:441)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.preferAttributesActiveInitializingShardsIt(IndexShardRoutingTable.java:488)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.preferAttributesActiveInitializingShardsIt(IndexShardRoutingTable.java:483)
        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.preferenceActiveShardIterator(PlainOperationRouting.java:169)
        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.getShards(PlainOperationRouting.java:80)
        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:80)
        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:42)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.&lt;init&gt;(TransportShardSingleOperationAction.java:121)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.&lt;init&gt;(TransportShardSingleOperationAction.java:97)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:74)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:49)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)
        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)
        at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)
        at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)

```

context:
- version: 0.90.9
- 3 node cluster
- 2 replicas
- 10 shards per index
</description><key id="24981251">4589</key><summary>NullPointerException in IndexShardRoutingTable.getActiveAttribute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">karol-gwaj</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T19:11:36Z</created><updated>2014-01-03T13:03:56Z</updated><resolved>2014-01-03T13:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-03T13:00:29Z" id="31520957">thanks for reporting this! I will push a fix soonish
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file></files><comments><comment>Check if node is still present when collecting attribute shard routings</comment></comments></commit></commits></item><item><title>Elasticsearch shouldn't try to balance shards from nodes with newer version of lucene to nodes with older versions of lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4588</link><project id="" key="" /><description>During a rolling upgrade I'm constantly seeing IndexFormatTooNewException exceptions as Elasticsearch repeatedly tries to rebalance a shard from an upgraded machine to a non-upgraded machine.  It is causing a large load spike on the non upgraded machine and won't work anyway.  Can Elasticsearch just not try?
</description><key id="24976583">4588</key><summary>Elasticsearch shouldn't try to balance shards from nodes with newer version of lucene to nodes with older versions of lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T17:34:33Z</created><updated>2014-01-03T14:58:35Z</updated><resolved>2014-01-03T14:58:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-02T18:18:08Z" id="31472041">While I agree that we shouldn't do this and it's on my list of thing that I should have been doing months ago, I don't think it will fix the spikes. You should really disable allocations / relocations while you restart though. But lets fix this problem since it could potentially lead to other issues.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/NodeVersionAllocationDecider.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/NodeVersionAllocationDeciderTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchAllocationTestCase.java</file></files><comments><comment>Add NodeVersionAllocationDecider that prevent allocations that require forward compatibility.</comment></comments></commit></commits></item><item><title>Rename fuzziness/min_similarity to edit_distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4587</link><project id="" key="" /><description>A lot of different API's currently use different names for the
same logical parameter. Since lucene moved away from the notion
of a `similarity` and now uses an `edit distance` we should generalize
this and encapsulate the generation, parsing and creation of these
settings across all queries.

This commit adds a new `EditDistance` class that handles the renaming
and generalization in a backwards compatible manner.

Closes #4082
</description><key id="24972155">4587</key><summary>Rename fuzziness/min_similarity to edit_distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-02T16:10:48Z</created><updated>2014-07-16T21:50:08Z</updated><resolved>2014-01-09T14:24:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-08T22:43:37Z" id="31884313">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A new generic timeout handler and changes to existing search classes to use it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4586</link><project id="" key="" /><description>A more effective approach to time-limiting activities such as search requests. Special runtime exceptions can now short-cut the execution of long-running calls to Lucene classes and are caught and reported back, not as a fatal error but using the existing “timedOut” flag in results.

Phases like the FetchPhase can now exit early and so also have a timed-out status. The SearchPhaseController does its best to assemble whatever hits, aggregations and facets have been produced within the provided time limits rather than returning nothing and throwing an error.

ActivityTimeMonitor is the new central class for efficiently monitoring all forms of thread overrun in a JVM.
The SearchContext setup is modified to register the start and end of query tasks with ActivityTimeMonitor.
Store.java is modified to add timeout checks (via calls to ATM) in the low-level file access routines by using a delegating wrapper for Lucene's IndexInput and IndexInputSlicer.
ContextIndexSearcher is modified to catch and unwrap ActivityTimedOutExceptions that can now come out of the Lucene calls and report them as timeouts along with any partial results.
FetchPhase is similarly modified to deal with the possibility of timeout errors.
</description><key id="24966167">4586</key><summary>A new generic timeout handler and changes to existing search classes to use it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2014-01-02T14:27:41Z</created><updated>2015-05-26T09:59:50Z</updated><resolved>2015-05-26T09:59:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-01-07T16:24:44Z" id="31752275">So I just found some weird runaway highlighting process that I found a request that took 57 minutes.  So +1 for this.
</comment><comment author="s1monw" created="2014-01-07T19:20:51Z" id="31769450">@nik9000 I don't think we can prevent this unless we fix the highlighter ;/
</comment><comment author="nik9000" created="2014-01-07T20:40:20Z" id="31776977">Even though we've got a fix for the highlighter I still think it'd be worth checking the timeout during the highlight process in case other bugs like this come up.  No reason it has to be part of this pull request though.
</comment><comment author="clintongormley" created="2014-07-18T11:11:49Z" id="49419853">@markharwood what's the status on this one?
</comment><comment author="markharwood" created="2014-07-18T11:14:38Z" id="49420068">Need to gather confidence about impl performance via benchmarking - following that we should consider where to apply these timeout checks across the platform
</comment><comment author="avleen" created="2014-08-05T00:00:21Z" id="51133989">Big +1 for this!
</comment><comment author="clintongormley" created="2014-08-08T08:55:56Z" id="51578089">Blocked by #6914 
</comment><comment author="clintongormley" created="2014-08-08T08:57:21Z" id="51578208">@markharwood - ping @mikemccand about getting this change benchmarked 
</comment><comment author="markharwood" created="2014-08-08T10:34:00Z" id="51586222">I had a quick look at rebasing this but it was messier than I thought - Lucene change https://issues.apache.org/jira/browse/LUCENE-4371 breaks it for one. I'll get back to this.
</comment><comment author="markharwood" created="2014-08-13T16:25:25Z" id="52072844">Now rebased on master and with changes to unit tests that pass.

The concerns I have at this stage are:
1) The existing FieldData loading logic treats all RuntimeExceptions from the Lucene calls as fatal and traps/wraps exceptions to reports as a failure. In the new scenario of a timeout triggered in Lucene's file access this is arguably non-fatal and should be reported as a timeout rather than a failure as per other requests
2) We try hard to return partial results in the event of a timeout - however, the only information passed back to clients about timeout events is a single "timedout" boolean in the search results. This feels a little "trappy" and clients now need to explicitly check this flag before reading too much into the (partial) results and automatically setting off alarms or launching missiles due to reported results. Maybe we should by default throw an error unless the client sends an optional "showPartialResults" flag in the request? 
3) Need to benchmark
4) Where else do we weave-in timeout checks e.g. indexing process and document tokenization?
</comment><comment author="markharwood" created="2014-08-26T16:27:21Z" id="53448764">Below are the results from a JMeter test rig I am using that has a mix of mostly-good queries (simple terms, phrases, aggs etc) with occasional problem ones (many terms/scripts/wildcards etc).
It shows that this PR is effective in shutting down all the nasty queries I have come up with so far without damaging the response times for the well-behaved queries. 

![pr4586](https://cloud.githubusercontent.com/assets/170925/4047607/b06b5504-2d3b-11e4-8a3b-bf9d63cfc5c9.png)

The gist for the test rig used to perform these benchmarks is here: https://gist.github.com/markharwood/0747e741b6fed9bbb32b
</comment><comment author="rmuir" created="2014-08-26T17:05:42Z" id="53454247">Can the benchmark include docvalues access as well? Because the change wraps, but doesnt override randomAccessSlice, I'm afraid it would effectively undo many performance improvements from lucene 4.9
</comment><comment author="rmuir" created="2014-08-26T17:10:57Z" id="53455008">By wrapping all indexinputs we also lose a good deal of NIOFS optimizations (e.g. reducing bounds checks for readlong/readvint and so on).
</comment><comment author="markharwood" created="2014-08-27T16:47:42Z" id="53603483">@rmuir Thanks for taking a look. I wrapped the RandomAccessInputs and was re-running benchmarks with doc values. I got side-tracked though because the benchmarks revealed a stack overflow issue. It took a while to track down but the misbehaving query is this one with multiple scripts: https://gist.github.com/markharwood/0747e741b6fed9bbb32b#file-stack-trace
I'll submit an update on this PR tomorrow.
</comment><comment author="markharwood" created="2014-08-28T11:23:49Z" id="53707116">Updated benchmarks using an index with Doc Values for one of the fields used in aggregations.

![pr4586dv](https://cloud.githubusercontent.com/assets/170925/4074754/8bdefd56-2ea5-11e4-811d-76189064165f.png)
</comment><comment author="clintongormley" created="2014-09-25T14:26:12Z" id="56826219">I talked to @rmuir about the query killer and his concerns are as follows (at least as I understand them):
- This wraps everything in Lucene, which incurs overhead whether it is going to be used or not
- The overhead may not be so apparent with searches (because of the way Lucene reads in bulk), but
- It will have a significant impact on things like doc values, because reads are not bulked. He has gone through the doc values code with a fine tooth comb to reduce the number of CPU instructions in order to make them as fast as they are, and adding a check (even if disabled) for every doc value read will be costly.

Who needs this feature?  Really, it's for the user who is running Elasticsearch as a service and who doesn't have control over the types of queries run against the cluster.  The query killer is intended as a safety net.  However, enabling this feature by default means that the user who **does** have control over their queries (or who doesn't want to use timeouts) will pay the cost of poorer performance regardless.

The query killer will be useful functionality for a subset of users, but should be disabled by default, and only enabled by those who need it.  Which begs the question of how to implement this.

It could be a config setting that is applied only at startup, or it could be a per-index setting (via an alternate directory implementation) that can be enabled only on specific indices.  Changing the setting would probably require closing and reopening the index.  The latter would be my preference.

Thoughts?
</comment><comment author="avleen" created="2014-09-25T14:43:02Z" id="56828824">I would be really happy with _any_ implementation. I like the per-index idea too. If I can just add the setting in the mapping template, that's an easy win.

As for who needs it: Yesterday one of my users ran a simply query against the `message` field of 1 day of logstash data, and got 71 results back. The field is analyzed with the standard analyser, and the index is about 4Tb, so quite a number of tokens in that field. It ran really fast!
Then they did a terms aggregation on the same field. After 3 hours of constant GC as all 24 Elasticsearch nodes tried to work on the query, I had to restart the cluster. This took 45 minutes.
After that, there was ~8 hours of lag as logs slowly filtered back in. Replicas didn't finish rebuilding until this morning, almost 24 hours later.
There are a number of us who don't have control over the queries run, but nor should we need to. We use Elasticsearch to enable others to work faster. Adding a layer in between to approve every possible query that could be run.. it's just not feasible.
</comment><comment author="clintongormley" created="2014-09-25T15:26:32Z" id="56835658">@avleen agreed - for those who need it (like yourself) this functionality would be very very helpful.  just as a side note: you know that you can disable the loading of field data on certain fields?   That would at least help you to prevent problems like the one you experienced: 
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/fielddata-formats.html#_disabling_field_data_loading
</comment><comment author="markharwood" created="2014-09-26T14:22:53Z" id="56967844">Coding something up with this new index setting (requires to be set on closed index):

```
index.thorough_timeout_checks : true
```

Default is false
</comment><comment author="avleen" created="2014-09-26T21:49:19Z" id="57025082">Thanks Clinton! I'm turning that on :-)
</comment><comment author="hazzadous" created="2015-02-04T16:00:03Z" id="72880734">Is there a target release for this?  Or is there any way I could help, this is one of my all time desired features.  Has anyone given this a whirl in production?
</comment><comment author="clintongormley" created="2015-05-26T09:59:49Z" id="105475154">Closing based on this comment: https://github.com/elastic/elasticsearch/pull/9168#issuecomment-105468391
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>rename _shard -&gt; _index and also rename classes and variables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4585</link><project id="" key="" /><description>closes #4584
</description><key id="24963693">4585</key><summary>rename _shard -&gt; _index and also rename classes and variables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-01-02T13:32:21Z</created><updated>2014-06-30T23:31:14Z</updated><resolved>2014-01-03T13:02:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-02T14:35:31Z" id="31455901">I suggest the following class names:
- ShardTermsLookup -&gt; IndexLookup
- ScriptTerms -&gt; IndexField
- ScriptTerm -&gt; IndexFieldTerm
- TermPosition -&gt; stay the same
</comment><comment author="brwe" created="2014-01-02T17:42:57Z" id="31469527">ok, renamed as suggested. want to take another look?
</comment><comment author="kimchy" created="2014-01-03T12:49:07Z" id="31520526">LGTM
</comment><comment author="s1monw" created="2014-01-03T12:49:23Z" id="31520535">+1 to push!
</comment><comment author="brwe" created="2014-01-03T13:02:51Z" id="31521058">pushed 9f54e97 (master) and e80c6e3 (0.90)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Revisit _shard / class names in exposing terms stats for scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4584</link><project id="" key="" /><description>The recent feature, #4161, include support for exposing lucene data / statistics for scripts. The naming though is misleading potentially, both on the class names and on how to access it in scripts.

The `_shard` key is used to access it in script. The class names are `ShardTermsLookup`, `ScriptTerms`, `ScriptTerm` for example. The `_shard` name feels too generic, while the `terms` names in the class names is misleading, since one can get data not only for terms (like doc count).

I think we can try to find a good name for it, and derive the script name and class names from it. Some thoughts include:
- `_ts` (script) / `TermsXXX` (class): though not all data exposed relates to terms, it encapsulates most of the stats one can get. The down side is, of course, that one can get more info than just terms.
- `_stats` (script) / `StatsXXX` (class): most of the data that is exposed relates to statistics information. Though again, not all, for example, payload... .
- `_index` (script) / `IndexXXX` (class): this is an "inverted index" level information, stats and other information that can be derived from the index itself.

I am personally leaning towards `_index`.
</description><key id="24960395">4584</key><summary>Revisit _shard / class names in exposing terms stats for scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T11:56:02Z</created><updated>2014-01-03T13:05:36Z</updated><resolved>2014-01-03T13:00:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-02T12:27:34Z" id="31449919">+1 for `_index` I wanted to suggest the same thing 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/script/AbstractSearchScript.java</file><file>src/main/java/org/elasticsearch/search/lookup/CachedPositionIterator.java</file><file>src/main/java/org/elasticsearch/search/lookup/IndexField.java</file><file>src/main/java/org/elasticsearch/search/lookup/IndexFieldTerm.java</file><file>src/main/java/org/elasticsearch/search/lookup/IndexLookup.java</file><file>src/main/java/org/elasticsearch/search/lookup/PositionIterator.java</file><file>src/main/java/org/elasticsearch/search/lookup/SearchLookup.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativeNaiveTFIDFScoreScript.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumNoRecordScoreScript.java</file><file>src/test/java/org/elasticsearch/benchmark/scripts/score/script/NativePayloadSumScoreScript.java</file><file>src/test/java/org/elasticsearch/script/IndexLookupTests.java</file></files><comments><comment>rename _shard -&gt; _index and also rename classes and variables</comment></comments></commit></commits></item><item><title>Fix toXContent in the All Mapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4583</link><project id="" key="" /><description>See #4579  #4581 #4582
</description><key id="24960122">4583</key><summary>Fix toXContent in the All Mapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-01-02T11:46:17Z</created><updated>2014-07-16T21:50:09Z</updated><resolved>2014-01-02T16:30:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-02T12:46:05Z" id="31450678">LGTM, I like the defaults serialization part, maybe (in another change) we can generalize it
</comment><comment author="jpountz" created="2014-01-02T13:06:24Z" id="31451537">+1 I like the serialization logic as well
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Term Vector settings should be treated like flags without propergation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4582</link><project id="" key="" /><description> today if a specific feature is disabled for term vectors with something
    like `"store_term_vector_positions" " false` term vectors might be disabled
    altogether even if `"tore_term_vectors" : true` in the mapping. This depends on the
    order of the values in the mapping since the more specific one might override
    the less specific on.
</description><key id="24959792">4582</key><summary>Term Vector settings should be treated like flags without propergation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T11:34:55Z</created><updated>2014-01-02T16:30:09Z</updated><resolved>2014-01-02T16:30:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file></files><comments><comment>Term Vector settings should be treated like flags without propergation</comment></comments></commit></commits></item><item><title>All field uses wrong setting for `term vectors`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4581</link><project id="" key="" /><description>In the all field mapper the settings that is used is `store_term_vector`  but it should be `store_term_vectors`.
</description><key id="24959595">4581</key><summary>All field uses wrong setting for `term vectors`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T11:27:56Z</created><updated>2014-01-02T16:30:09Z</updated><resolved>2014-01-02T16:30:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file></files><comments><comment>Simulate the entire toXContent instead of special caseing</comment></comments></commit></commits></item><item><title>Geo Clean Up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4580</link><project id="" key="" /><description>The default unit for measuring distances is _MILES_ in most cases. This commit moves ES
over to the _International System of Units_ and make it work on a default which relates
to _METERS_ . Also the current structures of the `GeoBoundingBox Filter` changed in
order to define the _Bounding_ by setting abitrary corners.
## Distances

Since the default unit for measuring distances has changed to a default unit
`DistanceUnit.DEFAULT` relating to _meters_, the **REST API** has changed at the
following places:
- `ScriptDocValues.factorDistance()` returns _meters_ instead of _miles_
- `ScriptDocValues.factorDistanceWithDefault()` returns _meters_ instead of _miles_
- `ScriptDocValues.arcDistance()` returns _meters_ instead of _miles_
      one might use `ScriptDocValues.arcDistanceInMiles()`
- `ScriptDocValues.arcDistanceWithDefault()` returns _meters_ instead of _miles_
- `ScriptDocValues.distance()` returns _meters_ instead of _miles_
      one might use `ScriptDocValues.distanceInMiles()`
- `ScriptDocValues.distanceWithDefault()` returns _meters_ instead of _miles_
      one might use `ScriptDocValues.distanceInMilesWithDefault()`
- `GeoDistanceFilter` default unit changes from _kilometers_ to _meters_
- `GeoDistanceRangeFilter` default unit changes from _miles_ to _meters_
- `GeoDistanceFacet` default unit changes from _miles_ to _meters_
## Geo Bounding Box Filter

The naming of the GeoBoundingBoxFilter properties allows to set arbitrary corners
(see #4084) namely `top_right`, `top_left`, `bottom_right` and `bottom_left`. This
change also includes the fields `topRight` and `bottomLeft` Also it is be possible to
set the single values by using just `top`, `bottom`, `left` and `right` parameters.

Closes #4515, #4084
</description><key id="24958211">4580</key><summary>Geo Clean Up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2014-01-02T10:39:36Z</created><updated>2014-06-15T04:04:42Z</updated><resolved>2014-01-11T12:58:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2014-01-08T14:17:26Z" id="31834416">@s1monw I worked in your suggestions. Maybe you can have another look.
</comment><comment author="chilling" created="2014-01-09T16:06:57Z" id="31946938">@s1monw I setup the changes we discussed. So please have another quick look.
</comment><comment author="s1monw" created="2014-01-09T16:48:46Z" id="31951787">@chilling I think it looks good though the only thing that I think we should change to use constants to parse the `top_left` etc fields instead of building all these dynamically? can you fix that?
</comment><comment author="s1monw" created="2014-01-10T14:22:46Z" id="32030690">I left more comments there are still inconsistencies with `north` vs. `top`
</comment><comment author="chilling" created="2014-01-10T15:10:49Z" id="32034740">@s1monw I hope I caught them now
</comment><comment author="s1monw" created="2014-01-10T19:21:46Z" id="32056731">LGTM +1 to push to master! Make sure you squash them before you push :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>All field might loose configuration on serialization.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4579</link><project id="" key="" /><description>The all field tries to optimize for default cases and doesn't generate XContent if everything is default. The settings are not tested well enough and there is already differences between master and 0.90. Master is already missing to check `autoboost` and `simiarity` and 0.90 has misses settings if `customFieldDataSettings` is set as the only setting as well.
</description><key id="24956241">4579</key><summary>All field might loose configuration on serialization.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2014-01-02T09:33:23Z</created><updated>2014-01-02T16:30:09Z</updated><resolved>2014-01-02T16:30:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file></files><comments><comment>Simulate the entire toXContent instead of special caseing</comment></comments></commit></commits></item><item><title>all_terms seems not supported in new terms aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4578</link><project id="" key="" /><description>In old faceting, I can set all_terms to true to let the facets results returns all terms even the terms were not match the filter. But in the new terms aggregation, I can't find the similar settings. Did I miss something?
Many thanks for help in advance.
</description><key id="24949985">4578</key><summary>all_terms seems not supported in new terms aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chunsheng</reporter><labels /><created>2014-01-02T02:47:36Z</created><updated>2014-01-09T13:08:57Z</updated><resolved>2014-01-09T13:08:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chunsheng" created="2014-01-02T03:53:00Z" id="31436867">In other word, I need those terms that aggregated as doc_count=0. 
</comment><comment author="chunsheng" created="2014-01-03T14:41:29Z" id="31525954">Can anyone help me?
</comment><comment author="uboness" created="2014-01-03T19:33:56Z" id="31547332">@chunsheng correct, we currently don't support it, though we are discussing about adding this support to aggs
</comment><comment author="chunsheng" created="2014-01-06T09:27:11Z" id="31636060">Thanks, then we have to wait. :)
</comment><comment author="jpountz" created="2014-01-09T13:08:57Z" id="31928963">Hi @chunsheng there is a new issue open to add the `all_terms` feature to terms aggregations: https://github.com/elasticsearch/elasticsearch/issues/4662
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>River does not start when using config/templates files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4577</link><project id="" key="" /><description>From elasticsearch 0.90.6, when you have templates files defined in `config/templates` dir, rivers don't start anymore.

Steps to reproduce:

Create `config/templates/default.json`:

``` javascript
{
  default:
  {
    "template" : "*",
    "mappings" : {
      "_default_" : {
      }
    }
  }
}
```

Start a dummy river:

``` sh
curl -XPUT 'localhost:9200/_river/my_river/_meta' -d '{ "type" : "dummy" }'
```

It gives:

```
[2014-01-01 22:08:38,151][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings [_default_]
[2014-01-01 22:08:38,239][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:38,245][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)
[2014-01-01 22:08:38,250][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:39,244][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:39,252][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:40,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:40,254][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:41,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:41,255][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms
[2014-01-01 22:08:42,249][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts
[2014-01-01 22:08:42,257][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts
```

With elasticsearch 0.90.2 or with no template file in `config/templates` dir, it gives:

```
[2014-01-01 22:22:32,096][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings []
[2014-01-01 22:22:32,221][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)
[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] create
[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] start
[2014-01-01 22:22:32,234][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)
```
</description><key id="24945928">4577</key><summary>River does not start when using config/templates files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>regression</label><label>v0.90.11</label><label>v1.0.0</label></labels><created>2014-01-01T21:23:03Z</created><updated>2014-01-21T06:24:39Z</updated><resolved>2014-01-20T17:04:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-01-03T14:59:26Z" id="31527111">This change: ae93ebbd056ff6a11f211446ea7eb5c4442d1e86 (see PR https://github.com/elasticsearch/elasticsearch/pull/4143) caused the regression. Gonna work on a fix.
</comment><comment author="dadoonet" created="2014-01-08T17:29:32Z" id="31857046">Actually, sounds like this issue was caused by #3782.
</comment><comment author="s1monw" created="2014-01-17T10:13:23Z" id="32593751">@dadoonet what's the status of this, can we get this in?
</comment><comment author="dadoonet" created="2014-01-17T10:37:11Z" id="32595362">Just talked about it with @javanna. He will review latest changes soonish.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/river/routing/RiversRouter.java</file><file>src/test/java/org/elasticsearch/river/RiverTests.java</file></files><comments><comment>River does not start when using config/templates files</comment></comments></commit></commits></item><item><title>CompressorFactory.uncompressIfNeeded is fragile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4576</link><project id="" key="" /><description>CompressorFactory.uncompressIfNeeded checks for a particular header to know whether data is compressed or not. This results in an unspecified behavior in the rare case that data is uncompressed and the first bytes match the header for compressed data.
</description><key id="24916522">4576</key><summary>CompressorFactory.uncompressIfNeeded is fragile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label></labels><created>2013-12-31T10:05:46Z</created><updated>2015-06-04T08:33:55Z</updated><resolved>2015-06-04T08:33:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-06-04T08:33:54Z" id="108785302">Fixed via https://github.com/elastic/elasticsearch/pull/11279. We now check that bytes are either some xcontent or compressed with a known format. Using it to compress arbitrary bytes is forbidden and cause the compression options to be removed on BinaryFieldMapper https://github.com/elastic/elasticsearch/pull/11280
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow analyzer to be specified for object and nested field mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4575</link><project id="" key="" /><description>Currently Elasticsearch allows analyzers to be set broadly at the type level or finely at the field level. However, there are scenarios where something in between would be very useful.

For example, where a nested object actually represents a dynamic string dictionary (i.e. a `Map&lt;String, String&gt;`) who's child field names aren't known up front. Assume also that the default analyzer for the type shouldn't apply to anything in this object field. When new keys are added to the dictionary, and new dynamic mappings appear, it would great to use the analyzer mapped to the parent field before falling back to the type default.

Imagine a put mapping request like the following:

&gt; {
&gt;     "mytype": {
&gt;         "analyzer": "standard",
&gt;         "properties": {
&gt;             "myobjectfield": {
&gt;                 "type": "object", 
&gt;                 **"analyzer": "some_other_analyzer",**
&gt;                 "dynamic": true
&gt;             }
&gt;         }
&gt;     }
&gt; }

Now when string fields are dynamically added to the "myobjectfield" object field, they will use "some_other_analyzer" rather than "standard".
</description><key id="24911647">4575</key><summary>Allow analyzer to be specified for object and nested field mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benquarmby</reporter><labels /><created>2013-12-31T04:56:41Z</created><updated>2013-12-31T08:13:55Z</updated><resolved>2013-12-31T08:13:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-31T08:09:06Z" id="31387220">Maybe [dynamic templates](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates) is what you are looking for? if so please close this issue.
</comment><comment author="benquarmby" created="2013-12-31T08:13:55Z" id="31387309">Yep, that looks like it'll get us out of trouble. Thanks for the signpost :+1:
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Azure backup repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4574</link><project id="" key="" /><description>Hey,

Would the ES team accept a PR for an Azure Blob Storage snapshot repository in reference to Backup API #3826 ?

If so; any guidelines as to how to develop them (affected classes etc).

I think I want to create an AzureRepository (inherits BlobStoreRepository) and related classes.
</description><key id="24910740">4574</key><summary>Azure backup repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Plasma</reporter><labels /><created>2013-12-31T03:47:29Z</created><updated>2013-12-31T04:18:42Z</updated><resolved>2013-12-31T04:18:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-31T04:18:42Z" id="31383343">Thanks. It's already part of azure cloud plugin which will be released soonish.
Wanna test a private Beta version? Ping me by email.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow 'omit_norms'  on the '_all' field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4573</link><project id="" key="" /><description>The '_all' field doesn't allow to omit norms. In certain scenarios
omitting the norm values makes a lot of sense to get senseable scoring.

Closes #3734
</description><key id="24902858">4573</key><summary>Allow 'omit_norms'  on the '_all' field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-12-30T22:06:12Z</created><updated>2014-06-16T12:13:25Z</updated><resolved>2014-01-02T21:03:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-12-31T17:20:57Z" id="31402886">Looks good to me!
</comment><comment author="s1monw" created="2014-01-02T21:03:30Z" id="31484380">merged
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Errors in _cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4572</link><project id="" key="" /><description>executing `curl -XGET localhost:9200/_cat/nodes` on a node (no indices) returns:

``` json
{
   "error": "StringIndexOutOfBoundsException[String index out of range: 4]",
   "status": 500
}
```

All the following APIs don't work with local node:

```
curl -XGET localhost:9200/_cat/master
curl -XGET localhost:9200/_cat/allocation
curl -XGET localhost:9200/_cat/shards
```

return

``` json
{
   "error": "ClassCastException[org.elasticsearch.common.transport.LocalTransportAddress cannot be cast to org.elasticsearch.common.transport.InetSocketTransportAddress]",
   "status": 500
}
```
</description><key id="24888172">4572</key><summary>Errors in _cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">uboness</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-30T15:43:30Z</created><updated>2014-01-09T16:58:29Z</updated><resolved>2014-01-09T16:58:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-01-02T21:58:30Z" id="31488413">Will fix the InetAddress problem, but I can't repro the array exception. Does it still happen for you?
</comment><comment author="imotov" created="2014-01-03T01:59:08Z" id="31500822">@drewr both issues are reproducible when you start elasticsearch with `-Des.node.local=true`
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestAllocationAction.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file></files><comments><comment>Handle node id shorter than the usual randomBase64UUID.</comment></comments></commit></commits></item><item><title>Expose filtered nodes on TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4571</link><project id="" key="" /><description>Expose the list of nodes that were filtered out with the `TransportClient`, for example, due to different cluster name. Relates to #4569
</description><key id="24885455">4571</key><summary>Expose filtered nodes on TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-30T14:27:32Z</created><updated>2013-12-30T14:27:59Z</updated><resolved>2013-12-30T14:27:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file></files><comments><comment>Expose filtered nodes on TransportClient</comment><comment>Expose the list of nodes that were filtered out with the TransportClient, for example, due to different cluster name. Relates to #4569</comment><comment>closes #4571</comment></comments></commit></commits></item><item><title>Add build hash to nodes info API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4570</link><project id="" key="" /><description /><key id="24882927">4570</key><summary>Add build hash to nodes info API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-30T12:56:41Z</created><updated>2013-12-30T13:12:37Z</updated><resolved>2013-12-30T13:12:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-30T13:12:37Z" id="31345109">Fixed e67cad31273f09d958cadd2d85fe0221dcb1df61.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Throw an exception when a Transport Client is not part of the connected cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4569</link><project id="" key="" /><description>Hi,

It would be really nice to let developers know when a Transport Client is not connected to the right cluster, instead of simply logging a WARN exception.

Developers should be able to handle that on their own, and decided whether it is a warning or an error.

Thanks.
Yann
</description><key id="24882539">4569</key><summary>Throw an exception when a Transport Client is not part of the connected cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YannBrrd</reporter><labels><label>:Java API</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-12-30T12:38:00Z</created><updated>2016-11-07T08:52:58Z</updated><resolved>2016-11-07T08:52:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-28T08:38:56Z" id="87191814">Hi @YannBrrd sorry it took ages to get back to you on this. We discussed this and we think it's a valid improvement. Throwing exceptions when there's a problem instead of only logging it seems like a good idea in general.

Given that the transport client supports connecting to multiple nodes though, I wonder when we should throw an exception. I guess we should do it when all of the nodes get filtered out after the cluster name check, cause anyways we would get a `NoNodeAvailableException` at the first request performed using the client. But what about when only some of the nodes get filtered out, but some (at least one) are valid ones that we can connect to? In that case warning seems ok to me. Let me know what you think.

Marking as adoptme as this is something we would like to get in.
</comment><comment author="YannBrrd" created="2015-03-28T09:01:26Z" id="87194325">Hi,

Fully agree with the approach. What is misleading as for now is having a
client running without any connection set (which may be because of client
conf or cluster unavailable). Having an exception thrown them is a good
idea to inform the client.

Then if some node are unavailable, I'd also send an exception informing how
many are down, letting client decide to go on our not...

How this would behave in case of brain split for example ?

Cheers,
Yann

Le sam. 28 mars 2015 09:39, Luca Cavanna notifications@github.com a
écrit :

&gt; Hi @YannBrrd https://github.com/YannBrrd sorry it took ages to get back
&gt; to you on this. We discussed this and we think it's a valid improvement.
&gt; Throwing exceptions when there's a problem instead of only logging it seems
&gt; like a good idea in general.
&gt; 
&gt; Given that the transport client supports connecting to multiple nodes
&gt; though, I wonder when we should throw an exception. I guess we should do it
&gt; when all of the nodes get filtered out after the cluster name check, cause
&gt; anyways we would get a NoNodeAvailableException at the first request
&gt; performed using the client. But what about when only some of the nodes get
&gt; filtered out, but some (at least one) are valid ones that we can connect
&gt; to? In that case warning seems ok to me. Let me know what you think.
&gt; 
&gt; Marking as adoptme as this is something we would like to get in.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/4569#issuecomment-87191814
&gt; .
</comment><comment author="javanna" created="2015-03-28T09:30:37Z" id="87197687">The transport client checks the cluster name to make sure that it's connecting to the right cluster, and that is why you see the warning on the log when some nodes get filtered out because the cluster name doesn't match. This doesn't have anything to do with split brain situations, where the cluster name is correct and stays the same, but the different splits have different masters. Makes sense?

I am not sure about throwing exception when we can connect to at least one node, but some belong to a different cluster. Seems like we can make things work and nothing bad happens as we will connect to a single cluster anyways. I am for only warning in that case.

By the way you could potentially double check in the client which nodes get filtered out, if any, and act accordingly (e.g. throw yourself an exception), have a look [here](https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/client/transport/TransportClient.java#L220).
</comment><comment author="YannBrrd" created="2015-03-28T09:40:11Z" id="87198040">Ok, makes sense.
Thanks.

Cordialement,
Yann Barraud

2015-03-28 10:31 GMT+01:00 Luca Cavanna notifications@github.com:

&gt; The transport client checks the cluster name to make sure that it's
&gt; connecting to the right cluster, and that is why you see the warning on the
&gt; log when some nodes get filtered out because the cluster name doesn't
&gt; match. This doesn't have anything to do with split brain situations, where
&gt; the cluster name is correct and stays the same, but the different splits
&gt; have different masters. Makes sense?
&gt; 
&gt; I am not sure about throwing exception when we can connect to at least one
&gt; node, but some belong to a different cluster. Seems like we can make things
&gt; work and nothing bad happens as we will connect to a single cluster
&gt; anyways. I am for only warning in that case.
&gt; 
&gt; By the way you could potentially double check in the client which nodes
&gt; get filtered out, if any, and act accordingly (e.g. throw yourself an
&gt; exception), have a look here
&gt; https://github.com/elastic/elasticsearch/blob/master/src/main/java/org/elasticsearch/client/transport/TransportClient.java#L220
&gt; .
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/4569#issuecomment-87197687
&gt; .
</comment><comment author="clintongormley" created="2016-11-06T11:05:55Z" id="258673899">@javanna is this still an issue?
</comment><comment author="javanna" created="2016-11-07T08:52:46Z" id="258778320">no @clintongormley I have re-read the discussion and I don't think there's anything left to be done here. If there are no nodes to connect to, an exception is thrown. If some nodes are filtered out they can be programmatically retrieved through the `filteredNodes` method besides the warning that gets logged.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java</file></files><comments><comment>Expose filtered nodes on TransportClient</comment><comment>Expose the list of nodes that were filtered out with the TransportClient, for example, due to different cluster name. Relates to #4569</comment><comment>closes #4571</comment></comments></commit></commits></item><item><title>Odd interaction between refresh and parent/child queries </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4568</link><project id="" key="" /><description>I've been having some issues with the following sequence of queries:

```
curl -XDELETE 'http://localhost:9201/test__garments'
curl -XPOST 'http://localhost:9201/test__garments/garment/1' -d '{"id":1, "name":"Some Garment"}'
curl -XPOST 'http://localhost:9201/test__garments/_refresh'
curl -XPUT 'http://localhost:9201/test__garments/verdict/_mapping' -d '{"verdict":{"_parent":{"type":"garment"},"properties":{"id":{"type":"integer"}}}}'
curl -XPOST 'http://localhost:9201/test__garments/verdict/1?parent=1' -d '{"id":1}'

curl -XPOST 'http://localhost:9201/test__garments/_refresh'
curl -XPOST 'http://localhost:9201/test__garments/verdict/_search' -d '
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
          "has_parent": {
            "type": "garment",
            "query": {
              "match_all": {}
            }
          }
      }
    }
  }
}
  '
```

It should produce 1 result (I'm indexing a document (garment), adding a child document (verdict) and then querying for child documents that have a parent), but it produces no results (and continues to do so no matter how long I wait). If I remove the first refresh it seems ok or if I do

```
curl -XPOST 'http://localhost:9200/_cache/clear?id_cache=true'
```

before the search then I will also get a result (as discussed at https://groups.google.com/forum/#!topic/elasticsearch/oD8EKEYeZuM). I noticed this behavior when moving from 0.20.6 to 0.90.9

Ideally I would not have to expire the cache by hand - or the circumstances in which one needs to do this should be documented
</description><key id="24882525">4568</key><summary>Odd interaction between refresh and parent/child queries </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">fcheung</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-30T12:37:27Z</created><updated>2014-01-06T09:48:54Z</updated><resolved>2014-01-06T09:48:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-30T21:09:14Z" id="31368878">@fcheung I can confirm this bug. The reason why this occurs b/c in `0.90.9` ES only loads the ids of parent documents, whereas in `0.20.6` the ids of all documents are loaded into memory. The way ES decides if a doc is a parent doc, is based on what has been defined in the mapping. At the time the first refresh occurs there is no `_parent` mapping defined and no ids are loaded into memory, but when the second refresh occurs there is a parent mapping. However the first document doesn't get included in the id cache, because the segment that document is in has already been processed. ES needs to invalidate that particular cache (on the segment level) if a new parent type has been added.
</comment><comment author="fcheung" created="2013-12-31T13:04:08Z" id="31394088">Thanks, that's very helpful information - enough for me to upgrade to 0.90 with confidence.
</comment><comment author="martijnvg" created="2014-01-03T15:40:30Z" id="31530042">@fcheung I pushed a fix for this in master and the fix will be part of the next 1.0-RC1 release. I didn't backport this change since the fix relies on some infrastructure in the mapping that isn't available in the 0.90 branch.
</comment><comment author="fcheung" created="2014-01-03T15:58:14Z" id="31531297">Awesome - thanks for the quick turnaround!
</comment><comment author="martijnvg" created="2014-01-06T09:48:54Z" id="31637067">Fixed via https://github.com/elasticsearch/elasticsearch/commit/38f038f899f2b321a4d691343514b0392411bbbc
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/cache/id/simple/SimpleIdCache.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentTypeListener.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/percolator/PercolatorQueriesRegistry.java</file><file>src/test/java/org/elasticsearch/index/cache/id/SimpleIdCacheTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Already loaded SimpleIdReaderCache should be reloaded when a new `_parent` has been introduced.</comment></comments></commit></commits></item><item><title>es1.0beta2 shard lost error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4567</link><project id="" key="" /><description>Our es depolys on Esx5.
Each Index is 20G  with 7 shards.
each vm has 12G Ram. 
vm.max_map_count = 262144
But we still one shard of the last index losts randomly.

Error as follows:

[2013-12-30 09:54:45,495][WARN ][index.engine.robin       ] [ilog1] [logstash-2013-12-20][1] failed to flush after setting shard to inactive
org.elasticsearch.index.engine.FlushFailedEngineException: [logstash-2013-12-20][1] Flush failed
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:820)
    at org.elasticsearch.index.engine.robin.RobinEngine.updateIndexingBufferSize(RobinEngine.java:226)
    at org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:201)
    at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:426)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)
    at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283)
    at org.apache.lucene.store.MMapDirectory$MMapIndexInput.&lt;init&gt;(MMapDirectory.java:228)
    at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:458)
    at org.apache.lucene.codecs.BlockTreeTermsReader.&lt;init&gt;(BlockTreeTermsReader.java:121)
    at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProducer(Lucene41PostingsFormat.java:437)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.&lt;init&gt;(BloomFilterPostingsFormat.java:131)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:102)
    at org.elasticsearch.index.codec.postingsformat.ElasticSearch090PostingsFormat.fieldsProducer(ElasticSearch090PostingsFormat.java:79)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.&lt;init&gt;(PerFieldPostingsFormat.java:195)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:244)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:115)
    at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:95)
    at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141)
    at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:235)
    at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:100)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:382)
    at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111)
    at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
    at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1465)
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:804)
    ... 10 more
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)
    ... 32 more
</description><key id="24873608">4567</key><summary>es1.0beta2 shard lost error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jingetema</reporter><labels /><created>2013-12-30T04:42:37Z</created><updated>2014-12-24T17:46:08Z</updated><resolved>2014-12-24T17:46:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:46:08Z" id="68066424">Hi @jingetema 

Sorry it has taken so long to look at this.  Given that this issue refers to a very old version, I think it is not worth investigating further.  The related code has changed a lot in the meantime.  Of course, if you experience similar problems in a more recent version please open another issue

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replaced multi-field type with multi-field syntax.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4566</link><project id="" key="" /><description>Replaced the multi-field type in favour for the multi fields option that can be set on any core field. The `path` option isn't changed in this PR.

Options defined in the main core field, are treated as defaults for any multi fields defined under this core field.
When upgrading to ES 1.0 the existing mappings with a multi-field type automatically get replaced to a core field with the new `fields` option.

Note: Any multi-field type defined in a dynamic template or index template will not be automatically replaced by a core field with multi fields. When dynamic templates or index templates are trigger and multi-field type is defined, then this will be converted in a core field with multi fields in the target mapping being generated.

Relates to #4521
</description><key id="24867235">4566</key><summary>Replaced multi-field type with multi-field syntax.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-29T21:25:10Z</created><updated>2015-05-18T23:33:07Z</updated><resolved>2014-01-13T08:47:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-12-29T21:58:20Z" id="31326415">What happens with something like this?  I would like to define a field mapping once then be able to "copy" a fields value into it without requiring additional mapping.  

```
{
     "tweet" : {
         "properties" : {
             "raw": {
                 "type": "string",
                 "index": "not_analyzed"
             },
             "name" : {
                 "type" : "string",
                 "index" : "analyzed",
                 "fields" : {
                     "raw" : {}
                 }
             }
         }
     }
 }
```

Or if someone is asking for trouble like this?  Infinite loop? lol.

```
{
     "tweet" : {
         "properties" : {
             "name2": {
                 "type": "string",
                 "fields": {
                     "name1": {}
                 }
             },
             "name1" : {
                 "type" : "string",
                 "fields" : {
                     "name2" : {}
                 }
             }
         }
     }
 }
```
</comment><comment author="martijnvg" created="2013-12-30T00:09:53Z" id="31329033">In both examples there is no unexpected behaviour, because the `path` by default is `full` and as a result of that there are no overlapping fields. The `path` option can be specified on the same level as where `fields` is specified.

Consider the following doc:

``` json
{
   "name" : "value1",
   "raw" : "value2"
}
```

With the first mapping this will result in the following field value pairs:
- name : value1
- name.raw: value1
- raw: value2

And the following doc:

``` json
{
   "name1" : "value1",
   "name2" : "value2",
}
```

With the second mapping will result in the following field value pairs:
- name1 : value1
- name1.name2 : value1
- name2 : value2
- name2.name1 : value2

However if the `path` is specified to be `just_name`, then the field value pair output is different.

For the first mapping and the first doc:
- name : value1
- raw : value1, value2

For the second mapping and the second doc:
- name1 : value1, value2
- name2 : value1, value2

In order for this behaviour to be working correctly the analyzers of fields that overlap need to be consistent, otherwise unexpected behaviour occurs.

Btw the `path` option will be replaced with `index_path`, which allows you to specify a full path for a field. It is a more direct way of specifying the field's name that will be used in the index compared to the current `path` option.
</comment><comment author="martijnvg" created="2014-01-10T10:08:21Z" id="32015666">Updated PR:
- Rebased with master.
- Removed the properties inheriting for core fields in `fields` from main field.
- Made automatic `multi_field` field type upgrading more lenient. Instead of failing when the default field in a `multi_field` can't be found, it tries to get the first field from the multi field and use that as main field. When that happens `index` is set to `no` for the main field. If the multi_field doesn't have any `fields` then it becomes a string field, also `index` is then set to `no`.
</comment><comment author="martijnvg" created="2014-01-13T08:47:32Z" id="32152391">Pushed to master: https://github.com/elasticsearch/elasticsearch/commit/943b62634c1ca798a0a8b47918f2b23f707d8b06
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update CountRequest and ValidateRequest to allow querying of types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4565</link><project id="" key="" /><description>Can get all other fields but not types.
</description><key id="24852933">4565</key><summary>Update CountRequest and ValidateRequest to allow querying of types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">sksamuel</reporter><labels /><created>2013-12-28T23:16:11Z</created><updated>2014-06-14T21:06:47Z</updated><resolved>2014-01-03T13:30:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-28T23:56:53Z" id="31307776">The changes look good to me though - maybe you can add some more informative commit messages like 
`Change visibility of CountRequest#types()`. One thing I wonder is why do you need to access the types here you passed them in already right so you could potentially access them? I think it makes sense to make them public here though.
</comment><comment author="sksamuel" created="2013-12-28T23:59:23Z" id="31307809">You could say the same thing about all the other fields though, they are all public too. Should be consistent either way. The reason I want them is that I have a DSL that creates these requests, and I want to test that the scala DSL -&gt; Java Builder -&gt; correct.
</comment><comment author="s1monw" created="2014-01-03T13:18:20Z" id="31521741">I will merge this in soon. I will squash the two commits into one update the commit message. Thanks for raising this.
</comment><comment author="s1monw" created="2014-01-03T13:30:55Z" id="31522321">pushed
</comment><comment author="s1monw" created="2014-01-03T13:31:24Z" id="31522347">for the record, here is the commit https://github.com/elasticsearch/elasticsearch/commit/87947cb00688b74d6dfe09fcb97f5f8d5da0f632
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Closing all indices doesn't work when using wildcard only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4564</link><project id="" key="" /><description>Closing all indices doesnt' work when doing:

```
curl -XPOST localhost:9200/*/_close
```

although it works using `_all` or using a wildcard expression like `index*`.
</description><key id="24837898">4564</key><summary>Closing all indices doesn't work when using wildcard only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-28T00:08:52Z</created><updated>2014-01-08T17:48:27Z</updated><resolved>2014-01-08T14:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/path/PathTrie.java</file><file>src/test/java/org/elasticsearch/common/path/PathTrieTests.java</file></files><comments><comment>Fixed open/close index api when using wildcard only</comment></comments></commit></commits></item><item><title>GeoPointFieldMapper.doXContentBody doesn't honor `includeDefaults`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4563</link><project id="" key="" /><description>Relates to #3941
</description><key id="24834917">4563</key><summary>GeoPointFieldMapper.doXContentBody doesn't honor `includeDefaults`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-27T22:01:21Z</created><updated>2013-12-30T12:54:21Z</updated><resolved>2013-12-30T12:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file></files><comments><comment>Honor `includeDefaults` in GeoPointFieldMapper.</comment></comments></commit></commits></item><item><title>Don't index geo points by default, use doc values instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4562</link><project id="" key="" /><description>Today, geo points are indexed as a single token `&lt;latitude&gt;,&lt;longitude&gt;`. This index is later uninverted into field data which is in-turn used for geo-distance computations. In practice, we never do term-based search on geo points so we should rather use doc values instead of the inverted index: we wouldn't waste time inverting and later uninverting the data, and this would be more space efficient.

In order to keep the experience consistent with previous versions, by default field data is going to be loaded into memory from doc values. In order to leave data on disk, the `doc_values` field data format will need to be configured explicitely.
</description><key id="24834824">4562</key><summary>Don't index geo points by default, use doc values instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-27T21:58:02Z</created><updated>2016-01-09T09:15:33Z</updated><resolved>2015-10-14T12:11:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T12:11:17Z" id="148029944">No longer relevant with the new geo fields coming in 2.1
</comment><comment author="sumesh-agarwal" created="2016-01-08T14:08:14Z" id="170012447">What is the status of this in 2.0? If doc_values for a 'geo_point' type field is set to true, does it mean that it will be stored in disk? I ask because, my 'fielddata' size doesn't seem to go down even after setting doc_values to true (and re-indexing ofcourse). Could someone please help?
</comment><comment author="jpountz" created="2016-01-08T17:42:32Z" id="170070243">@sumesh-agarwal when you break down fielddata usage per-field,a are the fields using most memory storing geo points? Also note that geo point fields are getting completely revamped in the upcoming 2.2 release.
</comment><comment author="sumesh-agarwal" created="2016-01-09T05:00:42Z" id="170195458">@jpountz Yes. Of around 1.6 GBs of field data used per node, around 800 MBs is being used by a geo point field (on normal load). I don't understand this. If geo point fields get their field data stored on disk by default, shouldn't it not appear in the usage? I read somewhere that we see non-analyzed strings in the field data usage because of the global ordinals built for them, and I too see a few non-analyzed string fields in my cluster appearing under the usage (a few KBs only). But geo point appearing there seems quite unsettling. On heavy load, my nodes go out of memory and I can't seem to get around this problem. On analyzing a heap dump I found that Long objects are taking the maximum space there, so I figured doc_values could solve this problem. Could you please throw some light on this?
</comment><comment author="nknize" created="2016-01-09T06:52:45Z" id="170202408">@sumesh-agarwal I'm assuming you're on a 2.0 or 2.1? Are you using `lat_lon : true` in the `geo_point` mapping? And are you using any geohash aggregators?
</comment><comment author="sumesh-agarwal" created="2016-01-09T09:15:33Z" id="170214305">@nknize I am on 2.0, and no, I have not set lat_lon : true in the geo_point mapping. Yes, I am using geohash aggregators with precision value - 5 and size - 1000.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Explicit doc_values setting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4561</link><project id="" key="" /><description>Once doc values are enabled on a field, they can't be disabled.

Close #4560
</description><key id="24830733">4561</key><summary>Explicit doc_values setting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-27T19:24:40Z</created><updated>2014-06-18T03:43:10Z</updated><resolved>2013-12-30T10:22:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-29T00:02:30Z" id="31307855">just to get it right - this PR makes sure that you set DV with the mapping and if you don't do so (ie with dynamic mapping) you can't add it later. So you need to make the decision ahead of time and you can't go back, right?
</comment><comment author="jpountz" created="2013-12-29T15:00:59Z" id="31318418">This is correct: decision needs to be made ahead of time and doc values can't be removed from an existing field. Also there are two ways to set them, either explicitely (`doc_values: true`) or because the fielddata format requires them (`fielddata: { format: "doc_values" }`).
</comment><comment author="s1monw" created="2013-12-29T21:03:29Z" id="31325283">good +1 in that case!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Explicit doc values setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4560</link><project id="" key="" /><description>Right now doc values are enabled on a field if its fielddata format is `doc_values` at creation time of the field mapper.

So if you create a field with the `doc_values` fielddata format, then use the update mapping API to change the format to `paged_bytes` instead and restart the node, the field mapper won't know that it needs to index doc values and the `doc_values` format won't be usable anymore.

I'd like to have a `doc_values` setting on the same level as `index` and `store` in the mappings that would remain true even when the field data format becomes `paged_bytes` so that doc values keep being indexed and the fielddata format can be later set to `doc_values` again.
</description><key id="24829681">4560</key><summary>Explicit doc values setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-27T18:46:51Z</created><updated>2014-01-28T06:03:10Z</updated><resolved>2013-12-30T10:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-27T18:48:30Z" id="31274795">+1 I think this should be antiviral as well.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IdFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/IndexFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/RoutingFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TypeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/UidFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/VersionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/string/SimpleStringMappingTests.java</file></files><comments><comment>Explicit doc_values setting.</comment></comments></commit></commits></item><item><title>Page-based cache recycling.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4559</link><project id="" key="" /><description>Here is an attempt to refactor cache recycling so that it only caches large
arrays (pages) that can later be used to build more complex data-structures
such as hash tables.
- QueueRecycler now takes a limit like other non-trivial recyclers.
- New PageCacheRecycler (inspired of CacheRecycler) has the ability to cache
  byte[], int[], long[], double[] or Object[] arrays using a fixed amount of
  memory (either globally or per-thread depending on the Recycler impl, eg.
  queue is global while thread_local is per-thread).
- Paged arrays in o.e.common.util can now optionally take a PageCacheRecycler
  to reuse existing pages.
- All aggregators' data-structures now use PageCacheRecycler:
  - all arrays
  - LongHash can now take a PageCacheRecycler
  - there is a new BytesRefHash (inspired from Lucene but quite different,
    still; for instance it cheats on BytesRef comparisons by using Unsafe)
    that also takes a PageCacheRecycler

Close #4557
</description><key id="24826721">4559</key><summary>Page-based cache recycling.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-27T17:15:21Z</created><updated>2014-06-25T21:31:05Z</updated><resolved>2014-01-06T18:07:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-02T15:29:58Z" id="31459424">@kimchy @s1monw I pushed a new commit that addresses your concerns:
- uses componentSettings
- use of Releasable and Releasable.release(Releasable...)
- try/finally for release
- better configuration of the cache size, by default 10% of the heap size
</comment><comment author="jpountz" created="2014-01-03T11:17:21Z" id="31516970">New commit pushed:
- added double-release detection
- better separation of concerns: parseBytesSizeValueOrHeapRatio moved to MemorySizeValue
- added comments about the weighting per data type
</comment><comment author="s1monw" created="2014-01-03T20:21:09Z" id="31550577">added some smallish comments - looks really good though. Lets get this in soon!
</comment><comment author="jpountz" created="2014-01-06T13:59:56Z" id="31649876">Thanks for the comments, @s1monw. I rebased and did the changes you suggested:
- new `Releasable.release(boolean success, Releasable... releasables)` that forwards to release when success is true and releaseWhileHandlingException otherwise
- fixed style
- MockPageCacheRecycler gets the seed from TestCluster
- s/AssertionError/ElasticsearchIllegalStateException/
</comment><comment author="s1monw" created="2014-01-06T14:45:58Z" id="31653047">LGTM +1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose `simple_query_string` flags in `flags` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4558</link><project id="" key="" /><description>This exposes the built-in flags for XSimpleQueryParser as part of the query DSL.

Fixes #4490
</description><key id="24826141">4558</key><summary>Expose `simple_query_string` flags in `flags` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-12-27T16:55:54Z</created><updated>2014-06-27T05:08:29Z</updated><resolved>2014-01-03T23:17:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-30T17:42:15Z" id="31357676">I left comments on the commit - I should have commented on the files changed but I realised it too late! 
</comment><comment author="dakrone" created="2014-01-03T18:49:44Z" id="31544122">@s1monw I addressed your feedback and rebased since this was over 300 commits behind master, let me know what you think.
</comment><comment author="s1monw" created="2014-01-03T19:20:16Z" id="31546314">LGTM
</comment><comment author="dakrone" created="2014-01-03T23:17:26Z" id="31562173">Merged in https://github.com/elasticsearch/elasticsearch/commit/5463f7953f9d16e86a736302c6e10f6aa9ef645b and https://github.com/elasticsearch/elasticsearch/commit/18cdde624c91cba8bd171a510f5a6413d8472f55
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improving cache recycling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4557</link><project id="" key="" /><description>Java has a generational collector that relies on the fact that most objects die young. Sometimes, it may happen that there is pressure on the young gen because of the allocation of large objects, and some objects that were actually going to die young are promoted to the old generation in order to make space for new objects. This is bad because collections of the old generation are typically **much** more costly so this is something that we should try to avoid whenever possible.

So here comes the cache recycler: by reusing large objects, these objects are promoted to the old generation (because we keep strong refs on them), but on the other hand they help diminish the allocation rate of large objects in the young generation, and this makes short-lived objects more unlikely to be promoted to the old generation.

Although this is nice from a GC point-of-view, this can have bad implications on the application side. Typically today, these cached data-structures grow with time and at some point, they may become oversized for the data that they need to carry. Typically, if you store 10 entries in a hash table of capacity 1M, the cost of iterating over all entries is proportional to 1M, not 10. Moreover, over-sized data-structures also tend to not play nicely with CPU caches.

In order to improve it, an idea could be: instead of recycling whole data-structures, we could build paged data-structures and recycle pages individually. This is nice on several levels:
- it would retain the advantage of cache recycling while avoiding over-sized structures
- the recycling logic would be simpler since it would only care about recycling fixed-length arrays
- it is easy to compute the size of Java arrays so we could do byte accounting and make the cache size (in bytes) configurable, eg. "reuse at most 50MB of memory per thread".

My plan is to use aggregations to experiment with this idea: they already use paged arrays and hash tables that we could modify to reuse pages.
</description><key id="24822278">4557</key><summary>Improving cache recycling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-27T15:01:29Z</created><updated>2014-01-06T18:08:05Z</updated><resolved>2014-01-06T18:07:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-12-27T15:11:57Z" id="31264804">This sounds like a great idea but strikes me as the most un-java thing I've seen in a while.  Not that it is a bad thing.  It just needs documentation careful documentation so new hackers know what they are dealing with.  It'd be really obvious if this does something surprising like not implementing the Map interface.

In another sense it is a very java thing because it is designed to work well with the generational GC....
</comment><comment author="jpountz" created="2013-12-27T17:38:01Z" id="31272015">@nik9000 I agree this may look scary and un-java-ish. Nevertheless Elasticsearch has had this mechanism in place for a very long time (the CacheRecycler cache) and it proved to help in production to avoid pollution of the old gen by short-living objects in case of memory pressure. CacheRecycler is also quite safe in the sense that not releasing entries doesn't create memory leaks, the only drawback of not releasing objects is pollution of the old gen (which is still an issue and should be avoided as much as possible, but not as bad as getting OOME because of a memory leak). The point of this issue is to try to retain the benefits while fixing the drawbacks by having more control on the sizes of the data-structures and the maximum amount of memory of the cache. And indeed, these data-structures are neither going to implement the Map or List interface nor to be exposed in public APIs (such as the ones you need to write plugins against), they are solely for internal use.
</comment><comment author="nik9000" created="2013-12-27T18:36:08Z" id="31274336">un-java-ish doesn't mean bad, just more important to document.  I have no objections and I honestly think its pretty cool.
</comment><comment author="s1monw" created="2013-12-27T18:45:48Z" id="31274699">Yeah I agree we play some tricks here as well as in lucene land that seem to be `back to manage your own memory` but we know a little better at which places we need / can reuse memory and supporting the JVM to allocate same sized objects / memory might even improve things more. We often deal with very tight memory situations where consecutive memory works much better than Object based datastructures. Nevertheless I agree it's important to be documented well while I still think most of the documenation should be in the code here or do you refer to our Guide on the website?
</comment><comment author="nik9000" created="2013-12-27T19:05:59Z" id="31275504">Certainly in code!  Unless you're exposing knobs like the recycler parameters I don't think it is too important to get into it.  Also your audience is less java centric in the guide so they are less likely to think in terms of JVM collection classes.

You're doing the right thing.  I suppose my comment was more "please try and make the surprising places super obvious so that casuals like me can still figure out what is going on if we have to poke around there."
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/TransportValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/action/count/TransportCountAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportShardDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/cache/recycler/CacheRecycler.java</file><file>src/main/java/org/elasticsearch/cache/recycler/DefaultPageCacheRecyclerModule.java</file><file>src/main/java/org/elasticsearch/cache/recycler/PageCacheRecycler.java</file><file>src/main/java/org/elasticsearch/cache/recycler/PageCacheRecyclerModule.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/common/lease/Releasables.java</file><file>src/main/java/org/elasticsearch/common/recycler/NoneRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/QueueRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/Recycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/SoftThreadLocalRecycler.java</file><file>src/main/java/org/elasticsearch/common/recycler/ThreadLocalRecycler.java</file><file>src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java</file><file>src/main/java/org/elasticsearch/common/settings/Settings.java</file><file>src/main/java/org/elasticsearch/common/unit/MemorySizeValue.java</file><file>src/main/java/org/elasticsearch/common/util/AbstractArray.java</file><file>src/main/java/org/elasticsearch/common/util/AbstractBigArray.java</file><file>src/main/java/org/elasticsearch/common/util/BigArray.java</file><file>src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>src/main/java/org/elasticsearch/common/util/BigByteArray.java</file><file>src/main/java/org/elasticsearch/common/util/BigDoubleArray.java</file><file>src/main/java/org/elasticsearch/common/util/BigDoubleArrayList.java</file><file>src/main/java/org/elasticsearch/common/util/BigFloatArrayList.java</file><file>src/main/java/org/elasticsearch/common/util/BigIntArray.java</file><file>src/main/java/org/elasticsearch/common/util/BigLongArray.java</file><file>src/main/java/org/elasticsearch/common/util/BigObjectArray.java</file><file>src/main/java/org/elasticsearch/common/util/ByteArray.java</file><file>src/main/java/org/elasticsearch/common/util/UnsafeUtils.java</file><file>src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java</file><file>src/main/java/org/elasticsearch/indices/fielddata/cache/IndicesFieldDataCache.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/SearchService.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregationPhase.java</file><file>src/main/java/org/elasticsearch/search/aggregations/Aggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorFactories.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/AbstractHash.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BucketsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/BytesRefHash.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/LongHash.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/DoubleTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/LongTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/avg/AvgAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/max/MaxAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/min/MinAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsAggegator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/sum/SumAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/test/java/org/elasticsearch/cache/recycler/MockPageCacheRecycler.java</file><file>src/test/java/org/elasticsearch/common/recycler/AbstractRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/recycler/QueueRecyclerTests.java</file><file>src/test/java/org/elasticsearch/common/util/BigArraysTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/BytesRefHashTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/LongHashTests.java</file><file>src/test/java/org/elasticsearch/test/TestCluster.java</file><file>src/test/java/org/elasticsearch/test/cache/recycler/MockPageCacheRecyclerModule.java</file></files><comments><comment>Page-based cache recycling.</comment></comments></commit></commits></item><item><title>Use `recycler` prefix for CacheRecycler settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4556</link><project id="" key="" /><description>Currently the cache recycler uses top level settings for it's
configuration. We should prefix them with a `recycler` to make sure
we don't have any confilicts.
</description><key id="24817062">4556</key><summary>Use `recycler` prefix for CacheRecycler settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2013-12-27T11:33:11Z</created><updated>2014-10-20T12:59:28Z</updated><resolved>2014-10-20T12:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-27T11:34:12Z" id="31257321">+1 Since it is a node level service.
</comment><comment author="s1monw" created="2013-12-27T11:38:18Z" id="31257442">@kimchy I also wonder if we should document these settings at some point. Currently I don't see them documented so it's not really a break or anything.
</comment><comment author="jpountz" created="2013-12-27T12:07:32Z" id="31258150">+1 to add a namespace

+1 to documentation as well, but since this is very internal, I think documentation should state that we don't guarantee backward compatibility for them (similarly to Lucene's `@lucene.internal` javadoc tag)?
</comment><comment author="kimchy" created="2013-12-27T12:58:26Z" id="31259392">regarding the setting name, then the bug is not using `componentSettings` when getting the settings, in which case, it will automatically be prefixed by the package, i.e., it will be `cache.recycler.type`.
</comment><comment author="s1monw" created="2013-12-31T08:55:03Z" id="31388146">pushed a new commit using component settings. I will add docs soon as well.
</comment><comment author="s1monw" created="2013-12-31T17:17:23Z" id="31402751">I am not sure where to put the documentation for this to be honest @clintongormley any ideas?
</comment><comment author="kimchy" created="2014-03-27T23:55:39Z" id="38875573">+1, the code has changed quite a bit though....
</comment><comment author="clintongormley" created="2014-08-08T08:53:25Z" id="51577861">@s1monw ping?  perhaps we don't want to document this given that the plan is to remove the recycler?
</comment><comment author="clintongormley" created="2014-10-20T12:59:28Z" id="59745805">The cache recycler has been removed in master. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Eager norms loading options.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4555</link><project id="" key="" /><description>Norms can be eagerly loaded on a per-field basis by setting `norms.loading` to
`eager` instead of the default `lazy`, very similarly to fielddata:

```
"my_string_field" : {
  "type": "string",
  "norms": {
    "enabled": true
    "loading": "eager"
  }
}
```

In case this behavior should be applied to all fields, it is possible to change
the default value by setting `index.norms.loading` to `eager`.

There is no such option for terms as #4079 suggested because all terms
implementations in Lucene load the terms dictionary eagerly.

Close #4079
</description><key id="24816884">4555</key><summary>Eager norms loading options.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-27T11:25:13Z</created><updated>2014-06-21T03:10:02Z</updated><resolved>2014-01-06T09:44:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-01-03T10:22:50Z" id="31514774">I pushed a new commit that tries to address concerns:
- single-threaded loading of norms
- warmers now return handles to the warming tasks that are being executed instead of waiting for completion of all their tasks before returning (allowing eg. to load field data and norms in parallel if the warmer thread pool has a size that is greater than 1)
- new test (disabled for now) that checks memory usage depending on whether norms are loaded eagerly or lazily
</comment><comment author="kimchy" created="2014-01-03T16:40:07Z" id="31534526">I vote to not increase visibility in ThreadPool from Executor to ExecutorService, and instead, implement the future logic in the warmers, other than that, +1 to push
</comment><comment author="s1monw" created="2014-01-03T20:29:44Z" id="31551117">I left one comment other than that +1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>0.90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4554</link><project id="" key="" /><description /><key id="24807120">4554</key><summary>0.90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pj1987111</reporter><labels /><created>2013-12-27T03:41:22Z</created><updated>2014-07-16T21:50:13Z</updated><resolved>2013-12-27T04:49:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-27T04:49:34Z" id="31247139">???
Closing...
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update frontends.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4553</link><project id="" key="" /><description>Fix typo
</description><key id="24805342">4553</key><summary>Update frontends.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dpen2000</reporter><labels /><created>2013-12-27T01:47:03Z</created><updated>2014-06-16T22:51:51Z</updated><resolved>2014-01-16T12:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-16T12:22:04Z" id="32464322">Closed by https://github.com/elasticsearch/elasticsearch/commit/bb1941212280203f18441b9906763724c93af7b8

Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Don't accept type wrapper in indexing request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4552</link><project id="" key="" /><description>Currently it is possible to index a document as:

```
POST /myindex/mytype/1
{ "foo"...}
```

or as:

```
POST /myindex/mytype/1
{
    "mytype": {
        "foo"...
    }
}
```

This makes indexing non-deterministic and fields can be misinterpreted
as type names.

This changes makes Elasticsearch accept only the first form, ie without
the type wrapper.

Fixes #4484
</description><key id="24795946">4552</key><summary>Don't accept type wrapper in indexing request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-26T18:30:29Z</created><updated>2014-06-16T16:42:38Z</updated><resolved>2014-01-13T21:46:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-07T16:19:23Z" id="31751707">in general this LGTM but I wonder how we tell people they are doing the wrong thing if they move from 0.90 to 1.0?
</comment><comment author="dakrone" created="2014-01-07T16:49:01Z" id="31754835">@s1monw unfortunately, I couldn't think of a way to tell people they're doing the wrong thing, since an object could be an actual object type instead of wrapping the document in a type (which is why this change is needed, since that is confusing).
</comment><comment author="s1monw" created="2014-01-07T19:19:01Z" id="31769289">yeah I agree!! @kimchy any idea - I don't have any!
</comment><comment author="kimchy" created="2014-01-07T19:37:45Z" id="31771162">I don't think its possible.., let me comment on the issue though, I have concerns about this change...
</comment><comment author="dakrone" created="2014-01-13T18:03:33Z" id="32194098">@kimchy I've updated this PR with the setting we discussed in #4484 as well as docs and an additional test.
</comment><comment author="imotov" created="2014-01-13T20:58:40Z" id="32210686">LGTM
</comment><comment author="s1monw" created="2014-01-13T21:31:00Z" id="32213573">LGTM as well. one thing I would want to have for changes like this in the future and I think we should add a new issue is some kind of interceptor that randomized the wrapping type for instance in `AssertingLocalTransport` we can   check if it is an indexing request and just wrap it with the type etc. Not sure how feasible it is but we might be able to do something that helps to test stuff like this in the future!
</comment><comment author="dakrone" created="2014-01-13T21:46:37Z" id="32215117">Merged in 329f27c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make RangeAggregator a MULTI_BUCKETS aggregator.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4551</link><project id="" key="" /><description>Until now, RangeAggregator was a PER_BUCKET aggregator, expecting to be always
collected with owningBUcketOrdinal == 0. However, since the number of buckets
it creates is known in advance, it can be changed to a MULTI_BUCKETS aggregator
by just multiplying the bucket ordinal by the number of ranges.

This makes aggregations that have ranges as sub aggregations of PER_BUCKET
aggregators more efficient.

Close #4550
</description><key id="24794909">4551</key><summary>Make RangeAggregator a MULTI_BUCKETS aggregator.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-26T17:50:22Z</created><updated>2014-07-16T21:50:14Z</updated><resolved>2013-12-27T11:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-12-26T18:16:24Z" id="31229946">added a small comment, but in general LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Better RangeAggregator when used as a sub-aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4550</link><project id="" key="" /><description>RangeAggregator is different from other PER_BUCKET aggregators (such as terms or histogram aggregators) in that it knows the number of buckets it will create in advance. So it could actually be a MULTI_BUCKETS aggregator by just multiplying the bucket ordinals by the number of ranges.

This will make RangeAggregator faster and more memory-efficient when used as a sub-aggregator of other PER_BUCKET aggregators.
</description><key id="24794765">4550</key><summary>Better RangeAggregator when used as a sub-aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-26T17:47:14Z</created><updated>2013-12-27T11:43:55Z</updated><resolved>2013-12-27T11:43:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/RangeTests.java</file></files><comments><comment>Make RangeAggregator a MULTI_BUCKETS aggregator.</comment></comments></commit></commits></item><item><title>Allow to disable destructive operations on all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4549</link><project id="" key="" /><description>Add `action.destructive_requires_name` in order to control whether wildcards and `_all` are allowed for destructive operations. 

The following APIs are affected by this option and are considered destructive: delete index, close index, open index, delete_by_query and delete mapping. 

The `action.destructive_requires_name` will default to `false`, meaning that using wildcards and `_all` in the mentioned APIs is allowed. 

In the delete api specifying an index will be made required, so `DELETE /` will not be allowed at all times regardless of `action.destructive_requires_name`. (Like is mentioned in #4481) 

Also the `action.disable_delete_all_indices` option which is applicable for the delete index api and the `action.disable_close_all_indices` which is applicable for the close api will be dropped in favour for `action.destructive_requires_name`
</description><key id="24791520">4549</key><summary>Allow to disable destructive operations on all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-26T15:47:36Z</created><updated>2014-07-20T16:13:47Z</updated><resolved>2014-01-09T10:50:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-26T15:49:59Z" id="31224973">+1
</comment><comment author="martijnvg" created="2013-12-27T10:37:15Z" id="31255757">+1 for delete_by_query, maybe also delete mapping? Being invoked from the Java api, doesn't require indices to be set at all.
</comment><comment author="kimchy" created="2013-12-27T13:00:08Z" id="31259427">Agreed on delete mapping!
</comment><comment author="martijnvg" created="2014-01-03T13:42:05Z" id="31522822">I updated the the issue description, to better describe what will be changed. Not sure about the naming, so if anyone has a better name for it then please share it :)
</comment><comment author="javanna" created="2014-01-03T13:48:20Z" id="31523127">I think we can also replace `action.disable_close_all_indices` with this new option?
</comment><comment author="kimchy" created="2014-01-03T13:52:53Z" id="31523336">Don't have a good name, here is the best I can come up with: `action.dirty_all_indices`...
</comment><comment author="martijnvg" created="2014-01-03T13:59:24Z" id="31523658">@javanna Good point, I've updated the issue description.

Maybe just `action.operate_all_indices`, since this setting just allows or disallows all indices being operated on.
</comment><comment author="javanna" created="2014-01-03T14:23:25Z" id="31524924">`allow` wouldn't have any effect on delete and close api, as the index is or will be (#4481) required with those apis. The `allow` option would still be useful for other apis like delete by query and delete mapping when specifying no indices though. Thoughts?
</comment><comment author="martijnvg" created="2014-01-03T17:25:02Z" id="31537904">@javanna This issue should also fix #4481, since the default will be `disallow`, so by default deleting or closing all indices shouldn't be possible. I suggest that this issue will encapsulate #4481.
</comment><comment author="martijnvg" created="2014-01-05T20:57:46Z" id="31615332">Perhaps the `action.operate_all_indices` should be overridable on a per request basis via a query string parameter instead of changing the default for `action.operate_all_indices` via cluster update settings api.
Something like this: `DELETE /_all?operate_all_indices=allow`
</comment><comment author="javanna" created="2014-01-06T07:54:01Z" id="31632413">Makes sense @martijnvg , I will close #4481 then. Let's also remember that currently `curl -XDELETE localhost:9200/` deletes all indices, while `curl -XPOST localhost:9200/_close` throws an error, as a close index requires an index. Looks like we want to allow closing all indices when specifying no indices, iff `action.operate_all_indices=allow`.
</comment><comment author="martijnvg" created="2014-01-06T08:39:25Z" id="31633998">@javanna Right, the close index api requires you to at least specify one index, alias or wildcard expression, this requirement should be removed.
</comment><comment author="martijnvg" created="2014-01-06T11:17:22Z" id="31641707">Changed to option name from `action.operate_all_indices` to `action.destructive_all_indices`. 
Like is mentioned in PR #4622, perhaps the default should be `allow_if_stated` instead of `disallow`?
For now lets not add `operate_all_indices` to related apis, this can if needed be done later.
</comment><comment author="javanna" created="2014-01-06T13:57:54Z" id="31649742">I created #4627 to allow empty index in open/close index api. We might want to consider adding the open index api to the list of potentially dangerous apis when executed on all indices. In fact, if one opens for instance many big indices by mistake, this can result in more resources suddenly used on the cluster and lead to issues.
</comment><comment author="clintongormley" created="2014-01-07T13:20:17Z" id="31736296">What reason is there for supporting `allow`?  It just allows people to shoot themselves in the foot.

I think that all of these operations should require a value for `{index}`, with the default being to accept `prefix*`, `*` or `_all`.

Then this option becomes a `true`/`false` choice:

```
action.destructive_requires_name: true | false
```

... which defaults to `false`
</comment><comment author="javanna" created="2014-01-07T15:42:21Z" id="31748080">Makes sense, it makes things simpler and easier to understand for users. We don't need any change to open/close index api anymore, thus I closed #4627 and the related PR.
</comment><comment author="martijnvg" created="2014-01-07T15:52:07Z" id="31749049">I'll update this issue description. The small difference is that with `action.destructive_requires_name` we disallow any wildcard expressions (`prefix*`, `*` or `_all`), which make this protection more predictable (you don't really know upfront if an expression is going to match all indices).

Also for the `action.destructive_requires_name` option the delete index api requires an index, like is described in #4481. 
</comment><comment author="martijnvg" created="2014-01-08T14:25:06Z" id="31835377">Updated the issue description with what has been discussed.
</comment><comment author="jeffsteinmetz" created="2014-07-20T16:13:47Z" id="49551175">can this also be configured in elasticsearch.yml?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/support/DestructiveOperations.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/test/java/org/elasticsearch/indices/state/CloseIndexDisableCloseAllTests.java</file><file>src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/RecoveryPercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Added `action.destructive_requires_name` that controls whether wildcard expressions and `_all` is allowed to be used for destructive operat Also the delete index api requires always an index to be specified (either concrete index, alias or wildcard expression)</comment></comments></commit></commits></item><item><title>Doc values for geo points.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4548</link><project id="" key="" /><description>This commits add doc values support to geo point using the exact same approach
as for numeric data: geo points for a given document are stored uncompressed
and sequentially in a single binary doc values field.

Close #4207
</description><key id="24789167">4548</key><summary>Doc values for geo points.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-26T14:11:31Z</created><updated>2014-06-30T16:11:47Z</updated><resolved>2013-12-27T11:52:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-27T11:18:26Z" id="31256878">LGTM +1 to push
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>es1.0beta2 bulk insert error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4547</link><project id="" key="" /><description>Currently we use elasticsearch1.0 beta2.In our case , we use flume 1.4.0 to insert logs.

We can mannualy create indexs , but errors occured when beginning inserts.

Anybody has encoutered this error?

Error logs as follow:

[2013-12-26 11:36:42,149][WARN ][index.codec              ] [ilog4] [logstash-2013-12-22] no index mapper found for field: [val] returning default postings format
[2013-12-26 12:18:04,796][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-23][3] failed to flush after setting shard to inactive
org.elasticsearch.index.engine.FlushFailedEngineException: [logstash-2013-12-23][3] Flush failed
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:820)
    at org.elasticsearch.index.engine.robin.RobinEngine.updateIndexingBufferSize(RobinEngine.java:226)
    at org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:201)
    at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:426)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)
    at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283)
    at org.apache.lucene.store.MMapDirectory$MMapIndexInput.&lt;init&gt;(MMapDirectory.java:228)
    at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:458)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.&lt;init&gt;(CompressingStoredFieldsReader.java:130)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:128)
    at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:95)
    at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141)
    at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:235)
    at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:100)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:382)
    at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111)
    at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
    at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1465)
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:804)
    ... 10 more
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)
    ... 27 more
[2013-12-26 12:19:36,436][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-20][3] failed to flush after setting shard to inactive
org.elasticsearch.index.engine.FlushFailedEngineException: [logstash-2013-12-20][3] Flush failed
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:820)
    at org.elasticsearch.index.engine.robin.RobinEngine.updateIndexingBufferSize(RobinEngine.java:226)
    at org.elasticsearch.indices.memory.IndexingMemoryController$ShardsIndicesStatusChecker.run(IndexingMemoryController.java:201)
    at org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:426)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)
    at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283)
    at org.apache.lucene.store.MMapDirectory$MMapIndexInput.&lt;init&gt;(MMapDirectory.java:228)
    at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:458)
    at org.apache.lucene.codecs.BlockTreeTermsReader.&lt;init&gt;(BlockTreeTermsReader.java:121)
    at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProducer(Lucene41PostingsFormat.java:437)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.&lt;init&gt;(BloomFilterPostingsFormat.java:131)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:102)
    at org.elasticsearch.index.codec.postingsformat.ElasticSearch090PostingsFormat.fieldsProducer(ElasticSearch090PostingsFormat.java:79)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.&lt;init&gt;(PerFieldPostingsFormat.java:195)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:244)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:115)
    at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:95)
    at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141)
    at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:235)
    at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:100)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:382)
    at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:111)
    at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
    at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1465)
    at org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:804)
    ... 10 more
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)
    ... 32 more
[2013-12-26 12:19:48,487][WARN ][index.merge.scheduler    ] [ilog4] [logstash-2013-12-25][3] failed to merge
java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)
    at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283)
    at org.apache.lucene.store.MMapDirectory$MMapIndexInput.&lt;init&gt;(MMapDirectory.java:228)
    at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:458)
    at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader.&lt;init&gt;(Lucene41PostingsReader.java:81)
    at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProducer(Lucene41PostingsFormat.java:430)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.&lt;init&gt;(BloomFilterPostingsFormat.java:131)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:102)
    at org.elasticsearch.index.codec.postingsformat.ElasticSearch090PostingsFormat.fieldsProducer(ElasticSearch090PostingsFormat.java:79)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.&lt;init&gt;(PerFieldPostingsFormat.java:195)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:244)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:115)
    at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:95)
    at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141)
    at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4184)
    at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3654)
    at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
    at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)
    ... 20 more
[2013-12-26 12:19:48,491][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] failed engine
org.apache.lucene.index.MergePolicy$MergeException: java.io.IOException: Map failed
    at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:109)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)
Caused by: java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:888)
    at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:283)
    at org.apache.lucene.store.MMapDirectory$MMapIndexInput.&lt;init&gt;(MMapDirectory.java:228)
    at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:195)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
    at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:458)
    at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader.&lt;init&gt;(Lucene41PostingsReader.java:81)
    at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProducer(Lucene41PostingsFormat.java:430)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.&lt;init&gt;(BloomFilterPostingsFormat.java:131)
    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:102)
    at org.elasticsearch.index.codec.postingsformat.ElasticSearch090PostingsFormat.fieldsProducer(ElasticSearch090PostingsFormat.java:79)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.&lt;init&gt;(PerFieldPostingsFormat.java:195)
    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:244)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:115)
    at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:95)
    at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:141)
    at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4184)
    at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3654)
    at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
    at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:885)
    ... 20 more
[2013-12-26 12:19:48,501][ERROR][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] failed to acquire searcher, source search_factory
org.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed
    at org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:97)
    at org.elasticsearch.index.engine.robin.RobinEngine.acquireSearcher(RobinEngine.java:691)
    at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearchFactory.newSearcher(RobinEngine.java:1558)
    at org.apache.lucene.search.SearcherManager.getSearcher(SearcherManager.java:155)
    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:122)
    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)
    at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:155)
    at org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:204)
    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:733)
    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:465)
    at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:922)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2013-12-26 12:19:48,528][WARN ][cluster.action.shard     ] [ilog4] [logstash-2013-12-25][3] sending failed shard for [logstash-2013-12-25][3], node[wZKZwd3dSWSxQdJU7S7exg], [P], s[STARTED], indexUUID [fdY2dmTPQCqbFlx9-4Q_Eg], reason [engine failure, message [MergeException[java.io.IOException: Map failed]; nested: IOException[Map failed]; nested: OutOfMemoryError[Map failed]; ]]
[2013-12-26 13:10:09,438][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] shard is locked, releasing lock
[2013-12-26 13:10:09,439][WARN ][indices.cluster          ] [ilog4] [logstash-2013-12-25][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2013-12-25][3] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-2013-12-25][3] failed to create engine
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:261)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:704)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock
    at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
    at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1338)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:259)
    ... 6 more
[2013-12-26 13:10:09,474][WARN ][cluster.action.shard     ] [ilog4] [logstash-2013-12-25][3] sending failed shard for [logstash-2013-12-25][3], node[wZKZwd3dSWSxQdJU7S7exg], [P], s[INITIALIZING], indexUUID [fdY2dmTPQCqbFlx9-4Q_Eg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2013-12-25][3] failed recovery]; nested: EngineCreationFailureException[[logstash-2013-12-25][3] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock]; ]]
[2013-12-26 13:10:09,828][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] shard is locked, releasing lock
[2013-12-26 13:10:09,828][WARN ][indices.cluster          ] [ilog4] [logstash-2013-12-25][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2013-12-25][3] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-2013-12-25][3] failed to create engine
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:261)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:704)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock
    at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
    at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1338)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:259)
    ... 6 more
[2013-12-26 13:10:09,865][WARN ][cluster.action.shard     ] [ilog4] [logstash-2013-12-25][3] sending failed shard for [logstash-2013-12-25][3], node[wZKZwd3dSWSxQdJU7S7exg], [P], s[INITIALIZING], indexUUID [fdY2dmTPQCqbFlx9-4Q_Eg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2013-12-25][3] failed recovery]; nested: EngineCreationFailureException[[logstash-2013-12-25][3] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock]; ]]
[2013-12-26 13:10:10,283][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] shard is locked, releasing lock
[2013-12-26 13:10:10,283][WARN ][indices.cluster          ] [ilog4] [logstash-2013-12-25][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2013-12-25][3] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-2013-12-25][3] failed to create engine
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:261)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:704)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock
    at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
    at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1338)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:259)
    ... 6 more
[2013-12-26 13:10:10,316][WARN ][cluster.action.shard     ] [ilog4] [logstash-2013-12-25][3] sending failed shard for [logstash-2013-12-25][3], node[wZKZwd3dSWSxQdJU7S7exg], [P], s[INITIALIZING], indexUUID [fdY2dmTPQCqbFlx9-4Q_Eg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2013-12-25][3] failed recovery]; nested: EngineCreationFailureException[[logstash-2013-12-25][3] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock]; ]]
[2013-12-26 13:10:10,704][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] shard is locked, releasing lock
[2013-12-26 13:10:10,705][WARN ][indices.cluster          ] [ilog4] [logstash-2013-12-25][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2013-12-25][3] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-2013-12-25][3] failed to create engine
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:261)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:704)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock
    at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
    at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1338)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:259)
    ... 6 more
[2013-12-26 13:10:10,741][WARN ][cluster.action.shard     ] [ilog4] [logstash-2013-12-25][3] sending failed shard for [logstash-2013-12-25][3], node[wZKZwd3dSWSxQdJU7S7exg], [P], s[INITIALIZING], indexUUID [fdY2dmTPQCqbFlx9-4Q_Eg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2013-12-25][3] failed recovery]; nested: EngineCreationFailureException[[logstash-2013-12-25][3] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock]; ]]
[2013-12-26 13:10:11,131][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] shard is locked, releasing lock
[2013-12-26 13:10:11,131][WARN ][indices.cluster          ] [ilog4] [logstash-2013-12-25][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2013-12-25][3] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-2013-12-25][3] failed to create engine
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:261)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:704)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock
    at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
    at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1338)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:259)
    ... 6 more
[2013-12-26 13:10:11,161][WARN ][cluster.action.shard     ] [ilog4] [logstash-2013-12-25][3] sending failed shard for [logstash-2013-12-25][3], node[wZKZwd3dSWSxQdJU7S7exg], [P], s[INITIALIZING], indexUUID [fdY2dmTPQCqbFlx9-4Q_Eg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2013-12-25][3] failed recovery]; nested: EngineCreationFailureException[[logstash-2013-12-25][3] failed to create engine]; nested: LockReleaseFailedException[Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock]; ]]
[2013-12-26 13:10:11,530][WARN ][index.engine.robin       ] [ilog4] [logstash-2013-12-25][3] shard is locked, releasing lock
[2013-12-26 13:10:11,530][WARN ][indices.cluster          ] [ilog4] [logstash-2013-12-25][3] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2013-12-25][3] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:248)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [logstash-2013-12-25][3] failed to create engine
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:261)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:704)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:189)
    ... 3 more
Caused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /home/ilog/software/elasticsearch-1.0.0.Beta2/data/elasticsearch/nodes/0/indices/logstash-2013-12-25/3/index/write.lock
    at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)
    at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1338)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:259)
    ... 6 more
</description><key id="24784722">4547</key><summary>es1.0beta2 bulk insert error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jingetema</reporter><labels /><created>2013-12-26T10:43:14Z</created><updated>2014-01-05T21:05:14Z</updated><resolved>2013-12-26T13:22:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-12-26T13:22:19Z" id="31220926">I think you just hit the maximum map count limit, which made mmap fail and caused an OutOfMemoryError. This limit can be rased: for example, if you are running a Linux server, `sysctl -w vm.max_map_count=262144` would raise the number of regions that can be mmaped to 262144, instead of 65530 (default).
</comment><comment author="jingetema" created="2013-12-27T02:29:00Z" id="31244251">Really appreciate for  your reply , I will try it!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elasticsearch query with script filter failed, uncomparable values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4546</link><project id="" key="" /><description>&gt; Query Failed [Failed to execute main query]]; nested: RuntimeException[java.lang.RuntimeException: uncomparable values &lt;&lt; \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0006P&gt;&gt; and &lt;&lt;10&gt;&gt;]; nested: RuntimeException[uncomparable values &lt;&lt; \u0001\u0000\u0000\u0000\u0000\u0000\u0000\u0003\u0006P&gt;&gt; and &lt;&lt;10&gt;&gt;]; nested: ClassCastException[java.lang.Integer cannot be cast to java.lang.String]; }]","status":500})

the very line cause the exception is

```
doc['insure_value'].value &gt; 10
```

and I believe insure_value field is numberic with default mapping

any suggestions?
</description><key id="24767325">4546</key><summary>elasticsearch query with script filter failed, uncomparable values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lazing</reporter><labels /><created>2013-12-25T11:49:22Z</created><updated>2013-12-27T17:20:40Z</updated><resolved>2013-12-27T17:20:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-12-27T17:20:40Z" id="31271280">Could you please send this type of questions to our [mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch). It will get the right attention there, here we try to keep track of bugs and features we are going to work on.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>put "maxBackupIndex: 7" in logging.xml but doesn't work and giving out WARN.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4545</link><project id="" key="" /><description>add one line as below:
  file:
    type: dailyRollingFile
    file: ${path.logs}/${cluster.name}.log
    datePattern: "'.'yyyy-MM-dd"
    maxBackupIndex: 7
    layout:
      type: pattern
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

and getting WARN like below:
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.DailyRollingFileAppender.

Just want to only keep 7 days log, how can I get what I expected?

thanks
</description><key id="24762286">4545</key><summary>put "maxBackupIndex: 7" in logging.xml but doesn't work and giving out WARN.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">liycheng</reporter><labels /><created>2013-12-25T06:09:00Z</created><updated>2013-12-27T17:22:58Z</updated><resolved>2013-12-27T17:22:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-12-27T17:22:58Z" id="31271379">Could you please send your question to our [mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch)? We use github issues only for bugs and features we are going to work on.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>NPE in cat/shards when UNASSIGNED</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4544</link><project id="" key="" /><description>This was introduced with the associative refactoring in #4433.
</description><key id="24752030">4544</key><summary>NPE in cat/shards when UNASSIGNED</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>bug</label><label>regression</label><label>v1.0.0.RC1</label></labels><created>2013-12-24T16:59:42Z</created><updated>2013-12-24T17:02:11Z</updated><resolved>2013-12-24T17:02:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/Table.java</file></files><comments><comment>Need to make sure we always end up with a Cell, even if it's null.</comment></comments></commit></commits></item><item><title>Cat: Add cache numbers to cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4543</link><project id="" key="" /><description>fielddata, filter, maybe bloom as well...
</description><key id="24750108">4543</key><summary>Cat: Add cache numbers to cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-24T15:50:37Z</created><updated>2014-01-03T16:26:51Z</updated><resolved>2014-01-03T16:26:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-12-24T15:51:28Z" id="31175872">parent id cache also would be great.
</comment><comment author="drewr" created="2013-12-24T15:52:55Z" id="31175928">Indeed! Knew I was forgetting one. :hamburger:
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/cat/RestNodesAction.java</file></files><comments><comment>Add cache stats to cat/nodes.</comment></comments></commit></commits></item><item><title>The `fields` option should always return an array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4542</link><project id="" key="" /><description>The `fields` options allows to extract field values from _source or load specific stored fields. The fields option is supported in various apis (get, search and explain).

The behaviour when it comes to array fields with a single value is inconsistent between apis, between source and stored fields. Based on the previous an array field is either serialised as a single value or an array containing a single value.

Doing the right thing here is difficult because the field option works on both _source and stored fields. The _source contains the meta information (json) to serialise field values correctly, but this information isn't available in stored fields. The plan is to make fields always return an array for both _source and stored fields and in all APIs with the goal to be consistent. Also the `fields` option can only serialise leaf fields, this to be further consistent between stored fields and _source. Metadata (`_id`, `_routing`, `_parent` etc.) fields are always single values, for this reason in the response the metadata fields are never wrapped in a json array.

If better serialisation is required for _source, the source filtering feature should be used instead: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-request-source-filtering.html
</description><key id="24745188">4542</key><summary>The `fields` option should always return an array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-24T12:30:33Z</created><updated>2014-08-28T18:04:30Z</updated><resolved>2014-01-03T16:30:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-02T13:14:04Z" id="31451844">What about `_version`, `_timestamp` etc, where we know they are single-valued fields?
</comment><comment author="kimchy" created="2014-01-02T13:14:52Z" id="31451874">good Q..., `_routing` also...
</comment><comment author="martijnvg" created="2014-01-02T13:16:49Z" id="31451958">I lean towards having the same behaviour for both json and metadata fields for consistency.
</comment><comment author="clintongormley" created="2014-01-02T13:48:25Z" id="31453288">Hmmm, not so sure... 

For stored fields (or extracted from source) we don't know if they are single or multi-valued, so there it feels right to default to arrays. But for metadata fields, returning these values as an array starts to feel like a lot of overhead. We know and the user knows that these are always single-valued fields, and it feels cleaner to treat them as such.
</comment><comment author="martijnvg" created="2014-01-02T13:50:09Z" id="31453356">I'm okay with this exception to the rule, I'll update the issue description.
</comment><comment author="kimchy" created="2014-01-02T13:54:37Z" id="31453587">++, agreed. I would love to eventually also have an "_source + all_metadata" option, since it is needed when reindexing for example (since the routing doesn't have to be part of the _source). This can be a different issue though.
</comment><comment author="clintongormley" created="2014-01-02T15:57:43Z" id="31461428">++ agreed re source and all metadata!
</comment><comment author="ehsanul" created="2014-02-26T04:33:07Z" id="36090039">This was a breaking change for 1.0, but was unfortunately not mentioned in the breaking changes section of the manual. Our application relied on getting back the fields as the same type they were stored as originally in the _source, but getting back an array with a single result broke that assumption.

Not sure about the best way to prevent this breakage, without trying to handle this in the application wherever we've used `fields` options. @Mpdreamz [mentioned](https://github.com/Mpdreamz/NEST/issues/481#issuecomment-35765858) using the [includes / excludes](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-source-field.html#include-exclude) option for fields mapping, but I'm unsure how that works.
</comment><comment author="ehsanul" created="2014-02-26T04:40:47Z" id="36090320">Oh, I think I see. The link was to the source mapping, but I'm guessing I just want to use [source filtering](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-source-filtering.html) here.
</comment><comment author="javanna" created="2014-02-26T08:28:14Z" id="36101887">Hi @ehsanul , the breaking change is mentioned [here](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_return_values.html). You should indeed switch to source filtering, which is meant to extract portions of the `_source` and is more flexible as well.
</comment><comment author="ehsanul" created="2014-02-26T23:34:56Z" id="36191817">Ah, I must have not read that closely enough, my apologies. Thanks for the confirmation, I will give source filtering a shot!
</comment><comment author="nemosupremo" created="2014-03-07T00:20:31Z" id="36954711">This is probably a done deal and forgive my ignorance on the subject / elasticsearch. I am using ES as my single datastore, and I have relatively large `_source` objects (a single field has up to 256kb, the rest are tiny). However when I want to query objects, I don't need that field so I `_source_exclude` it.

The problem is compared using `_source_exclude` is slow (I'd imagine because you have to parse the JSON object), and when making a query for 500+ objects at time, I see query times jump from 5-15ms on average to 800-1400ms. I also only have less than 10,000 objects (a tiny database).

The solution I came up with was to store the fields I need in the index, then return those with the fields parameter - and it works, the queries are back down 5-15ms range and only I have the data I need.

Now I don't know how common my use case is, but having to deal with the array is bothering and in my specific use case, in Go, it means having two struct definitions - 1 for the `fields` field that handles arrays, and another for everything else (getting/putting/etc.)

**Edit**: I'm not sure if you wish to tackle this use case, but with large `_source` fields, `_source_exclude` can cripple a query. Making a query with 100 results on my local macbook air (no network latency) gives me ~5ms with `_source=false`, ~200ms with `_source=true` (the response size is over 100M), ~160ms for `_source=true&amp;fields=subject,date,etc` and 30000ms for `_source_exclude=ast` - over 150x slower (and understandably so, from my understanding to exclude the fields, you need to parse and then reserialize the whole JSON object). 

The reason I am asking for greater flexibility over the `fields` property is it allows me to store all my data in ES and have quicker access to fields I need in the source without parsing the `_source`. Now I'm new to ES and I'm not sure my use case common, or if I am properly using ES but I thought it would be useful to present it.
</comment><comment author="ajhalani" created="2014-03-10T23:20:00Z" id="37246897">Upgrading to v1.0.x and just ran into this change. I am worried once we switch to source filtering and in future decide to store fields for performance reasons, it's going to be very difficult to switch back to "fields" because of the inconsistency of returning non-array fields as array. 

Maybe we could have a field level option like allow_array:true|false (defaults to true), which if set to false allows to index/returns only non-array value so users can switch from source filtering to fields w/o breaking their code. Just an idea from top of my head, I am sure there may be better ways. 
</comment><comment author="seti123" created="2014-03-31T07:23:18Z" id="39059863">+1 We have to change whole application and check where we query with fields and handle array instead of value. Please note it is a lot of work to check all places in ES Applications if a field query is used, and changing handling of results. This delays all my projects.

Is there any option to switch it OFF - so that single values are NOT returned as array? Is there an option in the node.js elastic search client to convert it? 
</comment><comment author="seti123" created="2014-03-31T07:44:29Z" id="39061129">Please note, I'm talking about the REST interface (I don't care about internal structure in ES, but if I store single field I expect to get the same back ...). Since a week we put code like (typeof doc.fields.xx == Array) //es 1.x then doc.fields.xx = doc.fields.xx[0] all around in our applications. 
</comment><comment author="kimchy" created="2014-03-31T07:47:19Z" id="39061295">@seti123 now in 1.0 the response is much more consistent, though for rarer cases, but if you had single value vs. multiple values, you would have had to check it as well, worse when it was doing the _source extraction compared to stored fields, in which case the format was broken for different type of structures between _source and stored fields. Now, it only handles stored fields, and its much more consistent in returning an array of "core" values always.
</comment><comment author="seti123" created="2014-03-31T07:56:35Z" id="39061869">Ok, I understand and we started already to do the changes. I think its more a business discussion, we have a product in evaluation by customers, for some other reason we had to update to 1.x (now to 1.1) - and all the effort delayed it. 
Back to the technical part: is there a way around, can we change mapping (e.g. declare all relevant fields used in Report and UI as "stored" and it will not be wrapped to "[]" ? ). Could somebody pls. advise on it. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[TEST] Fix yaml tests after #4542</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/get/GetField.java</file><file>src/main/java/org/elasticsearch/index/get/GetResult.java</file><file>src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/search/SearchHitField.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHit.java</file><file>src/main/java/org/elasticsearch/search/internal/InternalSearchHitField.java</file><file>src/test/java/org/elasticsearch/get/GetActionTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>The `fields` option should always return an array for json document fields and single valued field for metadata fields.</comment><comment>Also the `fields` option can only be used to fetch leaf fields, trying to do fetch object fields will return in a client error.</comment></comments></commit></commits></item><item><title>Added rest-spec directly to the core repo, removed rest-spec submodule</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4541</link><project id="" key="" /><description>Added rest-spec directly to the core repo, removed rest-spec submodule

Closes #4540
</description><key id="24743954">4541</key><summary>Added rest-spec directly to the core repo, removed rest-spec submodule</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-12-24T11:37:57Z</created><updated>2014-06-14T02:41:47Z</updated><resolved>2013-12-27T20:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-24T12:57:02Z" id="31170718">+1 LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Merge rest-spec-api into elasticsearch core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4540</link><project id="" key="" /><description>The REST tests added with #4469 rely on the external https://github.com/elasticsearch/elasticsearch-rest-api-spec repo which is currently pulled in as a git submodule.

When needing to make a non backwards compatible change to the REST layer, we need to update the rest-api-spec submodule as well. This process is not straightforward as we would like it to be.

Therefore we decided to integrate the rest-api-spec repository into the elasticsearch core itself.
</description><key id="24743647">4540</key><summary>Merge rest-spec-api into elasticsearch core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-24T11:24:48Z</created><updated>2013-12-27T20:15:20Z</updated><resolved>2013-12-27T20:15:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file></files><comments><comment>re-enabled FileUtilsTests and REST tests as rest-api-spec has been added back</comment></comments></commit><commit><files /><comments><comment>merged rest-api-spec repo into es core</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java</file><file>src/test/java/org/elasticsearch/test/rest/junit/RestTestSuiteRunner.java</file><file>src/test/java/org/elasticsearch/test/rest/support/FileUtils.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file></files><comments><comment>removed rest-spec submodule and prepared project for same files added directly to the codebase (no submodule) within rest-api-spec</comment></comments></commit></commits></item><item><title>Remove GET `_aliases` api in favour for GET `_alias` api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4539</link><project id="" key="" /><description>Currently there are two get aliases apis that both have the same functionality, but have a different response structure. The reason for having 2 apis is historic. 

The GET `_alias` api was added in 0.90.x and is more efficient since it only sends the needed alias data from the cluster state between the master node and the node that received the request.  In the GET `_aliases` api the complete cluster state is send to the node that received the request and then the right information is filtered out and send back to the client.

The GET `_aliases` api should be removed in favour for the `alias` api
</description><key id="24743588">4539</key><summary>Remove GET `_aliases` api in favour for GET `_alias` api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label></labels><created>2013-12-24T11:22:28Z</created><updated>2014-04-28T11:18:49Z</updated><resolved>2014-01-02T12:57:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-12-27T13:50:55Z" id="31260862">The `_aliases` API is the only one that can make multiple changes atomically. We have to have that functionality.
</comment><comment author="kimchy" created="2013-12-27T13:51:53Z" id="31260880">I believe @martijnvg means removing _GET_ on `/aliases`, in favor of the better GET on `/_alias`.
</comment><comment author="javanna" created="2013-12-27T17:18:46Z" id="31271200">yes I think he was referring to `RestIndicesAliasesAction` vs `RestGetAliasesAction`. They are both readonly operations. I am +1 on this, let's just make sure we keep the ability to retrieve all aliases for a specific index (not only get alias by name).
</comment><comment author="martijnvg" created="2013-12-27T23:53:14Z" id="31285647">@kimchy @clintongormley  Yes, I meant the GET aliases api, apologies for the confusion.

@javanna Good point, I'll make sure that the GET /alias api is able to return all aliases as well.
</comment><comment author="martijnvg" created="2014-01-02T12:57:47Z" id="31451141">Implemented via: https://github.com/elasticsearch/elasticsearch/commit/aa548f514807b4f0747e5f387fc7720a1d459c69
</comment><comment author="clintongormley" created="2014-01-15T11:04:12Z" id="32352088">This change was reverted as the `_aliases` endpoint provides functionality that is used by Kibana which is not available elsewhere, to wit, it resolves all index and alias names to their concrete index names, whether they have an alias or not.

Once #4614 is resolved, we can review this decision.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[TEST] add name parameter to get_alias in update_alias tests</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/RestGetIndicesAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/head/RestAliasesExistAction.java</file></files><comments><comment>Remove GET `_aliases` api in favour for GET `_alias` api</comment></comments></commit></commits></item><item><title>Parent is not getting set, using UpdateRequest in JavaAPI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4538</link><project id="" key="" /><description>I found a similar bug https://github.com/elasticsearch/elasticsearch/issues/3444, but it is not fixed in JavaAPI with requesting using UpdateReuest.
To resolve this, "UpdateRequest.java" should have property "parent" &amp; it should be used while generating upsertRequest. (I am using version 0.90.5)
Test with the following code : 

curl -XPOST 'http://localhost:9200/123'
curl -XPOST 'http://localhost:9200/123/TableWithParent/_mapping' -d '
{"TableWithParent" : {"_parent" : { "type" : "Table1" }}}'
## Inserting data using JavaAPI:

public static void main(String[] args) {
        Client client = NodeBuilder.nodeBuilder().node().client();
        Map&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;();
        data.put("column1", "colVal1");
        UpdateRequest updateRequest = new UpdateRequest();
        updateRequest.index("123");
        updateRequest.docAsUpsert(true);
        updateRequest.routing("123");
        updateRequest.parent("parentID");
        updateRequest.id("123|8|627110220645727|662022187154530");
        updateRequest.type("TableWithParent");
        updateRequest.doc(data);
        client.update(updateRequest);
    }

Below query, should return parent object:
curl -XGET 'http://localhost:9200/123/TableWithParent/123|8|627110220645727|662022187154530?routing=123&amp;pretty&amp;fields=_parent,_source'

Thanks
</description><key id="24724830">4538</key><summary>Parent is not getting set, using UpdateRequest in JavaAPI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">sandy6jain</reporter><labels><label>low hanging fruit</label></labels><created>2013-12-23T21:24:23Z</created><updated>2015-03-28T07:54:57Z</updated><resolved>2015-03-28T07:54:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-24T12:49:48Z" id="31170522">The UpdateRequest#parent() method should only be used to set routing if not set already via the routing setter, it doesn't set the _parent field (see the java doc).

You should set the `_parent` field in the index request that you can pass via UpdateRequest#doc(IndexRequest)
</comment><comment author="sandy6jain" created="2013-12-25T02:48:25Z" id="31190166">Thanks @martijnvg, that works :+1:  but, when docAsUpsert as true &amp; it can create new document than it should set _parent mapping field. It seems little confusing to me as parent() method in update action has different behaviour than other actions.
If you have any performance numbers of setting parent via UpdateRequest in a very high update(refresh) payload, can you please help me in that.
Thanks a lot for your response
</comment><comment author="martijnvg" created="2013-12-27T11:20:16Z" id="31256930">In the update request, the parent method's purpose if for routing only. I think it should be removed, to avoid confusion.
</comment><comment author="martijnvg" created="2015-01-13T13:18:20Z" id="69742514">I misunderstood what had to be done for this issue. The `parent` option should not have been removed instead the parent option shouldn't update the parent of an existing document. This was already the case as the update logic ignores the parent option (and routing option too).

The issue here is that although in normal use case the parent option is ignore and just a place holder for routing, in the case a document doesn't exist the parent option should be delegated to the upsert document.

I'll do the following changes:
- Revert the commit on master.
- Make sure that if parent has been set on update request that it delegates this to the parent option on upsert request or doc request if `doc_as_upsert` option has been set to true.
</comment><comment author="lukens" created="2015-02-27T11:26:51Z" id="76380797">Any timeline as to when this might get resolved?

After spending a morning trying to figure out why my `has_child` query wasn't working, I tracked it down to the Java API not setting the parent when inserting on an upsert.

This has left me a bit scuppered.

Any suggestions for a workaround would also be appreciated. 
</comment><comment author="martijnvg" created="2015-02-27T11:31:46Z" id="76381350">@lukens #9612 fixes the issue. It has been targeted to 2.0,  but I think it can be included in 1.x branch, which means it make it in 1.5 release.
</comment><comment author="lukens" created="2015-02-27T12:40:25Z" id="76388590">Thanks, I've copied in the changes to `UpdateRequest` and `UpdateHelper` from the pull request into the 1.4.4 versions of those classes, and have managed to get it working.

I believe I found an issue with the changes though, and have added a comment: https://github.com/elasticsearch/elasticsearch/pull/9612/files#r25503572
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateHelper.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file></files><comments><comment>Make sure that the parent option on the update request only is delgated to upsert index request.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/bulk/BulkRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/rest/action/update/RestUpdateAction.java</file><file>src/test/java/org/elasticsearch/document/BulkTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/ChildrenTests.java</file></files><comments><comment>Removed parent parameter from update request, because it is just sets the routing.</comment></comments></commit></commits></item><item><title>rivers cleanup task deleting valid rivers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4537</link><project id="" key="" /><description>after issuing two commands below i would expect the river to be created:

``` java
Map&lt;String, Object&gt; source = new HashMap&lt;String, Object&gt;();
source.put("type", "dummy");

_client.admin().indices().prepareDeleteMapping() 
            .setIndices("_river")
            .setType("my-river")
            .get();

_client.prepareIndex()
            .setIndex("_river")
            .setType("my-river")
            .setId("_meta")
            .setSource(source)
            .get();
```

instead it looks like if the commands execute fast enough one after another (happens every time on single node deployment),
there river is created and then deleted by riverClusterStateListener task

log dump for the case above:

```
[2013-12-23 17:23:43,794][INFO ][river.dummy              ] [test] [dummy][my-river] close
[2013-12-23 17:23:43,854][INFO ][river.dummy              ] [test] [dummy][my-river] create
[2013-12-23 17:23:43,854][INFO ][river.dummy              ] [test] [dummy][my-river] start
[2013-12-23 17:23:44,355][INFO ][river.dummy              ] [test] [dummy][my-river] close
```
</description><key id="24715964">4537</key><summary>rivers cleanup task deleting valid rivers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karol-gwaj</reporter><labels /><created>2013-12-23T17:31:51Z</created><updated>2015-04-07T17:01:20Z</updated><resolved>2015-04-07T17:01:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-22T11:39:53Z" id="35800659">I spent some time on this and I can confirm this might happen when creating and deleting the same river quickly. The reason is that the delete mapping does return after the type has been deleted from the cluster state (on all nodes), but that triggers additional _asynchronous_ events in case the operation happens under the `_river` index. 

If the actual river deletion hasn't happened yet when you send the second command (to register again the same river), which can happen given its asynchronous nature, it might happen that the river gets added and immediately deleted as well.

The way to solve it so that we don't require a `sleep` between the two operations is making this whole process completely synchronous so that when the delete mapping returns everything is done, which is not as easy as it might sound given all the little async steps this overall process is composed of.
</comment><comment author="javanna" created="2015-04-07T17:01:20Z" id="90646583">Rivers are deprecated, see #10345 . 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>OSGI issue: Classes org.apache.lucene of ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4536</link><project id="" key="" /><description>As ES packages its own org.apache.lucene classes, that does not work on OSGI platform as package name should be unique / bundle. 

Question : Is it possible to refactor org.apache.lucene\* classes of ES to use a different name which is not in conflict with package used by Lucene ?
</description><key id="24713833">4536</key><summary>OSGI issue: Classes org.apache.lucene of ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cmoulliard</reporter><labels /><created>2013-12-23T16:41:42Z</created><updated>2013-12-24T08:10:13Z</updated><resolved>2013-12-24T08:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-23T16:44:12Z" id="31129263">not really..., since we need it there to use package protected access. I have heard of people successfully working around it, though have no idea how.
</comment><comment author="cmoulliard" created="2013-12-23T16:48:31Z" id="31129502">Until now, we repackage elasticsearch + lucene as a bundle but the merging of SPI files of META-INF/Services does not work. So the the files merged does not contain classes name coming from jar of lucene with classes name coming from ES.
</comment><comment author="cmoulliard" created="2013-12-24T08:10:13Z" id="31162818">Been able to find a [workaround](https://issues.apache.org/jira/browse/SMX4-1637). I'm currently working on a small project to deploy ES as a node on Apache Karaf (OSGI Java Container) - https://github.com/cmoulliard/elasticsearch-karaf
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filter on the Term Facet Results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4535</link><project id="" key="" /><description>Facet filter allows to filter the documents that need to be included in the facet building. However , there can be use case where the facet results should be filterable. For e.g. if I do a terms facet and want to filter on the count. Display only the facets where the count exceeds a number or count falls in a specific range. 

"facets": 
{
   "locations":
   {
        "terms": 
        {
        "field": "location"
        },  
        "facet_filter":
        {  
            "terms": { "location": [ "US", "UK", "DE", "FR", "JP"  ]} 
        }
   }
}

I need only the locations where the count is greater than 5. For e.g. US has 7, UK has 5 , and rest has 3 each. So want to return US and UK only in the result. 
</description><key id="24707341">4535</key><summary>Filter on the Term Facet Results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vvbiju2005</reporter><labels /><created>2013-12-23T14:05:52Z</created><updated>2014-12-24T17:41:43Z</updated><resolved>2014-12-24T17:41:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:41:43Z" id="68066215">Hi @vvbiju2005 

Sorry it has taken a while to look at this.  This feature will be added as part of #8110 

thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Named filter and query don't work with parent/child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4534</link><project id="" key="" /><description>The named filter and query support (`_name`) don't work with with the `top_children`, `has_child` and `has_parent` queries and filters.

Originates from #4529
</description><key id="24705015">4534</key><summary>Named filter and query don't work with parent/child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-23T12:52:02Z</created><updated>2013-12-23T13:02:14Z</updated><resolved>2013-12-23T12:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/HasChildFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/HasParentQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/TopChildrenQueryParser.java</file><file>src/test/java/org/elasticsearch/search/child/SimpleChildQuerySearchTests.java</file></files><comments><comment>Fixed named filter and query support for the top_children, has_child and has_parent queries and filters.</comment></comments></commit></commits></item><item><title>Get UnsatisfiedResolutionException on Spring-data-elasticsearch with CDI enabled </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4533</link><project id="" key="" /><description>I got the below exception when trying to deploy my web application on glassfish4 or JBoss EAP 6.2.
SEVERE:   Exception while loading the app : CDI definition failure:Exception List with 1 exceptions:
Exception 0 :
javax.enterprise.inject.UnsatisfiedResolutionException: Unable to resolve a bean for 'org.springframework.data.elasticsearch.core.ElasticsearchOperations' with qualifiers [@javax.enterprise.inject.Any(), @javax.enterprise.inject.Default()].
    at org.springframework.data.elasticsearch.repository.cdi.ElasticsearchRepositoryExtension.createRepositoryBean(ElasticsearchRepositoryExtension.java:71)
    at org.springframework.data.elasticsearch.repository.cdi.ElasticsearchRepositoryExtension.afterBeanDiscovery(ElasticsearchRepositoryExtension.java:61)
</description><key id="24701721">4533</key><summary>Get UnsatisfiedResolutionException on Spring-data-elasticsearch with CDI enabled </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">nabilstawfik</reporter><labels /><created>2013-12-23T10:55:24Z</created><updated>2014-01-02T07:53:56Z</updated><resolved>2014-01-02T07:53:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-02T07:53:49Z" id="31440811">Hey,

this looks like a bug in the spring-data-elasticsearch project (note the `org.springframework` package name). Can you file a bug report over there at https://github.com/spring-projects/spring-data-elasticsearch?

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Update configuration.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4532</link><project id="" key="" /><description>I think this should be node.name and not network.host?
</description><key id="24685424">4532</key><summary>Update configuration.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">hura</reporter><labels /><created>2013-12-22T18:32:27Z</created><updated>2014-06-19T09:42:22Z</updated><resolved>2014-02-21T17:38:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-01-02T08:26:03Z" id="31441433">Hey,

thats definately a typo. Can you sign the CLA at http://www.elasticsearch.org/contributor-agreement/ - so we can get that change in? Thanks a lot!
</comment><comment author="spinscale" created="2014-02-24T10:33:46Z" id="35874040">thanks for the reminder, added now :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ClusterHealthResponse.validationFailures now maps to RoutingTableValidation.allFailures()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4531</link><project id="" key="" /><description>The ClusterHealthResponse.validationFailures is currently wired to RoutingTable.failures , which contains cluster level validation errors. That means that the list doesn't contain any index level validation failure, if exists. Since we by default return only the top level information (`level=cluster`), this important information is hidden to the rest layer. This PR adds the index validation failures to this list.  The Java API always return index level information but one needs to check it for every index, which means this change is imho good here as well.
</description><key id="24683540">4531</key><summary>ClusterHealthResponse.validationFailures now maps to RoutingTableValidation.allFailures()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Core</label></labels><created>2013-12-22T16:18:37Z</created><updated>2016-03-07T09:21:26Z</updated><resolved>2016-03-07T09:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-23T16:21:22Z" id="31127822">@bleskes thinking about it, I think we should remove all the validation part, its no longer really meaningful, I mean, we have all those tested internally already...

I would potentially add something like `RoutingNodes#assertShardStats` that is asserted on the routing table level that keeps those checks around?
</comment><comment author="clintongormley" created="2014-08-08T08:51:41Z" id="51577718">@bleskes is this still relevant?
</comment><comment author="bleskes" created="2014-08-15T12:54:33Z" id="52302324">@clintongormley yeah, I agree with Shay that it should move to some assertion in our testing infra structure and the validationFailure part of the ClusterHealthResponse can go away. It will be a minor breaking change to do so, so I guess it better wait for 2.0.
</comment><comment author="clintongormley" created="2014-11-11T18:43:56Z" id="62595339">@bleskes can this be merged yet?
</comment><comment author="dakrone" created="2016-03-03T18:58:22Z" id="191913229">@bleskes ping on this, can we close this as crusty and outdated?
</comment><comment author="bleskes" created="2016-03-07T09:21:26Z" id="193175907">@dakrone Thanks for the ping... I still think Shay's comment is valid - much time have passed and I have never seen these validation failures. I think we can just remove them from the api and move to assertions. I opened #16979 as an adopt me. Closing this one.... 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Reverting back to 0.90.7 config/templates loading behaviour</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4530</link><project id="" key="" /><description>Closes #4511
</description><key id="24623143">4530</key><summary>Reverting back to 0.90.7 config/templates loading behaviour</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-20T13:59:57Z</created><updated>2014-06-18T17:13:33Z</updated><resolved>2013-12-20T15:24:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-20T15:10:39Z" id="31015621">LGTM this reverts basically https://github.com/elasticsearch/elasticsearch/commit/4774439436d44bb6d6cceb5878eb7acfbe89c49c
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Named and-filter breaks has_parent-filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4529</link><project id="" key="" /><description>``` bash
#curl -XDELETE 'http://localhost:9200/test/'
curl -XPUT 'http://localhost:9200/test/'

curl -XPUT 'http://localhost:9200/test/ancestor/_mapping' -d '
{
    "ancestor" : {
        "properties" : {
            "id" : {"type" : "integer", "store" : "yes"},
            "name" : {"type" : "string", "store" : "yes"}
        }
    }
}
'

curl -XPUT 'http://localhost:9200/test/person/_mapping' -d '
{
    "person" : {
        "properties" : {
            "id" : {"type" : "integer", "store" : "yes"},
            "name" : {"type" : "string", "store" : "yes"}
        },
        "_parent" : {
            "type" : "ancestor"
        }
    }
}
'

curl -XPUT 'http://localhost:9200/test/ancestor/1' -d '
{
        "id" : 1,
        "name" : "mueller"
}
'
curl -XPUT 'http://localhost:9200/test/person/2?parent=1' -d '
{
        "id" : 2,
        "name" : "mueller"
}
'

curl -XPOST 'http://localhost:9200/test/person/_search' -d '
{
    "query": {
        "filtered": {
            "filter": {
                "and": {
                    "filters": [
                        {
                            "has_parent": {
                                "filter": {
                                    "term": {
                                        "id": 1
                                    }
                                },
                                "type": "ancestor"
                            }
                        }
                    ]
                }
            },
            "query": {
                "bool": {
                    "should": [
                        {
                            "match_all": {}
                        }
                    ]
                }
            }
        }
    }
}
'

#this one does not work
curl -XPOST 'http://localhost:9200/test/person/_search' -d '
{
    "query": {
        "filtered": {
            "filter": {
                "and": {
                        "_name" : "test",
                    "filters": [
                        {
                            "has_parent": {
                                "filter": {
                                    "term": {
                                        "id": 1
                                    }
                                },
                                "type": "ancestor"
                            }
                        }
                    ]
                }
            },
            "query": {
                "bool": {
                    "should": [
                        {
                            "match_all": {}
                        }
                    ]
                }
            }
        }
    }
}
'
```

results in 
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"index":"test","shard":2,"status":500,"reason":"ElasticSearchIllegalStateException[has_parent filter hasn't executed properly]"}]},"hits":{"total":1,"max_score":1.0,"hits":[]}}
</description><key id="24622306">4529</key><summary>Named and-filter breaks has_parent-filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">fivetide</reporter><labels><label>bug</label></labels><created>2013-12-20T13:41:23Z</created><updated>2014-01-02T12:16:18Z</updated><resolved>2014-01-02T09:50:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-23T09:18:07Z" id="31110030">Hey @fivetide What version of ES are you using?
</comment><comment author="martijnvg" created="2013-12-23T12:31:34Z" id="31116752">@fivetide Unfortunately the named queries / filters don't work for all parent child queries and filters. From version `0.90.6` ES doesn't throw an error, however the named filters don't get matched. 

Supporting named queries/filters for p/c queries will be more expensive than for regular queries, since the query/filter need to be re-evaluate all parent or child docs in a shard, whereas for regular queries or filters only need to be executed against the matching document. 
</comment><comment author="martijnvg" created="2013-12-23T12:55:33Z" id="31117667">@fivetide I've pushed a fix for the issue you have found via #4534. Thanks for reporting this bug!
</comment><comment author="fivetide" created="2013-12-30T18:15:59Z" id="31359404">Thanks! Keep up the awesome work guys! 
</comment><comment author="martijnvg" created="2014-01-02T09:50:03Z" id="31443966">Fixed via #4534 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cluster Health API returns wrong shard numbers if one of the indices is in RED status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4528</link><project id="" key="" /><description>If one of the indices in the cluster is in the RED status, the top level shard counts are not counted correctly. Note that the cluster status is correctly indicated as RED.

This is issue was introduced in: a221da0c2b0ab70468aa9d429791d3f7f275a78a
</description><key id="24618750">4528</key><summary>Cluster Health API returns wrong shard numbers if one of the indices is in RED status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.9</label><label>v1.0.0.RC1</label></labels><created>2013-12-20T12:11:55Z</created><updated>2013-12-21T11:28:07Z</updated><resolved>2013-12-20T12:20:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/IndexRoutingTable.java</file><file>src/test/java/org/elasticsearch/cluster/ClusterHealthResponsesTests.java</file></files><comments><comment>Added unit tests for ClusterIndexHealth and ClusterHealthResponse</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java</file></files><comments><comment>Counting shards was wrong if one of the indices was in the RED status</comment></comments></commit></commits></item><item><title>search for null or blank field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4527</link><project id="" key="" /><description>I had a date field in my index.....i need to write a query to search all the records that have some value in the date field..........How can i do that .......................................
I uses ExistsFilter but i shows no record..................
then i uses MissingFilter it shows records but that field doesn't present in the result
</description><key id="24608677">4527</key><summary>search for null or blank field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GJ2511</reporter><labels /><created>2013-12-20T07:32:41Z</created><updated>2013-12-20T10:53:40Z</updated><resolved>2013-12-20T07:36:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-20T07:36:05Z" id="30993525">Please ask your question in the mailing list. And gist a full curl recreation: http://www.elasticsearch.org/help/

Thanks!
</comment><comment author="GJ2511" created="2013-12-20T10:53:40Z" id="31002235">i m not using curl or linkux i m using elastic search on windows
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Failed search on a shard tries a local replica on a network thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4526</link><project id="" key="" /><description>When a search on a shard to a remove node fails, and then replica exists on the local node, then the execution of the search is done on the network thread. This is problematic since we need to execute it on the actual search thread pool, but can also explain #4519, where the get happens on the network thread and it waits to send the get request till the network thread we use is freed (deadlock...)
</description><key id="24586503">4526</key><summary>Failed search on a shard tries a local replica on a network thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.9</label><label>v1.0.0.RC1</label></labels><created>2013-12-19T20:44:16Z</created><updated>2014-05-01T23:51:28Z</updated><resolved>2013-12-19T21:19:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file></files><comments><comment>Failed search on a shard tries a local replica on a network thread</comment><comment>When a search on a shard to a remove node fails, and then replica exists on the local node, then the execution of the search is done on the network thread. This is problematic since we need to execute it on the actual search thread pool, but can also explain #4519, where the get happens on the network thread and it waits to send the get request till the network thread we use is freed (deadlock...)</comment><comment>fixes #4526</comment></comments></commit></commits></item><item><title>Allow to enable / disable bloom filter loading on an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4525</link><project id="" key="" /><description>Allow to have a new index level setting `index.codec.bloom.load` (default to `true`), that can control if the boom filters will be loaded or not. This is an updateable setting, that can be updated on a live index using the update settings API.

Note though, when this setting is updated, a fresh Lucene index will be reopened, causing associate caches to be dropped potentially.
</description><key id="24585607">4525</key><summary>Allow to enable / disable bloom filter loading on an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.90.9</label><label>v1.0.0.RC1</label></labels><created>2013-12-19T20:30:24Z</created><updated>2013-12-19T20:32:20Z</updated><resolved>2013-12-19T20:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Documented index.codec.bloom.load for #4525</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineIntegrationTest.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineTests.java</file><file>src/test/java/org/elasticsearch/index/merge/policy/MergePolicySettingsTest.java</file></files><comments><comment>Allow to enable / disable bloom filter loading on an index</comment><comment>Allow to have a new index level setting index.codec.bloom.load (default to true), that can control if the boom filters will be loaded or not. This is an updateable setting, that can be updated on a live index using the update settings API.</comment></comments></commit></commits></item><item><title>Close Directory / Store once all resources have been released</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4524</link><project id="" key="" /><description>Currently we close the store and therefor the underlying directory
when the engine / shard is closed ie. during relocation etc. We also
just close it while there are still searches going on and/or we are
recovering from it. The recoveries might fail which is ok but searches
etc. will be working like pending fetch phases.

The contract of the Directory doesn't prevent to read from a stream
that was already opened before the Directory was closed but from a
system boundary perspective and from lifecycles that we test it seems
to be the right thing to do to wait until all resources are released.

Additionally it will also help to make sure everything is closed
properly before directories are closed itself.

The code needs some work though but I wanted to get it out there for discussion
</description><key id="24583578">4524</key><summary>Close Directory / Store once all resources have been released</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-12-19T19:57:06Z</created><updated>2014-06-17T07:22:15Z</updated><resolved>2014-03-21T14:32:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-21T14:32:26Z" id="38280577">closed by https://github.com/elasticsearch/elasticsearch/commit/2398bb4f1cc535f8424772f514844e1d6b28963d
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Made single shards APIs fail if routing is configured to be required in the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4523</link><project id="" key="" /><description>This change make single shard requests fail when no routing is specified and routing has been configured to be required in the mapping. This change affects the following APIs: get, mget, explain, termvector, multi term vector and exists.

Relates to #4506
</description><key id="24573039">4523</key><summary>Made single shards APIs fail if routing is configured to be required in the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-19T18:01:24Z</created><updated>2015-05-18T23:33:22Z</updated><resolved>2014-01-02T09:48:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-02T09:48:34Z" id="31443913">pushed to master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add the ability to disable simple_query_string operators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4522</link><project id="" key="" /><description>One of the nice features of the simple query parser is that it is possible to only enable certain operators. It allows to do quite interesting things.

For example, if you are using the keyword analyzer and disable the whitespace operator, `the fox | jumped over` would be parsed into `the fox` OR `jumped over` (`the fox` would actually be treated as a single token).

One could also prevent users from running too expensive queries that could kill the cluster such as prefix queries.
</description><key id="24569319">4522</key><summary>Add the ability to disable simple_query_string operators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-19T17:01:46Z</created><updated>2013-12-19T17:17:02Z</updated><resolved>2013-12-19T17:17:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-12-19T17:14:26Z" id="30947144">This is a duplicate of https://github.com/elasticsearch/elasticsearch/issues/4490, but definitely a good idea! :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve multi_field syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4521</link><project id="" key="" /><description>## Proposed syntax for multi_field

Any field type (other than `object`, `nested`) should accept a `fields` parameter which defines any extra fields that should be indexed.  So for example a simple field like:

```
{
    "title": {
        "type": "string"
}}
```

can be converted into a multi-field by adding a `fields` parameter:

```
{
    "title": {
        "type": "string",
        "fields": {
            "raw": { "type": "string", "index": "not_analyzed" }
       }
}}
```
</description><key id="24568001">4521</key><summary>Improve multi_field syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-19T16:42:27Z</created><updated>2014-01-28T06:03:10Z</updated><resolved>2014-01-14T20:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-14T20:55:34Z" id="32306405">Moved the `path` part of the issue to to #4729 
</comment><comment author="martijnvg" created="2014-01-14T20:57:08Z" id="32306521">Pushed to master via: https://github.com/elasticsearch/elasticsearch/commit/943b62634c1ca798a0a8b47918f2b23f707d8b06
</comment><comment author="apatrida" created="2014-01-22T08:20:28Z" id="33001260">It isn't clear if the settings inheritance works, for example if "type" is left out of the sub-field, it produces an error:

{"error":"MapperParsingException[mapping [_default_]]; nested: MapperParsingException[No type specified for property [stopped_fuzzy_metaphone]]; ","status":400}

So does that work, or just not for type (therefore the sample is wrong).  What settings inherit, which do not?
</comment><comment author="martijnvg" created="2014-01-22T08:48:15Z" id="33002687">@jaysonminard The settings inheriting didn't get in. For some reason I forgot to update the issue description and documentation. I'll update this now.
</comment><comment author="apatrida" created="2014-01-22T09:03:43Z" id="33003629">@martijnvg 

Ok, but the default field being from the root still works ok?  For example:

```
{
    "title": {
        "type": "string",
        "fields": {
            "raw": { "type": "string", "index": "not_analyzed" }
       }
  }
}
```

produces title and title.raw?
</comment><comment author="martijnvg" created="2014-01-22T09:05:36Z" id="33003729">Yes, that produces a `title` and a `title.raw` field.
</comment><comment author="apatrida" created="2014-01-22T09:39:43Z" id="33006023">ok, with a field pushing content to one of these (a source field using copy_to into a dest multi-field), I am not getting the expected results, will try a small experiment to see more specifically what is going on.
</comment><comment author="martijnvg" created="2014-01-22T09:42:47Z" id="33006229">If your document looks like this:

``` json
{
    "title" : "my title"
}
```

and using the above mapping snippet should yield two fields a `title` and `title.raw` field.
</comment><comment author="apatrida" created="2014-01-22T09:48:05Z" id="33006581">either copy_to doesn't work, or it doesn't work in conjunction with this.  1 moment for example
</comment><comment author="apatrida" created="2014-01-22T09:57:42Z" id="33007260">ok, don't know where I got the idea that copy_to was in 1.0.0.RC1, maybe only on master (the docs are only for 0.90 branch or master, there are no 1.0.0.RC1 specific docs?).  That is the cause of my problem, using something allowed by mappings parser, but doesn't actually work.  I'll go back to full names and using path variable to make the copies.

```
"title": {
            "type": "string",
            "index": "no",
            "store": "no",
            "copy_to" : [ "search_title", "search_combined", "other_title" ],
            "fielddata": {
                "format": "disabled"
            }
        },

...
"search_title": {
            "type": "string",
            "index": "analyzed",
            "fields": {
                "raw": {
                    "type": "string",
                    "index": "not_analyzed"
                }
            }
        },
```
</comment><comment author="martijnvg" created="2014-01-22T10:00:27Z" id="33007442">Yes, `copy_to` isn't in 1.0.0.RC1 and there are no rc1 specific docs... there will be specific docs for 1.0 when 1.0.0.GA gets out. 
</comment><comment author="apatrida" created="2014-01-22T10:02:51Z" id="33007612">Ok, that makes it hard to test 1.0.0.RC1 with 1.0.0.RC1 features.  The release candidate isn't the main download link, has no docs, and seems like it could use more attention getting markers so that it is heavily tested before GA.

By the way, this link related to this actual issue (multi fields docs) on master is broken:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/multi-fields.html

(linked to from: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/mapping.html)

So there is not current multi-field docs that I can find.
</comment><comment author="martijnvg" created="2014-01-22T10:05:58Z" id="33007872">I just removed that specific link from the documentation and added the multi fields documentation to the core types page: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/mapping-core-types.html#_multi_fields_3

I guess this must be some browser cache, that showed the old link.
</comment><comment author="martijnvg" created="2014-01-22T10:07:54Z" id="33008001">Also I'll annotate the multi fields and copy to with the right version, in order to avoid confusion.
</comment><comment author="apatrida" created="2014-01-22T10:14:16Z" id="33008395">Ok.  Without copy_to there is no way to do what I wanted.  Path of "just_name" affects all 'children' of the mutli-field, when I only want one of them to go to the just_name and the rest to be suffixes.  path only applies at the top level of the field yes, and can't change for just one of the multi-values?

or can the name include a period.  so that TITLE could go to TITLE, TITLE.NOSTOP, TITLE.EXACT, and ALL_TITLES at the same time without changing the input JSON.
</comment><comment author="martijnvg" created="2014-01-22T10:43:27Z" id="33010621">Yes, `path` applies to all multi fields of a core field, but you can define another `fields` inside another `fields`. I think that something like this will result into something that you want:

``` bash
curl -XPUT "http://localhost:9200/test1" -d'
{
    "mappings": {
        "test1" : {
            "properties": {
                "title" : {
                    "type": "string",
                    "fields": {
                        "raw" : {
                            "type": "string"
                        },
                        "search_title" : {
                            "type" : "string",
                            "index": "no", 
                            "path": "just_name",
                            "fields" : {
                                "x" : {
                                    "type": "string",
                                    "index_name": "search_title"
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}'
```

But this way more verbose and less readable than when `copy_to` is used.
</comment><comment author="apatrida" created="2014-01-22T10:46:55Z" id="33010850">Ok, the default field doesn't actually work right.  (not related to copy_to)

```
 "content_cleaned": {
                    "type": "string",
                    "index": "analyzed",
                    "analyzer": "SearchWithSpecials",
                    "store": false,
                    "norms.enabled": true,
                    "term_vector": "with_positions_offsets",
                    "fielddata": {
                        "format": "disabled"
                    },
                    "fields": {
                        "stopped": {
                            "analyzer": "SearchWithSpecials",
                            "type": "string",
                            "index": "analyzed",
                            "store": false,
                            "norms.enabled": true,
                            "term_vector": "with_positions_offsets",
                            "fielddata": {
                                "format": "disabled"
                            }
                        },
...
```

note that content_cleaned and content_cleaned.stopped should do the same thing but content_cleaned is not searchable for a word, that is found when searching content_cleaned.stopped.  So the base default field doesn't work unless repeated within the fields list.
</comment><comment author="apatrida" created="2014-01-22T10:51:48Z" id="33011190">Doing this, does not resolve the problem that content_cleaned does not find words that content_cleaned.stopped finds (although their declaration is the same):

```
 "content_cleaned": {
                    "type": "string",
                    "index": "analyzed",
                    "analyzer": "SearchWithSpecials",
                    "store": false,
                    "norms.enabled": true,
                    "term_vector": "with_positions_offsets",
                    "fielddata": {
                        "format": "disabled"
                    },
                    "fields": {
                        "content_cleaned": {
                            "analyzer": "SearchWithSpecials",
                            "type": "string",
                            "index": "analyzed",
                            "store": false,
                            "norms.enabled": true,
                            "term_vector": "with_positions_offsets",
                            "fielddata": {
                                "format": "disabled"
                            }
                        },
                        "stopped": {
                            "analyzer": "SearchWithSpecials",
                            "type": "string",
                            "index": "analyzed",
                            "store": false,
                            "norms.enabled": true,
                            "term_vector": "with_positions_offsets",
                            "fielddata": {
                                "format": "disabled"
                            }
                        },
...
```
</comment><comment author="martijnvg" created="2014-01-22T10:59:35Z" id="33011729">I'm lost on what you're trying to achieve. Can you share a full example (via gist or something like that), with the mapping you're creating, index a sample document and search requests you expect to work.
</comment><comment author="apatrida" created="2014-01-22T11:13:42Z" id="33012696">I figured it out, had a type specific override breaking the mappings that I changed, had to change it as well.  All good, wrote a test case that covered the normal usage and it worked fine.
</comment><comment author="s1monw" created="2014-01-22T11:15:49Z" id="33012815">if you wanna contribute that testcase that is always welcome as well! just open a PR
</comment><comment author="apatrida" created="2014-01-22T11:21:11Z" id="33013144">Of course, thanks Simon
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/FieldMapperListener.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperBuilders.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/AllFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/multifield/MultiFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/MultiFieldsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/multifield/merge/JavaMultiFieldMergeTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file></files><comments><comment>Replaced the multi-field type in favour for the multi fields option that can be set on any core field.</comment></comments></commit></commits></item><item><title>Custom _all fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4520</link><project id="" key="" /><description>In the quest for a cleaner way of setting up custom `_all` fields, there are two questions that need to be answered:
1. Does it ever make sense to index tokens from different analyzer chains into a single field?
2. Can we support per-field boosting on the custom `_all` field (like we do with the `_all` field), and can we only pay the query-time price of per-field boosting if it used?
## Different analyzer chains

I can't think of a good use case where it makes sense to combine the output from different analyzer chains into a single field.  The field can only ever be searched via a single analyzer, multiple analyzers can produce tokens which interfere with each other (and so produce wrong results) and the term frequencies for overlapping tokens will be badly messed up.  Also, a clean token stream should never have offsets move "backwards".

So I think we can discount multiple analyzers outputting to a single field.
## Per-field boosting

When combining multiple fields into a single field, you lose the effect of field norms (ie title is shorter and thus more important than body).  Field-level boosting at index time is the only way to maintain this distinction.

The `_all` field takes field-level boosts into account by storing any boost that is not 1.0 as a payload with each term.  Retrieving these payloads has an impact on query performance, but the `_all` field has an optimization called "auto_boost" which allows you to only pay the price of payloads if any included field has a boost other than 1.0.  

I think field-level boosts should be supported with custom `_all` fields too.
# Proposed syntax

Given that we're not going to support separate analyzer chains, the current way of implementing custom `_all` fields with multi-fields is verbose and misleading, as it implies that each source field can apply its own analyzer.  

Instead, we suggest the following:

```
{ "title": {
    "type": "string",
    "copy_to": "my_all_field"
}}
```

The `copy_to` parameter can also support an array of fieldnames:

```
"copy_to": [ "my_all_field_1", "my_all_field_2" ]
```

Per-field boosting could be specified in two ways:
1. With the caret `^` syntax:
   
   "copy_to": "my_all_field^2"
2. As an object:
   
   "copy_to": { "field": "my_all_field", "boost": 2 }

The destination custom `_all` field can be defined in the mapping:

```
"my_all_field": {
    "type": "string",
    "analyzer": "my_analyzer"
}
```

If it is not defined in the mapping, then it should be added using dynamic mapping (or fail if dynamic mapping is disabled)
</description><key id="24566690">4520</key><summary>Custom _all fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v1.0.0</label></labels><created>2013-12-19T16:22:58Z</created><updated>2014-01-31T18:23:56Z</updated><resolved>2014-01-20T16:05:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-12-19T16:59:37Z" id="30945768">@clintongormley  

On Different analyzer chains:
I would not discount the value of different analyzer chains. Any chain that create multiple tokens at the same position (synonyms, stemmers) currently handled gracefully in various AND queries by treating tokens at the same position as (OR) fragment. It works very well for cases where I want to have an _all-like field but I want to decide which contributing fields should be stemmed and which should be precise (would not want to stem people names for example whlie would want to stem their comment). With potential implementation of text fragment boosting #4364 we could even have logic to boost originals higher that stemmed/synonym tokens

On Proposed syntax:
1. It does not support different analyzer chains :-)
2. It does not allow inheriting boosts of the contributing fields forcing us to repeat them. It would be nice if it would inherit it unless overridden

I would consider it a shorthand form but would like to retain complete verbose form
</comment><comment author="clintongormley" created="2013-12-19T17:03:52Z" id="30946217">@roytmana stemming some words and not others is pretty meaningless - you have to choose at query time whether you want to query the stemmed form or the unstemmed form. At that stage better to have it in two different fields. 

Putting tokens from multiple analysis chains results in a mess - it really doesn't work well.

Second, for field-level index time boosting:  I don't recommend using it for a single field.  You lose precision in field norms and you have to reindex if you want to change it. Much better to use query-time boosting on a field instead.

For the custom `_all` field you can't do it at query time, which is why I would like to support it there.  So you should only end up specifying it once: in the `copy_to` parameter.
</comment><comment author="roytmana" created="2013-12-19T17:18:01Z" id="30947470">It is not meaningless. Yes you have freedom to choose analyzer at query time but you do not have to. As I said latest ES versions handle AND queries for tokens on the same position gracefully removing issue with not being able to use the same (stem+no-stem) analyzer at query time

In some cases it will create a mess and in some no. In the case I outlined above it works better for me than trying to combine several flavors of _all like field (stemmed and unstemmed) and it is the only way to have an _all like field combining stemmed and unstemmed input very importand for cases where stemming of certain contributing fileds can screw up data (like stemming people names)

in cases when you have hundreds of fields contributing to an _all like field I would like to have as much control over how it is put together (boosts, position gaps and analyzer chain) as possible. It would be up to me to make sure it is not mess in the end. I would not want ES preventing me from getting burned by denying me such functionality. Not to mention that there could be many people who do use it already and removing it would break their code. 

I do not dispute that _all like field level analyzer chain without per contributing field chains is the most common use case but why not use shorthand default config - absence of analyzers on contributing field definition which will be the case when using your shorthand version as indication that the _all like field analyzers should be used
</comment><comment author="jpountz" created="2013-12-19T17:52:51Z" id="30950303">I'm wondering that it may actually be a better idea to stem your family names at indexing time? For example, let's imagine that one of the family names is Y, which is also a common name whose stem is X. I assume that you would apply stemming at query time so a query on `Y` would be translated into (`X` or `Y`). And then if you didn't apply stemming at indexing time, X is going to have a lower frequency, so matches on X are going to get better scores?
</comment><comment author="roytmana" created="2013-12-19T18:25:59Z" id="30952799">from that perspective, yes (ideally I would want to give stemmed form a slight negative boost), I did not test it enough with real data as I had to switch from all-like to back to _all field due to field based boosts not supported. 

But here is another scenario: I am most interested in real words not people names. I am searching on "turn" but getting also Turner because names were stemmed.

also in case of synonyms it is not as obvious

I guess it is never perfect for all the scenarious
</comment><comment author="roytmana" created="2013-12-20T15:43:02Z" id="31018123">@clintongormley if we use copy-to syntax, it would be great if we could copy multifields recursively into other multifields.

For example I may have a my_all field which includes 100 fields and I want a stemmed version of it and shingled one being able to create my_all_stemmed by copying my_all would be a huge benefit
</comment><comment author="clintongormley" created="2013-12-20T15:47:34Z" id="31018493">@roytmana I don't think that would work with how ES uses stream parsing.  We would have to hold on to a bunch of information to support this, plus would have to handle circular dependencies. Sounds more complex than we want to make this.  

Instead, you'll just be able to specify:  

```
"copy_to": [ "my_all", "my_all_stemmed"]
```

(yes I realise you'll have to do it on all 100 fields, but I think the advantages of being explicit outweigh the complications of recursion here)
</comment><comment author="roytmana" created="2013-12-20T16:29:51Z" id="31021859">@clintongormley fair enough it's not too hard . 

what about reversing it: 

```
"my_all":{"copy-from":["message.title", "message.body", "message.sender.email"....]}
```

makes it easy to maintain all in one place the big disadvantage is the need to use full property names

I still have some concern re. using just copy-to form:

I would like to be able to inherit boosts from contributing fields if no boost is specified in copy-to statement and I would like to be able to specify position gap offset for each contributing field even if you decide not to support different analysis pipelines

Will copy_to support bott strings ( field names to copy to) and objects with field name and options such as boos and gap offset and anything else we may need in the future. The string form would be a shorthand for default copy logic
</comment><comment author="clintongormley" created="2013-12-20T16:37:27Z" id="31022508">We did consider `copy_from`, but it suffers from similar issues with stream parsing. You essentially need to reparse the document in order to get all of the values from the other fields.

As far as `position_offset_gap`, that would be configured (like analyzer, type, etc) in the mapping for the destination field, as it is a single setting per analyzer (and we only have one analyzer -- the analyzer associated with the destination field).

Re inheriting boosts... hmmm, I suppose we could do that.  However, I repeat, using field-level index-time boosting is a bad idea, with the exception of when you use a custom `_all` field and are left with no other option.

&gt; and anything else we may need in the future

There shouldn't be anything other than boost. All we're doing is taking the value from one place and indexing as a different field, which has all the settings you need.  The only exception being per-field boost.
</comment><comment author="roytmana" created="2013-12-20T16:51:38Z" id="31023607">thanks for the explanation @clintongormley  I still feel that providing flexibility in hos all-like fields are put together (multiple pipelines) would have very valuable but it is your call of course :-)

Will traditional field-scoped concept of multifield remain (say for not analyzed version of a field no copying from multiple sources involved) or will we have to declare them separately and then use copy_to?

is this slotted for near future 0.9.x or 1.0.x? I just want to plan better as I have a rather big mapping file to rework. Thankfully it is all defined in javascript code and generates itself including proper naming (full name) of multifields where  both all-like and field scoped multifields are needed but still it is fair amount of work.
</comment><comment author="clintongormley" created="2013-12-20T16:53:47Z" id="31023768">&gt; Will traditional field-scoped concept of multifield remain (say for not analyzed version of a field no copying from multiple sources involved) or will we have to declare them separately and then use copy_to?

Multi-fields will remain, although I'd like to see their syntax improved as per #4521 

&gt; is this slotted for near future 0.9.x or 1.0.x?

It won't be in 0.90 but hoping to get it in for 1.0
</comment><comment author="roytmana" created="2013-12-20T16:57:47Z" id="31024067">@clintongormley many thanks!  
#4521 would be very nice to have as well.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file></files><comments><comment>Remove support for boost in copy_to field</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/FieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ParseContext.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BinaryFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/BooleanFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ByteFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/DoubleFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/FloatFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/LongFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/NumberFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/ShortFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TokenCountFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/TypeParsers.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/BoostFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperIntegrationTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/copyto/CopyToMapperTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/completion/CompletionPostingsFormatTest.java</file></files><comments><comment>Initial implementation of custom _all field</comment></comments></commit></commits></item><item><title>Search with Terms lookup might get stuck while doing a get for the terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4519</link><project id="" key="" /><description>Have seem it in the field, it seems to be stuck on getting the doc. Might be because of execution a failed search on the network worker thread (which is wrong, should be on the search thread pool). Also, we need to introduce a timeout so at least it won't get stuck on the get operation.
</description><key id="24563254">4519</key><summary>Search with Terms lookup might get stuck while doing a get for the terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.9</label><label>v1.0.0.RC1</label></labels><created>2013-12-19T15:34:06Z</created><updated>2013-12-23T12:34:44Z</updated><resolved>2013-12-23T10:42:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-23T10:42:18Z" id="31112873">Seems to be fixed thanks to #4526, a search that used to fail due to it (geo shape fetching) has been re-enabled and has not failed...
</comment><comment author="lmenezes" created="2013-12-23T12:34:44Z" id="31116864">:+1:  will give it a try when possible and give you some feedback on that. thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file></files><comments><comment>Failed search on a shard tries a local replica on a network thread</comment><comment>When a search on a shard to a remove node fails, and then replica exists on the local node, then the execution of the search is done on the network thread. This is problematic since we need to execute it on the actual search thread pool, but can also explain #4519, where the get happens on the network thread and it waits to send the get request till the network thread we use is freed (deadlock...)</comment><comment>fixes #4526</comment></comments></commit></commits></item><item><title>Use BINARY doc values instead of SORTED_SET doc values to store numeric data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4518</link><project id="" key="" /><description>Although SORTED_SET doc values make things like terms aggregations very fast
thanks to the use of ordinals, ordinals are usually not that useful on numeric
data. We are more interested in the values themselves in order to be able to
compute sums, averages, etc. on these values. However, SORTED_SET is quite slow
at accessing values, so BINARY doc values are better suited at storing numeric
data.

It is only allowed to have a single BINARY doc values field instance per field
name per document, which makes it quite challenging to use for multi-valued
fields since all values need to be buffered in memory and converted to a single
field instance in the end. In order to do so easily, all mappers (not only the
root mappers) now have a preParse and a postParse phase, which are called before
and after all fields have been visited for a single document. In the case of the
numeric field mappers, parse now takes care to buffer values and postParse takes
these values, sorts them and deduplicates them before encoding them into a
BINARY doc values field.

floats and doubles are encoded without compression with little-endian byte order
(so that it may be optimizable through sun.misc.Unsafe in the future given that
most computers nowadays use the little-endian byte order) and byte, short, int,
and long are encoded using vLong encoding: they first encode the minimum value
using zig-zag encoding (so that negative values become positive) and then deltas
between successive values.

I ran `TermsAggregationSearchBenchmark` to get an idea of the impact of this
change and results are promising:

| Task | Before this change | After this change | Difference |
| --: | --: | --: | --: |
| terms_agg_l_dv | 235 | 145 | 38% faster |
| terms_agg_lm_dv | 1705 | 714 | 58% faster |

For reference, `terms_agg_l_dv` is an aggregation on a single-valued long field
stored in doc values while `terms_agg_lm_dv` is an aggregation on a multi-valued
long field (10 values per document) stored in doc values.

Close #3993
</description><key id="24561310">4518</key><summary>Use BINARY doc values instead of SORTED_SET doc values to store numeric data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-19T15:02:19Z</created><updated>2014-06-14T22:27:13Z</updated><resolved>2013-12-26T09:05:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-12-20T16:02:56Z" id="31019755">Simon and I discussed about a better way to do the buffering and I'll explore a different approach that would use the Document object instead of the field mapper to do the buffering.
</comment><comment author="jpountz" created="2013-12-23T10:56:05Z" id="31113335">Here is a new approach that does the buffering at the document level (see ParseContext.Document). Since many field mappers were just reverted to go back to what they are in master, I squashed the commits to make it easier to review.
</comment><comment author="s1monw" created="2013-12-23T11:33:10Z" id="31114621">I left some minor comments but this looks awesome. +1 in general I think the next iter goes in though!
</comment><comment author="jpountz" created="2013-12-23T16:19:07Z" id="31127692">New commit pushed:
- Behavior of NaN is now consistent with uninverted field data, appearing at most once and always at the last position (compares greater than POSITIVE_INFINITY).
- Sorting and deduplication utility methods moved to a utility class.
</comment><comment author="s1monw" created="2013-12-25T10:27:20Z" id="31196611">did another round and left some minor comments that can be handled once its pushed. +1 LGTM
</comment><comment author="jpountz" created="2013-12-26T09:05:40Z" id="31215421">Thanks for the reviews, Simon. Much appreciated!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix loading templates in config/ directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4517</link><project id="" key="" /><description>The fixes introduced in #4235 and #4411 do not take into account, that a
template JSON in the config/ directory includes a template name, as opposed
when calling the Put Template API.

This PR allows to put both formats (either specifying a template name or not)
into files. However you template name/id may not be one of the template
element names like "template", "settings", "order" or "mapping".

Closes #4511
</description><key id="24555799">4517</key><summary>Fix loading templates in config/ directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-19T13:23:21Z</created><updated>2014-06-14T22:27:14Z</updated><resolved>2013-12-22T21:12:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-20T13:30:25Z" id="31009393">talked to @s1monw - we will revert back to the old behaviour and will not support the one which was introduced in 0.90.8
</comment><comment author="s1monw" created="2013-12-22T20:56:57Z" id="31095755">We decided that it will be a much better user experience if we keep the behaviour that was introduced by the original commit and add support for both the top level template name as well as the version that doesn't have a name. I will update this PR an pull it in.
</comment><comment author="s1monw" created="2013-12-22T21:12:53Z" id="31096080">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Javadoc for BoolQueryBuilder.hasClauses is wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4516</link><project id="" key="" /><description>The Javadoc for the method BoolQueryBuilder.hasClauses() in the Java API currently states: 
`Return &lt;code&gt;true&lt;/code&gt; if the query being built has no clause yet`

Which is wrong since hasClauses returns true if the query has any clauses, not if it doesn't.

I would suggest changing the doc to something like:
`Return &lt;code&gt;true&lt;/code&gt; if the query being built has any clause yet`

I could submit a pull-request for this, but I don't really think it's worth the hassle of signing a CLA just for a trivial change like this...
</description><key id="24550468">4516</key><summary>Javadoc for BoolQueryBuilder.hasClauses is wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dklotz</reporter><labels /><created>2013-12-19T11:22:26Z</created><updated>2014-12-24T17:40:59Z</updated><resolved>2014-12-24T17:40:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:40:59Z" id="68066182">Hi @dklotz 

Sorry it has taken a while to look at this.  In the meantime, the Javadocs have already been fixed. Thanks for reporting.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Geo clean up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4515</link><project id="" key="" /><description>The default unit for measuring distances is _MILES_ in most cases. ES should move over to the _International System of Units_ and return _METERS_ by default and internally work on a default unit.
If the internal measurement of explicitly changes to a default unit. `DistanceUnit.DEFAULT` which should relate to _meters_ will also effect the **REST API** at the following places:
- `ScriptDocValues.factorDistance()` returns _meters_ instead of _miles_
- `ScriptDocValues.factorDistanceWithDefault()` returns _meters_ instead of _miles_
- `ScriptDocValues.arcDistance()` returns _meters_ instead of _miles_
      one might use `ScriptDocValues.arcDistanceInMiles()`
- `ScriptDocValues.arcDistanceWithDefault()` returns _meters_ instead of _miles_
- `ScriptDocValues.distance()` returns _meters_ instead of _miles_
      one might use `ScriptDocValues.distanceInMiles()`
- `ScriptDocValues.distanceWithDefault()` returns _meters_ instead of _miles_
      one might use `ScriptDocValues.distanceInMilesWithDefault()`
- `GeoDistanceFilter` default unit changes from _kilometers_ to _meters_
- `GeoDistanceRangeFilter` default unit changes from _miles_ to _meters_
- `GeoDistanceFacet` default unit changes from _miles_ to _meters_

The naming of the `GeoBoundingBoxFilter` properties should allow to set the opposite corners (see #4084) namely `top_right` and `bottom_left`. This change also includes the fields `topRight` and `bottomLeft`. Also it should be possible to set the single values by using just `top`, `bottom`, `left` and `right` parameters.
</description><key id="24545322">4515</key><summary>Geo clean up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-19T09:31:30Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2014-01-11T12:58:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoHashUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoPoint.java</file><file>src/main/java/org/elasticsearch/common/geo/GeoUtils.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/CircleBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/EnvelopeBuilder.java</file><file>src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java</file><file>src/main/java/org/elasticsearch/common/unit/DistanceUnit.java</file><file>src/main/java/org/elasticsearch/index/fielddata/ScriptDocValues.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoDistanceRangeFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/GeoPolygonFilter.java</file><file>src/main/java/org/elasticsearch/index/search/geo/IndexedGeoBoundingBoxFilter.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file><file>src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetParser.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/test/java/org/elasticsearch/common/unit/DistanceUnitTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java</file><file>src/test/java/org/elasticsearch/index/query/SimpleIndexQueryParserTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file><file>src/test/java/org/elasticsearch/validate/SimpleValidateQueryTests.java</file></files><comments><comment>Geo clean Up</comment><comment>============</comment><comment>The default unit for measuring distances is *MILES* in most cases. This commit moves ES</comment><comment>over to the *International System of Units* and make it work on a default which relates</comment><comment>to *METERS* . Also the current structures of the `GeoBoundingBox Filter` changed in</comment><comment>order to define the *Bounding* by setting abitrary corners.</comment></comments></commit></commits></item><item><title>In the spirit of the soon-to-be New Year 2014?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4514</link><project id="" key="" /><description /><key id="24545265">4514</key><summary>In the spirit of the soon-to-be New Year 2014?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels /><created>2013-12-19T09:30:18Z</created><updated>2014-07-16T21:50:17Z</updated><resolved>2014-01-02T21:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-19T09:34:08Z" id="30915655">;)
</comment><comment author="kimchy" created="2013-12-19T20:41:42Z" id="30964479">wait, what happened to 2012 and 2013?
</comment><comment author="dweiss" created="2013-12-20T15:25:46Z" id="31016837">You tell me ;) This so much shows how much true developers care about legal issues and how much they'd rather just focus on the code...
</comment><comment author="s1monw" created="2014-01-02T21:11:37Z" id="31485011">pushed thx ;)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>elasticsearch close index error  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4513</link><project id="" key="" /><description>elasticsearch 0.90.5

[2013-12-19 16:14:25,278][DEBUG][indices.cluster          ] [Basilisk] [6666][0] removing shard (index is closed)
[2013-12-19 16:14:25,278][WARN ][cluster.service          ] [Basilisk] failed to apply updated cluster state:
version [7], source [close-index [6666]]
nodes: 
   [Basilisk][GzEzIl3dSbOGzxSgKHxwMw][inet[/128.18.110.126:9300]], local, master
routing_table:
routing_nodes:
-----node_id[GzEzIl3dSbOGzxSgKHxwMw][V]
---- unassigned

org.elasticsearch.index.IndexShardMissingException: [6666][0] missing
    at org.elasticsearch.index.service.InternalIndexService.shardInjectorSafe(InternalIndexService.java:296)
    at de.spinscale.elasticsearch.service.suggest.SuggestService$2.beforeIndexShardClosed(SuggestService.java:73)
    at org.elasticsearch.indices.InternalIndicesLifecycle.beforeIndexShardClosed(InternalIndicesLifecycle.java:104)
    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:371)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedShards(IndicesClusterStateService.java:285)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:395)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
</description><key id="24542391">4513</key><summary>elasticsearch close index error  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">zhyj0121</reporter><labels /><created>2013-12-19T08:16:46Z</created><updated>2013-12-20T06:47:15Z</updated><resolved>2013-12-20T06:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-19T13:39:32Z" id="30928745">Hey,

can you provide more information how this happened? Can you reproduce it actually? Was your cluster very busy when this happened? Or this node unresponsive? Anything in your environment we should know? Did you shutdown this node or scheduled it for shutdown? Were shards copied around?
</comment><comment author="spinscale" created="2013-12-19T13:41:48Z" id="30928896">Oh, I just saw, that you actually used the suggest plugin.

Can you create the bug report instead in the suggest plugin repository as it does not like a core elasticsearch problem: https://github.com/spinscale/elasticsearch-suggest-plugin

As a side note: Have you tried the new completion suggester, which is part of the core now? It might do the same job, but does not require you to install an external plugin. Would be interested, if the completion suggester fits your usecase.
</comment><comment author="zhyj0121" created="2013-12-20T06:47:15Z" id="30991903">ok,thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add the memory used on segment/segments stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4512</link><project id="" key="" /><description>The memory used for the Lucene index (term dict, bloom filter, ...) can now be reported per segment using the segments API, and on the segments flag on node/indices stats
</description><key id="24519394">4512</key><summary>Add the memory used on segment/segments stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label></labels><created>2013-12-18T21:21:14Z</created><updated>2013-12-19T21:08:26Z</updated><resolved>2013-12-18T21:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-19T21:08:26Z" id="30966663">removed tags, this has been disabled due to a bug in Lucene: https://issues.apache.org/jira/browse/LUCENE-5373, we will re-enable it once its fixed.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/codec/CodecService.java</file><file>src/main/java/org/elasticsearch/index/codec/postingsformat/BloomFilterPostingsFormat.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/store/Store.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineIntegrationTest.java</file><file>src/test/java/org/elasticsearch/index/engine/robin/RobinEngineTests.java</file><file>src/test/java/org/elasticsearch/index/merge/policy/MergePolicySettingsTest.java</file></files><comments><comment>Allow to enable / disable bloom filter loading on an index</comment><comment>Allow to have a new index level setting index.codec.bloom.load (default to true), that can control if the boom filters will be loaded or not. This is an updateable setting, that can be updated on a live index using the update settings API.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentResponse.java</file><file>src/main/java/org/elasticsearch/index/engine/Segment.java</file><file>src/main/java/org/elasticsearch/index/engine/SegmentsStats.java</file><file>src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java</file><file>src/test/java/org/elasticsearch/indices/stats/SimpleIndexStatsTests.java</file></files><comments><comment>Add the memory used on segment/segments stats</comment><comment>The memory used for the Lucene index (term dict, bloom filter, ...) can now be reported per segment using the segments API, and on the segments flag on node/indices stats</comment><comment>closes #4512</comment></comments></commit></commits></item><item><title>template mappings are not loading in 0.90.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4511</link><project id="" key="" /><description>I have templates that define specific mappings. In testing 0.90.8 today, I noticed that the template mappings are not being used when I load data in to an index that matches the template.  This has worked in all previous versions of ElasticSearch that I have used (0.19x -&gt; 0.90.7).
</description><key id="24518909">4511</key><summary>template mappings are not loading in 0.90.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">seallison</reporter><labels><label>bug</label><label>v0.90.9</label><label>v1.0.0.RC1</label></labels><created>2013-12-18T21:12:57Z</created><updated>2013-12-22T21:12:26Z</updated><resolved>2013-12-22T21:12:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-18T22:30:06Z" id="30887744">Hey @seallison can you gimme more information what you mean by template mapping and maybe provide a small recreation gist for the problem you see?
</comment><comment author="spinscale" created="2013-12-18T22:54:56Z" id="30889637">Hey,

there were a couple of bugs fixed for 0.90.8 regarding file based mapping template loading - actually template loading by file was not working until 0.90.8 in a couple of previous 0.90 releases (wondering that this has worked for you so far). I am very happy to help and debug the issue with you, if you help with a bit more information.
</comment><comment author="seallison" created="2013-12-18T23:32:16Z" id="30892152">This is an example of what I'm talking about:  https://gist.github.com/seallison/8031640

Let me know if you need any additional information. Thanks!
</comment><comment author="seallison" created="2013-12-18T23:38:12Z" id="30892510">@spinscale all of my mappings have been in templates that I construct like in my gist. I've deployed this using 0.90.0, 0.90.3, and 0.90.5 without issue.
</comment><comment author="spinscale" created="2013-12-19T09:49:21Z" id="30916528">I can see why it happens. A quick fix is to remove the most outer data structure named `template_testlocations` and directly start with `template` like this:

```
{
        "template": "locations*",
        "mappings": {
            "locations": {
                "_source": {
                    "compress": true
                },
                "_all": {
                    "enabled": false
                },
                "properties": {
                    "contactPersonId" : {
                        "index_analyzer": "keyword",
                        "type" : "string"
                    },
                    "state": {
                        "index_analyzer": "keyword",
                        "type": "string"
                    }
                }
            }
        }
    }
```
</comment><comment author="s1monw" created="2013-12-22T20:58:17Z" id="31095781">We gonna revert the revert to make sure people that were on `0.90.8` and move to the next version will have a better user experience if we support both formats hence I reopened https://github.com/elasticsearch/elasticsearch/pull/4517
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/alias/Alias.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexAliasesService.java</file></files><comments><comment>Alias code cleanup</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/ToAndFromJsonMetaDataTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Fix loading templates in config/ directory</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Reverting back to 0.90.7 config/templates loading behaviour</comment></comments></commit></commits></item><item><title>Fix compilation on Java 8 + tests that rely on ordering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4510</link><project id="" key="" /><description>Note, we still have tests failing because of mvel compilation bugs, see more here: http://jira.codehaus.org/browse/MVEL-299
</description><key id="24501886">4510</key><summary>Fix compilation on Java 8 + tests that rely on ordering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.9</label><label>v1.0.0.RC1</label></labels><created>2013-12-18T16:42:51Z</created><updated>2013-12-18T16:54:08Z</updated><resolved>2013-12-18T16:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/jsr166e/ConcurrentHashMapV8.java</file><file>src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/UpdateNumberOfReplicasTests.java</file></files><comments><comment>Fix compilation on Java 8 + tests that rely on ordering</comment><comment>Note, we still have tests failing because of mvel compilation bugs, see more here: http://jira.codehaus.org/browse/MVEL-299</comment><comment>closes #4510</comment></comments></commit></commits></item><item><title>[feature request] Automatic default field for multifield</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4509</link><project id="" key="" /><description>Hi, I have quite complex ES mapping (600 lines) and I use python script to build it. Python script is quite small since some blocks are reused. But I have some problem with multifield mapping since I need to include it in _all field. I must pass default field name through all nested blocks in my scripts. 

Here is an example:
https://gist.github.com/serj-p/8024737

So my purpose is to add some extra key to recognize field as default automatically without need of passing name of containing multifield field.
</description><key id="24498550">4509</key><summary>[feature request] Automatic default field for multifield</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">serj-p</reporter><labels /><created>2013-12-18T15:56:51Z</created><updated>2014-07-23T14:02:22Z</updated><resolved>2014-07-23T14:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T14:02:22Z" id="49877448">This should be considerably easier with the new multi-fields syntax from version 1.0 onwards
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve query/filter parsing strictness </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4508</link><project id="" key="" /><description>Make the parsing strict or stricter for:
- `geo_shape` filter &amp; query
- `common` query

Parsing must be strict in order to avoid misleading behaviour.
</description><key id="24494374">4508</key><summary>Improve query/filter parsing strictness </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-18T14:55:42Z</created><updated>2013-12-18T16:57:13Z</updated><resolved>2013-12-18T16:57:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/CommonTermsQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file></files><comments><comment>Make parsing strict for `geo_shape` query &amp; filter and stricter for `common` query.</comment></comments></commit></commits></item><item><title>Table: don't eat multiple consecutive spaces in fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4507</link><project id="" key="" /><description>The kibana table display includes strings in the `&lt;td&gt;` elements from messages that are merely escaped but spaces are untouched.  In HTML, two consecutive spaces are interpreted as a single space.  This is bad because I am trying to display sets of log lines that are to be viewed together and they are formatted in a grid.  In this text grid, multiple spaces are inserted for alignment purposes, and I need to have them preserved in the Web interface.

I came up with this small change to replace each space with &amp;nbsp;, and it seems to fix the issue for me.

(I also changed the css to display the table in Monospace font, but that's another issue.)

```
diff --git a/src/app/panels/table/module.js b/src/app/panels/table/module.js
index 20b0736..8a535e6 100644
--- a/src/app/panels/table/module.js
+++ b/src/app/panels/table/module.js
@@ -380,7 +380,9 @@ function (angular, app, _, kbn, moment) {
   module.filter('tableTruncate', function() {
     return function(text,length,factor) {
       if (!_.isUndefined(text) &amp;&amp; !_.isNull(text) &amp;&amp; text.toString().length &gt; 0) {
-        return text.length &gt; length/factor ? text.substr(0,length/factor)+'...' : text;
+         text = text.length &gt; length/factor ? text.substr(0,length/factor)+'...' : text;
+         text = text.replace(/ /g, "&amp;nbsp;");
+         return text;
       }
       return '';
     };
```
</description><key id="24491848">4507</key><summary>Table: don't eat multiple consecutive spaces in fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gjcarneiro</reporter><labels /><created>2013-12-18T14:12:58Z</created><updated>2013-12-19T13:30:30Z</updated><resolved>2013-12-19T13:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-19T13:30:30Z" id="30928179">Hey,

can you add this issue maybe to the https://github.com/elasticsearch/kibana repository - as this repository is only about the elasticsearch itself.

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Single shards APIs should fail if routing is required.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4506</link><project id="" key="" /><description>If routing for a specific type is configured to be required, but no routing is specified during a single shard api call a client error should be thrown. The following APIs need this validation: get, mget, explain, termvector, multi term vector and exists.

Note: this validation already happens in the index and delete APIs, but the error will change to a client error instead of the internal server error.
</description><key id="24488262">4506</key><summary>Single shards APIs should fail if routing is required.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-18T13:01:15Z</created><updated>2014-01-02T09:48:15Z</updated><resolved>2014-01-02T09:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[TEST] split tests with parent to pre/post 1.0 in the yaml test suite</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/RoutingMissingException.java</file><file>src/main/java/org/elasticsearch/action/explain/TransportExplainAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/TransportMultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorRequest.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportMultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/TransportSingleShardTermVectorAction.java</file><file>src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/test/java/org/elasticsearch/mget/SimpleMgetTests.java</file><file>src/test/java/org/elasticsearch/routing/AliasRoutingTests.java</file><file>src/test/java/org/elasticsearch/routing/SimpleRoutingTests.java</file></files><comments><comment>Made single shards APIs fail if routing is configured to be required in the mapping.</comment></comments></commit></commits></item><item><title>Make doc lookup in geo_shape filter and query consistent with terms lookup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4505</link><project id="" key="" /><description>The `geo_shape filter and query` option in geo_shape filter and query has been replaced with the `path` option, which allows these filter and query to fetch shapes from within objects as well.

Relates to #4486
</description><key id="24488045">4505</key><summary>Make doc lookup in geo_shape filter and query consistent with terms lookup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-12-18T12:55:42Z</created><updated>2015-05-18T23:33:20Z</updated><resolved>2013-12-23T10:22:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-21T22:23:33Z" id="31073774">LGTM
</comment><comment author="martijnvg" created="2013-12-23T10:22:21Z" id="31112175">pushed to master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Threadpool information in node stats is insufficient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4504</link><project id="" key="" /><description>Currently the only way to see threadpool information is by retrieving the threadpool node stats.

Threadool node stats currently do not show the queue type (fixed etc) and more importantly do not show things like queue_size.

Would be great if one could simply retrieve the current threadpool configuration apart from the threadpool stats. (we've had issues for example with the default queue_size for bulk operations that was recently set to 50, but we couldnt see this in the node stats)
</description><key id="24484013">4504</key><summary>Threadpool information in node stats is insufficient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">folke</reporter><labels /><created>2013-12-18T11:25:03Z</created><updated>2014-01-05T12:10:38Z</updated><resolved>2013-12-18T11:37:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-12-18T11:37:54Z" id="30835010">This is available via the nodes-info API:

```
curl -XGET "http://localhost:9200/_nodes/thread_pool?pretty"
```
</comment><comment author="folke" created="2014-01-05T12:10:38Z" id="31602626">Wasn't aware of that. Exactly what we need. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>On node join, evict existing node(s) with the same transport address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4503</link><project id="" key="" /><description>Make sure to evict an existing node with the same transport address as a new node that joins. This can happen for example when there is a bug in a cluster state event handler, which causes the "old" node to not be evicted, or a load on the master node that will take time for the "old" node leaving to be processed.
</description><key id="24483812">4503</key><summary>On node join, evict existing node(s) with the same transport address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-18T11:20:41Z</created><updated>2013-12-18T11:22:07Z</updated><resolved>2013-12-18T11:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/discovery/zen/ZenDiscovery.java</file></files><comments><comment>On node join, evict existing node(s) with the same transport address</comment><comment>Make sure to evict an existing node with the same transport address as a new node that joins. This can happen for example when there is a bug in a cluster state event handler, which causes the "old" node to not be evicted, or a load on the master node that will take time for the "old" node leaving to be processed.</comment><comment>closes #4503</comment></comments></commit></commits></item><item><title>Don't delete local shard data when its allocated on a node that doesn't exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4502</link><project id="" key="" /><description>This is an extreme case, exposed by a bug we had in our allocation in local gateway, causing a cluster state that doesn't include a node in the nodes list, but still has the shard in the routing table pointing at the non existent node. Then, when a node on the same box comes back, it will cause the local shard data to be deleted because it thinks its fully allocated on other nodes.
</description><key id="24481588">4502</key><summary>Don't delete local shard data when its allocated on a node that doesn't exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-18T10:35:38Z</created><updated>2014-01-21T17:32:25Z</updated><resolved>2013-12-18T10:37:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/indices/store/IndicesStore.java</file></files><comments><comment>Don't delete local shard data when its allocated on a node that doesn't exists</comment><comment>This is an extreme case, exposed by a bug we had in our allocation in local gateway, causing a cluster state that doesn't include a node in the nodes list, but still has the shard in the routing table pointing at the non existent node. Then, when a node on the same box comes back, it will cause the local shard data to be deleted because it thinks its fully allocated on other nodes.</comment><comment>fixes #4502</comment></comments></commit></commits></item><item><title>Having exact match in the search </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4501</link><project id="" key="" /><description>{
    DATA:this is a sample querwsf QW$E%#@% T#%#TGRF#@T Y$Y%@# $T%HYT#R $GNBMKL:?"P":KIJU:UILOKITRTREQFGRT%RTRHGRV ;key=value;key2=value2; quer
    key:value
    key2:valu2
    key:value1
    appgroup:value1
}

appgroup=value1 yes
key=value1 no
"key=value1" no
"sample value" no
"sample quer" yes, but quer at the end should not be highlighted
"\"P\":KL" yes
"# $T%HYT#R $" yes
"# $T%HYT#R ^" no
key=value yes

those with yes should match and no should not return any result
</description><key id="24477505">4501</key><summary>Having exact match in the search </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">desaxena</reporter><labels /><created>2013-12-18T09:13:32Z</created><updated>2013-12-18T10:05:22Z</updated><resolved>2013-12-18T10:05:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="desaxena" created="2013-12-18T09:16:03Z" id="30826813">Data is to be indexed in the format we are searching, I put this query on google groups but did not get any satisfactory answer.
</comment><comment author="dadoonet" created="2013-12-18T10:05:22Z" id="30829657">May be some text in google groups will help to understand the question?
A curl recreation would help also other users to help you without spending time on reproducing your case.

See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>StreamOutput check array type error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4500</link><project id="" key="" /><description>none
</description><key id="24477271">4500</key><summary>StreamOutput check array type error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">melin</reporter><labels /><created>2013-12-18T09:07:48Z</created><updated>2013-12-18T12:52:31Z</updated><resolved>2013-12-18T12:52:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Accurate GeoDistance Function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4499</link><project id="" key="" /><description>- setup accurate GeoDistance Function
- adapt tests
- introduced default GeoDistance function
- Updated docs

closes #4498
</description><key id="24474896">4499</key><summary>Accurate GeoDistance Function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-12-18T08:03:35Z</created><updated>2014-06-15T16:22:29Z</updated><resolved>2013-12-27T11:10:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-12-18T08:04:40Z" id="30823387">I also modiefied the tests. But croping out the pole areas is not the right way in opinion. Also the test still fail in other areas. Since we know the function is sloppy, we can drop the accuracy tests I guess.
</comment><comment author="jpountz" created="2013-12-18T08:11:04Z" id="30823645">PR looks good to me. I would just vote to change `sloppy` to `sloppy_arc` to make clearer that the distance computation takes into account the fact that the earth is round on the contrary to `plane`? Also can you give examples of where the distance computation fails so that we can look into it?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Accurate GeoDistance Function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4498</link><project id="" key="" /><description>The `GeoDistance` function setup in #4225 is not very accurate measuring distances close to the pole areas. So an accurate implementation should be setup next to this implementation. Nevertheless the sloppy distance function should kept as _default_ for distance calculations.
</description><key id="24471285">4498</key><summary>Accurate GeoDistance Function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-18T05:52:41Z</created><updated>2014-01-03T07:23:03Z</updated><resolved>2013-12-27T11:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/geo/GeoDistance.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file><file>src/main/java/org/elasticsearch/search/facet/geodistance/GeoDistanceFacetParser.java</file><file>src/main/java/org/elasticsearch/search/sort/GeoDistanceSortParser.java</file><file>src/test/java/org/apache/lucene/util/SloppyMathTests.java</file></files><comments><comment>* setup accurate GeoDistance Function</comment><comment>* adapt tests</comment><comment>* introduced default GeoDistance function</comment><comment>* Updated docs</comment></comments></commit></commits></item><item><title>Add support for flat_settings flag to all REST APIs that output settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4497</link><project id="" key="" /><description> Closes #4140

Alternative implementation with `flat_settings` parameter instead of `human` flag.
</description><key id="24466836">4497</key><summary>Add support for flat_settings flag to all REST APIs that output settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-12-18T03:04:54Z</created><updated>2014-07-01T06:28:58Z</updated><resolved>2014-01-08T15:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-18T15:23:50Z" id="30849932">looks good, lets default flat_settings to false in master, and if we backport to 0.90, to preserve backward comp., we can default it to true.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make partial dates without year to be 1970 based instead of 2000</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4496</link><project id="" key="" /><description>Fixes #4451

Date fields without date (HH:mm:ss, for example) are parsed as time on Jan 1, 1970 UTC. However, before this change partial dates without year (MMM dd HH:mm:ss, for example) were parsed as as days of they year 2000. This change makes all partial dates to be treated based on year 1970. This is breaking change - before this change "Dec 15, 10:00:00" in most cases was parsed (and indexed) as "2000-12-15T10:00:00Z". After this change, it will be consistently parsed and indexed as  "1970-12-15T10:00:00Z"
</description><key id="24466114">4496</key><summary>Make partial dates without year to be 1970 based instead of 2000</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-12-18T02:38:54Z</created><updated>2014-07-16T21:50:19Z</updated><resolved>2014-01-07T01:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding JS Client docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4495</link><project id="" key="" /><description>Already added to master
</description><key id="24464531">4495</key><summary>Adding JS Client docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels /><created>2013-12-18T01:48:30Z</created><updated>2014-07-16T21:50:19Z</updated><resolved>2013-12-18T01:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>JavaScript client docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4494</link><project id="" key="" /><description>Added doc page for the JavaScipt client, and listed it in the clients list.
</description><key id="24455916">4494</key><summary>JavaScript client docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels /><created>2013-12-17T22:29:10Z</created><updated>2014-07-08T08:21:33Z</updated><resolved>2013-12-17T22:41:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Merge pull request #4494 from spenceralger/add_js_docs</comment></comments></commit></commits></item><item><title>Made APIs consistently accept a query in the request body's `query` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4493</link><project id="" key="" /><description>The following APIs now accept the query in a top level `query` field like:
- delete_by_query
- validate_query
- count

These APIs used to accept the query directly in the request body which was inconsistent with the search and explain APIs. For this reason this is a breaking change.

Count api note: It still has its own piece if code and isn't a shortcut for /_search?search_type=count (so no facets, aggs etc.) The count api code is fine tuned for simple counting and I think it should stay that way. 

Relates to #4074
</description><key id="24438873">4493</key><summary>Made APIs consistently accept a query in the request body's `query` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-17T18:02:56Z</created><updated>2015-05-18T23:33:24Z</updated><resolved>2014-01-02T09:07:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-02T09:07:07Z" id="31442529">pushed to master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add _source, _doc and _field support to fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4492</link><project id="" key="" /><description>When requesting `fields` in the search, get, update, etc APIs, we should be able to control where those values come from with these prefixes:
- `_source`: extract the field value from the `_source` field
- `_fields`: retrieve a `stored` field value
- `_doc`: retrieve the value from fielddata

The `_doc` option is particularly useful for retrieving dates as, no matter the format they're specified in in the source, they will be accessible as milliseconds-since-the-epoch from fielddata, without the need to allow dynamic scripts.

While on the subject, perhaps we should support this same syntax in scripting, instead of `docs['foo']`?
</description><key id="24438352">4492</key><summary>Add _source, _doc and _field support to fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0</label></labels><created>2013-12-17T17:54:49Z</created><updated>2014-02-24T13:56:44Z</updated><resolved>2014-01-21T16:12:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-09T17:25:03Z" id="31955740">@clintongormley to make sure I understand, you proposing this syntax:

```
curl 'localhost:9200/_search?q=foo:bar&amp;fields=_source:body,_fields:foo,_doc:date'
```

and

```
curl 'localhost:9200/_search' -d'{
  "query": {"match_all": {}},
  "fields": ["_source:body", "_fields:foo", "_doc:date"]
}'
```

Is that correct?
</comment><comment author="dakrone" created="2014-01-09T19:15:57Z" id="31965816">We discussed this and decided to add a `fielddata_fields` parameter similar to `script_fields` to extract fields from the field data for returning. It'll also remove the workaround here: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/fetch/FieldsParseElement.java#L40
</comment><comment author="kimchy" created="2014-01-11T19:29:52Z" id="32104660">@dakrone and the workaround in the GET API....
</comment><comment author="dakrone" created="2014-01-14T17:42:59Z" id="32287975">@kimchy I currently have `fielddata_fields` working just like `script_fields` does, however, if I remove the workaround for `_source.&lt;name&gt;` and `doc['name'].value`, then it's not possible to retrieve field data values via the GET API.

Is this acceptable, or should I work on adding support for `fielddata_fields` in the GET API? (`script_fields` is also not supported in the GET API).
</comment><comment author="kimchy" created="2014-01-14T18:26:30Z" id="32291981">I am personally ok with it, we have source filtering now for the get API, and doc notation is not supported for GET. The reason why doc / fielddata is problematic with GET API is that we support realtime get, and we might fetch the doc from the transaction log and not form Lucene.
</comment><comment author="dakrone" created="2014-01-14T18:34:42Z" id="32292791">Sounds good, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/index/get/ShardGetService.java</file><file>src/main/java/org/elasticsearch/percolator/PercolateContext.java</file><file>src/main/java/org/elasticsearch/search/SearchModule.java</file><file>src/main/java/org/elasticsearch/search/builder/SearchSourceBuilder.java</file><file>src/main/java/org/elasticsearch/search/fetch/FetchPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/FieldsParseElement.java</file><file>src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsContext.java</file><file>src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsFetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/fetch/fielddata/FieldDataFieldsParseElement.java</file><file>src/main/java/org/elasticsearch/search/internal/DefaultSearchContext.java</file><file>src/main/java/org/elasticsearch/search/internal/SearchContext.java</file><file>src/test/java/org/elasticsearch/document/DocumentActionsTests.java</file><file>src/test/java/org/elasticsearch/explain/ExplainActionTests.java</file><file>src/test/java/org/elasticsearch/index/search/child/TestSearchContext.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>Add the ability to retrieve fields from field data</comment></comments></commit></commits></item><item><title>Include/Exclude Filtering Behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4491</link><project id="" key="" /><description>Currently there is a bug in elasticsearch (https://github.com/elasticsearch/elasticsearch/issues/4047) where empty objects are not stored in `_source` when an include/exclude list is present.

This is because elasticsearch aggressively removes empty objects from the `_source`.

For example, if I have an object

```
{ 'data': { 'key': 'value' } }
```

the following filters will all result in removing `data` from `_source`: `excludes = ['data']`, `excludes = ['data.*']`, `excludes = ['data.key']`, `excludes = ['*.key']`, `includes = ['data.other']`

I believe this behavior is incorrect. I think that we should only remove an object if it is explicitly excluded (`excludes = ['data']`, `excludes = ['data.*']`) or if no elements are included (`includes = ['other_data.*']).  For situations where the object is referenced in an includes list but there is no match, I think the object should remain as an empty object (`includes = ['data.other']`).

This use case makes more sense if we are talking about some nested object that is indexed...

Example:

```
{
  "name": "John Doe",
  "identifiers": {
    "ssn": "987-65-4320",
    "facebook_uid": "12345",
    "twitter_uid": "54321"
  }
```

and `excludes = [ "*.ssn"]` would drop the entire `identifiers` object if the only key was `ssn` for that object, even if we want the empty identifiers object to remain under all circumstances.  We have the same problem with `includes = ["*.instagram_uid"]`.
</description><key id="24435180">4491</key><summary>Include/Exclude Filtering Behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">RobCherry</reporter><labels /><created>2013-12-17T17:08:26Z</created><updated>2014-08-15T12:39:50Z</updated><resolved>2014-01-16T12:00:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-12-18T08:33:00Z" id="30824569">Thanks for opening it. One quick comment about:

&gt; We have the same problem with includes = ["*.instagram_uid"].

This is tricky - in your example it makes sense but the implications are that we'd have to leave all other objects intact - which will potentially result in a lot of empty objects (trees). At least for this case I'd use `include [ "identifiers", "*.instagram_uid"`  . 
</comment><comment author="RobCherry" created="2013-12-19T00:31:35Z" id="30895536">Fair enough.  Using `includes = [ "identifiers", "*.instagram_uid"]` would be correct in that case.
</comment><comment author="bleskes" created="2014-01-14T16:13:42Z" id="32278753">Hi Rob,

I've implemented a solution that is not exactly what you ask for but I feel it's simpler to reason &amp; understand how it works. Effectively it makes excludes maintain document structure above the point of exclusion.  You can see it here: https://github.com/elasticsearch/elasticsearch/issues/4715

I used part of the tests you wrote as a basis so I included your name in the commit to give you credit for the work.

Can you please check if you still feel this case is needed and if not close it?

Thx,
Boaz
</comment><comment author="RobCherry" created="2014-01-14T22:43:55Z" id="32315946">Ignore that commit for now, it is not actually testing what I wanted to test.  I will update it tomorrow.
</comment><comment author="RobCherry" created="2014-01-15T14:39:30Z" id="32366520">@bleskes 

What you're describing as the proposed change

&gt; maintain document structure above the point of exclusion

seems like a great solution.

I was originally going to add some additional tests to ensure that document structure is maintained both with and without includes, but then I noticed that `testNotOmittingObjectWithNestedExcludedObject` was covering most of what I wanted.
</comment><comment author="bleskes" created="2014-01-16T11:43:51Z" id="32462095">@RobCherry happy you like it. Can we close this issue then?
</comment><comment author="RobCherry" created="2014-01-16T12:00:44Z" id="32463077">Yes
</comment><comment author="bleskes" created="2014-01-16T12:01:30Z" id="32463113">cool. Once again, thx for your help?.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java</file><file>src/test/java/org/elasticsearch/common/xcontent/support/XContentMapValuesTests.java</file><file>src/test/java/org/elasticsearch/search/fields/SearchFieldsTests.java</file></files><comments><comment>excluding all fields of an object should not remove parent.</comment></comments></commit></commits></item><item><title>Expose flags in simple_query_string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4490</link><project id="" key="" /><description>The `simple_query_string` query allows the user to enable/disable parts of the syntax, eg prefixes, phrases etc.

I think it makes sense to support the full syntax by default, but to allow the user to disable the parts that they don't want, eg:

```
"simple_query_string": {
    "query": "....",
    "disable": "PREFIX|PHRASE"
}
```

... in a similar way that we do with `flags` in the regexp query/filter
</description><key id="24431055">4490</key><summary>Expose flags in simple_query_string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-17T16:11:29Z</created><updated>2014-01-10T16:05:38Z</updated><resolved>2014-01-03T23:17:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-01-03T23:17:49Z" id="31562196">Merged in https://github.com/elasticsearch/elasticsearch/commit/5463f7953f9d16e86a736302c6e10f6aa9ef645b and https://github.com/elasticsearch/elasticsearch/commit/18cdde624c91cba8bd171a510f5a6413d8472f55
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[Docs] Fix Typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4489</link><project id="" key="" /><description>Fixes small typo in the geo_distance aggregation docs. ("onceptually" -&gt; "conceptually")

fixes #4487
</description><key id="24430594">4489</key><summary>[Docs] Fix Typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">simplechris</reporter><labels /><created>2013-12-17T16:05:29Z</created><updated>2014-06-28T09:00:03Z</updated><resolved>2013-12-18T10:31:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-12-18T10:31:58Z" id="30831260">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Combine cluster.routing.allocation.disable.* settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4488</link><project id="" key="" /><description>Currently we have three different disable-allocation config options:

`cluster.routing.allocation.disable_new_allocation`:

Allows to disable new primary allocations. Note, this will prevent allocations for newly created indices. This setting really make sense when dynamically updating it using the cluster update settings API.

`cluster.routing.allocation.disable_allocation`:

Allows to disable either primary or replica allocation (does not apply to newly created primaries, see disable_new_allocation above). Note, a replica will still be promoted to primary if one does not exist. This setting really make sense when dynamically updating it using the cluster update settings API.

`cluster.routing.allocation.disable_replica_allocation`:

Allows to disable only replica allocation. Similar to the previous setting, mainly make sense when using it dynamically using the cluster update settings API.

Besides the fact that `disable: true` reads a bit like a double negative, these settings seem to overlap.  Could we not merge them into a single setting:

```
cluster.routing.allocation.enable: all | primaries | new_primaries | none
```

with the default set to `all`.

The existing disable allocation settings will be deprecated in favour for the enable allocation setting.
</description><key id="24430534">4488</key><summary>Combine cluster.routing.allocation.disable.* settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-17T16:04:43Z</created><updated>2014-01-15T10:22:55Z</updated><resolved>2014-01-09T09:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-03T09:48:58Z" id="31513383">I think we should also change the per index allocation settings: `index.routing.allocation.disable_allocation`, `index.routing.allocation.disable_new_allocation` and `index.routing.allocation.disable_replica_allocation`, into:
`index.routing.allocation.enable: all | primaries | new_primaries | none`
</comment><comment author="kimchy" created="2014-01-03T11:06:38Z" id="31516577">it would be great if we can do this change in a backward compatible manner..., even for 1.0.
</comment><comment author="s1monw" created="2014-01-03T12:48:47Z" id="31520512">so the question is if we push this to 0.90 as well since folks might be able to upgrade more easily? It seems like we can just put another commit on top of the existing one and then only backport the current commit? this might be nice though?
</comment><comment author="kimchy" created="2014-01-03T12:51:37Z" id="31520614">I was thinking of keeping the existing settings supported for 1.0, I don't think we need to backport this anyhow to 0.90. Its not urgent though, we can just communicate the breaking change, though since those are very popular settings, my thought was that it might be worth the effort to support the old (deprecated) settings for 1.0 as well.
</comment><comment author="martijnvg" created="2014-01-03T14:21:56Z" id="31524849">No strong feelings, but I think it nice to just deprecate the existing settings in 1.0, because these settings were around for many versions and this will give people some time to properly migrate.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDecidersModule.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationTests.java</file></files><comments><comment>Deprecated disable allocation decider which has the following options:</comment><comment>`allocation.disable_new_allocation`, `allocation.disable_allocation`, `allocation.disable_replica_allocation`,</comment><comment>in favour for the enable allocation decider which has a single option `allocation.enable` wich can be set to the following values:</comment><comment>`none`, `new_primaries`, `primaries` and `all` (default).</comment></comments></commit></commits></item><item><title>Fix Typo in Geo Distance Aggregation docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4487</link><project id="" key="" /><description>Fixed a typo I noticed in the new aggregations docs. Creating an issue as per the [ contributing guidelines](https://github.com/elasticsearch/elasticsearch/blob/master/CONTRIBUTING.md)
</description><key id="24430527">4487</key><summary>Fix Typo in Geo Distance Aggregation docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simplechris</reporter><labels><label>docs</label></labels><created>2013-12-17T16:04:36Z</created><updated>2013-12-18T10:31:40Z</updated><resolved>2013-12-18T10:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-12-18T10:31:40Z" id="30831233">Fixed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make doc lookups in queries/filters consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4486</link><project id="" key="" /><description>The `terms` lookup filter uses index, type, id and path while the geoshape filter and query use index, type, id and shape_field_name.

The geoshape filter/query should be changed to use `path` instead.
</description><key id="24427832">4486</key><summary>Make doc lookups in queries/filters consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-17T15:27:26Z</created><updated>2013-12-23T10:21:08Z</updated><resolved>2013-12-23T10:21:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] Changed `shape_field_name` to `path` in geo_shape filter documentation.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/GeoShapeQueryParser.java</file><file>src/main/java/org/elasticsearch/index/search/shape/ShapeFetchService.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoShapeIntegrationTests.java</file></files><comments><comment>Make doc lookup in geo_shape filter and query consistent with terms lookup.</comment></comments></commit></commits></item><item><title>Option to disable logging source in slowlog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4485</link><project id="" key="" /><description>It would be nice to have an option to disable the logging of the source field when indexing documents.

We are logging indexing requests that take a lot of time, but our documents are too large to be logged in full. Only metadata is interesting in this case (time taken and id).
</description><key id="24427246">4485</key><summary>Option to disable logging source in slowlog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">henrikno</reporter><labels><label>adoptme</label><label>low hanging fruit</label></labels><created>2013-12-17T15:19:48Z</created><updated>2015-08-11T20:57:01Z</updated><resolved>2015-08-11T20:57:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-03-27T10:27:05Z" id="86893268">sounds reasonable to me. Adding an adopt me and a low hanging fruit.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>core/src/main/java/org/elasticsearch/cluster/metadata/IndexMetaData.java</file><file>core/src/main/java/org/elasticsearch/common/Strings.java</file><file>core/src/main/java/org/elasticsearch/index/indexing/IndexingSlowLog.java</file><file>core/src/main/java/org/elasticsearch/index/settings/IndexDynamicSettingsModule.java</file><file>core/src/test/java/org/elasticsearch/common/StringsTests.java</file><file>core/src/test/java/org/elasticsearch/index/indexing/IndexingSlowLogTests.java</file></files><comments><comment>Logging: Log less source in slowlog</comment></comments></commit></commits></item><item><title>Don't accept type wrapper in index request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4484</link><project id="" key="" /><description>Currently it is possible to index a document as:

```
POST /myindex/mytype/1
{ "foo"...}
```

or as:

```
POST /myindex/mytype/1
{
    "mytype": {
        "foo"...
    }
}
```

This makes indexing non-deterministic and fields can be misinterpreted as type names.

We should accept only the first form, ie without the type wrapper.
</description><key id="24426662">4484</key><summary>Don't accept type wrapper in index request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-17T15:11:50Z</created><updated>2014-01-13T21:46:25Z</updated><resolved>2014-01-13T21:46:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-07T19:41:31Z" id="31771516">I have concerns about this change... . The reason why it was supported to begin with was that some serialization tools that convert objects to json objects tend to sometime also include a top level "type" in it... (not many, I admit...). 

I am also concerned about the backward comp. questions that will happen here..., I know we can break 1.0 backward comp., but this can be very annoying (think about users relaying on it, and now they will need to somehow strip the top level).

I agree that currently, its annoying, because if a json document has the first field name the same as the type, its very annoying since it breaks badly. Which I think this is where this change is coming from.

But, at the end, if we can support the above, if there is a single top level object, and thats the same as the type name, then we treat it as the type, and parse the fields within it. If its not (its a field, or there is another object at the same level), then treat it as a "regular" document. If we manage to do this, then I would say its better.
</comment><comment author="clintongormley" created="2014-01-09T17:14:11Z" id="31954578">I have never seen anybody gist an example where they index a document with the body wrapped with the type name.  I have however seen a few people have issues because of the ambiguity introduced by supporting optional type wrapping. Better heuristics may make the issue less frequent, but won't solve it completely.

I'd vote for pushing the change as it is or, alternatively, adding a setting which allows the user to continue using the old behaviour should they so choose, but disable it by default.
</comment><comment author="dakrone" created="2014-01-09T17:20:23Z" id="31955254">I agree with Clint also, I've never seen anyone index documents wrapped by the type name, and trying to determine whether the outer object is a type-name or object-name will be confusing to the end user since the decision is hidden inside internal ES logic. I think we should remove the wrapping option. What do you think @kimchy?
</comment><comment author="kimchy" created="2014-01-11T19:19:12Z" id="32104385">I am ok with putting this behind a setting (can be an index based setting even, @dakrone, the Builder gets indexSettings), and the setting can default to not allow it.
</comment><comment author="dakrone" created="2014-01-11T21:51:47Z" id="32108289">Okay, I'll add a setting for this so it can be enabled/disabled.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/simple/SimpleMapperTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/typelevels/ParseDocumentTypeLevelsTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Default to not accepting type wrapper in indexing requests</comment></comments></commit></commits></item><item><title>Don't repeat type name in update mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4483</link><project id="" key="" /><description>Currently the `update_mapping` API requires the type name to be repeated in the body, eg:

```
POST /my_index/my_type/_mapping
{
    "my_type": {
        "properties": {...}
    }
}
```

This repetition is confusing and unnecessary.  Instead should be OK to do:

```
 POST /my_index/my_type/_mapping
{
    "properties": {...}
}
```

Bonus points for making the change in a backwards compatible way :)
</description><key id="24426499">4483</key><summary>Don't repeat type name in update mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-17T15:09:21Z</created><updated>2014-05-08T10:59:17Z</updated><resolved>2014-01-13T16:34:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-11T19:25:05Z" id="32104529">first, I don't fully agree with this issue, I don't think putting the type as top level is repetitive. It actually maps into what we are going to return in the get mapping API.

second, if we are going to do it, we _have_ to make it backward compatible.
</comment><comment author="dakrone" created="2014-01-11T21:44:11Z" id="32108098">@kimchy this change is entirely backwards compatible. Either specifying the type (old method), or not specifying the type are supported.

See http://p.writequit.org/org/4483-dont-repeat-type.html for examples.
</comment><comment author="dakrone" created="2014-01-11T21:50:40Z" id="32108256">@kimchy I see your comment about the additional parameters at the top level, I'll make sure this is entirely backwards compatible with those settings, thanks for pointing those out!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/ObjectMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java</file><file>src/test/java/org/elasticsearch/index/mapper/all/SimpleAllMapperTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/routing/SimpleRoutingTests.java</file><file>src/test/java/org/elasticsearch/search/sort/SimpleSortTests.java</file></files><comments><comment>Check if root mapping is actually valid</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/mapper/DocumentMapperParser.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>Make type wrapping optional for PUT Mapping API request</comment></comments></commit></commits></item><item><title>Multiple parents support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4482</link><project id="" key="" /><description>Is is possible to support multiple parents? It would be the user's responsibility to make sure that they are routed in the same shard. Are there any impediments to such a feature (many-to-many relationships)?
</description><key id="24425843">4482</key><summary>Multiple parents support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">randunel</reporter><labels /><created>2013-12-17T14:59:56Z</created><updated>2014-12-26T19:17:07Z</updated><resolved>2014-12-24T17:38:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:38:30Z" id="68066089">Hi @randunel 

Sorry it has taken a while to look at this.  By multiple parents do you mean multiple generations (with one parent in each generation) or multiple parents in a single generation?  The first is already supported.  The second cannot be supported. 
</comment><comment author="randunel" created="2014-12-26T19:17:07Z" id="68154086">This particular issue refers to multiple parents in a single generation.
Thank you for taking a look at this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make index required in DELETE index API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4481</link><project id="" key="" /><description>Currently this request:

```
DELETE /
```

... will delete all indices.  We should change this to require an index name or wildcard, to prevent data loss from typos.

The delete-all syntax would be either of:

```
DELETE /_all
DELETE /*
```
</description><key id="24425452">4481</key><summary>Make index required in DELETE index API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-17T14:54:45Z</created><updated>2014-04-28T16:59:01Z</updated><resolved>2014-01-06T08:03:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-12-17T14:56:49Z" id="30757089">+1 for either syntax.  Doesn't the /_all syntax match the close api?  I'm going from memory here.
</comment><comment author="clintongormley" created="2013-12-17T14:59:09Z" id="30757284">It does, but you can also use `/*` wildcards for delete. We will be adding wildcards for `open`/`close` too.
</comment><comment author="kwloafman" created="2013-12-17T15:09:28Z" id="30758240">As one who has learned painfully from that typo, I +1 this idea.

On Tue, Dec 17, 2013 at 8:59 AM, Clinton Gormley
notifications@github.comwrote:

&gt; It does, but you can also use /\* wildcards for delete. We will be adding
&gt; wildcards for open/close too.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/4481#issuecomment-30757284
&gt; .
</comment><comment author="javanna" created="2014-01-06T08:03:46Z" id="31632734">This issue will be incorporated in #4549, which consists of disallowing all destructive operations by default. The idea is to keep the `DELETE /` syntax around as it makes sense REST-wise, but will need to be enabled through a configuration parameter, that will work cross-APIs for all the potentially desctructive operations.
</comment><comment author="javanna" created="2014-01-07T15:56:41Z" id="31749498">Update: the `DELETE /` won't be kept around in the end, but will be removed as it's harmful. Ignore my last comment.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/TransportDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/support/DestructiveOperations.java</file><file>src/main/java/org/elasticsearch/cluster/settings/ClusterDynamicSettingsModule.java</file><file>src/test/java/org/elasticsearch/indices/state/CloseIndexDisableCloseAllTests.java</file><file>src/test/java/org/elasticsearch/operateAllIndices/DestructiveOperationsIntegrationTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/RecoveryPercolatorTests.java</file><file>src/test/java/org/elasticsearch/percolator/TTLPercolatorTests.java</file><file>src/test/java/org/elasticsearch/search/suggest/CompletionSuggestSearchTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Added `action.destructive_requires_name` that controls whether wildcard expressions and `_all` is allowed to be used for destructive operat Also the delete index api requires always an index to be specified (either concrete index, alias or wildcard expression)</comment></comments></commit></commits></item><item><title>Make exists, found, not_found consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4480</link><project id="" key="" /><description>The GET and multi_get APIs return `{"exists": true|false}` while the DELETE and bulk delete APIs return `{"found": true|false}`, and the DeleteResponse class uses `notFound`.

All of these should be changed to use `found`, in order to make things more consistent.
</description><key id="24425267">4480</key><summary>Make exists, found, not_found consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-17T14:52:23Z</created><updated>2014-02-17T23:29:43Z</updated><resolved>2014-01-07T16:46:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-17T14:57:56Z" id="30757179">If we look at the discussion here: https://github.com/elasticsearch/elasticsearch/issues/4416#issuecomment-30749423
We could think of having a `status: 404` or `status: 200`?

WDYT?
</comment><comment author="kimchy" created="2014-01-02T18:10:23Z" id="31471520">I don't think we should do status here, the status is already returned in the response, and for the non error use case, its much nicer to be descriptive, yet in a consistent manner.
</comment><comment author="kimchy" created="2014-01-02T18:11:44Z" id="31471622">also, I would add that the index action should have a "created" flag in the rest response.
</comment><comment author="dakrone" created="2014-01-07T16:46:34Z" id="31754576">Merged in 2cb40fcb
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[DOCS] updated json responses after #4310 and #4480</comment></comments></commit></commits></item><item><title>Common interface for GetField and SearchHitField</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4479</link><project id="" key="" /><description>Hi, this is a Java client-related suggestion/issue: 
Today I was looking to combine some results retrieved from a GetResponse and a SearchResponse, but I found they don't share an interface, even though they basically serve the same purpose and have the same common methods. It makes perfect sense to let them have a common "GenericField" interface or something similar, containing at least:
String getName();
Object getValue();

Not having such forced to implement a solution that was far from elegant.

Don't you think it's a good idea? Thanks!
</description><key id="24423034">4479</key><summary>Common interface for GetField and SearchHitField</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phrone</reporter><labels><label>:Java API</label><label>enhancement</label><label>low hanging fruit</label><label>won't fix</label></labels><created>2013-12-17T14:17:33Z</created><updated>2016-12-23T10:12:34Z</updated><resolved>2016-12-23T10:10:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-03-27T10:29:07Z" id="86894086">It makes sense that Get and Search APIs should share common interfaces where possible for the Field/Doc level constructs returned in results. Any refactoring or introduction of interfaces should be done in a backwards compatible fashion.
</comment><comment author="clintongormley" created="2016-12-23T10:10:29Z" id="268967609">Given that three years have passed with no action, and we're moving away from the transport client to the rest client, I'm going to close this as won't fix</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve allocation of unassigned shards with early termination</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4478</link><project id="" key="" /><description>When we allocate unassigned shards we can terminate early for some
shards like if we already tried to allocate a replica we don't need
to try the same replica if the first one got rejected. We also
can check if certain nodes can't allocate any primaries or shrads
at all and take those nodes out of the picture for the current round
since it will not change in the current round.
</description><key id="24414858">4478</key><summary>Improve allocation of unassigned shards with early termination</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-12-17T11:34:24Z</created><updated>2014-07-16T21:50:21Z</updated><resolved>2013-12-17T12:56:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Stricter parsing of aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4477</link><project id="" key="" /><description>- Only one aggregation type is allowed per aggregation definition.
- Return an error in the parsers when an unknown field is encountered.

Close #4464
</description><key id="24408499">4477</key><summary>Stricter parsing of aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-17T09:26:03Z</created><updated>2014-07-12T05:53:11Z</updated><resolved>2013-12-20T09:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-18T13:44:46Z" id="30842218">+1 lgtm!
</comment><comment author="jpountz" created="2013-12-20T09:00:41Z" id="30996693">Thanks @martijnvg !
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support Read Consistency Level for Document fetches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4476</link><project id="" key="" /><description>Extending on Issue #444, support a consistency argument when running a GET request on a document. A write operation of "quorum" followed by a read operation of "quorum" would return the latest version of a document.
</description><key id="24403064">4476</key><summary>Support Read Consistency Level for Document fetches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2013-12-17T06:52:34Z</created><updated>2013-12-17T09:54:16Z</updated><resolved>2013-12-17T09:46:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-12-17T07:05:42Z" id="30730318">Can you explain a bit more what quorum in a read would look like?  A quorum agrees on the version (and checks for the max)?  Seems like just specifying either `consistency=all` on the write or `preference=_primary` on the read would provide the same guarantees.
</comment><comment author="kimchy" created="2013-12-17T09:43:02Z" id="30737565">This does not really apply to elasticsearch... . GET request following a WRITE request will always aim at returning the last WRITE, it does't require a quorum semantics on it.
</comment><comment author="Downchuck" created="2013-12-17T09:46:45Z" id="30737801">@kimchy Good to hear. I was poking around in the ES docs and on the mailing list, didn't see anything conclusive on that account. Does it use the document ID to identify the elected master for that shard? It'd be nice to know a little more (or see a link) about how the guarantee is made. Closing this bug, as it sounds like this already exists following a write.
</comment><comment author="kimchy" created="2013-12-17T09:50:48Z" id="30738049">Since replication is sync (by default), then a get following a write will always return the "last" document (at least, obviously another write might have happened from a different thread/process, thats why we have versions). Even with async replication, you can use the `preference` parameter to the get API and ask it to only go to the `_primary`.
</comment><comment author="Downchuck" created="2013-12-17T09:54:16Z" id="30738253">Thanks Shay, thoroughly covers it. For all who stumble upon this page: the 
[Preference API for GET](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-preference.html).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>field "tags" was indexed without position data; cannot run PhraseQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4475</link><project id="" key="" /><description>Yestoday I update ES to 0.90.7 and logstash to 1.3.1
use kibana filter with tag I get these error

[2013-12-17 11:42:29,128][DEBUG][action.search.type       ] [node-10.7.3.121] [logstash-2013.12.17][3], node[aLYmR50MR_W0cQdMGG8Uxw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@51fc89c6]
org.elasticsearch.transport.RemoteTransportException: [node-10.7.3.122][inet[/10.7.3.122:9300]][search/phase/query]
Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2013.12.17][3]: query[filtered(ConstantScore(_:_))-&gt;BooleanFilter(+_:_ +cache(@timestamp:[1387165326783 TO 1387251749031]) +cache(QueryWrapperFilter(+(tags:"syq yxg") +(tags:apache))) +BooleanFilter(+_:_))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@684f9f43&gt;!]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:623)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:612)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IllegalStateException: field "tags" was indexed without position data; cannot run PhraseQuery (term=syq)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:311)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:587)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:539)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:112)

how to fix it ?
</description><key id="24398248">4475</key><summary>field "tags" was indexed without position data; cannot run PhraseQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tinytub</reporter><labels><label>non-issue</label></labels><created>2013-12-17T03:43:43Z</created><updated>2014-01-14T11:01:34Z</updated><resolved>2014-01-14T11:01:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ydnitin" created="2013-12-17T04:37:30Z" id="30725696">I am stuck in the same situation. It seems to be applying default template which I don't want it to. I have rolled back to logstash-1.1.13. Dropped the latest index but it creates a new one with default template. 
I stick this index.mapper.dynamic: false in elasticsearch.yml and restarted elasticsearch it creates a new index with no default template but rest of my indexes becomes useless. 

Kibana throws this error 
"Oops! IllegalStateException[field "type" was indexed without position data"

I can't seem to find out where it's picking up the default template from?
</comment><comment author="N-Mi" created="2014-01-14T10:50:21Z" id="32254893">I have the same issue (ES 0.90.7 and last logstash build 1.4.0-dev).

On syslog messages, Kibana thows this error on field "host" and "facility", but not "program" or some other custom fields I tested.

Copy paste of error message in elasticsearch.log :

```
[2014-01-14 10:46:51,240][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][1], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3fe72c9c]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][1]: query[ConstantScore(*:*)],from[0],size[0]: Query Failed [Failed to execute global facets]
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:188)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:131)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toSafeBits(DocIdSets.java:130)
    at org.elasticsearch.common.lucene.search.FilteredCollector.setNextReader(FilteredCollector.java:69)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:186)
    ... 10 more
[2014-01-14 10:46:51,249][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][0], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3fe72c9c]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][0]: query[ConstantScore(*:*)],from[0],size[0]: Query Failed [Failed to execute global facets]
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:188)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:131)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toSafeBits(DocIdSets.java:130)
    at org.elasticsearch.common.lucene.search.FilteredCollector.setNextReader(FilteredCollector.java:69)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:186)
    ... 10 more
[2014-01-14 10:46:51,282][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][2], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@502fcd07]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][2]: query[filtered(ConstantScore(*:*))-&gt;BooleanFilter(+cache(QueryWrapperFilter((tags:backbone))) +cache(@timestamp:[1389692812416 TO 1389696411250]) +cache(QueryWrapperFilter((facility:"syslog 6 info"))) -cache(QueryWrapperFilter((tags:noise))))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@519fc4c6&gt;!]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:587)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:539)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:112)
    ... 9 more
[2014-01-14 10:46:51,286][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][1], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@502fcd07]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][1]: query[filtered(ConstantScore(*:*))-&gt;BooleanFilter(+cache(QueryWrapperFilter((tags:backbone))) +cache(@timestamp:[1389692812416 TO 1389696411250]) +cache(QueryWrapperFilter((facility:"syslog 6 info"))) -cache(QueryWrapperFilter((tags:noise))))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@723dafb1&gt;!]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:587)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:539)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:112)
    ... 9 more
[2014-01-14 10:46:51,289][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][3], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3fe72c9c]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][3]: query[ConstantScore(*:*)],from[0],size[0]: Query Failed [Failed to execute global facets]
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:188)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:131)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toSafeBits(DocIdSets.java:130)
    at org.elasticsearch.common.lucene.search.FilteredCollector.setNextReader(FilteredCollector.java:69)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:186)
    ... 10 more
[2014-01-14 10:46:51,290][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][3], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@502fcd07]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][3]: query[filtered(ConstantScore(*:*))-&gt;BooleanFilter(+cache(QueryWrapperFilter((tags:backbone))) +cache(@timestamp:[1389692812416 TO 1389696411250]) +cache(QueryWrapperFilter((facility:"syslog 6 info"))) -cache(QueryWrapperFilter((tags:noise))))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@2fd9c80f&gt;!]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:587)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:539)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:112)
    ... 9 more
[2014-01-14 10:46:51,292][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][4], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@502fcd07]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][4]: query[filtered(ConstantScore(*:*))-&gt;BooleanFilter(+cache(QueryWrapperFilter((tags:backbone))) +cache(@timestamp:[1389692812416 TO 1389696411250]) +cache(QueryWrapperFilter((facility:"syslog 6 info"))) -cache(QueryWrapperFilter((tags:noise))))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@2dd6954&gt;!]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:587)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:539)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:112)
    ... 9 more
[2014-01-14 10:46:51,293][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][4], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3fe72c9c]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][4]: query[ConstantScore(*:*)],from[0],size[0]: Query Failed [Failed to execute global facets]
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:188)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:131)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toSafeBits(DocIdSets.java:130)
    at org.elasticsearch.common.lucene.search.FilteredCollector.setNextReader(FilteredCollector.java:69)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:186)
    ... 10 more
[2014-01-14 10:46:51,295][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][2], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3fe72c9c]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][2]: query[ConstantScore(*:*)],from[0],size[0]: Query Failed [Failed to execute global facets]
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:188)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:131)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toSafeBits(DocIdSets.java:130)
    at org.elasticsearch.common.lucene.search.FilteredCollector.setNextReader(FilteredCollector.java:69)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:186)
    ... 10 more
[2014-01-14 10:46:51,296][DEBUG][action.search.type       ] [Timberius] All shards failed for phase: [query]
[2014-01-14 10:46:51,294][DEBUG][action.search.type       ] [Timberius] [logstash-2014.01.14][0], node[NwuY3JOmTJ2ifkmfW29DHA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@502fcd07]
org.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.01.14][0]: query[filtered(ConstantScore(*:*))-&gt;BooleanFilter(+cache(QueryWrapperFilter((tags:backbone))) +cache(@timestamp:[1389692812416 TO 1389696411250]) +cache(QueryWrapperFilter((facility:"syslog 6 info"))) -cache(QueryWrapperFilter((tags:noise))))],from[0],size[500],sort[&lt;custom:"@timestamp": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@162e0b44&gt;!]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:229)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.IllegalStateException: field "facility" was indexed without position data; cannot run PhraseQuery (term=syslog)
    at org.apache.lucene.search.PhraseQuery$PhraseWeight.scorer(PhraseQuery.java:273)
    at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)
    at org.elasticsearch.common.lucene.docset.DocIdSets.toCacheable(DocIdSets.java:79)
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:171)
    at org.elasticsearch.common.lucene.search.XBooleanFilter.getDocIdSet(XBooleanFilter.java:83)
    at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:131)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:587)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:539)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:112)
    ... 9 more
[2014-01-14 10:46:51,298][DEBUG][action.search.type       ] [Timberius] All shards failed for phase: [query]
```
</comment><comment author="s1monw" created="2014-01-14T11:01:28Z" id="32255625">I guess that is due to a set `index_option` on those fields. If they are `not_analyzed` the `index_option` defaults to `docs` and not to `positions` which means you can not execute phrase queries on them. The only way to resolve this is to reindex with `index_option` set to `positions`  (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html) but I doubt that this is useful on a field that is not_analyzed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>XSS vulnerability detected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4474</link><project id="" key="" /><description>Hi all. My IT department has detected a cross site scripting vulnerability related to ES - specifically that when the string `"/&lt;script&gt;cross_site_scripting.nasl&lt;/script&gt;.asp"` gets sent to ES, it doesn't get properly escaped. I was surprised that I hadn't seen it come up before this when I searched. Is this something reasonably fixable?
The report is attached, please let me know what other info I can provide. This is present in 0.90.3-0.90.7, at least. I have not tried 1.x.

```
First Discovered: Dec 12, 2013 20:18
Last Observed:  Dec 16, 2013 8:52

DNS Name:  
NetBIOS Name:  

Synopsis: The remote web server is prone to cross-site scripting attacks.

Description
The remote host is running a web server that fails to adequately sanitize request strings of malicious JavaScript. By leveraging this issue, an attacker may be able to cause arbitrary HTML and script code to be executed in a user's browser within the security context of the affected site.

Solution
Contact the vendor for a patch or upgrade.

See Also
http://en.wikipedia.org/wiki/Cross-site_scripting

Risk Factor: Medium

CVSS Base Score
4.3 (CVSS2#AV:N/AC:M/Au:N/C:N/I:P/A:N)

CVSS Temporal Score
3.6 (CVSS2#E:F/RL:OF/RC:C)

Plugin Output
The request string used to detect this flaw was : 

/&lt;script&gt;cross_site_scripting.nasl&lt;/script&gt;.asp 

The output was : 

HTTP/1.1 400 Bad Request 
Access-Control-Allow-Origin: * 
Content-Type: text/plain; charset=UTF-8 
Content-Length: 91 


No handler found for uri [/&lt;script&gt;cross_site_scripting.nasl&lt;/script&gt;.as 
p] and method [GET]

CVE
CVE-2002-1700
CVE-2003-1543
CVE-2005-2453
CVE-2006-1681
CVE-2012-3382

BID
5011
5305
7344
7353
8037
14473
17408
54344

Cross-References
OSVDB:4989
OSVDB:18525
OSVDB:24469
OSVDB:42314
OSVDB:58976
OSVDB:83683
CWE:79
CWE:80
CWE:81
CWE:83
CWE:20
CWE:74
CWE:442
CWE:712
CWE:722
CWE:725
CWE:811
CWE:751
CWE:801
CWE:116

Vulnerability Publication Date: 2004/04/09

Plugin Publication Date: 2001/11/30

Plugin Last Modification Date: 2013/09/04

Public Exploit Available: True
```
</description><key id="24393703">4474</key><summary>XSS vulnerability detected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">hijakk</reporter><labels><label>non-issue</label></labels><created>2013-12-17T01:19:08Z</created><updated>2014-01-05T09:43:18Z</updated><resolved>2014-01-05T09:43:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-17T09:57:01Z" id="30738440">hmm I guess we need to HTML escape the string that comes in though. Thanks for raising this.
</comment><comment author="s1monw" created="2013-12-17T10:03:06Z" id="30738796">Actually I guess it would be easier to just not return `No handler found` without the value provided. The handler is pretty much given by the URL. @clintongormley  what do you think is this important?
</comment><comment author="clintongormley" created="2013-12-17T10:23:58Z" id="30739981">Given that these error messages are return as mime-type `text/plain` this should not be a vulnerability.  The only time it would be is with browsers which incorrectly treat this output as html, in which case the vulnerability would be for them to fix.
</comment><comment author="s1monw" created="2013-12-17T10:35:32Z" id="30740650">oh I missed that, I agree thanks @clintongormley 
</comment><comment author="Downchuck" created="2014-01-05T02:36:41Z" id="31594662">No browsers are going to render as html when the mime-type is text/plain. Additionally, browsers escape these characters when issuing HTTP requests (chrome does anyway). Issue can be closed :-)
</comment><comment author="s1monw" created="2014-01-05T09:43:18Z" id="31600181">close as no-issue
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>ES with Multi-Cluster Search Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4473</link><project id="" key="" /><description>## ES with Multi-Cluster Search Support

It'd be nice to be able to query across multiple clusters and get their aggregated results.

Our initial motivation is to view Kibana results across multiple clusters. See: [Enhancement: Allow conntections to multiple ES backends from a single Kibana instance](https://github.com/elasticsearch/kibana/issues/22).

However, since we also use/query ES directly, a proxy would work far better than changing Kibana.

In the discussion above, it was decided (by Shay) that the place to do it would be at the ES level.

The following is a proposal to achieve it at the ES level. This is some very early planning and I've decided to post it early to make sure I'm not duplicating work or am on a totally wrong path (which is totally possible). Any feedback/comments are appreciated.
## Proposal

The general plan is to make a query only node (termed 'search load balancer' in `elasticsearch.yml`) and have a list of cluster names that we want to query.

During a search, the node will query from each of the shards in each of the clusters and aggregate the results.
## Details
- Make a query only node
  - In `elasticsearch.yml`
    - `node.master: false`
    - `node.data:   false`
  - I'm hoping this means that we can isolate code changes to only the search portion.
- Accommodate multiple clusters
  - Have `ZenDiscover` reach out to all the listed clusters to get their state.
  - `ClusterService` will contain a map of `ClusterName -&gt; ClusterState`.
    - To maintain the interface, `clusterService.state()` will just return the first cluster.
    - We can have another interface that will allow us to get the the cluster map.
  - `MulticastZenPing` will have to listen for changes in the listed clusters and update the corresponding cluster state.
- Searching across multiple clusters
  - I've only looked at `TransportSearchTypeAction` and `TransportSearchQueryThenFetchAction` so far.
  - In the `BaseAsyncAction`, we'll need to get all the relevant shards across each cluster and query them (`sendExecuteFirstPhase`).
  - We change `expectedSuccessfulOps` and `expectedTotalOps` to the multi-cluster counts so that in `onFirstPhaseResult` we know when to move on (`innerMoveToSecondPhase`).
  - In `moveToSecondPhase`, we again use the metadata from each cluster and do the actual fetch of the documents.
  - Finally in `innerFinishHim`, we merge all the results with the `SearchPhaseController` and return the response via the normally.

Thanks!
</description><key id="24389521">4473</key><summary>ES with Multi-Cluster Search Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gseng</reporter><labels /><created>2013-12-16T23:40:19Z</created><updated>2014-01-14T00:02:54Z</updated><resolved>2014-01-13T23:37:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2014-01-13T19:28:16Z" id="32202043">Just noticed an interesting commit that addresses this issue: https://github.com/elasticsearch/elasticsearch/issues/4708

Definitely a more thought out approach since it merges the cluster states, but requires a new "tribe" node. I wonder how it will works for single cluster actions that do not require a merged cluster state.
</comment><comment author="gseng" created="2014-01-13T23:37:02Z" id="32224402">Thanks brusic, that sounds like exactly what we want, and as you said, more elegant. Am working on seeing if it works out for us (comments in #4708 ).
</comment><comment author="brusic" created="2014-01-14T00:02:54Z" id="32226101">Glad to see that my comment was useful and that you are already testing out the feature. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Support ordering by sub-aggregations for the terms-aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4472</link><project id="" key="" /><description>The `terms`-aggregator does not currently seem to fully support its documented orderings: it has troubles with ordering by sub-aggregators.

Here's an example that demonstrates the problem:

```
export ELASTICSEARCH_ENDPOINT=http://localhost:9200
ELASTICSEARCH_ENDPOINT=http://localhost:9200
curl -XPUT http://localhost:9200/foo -d '{
    "settings": {"index": {"number_of_shards": 5}},
    "mappings": {
        "type": {
            "properties": {
                "host": {
                    "type": "string",
                    "index": "not_analyzed"
                }
            }
        }
    }
}'
{"ok":true,"acknowledged":true}+ curl -XPOST 'http://localhost:9200/_bulk?refresh=true' -d '
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app1","cpu":95.1,"timestamp":"2013-12-24T12:00:00Z"}
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app1","cpu":75.1,"timestamp":"2013-12-24T12:01:00Z"}
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app2","cpu":25.1,"timestamp":"2013-12-24T12:00:00Z"}
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app2","cpu":55.1,"timestamp":"2013-12-24T12:01:00Z"}
'
{"took":364,"errors":false,"items":[{"create":{"_index":"foo","_type":"bar","_id":"dic5iqJRX2KimXqVq0tJg","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"Gs1YrXtiSy-GE16bCSblng","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"tfdbxfteSCO_ByUFiNYL6g","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"kWRoKKqlSdOGWaZzEstziw","_version":1,"ok":true,"status":201}}]}
curl -XPOST 'http://localhost:9200/foo/_search?pretty' -d '
{
    "size": 0,
    "aggregations": {
        "host": {
            "terms": {
                "field": "host",
                "order": {
                    "cpu_avg": "asc"
                }
            },
            "aggs": {
                "cpu_avg": {
                    "avg": {
                        "field": "cpu"
                    }
                }
            }
        }
    }
}
'
{
  "took" : 164,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 4,
    "failed" : 1,
    "failures" : [ {
      "index" : "foo",
      "shard" : 0,
      "status" : 500,
      "reason" : "RemoteTransportException[[Piper][inet[/10.0.1.14:9301]][search/phase/query]]; nested: NullPointerException; "
    } ]
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "host" : {
      "buckets" : [ {
        "key" : "app2",
        "doc_count" : 1,
        "cpu_avg" : {
          "value" : 25.1
        }
      } ]
    }
  }
}
+ curl -XPOST 'http://localhost:9200/foo/_search?pretty' -d '
{
    "size": 0,
    "aggregations": {
        "host": {
            "terms": {
                "field": "host",
                "order": {
                    "cpu.avg": "desc"
                }
            },
            "aggs": {
                "cpu": {
                    "stats": {
                        "field": "cpu"
                    }
                }
            }
        }
    }
}
'
{
  "took" : 12,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 4,
    "failed" : 1,
    "failures" : [ {
      "index" : "foo",
      "shard" : 0,
      "status" : 500,
      "reason" : "NullPointerException[null]"
    } ]
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "host" : {
      "buckets" : [ {
        "key" : "app2",
        "doc_count" : 1,
        "cpu" : {
          "count" : 1,
          "min" : 25.1,
          "max" : 25.1,
          "avg" : 25.1,
          "sum" : 25.1
        }
      } ]
    }
  }
}
```

With the provided changes, I get the expected result:

```
export ELASTICSEARCH_ENDPOINT=http://localhost:9200
ELASTICSEARCH_ENDPOINT=http://localhost:9200
curl -XPUT http://localhost:9200/foo -d '{
    "settings": {"index": {"number_of_shards": 5}},
    "mappings": {
        "type": {
            "properties": {
                "host": {
                    "type": "string",
                    "index": "not_analyzed"
                }
            }
        }
    }
}'
{"ok":true,"acknowledged":true}+ curl -XPOST 'http://localhost:9200/_bulk?refresh=true' -d '
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app1","cpu":95.1,"timestamp":"2013-12-24T12:00:00Z"}
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app1","cpu":75.1,"timestamp":"2013-12-24T12:01:00Z"}
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app2","cpu":25.1,"timestamp":"2013-12-24T12:00:00Z"}
{"index":{"_index":"foo","_type":"bar"}}
{"host":"app2","cpu":55.1,"timestamp":"2013-12-24T12:01:00Z"}
'
{"took":238,"errors":false,"items":[{"create":{"_index":"foo","_type":"bar","_id":"MGBw21a_RReC2_7Aos8g2g","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"40JZSDw0SDevcm4ERm5ReA","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"OZ4h_SQxRpeCWbxdta1dSA","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"Bvrqvfs5T9ak-GEshuIqwQ","_version":1,"ok":true,"status":201}}]}+ curl -XPOST 'http://localhost:9200/foo/_search?pretty' -d '
{
    "size": 0,
    "aggregations": {
        "host": {
            "terms": {
                "field": "host",
                "order": {
                    "cpu_avg": "asc"
                }
            },
            "aggs": {
                "cpu_avg": {
                    "avg": {
                        "field": "cpu"
                    }
                }
            }
        }
    }
}
'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "host" : {
      "buckets" : [ {
        "key" : "app2",
        "doc_count" : 2,
        "cpu_avg" : {
          "value" : 40.1
        }
      }, {
        "key" : "app1",
        "doc_count" : 2,
        "cpu_avg" : {
          "value" : 85.1
        }
      } ]
    }
  }
}
curl -XPOST 'http://localhost:9200/foo/_search?pretty' -d '
{
    "size": 0,
    "aggregations": {
        "host": {
            "terms": {
                "field": "host",
                "order": {
                    "cpu.avg": "desc"
                }
            },
            "aggs": {
                "cpu": {
                    "stats": {
                        "field": "cpu"
                    }
                }
            }
        }
    }
}
'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "host" : {
      "buckets" : [ {
        "key" : "app1",
        "doc_count" : 2,
        "cpu" : {
          "count" : 2,
          "min" : 75.1,
          "max" : 95.1,
          "avg" : 85.1,
          "sum" : 170.2
        }
      }, {
        "key" : "app2",
        "doc_count" : 2,
        "cpu" : {
          "count" : 2,
          "min" : 25.1,
          "max" : 55.1,
          "avg" : 40.1,
          "sum" : 80.2
        }
      } ]
    }
  }
}
```

I'm not nearly familiar with the code to claim that these changes are sufficient, but it's a start at least. :)
</description><key id="24388865">4472</key><summary>Support ordering by sub-aggregations for the terms-aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">alexbrasetvik</reporter><labels /><created>2013-12-16T23:26:40Z</created><updated>2014-06-14T16:25:20Z</updated><resolved>2014-01-15T17:41:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexbrasetvik" created="2013-12-18T14:29:35Z" id="30845366">@jpountz @uboness A lot of PRs these days, so just mentioning you in this one, to prevent possible duplication of effort. No rush. :)
</comment><comment author="uboness" created="2013-12-27T02:51:06Z" id="31244729">@alexbrasetvik ,

Do you mind if I take over this? I'd like to implement an optimization to avoid creating too many objects for nothing. I'll work on it, and incorporate all the changes you've made so far.

Cheers,
Uri
</comment><comment author="alexbrasetvik" created="2013-12-27T09:48:56Z" id="31254368">Great! Go ahead :)
</comment><comment author="alexbrasetvik" created="2014-01-15T17:41:49Z" id="32389331">Just took your latest fix for a spin. Seems to work well! Thanks a lot.

Closing this one, fixed by #4643 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fixing up code block delimeters for asciidoctor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4471</link><project id="" key="" /><description>You can now successfully run the docs through asciidoctor
</description><key id="24387263">4471</key><summary>Fixing up code block delimeters for asciidoctor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LightGuard</reporter><labels /><created>2013-12-16T23:03:03Z</created><updated>2014-07-16T21:50:24Z</updated><resolved>2014-01-13T14:27:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-01-13T14:27:31Z" id="32173203">Merged  - many thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Include Line Number(s) with Highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4470</link><project id="" key="" /><description>I'm interesting in having access to the line number(s) of a highlight.
</description><key id="24383339">4470</key><summary>Include Line Number(s) with Highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mateu</reporter><labels /><created>2013-12-16T22:11:51Z</created><updated>2014-12-24T17:35:43Z</updated><resolved>2014-12-24T17:35:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:35:43Z" id="68065961">Hi @mateu 

Sorry it has taken a while to look at this. Unfortunately, Lucene has no concept of line numbers, nor paragraphs etc.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add tests for REST layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4469</link><project id="" key="" /><description>The idea is to test the REST layer by making use of yaml test suites that can be found on the [elasticsearch-rest-api-spec project](https://github.com/elasticsearch/elasticsearch-rest-api-spec), which are already used to test all the official elasticsearch clients.
</description><key id="24378668">4469</key><summary>Add tests for REST layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T21:04:35Z</created><updated>2013-12-24T11:24:48Z</updated><resolved>2013-12-17T14:37:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/com/carrotsearch/randomizedtesting/StandaloneRandomizedContext.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/TestCluster.java</file><file>src/test/java/org/elasticsearch/test/junit/listeners/ReproduceInfoPrinter.java</file><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java</file><file>src/test/java/org/elasticsearch/test/rest/RestTestExecutionContext.java</file><file>src/test/java/org/elasticsearch/test/rest/client/RestClient.java</file><file>src/test/java/org/elasticsearch/test/rest/client/RestException.java</file><file>src/test/java/org/elasticsearch/test/rest/client/RestResponse.java</file><file>src/test/java/org/elasticsearch/test/rest/client/http/HttpDeleteWithEntity.java</file><file>src/test/java/org/elasticsearch/test/rest/client/http/HttpGetWithEntity.java</file><file>src/test/java/org/elasticsearch/test/rest/client/http/HttpRequestBuilder.java</file><file>src/test/java/org/elasticsearch/test/rest/client/http/HttpResponse.java</file><file>src/test/java/org/elasticsearch/test/rest/json/JsonPath.java</file><file>src/test/java/org/elasticsearch/test/rest/junit/DescriptionHelper.java</file><file>src/test/java/org/elasticsearch/test/rest/junit/RestReproduceInfoPrinter.java</file><file>src/test/java/org/elasticsearch/test/rest/junit/RestTestCandidate.java</file><file>src/test/java/org/elasticsearch/test/rest/junit/RestTestSuiteRunner.java</file><file>src/test/java/org/elasticsearch/test/rest/junit/RunAfter.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/DoSectionParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/GreaterThanParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/IsFalseParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/IsTrueParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/LengthParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/LessThanParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/MatchParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/RestTestFragmentParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/RestTestParseException.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/RestTestSectionParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParseContext.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/RestTestSuiteParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/SetSectionParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/SetupSectionParser.java</file><file>src/test/java/org/elasticsearch/test/rest/parser/SkipSectionParser.java</file><file>src/test/java/org/elasticsearch/test/rest/section/ApiCallSection.java</file><file>src/test/java/org/elasticsearch/test/rest/section/Assertion.java</file><file>src/test/java/org/elasticsearch/test/rest/section/DoSection.java</file><file>src/test/java/org/elasticsearch/test/rest/section/ExecutableSection.java</file><file>src/test/java/org/elasticsearch/test/rest/section/GreaterThanAssertion.java</file><file>src/test/java/org/elasticsearch/test/rest/section/IsFalseAssertion.java</file><file>src/test/java/org/elasticsearch/test/rest/section/IsTrueAssertion.java</file><file>src/test/java/org/elasticsearch/test/rest/section/LengthAssertion.java</file><file>src/test/java/org/elasticsearch/test/rest/section/LessThanAssertion.java</file><file>src/test/java/org/elasticsearch/test/rest/section/MatchAssertion.java</file><file>src/test/java/org/elasticsearch/test/rest/section/RestTestSuite.java</file><file>src/test/java/org/elasticsearch/test/rest/section/SetSection.java</file><file>src/test/java/org/elasticsearch/test/rest/section/SetupSection.java</file><file>src/test/java/org/elasticsearch/test/rest/section/SkipSection.java</file><file>src/test/java/org/elasticsearch/test/rest/section/TestSection.java</file><file>src/test/java/org/elasticsearch/test/rest/spec/RestApi.java</file><file>src/test/java/org/elasticsearch/test/rest/spec/RestApiParser.java</file><file>src/test/java/org/elasticsearch/test/rest/spec/RestSpec.java</file><file>src/test/java/org/elasticsearch/test/rest/support/FileUtils.java</file><file>src/test/java/org/elasticsearch/test/rest/support/VersionUtils.java</file><file>src/test/java/org/elasticsearch/test/rest/test/AbstractParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/AssertionParsersTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/DoSectionParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/JsonPathTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/RestApiParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/RestTestParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/SetSectionParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/SetupSectionParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/SkipSectionParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/TestSectionParserTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/VersionUtilsTests.java</file></files><comments><comment>added REST test suites runner</comment></comments></commit></commits></item><item><title>Move index health calculations to ClusterIndexHealth so it can be reused</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4468</link><project id="" key="" /><description /><key id="24378610">4468</key><summary>Move index health calculations to ClusterIndexHealth so it can be reused</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-12-16T21:03:48Z</created><updated>2014-07-16T21:50:25Z</updated><resolved>2013-12-17T10:32:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-16T22:00:10Z" id="30705353">this looks good to me - I think it makes sense to reuse this.
</comment><comment author="bleskes" created="2013-12-17T10:32:59Z" id="30740473">pushed: 9fb361cea166729038ad92945266e93a014dbf66 &amp; e79fc878fe1d52218eace09c0443af4037255a55

Thx simon
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>[feature] date format for minimum / maximum aggs in beta2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4467</link><project id="" key="" /><description>The query below with an aggregate on any date field will not respect the "format" property that date histogram supports. 

curl -XGET http://localhost:9200/vitals/vital/_search?pretty=true -d '
{
    "size": 0,
    "query": {
                "match_all": {}
    },
    "aggs": {
        "minimum_vitals_datetime" : {
            "min" : {
                "field" : "recorded_time",
                "format": "date_time"
            }
        }
    }
}'

result is:

{
  "took" : 2342,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 67733027,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "minimum_vitals_datetime" : {
      "value" : 1.38685008E12
    }
  }
}

without a formatted version. 
</description><key id="24364738">4467</key><summary>[feature] date format for minimum / maximum aggs in beta2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikeasick</reporter><labels /><created>2013-12-16T17:25:07Z</created><updated>2014-12-24T17:33:39Z</updated><resolved>2014-12-24T17:33:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kjelle" created="2014-05-27T09:18:49Z" id="44252449">Bump. My problem is that max aggregation on a timestamp returns a long instead of e.g. a timestamp :) (e.g. "value" : 1.38685008E12)
</comment><comment author="clintongormley" created="2014-12-24T17:33:38Z" id="68065841">Closing in favour of #9032
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Made parsing of ByteSizeValue case independent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4466</link><project id="" key="" /><description>This allows to parse '12GB' as well as '12gb'. This has already been done with a few units, but not with all of them, which can be confusing for the user.

Closes #4442
</description><key id="24349637">4466</key><summary>Made parsing of ByteSizeValue case independent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-16T13:40:57Z</created><updated>2014-07-16T21:50:25Z</updated><resolved>2014-01-02T12:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-12-18T08:23:53Z" id="30824154">+1 LGTM.
</comment><comment author="martijnvg" created="2013-12-18T09:09:58Z" id="30826477">+1 lgtm
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Make function_score geo distance implementation configurable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4465</link><project id="" key="" /><description>`function_score` is hard-coded to use `ARC` distance computations. It should be possible to use `PLANE` distance computations as well.
</description><key id="24349114">4465</key><summary>Make function_score geo distance implementation configurable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Geo</label><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-12-16T13:30:16Z</created><updated>2016-11-06T11:00:24Z</updated><resolved>2016-11-06T11:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-10T19:45:30Z" id="62442389">And it should probably default to sloppy_arc
</comment><comment author="clintongormley" created="2016-11-06T11:00:24Z" id="258673623">Closing in favour of https://github.com/elastic/elasticsearch/issues/15616
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Stricter parsing for aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4464</link><project id="" key="" /><description>An `aggregations` object in the JSON document representing the query can only wrap a single aggregation type. The parser logic should be stricter and fail the query if several of them are present instead of only taking one of them into account.
</description><key id="24346058">4464</key><summary>Stricter parsing for aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T12:25:26Z</created><updated>2013-12-20T09:00:22Z</updated><resolved>2013-12-20T09:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-16T12:33:03Z" id="30657077">++, also, I would love the parsing from json (xcontent) to be stricter when it comes to unknown fields in the body of each agg (similar to what we do in queries today)
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/aggregations/AggregatorParsers.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/HistogramParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/missing/MissingParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/nested/NestedParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/date/DateRangeParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/geodistance/GeoDistanceParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/range/ipv4/IpRangeParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/ValuesSourceMetricsAggregatorParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/metrics/valuecount/ValueCountParser.java</file><file>src/test/java/org/elasticsearch/search/aggregations/ParsingTests.java</file></files><comments><comment>Stricter parsing of aggregations.</comment></comments></commit></commits></item><item><title>Added Cluster Stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4463</link><project id="" key="" /><description>Closes #4460
</description><key id="24344563">4463</key><summary>Added Cluster Stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-12-16T11:50:40Z</created><updated>2014-06-26T14:48:18Z</updated><resolved>2013-12-17T12:18:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-16T17:23:34Z" id="30680316">+1 looks good to me. One minor thing: I saw that both jcf and hppc is used, maybe we should use hppc on all places?
</comment><comment author="bleskes" created="2013-12-17T12:18:16Z" id="30746320">Committed: https://github.com/elasticsearch/elasticsearch/commit/63d69cffaa6a3f9e79d5d7342c2a144d7d266a87 &amp; https://github.com/elasticsearch/elasticsearch/commit/2b6214cff72423fc5c948c7b4c39027c93494400
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow GetAliasRequest to retrieve all aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4462</link><project id="" key="" /><description>Results in less data being sent over the wire, as the Cat API does not
need to have the whole cluster state.

Also added matchers for hasKey() for immutable open map (I think we should
add more of those to have map style assertions).

Closes #4455
</description><key id="24344197">4462</key><summary>Allow GetAliasRequest to retrieve all aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-16T11:41:45Z</created><updated>2014-06-25T21:30:43Z</updated><resolved>2014-01-02T11:08:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-16T11:44:28Z" id="30654552">+1 lgtm!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Renamed top level `filter` to `post_filter`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4461</link><project id="" key="" /><description /><key id="24342885">4461</key><summary>Renamed top level `filter` to `post_filter`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-12-16T11:12:37Z</created><updated>2015-05-18T23:33:25Z</updated><resolved>2013-12-16T16:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-16T11:14:43Z" id="30652987">LGTM, I suggest we also backport it? it doesn't break backward comp., and will allow users to move to use it in 0.90
</comment><comment author="martijnvg" created="2013-12-16T11:15:26Z" id="30653025">Makes sense, I'll backport it to 0.90 as well.
</comment><comment author="martijnvg" created="2013-12-16T16:30:57Z" id="30675141">pushed
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add `_cluster/stats/` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4460</link><project id="" key="" /><description>Currently you can get cluster wide information using different endpoints for different means. The node info API will return static information with respects to nodes (like jvm versions). The node stats API will return dynamic information (like memory and cpu usage) and the index stats API will return aggregated (and per index) information regarding the cluster data.

With the new Cluster Stats API, we will have a single endpoint that returns all the information needed to get an overview of the cluster. The output will contain basic index metrics (shard numbers, store size, memory usage) and information about the current nodes that form the cluster (number, roles,os, jvm versions, memory usage, cpu and installed plugins).
</description><key id="24342695">4460</key><summary>Add `_cluster/stats/` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>feature</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T11:08:27Z</created><updated>2013-12-17T12:15:07Z</updated><resolved>2013-12-17T12:15:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java</file></files><comments><comment>Added cluster health status to the Cluster Stats API</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/action/ActionModule.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/PluginInfo.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsIndices.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodeResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsNodes.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/TransportClusterStatsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStatsFlags.java</file><file>src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/Requests.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/monitor/fs/FsStats.java</file><file>src/main/java/org/elasticsearch/monitor/os/OsInfo.java</file><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/stats/RestClusterStatsAction.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java</file></files><comments><comment>Added Cluster Stats API</comment></comments></commit></commits></item><item><title>Shouldn't be necessary to loop over ShardRoutings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4459</link><project id="" key="" /><description>As several other classes can change the internal state of the RoutingNodes data structure, inefficient looping over nodes and assigned shards was necessary in the AllocationDeciders.

With larger clusters, reallocation gets too slow. In our current case, we have 5 years of data with daily indices, 6 shards per index, replication factor 1. Recalculating cluster state can take minutes, with the master sitting at 100% CPU in RoutingNodes.shardsRoutingFor( MutableShardRouting ).

The taken approach is
a) making RoutingNodes a singleton, since only one active instance should ever exist anyhow,
b) notifying RoutingNodes of changes in MutableShardRouting instances state.

This certainly is not the most elegant approach and adds complexity instead of removing it, but is what can be done without a major refactoring of allocation.

In the supplied test case execution of the final reallocation is sped up from 22 seconds on my test machine to 4.2 seconds.

There is already a PR for this: https://github.com/elasticsearch/elasticsearch/pull/4257
</description><key id="24342098">4459</key><summary>Shouldn't be necessary to loop over ShardRoutings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>resiliency</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T10:54:17Z</created><updated>2014-07-05T11:29:03Z</updated><resolved>2013-12-16T11:12:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-16T11:12:40Z" id="30652889">pushed 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Improve RoutingNodes API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4458</link><project id="" key="" /><description>Currently the `RoutingNodes` API allows modification of it's internal state outside of the class. We already  have a pullrequest (https://github.com/elasticsearch/elasticsearch/pull/4257) that adds 10x performance improvements for shard allocation but it handles most of the state changes manually outside of `RoutingNodes`. We should make sure that the `RoutingNodes` is consistent  at all time and encapsulate it's internal datastructures.
</description><key id="24341606">4458</key><summary>Improve RoutingNodes API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>resiliency</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T10:45:02Z</created><updated>2014-07-05T11:28:16Z</updated><resolved>2013-12-16T11:11:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-16T10:53:34Z" id="30651814">looks great!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingNode.java</file><file>src/main/java/org/elasticsearch/cluster/routing/RoutingNodes.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/EvenShardsCountAllocator.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/ShardsAllocators.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/AllocateAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/MoveAllocationCommand.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/RebalanceOnlyWhenActiveAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ReplicaAfterPrimaryActiveAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file><file>src/main/java/org/elasticsearch/gateway/blobstore/BlobReuseExistingGatewayAllocator.java</file><file>src/main/java/org/elasticsearch/gateway/local/LocalGatewayAllocator.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ClusterRerouteTests.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/SimpleAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AddIncrementallyTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AwarenessAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/BalanceConfigurationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ClusterRebalanceRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/DeadNodesAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/FailedShardsRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RandomAllocationDeciderTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RebalanceAfterActiveTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesIntegrityTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/RoutingNodesUtils.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/SameShardRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/ShardsLimitAllocationTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/SingleShardNoReplicasRoutingTests.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDeciderTests.java</file><file>src/test/java/org/elasticsearch/indexlifecycle/IndexLifecycleActionTests.java</file></files><comments><comment>Improve RoutingNodes API</comment></comments></commit></commits></item><item><title>Cancel Allocation fails to reset the state of the source shard when moving</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4457</link><project id="" key="" /><description>When we move a shard from Node `A` to Node `B` and cancel the relocation the source node still remains in state `RELOCATING`.
</description><key id="24340327">4457</key><summary>Cancel Allocation fails to reset the state of the source shard when moving</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T10:19:53Z</created><updated>2013-12-16T11:11:32Z</updated><resolved>2013-12-16T11:11:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/command/CancelAllocationCommand.java</file><file>src/test/java/org/elasticsearch/cluster/routing/allocation/AllocationCommandsTests.java</file></files><comments><comment>Reset source shards to `started` if canceling relocation.</comment></comments></commit></commits></item><item><title>Start the task timeout checking *after* adding it to the execution queue of PrioritizedEsThreadPoolExecutor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4456</link><project id="" key="" /><description>This prevents missing very short timeouts which fire before the calling thread had the chance to add the task to the queue and are therefore ignored. This is mostly of importance for testing where we explicitly want tasks to timeout and set it to a very low value.
</description><key id="24339208">4456</key><summary>Start the task timeout checking *after* adding it to the execution queue of PrioritizedEsThreadPoolExecutor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-12-16T09:57:11Z</created><updated>2014-07-04T11:57:53Z</updated><resolved>2013-12-24T20:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-23T16:09:38Z" id="31127153">LGTM
</comment><comment author="bleskes" created="2013-12-24T20:05:38Z" id="31183735">Pushed: e6d9bf4719819c695d5a09a4b8f6db310b58d9ba &amp; 6fbcd8f8ff49b9e0e4f1f1ba125ae7410b19865b . Thx @kimchy 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow IndicesAdminClient.getAliases() to return all aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4455</link><project id="" key="" /><description>In order to not submit the whole cluster state in the Cat API it makes sense to allow the `GetAliasesRequest` to allow to return all aliases and remove the validation.
</description><key id="24338492">4455</key><summary>Allow IndicesAdminClient.getAliases() to return all aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T09:41:23Z</created><updated>2014-01-03T08:31:23Z</updated><resolved>2014-01-02T11:08:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesRequest.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestAliasAction.java</file><file>src/test/java/org/elasticsearch/aliases/IndexAliasesTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/CollectionAssertions.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/CollectionMatchers.java</file></files><comments><comment>Allow GetAliasRequest to retrieve all aliases</comment></comments></commit></commits></item><item><title>AllocationDeciders should be executed in order, starting at "cheap execution" and "most likely to return no"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4454</link><project id="" key="" /><description>The different AllocationDeciders are more or less expensive processing wise. They are also more or less likely to return a Decision.NO or Decision.THROTTLE. For large clusters this can result in a 10-15% speedup in recalculating the cluster state (tested with https://github.com/geidies/elasticsearch/blob/optimize_RoutingNodes/src/test/java/org/elasticsearch/cluster/routing/allocation/MassiveClusterRebalanceRoutingTests.java - 3.7 seconds on my test machine compared to 4.2 seconds without the optimization).

ConcurrentRebalanceAllocationDecider - loops over all ShardRoutings, O(n), optimization to O(1) for that in separate PR
DisableAllocationDecider - is O(1) 
ClusterRebalanceAllocationDecider - is O( #shards_unassigned ) + O( #shards ) or O( 1 ) + O( #shards ) - optimization for that in separate PR, making it O( 2 )
DiskThresholdDecider - O( 2 )
SnapshotInProgressAllocationDecider - O( 1 )
ReplicaAfterPrimaryActiveAllocationDecider and RebalanceOnlyWhenActiveAllocationDecider - O( #shards ) with current implementation of RoutingNodes.getShardsRoutingFor( MutableShardRouting ) - optimization in separate PR, making it O( #shards in replica set )
ShardsLimitAllocationDecider is O( # shards on node ) + O( 1 ).
AwarenessAllocationDecider is O( # shards in cluster ) \* # awareness attributes, making it the most expensive, but least likely to be turned on.
SameShardAllocationDecider is O( # shards on node ) \* # nodes on host
ThrottlingAllocationDecider, which is O( #shards_per_node ) + O( #shards_per_node )

In addition to the re-ordering, instead of applying all AllocationDeciders, skip the rest of one return a Decision.NO. This logic is ported from the Decision.Multi class.
</description><key id="24336798">4454</key><summary>AllocationDeciders should be executed in order, starting at "cheap execution" and "most likely to return no"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-16T09:02:54Z</created><updated>2013-12-16T09:03:36Z</updated><resolved>2013-12-16T09:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-16T09:03:07Z" id="30645009">issue for https://github.com/elasticsearch/elasticsearch/pull/4259
</comment><comment author="s1monw" created="2013-12-16T09:03:36Z" id="30645031">closed in https://github.com/elasticsearch/elasticsearch/commit/6ed126deaf1e50727b1e9f81271865f980dde58f
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replace `ignore_indices` with `ignore_unavailable`, `expand_wildcards` and `allow_no_indices`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4453</link><project id="" key="" /><description>- `ignore_unavailable` - Controls whether to ignore if any specified indices are unavailable, this includes indices
  that don't exist or closed indices. Either `true` or `false` can be specified.
- `allow_no_indices` - Controls whether to fail if a wildcard indices expressions results into no concrete indices.
  Either `true` or `false` can be specified. For example if the wildcard expression `foo*` is specified and no indices
  are available that start with `foo` then depending on this setting the request will fail. This setting is also applicable
  when `_all`, `*` or no index has been specified.
- `expand_wildcards` - Controls to what kind of concrete indices wildcard indices expression expand to. If `open` is
  specified then the wildcard expression if expanded to only open indices and if `closed` is specified then the wildcard expression if expanded only to closed indices. Also both values (`open,closed`) can be specified to expand to all indices.

There're a number of apis that support multiple indices, but didn't support the `ignore_indices` option. I haven't cut these apis over to the new indices options. I'll do that after this PR gets in. The following apis don't support for the new indices options yet: mlt, delete index, delete mapping, delete warmer, index exists, get indices settings, put index settings, put mappings and put warmer apis

Relates to #4436
</description><key id="24336545">4453</key><summary>Replace `ignore_indices` with `ignore_unavailable`, `expand_wildcards` and `allow_no_indices`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-16T08:57:20Z</created><updated>2015-05-18T23:33:25Z</updated><resolved>2014-01-02T11:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-12-18T16:24:37Z" id="30855901">Re `allow_no_indices`, I'd be tempted to leave that change out and only add it if somebody actually asks for it.  

`expand_wildcards` and `ignore_unavailable` look fine to me
</comment><comment author="martijnvg" created="2014-01-02T11:21:26Z" id="31447385">pushed to master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose `dist`/`pre`/`post` options for SpanNotQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4452</link><project id="" key="" /><description>Lucene 4.5 provides more options for the SpanNotQuery. Not sure if to update the docs with the pull request or not. If accepted and I knew the potential elasticsearch version which might contain it, I can add the appropriate "Added in ..." message in the documentation.
</description><key id="24330435">4452</key><summary>Expose `dist`/`pre`/`post` options for SpanNotQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">brusic</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.4.1</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2013-12-16T05:10:23Z</created><updated>2015-06-07T16:56:43Z</updated><resolved>2014-11-24T12:33:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2014-02-21T16:02:26Z" id="35743962">Merged latest master (broke on ElasticSearch -&gt; Elasticsearch renaming)
</comment><comment author="brusic" created="2014-07-30T16:44:09Z" id="50643909">Will attempt to rebase soon. I will also try to incorporate the test cases from the Lucene repo

https://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
</comment><comment author="clintongormley" created="2014-10-20T12:49:13Z" id="59744675">Hi @brusic - just pinging to find out if you've had a chance to look at this yet.
</comment><comment author="brusic" created="2014-10-21T00:26:05Z" id="59861667">I thought the ES crew forgot about this PR. :) I will be traveling this
week, but hopefully I can merge with master and perhaps add a few more
tests within a week.

On Mon, Oct 20, 2014 at 5:49 AM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @brusic https://github.com/brusic - just pinging to find out if
&gt; you've had a chance to look at this yet.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/4452#issuecomment-59744675
&gt; .
</comment><comment author="jadetr" created="2014-11-06T15:22:30Z" id="61994396">+1 to Expose dist/pre/post options for SpanNotQuery
</comment><comment author="brusic" created="2014-11-18T22:20:37Z" id="63556417">Rebased with latest master
</comment><comment author="martijnvg" created="2014-11-24T12:34:52Z" id="64187989">Thanks @brusic, although it took a while, it looked good and I pulled this in. 
</comment><comment author="jadetr" created="2014-11-28T15:21:07Z" id="64904162">@brusic Thanks for this fix and the answers on ES group. 
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java</file><file>src/main/java/org/elasticsearch/index/query/SpanNotQueryParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Applied PR, changed the way defaults are handled and updated the docs.</comment></comments></commit></commits></item><item><title>Inconsistent treatment of dates without year</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4451</link><project id="" key="" /><description>When dates are specified without a year, for example: `Dec 15 10:00:00` they are treated as dates in 2000 during indexing and range searches except for the upper included bound `lte`, where they are treated as dates in 1970. Repro: https://gist.github.com/imotov/7978186. Might be related to #2731.
</description><key id="24319533">4451</key><summary>Inconsistent treatment of dates without year</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>breaking</label><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-15T21:15:17Z</created><updated>2014-07-30T15:58:10Z</updated><resolved>2014-01-07T01:13:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/joda/FormatDateTimeFormatter.java</file><file>src/test/java/org/elasticsearch/index/mapper/date/SimpleDateMappingTests.java</file></files><comments><comment>Make partial dates without year to be 1970 based instead of 2000</comment></comments></commit></commits></item><item><title>Start elasticsearch in the foreground by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4450</link><project id="" key="" /><description>Instead of using the '-f' parameter to start elasticsearch in the
foreground, this is now the default modus.

In order to start elasticsearch in the background, the '-d' parameter
can be used.

Closes #4392
</description><key id="24315945">4450</key><summary>Start elasticsearch in the foreground by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-15T18:02:46Z</created><updated>2014-07-02T13:15:08Z</updated><resolved>2013-12-17T09:40:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-15T19:13:49Z" id="30617701">added small naming comment, other than that, looks great!
</comment><comment author="s1monw" created="2013-12-15T19:15:12Z" id="30617728">+1 foreground it much better default IMO
</comment><comment author="spinscale" created="2013-12-15T19:33:44Z" id="30618148">fixed the naming issue. We could go further and change the `foreground` system property to `background`, but this works as well.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Aggregation framework - filtering aggregations by nested object fields doesn't work correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4449</link><project id="" key="" /><description>Here i setup an mapping, queries and result:
https://gist.github.com/darklow/7964005

I also tried different queries, more simplified, without so much "nested" filters and queries (since in mapping i have `include_in_parent: true` nested is not required), but results all the time were same. 
</description><key id="24298364">4449</key><summary>Aggregation framework - filtering aggregations by nested object fields doesn't work correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darklow</reporter><labels /><created>2013-12-14T19:53:59Z</created><updated>2013-12-16T13:48:10Z</updated><resolved>2013-12-16T13:48:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-12-16T11:03:34Z" id="30652369">Does the following aggregation give back the result that you are expecting?

``` javascript
GET /movies/_search
{
   "query": {
      "nested": {
         "path": "credits",
         "query": {
            "match": {
               "credits.person_id": 1
            }
         }
      }
   },
   "aggs": {
      "credits" : {
          "nested" : {
              "path" : "credits"
          },
          "aggs" : {
              "person_1" : {
                  "filter" : {
                      "term" : {
                          "person_id" : 1
                      }
                  },
                  "aggs" : {
                      "departments" : {
                          "terms" : {
                              "field" : "department"
                          }
                      }
                  }
              }
          }
      }
   }
}
```

For every match, the aggregator:
- first goes from the movies to the credits ("credits" aggregation)
- then filter credits to only keep those that are about `person_id: 1` ("person_1" aggregation)
- finally compute counts of the departments for `person_id: 1` ("department" aggregation)
</comment><comment author="darklow" created="2013-12-16T13:48:10Z" id="30661264">This resolved my issue and returned correct results. Apparently i used wrong syntax. 
Thank you for explaining and giving full example. New aggregation feature is great! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to provide parameters not only through -D but as long parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4448</link><project id="" key="" /><description>All getopt long style parameters are now set as es. properties,

elasticsearch --path.data=/some/path

results in -Des.path.data=/some/path

Closes #4393

Tested with ubuntu and ubuntu lts (because they use dash, not bash), centos, opensuse and a debian 7.
</description><key id="24293847">4448</key><summary>Allow to provide parameters not only through -D but as long parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-14T15:33:40Z</created><updated>2014-07-16T21:50:29Z</updated><resolved>2013-12-17T09:47:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-14T18:24:08Z" id="30582617">Would be nice to have it for windows as well, right?
</comment><comment author="kimchy" created="2013-12-15T19:53:34Z" id="30618612">LGTM, I would also update the docs where we give a -D example? I think this is much nicer :)

@dadoonet agreed on the windows script, though its not related to this change, we never supported passing arguments in the windows script, its a different issue.
</comment><comment author="spinscale" created="2013-12-16T10:31:39Z" id="30650464">Added a `--node.name` example to the `-D` to show all possibilites
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cat API: Add endpoint to show aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4447</link><project id="" key="" /><description>This endpoint allows to check aliases, their indices, if a filter is
configured along with routing values for searching and indexing.

Basic usage:

```
curl "localhost:9200/_cat/alias?v"
alias                     index filter index_routing search_routing
test-alias-filter         test1 *      -             -
test-alias-search-routing test1 -      -             foo,bar
test                      test1 -      -             -
test-alias-routing        test1 -      foo           foo
test-alias-index-routing  test1 -      foo           -
test-two-indices          test1 -      -             -
test-two-indices          test2 -      -             -
```

Closes #4414
</description><key id="24292320">4447</key><summary>Cat API: Add endpoint to show aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-14T14:07:33Z</created><updated>2014-06-16T09:20:30Z</updated><resolved>2013-12-16T09:39:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-15T19:55:43Z" id="30618643">I think the endpoint should be `aliases`, to follow the other endpoints cat like `shards` and `nodes`. Other than that, looks great!
</comment><comment author="spinscale" created="2013-12-16T09:39:51Z" id="30647061">closed by https://github.com/elasticsearch/elasticsearch/commit/6a856c86e8d3757aa6e0b29612abeb9784a22476
</comment><comment author="clintongormley" created="2014-01-16T20:29:41Z" id="32537777">@spinscale The `_cat/aliases` endpoint is not documented, and these lines:

```
void documentation(StringBuilder sb) {
     sb.append("/_cat_aliases");
     sb.append("/_cat_aliases/{index}");
```

need to have a `\n` at the end
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cannot map timestamp with field in source </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4446</link><project id="" key="" /><description>Hi ! 

I have trouble mapping default "timestamp" with field from source.
Even example from manual (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-timestamp-field.html) is not working on my setup.  
Always i get error : 

{"error":"ElasticSearchParseException[failed to parse doc to extract routing/timestamp]; nested: TimestampParsingException[failed to parse timestamp [null]];

I install es with puppet provisioning with latest version (https://github.com/kenny13/elasticsearch/blob/master/manifests/localInstance.pp). 

Sample code and results can be found here : 
https://gist.github.com/kenny13/7959300

What can be wrong on this default setup and few lines of code ?
</description><key id="24291855">4446</key><summary>Cannot map timestamp with field in source </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ogavrisevs</reporter><labels /><created>2013-12-14T13:42:17Z</created><updated>2014-01-14T02:01:16Z</updated><resolved>2013-12-17T20:34:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-12-16T03:34:16Z" id="30631770">The path to timestamp is your example is incorrect. You can fix it by removing `twitter` object from your document:

```
curl -X PUT http://localhost:9200/twitter_33/tweet/1 -d '{
    "message" : "You know, for Search",
    "post_date" : "2009-11-15T14:12:12"
}'
```

Or by correcting path in the mapping:

```
curl -X PUT  http://localhost:9200/twitter_33/ -d '{
    "mappings": {
        "tweet": {
            "_timestamp" : {
                "enabled" : "yes",
                "path" : "tweet.post_date"
            },
            "properties": {
                "message": {
                    "type": "string"
                }
            }
        }
    }
}'
```
</comment><comment author="ogavrisevs" created="2013-12-17T20:36:05Z" id="30788314">thanks it helped ! 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>has_child hang, could anyone HELP?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4445</link><project id="" key="" /><description>The following query will hang in our production server, but if I remove
all filters (the term filter and bool filter) in has_child, the query will return 
in a few ms, does anyone has any suggestion?

Any help is greatly appreciated!

curl 'localhost:9200/recruitmentportal/47461a39-1863-4c37-9f98-cfd7e291e6d4/_search?routing=100100&amp;pretty' -d '
{
  "from": 0, 
  "size": 100, 
  "query": {
    "constant_score": {
      "filter": {
        "and": {
          "filters": [
            {
              "term": {
                "_tenantid": "100100"
              }
            }, 
            {
              "bool": {
                "must": [
                  {
                    "term": {
                      "RecruitmentPortal.PersonProfile.R_IsDeleted": "0"
                    }
                  }, 
                  {
                    "term": {
                      "RecruitmentPortal.PersonProfile.R_CommonState": "0"
                    }
                  }, 
                  {
                    "term": {
                      "RecruitmentPortal.PersonProfile.R_IsTms": "1"
                    }
                  }, 
                  {
                    "has_child": {
                      "query": {
                        "constant_score": {
                          "filter": {
                            "and": {
                              "filters": [
                                {
                                  "term": {
                                    "_tenantid": "100100"
                                  }
                                }, 
                                {
                                  "bool": {
                                    "must": [
                                      {
                                        "query": {
                                          "text": {
                                            "content": {
                                              "query": "abc", 
                                              "type": "phrase"
                                            }
                                          }
                                        }
                                      }
                                    ]
                                  }
                                }
                              ]
                            }
                          }, 
                          "boost": 1
                        }
                      }, 
                      "type": "ResumeFullTextForMetadata"
                    }
                  }
                ]
              }
            }
          ]
        }
      }, 
      "boost": 1
    }
  }
}'
</description><key id="24287347">4445</key><summary>has_child hang, could anyone HELP?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mashudong</reporter><labels /><created>2013-12-14T07:17:10Z</created><updated>2013-12-17T10:16:59Z</updated><resolved>2013-12-17T10:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-12-16T03:25:00Z" id="30631537">Could you rerun the query and check available heap space to see if you are running out of memory on this request. You can do it by running `curl "localhost:9200/_nodes/stats?jvm=true&amp;pretty"` and comparing number in the `heap_used_in_bytes` field to the amount of memory that you gave to elasticsearch. If you are not running out of memory, could you run `curl "localhost:9200/_nodes/hot_threads"` while query is executing and add the output of this command to the ticket?
</comment><comment author="martijnvg" created="2013-12-16T09:00:44Z" id="30644872">@mashudong What ES version are you running?
</comment><comment author="mashudong" created="2013-12-16T09:48:30Z" id="30647588">Thanks very much for your reply, @imotov, @martijnvg.

My ES version is 0.20.2.

Heap space is not exhausted, 

Info from curl 'localhost:9200/_nodes/stats?jvm=true&amp;pretty'

heap_used: 12.5gb
heap_committed: 15.5gb
CMS Old Gen used: 12.3gb
id_cache_size: 3.8gb

but, about 80% heap space is used, would it be a good idea to 
give more heap space to es? We have 32GB memory.

hot_threads indicate that time is spent at SimpleIdCache.checkIfCanReuse(),
I'll try to disable SimpleIdCache reuse and see if that helps.
</comment><comment author="mashudong" created="2013-12-16T09:49:48Z" id="30647661">Output from hot_threads

   94.0% (470ms out of 500ms) cpu usage by thread 'elasticsearch[Gertrude Yorkes][search][T#1]'
     6/10 snapshots sharing following 18 elements
       org.elasticsearch.common.bytes.HashedBytesArray.hashCode(HashedBytesArray.java:135)
       org.elasticsearch.common.trove.impl.hash.TObjectHash.hash(TObjectHash.java:482)
       org.elasticsearch.common.trove.impl.hash.TObjectHash.index(TObjectHash.java:168)
       org.elasticsearch.common.trove.ExtTObjectIntHasMap.key(ExtTObjectIntHasMap.java:48)
       org.elasticsearch.index.cache.id.simple.SimpleIdReaderTypeCache.canReuse(SimpleIdReaderTypeCache.java:82)
       org.elasticsearch.index.cache.id.simple.SimpleIdReaderCache.canReuse(SimpleIdReaderCache.java:82)
       org.elasticsearch.index.cache.id.simple.SimpleIdCache.checkIfCanReuse(SimpleIdCache.java:233)
       org.elasticsearch.index.cache.id.simple.SimpleIdCache.refresh(SimpleIdCache.java:136)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:87)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:316)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:243)
       org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:75)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
       java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       java.lang.Thread.run(Unknown Source)
     4/10 snapshots sharing following 17 elements
       org.elasticsearch.common.bytes.HashedBytesArray.hashCode(HashedBytesArray.java:135)
       org.elasticsearch.common.trove.impl.hash.TObjectHash.hash(TObjectHash.java:482)
       org.elasticsearch.common.trove.impl.hash.TObjectHash.index(TObjectHash.java:168)
       org.elasticsearch.common.trove.ExtTObjectIntHasMap.key(ExtTObjectIntHasMap.java:48)
       org.elasticsearch.index.cache.id.simple.SimpleIdCache$TypeBuilder.canReuse(SimpleIdCache.java:276)
       org.elasticsearch.index.cache.id.simple.SimpleIdCache.checkIfCanReuse(SimpleIdCache.java:240)
       org.elasticsearch.index.cache.id.simple.SimpleIdCache.refresh(SimpleIdCache.java:136)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:87)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:316)
       org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:243)
       org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:75)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
       org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
       java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       java.lang.Thread.run(Unknown Source)

```
0.0% (0s out of 500ms) cpu usage by thread 'Reference Handler'
 10/10 snapshots sharing following 3 elements
   java.lang.Object.wait(Native Method)
   java.lang.Object.wait(Object.java:485)
   java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)

0.0% (0s out of 500ms) cpu usage by thread 'Finalizer'
 10/10 snapshots sharing following 4 elements
   java.lang.Object.wait(Native Method)
   java.lang.ref.ReferenceQueue.remove(Unknown Source)
   java.lang.ref.ReferenceQueue.remove(Unknown Source)
   java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)
```
</comment><comment author="martijnvg" created="2013-12-16T09:53:16Z" id="30647924">I think that disabling the id reuse in simple id cache will fix your issue if you have nodes that hold many shards.

Btw maybe you should upgrade to the latest `0.90.X` release? Many improvements have been made since version `0.20.2` in general and to the has_child query.
</comment><comment author="mashudong" created="2013-12-16T10:21:12Z" id="30649627">There are about 600 shards on my server, we're planning to upgrade, but not now.

And would you think it be a good idea to give more heap space to es?

Info from curl 'localhost:9200/_nodes/stats?jvm=true&amp;pretty'

heap_used: 12.5gb
heap_committed: 15.5gb
CMS Old Gen used: 12.3gb
id_cache_size: 3.8gb
</comment><comment author="martijnvg" created="2013-12-16T10:26:37Z" id="30650013">&gt; There are about 600 shards on my server, we're planning to upgrade, but not now.

That explains the long id cache loading time, for each id it loads, it will try to reuse the id for segment in each shard.

About giving more heap, that depends:
- How much RAM is available on your machine that runs this node?
- Are you running multiple ES nodes per machine?
- Much time is spent in jvm garbage collection? and how long is your node running? (this info is inside the jvm stats)
- Besides the id_cache what else is taking the heap memory? (fielddata?)
</comment><comment author="mashudong" created="2013-12-16T13:56:13Z" id="30661806">```
How much RAM is available on your machine that runs this node?
```

32GB.

```
Are you running multiple ES nodes per machine?
```

NO.

```
Much time is spent in jvm garbage collection? and how long is your node running? (this info is inside the jvm stats)
```

It seems no (see below).

```
Besides the id_cache what else is taking the heap memory? (fielddata?)
```

Not sure.

Output of curl 'localhost:9200/_nodes/stats?jvm&amp;pretty'

```
  "indices" : {
    "store" : {
      "size" : "58.9gb",
      "size_in_bytes" : 63245633724,
      "throttle_time" : "0s",
      "throttle_time_in_millis" : 0
    },
    "docs" : {
      "count" : 64436600,
      "deleted" : 1772178
    },
    "indexing" : {
      "index_total" : 708794,
      "index_time" : "16.9m",
      "index_time_in_millis" : 1017727,
      "index_current" : 0,
      "delete_total" : 1399,
      "delete_time" : "964ms",
      "delete_time_in_millis" : 964,
      "delete_current" : 0
    },
    "get" : {
      "total" : 0,
      "time" : "0s",
      "time_in_millis" : 0,
      "exists_total" : 0,
      "exists_time" : "0s",
      "exists_time_in_millis" : 0,
      "missing_total" : 0,
      "missing_time" : "0s",
      "missing_time_in_millis" : 0,
      "current" : 0
    },
    "search" : {
      "query_total" : 4836816,
      "query_time" : "277.2d",
      "query_time_in_millis" : 23952877618,
      "query_current" : 0,
      "fetch_total" : 988048,
      "fetch_time" : "42.4m",
      "fetch_time_in_millis" : 2547904,
      "fetch_current" : 0
    },
    "cache" : {
      "field_evictions" : 0,
      "field_size" : "459.6mb",
      "field_size_in_bytes" : 481973798,
      "filter_count" : 3058,
      "filter_evictions" : 0,
      "filter_size" : "1gb",
      "filter_size_in_bytes" : 1170734992,
      "bloom_size" : "95.2mb",
      "bloom_size_in_bytes" : 99832336,
      "id_cache_size" : "3.4gb",
      "id_cache_size_in_bytes" : 3667700221
    },
    "merges" : {
      "current" : 0,
      "current_docs" : 0,
      "current_size" : "0b",
      "current_size_in_bytes" : 0,
      "total" : 4336,
      "total_time" : "7.5m",
      "total_time_in_millis" : 455785,
      "total_docs" : 10214856,
      "total_size" : "6gb",
      "total_size_in_bytes" : 6541099414
    },
    "refresh" : {
      "total" : 127405,
      "total_time" : "17.5m",
      "total_time_in_millis" : 1055197
    },
    "flush" : {
      "total" : 72849,
      "total_time" : "1.3h",
      "total_time_in_millis" : 4937001
    }
  },
  "jvm" : {
    "timestamp" : 1387201720989,
    "uptime" : "58 hours, 20 minutes, 58 seconds and 541 milliseconds",
    "uptime_in_millis" : 210058541,
    "mem" : {
      "heap_used" : "9.2gb",
      "heap_used_in_bytes" : 9921806768,
      "heap_committed" : "15.5gb",
      "heap_committed_in_bytes" : 16748904448,
      "non_heap_used" : "53.9mb",
      "non_heap_used_in_bytes" : 56596640,
      "non_heap_committed" : "83.2mb",
      "non_heap_committed_in_bytes" : 87289856,
      "pools" : {
        "Code Cache" : {
          "used" : "10.9mb",
          "used_in_bytes" : 11473216,
          "max" : "48mb",
          "max_in_bytes" : 50331648,
          "peak_used" : "11.1mb",
          "peak_used_in_bytes" : 11670080,
          "peak_max" : "48mb",
          "peak_max_in_bytes" : 50331648
        },
        "Par Eden Space" : {
          "used" : "67.6mb",
          "used_in_bytes" : 70919000,
          "max" : "216.3mb",
          "max_in_bytes" : 226885632,
          "peak_used" : "216.3mb",
          "peak_used_in_bytes" : 226885632,
          "peak_max" : "216.3mb",
          "peak_max_in_bytes" : 226885632
        },
        "Par Survivor Space" : {
          "used" : "4.9mb",
          "used_in_bytes" : 5169248,
          "max" : "27mb",
          "max_in_bytes" : 28311552,
          "peak_used" : "27mb",
          "peak_used_in_bytes" : 28311552,
          "peak_max" : "27mb",
          "peak_max_in_bytes" : 28311552
        },
        "CMS Old Gen" : {
          "used" : "9.1gb",
          "used_in_bytes" : 9845718520,
          "max" : "15.3gb",
          "max_in_bytes" : 16493707264,
          "peak_used" : "14.1gb",
          "peak_used_in_bytes" : 15184404376,
          "peak_max" : "15.3gb",
          "peak_max_in_bytes" : 16493707264
        },
        "CMS Perm Gen" : {
          "used" : "43mb",
          "used_in_bytes" : 45123424,
          "max" : "82mb",
          "max_in_bytes" : 85983232,
          "peak_used" : "43mb",
          "peak_used_in_bytes" : 45123424,
          "peak_max" : "82mb",
          "peak_max_in_bytes" : 85983232
        }
      }
    },
    "threads" : {
      "count" : 219,
      "peak_count" : 654
    },
    "gc" : {
      "collection_count" : 18749,
      "collection_time" : "9 minutes, 23 seconds and 337 milliseconds",
      "collection_time_in_millis" : 563337,
      "collectors" : {
        "ParNew" : {
          "collection_count" : 18733,
          "collection_time" : "9 minutes, 22 seconds and 447 milliseconds",
          "collection_time_in_millis" : 562447
        },
        "ConcurrentMarkSweep" : {
          "collection_count" : 16,
          "collection_time" : "890 milliseconds",
          "collection_time_in_millis" : 890
        }
      }
    }
  }
}
```

  }
}
</comment><comment author="martijnvg" created="2013-12-16T15:24:22Z" id="30668639">@mashudong From what I can see here, I see no need to increase the jvm heapspace.
</comment><comment author="mashudong" created="2013-12-17T01:40:05Z" id="30719351">OK, Thank you very much @martijnvg
</comment><comment author="martijnvg" created="2013-12-17T10:10:09Z" id="30739200">Closing this issue because it is an known bug in ES version 0.20.4 and before:
https://github.com/elasticsearch/elasticsearch/issues/2605
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Resolve potential deadlock state during EsThreadPoolExecutor shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4444</link><project id="" key="" /><description>Fixes #4334

The deadlock occurs between monitor object of EsThreadPoolExecutor and mainLock of ThreadPoolExecutor. The shutdown method of EsThreadPoolExecutor obtains the lock on monitor first and waits for mainLock of ThreadPoolExecutor in ThreadPoolExecutor#shutdown for part of the processing, while EsThreadPoolExecutor#terminated is executed under mainLock and tries to obtain monitor to notify listeners.
</description><key id="24281470">4444</key><summary>Resolve potential deadlock state during EsThreadPoolExecutor shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-12-14T00:27:37Z</created><updated>2014-07-11T09:30:39Z</updated><resolved>2013-12-15T03:13:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-14T00:43:11Z" id="30555167">LGTM, nice catch
</comment><comment author="s1monw" created="2013-12-14T08:16:03Z" id="30562653">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding document to index fails when the number of Unassigned shards is greater than 1.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4443</link><project id="" key="" /><description>You can reach this state by creating an index with more replicas than active nodes or by failing nodes after index was set up correctly.

Repro:
(copied from https://gist.github.com/creg/7775457.  He is sitting next to me)
# clean up test index

curl -XDELETE "http://localhost:9200/toomanyreplicas"
# create index

curl -XPOST "http://localhost:9200/toomanyreplicas" -d '
{
    "index" : {
        "analysis" : {
            "analyzer" : {
                "language_analyzer" : {
                    "type" : "snowball",
                    "language" : "English"
                }
            }
        },
        "number_of_shards": 5,
    "number_of_replicas": 2
    }
}
'
# create mapping

curl -XPOST "http://localhost:9200/toomanyreplicas/Document/_mapping" -d '
{
    "Document": {
        "_source": {
            "compress": false,
            "excludes": [
                "fileAttachment"
            ]
        },
        "properties": {
            "fileAttachment": {
                "type": "attachment",
                "path": "full",
                "fields": {
                    "fileAttachment": {
                        "type": "string",
                        "term_vector": "with_positions_offsets",
                        "index_analyzer": "language_analyzer",
                        "search_analyzer": "language_analyzer"
                    },
                    "author": {
                        "type": "string",
                        "store": true
                    },
                    "title": {
                        "type": "string",
                        "store": true,
                        "index_analyzer": "language_analyzer"
                    },
                    "name": {
                        "type": "string"
                    },
                    "date": {
                        "type": "date",
                        "format": "dateOptionalTime"
                    },
                    "keywords": {
                        "type": "string"
                    },
                    "content_type": {
                        "type": "string",
                        "store": true
                    },
                    "content_length": {
                        "type": "integer"
                    }
                }
            },
            "id": {
                "type": "string"
            },
            "currentVersionId": {
                "type": "string"
            }
        }
    }
}
'
# Attempt to index the document

curl -XPOST "http://localhost:9200/toomanyreplicas/testdocument" -d '
{
    "fileAttachment" : "ZmlnaHRpbmc="
}
'

Expect:
document to be added to the good node

Actual:

{
  "error":"UnavailableShardsException[[toomanyreplicas][4] [3] shardIt, [1] active : Timeout waiting for [1m], request: index {[toomanyreplicas][testdocument][QegOmpSTQZePZ1IaFeEmag], source[\n{\n    \"fileAttachment\" : \"ZmlnaHRpbmc=\"\n}\n]}]",
  "status":503
}

---

below is the response from http://localhost:9200/_cluster/state in this condition

{"cluster_name":"blenihan-ES","master_node":"chKSwPhQRVqmqJTZnrIZfQ","blocks":{},"nodes":{"chKSwPhQRVqmqJTZnrIZfQ":{"name":"Beetle","transport_address":"inet[/10.3.60.127:9300]","attributes":{}}},"metadata":{"templates":{},"indices":{"toomanyreplicas":{"state":"open","settings":{"index.analysis.analyzer.language_analyzer.language":"English","index.number_of_replicas":"2","index.number_of_shards":"5","index.analysis.analyzer.language_analyzer.type":"snowball","index.version.created":"900799","index.uuid":"qjdWPfMoQdCU5SjD7FrA3g"},"mappings":{"Document":{"_source":{"compress":false,"excludes":["fileAttachment"]},"properties":{"id":{"type":"string"},"fileAttachment":{"path":"full","type":"attachment","fields":{"author":{"store":true,"type":"string"},"title":{"index_analyzer":"language_analyzer","store":true,"type":"string"},"keywords":{"type":"string"},"fileAttachment":{"analyzer":"language_analyzer","term_vector":"with_positions_offsets","type":"string"},"name":{"type":"string"},"content_length":{"type":"integer"},"date":{"format":"dateOptionalTime","type":"date"},"content_type":{"store":true,"type":"string"}}},"currentVersionId":{"type":"string"}}},"testdoc":{"properties":{"fileAttachment":{"type":"string"}}}},"aliases":[]}}},"routing_table":{"indices":{"toomanyreplicas":{"shards":{"0":[{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":0,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":0,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":0,"index":"toomanyreplicas"}],"1":[{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":1,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":1,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":1,"index":"toomanyreplicas"}],"2":[{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":2,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":2,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":2,"index":"toomanyreplicas"}],"3":[{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":3,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":3,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":3,"index":"toomanyreplicas"}],"4":[{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":4,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":4,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":4,"index":"toomanyreplicas"}]}}}},"routing_nodes":{"unassigned":[{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":0,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":0,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":1,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":1,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":2,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":2,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":3,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":3,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":4,"index":"toomanyreplicas"},{"state":"UNASSIGNED","primary":false,"node":null,"relocating_node":null,"shard":4,"index":"toomanyreplicas"}],"nodes":{"chKSwPhQRVqmqJTZnrIZfQ":[{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":0,"index":"toomanyreplicas"},{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":1,"index":"toomanyreplicas"},{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":2,"index":"toomanyreplicas"},{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":3,"index":"toomanyreplicas"},{"state":"STARTED","primary":true,"node":"chKSwPhQRVqmqJTZnrIZfQ","relocating_node":null,"shard":4,"index":"toomanyreplicas"}]}},"allocations":[]}
</description><key id="24266595">4443</key><summary>Adding document to index fails when the number of Unassigned shards is greater than 1.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blenihan</reporter><labels /><created>2013-12-13T19:20:42Z</created><updated>2013-12-16T16:46:56Z</updated><resolved>2013-12-14T12:56:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="blenihan" created="2013-12-13T23:15:30Z" id="30551609">also checked /_cluster/health.  status: "yellow"

{
cluster_name: "blenihan-ES"
status: "yellow"
timed_out: false
number_of_nodes: 1
number_of_data_nodes: 1
active_primary_shards: 5
active_shards: 5
relocating_shards: 0
initializing_shards: 0
unassigned_shards: 10
}
</comment><comment author="s1monw" created="2013-12-14T12:56:46Z" id="30570116">aahh I see what's going on.. this is actually expected behaviour since the [write consistency](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html#index-consistency) is set to quorum by default which means that you need to have at least once replica to be available in order to index your document.
</comment><comment author="blenihan" created="2013-12-16T16:46:56Z" id="30676660">Thanks for clearing that up.  What sorts of bad behavior would I expect to see if a write did happen on the wrong side of a partition?  I assume there is a good reason that quorum is the default.  I'd like to understand it better before I start overriding things like this.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>indices.fielddata.cache.expire documentation correction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4442</link><project id="" key="" /><description>According to the documentation http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-fielddata.html

The example for absolute value for indices.fielddata.cache.expire is 12GB. However, using GB throws an error:

{0.90.7}: Initialization Failed ...
1) ElasticSearchParseException[Failed to parse [7GB]]
        NumberFormatException[For input string: "7GB"]

Changing it to 7G worked properly.
</description><key id="24266202">4442</key><summary>indices.fielddata.cache.expire documentation correction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">stonith</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-13T19:13:42Z</created><updated>2014-01-03T08:34:16Z</updated><resolved>2014-01-02T12:03:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-16T13:17:29Z" id="30659560">Hey,

just to make sure: You are not talking about the `expire` but about the `size` parameter?
Making the parsing a bit more relaxed would solve this problem, as it is case-sensitive at the moment. Using `7gb` will work at the moment.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/unit/ByteSizeValue.java</file><file>src/test/java/org/elasticsearch/common/unit/ByteSizeValueTests.java</file></files><comments><comment>Made parsing of ByteSizeValue case independent</comment></comments></commit></commits></item><item><title>Take fuzzy edit distance into account when computing the score for completion suggestion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4441</link><project id="" key="" /><description>Currently the completion suggester uses the configured weight as the score of the suggestion. It would be nice to take the edit distance into account when calculating the score for the completion suggester used with fuzzy option as the term suggester does.
</description><key id="24256050">4441</key><summary>Take fuzzy edit distance into account when computing the score for completion suggestion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">MichaelSueess</reporter><labels><label>enhancement</label></labels><created>2013-12-13T16:20:43Z</created><updated>2015-06-07T16:19:43Z</updated><resolved>2014-12-11T20:38:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-13T16:42:17Z" id="30523406">I agree - yet this needs some lucene work though... we could discount based on the edits...
</comment><comment author="darklow" created="2014-02-16T11:21:52Z" id="35194349">Yes, this would be very useful, fuzzy feature is great and useful for mistypes, but in lot of cases it really messes good results, so we would like to lower weight for fuzzy matches, so that fuzzy results appears after good matches.

I think issue #4759 is about this same thing.
</comment><comment author="pablomolnar" created="2014-04-09T00:51:10Z" id="39918632">+1. Currently I have a custom ordering logic in the client side to deal with the issue.
</comment><comment author="areek" created="2014-12-11T20:38:45Z" id="66684610">closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8909
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Expose number of queries in percolator stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4440</link><project id="" key="" /><description>The percolator statistics response contains the current number of registered percolation queries. This number is not exposed via the rest endpoint.
</description><key id="24238538">4440</key><summary>Expose number of queries in percolator stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-13T10:33:26Z</created><updated>2013-12-13T10:37:25Z</updated><resolved>2013-12-13T10:37:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/percolator/stats/PercolateStats.java</file></files><comments><comment>Add `queries` to XContent output of PercolateStats</comment></comments></commit></commits></item><item><title>Fixing typo and grammar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4439</link><project id="" key="" /><description /><key id="24234917">4439</key><summary>Fixing typo and grammar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gregquat</reporter><labels /><created>2013-12-13T09:23:55Z</created><updated>2014-07-16T21:50:31Z</updated><resolved>2013-12-17T10:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-12-17T10:41:18Z" id="30741010">Merged, thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>there are lots of port ESTABLISHED when i open elasticsearch </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4438</link><project id="" key="" /><description>  TCP    127.0.0.1:50075        127.0.0.1:50076        ESTABLISHED     1316
  TCP    127.0.0.1:50076        127.0.0.1:50075        ESTABLISHED     1316
  TCP    127.0.0.1:50077        127.0.0.1:50078        ESTABLISHED     1316
  TCP    127.0.0.1:50078        127.0.0.1:50077        ESTABLISHED     1316
  TCP    127.0.0.1:50079        127.0.0.1:50080        ESTABLISHED     1316
  TCP    127.0.0.1:50080        127.0.0.1:50079        ESTABLISHED     1316
  TCP    127.0.0.1:50081        127.0.0.1:50082        ESTABLISHED     1316
  TCP    127.0.0.1:50082        127.0.0.1:50081        ESTABLISHED     1316
  TCP    127.0.0.1:50083        127.0.0.1:50084        ESTABLISHED     1316
  TCP    127.0.0.1:50084        127.0.0.1:50083        ESTABLISHED     1316
  TCP    127.0.0.1:50085        127.0.0.1:50086        ESTABLISHED     1316
  TCP    127.0.0.1:50086        127.0.0.1:50085        ESTABLISHED     1316
  TCP    127.0.0.1:50087        127.0.0.1:50088        ESTABLISHED     1316
  TCP    127.0.0.1:50088        127.0.0.1:50087        ESTABLISHED     1316
  TCP    127.0.0.1:50089        127.0.0.1:50090        ESTABLISHED     1316
  TCP    127.0.0.1:50090        127.0.0.1:50089        ESTABLISHED     1316
  TCP    127.0.0.1:50091        127.0.0.1:50092        ESTABLISHED     1316
  TCP    127.0.0.1:50092        127.0.0.1:50091        ESTABLISHED     1316
  TCP    127.0.0.1:50093        127.0.0.1:50094        ESTABLISHED     1316
  TCP    127.0.0.1:50094        127.0.0.1:50093        ESTABLISHED     1316
  TCP    127.0.0.1:50095        127.0.0.1:50096        ESTABLISHED     1316
  TCP    127.0.0.1:50096        127.0.0.1:50095        ESTABLISHED     1316
  TCP    127.0.0.1:50097        127.0.0.1:50098        ESTABLISHED     1316
  TCP    127.0.0.1:50098        127.0.0.1:50097        ESTABLISHED     1316
  TCP    127.0.0.1:50122        127.0.0.1:50123        ESTABLISHED     1316
  TCP    127.0.0.1:50123        127.0.0.1:50122        ESTABLISHED     1316
  TCP    127.0.0.1:50124        127.0.0.1:50125        ESTABLISHED     1316
  TCP    127.0.0.1:50125        127.0.0.1:50124        ESTABLISHED     1316
  TCP    127.0.0.1:50126        127.0.0.1:50127        ESTABLISHED     1316
  TCP    127.0.0.1:50127        127.0.0.1:50126        ESTABLISHED     1316
  TCP    127.0.0.1:50128        127.0.0.1:50129        ESTABLISHED     1316
  TCP    127.0.0.1:50129        127.0.0.1:50128        ESTABLISHED     1316
  TCP    127.0.0.1:50130        127.0.0.1:50131        ESTABLISHED     1316
  TCP    127.0.0.1:50131        127.0.0.1:50130        ESTABLISHED     1316
  TCP    127.0.0.1:50132        127.0.0.1:50133        ESTABLISHED     1316
  TCP    127.0.0.1:50133        127.0.0.1:50132        ESTABLISHED     1316
  TCP    127.0.0.1:50134        127.0.0.1:50135        ESTABLISHED     1316
  TCP    127.0.0.1:50135        127.0.0.1:50134        ESTABLISHED     1316
  TCP    127.0.0.1:50136        127.0.0.1:50137        ESTABLISHED     1316
  TCP    127.0.0.1:50137        127.0.0.1:50136        ESTABLISHED     1316
</description><key id="24223405">4438</key><summary>there are lots of port ESTABLISHED when i open elasticsearch </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">zhyj0121</reporter><labels /><created>2013-12-13T02:21:35Z</created><updated>2014-07-18T10:39:03Z</updated><resolved>2014-07-18T10:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-13T15:54:08Z" id="30519457">Hey

can you be a little more verbose instead of copy pasting your netstat output?
- Elasticsearch version, JVM version, OS
- Setup. Is this a cluster or a single node?
- Access? Are there node clients involved?
- How did you identify this as elasticsearch connections?

Thanks!
</comment><comment author="zhyj0121" created="2013-12-14T00:51:05Z" id="30555457">C:\Users\zkf62478&gt;netstat -ano | findstr 6300
  TCP    10.108.100.96:9200     0.0.0.0:0              LISTENING       6300
  TCP    10.108.100.96:9300     0.0.0.0:0              LISTENING       6300
  TCP    10.108.100.96:9300     10.108.100.96:59697    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59698    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59699    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59700    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59701    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59702    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59703    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59704    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59705    ESTABLISHED     6300
  TCP    10.108.100.96:9300     10.108.100.96:59706    ESTABLISHED     6300
  TCP    10.108.100.96:59697    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59698    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59699    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59700    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59701    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59702    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59703    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59704    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59705    10.108.100.96:9300     ESTABLISHED     6300
  TCP    10.108.100.96:59706    10.108.100.96:9300     ESTABLISHED     6300
  TCP    127.0.0.1:59648        127.0.0.1:59649        ESTABLISHED     6300
  TCP    127.0.0.1:59649        127.0.0.1:59648        ESTABLISHED     6300
  TCP    127.0.0.1:59650        127.0.0.1:59651        ESTABLISHED     6300
  TCP    127.0.0.1:59651        127.0.0.1:59650        ESTABLISHED     6300
  TCP    127.0.0.1:59652        127.0.0.1:59653        ESTABLISHED     6300
  TCP    127.0.0.1:59653        127.0.0.1:59652        ESTABLISHED     6300
  TCP    127.0.0.1:59654        127.0.0.1:59655        ESTABLISHED     6300
  TCP    127.0.0.1:59655        127.0.0.1:59654        ESTABLISHED     6300
  TCP    127.0.0.1:59656        127.0.0.1:59657        ESTABLISHED     6300
  TCP    127.0.0.1:59657        127.0.0.1:59656        ESTABLISHED     6300
  TCP    127.0.0.1:59658        127.0.0.1:59659        ESTABLISHED     6300
  TCP    127.0.0.1:59659        127.0.0.1:59658        ESTABLISHED     6300
  TCP    127.0.0.1:59660        127.0.0.1:59661        ESTABLISHED     6300
  TCP    127.0.0.1:59661        127.0.0.1:59660        ESTABLISHED     6300
  TCP    127.0.0.1:59662        127.0.0.1:59663        ESTABLISHED     6300
  TCP    127.0.0.1:59663        127.0.0.1:59662        ESTABLISHED     6300
  TCP    127.0.0.1:59664        127.0.0.1:59665        ESTABLISHED     6300
  TCP    127.0.0.1:59665        127.0.0.1:59664        ESTABLISHED     6300
  TCP    127.0.0.1:59666        127.0.0.1:59667        ESTABLISHED     6300
  TCP    127.0.0.1:59667        127.0.0.1:59666        ESTABLISHED     6300
  TCP    127.0.0.1:59668        127.0.0.1:59669        ESTABLISHED     6300
  TCP    127.0.0.1:59669        127.0.0.1:59668        ESTABLISHED     6300
  TCP    127.0.0.1:59670        127.0.0.1:59671        ESTABLISHED     6300
  TCP    127.0.0.1:59671        127.0.0.1:59670        ESTABLISHED     6300
  TCP    127.0.0.1:59672        127.0.0.1:59673        ESTABLISHED     6300
  TCP    127.0.0.1:59673        127.0.0.1:59672        ESTABLISHED     6300
  TCP    127.0.0.1:59674        127.0.0.1:59675        ESTABLISHED     6300
  TCP    127.0.0.1:59675        127.0.0.1:59674        ESTABLISHED     6300
  TCP    127.0.0.1:59676        127.0.0.1:59677        ESTABLISHED     6300
  TCP    127.0.0.1:59677        127.0.0.1:59676        ESTABLISHED     6300
  TCP    127.0.0.1:59678        127.0.0.1:59679        ESTABLISHED     6300
  TCP    127.0.0.1:59679        127.0.0.1:59678        ESTABLISHED     6300
  TCP    127.0.0.1:59680        127.0.0.1:59681        ESTABLISHED     6300
  TCP    127.0.0.1:59681        127.0.0.1:59680        ESTABLISHED     6300
  TCP    127.0.0.1:59682        127.0.0.1:59683        ESTABLISHED     6300
  TCP    127.0.0.1:59683        127.0.0.1:59682        ESTABLISHED     6300
  TCP    127.0.0.1:59707        127.0.0.1:59708        ESTABLISHED     6300
  TCP    127.0.0.1:59708        127.0.0.1:59707        ESTABLISHED     6300
  TCP    127.0.0.1:59709        127.0.0.1:59710        ESTABLISHED     6300
  TCP    127.0.0.1:59710        127.0.0.1:59709        ESTABLISHED     6300
  TCP    127.0.0.1:59711        127.0.0.1:59712        ESTABLISHED     6300
  TCP    127.0.0.1:59712        127.0.0.1:59711        ESTABLISHED     6300
  TCP    127.0.0.1:59713        127.0.0.1:59714        ESTABLISHED     6300
  TCP    127.0.0.1:59714        127.0.0.1:59713        ESTABLISHED     6300
  TCP    127.0.0.1:59715        127.0.0.1:59716        ESTABLISHED     6300
  TCP    127.0.0.1:59716        127.0.0.1:59715        ESTABLISHED     6300
  TCP    127.0.0.1:59717        127.0.0.1:59718        ESTABLISHED     6300
  TCP    127.0.0.1:59718        127.0.0.1:59717        ESTABLISHED     6300
  TCP    127.0.0.1:59719        127.0.0.1:59720        ESTABLISHED     6300
  TCP    127.0.0.1:59720        127.0.0.1:59719        ESTABLISHED     6300
  TCP    127.0.0.1:59721        127.0.0.1:59722        ESTABLISHED     6300
  TCP    127.0.0.1:59722        127.0.0.1:59721        ESTABLISHED     6300
  TCP    127.0.0.1:59723        127.0.0.1:59724        ESTABLISHED     6300
  TCP    127.0.0.1:59724        127.0.0.1:59723        ESTABLISHED     6300
  UDP    0.0.0.0:54328          _:_                                    6300
  UDP    [::]:54328             _:_                                    6300

pid 6300 is elasticsearch 
•Elasticsearch version 0.90.5 
•JVM version jdk1.6.0_29
OS  win7
single node
 Are there node clients involved?    no
•How did you identify this as elasticsearch connections?  just open it  
elasticsearch-0.90.5\elasticsearch-0.90.5\bin\elasticsearch.bat
</comment><comment author="ch0uCas" created="2014-02-26T15:43:37Z" id="36138813">Have you found a solution !
</comment><comment author="spinscale" created="2014-07-18T10:39:03Z" id="49417709">To be honest, this is not really a problem and nothing to worry about. Elasticsearch opens some permanent TCP connections after startup to make sure that node-to-node communication does not suffer impact from creating connections when they are needed.

Closing this
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Features/repo base path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4437</link><project id="" key="" /><description>A couple of small changes for snapshot/restore 
</description><key id="24221831">4437</key><summary>Features/repo base path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-12-13T01:30:28Z</created><updated>2014-07-16T21:50:31Z</updated><resolved>2013-12-19T01:52:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-12-13T21:40:14Z" id="30545825">This looks good to me, +1
</comment><comment author="dadoonet" created="2013-12-18T16:30:42Z" id="30856522">LGTM. +1 Tested with AWS S3.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Replace `ignore_indices` with 3 other options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4436</link><project id="" key="" /><description>Today the `ignore_indices` option controls what to do when an specified index doesn't exists. 

The new following options will replace `ignore_indices` to make ignoring indices more flexible:
- `ignore_unavailable` - Controls whether to ignore specified indices that are unavailable. Unavailable refers to indices that don't exist or are closed. 
- `expand_wildcards` - Controls what kind of indices should be expanded with wildcard expressions. If `open` is specified a wildcard expression expands only into open indices and if `closed` is specified a wildcard expression only expands into closed indices. The latter only makes sense in the close index api, which it will default to.
- `allow_no_indices` - Controls whether it is allowed that a wildcard expression expands  into zero concrete indices. This option also applies when `_all` or no index has been specified.
</description><key id="24205962">4436</key><summary>Replace `ignore_indices` with 3 other options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-12T20:26:14Z</created><updated>2014-01-02T11:21:15Z</updated><resolved>2014-01-02T11:21:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-02T11:21:15Z" id="31447380">Closed via: https://github.com/elasticsearch/elasticsearch/commit/f4bf0d5112b5c6f29b651586d72c3972db5a2834
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/TransportClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/TransportCreateSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/TransportRestoreSnapshotAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/TransportAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/BaseAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/TransportGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/TransportCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/TransportDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/TransportIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TransportTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/TransportDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/TransportPutMappingAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/TransportOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/TransportUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/UpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/TransportPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequest.java</file><file>src/main/java/org/elasticsearch/action/percolate/TransportMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/search/MultiSearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequest.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/TransportSearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/type/TransportSearchTypeAction.java</file><file>src/main/java/org/elasticsearch/action/support/IgnoreIndices.java</file><file>src/main/java/org/elasticsearch/action/support/IndicesOptions.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/ClusterInfoRequest.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/ClusterInfoRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java</file><file>src/main/java/org/elasticsearch/action/support/replication/IndicesReplicationOperationRequest.java</file><file>src/main/java/org/elasticsearch/action/support/replication/IndicesReplicationOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/replication/TransportIndicesReplicationOperationAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java</file><file>src/main/java/org/elasticsearch/common/collect/ImmutableOpenMap.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/IndicesQueryParser.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/cluster/shards/RestClusterSearchShardsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/get/RestGetAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/head/RestAliasesExistAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/cache/clear/RestClearIndicesCacheAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/close/RestCloseIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/delete/RestDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/exists/indices/RestIndicesExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/exists/types/RestTypesExistsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/flush/RestFlushAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/gateway/snapshot/RestGatewaySnapshotAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/delete/RestDeleteMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/put/RestPutMappingAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/open/RestOpenIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/optimize/RestOptimizeAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/refresh/RestRefreshAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/segments/RestIndicesSegmentsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/settings/RestUpdateSettingsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/stats/RestIndicesStatsAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/status/RestIndicesStatusAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/validate/query/RestValidateQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/delete/RestDeleteWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/warmer/put/RestPutWarmerAction.java</file><file>src/main/java/org/elasticsearch/rest/action/count/RestCountAction.java</file><file>src/main/java/org/elasticsearch/rest/action/deletebyquery/RestDeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestMultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/percolate/RestPercolateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestMultiSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java</file><file>src/main/java/org/elasticsearch/rest/action/suggest/RestSuggestAction.java</file><file>src/main/java/org/elasticsearch/snapshots/RestoreService.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotUtils.java</file><file>src/main/java/org/elasticsearch/snapshots/SnapshotsService.java</file><file>src/test/java/org/elasticsearch/action/percolate/MultiPercolatorRequestTests.java</file><file>src/test/java/org/elasticsearch/action/search/MultiSearchRequestTests.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/MetaDataTests.java</file><file>src/test/java/org/elasticsearch/deleteByQuery/DeleteByQueryTests.java</file><file>src/test/java/org/elasticsearch/indices/IgnoreIndicesTests.java</file><file>src/test/java/org/elasticsearch/indices/IndicesOptionsTests.java</file><file>src/test/java/org/elasticsearch/indices/exists/types/TypesExistsTests.java</file><file>src/test/java/org/elasticsearch/indices/state/OpenCloseIndexTests.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/RandomTests.java</file><file>src/test/java/org/elasticsearch/snapshots/SnapshotUtilsTests.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file></files><comments><comment>Replaced `ignore_indices` with `ignore_unavailable`, `expand_wildcards` and `allow_no_indices`.</comment></comments></commit></commits></item><item><title>Calculate weight of shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4435</link><project id="" key="" /><description>Let Elasticsearch figure out how much work it is spending on each shard
so it can do intelligent things with that data.  Intelligent things not
included in this commit.

The weights are calculated based on exponentially weighted moving averages
of the amount of time performing various tasks on the shard.  These times
could themselves be weighted or not.  That is an open question.  Either
way the weight of a shard is calculated as the proportion of that total
weight that is spent on the shard compared to either the cluster as a whole
to get the cluster weight or the node to get the node weight.

There are actually six weights based on the six moving averages mainted
for each time stat: 1 minute, 5 minutes, 15 minutes, 1 hour, 1 day, 1 week.
I'm not sure which one will be useful for things like shard balancing so
I calculated them all.

Closes #4434

Work in progress because:
I don't trust EWMA because it uses LongAdder's getAndReset method which
is documented as not working while threads are updating it.  Maybe it is
good enough for this.  Would it make more sense to just pass in the delta
between this tick and the last from the sum in MeanMetric.

When a shard is first made should it's averages default to the rate of
the first five seconds like it does now?  That is probably ok intelligent
things will have to take that into account.  I think routing would be better
off assuming that it is 2 or something like that because the shard will
soon get updates, probably more than others because it is new and phasing
in the actual weight with the a priori weight over time or maybe all at
once after a few minutes.

Is it OK to start so many jobs ticking every five seconds?

Will those jobs tick close enough to five seconds to make the figures
useful?

Should all the jobs run in the GENERAL thread pool like they are now?

I don't stop any of the jobs that I start.  Ever.  Insanity.

Not building weight using enough time statistics.  Which ones should I use?

Should the weight include things other than cpu times?

What should I do about the statistics of moved shard?  Copy their rates
over from the old shard and continue where it left off and let it start
over?  If I don't copy the rates, what good will the one week average be?

Not sure if times should be weighted.

Formatting.

Documentation.
</description><key id="24204215">4435</key><summary>Calculate weight of shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>adoptme</label></labels><created>2013-12-12T20:00:13Z</created><updated>2016-03-08T13:12:38Z</updated><resolved>2016-03-08T13:12:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-03-05T16:52:24Z" id="36764692">I know this is now super out of date but is there any interest in this?  My indexes really aren't worth similar weights and I can't combine them because of suggestions and, well, maintainability in general.
</comment><comment author="s1monw" created="2014-07-17T14:35:59Z" id="49315005">@nik9000 I think it is of interest - I hope I can pick that up at some point :)
</comment><comment author="nik9000" created="2014-08-08T22:37:34Z" id="51665424">Hey @s1monw!  I just noticed your comment from 22 days ago. I'm at a conference now so I'm not super with it but I'll try to have a look at this again when I get home next week. 
</comment><comment author="clintongormley" created="2014-11-11T18:42:33Z" id="62595136">Hi @nik9000 

Just pinging about this PR, hope you've found some tuits :)
</comment><comment author="vineelyalamarthy" created="2015-07-06T17:21:36Z" id="118929938">Is this open for me to take ?
</comment><comment author="nik9000" created="2015-07-06T17:27:21Z" id="118931618">&gt; Is this open for me to take ?

I won't complain if you want to grab it. I'll even review it! I just don't have time to work on it right now. Might get time in August but I don't want to cookie-lick because I don't know if it'll be at the top of my pile when I get time to work on Elasticsearch contributions again.
</comment><comment author="vineelyalamarthy" created="2015-07-06T17:34:33Z" id="118934324">@nik9000 could you please tell me,how to get started on this (like share any analysis/code that has been done before to get started)
</comment><comment author="nik9000" created="2015-07-06T17:50:08Z" id="118937566">&gt; @nik9000 could you please tell me,how to get started on this (like share any analysis/code that has been done before to get started)

If its your first contribution to Elasticsearch it'd be pretty difficult I think. Lots of moving parts in it. Low cost of failure though - we could certainly make a switch that just turns off running all the moving average if we find out that its a huge drain. That's probably a good idea.

I've done very little analysis on this - I'm more doing this by instinct. My instincts are like this:
1. Load average is a useful tool.
2. We have lots of sums already being calculated by Elasticsearch.
3. Elasticsearch already has code embedded into it to periodically snapshot those into moving averages.
4. Lets just plug those together and expose them so we can do with them.

I think step 0 is to decide if the commit should be rebased. Step 1 is to rebase or rewrite it. Step 2 would be to add a switch to turn the calculations on and off. Step 3 is to talk about all those open questions. Maybe we can add some moving averages for iowait or something. I'm not sure though. Its been a long time since I thought about this sadly.
</comment><comment author="vineelyalamarthy" created="2015-07-06T17:57:01Z" id="118939108">Thank you so much, Sure, possibly you can be passively involved, Since its my starting contribution, I will let you know what's going on and take feedback from you. 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Can Elasticsearch calculate the "weight" of a shard?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4434</link><project id="" key="" /><description>Can Elasticsearch calculate the "weight" of a shard?  It'd be nice just to be able to look at which shards are hot.  Also, it could be useful to rebalance hot shards away from one another.

This seems like it'll be likely in my deployment because my indexes all have very different sizes and usage patterns.
</description><key id="24204062">4434</key><summary>Can Elasticsearch calculate the "weight" of a shard?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2013-12-12T19:58:11Z</created><updated>2015-11-13T12:42:58Z</updated><resolved>2015-11-13T12:42:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-12-26T18:21:31Z" id="31230115">Any interest in this?  My primary use case would be some kind of allocation decider that slowly (but automatically) moves high traffic shards away from each other.  Would it be more useful if I bundled work on that into this?
</comment><comment author="markharwood" created="2015-11-13T10:26:51Z" id="156389628">@nik9000 We were discussing this on FixItFriday - do you have an updated viewpoint?

Our thoughts were that the allocation decider is currently pretty black or white in it's rule evaluation (this needs to be here because this rule says so). If I understand correctly this introduces shades of grey into the rule evaluation which starts to take us down the path of being less easily explainable as to why certain decisions were made. Given all the existing factors that go into the mix of allocation decisions it might be challenging to add this sort of fuzziness?
</comment><comment author="nik9000" created="2015-11-13T12:42:58Z" id="156423151">Ah, man, this one is ancient! At this point I imagine allocation deciders based on runtime conditions are too contentious.

This PR is so out of date its really only useful from an archaeological standpoint, but if I were to revive it I'd just try to get the weights calculated and fetch-able via `_cat` and friends. At that point it'd mostly be useful for detecting hotspots of various sorts.

The idea is that things like "time spent searching" and "time spent indexing" aren't particularly useful measures to _look_ at and make decisions about. But their derivative is. And we can get a decent estimate of it using load average style calculations quite cheaply. This is all stuff you can do in marvel with a derivative aggregation but that requires storing the data and having marvel, both of which make it not something elasticsearch can rely on having at all times.

Here is what I'll do: I'll close this. If we ever want to do smart stuff based off of the relative activity on the shards we can use this as a reference point.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Cat: Collapse/group column support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4433</link><project id="" key="" /><description>Need to be able to group columns, collapsing most by default, like the JSON APIs do.  Could also support an explicit list like `ps` does on Unixy systems.  `/_cat/nodes` suffers the worst currently.  It's already too big and we need to add more (cache sizes, eg).
</description><key id="24189884">4433</key><summary>Cat: Collapse/group column support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label></labels><created>2013-12-12T16:27:43Z</created><updated>2014-02-25T08:29:08Z</updated><resolved>2013-12-24T04:27:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-12-23T18:07:41Z" id="31134109">I have fixed ordering of columns based on `headers` and thinking about leaving it there.  It turns out to be sufficient because you can create any output you like.

Grouping is proving to be too confusing as a user.  What if I want the `mem` section but the default column placement isn't ideal? Now I have to explicitly list them in `headers` anyway.  The code is then equally as confusing.
</comment><comment author="s1monw" created="2014-02-20T21:46:28Z" id="35672926">@drewr I think this was wrongly tagged with `1.0.1` no?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/Table.java</file><file>src/main/java/org/elasticsearch/rest/action/support/RestTable.java</file></files><comments><comment>Add associative lookup of columns for arbitrary (and more intuitive) ordering.</comment></comments></commit></commits></item><item><title>Make field data changes immediately taken into account and add the ability to disallow field data loading.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4432</link><project id="" key="" /><description>This commit changes field data configuration updates so that they are
immediately taken into account for loading new segments. The way it works
is that field data configuration is now cached separately from the field
data cache, meaning that it is now possible to clear the field data
configuration from IndexFieldDataService while the cache will stay around. On
the next time that Elasticsearch will reload field data configuration, it will
check if there is already a cache entry, and reuse it if it exists.

To disable field data loading, all that is required is to change the field
data format to "none" (supported by all field data types) using the update
mapping API. Elasticsearch will then refuse to load field data on any new
segment, but field data which has been loaded on the previous segments will
remain available. So you need to clear the field data cache in order to
reclaim memory (otherwise memory will be reclaimed slower, as segments get
merged).

Close #4430
Close #4431
</description><key id="24182349">4432</key><summary>Make field data changes immediately taken into account and add the ability to disallow field data loading.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-12T14:41:12Z</created><updated>2014-07-06T00:33:05Z</updated><resolved>2013-12-16T14:50:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-12T17:09:50Z" id="30441422">I left some small comments, other than that LGMT - I am tempted to propose to default tokenized fields to this!
</comment><comment author="kimchy" created="2013-12-12T21:51:26Z" id="30465695">I am not sure I like calling it `none` or `no`, you explained it with the world `disable`, so maybe just call it `disable(d)`?
</comment><comment author="jpountz" created="2013-12-12T22:46:04Z" id="30469952">&gt; I am tempted to propose to default tokenized fields to this!

I like the idea given that problems typically arise when loading by mistake field data on fields that are used for full-text search. However, I think it may be a problem for the out-of-the-box experience given that fields are tokenized by default? It's not the first time that I'm a bit torn between usability and production use when deciding on default values. Maybe we could have hints from the user on what he's doing with Elasticsearch in order to pick default values accordingly (just thinking out loud)...

&gt; I am not sure I like calling it none or no, you explained it with the world disable, so maybe just call it disable(d)?

To say the truth, I didn't like it either, I was just out of inspiration. `disabled` sounds much better!
</comment><comment author="jpountz" created="2013-12-13T09:04:50Z" id="30495256">@s1monw @kimchy I just pushed a new commit based on your comments.
</comment><comment author="s1monw" created="2013-12-13T11:11:37Z" id="30502175">LGTM
</comment><comment author="kimchy" created="2013-12-16T13:21:28Z" id="30659773">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add the ability to disallow field data loading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4431</link><project id="" key="" /><description>Field data can be a black hole for memory (eg. on full-text fields), so it should be possible to disallow field data loading on a per-field basis.
</description><key id="24180117">4431</key><summary>Add the ability to disallow field data loading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-12T14:05:27Z</created><updated>2014-02-21T09:01:10Z</updated><resolved>2013-12-16T13:56:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-12T15:07:18Z" id="30429702">huge +1 to the feature :)
</comment><comment author="spinscale" created="2013-12-12T15:10:08Z" id="30429962">nice screw-up protection. what about documentation?
</comment><comment author="jpountz" created="2013-12-12T16:57:53Z" id="30440354">I just added a new commit with documentation. Shame on me for having forgotten it!
</comment><comment author="CamiloSierraH" created="2014-02-20T16:43:13Z" id="35641572">i think so there is a problem in the documentation wich says that "The disabled format is supported by all field types." it works for string, long but not for nested ! (i use ES 0.9.11)
bug or documentation not updated ?
</comment><comment author="jpountz" created="2014-02-20T17:07:13Z" id="35644266">There is no field data concept for the nested type so indeed the documentation should rather state that it is supported on all **core** types. I will fix it soon, thanks!
</comment><comment author="jpountz" created="2014-02-20T23:27:55Z" id="35682338">@CamiloSierraH to be sure I got you right, what were your expectations when setting the field data format on a nested field?
</comment><comment author="CamiloSierraH" created="2014-02-21T08:39:49Z" id="35708524">@jpountz  i have some nested that i like disabled, i only use this nested in filters (no facets, no sort) because i have some millions of documents and i need to use the ram only if is necessary!

Thanks
</comment><comment author="jpountz" created="2014-02-21T08:52:55Z" id="35709207">@CamiloSierraH In that case you should use the `disabled` format on the fields that are under the nested field, for example:

``` javascript
{
    "type1" : {
        "properties" : {
            "obj1" : {
                "type" : "nested",
                "properties": {
                    "prop1": {
                        "type": "long",
                        "fielddata": {
                             "format": "disabled"
                         }
                    }
                }
            }
        }
    }
}
```

**Note**: `disabled` is used on `obj1.prop1`, not `obj1`.
</comment><comment author="CamiloSierraH" created="2014-02-21T09:01:10Z" id="35709794">ok thank you @jpountz !! (i hope i can release some memory ram :+1: )
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/test/java/org/elasticsearch/index/fielddata/DisabledFieldDataFormatTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file></files><comments><comment>Make field data changes immediately taken into account and add the ability to disallow field data loading.</comment></comments></commit></commits></item><item><title>Take into account changes in field data settings on live indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4430</link><project id="" key="" /><description>AbstractFieldMapper.merge has the logic to handle changes of the field data configuration (in particular the format). However, these changes are not going to be taken into account until the field data cache is cleared. It should be possible to update the field data settings on a live index so that field data which has been loaded for the previous segments remains alive while new segments will be loaded using the new field data configuration. As time goes and merges happen, all segments will eventually be using the new format but the transition would be smoother than by clearing the field data cache and regenerating all entries on the next request.
</description><key id="24180049">4430</key><summary>Take into account changes in field data settings on live indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-12T14:04:09Z</created><updated>2013-12-16T14:49:51Z</updated><resolved>2013-12-16T13:56:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/MapperService.java</file><file>src/test/java/org/elasticsearch/index/fielddata/DisabledFieldDataFormatTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/IndexFieldDataServiceTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/MapperTestUtils.java</file><file>src/test/java/org/elasticsearch/index/search/child/ChildrenConstantScoreQueryTests.java</file></files><comments><comment>Make field data changes immediately taken into account and add the ability to disallow field data loading.</comment></comments></commit></commits></item><item><title>Log JVM stdout on service start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4429</link><project id="" key="" /><description>Based on http://stackoverflow.com/questions/8251933/how-can-i-log-the-stdout-of-a-process-started-by-start-stop-daemon

Closes #4428
</description><key id="24170174">4429</key><summary>Log JVM stdout on service start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">recastrodiaz</reporter><labels><label>:Packaging</label></labels><created>2013-12-12T10:53:50Z</created><updated>2014-12-24T17:31:04Z</updated><resolved>2014-12-24T17:31:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="obazoud" created="2014-01-30T10:56:09Z" id="33677680">+1
but should be INITD_LOG_FILE=$LOG_DIR/elasticsearch.init.d.log
</comment><comment author="nik9000" created="2014-02-24T17:10:21Z" id="35909440">+1  This bit me a week ago.
</comment><comment author="spinscale" created="2014-03-03T15:12:17Z" id="36518751">Hey folks,

I took your PR as a base and added support for all the packages and startup system we currently support. Can you take a look and tell if everything is like you need it to be? That would be awesome! Thanks a lot for your support and patience!

https://github.com/elasticsearch/elasticsearch/pull/5317
</comment><comment author="clintongormley" created="2014-12-24T17:31:04Z" id="68065713">Closing in favour of #5317
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Log JVM stdout on service start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4428</link><project id="" key="" /><description>I spent a few hours yesterday trying to figure out why elastic search service wouldn't start on my Ubuntu 12.04 machine just to find out that I made a typo:

``` SH
ES_HEAP_SIZE=8gb
# instead of
ES_HEAP_SIZE=8g
```

It would have been much easier to find the root of the problem if the start-stop-daemon logged the stdout of the JVM it launches
</description><key id="24170121">4428</key><summary>Log JVM stdout on service start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">recastrodiaz</reporter><labels /><created>2013-12-12T10:52:34Z</created><updated>2014-12-24T17:31:01Z</updated><resolved>2014-12-24T17:31:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:31:01Z" id="68065712">Closing in favour of #5317
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove IndexFieldData.getHighestNumberOfSeenUniqueValues().</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4427</link><project id="" key="" /><description>Closes #4426
</description><key id="24169415">4427</key><summary>Remove IndexFieldData.getHighestNumberOfSeenUniqueValues().</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-12T10:38:09Z</created><updated>2014-07-11T18:54:13Z</updated><resolved>2013-12-12T14:45:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-12T11:25:09Z" id="30408976">+1 Looks good to me.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Remove IndexFieldData.getHighestNumberOfSeenUniqueValues()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4426</link><project id="" key="" /><description>Although this method may look convenient in order to size data-structures when a request comes in, this value can only grow due to the way it is implemented: when a new field data instance is loaded, what basically happens is something like `highestNumberOfSeenUniqueValues = max(highestNumberOfSeenUniqueValues, atomicFieldData.getNumberUniqueValues());`.

So for example if you index lots of data into Elasticsearch and then remove most of it, this value will be highly over-estimated.

I think a better way to solve this issue would be to compute this information on a per-request basis by iterating over the atomic readers wrapped by the context index searcher. Although this might sound expensive, this is very likely to be very cheap compared to the query execution: there are usually less than 50 segments in an index while queries can easily match thousands or millions of documents. And it also has some advantages compared to the current approach:
- this number will be accurate,
- this number will only take into account segments that are wrapped by the searcher while something implemented at the index field data level could only return a number which is global to all live segments in the index.

This method isn't used currently in master so it will be easy to remove. Regarding 0.90, I plan to deprecate it in order not to break plugins that may rely on that method.
</description><key id="24169327">4426</key><summary>Remove IndexFieldData.getHighestNumberOfSeenUniqueValues()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-12T10:36:27Z</created><updated>2013-12-12T14:46:50Z</updated><resolved>2013-12-12T14:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/fielddata/AbstractIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/test/java/org/elasticsearch/index/fielddata/NoOrdinalsStringFieldDataTests.java</file></files><comments><comment>Remove IndexFieldData.getHighestNumberOfSeenUniqueValues().</comment></comments></commit></commits></item><item><title>Fix parsing of file based template loading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4425</link><project id="" key="" /><description>We support three different settings in templates
- "settings" : { "index" : { "number_of_shards" : 12 } }
- "settings" : { "index.number_of_shards" : 12 }
- "settings" : { "number_of_shards" : 12 }

The latter one was not supported by the fix in #4235

This commit fixes this issue and uses randomized testing to test any of the three cases above when running integration tests.

Closes #4411
</description><key id="24166439">4425</key><summary>Fix parsing of file based template loading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-12T09:37:45Z</created><updated>2014-06-29T01:32:59Z</updated><resolved>2013-12-17T13:49:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-17T13:32:09Z" id="30750537">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>dynamic templates not handled properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4424</link><project id="" key="" /><description>I've set up dynamic templates in config/default-mapping.json, and have new index created for each day. Occasionally, definitions in config/default-mapping.json are not used properly. The file has the following content:

```
{
    "_default_" : {
        "dynamic_templates" : [
            {
                "template_1" : {
                    "match" : "http_domain",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            },
            {
                "template_2" : {
                    "match" : "client",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            },
            {
                "template_3" : {
                    "match" : "username",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            },
            {
                "template_4" : {
                    "match" : "ids_sig_text",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            },
            {
                "template_5" : {
                    "match" : "reason",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            },
            {
                "template_6" : {
                    "match" : "action",
                    "mapping" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            }
        ]
    }
}
```

In most cases, the definitions are handled properly, but for some days, the above fields are split into terms which should not happen. This also happened for today and when querying the index mapping, I saw the following strange result:

 curl -XGET 'http://zapata.ad4.seb.net:9200/rsyslog-2013-12-12/_mapping?pretty=true'

```
{
  "rsyslog-2013-12-12" : {
    "logs" : {
      "dynamic_templates" : [ {
        "template_1" : {
          "mapping" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "match" : "wwwdomain"
        }
      } ],

...
...
    "events" : {
      "dynamic_templates" : [ {
        "template_1" : {
          "mapping" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "match" : "wwwdomain"
        }
      }, {
        "template_2" : {
          "mapping" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "match" : "client"
        }
      }, {
        "template_3" : {
          "mapping" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "match" : "username"
        }
      }, {
        "template_4" : {
          "mapping" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "match" : "ids_sig_text"
        }
      }, {
        "template_5" : {
          "mapping" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "match" : "reason"
        }
      }, {
        "template_6" : {
          "mapping" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "match" : "action"
        }
      } ],

...
...
```

As you can see, the mappings are different for "logs" and "events" although usually they are one and the same. Also "wwwdomain" is a field which I removed more than a month ago from my default-mapping.json file, and it is not used as a fieldname in any of my documents anymore.

I have seen the same behavior both for elasticsearch 0.90.3 and 0.90.7. What puzzles me most is why is a deleted configuration file entry reappearing in mappings, even though it was last used more than a month ago with 0.90.3, while I am currently running 0.90.7. I have checked all my elasticsearch config files with grep and wwwdomain does not appear anywhere.

regards,
risto
</description><key id="24164136">4424</key><summary>dynamic templates not handled properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ristov</reporter><labels><label>feedback_needed</label></labels><created>2013-12-12T08:45:20Z</created><updated>2014-12-29T11:04:09Z</updated><resolved>2014-12-29T11:04:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-13T16:00:46Z" id="30519987">The default-mapping.json is for mappings, but not for templates.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-mapper.html#_dynamic_default_mappings

Are you searching for this functionality?

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html#config

Just making sure, you are not mixing stuff up...
</comment><comment author="ristov" created="2013-12-23T12:02:54Z" id="31115710">I meant mappings. For example, the field 'client' corresponds to a DNS name in my setup. Normally, names like 

workstation1.domain.example.com
server2.domain.example.com

are not supposed to be split into terms with my configuration. However, about once per 2-3 weeks this happens for some index (I am creating a new index in the beginning of each day). For example, today I am seeing DNS names split into terms like

example.com
domain
workstation1
server2
</comment><comment author="clintongormley" created="2014-12-24T17:29:31Z" id="68065629">Hi @ristov 

Sorry it has taken so long to look at this. Given your description of how old fields started appearing, it sounds to me like you had an out of date config file lying around on one of your nodes.

Are you still seeing this issue (with a recent version of Elasticsearch)? or can I close this ticket?
</comment><comment author="ristov" created="2014-12-27T18:26:39Z" id="68186125">Clinton,
you can safely close this ticket, since I've done several upgrades since last December, and have not observed this issue with more recent elasticsearch versions.
kind regards,
risto
</comment><comment author="clintongormley" created="2014-12-29T11:04:09Z" id="68248762">thanks @ristov 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Some mappers are missing from MapperBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4423</link><project id="" key="" /><description>MapperBuilders misses the following mappers:
- completion
- geo_point
- geo_shape
- parent
- size
- ttl
</description><key id="24163958">4423</key><summary>Some mappers are missing from MapperBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-12T08:40:50Z</created><updated>2013-12-12T08:47:53Z</updated><resolved>2013-12-12T08:47:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/mapper/MapperBuilders.java</file><file>src/main/java/org/elasticsearch/index/mapper/core/CompletionFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoShapeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/ParentFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/SizeFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java</file></files><comments><comment>Add missing mappers to MapperBuilders.</comment></comments></commit></commits></item><item><title>Refactored create index api to make use of the new recently introduced generic ack mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4422</link><project id="" key="" /><description>Refactored create index api to make use of the new recently introduced generic ack mechanism

Closes #4421
</description><key id="24138774">4422</key><summary>Refactored create index api to make use of the new recently introduced generic ack mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-12-11T21:23:40Z</created><updated>2014-06-13T00:24:31Z</updated><resolved>2014-01-06T08:21:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-01-02T10:21:21Z" id="31445153">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move create index api to new acknowledgement mechanism</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4421</link><project id="" key="" /><description>Move create index api to new acknowledgement mechanism introduced in #3786 .
</description><key id="24138278">4421</key><summary>Move create index api to new acknowledgement mechanism</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-11T21:15:43Z</created><updated>2014-01-06T08:21:16Z</updated><resolved>2014-01-06T08:21:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexClusterStateUpdateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexResponse.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/cluster/ClusterModule.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/NodeIndexCreatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/main/java/org/elasticsearch/indices/cluster/IndicesClusterStateService.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file><file>src/test/java/org/elasticsearch/cluster/ack/AckTests.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file></files><comments><comment>Refactored create index api to make use of the new recently introduced generic ack mechanism</comment></comments></commit></commits></item><item><title>UPDATE Feature Request: Return old source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4420</link><project id="" key="" /><description>So I was wondering if it would be possible to add a feature to UPDATE for returning the old source of a document that exist before the update is applied. This would be useful in scenarios where you want to update a document and possibly take additional actions if the update caused certain values in the document to change. The current alternative is to do a GET operation before the update, but then you can run into possible timing issues. Not mention you take the additional network call to perform the GET.
</description><key id="24134003">4420</key><summary>UPDATE Feature Request: Return old source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jm4games</reporter><labels><label>feedback_needed</label></labels><created>2013-12-11T20:08:50Z</created><updated>2015-08-26T14:37:55Z</updated><resolved>2015-08-26T14:37:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Plasma" created="2014-01-10T21:54:20Z" id="32070215">+1 would like this feature right now for the same use case
</comment><comment author="bleskes" created="2015-03-27T10:19:07Z" id="86890958">It's an old issue, but it feels to me that a nice solution for this is to allow the script in a scripted update to return values that will be added to the response. This would allow you to make smarter about the change. The down side is that it introduces overhead if you really just want the original doc. @Plasma maybe you can comment?
</comment><comment author="jpountz" created="2015-08-26T14:37:55Z" id="135042943">I don't think we should make the update API more complicated. It is easy to get the previous source by issuing a GET first, then running the update on the same version of the document and retrying in case of conflict.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added new IndicesLifecycle.Listener method that allows to listen for any shard state change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4419</link><project id="" key="" /><description>Added new `IndicesLifecycle.Listener` method that allows to listen for any shard state change

Closes #4413
</description><key id="24132891">4419</key><summary>Added new IndicesLifecycle.Listener method that allows to listen for any shard state change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-11T19:52:47Z</created><updated>2014-06-27T04:19:38Z</updated><resolved>2013-12-16T14:24:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-12-12T09:11:07Z" id="30400246">Hey Luca,

Thanks for helping out -- looks good (and I love the test). Left some comments...
</comment><comment author="javanna" created="2013-12-12T18:28:30Z" id="30448156">I just updated the PR according to the reviews. Left a couple of new comments/questions, as soon as we sort those out I think this is ready to be pushed.
</comment><comment author="dakrone" created="2013-12-16T06:57:25Z" id="30639532">This looks good to me, +1
</comment><comment author="bleskes" created="2013-12-16T10:32:34Z" id="30650513">Left some minor comments. Looks good (and I still love the test :) )
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>searching across multiple types returns doesn't find all documents matching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4418</link><project id="" key="" /><description>I include a bash script that recreates the situation.

```
#!/bin/sh

curl -XDELETE "http://localhost:9200/test"
curl -XPUT "http://localhost:9200/test"

echo

curl -XPUT "http://localhost:9200/test/foo/_mapping" -d '{
    "foo" : { 
        "properties" : {
            "id": {
                "type" : "multi_field",
                "path": "full",
                "fields" : {
                    "foo_id_in_another_field" : {"type" : "long", include_in_all:false },
                "id" : {"type" : "long"}
                   }
        }
    }
    }
}'

echo

#foo is a basically a duplicate of the foo document to support search use cases
curl -XPUT "http://localhost:9200/test/bar/_mapping" -d '{
    "bar" : {
        "properties" : {
            "id": {
                "type" : "multi_field",
                "path": "full",
                "fields" : {
                    "bar_id_in_another_field" : {"type" : "long", include_in_all:false },
                    "id" : {"type" : "long"}
                   }
                },
        "foo": {
            "properties": {
                "id": {
                    "type" : "multi_field",
                    "path": "full",
                    "fields" : {
                        "foo_id_in_another_field" : {"type" : "long", include_in_all:false },
                        "id" : {"type" : "long"}
                    }
                }
            }
        }
        }
    }
}'

echo

curl -XPUT "http://localhost:9200/test/foo/1?refresh=true" -d '{
    "foo": {
                "id": 1
    }
}'

echo

#failure case appears even when not including the following JSON
# "bar": {
#   "id": 2,
#   "foo": {
#     "id": 3
#   }
# }
curl -XPUT "http://localhost:9200/test/bar/2?refresh=true" -d '{
    "bar": {
        "id": 2
    }
}'

echo

#expect two results, get one (FAIL)
curl -XPOST "http://localhost:9200/test/foo,bar/_search?pretty=true" -d '{
  "size": 10,
  "query": {
    "query_string": {
      "query": "foo.id:1 OR bar.id:2"
    }
  }
}'

echo

#except one result, get one (PASS)
curl -XPOST "http://localhost:9200/test/bar/_search?pretty=true" -d '{
  "size": 10,
  "query": {
    "query_string": {
      "query": "foo.id:1 OR bar.id:2"
    }
  }
}'

echo

#expect one result, get one result (PASS)
curl -XPOST "http://localhost:9200/test/foo/_search?pretty=true" -d '{
  "size": 10,
  "query": {
    "query_string": {
      "query": "foo.id:1 OR bar.id:2"
    }
  }
}'

echo

#expect two results, get tow results (PASS)
curl -XPOST "http://localhost:9200/test/_search?pretty=true" -d '{
  "size": 10,
  "query": {
    "query_string": {
      "query": "foo.id:1 OR bar.id:2"
    }
  }
}'
```
</description><key id="24131647">4418</key><summary>searching across multiple types returns doesn't find all documents matching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">billumina</reporter><labels /><created>2013-12-11T19:36:28Z</created><updated>2014-09-11T15:59:34Z</updated><resolved>2014-09-11T15:59:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="billumina" created="2014-09-11T15:59:34Z" id="55285892">Same as https://github.com/elasticsearch/elasticsearch/issues/4081
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Elasticsearch goes into infinite loop with long database names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4417</link><project id="" key="" /><description>The following command:

`curl -XPUT http://localhost:9200/test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack`

put my elasticsearch into an infinite loop, writing the following lines over and over to `elasticsearch.log`:

```
[2013-12-11 18:36:04,630][WARN ][cluster.action.shard     ] [Payback] [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] sending failed shard for [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1], node[8vmR12rFRp2FI3EA-icfrw], [P], s[INITIALIZING], indexUUID [zQIlZKm3R1C1blwKNxFWJg], reason [Failed to create shard, message [IndexShardCreationException[[test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] failed to create shard]; nested: IOException[File name too long]; ]]

[2013-12-11 18:36:04,630][WARN ][cluster.action.shard     ] [Payback] [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] received shard failed for [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1], node[8vmR12rFRp2FI3EA-icfrw], [P], s[INITIALIZING], indexUUID [zQIlZKm3R1C1blwKNxFWJg], reason [Failed to create shard, message [IndexShardCreationException[[test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] failed to create shard]; nested: IOException[File name too long]; ]]

[2013-12-11 18:36:04,638][WARN ][indices.cluster          ] [Payback][test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][2] failed to create shard org.elasticsearch.index.shard.IndexShardCreationException: [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][2] failed to create shard

    at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:347)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:651)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:569)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:414)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.IOException: File name too long
    at java.io.UnixFileSystem.canonicalize0(Native Method)
    at java.io.UnixFileSystem.canonicalize(UnixFileSystem.java:172)
    at java.io.File.getCanonicalPath(File.java:576)
    at org.apache.lucene.store.FSDirectory.getCanonicalPath(FSDirectory.java:129)
    at org.apache.lucene.store.FSDirectory.&lt;init&gt;(FSDirectory.java:143)
    at org.apache.lucene.store.NIOFSDirectory.&lt;init&gt;(NIOFSDirectory.java:64)
    at org.elasticsearch.index.store.fs.NioFsDirectoryService.newFSDirectory(NioFsDirectoryService.java:45)
    at org.elasticsearch.index.store.fs.FsDirectoryService.build(FsDirectoryService.java:129)
    at org.elasticsearch.index.store.distributor.AbstractDistributor.&lt;init&gt;(AbstractDistributor.java:35)
    at org.elasticsearch.index.store.distributor.LeastUsedDistributor.&lt;init&gt;(LeastUsedDistributor.java:36)
    at sun.reflect.GeneratedConstructorAccessor16.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)
    at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:345)
    ... 8 more
```

Now you might argue that this is a stupid thing to do, and that an ES server should be protected from the public so it's not a Denial of Service attack. Nevertheless, this happened to me by accident while I was developing a test harness for Haystack's ElasticSearch backend. However I think it would be better to respond gracefully to invalid input, instead of trying to fill up the hard disk with infinite useless logs.
</description><key id="24128208">4417</key><summary>Elasticsearch goes into infinite loop with long database names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">qris</reporter><labels><label>bug</label></labels><created>2013-12-11T18:44:22Z</created><updated>2014-08-13T13:10:55Z</updated><resolved>2014-08-13T13:10:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/indexing/IndexActionTests.java</file></files><comments><comment>Forbid index names over 100 characters in length</comment></comments></commit></commits></item><item><title>Some APIs don't contain information from http status in the body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4416</link><project id="" key="" /><description>When the information is only in the HTTP status code, which is often not communicated to the client code, it can get lost.

For index api which doesn't distinguish between a document being created and re-indexed I propose to add a `created` field to the resulting json:

```
# curl -X PUT localhost:9200/t/i/1?pretty -d '{}'
{
    "ok" : true,
    "_index" : "t",
    "_type" : "i",
    "_id" : "1",
    "_version" : 1,
    "created": true
}

# curl -X PUT localhost:9200/t/i/1?pretty -d '{}'
{
    "ok" : true,
    "_index" : "t",
    "_type" : "i",
    "_id" : "1",
    "_version" : 2,
    "created": false
}
```

If there are any other API endpoints that doesn't replicate the information from http status in the json body add them in this single issue.
</description><key id="24119965">4416</key><summary>Some APIs don't contain information from http status in the body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2013-12-11T16:47:51Z</created><updated>2014-02-21T23:17:39Z</updated><resolved>2014-01-09T16:59:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-12-11T17:50:57Z" id="30343677">Wouldn't the _version field denote whether or not a document was created or updated? Or course, this method would not work with applications that provide their own versioning mechanism.

The bulk API returns the type of action executed (IndexResponse, DeleteResponse, UpdateResponse), perhaps the API should return the action type and not simply created or not (but that might be redundant in a few settings).
</comment><comment author="HonzaKral" created="2013-12-11T18:17:52Z" id="30346075">the `_version` field is only telling when using automatic versioning and even then I wouldn't feel comfortable forcing people to check for version number in order to determine something like that - seems arbitrary and brittle.
</comment><comment author="brusic" created="2013-12-11T18:45:01Z" id="30349604">I agree with you, which is why I added the last sentence/paragraph. I am currently using the _version method, but would love to move away from it.

My point was rather than having "created": true, it should perhaps be something like "type": "created".
</comment><comment author="clintongormley" created="2013-12-17T11:24:15Z" id="30743517">Given that the bulk API now returns an HTTP `status` code with each entry, i think the consistent way to do this would be the same, ie include `status: 201|200` 
</comment><comment author="karmi" created="2013-12-17T13:14:01Z" id="30749423">I vote for `status: 201`, `status: 200`, `status: 404`, etc, i.e. using the numerical HTTP status codes.

The biggest benefit is that we don't have to invent any names and everybody can translate these codes to meaningful values themselves easily, potentially even leveraging existing dictionary in their codebase...
</comment><comment author="dadoonet" created="2013-12-17T14:58:49Z" id="30757255">+1 for `status: 200` etc...
</comment><comment author="HonzaKral" created="2014-01-09T16:59:31Z" id="31953047">Fixed by https://github.com/elasticsearch/elasticsearch/pull/4635
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Distributed percolator incorrect results with auto create type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4415</link><project id="" key="" /><description>If the `.percolator` mapping hasn't been created yet and two or more concurrent percolate index requests are processed, the first request would the `.percolator` type mapping, but the real time percolator listener wouldn't be active, this would result in that the subsequent concurrent requests wouldn't be parsed and kept in memory and would never be included in any percolate api result. 

This issue only occurs when `.percolator` type is created on the fly and _not_ when the mapping for `.percolator` has been defined before hand via put mapping or create index api.
</description><key id="24119870">4415</key><summary>Distributed percolator incorrect results with auto create type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-11T16:46:41Z</created><updated>2013-12-11T16:46:55Z</updated><resolved>2013-12-11T16:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-11T16:46:55Z" id="30337691">Fixed via https://github.com/elasticsearch/elasticsearch/commit/92c32dca9ec9920b395ee650def206198831b2aa
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add _cat/aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4414</link><project id="" key="" /><description>Would be nice to get info about aliases.  Maybe this can go in _cat/indices.
</description><key id="24119258">4414</key><summary>Add _cat/aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-11T16:39:05Z</created><updated>2013-12-16T09:39:25Z</updated><resolved>2013-12-16T09:39:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-14T14:01:16Z" id="30572020">Didnt reload my browser and missed you assigned yourself to it... did a quick impl, which works like this

```
curl "localhost:9200/_cat/alias?v"
alias                     index filter index_routing search_routing
test-alias-filter         test1 *      -             -
test-alias-search-routing test1 -      -             foo,bar
test                      test1 -      -             -
test-alias-routing        test1 -      foo           foo
test-alias-index-routing  test1 -      foo           -
test-two-indices          test1 -      -             -
test-two-indices          test2 -      -             -
```

can push if you want
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/rest/action/RestActionModule.java</file><file>src/main/java/org/elasticsearch/rest/action/cat/RestAliasAction.java</file></files><comments><comment>Cat API: Add endpoint to show aliases</comment></comments></commit></commits></item><item><title>IndicesLifecycle.Listener to support listening for any index shard state change</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4413</link><project id="" key="" /><description>Add new `IndicesLifecycle.Listener` method that allows to listen for any `IndexShardState` internal change.
</description><key id="24112189">4413</key><summary>IndicesLifecycle.Listener to support listening for any index shard state change</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-11T15:03:48Z</created><updated>2013-12-16T14:24:03Z</updated><resolved>2013-12-16T14:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/index/service/InternalIndexService.java</file><file>src/main/java/org/elasticsearch/index/shard/service/InternalIndexShard.java</file><file>src/main/java/org/elasticsearch/indices/IndicesLifecycle.java</file><file>src/main/java/org/elasticsearch/indices/InternalIndicesLifecycle.java</file><file>src/test/java/org/elasticsearch/indices/IndicesLifecycleListenerTests.java</file></files><comments><comment>Added new IndicesLifecycle.Listener method that allows to listen for any IndexShardState internal change.</comment></comments></commit></commits></item><item><title>compilation problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4412</link><project id="" key="" /><description>Someone oversaw something, somewhere: 

a mvn install yields

ERROR] .../src/main/java/jsr166e/ConcurrentHashMapV8.java:[4395,47] error: spliterator() in KeySetView cannot implement spliterator() in Set
[ERROR] return type ConcurrentHashMapSpliterator&lt;K&gt; is not compatible with Spliterator&lt;K&gt;
[ERROR] where K,E are type-variables:
[ERROR] K extends Object declared in class KeySetView
[ERROR] E extends Object declared in interface Set
[ERROR] ...src/main/java/jsr166e/ConcurrentHashMapV8.java:[4453,47] error: spliterator() in ValuesView cannot implement spliterator() in Collection
[ERROR] return type ConcurrentHashMapSpliterator&lt;V&gt; is not compatible with Spliterator&lt;V&gt;
[ERROR] where V,E are type-variables:
[ERROR] V extends Object declared in class ValuesView
[ERROR] E extends Object declared in interface Collection
[ERROR] ...src/main/java/jsr166e/ConcurrentHashMapV8.java:[4541,60] error: spliterator() in EntrySetView cannot implement spliterator() in Set
</description><key id="24107879">4412</key><summary>compilation problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">exercitussolus</reporter><labels /><created>2013-12-11T13:54:43Z</created><updated>2013-12-13T08:54:43Z</updated><resolved>2013-12-13T08:54:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-11T15:34:11Z" id="30330039">hmm did you run `mvn clean`? Also can you provide some insights which JVM / JDK you are using
</comment><comment author="kimchy" created="2013-12-11T15:48:38Z" id="30331428">maybe you are trying to compile with Java 8?
</comment><comment author="exercitussolus" created="2013-12-11T16:44:52Z" id="30337485">Ah right. I was using a Java 8 ( Oracle ) JDK. Compiling with JDK 7 works fine. 
</comment><comment author="kimchy" created="2013-12-13T08:54:43Z" id="30494764">great, thanks for checking!. There is already an issue about it: #3678, so closing this one.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>problem picking up templates in the config/templates directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4411</link><project id="" key="" /><description>I've got it working using the curl query to create a new template.
But when i set it up in the/templates directory it doesnt seem to get picked up at all.
I've seen a bug raised looking similar to this. What is the work around- can i explicitly set the templates path in the config file? Also should i be able to see anything in the logs telling me this the template files are being picked up or not?

Also while looking into this issue i was following the page to see if my templates are created ok : http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html to get an output of all the loaded up templates, And i've found it doesnt seem to work unless i specify the correct template exact name. So calling curl -XGET localhost:9200/_template/temp\* or curl -XGET localhost:9200/_template/ doesnt seem to bring back any output at all.

This was using elasticsearch-0.90.3
</description><key id="24107425">4411</key><summary>problem picking up templates in the config/templates directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">debhelsdon</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-11T13:46:13Z</created><updated>2013-12-19T13:23:21Z</updated><resolved>2013-12-17T13:49:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-12-12T08:15:08Z" id="30396018">Hey,

it seems you have hit this https://github.com/elasticsearch/elasticsearch/issues/4235

This will be fixed in the upcoming 0.90.8 release. If you want to test, you could test elasticsearch 1.0.0beta2 or the current 0.90 branch - would be happy to get feedback
</comment><comment author="spinscale" created="2013-12-12T09:34:41Z" id="30401530">I found another subtle bug, the #4235 works only if you use 

```
"settings" : { "index.number_of_shards" : 12 } }
```

but does not work for the shorter version

```
"settings" : { "number_of_shards" : 12 } }
```

will fix
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java</file><file>src/test/java/org/elasticsearch/cluster/metadata/ToAndFromJsonMetaDataTests.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Fix loading templates in config/ directory</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/cluster/metadata/IndexTemplateMetaData.java</file><file>src/test/java/org/elasticsearch/indices/template/IndexTemplateFileLoadingTests.java</file></files><comments><comment>Fix parsing of file based template loading</comment></comments></commit></commits></item><item><title>Batch processing mapping updates can cause missed merged mappings when batching multiple types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4410</link><project id="" key="" /><description>when we bulk changes, we need to use the same index metadata builder across the tasks, otherwise we might remove mappings erroneously
 also, when we check if we can use a higher order mapping, we need to verify that its for the same mapping type
</description><key id="24106063">4410</key><summary>Batch processing mapping updates can cause missed merged mappings when batching multiple types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-11T13:19:04Z</created><updated>2013-12-11T13:48:53Z</updated><resolved>2013-12-11T13:48:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-11T13:48:53Z" id="30321226">Fixed in 10cdb0ae221a672bbc348140a30454d5aab29554 (master), and 9c1cc954266837f33e6e905632810ccb7169f306 (0.90)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>fuzzy query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4409</link><project id="" key="" /><description>Can the fuzzy query be used in the GET request?

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html

the query returns the right results, while fuzzy cannot be parsed.

$ curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d ' 
{  
    "query" : { 
        "text" : { "user": "kimchy" }
    }  
}'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "2",
      "_score" : 1.0, "_source" : 
{ 
    "user": "kimchy", 
    "postDate": "2009-11-15T14:12:12", 
    "message": "Another tweet, will it be indexed?" 
}
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 0.30685282, "_source" : 
{ 
    "user": "kimchy", 
    "postDate": "2009-11-15T13:12:00", 
    "message": "Trying out Elastic Search, so far so good?" 
}
    } ]
  }
}

$ curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty=true' -d '{
    "fuzzy" : { "user" : "ki" }
}'
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[RJw2f6OwT6eA8yXPxkgvtA][twitter][3]: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"fuzzy\" : { \"user\" : \"ki\" }\n}]]]; nested: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][4]: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"fuzzy\" : { \"user\" : \"ki\" }\n}]]]; nested: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"fuzzy\" : { \"user\" : \"ki\" }\n}]]]; nested: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][2]: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"fuzzy\" : { \"user\" : \"ki\" }\n}]]]; nested: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][1]: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"fuzzy\" : { \"user\" : \"ki\" }\n}]]]; nested: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }]",
  "status" : 400
}
</description><key id="24091586">4409</key><summary>fuzzy query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">fulinlin924</reporter><labels /><created>2013-12-11T08:01:45Z</created><updated>2013-12-11T09:28:26Z</updated><resolved>2013-12-11T09:17:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-11T09:17:23Z" id="30304933">Please use the mailing list for questions.

I guess answer is something like this:

``` sh
$ curl -XGET 'http://localhost:9200/twitter/tweet/search?pretty=true' -d ' 
{
  "query" : { 
    "fuzzy" : { "user" : "ki" }
  }
}'
```
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Include ?pretty=true in PUT requests in README</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4408</link><project id="" key="" /><description>These return JSON objects too, so it's nicer to be able to read them on
multiple lines and get a newline character at the end.
</description><key id="24081738">4408</key><summary>Include ?pretty=true in PUT requests in README</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ongardie</reporter><labels /><created>2013-12-11T02:22:33Z</created><updated>2014-06-28T20:19:09Z</updated><resolved>2014-04-07T10:45:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-11T06:17:59Z" id="30297430">To be honest, I don't think we should merge it.
It does not bring so much value for PUT methods. 

It's only a README and I think that it should be kept as simple as possible.
My 2 cents.
</comment><comment author="ongardie" created="2013-12-11T08:31:35Z" id="30302513">The most annoying thing is that, without pretty, you don't get a newline after the results. So in the best case, your shell prompt ends up on the right side of your terminal after a bunch of JSON; even worse, the lines are so long they easily wrap.

BTW, why does pretty=false omit the trailing newline? And why can't pretty=true be the default, if the goal is to keep the examples as simple as possible?
</comment><comment author="kimchy" created="2013-12-11T08:58:39Z" id="30303941">the REST endpoint is mainly used by code clients, not by shell command lines, and we should not use pretty and add newlines in that case....
</comment><comment author="dadoonet" created="2013-12-11T09:12:23Z" id="30304638">@ongardie I often do my curl recreation scripts like this:

``` sh
curl -XPUT "http://localhost:9200/index/type/1" -d '{
  "foo": "bar"
}'; echo
```
</comment><comment author="javanna" created="2014-04-07T10:45:59Z" id="39716555">Closing as we prefer to keep the REST calls in the README without the `pretty` flag.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Print nice error in bin/elasticsearch if user needs to run Maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4407</link><project id="" key="" /><description>Before, people that cloned the repo and expected to be able to run
bin/elasticsearch would be met with an awful shell error: the shell
interprets Maven variables like ${project.build.finalName} as shell
variables yet can't handle names of the form ${x.y}. This commit
explicitly checks to make sure that Maven has done its substitutions
before continuing; if Maven hasn't been run, it gives a helpful error
message.

Fixes #2954.
</description><key id="24081717">4407</key><summary>Print nice error in bin/elasticsearch if user needs to run Maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ongardie</reporter><labels /><created>2013-12-11T02:21:58Z</created><updated>2014-06-27T04:19:30Z</updated><resolved>2013-12-13T09:34:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-11T06:13:23Z" id="30297288">Ha! I like it! 
</comment><comment author="spinscale" created="2013-12-12T10:09:52Z" id="30403633">Hey,

checked it out, tried to make it a bit simpler, do you think this makes sense to not have such a complex if-statement? Anything I overlooked?

```
IS_PACKAGED_VERSION='${project.name}'
if [ "$IS_PACKAGED_VERSION" != "elasticsearch" ]; then
... barf error here
fi
```
</comment><comment author="dadoonet" created="2013-12-12T11:22:54Z" id="30408590">@spinscale +1
Wondering if we should add it as well to `elasticsearch.bat` which is filtered as well by maven assembly plugin.
</comment><comment author="ongardie" created="2013-12-12T18:28:26Z" id="30448151">@spinscale that's much better, yep. I don't know Maven, so I didn't know there were other variables whose value we could count on. I think you can remove the IS_PACKAGED_VERSION variable too, unless you think it's clearer that way:

```
if [ '${project.name}' != 'elasticsearch' ]; then
... barf error here
fi
```
</comment><comment author="kimchy" created="2013-12-12T22:23:14Z" id="30468201">this is great!, so many users run into this problem, will help getting started with the source much nicer to users
</comment><comment author="spinscale" created="2013-12-13T09:34:39Z" id="30496688">@ongardie pushed to master and 0.90 branches. Thanks a lot!

See https://github.com/elasticsearch/elasticsearch/commit/a4814c2f698bafb50d908a7d0c1e453318ee7f58 and https://github.com/elasticsearch/elasticsearch/commit/c195ed8b7fec0476b4c1a328f1d81807a02791ad
</comment><comment author="ongardie" created="2013-12-13T21:27:54Z" id="30544963">@spinscale cool, thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>"offsets must not go backwards" exception when using index_options=offsets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4406</link><project id="" key="" /><description>to reproduce(on 0.90.8 snapshot):

```
curl -XPOST http://localhost:9200/foobar -d '{ "index": { "number_of_shards": "1", "analysis": { "filter": { "wordDelimiter": { "type": "word_delimiter", "split_on_numerics": "false", "generate_word_parts": "true", "generate_number_parts": "true", "catenate_words": "true", "catenate_numbers": "true", "catenate_all": "false" } }, "analyzer": { "content_analyzer": { "tokenizer": "whitespace", "filter": [ "wordDelimiter" ] } } } } }'

curl -XPOST http://localhost:9200/foobar/foobar/_mapping -d '{ "foobar": { "dynamic": "false", "_all" : { "enabled" : false }, "properties": { "id": { "type": "integer", "index": "not_analyzed", "store": "yes" }, "content": { "type": "string", "analyzer": "content_analyzer", "store": "yes", "term_vector" : "with_positions_offsets", "omit_norms": true, "index_options":"offsets" } } } }'

curl -XPUT http://localhost:9200/foobar/foobar/1 -d '{"id": 1, "content":"a,a b a/b/c"}'
```

yields: 
{"error":"IllegalArgumentException[offsets must not go backwards startOffset=0 is &lt; lastStartOffset=2 for field 'content']","status":500}~

I guess it's a bug on Lucene.
</description><key id="24076983">4406</key><summary>"offsets must not go backwards" exception when using index_options=offsets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-12-11T00:21:48Z</created><updated>2013-12-17T13:23:54Z</updated><resolved>2013-12-17T13:19:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-12-17T13:07:27Z" id="30749005">@s1monw anybody on that? We would really like to start testing the postings highlighter but that's a blocker for us. Talking to you directly since you were usually the one dealing with HL issues ;)
</comment><comment author="jpountz" created="2013-12-17T13:19:17Z" id="30749733">Indeed this bug is in Lucene: `WordDelimiterFilter` [may produce offsets that go backwards](https://issues.apache.org/jira/browse/LUCENE-5111) which is something that the indexing chain prohibits. There is no work-around that I know of so we need to wait for this issue to be fixed in Lucene.
</comment><comment author="lmenezes" created="2013-12-17T13:23:54Z" id="30749993">@jpountz ok. no go for us then :(

thanks anyway :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Documentation for the "stored scripts" feature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4405</link><project id="" key="" /><description>I can't find a way to make a simple MVEL script to work (for a bulk update request).
It's OK if it's embedded in the request, but if it's stored, it doesn't.

I've created a [gist](https://gist.github.com/jlecour/9f4463361b3205130951) with more information about my problem.

This issue is more about the documentation side of things. The current official documentation says very little about update scripts, and even less about stored scripts.
There is no example of a request that will use a basic stored MVEL script.

The test suite doesn't contain any test for stored scripts either.

Is there something out there that would explain a little more how to use stored scripts?

Thanks
</description><key id="24070873">4405</key><summary>Documentation for the "stored scripts" feature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jlecour</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2013-12-10T22:30:11Z</created><updated>2015-10-14T12:07:03Z</updated><resolved>2015-10-14T12:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jlecour" created="2014-02-04T16:21:32Z" id="34076711">My script was the issue and I managed to make it work as a stored script.
The documentation issue remains though.

There is another aspect of this issue : I didn't find a way to know if Elasticsearch has loaded some scripts.
I imagine an API on the master and/or the nodes that would list the loaded scripts. We could use that to verify that a script is present and loaded on all the nodes.
</comment><comment author="clintongormley" created="2014-12-24T17:21:58Z" id="68065278">Hi @jlecour 

Sorry it has taken so long to look at this.  I agree - the documentation for stored scripts is sorely lacking.  In fact the new `script_id` vs `script_file` parameters are not explained under the [Scripting](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#modules-scripting) docs.

We definitely need more and clearer examples here.
</comment><comment author="clintongormley" created="2015-10-14T12:07:03Z" id="148029142">These docs have subsequently been improved. Closing
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Filtering on results of an aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4404</link><project id="" key="" /><description>The case here can best be described by referring to the HAVING clause in SQL.  Basically we need to be able do an aggregation but limit the results of the query based on the value of the aggregation.  For example if I wanted to look at what age bands have an average height greater that 5 feet, my query would look something like this:

```
 "aggs": {
        "genders": {
            "terms": {
                "field": "gender"
            },
            "aggs": {
                "age_groups" : {
                    "range" : {
                        "field" : "age",
                        "ranges" : [
                            { "to" : 5 },
                            { "from" : 5, "to" : 10 },
                            { "from" : 10, "to" : 15 },
                            { "from" : 15}
                        ]
                    },
                    "aggs" : {
                        "avg_height" : { 
                            "avg" : { "field" : "height" } ,
                            "having" : { "from" : 60 }
                        }
                    }
                }
            }
        }
    }
```
</description><key id="24064336">4404</key><summary>Filtering on results of an aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jrick1977</reporter><labels /><created>2013-12-10T20:46:04Z</created><updated>2015-07-20T11:05:56Z</updated><resolved>2014-10-16T13:45:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lusid" created="2014-01-16T07:51:03Z" id="32448286">We also need a way to filter a multi valued aggregate down to a single value so we don't have to get so much data back. We retrieve values like biggest month, smallest month, busiest month, slowest month, etc, and I was hoping to at least be able to do something like below, but there is no "size" field on the histograms, so even though the results are ordered the way I want, I have to get back thousands of results just so I can print a single number on a website.

``` json
{
    "query": { "match_all": {} },
    "aggs": {
        "biggest_month": {
            "date_histogram": {
                "field": "PO.IssuedDate",
                "interval": "month",
                "order": { "sum_of_po_totals": "desc" },
                "format": "yyyy-MM",
                "size": 1  // no size???
            },
            "aggs": {
                "sum_of_po_totals": {
                    "sum": { "field": "PO.IssuedAmount" }
                }
            }
        },
        "smallest_month": {
            "date_histogram": {
                "field": "PO.IssuedDate",
                "interval": "month",
                "order": { "sum_of_po_totals": "asc" },
                "format": "yyyy-MM",
                "size": 1  // no size???
            },
            "aggs": {
                "sum_of_po_totals": {
                    "sum": { "field": "PO.IssuedAmount" }
                }
            }
        },
        "busiest_month": {
            "date_histogram": {
                "field": "PO.IssuedDate",
                "interval": "month",
                "order": { "_count": "desc" },
                "format": "yyyy-MM",
                "size": 1  // no size???
            }
        },
        "slowest_month": {
            "date_histogram": {
                "field": "PO.IssuedDate",
                "interval": "month",
                "order": { "_count": "asc" },
                "format": "yyyy-MM",
                "size": 1  // no size???
            }
        }
    },
    "size": 0
}
```

Better yet, I would love to see the single value aggregations (or something like them) work on multi value aggregations as a filter (which is currently not being used for anything), so something like this:

``` json
{
    "query": { "match_all": {} },
    "aggs": {
        "biggest_month": {
            "max": { "field": "by_month.sum_of_po_totals" }
        },
        "smallest_month": {
            "min": { "field": "by_month.sum_of_po_totals" }
        },
        "busiest_month": {
            "max": { "field": "by_month.count_of_pos" }
        },
        "slowest_month": {
            "min": { "field": "by_month.count_of_pos" }
        },
        "aggs": {
            "by_month": {
                "date_histogram": {
                    "field": "PO.IssuedDate",
                    "interval": "month",
                    "format": "yyyy-MM"
                },
                "aggs": {
                    "sum_of_po_totals": {
                        "sum": { "field": "PO.IssuedAmount" }
                    },
                    "count_of_pos": {
                        "value_count": { "field": "PO.ID" }
                    }
                }
            }
        }
    },
    "size": 0
}
```

Of course, the "having" modifier mentioned originally would also be fantastic for doing range queries, etc on these aggregate results as well, and would also solve our issue if we could use a "max" aggregate in a "having" clause (even though I personally find it a little less intuitive to have it at the bottom of the hierarchy, but I'm guessing it would probably be easier to implement that way since it is still a single valued aggregate operating under a multi-valued bucket aggregate).

No matter what the solution, this is still a pretty massive hole for our needs in an otherwise awesome aggregations framework, which is unfortunate.
</comment><comment author="SeyZ" created="2014-02-13T10:57:30Z" id="34967951">If I well understand this issue, you would like to have the equivalent of the `facet_filter` of the `facets` but for `aggregations`?
</comment><comment author="lusid" created="2014-02-13T16:00:57Z" id="34992894">The `filter` functionality already provides something like `facet_filter`, in that it filters the results prior to aggregating them. http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-filter-aggregation.html

I think what we are looking for is a way to further filter the result of the aggregation after the fact (i.e. the MAX of the SUM).
</comment><comment author="kajal23" created="2014-04-07T22:28:24Z" id="39791689">Also, would it be possible to apply aggregation in the query filter? We need to limit search result based on the aggregation result.  For Example, return all the employees in a department "dep1" whose salary is less than the average salary of a department "dep1".
</comment><comment author="kstachowiak-kcura" created="2014-06-20T22:09:14Z" id="46731984">A filter similar to "having" would be useful, I have been trying to find groups of documents with more than a certain number of matches overall. (i.e. SELECT id ... GROUP BY id HAVING COUNT(id) &gt; 4)
</comment><comment author="xenji" created="2014-07-29T13:12:05Z" id="50473809">Are there any plans to implement such a feature? We really need this for one of our "big data" projects.  Are there any approaches to implement this via a plugin? I would be very thankful for any hints.
</comment><comment author="xenji" created="2014-07-30T07:55:37Z" id="50584891">We are willing to provide financial support for this feature if that helps. Just let me know how to get in contact.
</comment><comment author="hjz" created="2014-07-30T18:12:34Z" id="50656560">+1. This feature is very much needed. Happy to provide resources to help if necessary.
</comment><comment author="flowbehappy" created="2014-08-04T09:20:09Z" id="51034304">Mark. We also want to migrate our database to ES but stopped by this issue.
</comment><comment author="mrdanadams" created="2014-08-11T21:17:30Z" id="51842186">+1
</comment><comment author="boboland" created="2014-08-13T13:35:40Z" id="52048261">+1
</comment><comment author="nicollette" created="2014-08-14T23:37:06Z" id="52258855">+1
</comment><comment author="hiteshagja" created="2014-08-22T09:47:24Z" id="53042528">+1
</comment><comment author="deepak-mohanakrishnan" created="2014-08-26T03:18:59Z" id="53370711">+1
</comment><comment author="headstar" created="2014-08-29T11:25:25Z" id="53864398">+1
</comment><comment author="Jeedey" created="2014-09-02T13:36:09Z" id="54151877">+1
</comment><comment author="ja-ilija" created="2014-09-05T11:18:40Z" id="54613018">+1
</comment><comment author="Schnouki" created="2014-09-08T08:46:03Z" id="54790875">+1 as well.
</comment><comment author="villadora" created="2014-09-10T10:28:57Z" id="55097465">+1
</comment><comment author="jan-molak" created="2014-09-12T14:49:30Z" id="55413686">+1 this functionality would be very useful
</comment><comment author="fehmicansaglam" created="2014-09-14T22:45:45Z" id="55542147">+1
</comment><comment author="aaronlevin" created="2014-09-17T20:27:30Z" id="55954628">+1
</comment><comment author="jswartsel" created="2014-10-06T19:16:14Z" id="58076630">+1
</comment><comment author="brupm" created="2014-10-10T21:57:06Z" id="58721524">:+1: 
</comment><comment author="DanielNill" created="2014-10-10T21:58:28Z" id="58721632">+1
</comment><comment author="colings86" created="2014-10-16T13:45:56Z" id="59363011">Closing in favour of #8110 
</comment><comment author="paullo0106" created="2015-03-16T12:17:46Z" id="81614285">+1
</comment><comment author="ignaciovazquez" created="2015-04-07T16:50:09Z" id="90642904">+1
</comment><comment author="nealvarner" created="2015-04-13T22:49:18Z" id="92523121">+1
</comment><comment author="TROODON" created="2015-05-05T10:33:50Z" id="99027207">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Merging similarities</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4403</link><project id="" key="" /><description>The current code [1] does not allow changing a field's similarity via a mapping update. While it is true that changing a similarity after an index has been created will probably cause havoc on the scoring, developers should have the flexibility to shoot their own foot. :) 

If I were to submit a patch to allow a configurable override of the current behavior, what is the possibility that it would be accepted? Do not want to write code if it will be denied.

[1] https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/core/AbstractFieldMapper.java?source=c#L526-L534
</description><key id="24056557">4403</key><summary>Merging similarities</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels /><created>2013-12-10T18:46:15Z</created><updated>2014-07-22T11:46:40Z</updated><resolved>2014-07-22T11:46:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T11:46:40Z" id="49728279">@brusic Actually,  the index contains all the metadata needed to support any of the built-in similarities.  In Lucene, similarities can be changed on the fly.  We don't support it in Elasticsearch yet, but if you're interested in working on this, we'd definitely be interested.

Closing this issue in favour of #6727.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add new `simple_query_string` query type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4402</link><project id="" key="" /><description>This adds support for Lucene's SimpleQueryParser by adding a new type
of query called the `simple_query_string`. The `simple_query_string`
query is designed to be able to parse human-entered queries without
throwing any exceptions.

Resolves #4159.
</description><key id="24055177">4402</key><summary>Add new `simple_query_string` query type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-12-10T18:25:44Z</created><updated>2014-12-12T16:27:48Z</updated><resolved>2013-12-12T19:31:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-12-12T17:30:39Z" id="30443321">Looks good to me, +1 to push
</comment><comment author="dakrone" created="2013-12-12T19:31:36Z" id="30453723">Merged in 7fb114db and 77fcf713, closing.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Renamed boost name to path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4401</link><project id="" key="" /><description>First commit makes the change in a backwards compatible manner, keeping the name around (Part that will be backported).
Second commit is only for master: removes the boost name and updates the docs accordingly.

Closes #3315
</description><key id="24044889">4401</key><summary>Renamed boost name to path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-10T16:03:30Z</created><updated>2014-06-14T06:07:20Z</updated><resolved>2014-01-09T10:11:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-01-07T09:53:48Z" id="31724962">I think you also need to change the behaviour of BoostFieldMapper to also support a path instead of just name.
</comment><comment author="javanna" created="2014-01-09T10:11:01Z" id="31917337">Closing this PR as it turns out this is not only a rename, but also making the document boost work with a path instead of a simple name (that has max depth 1). That said, we are going to deprecate document boost in #4664.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Fix bug in explain for function_score queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4400</link><project id="" key="" /><description>The explain output for function_score queries with score_mode=max or
score_mode=min was incorrect, returning instead the value of the last
function.  This change fixes this.

For example, for the query:

```
{
  "explain": true,
  "query": {
    "function_score": {
      "query": {
        "match_all": {}
      },
      "score_mode": "max",
      "functions": [
        {
          "boost_factor": 1
        },
        {
          "boost_factor": 3
        },
        {
          "boost_factor": 2
        }
      ]
    }
  }
}
```

Results are returned containing

```
hits: [
  {
   _score: 3,
   ...
   _explanation: {
      value: 2
      ...
  }
]
```

Similarly, if "score_mode": "min" is used, the value in the explanation is still 2.

I would expect the value in _explanation object to be the same as the _score value, and with this patch applied it is.
</description><key id="24036512">4400</key><summary>Fix bug in explain for function_score queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">rboulton</reporter><labels /><created>2013-12-10T14:01:17Z</created><updated>2014-06-25T14:35:29Z</updated><resolved>2013-12-10T17:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-10T14:17:45Z" id="30230132">Looks Good To Me! @brwe do you wanna pull this in?
</comment><comment author="brwe" created="2013-12-10T17:22:29Z" id="30248163">Done, also back ported to 0.90 (4160efe and 1037d07). Thanks a lot for the fix!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Made sure that we never throw IndexMissingException in indices query and filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4399</link><project id="" key="" /><description>It could happen although we internally use IgnoreIndices.MISSING, due to MetaData#concreteIndices contract, which throws IndexMissingException anyway if all requested indices are missing.
In case all the indices specified in the query/filter are missing, we just execute the no_match query/filter, no need to throw any error.

Closes #3428
</description><key id="24033810">4399</key><summary>Made sure that we never throw IndexMissingException in indices query and filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-12-10T13:09:27Z</created><updated>2014-06-27T04:19:23Z</updated><resolved>2013-12-11T21:30:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-10T14:20:40Z" id="30230341">LGTM in general added a small comment
</comment><comment author="javanna" created="2013-12-10T16:09:10Z" id="30240708">Thanks @s1monw I just updated the PR according to your previous comment (making `concreteIndices` final).
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Packaging: Ensure setting of sysctl vm.max_map_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4398</link><project id="" key="" /><description>In order to be sure that memory mapped lucene directories are working
one can configure the kernel about how many memory mapped areas
a process may have. This setting ensure for the debian and redhat initscripts
as well as the systemd startup, that this setting is set high enough.

Closes #4397
</description><key id="24032121">4398</key><summary>Packaging: Ensure setting of sysctl vm.max_map_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-10T12:42:34Z</created><updated>2014-06-15T15:50:07Z</updated><resolved>2013-12-11T08:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Startup scripts: Set vm.max_map_count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4397</link><project id="" key="" /><description>All the startup scripts in the packages (init and systemd scripts!) should set the above config parameter in order to support the newly default `mmapfs` setting as good as possible.
</description><key id="24023200">4397</key><summary>Startup scripts: Set vm.max_map_count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-10T09:43:25Z</created><updated>2015-03-18T13:16:53Z</updated><resolved>2013-12-11T08:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="recastrodiaz" created="2013-12-12T15:22:15Z" id="30431105">Running https://github.com/spinscale/elasticsearch/commit/81e13a870b112ad2921e1627bb3aabab48bf763c#diff-3866fe1739aff326f455c3ec4473ab48R160 on Ubuntu 12.04 throws the following error:

``` SH
~$ sudo service elasticsearch start
 * Starting ElasticSearch Server                                                                                                                                                                                                             error: Unknown parameter "-qw"
usage:  sysctl [-n] [-e] variable ...
        sysctl [-n] [-e] [-q] -w variable=value ...
        sysctl [-n] [-e] -a
        sysctl [-n] [-e] [-q] -p &lt;file&gt;   (default /etc/sysctl.conf)
        sysctl [-n] [-e] -A
```

Should it be

``` SH
sysctl -q -w vm.max_map_count=65535
```

instead?
</comment><comment author="spinscale" created="2013-12-12T16:47:34Z" id="30439330">yeah, tested it with current debian and ubuntu distros.. which obviously handle `-qw` in one parameter..

will fix it
</comment><comment author="recastrodiaz" created="2013-12-12T17:01:48Z" id="30440711">cheers!
</comment><comment author="spinscale" created="2013-12-12T17:09:57Z" id="30441438">pushed the change... thx a lot for testing!
</comment><comment author="Yashin-s" created="2015-03-18T13:16:53Z" id="82965951">Hello, I am running elasticsearch on a VPS centOS server and I am getting this issue as well. Elasticsearch is starting well but with the error. I can prevent the error by commenting out  vm.max_map_count setting but my service runs only for a couple of hours or minutes. Is it because the default value which is 65530 ( instead of 262144  ) is too weak? I do not have permission to change this value, what can I do to prevent elasticsearch from crashing, even if it is iddle?
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Packaging: Ensure setting of sysctl vm.max_map_count</comment></comments></commit></commits></item><item><title>Unified default ack timeout to 30 seconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4396</link><project id="" key="" /><description>Increased also default publish state timeout to 30 seconds (from 5 seconds) and introduced constant for it.
Introduced AcknowledgedRequest.DEFAULT_ACK_TIMEOUT constant (30 seconds).
Removed misleading timeout default values coming from the REST layer.
Removed (in a bw compatible manner) the timeout support in put/delete index template as the timeout parameter was ignored.

Closes #4395
</description><key id="24023132">4396</key><summary>Unified default ack timeout to 30 seconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-12-10T09:42:09Z</created><updated>2014-07-11T08:56:41Z</updated><resolved>2013-12-11T20:01:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-10T16:23:07Z" id="30242073">I would change the title to reflect the main change, which is `30s`, and the nice constant as a result of it. Also, I would change the publish state timeout (which is the main reason for this change as well) to `30s` - which is a better default than 5s to allow for better throttling for slow nodes.
</comment><comment author="javanna" created="2013-12-10T16:35:02Z" id="30243343">Updated PR title and description, also updated my commit to increase the publish state timeout too (and introduced constant for it too).
</comment><comment author="kimchy" created="2013-12-11T15:55:44Z" id="30332147">looks good!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Unify default ack timeout to 30 seconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4395</link><project id="" key="" /><description>The default ack timeout currently is 10 seconds in all apis but delete index (60 seconds). The timeout value is spreaded in different places, and sometimes has different defaults coming from the REST layer.

The goal of this issue is to have a unique constant containing the default ack timeout, and change the default to 30 seconds. Also, change the publish state timeout to 30s - which is a better default than 5s to allow for better throttling for slow nodes.
</description><key id="24022915">4395</key><summary>Unify default ack timeout to 30 seconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-10T09:37:44Z</created><updated>2013-12-11T20:01:26Z</updated><resolved>2013-12-11T20:01:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequest.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequest.java</file><file>src/main/java/org/elasticsearch/discovery/Discovery.java</file><file>src/main/java/org/elasticsearch/discovery/local/LocalDiscovery.java</file><file>src/main/java/org/elasticsearch/discovery/zen/publish/PublishClusterStateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/delete/RestIndexDeleteAliasesAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/alias/put/RestIndexPutAliasAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/create/RestCreateIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/delete/RestDeleteIndexAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/template/delete/RestDeleteIndexTemplateAction.java</file><file>src/main/java/org/elasticsearch/rest/action/admin/indices/template/put/RestPutIndexTemplateAction.java</file></files><comments><comment>Unified default ack timeout to 30 seconds</comment></comments></commit></commits></item><item><title>facet issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4394</link><project id="" key="" /><description>I have following data in my index

Action : "added"  Product_ID : 123
Action : "added"  Product_ID : 124
Action : "removed"  Product_ID : 124
Action : "purchased"  Product_ID : 123
Action : "added"  Product_ID : 123

I want to count number of products added or removed by product_ID wise.
e.g  for Product_ID : 123  added  = 2  and removed =0.
I can achieve this by firing two separate query for added and removed respectively using facets.

How can i get above output using single query? 
</description><key id="24020587">4394</key><summary>facet issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abhiage</reporter><labels /><created>2013-12-10T08:45:18Z</created><updated>2013-12-10T18:06:26Z</updated><resolved>2013-12-10T08:52:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow to provide parameters not only through -D</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4393</link><project id="" key="" /><description>It would be much nicer if, on top of `-D`, we could pass parameters to elasticsearch shell using something like `--`, for example, `elasticsearch --path.data=/some/path`
</description><key id="24020168">4393</key><summary>Allow to provide parameters not only through -D</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-10T08:34:36Z</created><updated>2013-12-17T13:37:30Z</updated><resolved>2013-12-17T09:47:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Allow to provide parameters not only through -D but as long parameters</comment></comments></commit></commits></item><item><title>elasticsearch command to run in foreground by default, -d to daemonize</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4392</link><project id="" key="" /><description>For 1.0, it might be a good chance to change how we run the elasticsearch command by default. Feels like the default should be in the foreground, and we should daemonize only with a `-d`.
</description><key id="24020114">4392</key><summary>elasticsearch command to run in foreground by default, -d to daemonize</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-10T08:33:11Z</created><updated>2013-12-19T09:25:19Z</updated><resolved>2013-12-17T09:40:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ongardie" created="2013-12-11T01:43:43Z" id="30286924">Agreed. I was also a bit hesitant to use "-f" initially, since it usually means force.
</comment><comment author="dweiss" created="2013-12-19T09:25:19Z" id="30915164">Oh God, thank you, thank you for this... The background mode by default (without any message to the console) was so confusing.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>Start elasticsearch in the foreground by default</comment></comments></commit></commits></item><item><title>AND operation between same field values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4391</link><project id="" key="" /><description>I have following data in elasticsearch

Action : "added"  Product_ID : 123
Action : "added"  Product_ID : 124
Action : "removed"  Product_ID : 124
Action : "purchased"  Product_ID : 123
Action : "added"  Product_ID : 127

I want to find that Product_IDs which have been "added" AND "purchased".
I have tried several bool queries but it wont work out for me.
what should be query in elastic search? help me out
</description><key id="24017340">4391</key><summary>AND operation between same field values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abhiage</reporter><labels /><created>2013-12-10T07:08:11Z</created><updated>2013-12-10T18:09:46Z</updated><resolved>2013-12-10T07:38:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-10T07:38:04Z" id="30205096">Please use mailing list for questions. See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Missing else?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4390</link><project id="" key="" /><description>[I think there should be an else here](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java#L205)

I think there should be an else here. Otherwise the code inside the if is irrelevant, because the put outside the if will overwrite the results. 
</description><key id="24006728">4390</key><summary>Missing else?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">twinforces</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-10T01:16:56Z</created><updated>2013-12-11T15:32:14Z</updated><resolved>2013-12-10T09:51:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-10T09:40:07Z" id="30211200">Makes sense - I will fix
</comment><comment author="twinforces" created="2013-12-10T16:01:01Z" id="30239931">Thanks Simon! You Rock!
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java</file></files><comments><comment>Do not override thetas if sum is less or equals to `0`</comment></comments></commit></commits></item><item><title>Master build fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4389</link><project id="" key="" /><description>Suite: org.elasticsearch.update.UpdateTests
Completed in 14.16s, 11 tests

Tests with failures (first 3 out of 5):
- org.elasticsearch.search.aggregations.bucket.DoubleTermsTests.singleValuedField_WithSubAggregation
- org.elasticsearch.search.aggregations.bucket.DoubleTermsTests.script_MultiValued_WithAggregatorInherited_WithExplicitType
- org.elasticsearch.indices.mapping.SimpleGetFieldMappingsTests.simpleGetFieldMappingsWithDefaults
</description><key id="23993120">4389</key><summary>Master build fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">florianleibert</reporter><labels /><created>2013-12-09T21:08:40Z</created><updated>2013-12-13T20:21:27Z</updated><resolved>2013-12-09T21:09:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-09T21:09:55Z" id="30173636">thank you for opening this. We are aware of these test failures. I will close this issue since we don't track master build failures via issues. thanks
</comment><comment author="florianleibert" created="2013-12-09T21:12:36Z" id="30173861">How do you guys track them? What's the preferred way for people starting to contribute to ES if one cannot build it :)
</comment><comment author="s1monw" created="2013-12-09T21:16:32Z" id="30174208">@florianleibert we run CI tests on different machines over 300 times a day. We use a randomized testing framework that is very picky so new features like aggregations can introduce failures that are not reproducible. I am afraid you will see failures here and there if you are on a dev branch like master especially if we are landing large features. That is almost unavoidable. Today I added a lot of trace logging on the aggregations stuff and removed an certain `wait` call that caused these failures to trigger certain logs or make certain failures more likely. I guess today was a bad day to run our tests the first time :)
</comment><comment author="florianleibert" created="2013-12-09T21:20:29Z" id="30174541">Thanks so much for the clarification. Makes sense now. I'll use the beta tag. 
</comment><comment author="s1monw" created="2013-12-09T21:23:01Z" id="30174774">I just pushed that 'wait' call back here: https://github.com/elasticsearch/elasticsearch/commit/2dfb1d98f4c5e5309a326301476094cd5af6df73

so master should be stable again! :) 
</comment><comment author="florianleibert" created="2013-12-09T23:50:13Z" id="30186678">The beta tag is unfortunately also broken:

Tests with failures:
- org.elasticsearch.search.aggregations.metrics.AvgTests.testSingleValuedField_WithValueScript
- org.elasticsearch.search.aggregations.metrics.AvgTests.testScript_MultiValued_WithParams

[INFO] JVM J0:     0.72 ..  1545.38 =  1544.65s
[INFO] Execution time total: 25 minutes 45 seconds
[INFO] Tests summary: 394 suites, 3267 tests, 2 errors, 9 ignored (8 assumptions)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 27:08.461s
[INFO] Finished at: Mon Dec 09 13:41:07 PST 2013
[INFO] Final Memory: 12M/118M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.0.14:junit4 (tests) on project elasticsearch: Execution tests of goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.0.14:junit4 failed: /Users/florian/mesosphere/elasticsearch/target/junit4-ant-5938995240590407239.xml:16: There were test failures: 394 suites, 3267 tests, 2 errors, 9 ignored (8 assumptions) -&gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[florian@Macintosh ~/mesosphere/elasticsearch ((v1.0.0.Beta2))]$ 
</comment><comment author="florianleibert" created="2013-12-13T20:16:37Z" id="30540163">Just FYI current master still breaks....

Tests with failures (first 3 out of 4):
- org.elasticsearch.action.suggest.SuggestActionTests.testStopwordsOnlyPhraseSuggest
- org.elasticsearch.action.suggest.SuggestActionTests.testSearchForRarePhrase
- org.elasticsearch.action.suggest.SuggestActionTests.testMarvelHerosPhraseSuggest
</comment><comment author="s1monw" created="2013-12-13T20:21:27Z" id="30540535">@florianleibert how do you actually run the tests? We haven't seen any of those failures on non of our systems. If you run on MacOS it's heavily depending on the JDK. can you tell me if a run with this succeeds:

`mvn clean test -Des.node.mode=local` 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A GeoHashGrid aggregation. Buckets GeoPoints into cells ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4388</link><project id="" key="" /><description>...whose dimensions are determined by a choice of GeoHash resolution.

Added a long-based representation of GeoHashes to GeoHashUtils for faster evaluation in aggregations.
Created new GeoHashCellsAggregator based on LongTermsAggregator but which formats bucket results with GeoHash strings rather than longs.
</description><key id="23991044">4388</key><summary>A GeoHashGrid aggregation. Buckets GeoPoints into cells ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels /><created>2013-12-09T20:35:19Z</created><updated>2014-06-18T03:23:42Z</updated><resolved>2014-01-22T12:08:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2013-12-09T22:21:20Z" id="30179978">Thanks for the comments Clinton - I'll address them here.

&gt; there should be code available already for converting a distance to a geohash length.

There is code for the other way around in GeoUtils. I used it to produce the table that explains how the only options for geohash length (1-12) relate to actual ground coverage. This doc also helps explain:
http://unterbahn.com/2009/11/metric-dimensions-of-geohash-partitions-at-the-equator/

&gt; Are 5, 9 and 11 correct?

Yep, according to existing GeoUtils.geoHashCellWidth/Height. They are also broadly in line with the figures given in the 3rd party link above.

&gt; I'd show an example of the agg using a filtered query &gt; filter &gt; geo_bounding_box in the main query, rather than a filter aggregation.

Not sure I understand the rationale? If not used here, when would there be a more appropriate use of a geo filter on an agg? I'd imagine putting a filter on the query would affect any other aggs/facets that you may want to be independent of any geo "zooms". 

&gt; Can we get back the lat/lon value of the central point in each bucket? Or top-left/bottom-right corners?

I have toyed with the idea of adding centre, top left lat/lons etc as optional extra info in results. The Geogrid can typically have many (1,000s!) of buckets so I wanted to keep things terse by default and stick to just geohashes which have good client support in all languages.
</comment><comment author="jpountz" created="2013-12-09T22:30:29Z" id="30180737">I left some comments but overall this looks very good to me!
</comment><comment author="markharwood" created="2013-12-10T14:41:55Z" id="30232182">&gt; My preference would be to have the same behavior as terms aggs for the moment here (shardSize=requiredSize)

I think that's a bad default and a counting policy that has generated a lot of complaints (see https://github.com/elasticsearch/elasticsearch/issues/1305 )
While we wait for a common policy/heuristic to be developed (I'll open another issue for that) I'd prefer something that isn't quite as bad a default.
</comment><comment author="uboness" created="2014-01-07T01:11:59Z" id="31704808">left a little comment, but other than that +1 on commit.

We should also document somewhere (maybe a todo for now) that once we have computable field data impl, we should move the geo_point-&gt;long computation to that (and skip this computation at aggregation time)
</comment><comment author="markharwood" created="2014-01-22T12:08:41Z" id="33015878">Pushed to master via https://github.com/elasticsearch/elasticsearch/commit/602de046922a42e7f85d0da060cfb784eb863d68
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Compress geo-point field data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4387</link><project id="" key="" /><description>This commit allows to trade precision for memory when storing geo points.
GeoPointFieldMapper now accepts a `precision` parameter that controls the
maximum expected error for storing coordinates. This option can be updated on
a live index with the PUT mapping API.

Default precision is 1cm, which requires 8 bytes per geo-point (50% memory
saving compared to using 2 doubles). With this default precision,
GeoDistanceSearchBenchmark reports times which are 12 to 23% slower than
the previous field data implementation.

Close #4386
</description><key id="23985223">4387</key><summary>Compress geo-point field data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-09T19:08:29Z</created><updated>2014-06-29T04:11:17Z</updated><resolved>2013-12-17T10:30:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-09T19:17:10Z" id="30163756">I think we should keep an option to not compress geopoints and keep it as fast as possible (with the trade-off with memory - as it is now).
I would set default to 3meters as I think most of use cases with geopoints don't go under 10 meters.
If 3 meters precision goes down to 6 bytes, that's seems to me a good default value.
</comment><comment author="jpountz" created="2013-12-12T09:36:53Z" id="30401660">Here is a new iteration of this pull request. Sorry to those who commented on the previous one but I did so many refactorings that I rather squashed the commits to make it more readable. Here are highlights of this new commit:
- the default format for geo points is the double[] array one and is called `array` (as in master) making sure there is no precision loss,
- the compressed field data format is now called `compressed` and the precision option is now part of the field data settings,
- GeoPointFieldMapper was changed to extend AbstractFieldMapper. This allows to automatically benefit from some niceties of AbstractFieldMapper, like ouf-of-the-box support of field data settings updates (through the update mapping API)
- I added duel tests between the array and the compressed field data formats.
- Documentation now includes a table that describes the memory savings given various precisions, and makes it explicit that the precision can be updated on a live index.

However, I kept the default precision to 1cm instead of 3m as suggested by David (which would store geo points on 6 bytes instead of 8). The reason is that, when using 6 bytes, the field data impl is a bit slower because there needs to be some bit packing routines happening under the hood while at 8 bytes, data is stored into 2 int[] arrays. To get some speed back, one needs to configure precision to 1km (which translates to 4 bytes) and two short[] arrays will be used under the hood.
</comment><comment author="dadoonet" created="2013-12-12T11:24:06Z" id="30408795">&gt; However, I kept the default precision to 1cm instead of 3m as suggested by David (which would store geo points on 6 bytes instead of 8). The reason is that, when using 6 bytes, the field data impl is a bit slower because there needs to be some bit packing routines happening under the hood while at 8 bytes, data is stored into 2 int[] arrays. To get some speed back, one needs to configure precision to 1km (which translates to 4 bytes) and two short[] arrays will be used under the hood.

Awesome explanation. It makes a lot of sense! Thanks.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Compress geo-point field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4386</link><project id="" key="" /><description>Today we use doubles in order to encode latitudes and longitudes when loading field data for geo points into memory. This is 16 bytes per geo point.

However, we could take advantage of the fact that values are in a fixed range, and maybe trade some precision for memory. In particular, I've been thinking about using a fixed-length encoding with configurable precision. This precision could be configurable in mappings:

``` javascript
PUT /test
{
    "mappings": {
        "test": {
            "properties": {
                "pin": {
                    "type": "geo_point",
                    "fielddata": {
                      "format": "compressed",
                      "precision": "1cm"
                   }
                }
            }
        }
    }
}
```

Here are some values of the number of bytes needed per geo point depending on the expected precision:

| Precision | Bytes per point | Size reduction |
| --- | --- | --- |
| 1km | 4 | 75% |
| 3m | 6 | 62.5% |
| 1cm | 8 | 50% |
| 1mm | 10 | 37.5% |

I plan to use `1cm` has the default, which is good I think since it would be accurate enough for most use-cases and would require 4 bytes per latitude and longitude, which can be efficiently stored in an `int[]` array, for best speed.

The same encoding could be used to implement doc values support (#4207).

For now, the default format is going to remain exact and based on two double[] arrays, so you need to explicitely opt-in for this format by configuring the field data format in the mappings.
</description><key id="23979395">4386</key><summary>Compress geo-point field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>feature</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-09T17:50:55Z</created><updated>2013-12-18T10:57:50Z</updated><resolved>2013-12-17T10:30:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-10T09:37:30Z" id="30211043">I really like it :) I just wonder what the perf hit is here. In general I am not too worried about Geo perf hits since most common usecase is calculating distances which is dominating anyways and should be done on the top N anyways. Maybe we should allow to still use `float` or `double` arrays?
</comment><comment author="jpountz" created="2013-12-10T09:44:47Z" id="30211532">David had concern too about the potential performance hit so I'm thinking about keeping the old format around so that users can get back to it if they want to. This is easy to do, I'll just try to share code about how to uninvert data, since the logic is essentially the same.
</comment><comment author="s1monw" created="2013-12-10T09:46:32Z" id="30211663">+1 I guess that make it a no-brainer! good stuff man I really like it
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/unit/DistanceUnit.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/IndexFieldDataService.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/AbstractGeoPointIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DisabledIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DocValuesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/DoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FSTBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/FloatArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointCompressedIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayAtomicFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/GeoPointDoubleArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PackedArrayIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/fielddata/plain/PagedBytesIndexFieldData.java</file><file>src/main/java/org/elasticsearch/index/mapper/geo/GeoPointFieldMapper.java</file><file>src/main/java/org/elasticsearch/index/query/GeoBoundingBoxFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoDistanceRangeFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeoPolygonFilterParser.java</file><file>src/main/java/org/elasticsearch/index/query/GeohashCellFilter.java</file><file>src/main/java/org/elasticsearch/index/query/functionscore/DecayFunctionParser.java</file><file>src/test/java/org/elasticsearch/index/fielddata/AbstractFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/fielddata/DuelFieldDataTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoEncodingTests.java</file><file>src/test/java/org/elasticsearch/index/mapper/geo/GeoMappingTests.java</file><file>src/test/java/org/elasticsearch/search/geo/GeoDistanceTests.java</file></files><comments><comment>Compressed geo-point field data.</comment></comments></commit></commits></item><item><title>Support postings highlighter in percolate api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4385</link><project id="" key="" /><description /><key id="23970929">4385</key><summary>Support postings highlighter in percolate api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-09T15:53:17Z</created><updated>2013-12-10T18:10:07Z</updated><resolved>2013-12-09T15:56:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/search/fetch/FetchSubPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file></files><comments><comment>Support postings highlighter in percolate api.</comment></comments></commit></commits></item><item><title>Support postings highlighter in percolate api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4384</link><project id="" key="" /><description>The postings hl now uses a searcher that only encapsulate the view of segment the document being highlighted is in, this should be better than using the top level engine searcher.
</description><key id="23958278">4384</key><summary>Support postings highlighter in percolate api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-12-09T12:40:26Z</created><updated>2015-05-18T23:33:33Z</updated><resolved>2013-12-09T15:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-09T15:22:13Z" id="30140218">Cool, will push it soon. Thanks for looking at it @javanna 
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Adding `force_source` in highlighting to support highlighting in percolator in stored fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4383</link><project id="" key="" /><description>Added the `force_source` option to highlighting that enforces to use of the _source even if there are stored fields. The percolator uses this option to deal with the fact that the MemoryIndex doesn't support stored fields, this is possible b/c the _source of the document being percolated is always present.
</description><key id="23956692">4383</key><summary>Adding `force_source` in highlighting to support highlighting in percolator in stored fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-09T12:08:48Z</created><updated>2015-05-18T23:33:31Z</updated><resolved>2013-12-17T09:20:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-17T09:20:00Z" id="30736274">pushed to master
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Simplify indices stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4382</link><project id="" key="" /><description>Note: This breaks backward compatibility
- Removed clear/all parameters, now all stats are returned by default
- Made the metrics part of the URL
- Removed a lot of handlers
- Added shards/indices/cluster level paremeter to change response serialization
- Returning translog statistics in IndicesStats
- Added TranslogStats class
- Added IndexShard.translogStats() method to get the stats from concrete implementation

Closes #4054 
</description><key id="23954111">4382</key><summary>Simplify indices stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>v1.0.0.RC1</label></labels><created>2013-12-09T11:12:47Z</created><updated>2014-07-05T23:07:16Z</updated><resolved>2014-01-06T06:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-12-10T12:00:12Z" id="30219838">LGTM
</comment><comment author="dakrone" created="2013-12-10T16:55:15Z" id="30245472">@spinscale shouldn't this include the documentation changes since this is a breaking change?
</comment><comment author="spinscale" created="2013-12-11T11:10:23Z" id="30311753">@dakrone totally. changed docs as well
</comment><comment author="otisg" created="2014-01-06T07:01:00Z" id="31630849">@spinscale  does this change the response format for previously returned stats or just parameters?
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Method to get all indices through the api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4381</link><project id="" key="" /><description>It would be nice if it existed an easy way to get all the indices. In our specific scenario we want it to make it easy to automate cleanup of indices we don't use any more.
</description><key id="23950881">4381</key><summary>Method to get all indices through the api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mastoj</reporter><labels /><created>2013-12-09T10:09:58Z</created><updated>2014-02-22T16:15:51Z</updated><resolved>2014-02-22T16:15:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-09T10:11:55Z" id="30119758">does `curl localhost:9200/_stats` help here maybe?
</comment><comment author="mastoj" created="2013-12-09T10:49:17Z" id="30122117">Yes it did help me, but I think this might be something that should be made more explicit. 
</comment><comment author="spinscale" created="2014-02-22T16:15:51Z" id="35806405">you might also want to take a look at curator, a neat helper script to take care of cleaning up indices, see http://www.elasticsearch.org/blog/curator-tending-your-time-series-indices/
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>cluster reroute returns riddles on failures :)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4380</link><project id="" key="" /><description>When issuing a `_cluster/reroute` command and it fails, its quite impossible to understand why it failed.

```
itamar@data001:~$ curl -XPOST 'localhost:9200/_cluster/reroute' -d '{ "commands" : [ { "move" : { "index" : "index1", "shard" : 0, "from_node" : "data006", "to_node" : "data019" } }, { "move" : { "index" : "index2", "shard" : 0, "from_node" : "data019", "to_node" : "data006" } } ] }'

{"error":"RemoteTransportException[[data014][inet[/10.10.5.14:9300]][cluster/reroute]]; nested: ElasticSearchIllegalArgumentException[[move_allocation] can't move [index1][0], from [data006][Lh1y_3zkQWiPjfS7DoSteg][inet[data006/10.10.5.1:9300]]{tag=gen1}, to [data019][12_HJbCyQFGx96iTPEoGxw][inet[data019/10.1.2.2:9300]]{tag=gen1}, since its not allowed, reason: [NO()][YES()][YES()][YES()][YES()][YES()][YES()]]; ","status":400}
```

The `[NO()][YES()][YES()][YES()][YES()][YES()][YES()]]` part is quite annoying do decrypt and isn't documented anywhere. I know each is a decision on allocation, rebalance etc but in case the command can't go through we need to know why. Either make this a more descriptive error or have this documented somewhere. I'd go with the former since it avoids docs becoming stale.

As always, I'll be happy to do this myself if this makes sense to you.
</description><key id="23947897">4380</key><summary>cluster reroute returns riddles on failures :)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2013-12-09T09:07:38Z</created><updated>2014-03-12T20:36:17Z</updated><resolved>2014-03-12T20:36:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-09T11:57:03Z" id="30126003">@synhershko this is on the list. We should add better logging here I agree! I will assign it to me for now
</comment><comment author="s1monw" created="2014-03-12T20:36:17Z" id="37461109">I think we can close this given that #2483 is in
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/TransportClusterRerouteAction.java</file><file>src/main/java/org/elasticsearch/cluster/node/DiscoveryNodeFilters.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/AllocationService.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/RoutingAllocation.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AllocationDeciders.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ClusterRebalanceAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ConcurrentRebalanceAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DisableAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/DiskThresholdDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/EnableAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/FilterAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/NodeVersionAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/RebalanceOnlyWhenActiveAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ReplicaAfterPrimaryActiveAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SameShardAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ShardsLimitAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/SnapshotInProgressAllocationDecider.java</file><file>src/main/java/org/elasticsearch/cluster/routing/allocation/decider/ThrottlingAllocationDecider.java</file></files><comments><comment>Add explanations for all AllocationDeciders</comment></comments></commit></commits></item><item><title>Remove 'term_index_interval' and 'term_index_divisor'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4379</link><project id="" key="" /><description>These settings are no longer relevant since they are codec /
postingsformat level settings since Lucene 4.0

Closes #3912
</description><key id="23930668">4379</key><summary>Remove 'term_index_interval' and 'term_index_divisor'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2013-12-08T20:38:57Z</created><updated>2014-07-16T21:50:45Z</updated><resolved>2013-12-10T15:56:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Add version to plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4378</link><project id="" key="" /><description>Plugin developpers can now add a version number to their es-plugin.properties file:

``` properties
plugin=org.elasticsearch.test.integration.nodesinfo.TestPlugin
version=0.0.7-SNAPSHOT
```

Also, for site plugins, it's recommended to add a `es-plugin.properties` file in root site directory with `description` and `version` properties:

``` properties
description=This is a description for a dummy test site plugin.
version=0.0.7-BOND-SITE
```

When running Nodes Info API, you will get information on versions:

``` sh
$ curl 'http://localhost:9200/_nodes?plugin=true&amp;pretty'
```

``` javascript
{
  "ok" : true,
  "cluster_name" : "test-cluster-MacBook-Air-de-David.local",
  "nodes" : {
    "RHMsToxiRcCXwHiS6mEaFw" : {
      "name" : "node2",
      "transport_address" : "inet[/192.168.0.15:9301]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9201]",
      "plugins" : [ {
        "name" : "dummy",
        "version" : "0.0.7-BOND-SITE",
        "description" : "This is a description for a dummy test site plugin.",
        "url" : "/_plugin/dummy/",
        "site" : true,
        "jvm" : false
      } ]
    },
    "IKiUOo-LSCq1Km1GUhBwPg" : {
      "name" : "node3",
      "transport_address" : "inet[/192.168.0.15:9302]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9202]",
      "plugins" : [ {
        "name" : "test-plugin",
        "version" : "0.0.7-SNAPSHOT",
        "description" : "test-plugin description",
        "site" : false,
        "jvm" : true
      } ]
    },
    "H64dcSF2R_GNWh6XRCYZJA" : {
      "name" : "node1",
      "transport_address" : "inet[/192.168.0.15:9300]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9200]",
      "plugins" : [ ]
    },
    "mGEZcYl8Tye0Rm5AACBhPA" : {
      "name" : "node4",
      "transport_address" : "inet[/192.168.0.15:9303]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9203]",
      "plugins" : [ {
        "name" : "test-plugin",
        "version" : "0.0.7-SNAPSHOT",
        "description" : "test-plugin description",
        "site" : false,
        "jvm" : true
      }, {
        "name" : "test-no-version-plugin",
        "version" : "NA",
        "description" : "test-no-version-plugin description",
        "site" : false,
        "jvm" : true
      }, {
        "name" : "dummy",
        "version" : "NA",
        "description" : "No description found for dummy.",
        "url" : "/_plugin/dummy/",
        "site" : true,
        "jvm" : false
      } ]
    }
  }
}
```

Relative to #2668.
Closes #2784.
</description><key id="23923454">4378</key><summary>Add version to plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-12-08T13:26:22Z</created><updated>2015-06-07T16:26:32Z</updated><resolved>2014-01-30T13:26:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-01-20T15:17:27Z" id="32767501">@dadoonet can we add this change without the `Plugin.java` change and just keep an additional Map in `PluginService` or have a tuple as the value in the existing map? This would make this change simpler and bw compatible? we can also allow pulling the version from the PluginService like

```
class PluginService { 
public String version(Plugin plugin) {
  // lookup the version
}

}
```
</comment><comment author="dadoonet" created="2014-01-20T16:47:09Z" id="32775973">@s1monw PR rebased on master and updated based on your comment. Much much better! Thanks!
Let me know what you think.
</comment><comment author="uboness" created="2014-01-20T17:16:47Z" id="32778692">this is a bit if a mess IMO... why don't we just eagerly build the plugin info when the plugins are loaded, and keep them in the Tuple instead of just holding the version there... the code right now is really inconsistent (_site plugin prop files are loaded on `info()` call, normal plugin prop files are read when the plugin is loaded) and not very extensible (next time when we'd like to add additional meta-data info to the prop files... this impl breaks)
</comment><comment author="dadoonet" created="2014-01-21T08:25:02Z" id="32828930">@uboness Yeah. I'm loading site version in info() because site plugins can be updated on a live cluster without the need to restart the node. Do you think I should not do that?
</comment><comment author="dadoonet" created="2014-01-21T10:29:55Z" id="32836650">@uboness as we cache the plugin list, it does not make sense not caching version for site plugins as well.

Going to modify and simplify this!

Thanks!
</comment><comment author="dadoonet" created="2014-01-30T00:47:24Z" id="33649578">@uboness @s1monw PR updated. Could you review it please? 
</comment><comment author="uboness" created="2014-01-30T04:13:56Z" id="33658974">over all, looking at it now, the use of `Tuple` here creates a bit of a messy code... I'd much rather see and `InternalPlugin` class that implements `Plugin`, wraps a `Plugin` and delegates to it, and also holds a `PluginInfo`. this will eliminate the need to deal with Tuples and will clean up the code. The `PluginInfo` can be defined in the `InternalPlugin` as well (so no need to add an extra class file).

The `InternalPlugin` can also hold the `List&lt;OnModuleReference&gt;`, which will clean up this whole class even further (and simplify the code)

I hope I'm not missing anything (could be that I am), but gut feeling is that it can work quite nicely
</comment><comment author="dadoonet" created="2014-01-30T07:49:23Z" id="33666515">@uboness I applied the first comments (easiest ones :) ). About `InternalPlugin`: does it mean that Plugin authors needs to extend this InternalPlugin (so breaks bwc)?
I don't see how it could work actually.

Also, moving PluginInfo into a subclass will also break bwc between 1.0.0.RC1 and above, right?
</comment><comment author="s1monw" created="2014-01-30T09:18:06Z" id="33671302">all tests pass for me - @dadoonet can you squash and push?
</comment><comment author="dadoonet" created="2014-01-30T13:26:05Z" id="33687072">Pushed in master and 1.x only.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Move MappingMetaData to use internal compressed generic content (prefer SMILE)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4377</link><project id="" key="" /><description>Today, `MappingMetaData` has an internal `CompressedString` that is used to store the json serialized version of the mapping. It would be much better if we had it stored as compressed, but in SMILE format, that will be much more storage efficient.

I think it would make sense to create a `CompressedXContent` class, that would store internally the content compressed as bytes (based on a specific content type if needed). Then, we can use it, in SMILE format, to store the mapping. Also, `CompressedXContent` can include helper methods to convert to json and so on (that can make use of the XContentHelper classes).

Last, for very large mappings, we can have a paged bytes instead of a single byte array in the `CompressedXContent` class.
</description><key id="23916849">4377</key><summary>Move MappingMetaData to use internal compressed generic content (prefer SMILE)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Mapping</label><label>enhancement</label></labels><created>2013-12-08T02:04:49Z</created><updated>2016-08-25T10:05:11Z</updated><resolved>2016-08-24T15:13:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-03-12T20:05:21Z" id="37457276">pushing out to `1.2` @kimchy can we find somebody to take this?
</comment><comment author="jpountz" created="2016-08-24T15:13:58Z" id="242100334">Closing: reasonable mappings are tiny once compressed anyway.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Added REST test suites runner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4376</link><project id="" key="" /><description>The REST layer can now be tested through tests that are shared between all the elasticsearch official clients.
The tests are based on REST specification that can be found on the elasticsearch-rest-api-spec project and consist of YAML files that describe the operations to be executed and the obtained results that need to be tested.

REST tests can be executed through the ElasticsearchRestTests class, which relies on the rest-spec git submodule that contains the rest spec and tests pulled from the elasticsearch-rest-spec-api project.

The following are the options supported by the REST tests runner:
- tests.rest[true|false|host:port]: determines whether the REST tests need to be run and if so whether to rely on an external cluster (providing host and port) or fire a test cluster (default)
- tests.rest.suite: comma separated paths of the test suites to be run (by default loaded from /rest-spec/test). it is possible to run only a subset of the tests providing a sub-folder or even a single yaml file (the default /rest-spec/test prefix is optional when files are loaded from classpath) e.g. -Dtests.rest.suite=index,get,create/10_with_id
- tests.rest.spec: REST spec path (default /rest-spec/api)
- tests.iters: runs multiple iterations
- tests.seed: seed to base the random behaviours on
- tests.appendseed[true|false]: enables adding the seed to each test section's description (default false)
- tests.cluster_seed: seed used to create the test cluster (if enabled)
</description><key id="23912176">4376</key><summary>Added REST test suites runner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-12-07T21:21:20Z</created><updated>2014-06-16T22:59:39Z</updated><resolved>2013-12-17T14:37:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-08T08:58:15Z" id="30077619">Luca this looks awesome. I did a brief review without going into details yet. I will do on monday or so. One think that I wonder is if we could drop the `tests.rest.shuffle` and just use `tests.seed` to initialise a `Random(seed)` that is then passed to `Collections.shuffle(collection, random)`?
</comment><comment author="javanna" created="2013-12-08T12:03:18Z" id="30080210">@s1monw that's what I do by default, but I exposed the option to disable shuffling the tests (true|false default true)...if it's not useful I can remove it.
</comment><comment author="s1monw" created="2013-12-08T15:15:54Z" id="30083502">well disabling it is nothing else than a static seed? that's why I think we can remove it though
</comment><comment author="javanna" created="2013-12-09T09:08:12Z" id="30115830">By disabling it I meant disabling calling shuffle, which would result in having tests executed alphabetically (order they are loaded from file system), you see the difference when running them from an IDE:

```
- bulk/01_....
- bulk/02_...
- create/01_...
etc.
```

But from the IDE you can also reorder tests manually if you want (regardless of their execution order). I agree we can always shuffle and remove the option.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file></files><comments><comment>re-enabled FileUtilsTests and REST tests as rest-api-spec has been added back</comment></comments></commit><commit><files /><comments><comment>merged rest-api-spec repo into es core</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/test/rest/ElasticsearchRestTests.java</file><file>src/test/java/org/elasticsearch/test/rest/junit/RestTestSuiteRunner.java</file><file>src/test/java/org/elasticsearch/test/rest/support/FileUtils.java</file><file>src/test/java/org/elasticsearch/test/rest/test/FileUtilsTests.java</file></files><comments><comment>removed rest-spec submodule and prepared project for same files added directly to the codebase (no submodule) within rest-api-spec</comment></comments></commit></commits></item><item><title>Add an `usage` key to the CPU section of OsStats.toXContent.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4375</link><project id="" key="" /><description>This is just the sum of existing `sys` and `user`

Although it's a very minor change, doing a PR for sanity check of the name `usage`.

Closes #4374
</description><key id="23911228">4375</key><summary>Add an `usage` key to the CPU section of OsStats.toXContent.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels /><created>2013-12-07T20:17:32Z</created><updated>2014-06-30T13:15:45Z</updated><resolved>2013-12-07T20:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-07T20:18:35Z" id="30063753">LGTM!
</comment><comment author="bleskes" created="2013-12-07T20:29:26Z" id="30063993">thx! comited: b7d6cce4c9432db322e1e1c6249151af2dfe0b59
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add a new `usage` metric to CPU stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4374</link><project id="" key="" /><description>The current `os.cpu` section of the `GET _cluster/nodes/stats`, returns `user` and `sys` CPU metrics. For systems that need to show just one number, it's convenient to have one more metric to indicate CPU usage without the distinction.
</description><key id="23911135">4374</key><summary>Add a new `usage` metric to CPU stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-07T20:10:39Z</created><updated>2014-01-13T11:35:18Z</updated><resolved>2013-12-07T20:28:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/monitor/os/OsStats.java</file></files><comments><comment>Add an `usage` key to the CPU section of OsStats.toXContent.</comment></comments></commit></commits></item><item><title>Optimize dynamic mapping updates on master by processing latest one per index/node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4373</link><project id="" key="" /><description>Instead of processing all the bulk of update mappings we have per index/node, we can only update the last ordered one out of those (cause they are incremented on the node/index level). This will improve the processing time of an index that have large updates of mappings.
</description><key id="23909507">4373</key><summary>Optimize dynamic mapping updates on master by processing latest one per index/node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-07T18:19:41Z</created><updated>2013-12-10T18:11:06Z</updated><resolved>2013-12-07T18:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java</file><file>src/main/java/org/elasticsearch/action/index/TransportIndexAction.java</file><file>src/main/java/org/elasticsearch/cluster/action/index/MappingUpdatedAction.java</file><file>src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java</file><file>src/test/java/org/elasticsearch/indices/mapping/UpdateMappingTests.java</file></files><comments><comment>Optimize dynamic mapping updates on master by processing latest one per index/node</comment><comment>Instead of processing all the bulk of update mappings we have per index/node, we can only update the last ordered one out of those (cause they are incremented on the node/index level). This will improve the processing time of an index that have large updates of mappings.</comment><comment>closes #4373</comment></comments></commit></commits></item><item><title>When es start,what does es load?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4372</link><project id="" key="" /><description>when es started ,it seems to load a lot of data into Heap Mem,I want to know what does es load and can I config it lower .
thanks
</description><key id="23901280">4372</key><summary>When es start,what does es load?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Lobster377</reporter><labels /><created>2013-12-07T07:08:43Z</created><updated>2013-12-10T18:11:27Z</updated><resolved>2013-12-07T07:45:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-07T07:45:52Z" id="30049986">Please ask questions on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Query String caching could cause matched_filters not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4371</link><project id="" key="" /><description>PR for #4361.

When searching with a query containing query_strings inside a bool query, the specified _name is randomly missing from the results due to caching.
</description><key id="23899443">4371</key><summary>Query String caching could cause matched_filters not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-07T04:17:05Z</created><updated>2014-06-14T14:45:16Z</updated><resolved>2013-12-07T21:54:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-07T12:00:14Z" id="30053485">Looks good, the bug is indeed related to caching, so it would be great if the pull request title and comment would reflect that.

Also, because its relating to caching, we can simply test for it. For example, index the docs, and execute the search requests with preference set to _primary, and execute it a couple of times, the second one should kick in the caching, and the bug.
</comment><comment author="dadoonet" created="2013-12-07T13:58:40Z" id="30055432">@kimchy PR updated. You're right. Using _primary cause it failing at every run before the patch.
</comment><comment author="kimchy" created="2013-12-07T19:42:56Z" id="30062967">look good, I would imply instead of  duplicating the code, run it in a for loop and do it a couple of times?
</comment><comment author="dadoonet" created="2013-12-07T21:55:51Z" id="30065935">@kimchy done and pushed. Thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Use random number of iteration for tests</comment><comment>Thanks @simonw for the review.</comment><comment>Related to #4361 and #4371.</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Use random number of iteration for tests</comment><comment>Thanks @simonw for the review.</comment><comment>Related to #4361 and #4371.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Query String caching could cause matched_filters not working</comment></comments></commit></commits></item><item><title>[Docs] Document which encoding should be used in order to make sense of the offsets returned by the term vectors API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4370</link><project id="" key="" /><description>Close #4363
</description><key id="23887160">4370</key><summary>[Docs] Document which encoding should be used in order to make sense of the offsets returned by the term vectors API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-12-06T21:41:35Z</created><updated>2014-07-16T21:50:48Z</updated><resolved>2013-12-06T21:53:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-06T21:44:26Z" id="30032121">LGTM
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Apply slop correctly if phrase query is wrapped in a filtered query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4369</link><project id="" key="" /><description>If a phrase query is wrapped in a filtered query due to type filtering
slop was not applied correctly. Also if the default field required a
type filter the filter was not applied.

Closes #4356
</description><key id="23887099">4369</key><summary>Apply slop correctly if phrase query is wrapped in a filtered query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2013-12-06T21:40:27Z</created><updated>2014-06-13T00:56:38Z</updated><resolved>2013-12-06T22:11:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-12-06T21:51:50Z" id="30032641">+1
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add wildcard support to field resolving in the Get Field Mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4368</link><project id="" key="" /><description>Closes #4367

Note that using `curl -XGET "http://localhost:9200/index/type/_mapping/field/*"` will also give you mappings for the root mappers `_all` , `_id` , `_source` etc. 
</description><key id="23885549">4368</key><summary>Add wildcard support to field resolving in the Get Field Mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-12-06T21:12:44Z</created><updated>2014-06-22T16:21:26Z</updated><resolved>2013-12-10T22:48:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-10T09:43:00Z" id="30211409">looks good
</comment><comment author="bleskes" created="2013-12-10T22:48:47Z" id="30276562">Pushed: 99b421925fee14603944f373030c474f6d8e6901
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Allow field wildcards in the Get Field Mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4367</link><project id="" key="" /><description>The Get Field Mappings API currently supports retrieving the mapping for specific fields using their explicit full name, index name or relative name. We should add support to resolve fields using wildcards.
</description><key id="23883644">4367</key><summary>Allow field wildcards in the Get Field Mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-06T20:37:30Z</created><updated>2014-03-10T08:17:17Z</updated><resolved>2013-12-10T22:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/TransportGetFieldMappingsAction.java</file></files><comments><comment>Add wildcard support to field resolving in the Get Field Mapping API</comment></comments></commit></commits></item><item><title>Remove the `field` and `text` queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4366</link><project id="" key="" /><description>The `text` query was replaced by the `match` query and has been
deprecated for quite a while.

The `field` query should be replaced by a `query_string` query with
the `default_field` specified.

Fixes #4033
</description><key id="23880602">4366</key><summary>Remove the `field` and `text` queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>breaking</label><label>v1.0.0.RC1</label></labels><created>2013-12-06T19:46:19Z</created><updated>2014-12-12T16:27:47Z</updated><resolved>2013-12-16T16:09:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-12-13T18:58:13Z" id="30533951">LGTM
</comment><comment author="martijnvg" created="2013-12-16T09:14:29Z" id="30645634">+1 looks good to me as well
</comment><comment author="dakrone" created="2013-12-16T16:09:35Z" id="30672996">Merged in db431b7cb3
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Incorrect indices status result from in-memory node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4365</link><project id="" key="" /><description>Greetings! I'm having some strange behavior (almost certainly a bug) with my in-memory ElasticSearch node that I use for unit testing.
The problem occurs when I'm trying to get the number of present indices with a prepareStatus() call from the Java client.

I'm pasting some code for your convenience:

``` java
import org.elasticsearch.action.admin.indices.status.IndicesStatusResponse;
import org.elasticsearch.client.Client;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.node.Node;
import org.elasticsearch.node.NodeBuilder;
import org.junit.Assert;
import org.junit.Test;

public class ElasticSearchBug {
    @Test
    public void inMemoryTest() {
        ImmutableSettings.Builder settings = ImmutableSettings.settingsBuilder();
        settings.put("path.logs","target/elasticsearch/logs")
            .put("path.data","target/elasticsearch/data")
            .put("gateway.type", "none")
            .put("index.store.type", "memory")
            .put("index.store.fs.memory.enabled", "true")
            .put("index.number_of_shards", 1)
            .put("index.number_of_replicas", 1)
            .put("http.enabled", false).build();

        NodeBuilder nBuilder = NodeBuilder.nodeBuilder().settings(settings).local(true).data(false);

        Node node = nBuilder.node();
        Client client = node.client();

        String testIndex = "test_idx";

        //create an index
        client.admin().indices().prepareCreate(testIndex).execute().actionGet();

        //try to look up index
        boolean isExists = client.admin().indices().prepareExists(testIndex).execute().actionGet().isExists();
        Assert.assertTrue("The newly created index could not be found!", isExists);

        //count the number of indices we have
        IndicesStatusResponse indicesStatusResponse = client.admin().indices().prepareStatus().execute().actionGet();
        int indicesCount = indicesStatusResponse.getIndices().size();

        //it obviously should be at least one
        Assert.assertTrue("Wrong indices status result!", indicesCount &gt; 0);
    }
}
```

Any thoughts? Thanks! 
</description><key id="23878745">4365</key><summary>Incorrect indices status result from in-memory node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phrone</reporter><labels><label>non-issue</label></labels><created>2013-12-06T19:15:25Z</created><updated>2013-12-11T15:31:50Z</updated><resolved>2013-12-10T09:16:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-12-10T01:58:11Z" id="30193102">@phrone index status returns information on per shard basis. In other words at least one shard has to be allocated for the index to be returned. In your test you are creating data-less node, which cannot be used to allocate shards. You can make this test to work by making this change to node creation:

``` java
NodeBuilder nBuilder = NodeBuilder.nodeBuilder().settings(settings).local(true).data(true);
```

and adding wait for index to go to yellow state before asking for status:

``` java
ClusterHealthResponse healthResponse = client.admin().cluster().prepareHealth(testIndex).setWaitForYellowStatus().execute().actionGet();
Assert.assertFalse("Health request timeout!", healthResponse.isTimedOut());
```
</comment><comment author="phrone" created="2013-12-10T09:16:57Z" id="30209840">@imotov Thanks for your time, Igor! Since that is the way things work, I'm closing this as a non-bug :)
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add ability to boost individual text fragments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4364</link><project id="" key="" /><description>@jpountz as we discussed, I am opening a separate ticket for it

An infrastructure for boosting individual text fragments would allow handling of several very useful scenarios. For example:
1. Pre-analyze text to find any text fragments of importance, mark it up with boost instructions and submit to ES for indexing. Field mapping for such field could indicate that it supports embedded boosts and thus content of the field should be analyzed for boost instructions
   `Quick Brown &lt;hint boost="10"&gt;Fox&lt;/hint&gt; Jumped...`
   and boost payloads should be used at search time 
2. Same infrastructure can be used for boosting multifield content based on boost specified for its contributing fields. Same applies to _all field
   please see #4108 for more details
3. Synonyms, stem etc. If you choose to put them together with the original tokens into the same field, you can have stemmer/synonym filter to give derived forms slightly lower boost. That may be an overkill performance wise since the same could be achieved by doing boolean MUST against combined stem/no-stem filed and SHOULD with OR against unstemmed field. I wonder which would be faster and better

BTW, &lt;hint&gt;&lt;/hint&gt; or similar infrastructure could be used for other purposes than boost alone
</description><key id="23873237">4364</key><summary>Add ability to boost individual text fragments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>discuss</label></labels><created>2013-12-06T17:41:43Z</created><updated>2014-07-25T09:29:17Z</updated><resolved>2014-07-25T09:29:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-12-06T17:46:11Z" id="30013853">Neat!  Having just fixed a bug with the FVH and boosting queries I'd argue that highlighting should be made aware of this as well.  Somehow.
</comment><comment author="jpountz" created="2013-12-06T22:03:17Z" id="30033494">Thanks for opening a ticket @roytmana. I think this feature makes a lot of sense!
</comment><comment author="clintongormley" created="2014-07-08T19:52:11Z" id="48391333">I think this feature has been implemented with [delimited_payload_filter ](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-delimited-payload-tokenfilter.html#analysis-delimited-payload-tokenfilter) and access to payloads [via scripts](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-advanced-scripting.html#_term_positions_offsets_and_payloads)

Please reopen if I'm missing something.
</comment><comment author="roytmana" created="2014-07-08T20:04:53Z" id="48392928">@clintongormley 

Thank you for pointing it out. It probably enables desired functionality but on a rather low level
1. translation of payloads to boosts ideally would be done as part of the regular boost implementation so it meshes well with regular boost
2. Ability to strip payload data from the submitted data. When I fetch _source  I would not want payload data in it I would much rather have "the quick fox" than "the|1 quick|2 fox|3"
3. In order to add payload to each individual term I would need to know how it will be tokenized. I would much rather associate the boost with a text fragment and let ES to tokenize the fragment and associate the payload with each token in the fragment
</comment><comment author="clintongormley" created="2014-07-25T09:29:17Z" id="50127503">After discussing this we have decided not to take this issue any further, It makes more sense to split the more important spans of text into separate fields, and to boost those fields.  A lot more efficient than using payloads.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Terms vector API should document the encoding which is used to compute the offsets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4363</link><project id="" key="" /><description>The new term vectors API exposes offsets. However, these offsets have been computed for the UTF-16 encoding, so they are going to look buggy if applied to a string which is not UTF-16-encoded. In particular, I'm thinking that if you are using a language such as Python 3 that uses UTF-8 as a default encoding for strings, you shouldn't use these offsets directly to compute sub-strings.
</description><key id="23866758">4363</key><summary>Terms vector API should document the encoding which is used to compute the offsets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>v1.0.0.RC1</label></labels><created>2013-12-06T16:02:43Z</created><updated>2013-12-10T18:11:59Z</updated><resolved>2013-12-06T21:52:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files /><comments><comment>[Docs] Document which encoding should be used in order to make sense of the offsets returned by the term vectors API.</comment></comments></commit></commits></item><item><title>Percolate API : highlight issue on field with custom analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4362</link><project id="" key="" /><description>The highlight is empty when field has a custom analyzer. (Using version 1.0.0.Beta2)

Here are the steps to reproduce the behaviour :
First, create an index with a custom analyzer

```
 curl -XPUT "http://localhost:9200/" -d'
 {
    "settings": {
        "index.analysis.analyzer.latin.tokenizer": "whitespace",
        "index.analysis.analyzer.latin.type": "custom"
    }
 }
```

Then create a mapping using this custom analyzer :

```
 curl -XPUT "http://localhost:9200/idx/message/_mapping" -d'
 {
   "message": {
      "properties": {
         "message": {
            "type": "string",
            "store": true,
            "analyzer": "latin"
         }
      }
   }
}'
```

Index a percolator query :

```
 curl -XPUT "http://localhost:9200/idx/.percolator/q1" -d'
 {
    "query" : {
        "term" : {
            "message" : "bonsai"
        }
    }
 }'
```

Finaly, use _percolate API with highlight on the field that uses a custom analyzer :

```
 curl -XPOST "http://localhost:9200/idx/message/_percolate" -d'
 {
   "highlight": {
      "fields": {
         "message": {}
      }
   },
   "size": 10,
   "doc": {
      "message": "A new bonsai tree in the office"
   }
 }'
```

It gives the following which do not have any highlight :

```
{
   "took": 19,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "total": 1,
   "matches": [
      {
         "_index": "idx",
         "_id": "q1",
         "highlight": {}
      }
   ]
}
```

Note that when using _search API, highlight is done properly :

Index a doc :

```
 curl -XPUT "http://localhost:9200/idx/message/1" -d'
 {
     "message" : "A new bonsai tree in the office"
 }'
```

Search with highlight on the field that uses a custom analyzer :

```
 curl -XPOST "http://localhost:9200/idx/message/_search" -d'
 {
    "query":{
        "term": {"message": "bonsai"}
    },
    "highlight": {
        "fields": {
            "message":{}
        }
    }
 }'
```

Gives the following result :

```
{
   "took": 3,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.375,
      "hits": [
         {
            "_index": "idx",
            "_type": "message",
            "_id": "1",
            "_score": 0.375,
            "_source": {
               "message": "A new bonsai tree in the office"
            },
            "highlight": {
               "message": [
                  "A new &lt;em&gt;bonsai&lt;/em&gt; tree in the office"
               ]
            }
         }
      ]
   }
}
```
</description><key id="23865509">4362</key><summary>Percolate API : highlight issue on field with custom analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">pdesoyres</reporter><labels /><created>2013-12-06T15:44:30Z</created><updated>2013-12-13T11:19:29Z</updated><resolved>2013-12-13T11:19:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pdesoyres" created="2013-12-09T08:47:33Z" id="30113727">In fact the problem has nothing to do with analyzer, but when field has `"store"="yes"`. So this issue duplicates #4348 
</comment><comment author="martijnvg" created="2013-12-13T11:19:29Z" id="30502584">Closing this is issue b/c it is a duplicate.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>matched_filters randomly missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4361</link><project id="" key="" /><description>When searching with a query containing query_strings inside a bool query, the specified _name is randomly missing from the results.

https://gist.github.com/laurikari/7824057

Sometimes "matched_filters" is present in one or both of the results, sometimes not.  The behaviour appears to be random.

This reproduces both on OS X Mavericks and Ubuntu Linux 12.04.03 (kernel 3.2.0) using elasticsearch-0.90.7.
</description><key id="23860105">4361</key><summary>matched_filters randomly missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">laurikari</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-06T14:11:07Z</created><updated>2013-12-10T18:13:56Z</updated><resolved>2013-12-07T04:19:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-06T17:31:20Z" id="30012740">Thanks! I'm now able to reproduce it with a JUnit test but it does not fail at each run :-(
As you said, random failure!
</comment><comment author="dadoonet" created="2013-12-07T04:19:02Z" id="30047343">PR #4371 opened to address this issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Use random number of iteration for tests</comment><comment>Thanks @simonw for the review.</comment><comment>Related to #4361 and #4371.</comment></comments></commit><commit><files><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Use random number of iteration for tests</comment><comment>Thanks @simonw for the review.</comment><comment>Related to #4361 and #4371.</comment></comments></commit><commit><files><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/test/java/org/elasticsearch/search/matchedqueries/MatchedQueriesTests.java</file></files><comments><comment>Query String caching could cause matched_filters not working</comment></comments></commit></commits></item><item><title>Add wrapperSort to SortBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4360</link><project id="" key="" /><description>Add `wrapperSort` to `SortBuilders` like `QueryBuilders.wrapperQuery` and `FilterBuilders.wrapperFilter` allowing a JSON string representing a sort to be wrapped as a `SortBuilder`.
</description><key id="23858720">4360</key><summary>Add wrapperSort to SortBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hudsonb</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2013-12-06T13:47:17Z</created><updated>2015-10-14T13:47:23Z</updated><resolved>2015-10-14T12:22:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vedil" created="2015-05-06T19:20:26Z" id="99579209">I was stuck with similar issue and thought i need SortBuilder that could take a json, but i figured SearchRequestBuilder.setExtraSource would be sufficient for me.
</comment><comment author="clintongormley" created="2015-10-14T12:22:28Z" id="148031954">Just discussed this with the people who have been working on the query refactoring, who are of the opinion that we shouldn't be passing JSON in the Java API. They're also considering removing wrapperQuery, so I'm going to close this one.
</comment><comment author="hudsonb" created="2015-10-14T12:50:46Z" id="148040367">So there would be no way to go from JSON to QueryBuilder? I rely heavily on this currently.

I have a thick client which generates the queries in JSON, these JSON queries are sent through a web service which then uses the wrapperQuery to combine the query (using bool query) with some other clauses that are added by the server.
</comment><comment author="javanna" created="2015-10-14T12:59:08Z" id="148042090">I think it would make more sense to send a json request through REST rather than using the java api for that. The idea is that the java api uses the transport layer (binary protocol), so you should be specifying java objects and no parsing should be involved. On the other hand the REST layer does parse queries into java objects. Makes sense?
</comment><comment author="hudsonb" created="2015-10-14T13:46:55Z" id="148054504">From the client we couldn't because we don't expose elasticsearch directly.

From the service we could, but we make heavy use of the Java API there - the query is just one piece that happens to be JSON since its generated by the client and sent to the service as JSON.

For example as part of one service call we do something like this:

```
HistogramBuilder histogram = histogram("h")
                                                 .field(field)
                                                 .interval(interval)
                                                 .minDocCount(minCount);

client.prepareSearch(alias)
        .setSearchType(SearchType.COUNT)
        .setTypes(eventType)
        .setQuery(filteredQuery(query, wrapperFilter(request.getFilter()))
        .addAggregation(histogram)
        .execute();
```

The only use of straight JSON is in the wrapperFilter call, where we get the JSON filter spec from the request sent by the client.
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Add transport.publish_port setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4359</link><project id="" key="" /><description>Add transport.publish_port setting to allow users to specify the port
other cluster members should use when connecting to an instance. This
is needed for systems such as OpenShift, where cluster communication
needs to use a publicly accessibly proxy port, because the normal port
(9300) is bound to a private loopback IP address.
</description><key id="23858245">4359</key><summary>Add transport.publish_port setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ncdc</reporter><labels /><created>2013-12-06T13:39:39Z</created><updated>2014-06-14T08:17:52Z</updated><resolved>2014-01-17T21:18:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="caruccio" created="2014-01-14T17:17:52Z" id="32285597">+1
We need it so bad to run ES cluster on OpenShift.
</comment><comment author="ncdc" created="2014-01-17T18:15:01Z" id="32630606">@kimchy hey I was wondering if you had a chance to review my updated PR? And there's also #4732 for 0.90.x. Thanks!
</comment><comment author="s1monw" created="2014-01-17T20:28:52Z" id="32643463">Hey Andy, I think it looks good though. Do you think you can add the relevant documentation to the guide under /docs as well?
</comment><comment author="ncdc" created="2014-01-17T20:31:13Z" id="32643636">@s1monw sure, I can definitely do that!
</comment><comment author="ncdc" created="2014-01-17T20:39:05Z" id="32644221">Looking at the documentation and the other settings in the NettyTransport, is `transport.publish_port` right, or should it be `transport.tcp.publish_port`?
</comment><comment author="s1monw" created="2014-01-17T20:40:04Z" id="32644286">I think `transport.publish_port` is fine!
</comment><comment author="ncdc" created="2014-01-17T21:00:56Z" id="32645830">Added some docs. How does it look? Also, if you merge this in, what's the easiest way to get this into the 0.90 branch as well? @caruccio took my original PR and backported it (#4732) but it doesn't include the doc commit I just made...
</comment><comment author="s1monw" created="2014-01-17T21:04:15Z" id="32646100">LGTM I will pull this in and backport it to `0.90` as well
</comment><comment author="s1monw" created="2014-01-17T21:18:30Z" id="32647983">pushed to master and 0.90 thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Document http.cors-settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4358</link><project id="" key="" /><description>These settings are not mentioned anywhere else.
</description><key id="23852037">4358</key><summary>Document http.cors-settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexbrasetvik</reporter><labels /><created>2013-12-06T11:39:55Z</created><updated>2014-07-12T06:41:37Z</updated><resolved>2014-03-31T09:35:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-03-31T09:35:04Z" id="39069845">Merged, finally! Sorry it took so long, and thanks
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Introduce CHANGES.txt</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4357</link><project id="" key="" /><description>In order to easier create release notes, we should switch to a CHANGES.txt file.

The problem with commits is, that they are written, when the developer is
kneedeep in code, which results in more likely technology focused commits
instead of feature/functionality focused commit messages. This file should
be used to explain, what really happened and what has been fixed from a
user point of view. This file has the huge advantage if being edited after the commit/PR has been finished in order to ensure good and useful changelog messages.

I filled up this file with the releases of 0.90 until now and all the 1.0beta releases
</description><key id="23850591">4357</key><summary>Introduce CHANGES.txt</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-12-06T11:08:38Z</created><updated>2014-07-16T21:50:51Z</updated><resolved>2013-12-16T10:34:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits /></item><item><title>Inconsistent search results when running proximity searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4356</link><project id="" key="" /><description>When running a simple query_string search, specifying the search fields (prefixed with the type), the search returns results as expected.
When running a proximity search query_string, using the same search fields, the search fails. If I remove the type prefix from the search field name, the search works.
# EXAMPLE:
## Setup:

curl -XPUT 'http://localhost:9201/test/product/1' -d '{ 
    "desc": "description of product one with something" 
}'
curl -XPUT 'http://localhost:9200/test/product/2' -d '{ 
    "desc": "description of product two with something else" 
}'
curl -XPUT 'http://localhost:9200/test/product/3' -d '{ 
    "desc": "description of product three with something else again" 
}'
curl -XPUT 'http://localhost:9200/test/customer/1' -d '{ 
    "desc": "description of customer one with something" 
}'
curl -XPUT 'http://localhost:9200/test/customer/2' -d '{ 
    "desc": "description of customer two with something else" 
}'
curl -XPUT 'http://localhost:9200/test/customer/3' -d '{ 
    "desc": "description of customer three with something else again" 
}'
## Simple Search:

curl -XPOST 'http://localhost:9200/test/_search?pretty=true' -d '{
  "query" : {
      "query_string" : {
          "query" : "description",
          "fields" : ["customer.desc", "product.desc"]
      }
  }
}'
## Failing proximity search:

curl -XPOST 'http://localhost:9200/test/_search?pretty=true' -d '{
  "query" : {
      "query_string" : {
          "query" : "\"customer else\"~5",
          "fields" : ["customer.desc", "product.desc"]
      }
  }
}'
## Successful proximity search:

curl -XPOST 'http://localhost:9200/test/_search?pretty=true' -d '{
  "query" : {
      "query_string" : {
          "query" : "\"customer else\"~5",
          "fields" : ["desc"]
      }
  }
}'

We want to be able to allow the user to search specific fields, hence prefixing the 'desc' field with either 'customer' or 'product' (our real ES instance has many 'title' and 'desc' fields, so we need to prefix these fields with the type). Is this an incorrect use of fields, or a defect with proximity search?

Java: 1.7.0_45
ES: 90.5
OS: Windows 7
</description><key id="23848527">4356</key><summary>Inconsistent search results when running proximity searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spaisey</reporter><labels><label>bug</label><label>regression</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-12-06T10:22:07Z</created><updated>2013-12-10T18:14:15Z</updated><resolved>2013-12-06T22:11:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-12-06T13:52:54Z" id="29995137">can you please ask this question as it is on the mailing list. This is a bug tracker while this seems to be a question rather than a bug. 

thanks,

simon
</comment><comment author="clintongormley" created="2013-12-06T15:01:52Z" id="30000276">@s1monw Actually I've found two bugs in the `query_string` query from the above.  More clearly demonstrated here:

```
curl -XPUT "http://localhost:9200/test/product/1" -d'
{
   "desc": "one two three"
}'

curl -XPUT "http://localhost:9200/test/customer/2" -d'
{
   "desc": "one two three"
}'
```

First bug: Setting the default field to `customer.field` does not limit the results to just docs of type `customer`.  No type filter is added, and both docs are returned:

```
curl -XPOST "http://localhost:9200/test/_search" -d'
{
   "query": {
      "query_string": {
         "default_field": "customer.desc",
         "query": "\"one three\"~5"
      }
   }
}'
```

Results:

```
  "hits": [
     {
        "_index": "test",
        "_type": "customer",
        "_id": "2",
        "_score": 0.2169777,
        "_source": {
           "desc": "one two three"
        }
     },
     {
        "_index": "test",
        "_type": "product",
        "_id": "1",
        "_score": 0.2169777,
        "_source": {
           "desc": "one two three"
        }
     }
  ]
```

When `customer.desc` is specified using `fields`, the type filter is correctly applied:

```
curl -XPOST "http://localhost:9200/test/_search" -d'
{
   "query": {
      "query_string": {
         "fields": [
            "customer.desc"
         ],
         "query": "\"one two\"~5"
      }
   }
}'
```

This returns:

```
  "hits": [
     {
        "_index": "test",
        "_type": "customer",
        "_id": "2",
        "_score": 0.30685282,
        "_source": {
           "desc": "one two three"
        }
     }
  ]
```

However, when the documents _requires_ `slop` on the phrase in order to match, nothing is returned:

```
curl -XPOST "http://localhost:9200/test/_search" -d'
{
   "query": {
      "query_string": {
         "fields": [
            "customer.desc"
         ],
         "query": "\"one three\"~5"
      }
   }
}'
```
</comment><comment author="s1monw" created="2013-12-06T21:39:44Z" id="30031785">Thanks @clintongormley for reopening. I must would have missed it! Sorry for jumping to conclusion so quickly.
I added tests and fixed the problem in the attached the commit. I will open a PR soonish
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java</file><file>src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java</file><file>src/test/java/org/elasticsearch/search/query/SimpleQueryTests.java</file></files><comments><comment>Apply slop correctly if phrase query is wrapped in a filtered query.</comment></comments></commit></commits></item><item><title>Internal: Make Node and Client interfaces Closeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4355</link><project id="" key="" /><description>org.elasticsearch.node.Node and org.elasticsearch.client.Client interfaces should extend Closeable. Doing so will allow user to use Client and Node instances with try-with-resources, IOUtils.closeQuietly, Guava Closer and similar utilities.

This change is backward compatible and doesn't require any changes except adding "extends Closeable" in two places. 
</description><key id="23827710">4355</key><summary>Internal: Make Node and Client interfaces Closeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">udat</reporter><labels><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2013-12-05T23:48:52Z</created><updated>2014-07-16T12:05:55Z</updated><resolved>2014-06-17T10:28:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/action/Action.java</file><file>src/main/java/org/elasticsearch/action/ActionRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/ClientAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/ClusterAction.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/hotthreads/NodesHotThreadsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/info/NodesInfoRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/restart/NodesRestartRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/shutdown/NodesShutdownRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/node/stats/NodesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/delete/DeleteRepositoryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/get/GetRepositoriesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/repositories/put/PutRepositoryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/reroute/ClusterRerouteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/settings/ClusterUpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/shards/ClusterSearchShardsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/create/CreateSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/delete/DeleteSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/get/GetSnapshotsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/restore/RestoreSnapshotRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/snapshots/status/SnapshotsStatusRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/state/ClusterStateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/cluster/tasks/PendingClusterTasksRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/IndicesAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/IndicesAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/exists/AliasesExistRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/BaseAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/alias/get/GetAliasesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/analyze/AnalyzeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/cache/clear/ClearIndicesCacheRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/close/CloseIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/delete/DeleteIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/indices/IndicesExistsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/exists/types/TypesExistsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/flush/FlushRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/delete/DeleteMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetMappingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetMappingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/open/OpenIndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/optimize/OptimizeRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/recovery/RecoveryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/refresh/RefreshRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/segments/IndicesSegmentsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/get/GetSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/settings/put/UpdateSettingsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/stats/IndicesStatsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/delete/DeleteIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/get/GetIndexTemplatesRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/template/put/PutIndexTemplateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/validate/query/ValidateQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/DeleteWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersAction.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/get/GetWarmersRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/admin/indices/warmer/put/PutWarmerRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/bench/AbortBenchmarkAction.java</file><file>src/main/java/org/elasticsearch/action/bench/AbortBenchmarkRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/bench/BenchmarkAction.java</file><file>src/main/java/org/elasticsearch/action/bench/BenchmarkRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/bench/BenchmarkStatusAction.java</file><file>src/main/java/org/elasticsearch/action/bench/BenchmarkStatusRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkAction.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java</file><file>src/main/java/org/elasticsearch/action/bulk/BulkRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/count/CountAction.java</file><file>src/main/java/org/elasticsearch/action/count/CountRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteAction.java</file><file>src/main/java/org/elasticsearch/action/delete/DeleteRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryAction.java</file><file>src/main/java/org/elasticsearch/action/deletebyquery/DeleteByQueryRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/explain/ExplainAction.java</file><file>src/main/java/org/elasticsearch/action/explain/ExplainRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/get/GetAction.java</file><file>src/main/java/org/elasticsearch/action/get/GetRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetAction.java</file><file>src/main/java/org/elasticsearch/action/get/MultiGetRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/index/IndexAction.java</file><file>src/main/java/org/elasticsearch/action/index/IndexRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisAction.java</file><file>src/main/java/org/elasticsearch/action/mlt/MoreLikeThisRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/MultiPercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateAction.java</file><file>src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/ClearScrollAction.java</file><file>src/main/java/org/elasticsearch/action/search/ClearScrollRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/MultiSearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/MultiSearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchAction.java</file><file>src/main/java/org/elasticsearch/action/search/SearchRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollAction.java</file><file>src/main/java/org/elasticsearch/action/search/SearchScrollRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/suggest/SuggestAction.java</file><file>src/main/java/org/elasticsearch/action/suggest/SuggestRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/broadcast/BroadcastOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/AcknowledgedRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/MasterNodeReadOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/master/info/ClusterInfoRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/nodes/NodesOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/replication/IndicesReplicationOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/replication/ShardReplicationOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/single/custom/SingleCustomOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/single/instance/InstanceShardOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/support/single/shard/SingleShardOperationRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/MultiTermVectorsRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorAction.java</file><file>src/main/java/org/elasticsearch/action/termvector/TermVectorRequestBuilder.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateAction.java</file><file>src/main/java/org/elasticsearch/action/update/UpdateRequestBuilder.java</file><file>src/main/java/org/elasticsearch/client/Client.java</file><file>src/main/java/org/elasticsearch/client/ClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/ElasticsearchClient.java</file><file>src/main/java/org/elasticsearch/client/FilterClient.java</file><file>src/main/java/org/elasticsearch/client/IndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/internal/InternalClient.java</file><file>src/main/java/org/elasticsearch/client/internal/InternalClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/internal/InternalGenericClient.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClient.java</file><file>src/main/java/org/elasticsearch/client/node/NodeClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/node/NodeIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/support/AbstractIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/TransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportClusterAdminClient.java</file><file>src/main/java/org/elasticsearch/client/transport/support/InternalTransportIndicesAdminClient.java</file><file>src/main/java/org/elasticsearch/node/Node.java</file><file>src/main/java/org/elasticsearch/node/internal/InternalNode.java</file><file>src/test/java/org/elasticsearch/action/admin/cluster/stats/ClusterStatsTests.java</file><file>src/test/java/org/elasticsearch/cluster/MinimumMasterNodesTests.java</file><file>src/test/java/org/elasticsearch/cluster/SpecificMasterNodesTests.java</file><file>src/test/java/org/elasticsearch/cluster/allocation/ShardsAllocatorModuleTests.java</file><file>src/test/java/org/elasticsearch/indices/settings/UpdateNumberOfReplicasTests.java</file><file>src/test/java/org/elasticsearch/search/preference/SearchPreferenceTests.java</file><file>src/test/java/org/elasticsearch/snapshots/AbstractSnapshotTests.java</file><file>src/test/java/org/elasticsearch/stresstest/refresh/RefreshStressTest1.java</file><file>src/test/java/org/elasticsearch/stresstest/search1/ParentChildStressTest.java</file><file>src/test/java/org/elasticsearch/test/CompositeTestCluster.java</file><file>src/test/java/org/elasticsearch/test/ElasticsearchIntegrationTest.java</file><file>src/test/java/org/elasticsearch/test/ExternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/InternalTestCluster.java</file><file>src/test/java/org/elasticsearch/test/TestCluster.java</file><file>src/test/java/org/elasticsearch/test/client/FilterClient.java</file><file>src/test/java/org/elasticsearch/test/client/RandomizingClient.java</file><file>src/test/java/org/elasticsearch/test/hamcrest/ElasticsearchAssertions.java</file><file>src/test/java/org/elasticsearch/tribe/TribeTests.java</file></files><comments><comment>[CLIENT] Remove unnecessary intermediate interfaces</comment></comments></commit></commits></item><item><title>when installing plugins, messaging is super confusing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4354</link><project id="" key="" /><description>I installed a plugin today:

```
zck:~$ sudo /usr/share/elasticsearch/bin/plugin --install elasticsearch-inout-plugin --url file:///home/zachary/elasticsearch-inout-plugin-0.5.0.jar 
-&gt; Installing elasticsearch-inout-plugin...
Trying file:/home/zachary/elasticsearch-inout-plugin-0.5.0.jar...
Downloading ..DONE
Installed elasticsearch-inout-plugin
Usage:
    -u, --url     [plugin location]   : Set exact URL to download the plugin from
    -i, --install [plugin name]       : Downloads and installs listed plugins [*]
    -r, --remove  [plugin name]       : Removes listed plugins
    -l, --list                        : List installed plugins
    -v, --verbose                     : Prints verbose messages
    -h, --help                        : Prints this help message

 [*] Plugin name could be:
     elasticsearch/plugin/version for official elasticsearch plugins (download from download.elasticsearch.org)
     groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)
     username/repository          for site plugins (download from github master)

Message:
   Command [--url] unknown.
zck:~$ 
```

Why did it print the "Usage" text? Why did it tell me that the `--url` command is unknown? For that matter, can't it infer the plugin name from the package somehow? Having to supply a url _and_ a name is sub-optimal.
</description><key id="23826186">4354</key><summary>when installing plugins, messaging is super confusing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zck</reporter><labels /><created>2013-12-05T23:16:43Z</created><updated>2013-12-10T18:14:29Z</updated><resolved>2013-12-07T03:14:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-12-06T04:11:18Z" id="29962412">What is your elasticsearch version?
</comment><comment author="zck" created="2013-12-06T21:52:10Z" id="30032661">I'm running 0.90.2.
</comment><comment author="dadoonet" created="2013-12-07T03:14:09Z" id="30046401">Could you update to 0.90.7?
This issue has been fixed.
If you still have the problem, please reopen this issue.
Thanks.
</comment><comment author="zck" created="2013-12-08T19:26:58Z" id="30089819">Ah, cool. I'll have to try to convince the people controlling that machine to upgrade. Thanks!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Custom Analyzer Used in Mapping Doesn't Work When Using Default Field in Query String</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4353</link><project id="" key="" /><description>I defined a custom analyzer called id_analyzer, and set it as the analyzer for the field "related_docs". I input a sample document and then executed a query that matches that document. When I used the default field, it didn't return anything. But if I set fields to "related_docs", it returned that document.

To see the problem, execute the following requests one by one using "sense", a Chrome plugin for ElasticSearch 

```
POST test
{
    "settings": {
        "analysis": {
            "analyzer": {
                "id_analyzer":{
                    "type": "custom",
                    "tokenizer": "whitespace",
                    "filter": ["lowercase", "id_shingle", "id_stop"]
                }
            },
            "filter": {
                "id_shingle": {
                    "type": "shingle",
                    "max_shingle_size": 3,
                    "token_separator": ""
                },
                "id_stop": {
                    "type": "stop",
                    "stopwords": ["us", "a1", "a2", "a9", "b1", "b2", "c1", "c2", "c3", "e", "h", "p1", "p2", "p3", "p4", "p9", "s"]
                }
            }
        }
    }
}

PUT test/doc/_mapping
{
    "doc": {
        "properties": {
            "related_docs":  {
                "type": "string",
                "analyzer": "id_analyzer",
                "store": "false"
            }
        }
    }
}

POST test/doc
{
    "related_docs": ["US 13/862983 A1", "US 12244551"]
}

POST test/doc/_search
{
    "query": {
        "query_string": {
           "query": "US13\\/862983A1",
           "fields": ["related_docs"], 
           "analyzer": "id_analyzer"
        }
    }
}

POST test/doc/_search
{
    "query": {
        "query_string": {
           "query": "US13\\/862983A1",
           "analyzer": "id_analyzer"
        }
    }
}
```

The last two requests show you the difference.
</description><key id="23816273">4353</key><summary>Custom Analyzer Used in Mapping Doesn't Work When Using Default Field in Query String</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jennaliu</reporter><labels /><created>2013-12-05T20:46:21Z</created><updated>2014-07-08T19:20:25Z</updated><resolved>2014-07-08T19:20:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T19:20:25Z" id="48387328">Hi @jennaliu 

The `_all` field has its own analyzer which defaults to the `standard` analyzer.  It takes the string values of other fields, not the tokens generated from each field.  
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Stop FVH from throwing away some query boosts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4352</link><project id="" key="" /><description>The FVH was throwing away some boosts on queries stopping a number of
ways to boost phrase matches to the top of the list of fragments from
working.

The plain highlighter also doesn't work for this but that is because it
doesn't support the concept of terms at different positions having different
weights.

Closes #4351
</description><key id="23813041">4352</key><summary>Stop FVH from throwing away some query boosts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2013-12-05T20:02:58Z</created><updated>2014-06-18T23:46:56Z</updated><resolved>2014-01-08T10:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-12-05T21:00:20Z" id="29937437">Uhg, these all look like trouble in lucene....
</comment><comment author="nik9000" created="2013-12-05T22:56:25Z" id="29947457">The queries based on QueryString are having trouble in the FVH because when it flattens queries it tends to squash out the boosts on those queries.  This causes things like: 
`(field1:match field1:all field1:the field1:things) (field1:"match all the things")^100`
 to map just like this:
`field1:match field1:all field1:the field1:things field1:"match all the things"`.

I'm not sure about the plain highlighter....
</comment><comment author="nik9000" created="2013-12-06T16:37:13Z" id="30008312">Funny thing: I can't reproduce the plain highlighter failures in Lucene right now.  I'll keep at it.
</comment><comment author="nik9000" created="2013-12-06T17:35:37Z" id="30013042">I just pushed an update which should actually fix the problem for the FVH.

I was able to reproduce the problem with the plain highlighter but wasn't able to fix it because the plain highlighter doesn't support assigning a different score to the same term at different positions.

I also filed an issue with Lucene at https://issues.apache.org/jira/browse/LUCENE-5361 and submitted a more exhaustive fix there.

The fix here just can be removed once Lucene is updated.  I figure applying the fix directly to Elasticsearch is OK because it only goes in CustomFieldQuery and that class already needs to have things removed from it when Lucene is updated.
</comment><comment author="nik9000" created="2014-01-04T19:05:04Z" id="31585834">Can anyone look at this or the LUCENE issue?  This issue's twin on my bug tracker has been staring at me mockingly for a while now.
</comment><comment author="jpountz" created="2014-01-04T20:38:33Z" id="31588010">Sorry for the delay, Nick. I'll have a look next week.
</comment><comment author="nik9000" created="2014-01-04T21:23:23Z" id="31588979">Thanks!

Sent from my iPhone

&gt; On Jan 4, 2014, at 3:38 PM, Adrien Grand notifications@github.com wrote:
&gt; 
&gt; Sorry for the delay, Nick. I'll have a look next week.
&gt; 
&gt; —
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="jpountz" created="2014-01-07T19:30:36Z" id="31770463">I left minor comments but PR looks good to me, let's get this in! Thanks again Nik!
</comment><comment author="nik9000" created="2014-01-07T21:13:04Z" id="31779837">Pushed a rebased, corrected version.
</comment><comment author="jpountz" created="2014-01-08T10:59:13Z" id="31822261">Pushed. Thanks again (and again and again) Nik!
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>A bunch of ways to highlight boost phrase matches over general term matches in score order highlighted fragments don't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4351</link><project id="" key="" /><description>A bunch of ways to highlight boost phrase matches over general term matches in score order highlighted fragments don't work.  I don't have time to make a curl recreation at the moment but I'll send a pull request with failing tests that I _think_ should all pass.  Note: they all pass for the postings highlighter, but the other two don't have it together.
</description><key id="23812958">4351</key><summary>A bunch of ways to highlight boost phrase matches over general term matches in score order highlighted fragments don't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v0.90.10</label><label>v1.0.0.RC1</label></labels><created>2013-12-05T20:01:46Z</created><updated>2014-01-08T13:34:24Z</updated><resolved>2014-01-08T10:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-12-05T20:03:29Z" id="29932549">I'll have a look at getting these to not fail at some point....
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/apache/lucene/search/vectorhighlight/CustomFieldQuery.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Stop FVH from throwing away some query boosts</comment></comments></commit></commits></item><item><title>Ordinal-based string aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4350</link><project id="" key="" /><description>String terms aggregations today work by accumulating counts into a hash table that stores the term values as keys (just like facets when provided `map` as an `execution_hint`).

Similarly to facets, we should also have an execution mode that allows to build the buckets based on string ordinals. This proved to be much faster than the `map` execution mode for facets, so hopefully it should help speed-up string terms aggregations as well.
</description><key id="23797130">4350</key><summary>Ordinal-based string aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.RC1</label></labels><created>2013-12-05T16:10:51Z</created><updated>2013-12-13T14:34:59Z</updated><resolved>2013-12-13T14:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/common/util/BigArrays.java</file><file>src/main/java/org/elasticsearch/common/util/BigLongArray.java</file><file>src/main/java/org/elasticsearch/common/util/LongArray.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/StringTermsAggregator.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsAggregatorFactory.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsBuilder.java</file><file>src/main/java/org/elasticsearch/search/aggregations/bucket/terms/TermsParser.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/AggregationContext.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/FieldDataSource.java</file><file>src/main/java/org/elasticsearch/search/aggregations/support/bytes/BytesValuesSource.java</file><file>src/test/java/org/elasticsearch/benchmark/search/aggregations/TermsAggregationSearchBenchmark.java</file><file>src/test/java/org/elasticsearch/common/util/BigArraysTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/RandomTests.java</file><file>src/test/java/org/elasticsearch/search/aggregations/bucket/StringTermsTests.java</file></files><comments><comment>Aggregations: Ordinals-based string bucketing support.</comment></comments></commit></commits></item><item><title>Make _all payload-based boosting more space efficient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4349</link><project id="" key="" /><description>When copying content from fields into `_all`, per-field boosts are copied into payloads (when the boost is != 1). These payloads store 4 bytes which are the 4 bytes of the float boost.

In practice this is quite some overhead since this payload needs to be stored for every occurrence of every term. I think using Lucene's `SmallFloat` utility class to store the boosts in a single-byte (like Lucene does for norms) would be a better trade-off?

Another option I'm thinking of is trying to improve Lucene to compress payloads: since the postings format is based on blocks, all payloads are stored sequentially, and using lz compression such as lz4 could easily take advantage of the fact that payloads are redundant (if you have only 10 string fields, there are at most 10 unique values of the boost although there are at least 128 positions per block).
</description><key id="23796495">4349</key><summary>Make _all payload-based boosting more space efficient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>adoptme</label><label>enhancement</label></labels><created>2013-12-05T16:02:13Z</created><updated>2016-11-06T10:59:42Z</updated><resolved>2016-11-06T10:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-24T17:15:45Z" id="68064944">@jpountz - is this still relevant?
</comment><comment author="jpountz" created="2015-08-26T14:34:57Z" id="135042135">Yes it is! I opened https://issues.apache.org/jira/browse/LUCENE-6764 on the Lucene side, but even if it gets in using 1 byte instead of 4 would still be useful.
</comment><comment author="clintongormley" created="2016-11-06T10:59:42Z" id="258673596">Closing in favour of https://github.com/elastic/elasticsearch/issues/19784
</comment></comments><attachments /><subtasks /><customfields /><commits /></item><item><title>Percolate API : Highlight on sub-field issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/4348</link><project id="" key="" /><description>Highlight is not done on sub-field of an object field when using Percolate API (**1.0.0.Beta2**) :

create index :

```
 curl -XPUT "http://localhost:9200/test-index"
```

create a mapping with a field "content" containing two sub-fields (which are stored)

```
 curl -XPUT "http://localhost:9200/test-index/test-mapping/_mapping" -d'
 {"test-mapping":
     {
         "properties": {
             "content" : {
                 "properties": {
                     "message": {"type": "string","store": "yes"},
                     "lang": {"type": "string","store": "yes"}
                 }
             }
         }
     }
 }'
```

index query into `.percolator` :

```
 curl -XPUT "http://localhost:9200/test-index/.percolator/q1" -d'
 {
     "query" : {
         "term" : {
             "content.message" : "bonsai"
         }
     }
 }'
```

use Percolate API with highlight on `content.message` field :

```
 curl -XPOST "http://localhost:9200/test-index/test-mapping/_percolate" -d'
 {
     "size":10,
     "highlight": {
         "fields": {"content.message":{}}
     },
     "doc" : {
         "content": {
             "message": "bonsai tree office",
             "lang": "en"
         }
     }
 }'
```

result doesn't show highlight :

```
 {
    "took": 3,
    "_shards": {
       "total": 5,
       "successful": 5,
       "failed": 0
    },
    "total": 1,
    "matches": [
       {
          "_index": "test-index",
          "_id": "q1",
          "highlight": {}
       }
    ]
 }
```
</description><key id="23790722">4348</key><summary>Percolate API : Highlight on sub-field issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">pdesoyres</reporter><labels><label>bug</label><label>v1.0.0.RC1</label></labels><created>2013-12-05T14:42:19Z</created><updated>2013-12-16T09:01:41Z</updated><resolved>2013-12-13T12:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-12-07T21:48:08Z" id="30065752">Thanks for reporting this! This bug doesn't seem to be caused by the fact that `content` is a subfield, but that this field is stored separately in the mapping (`"store" : "yes"`).
</comment></comments><attachments /><subtasks /><customfields /><commits><commit><files><file>src/main/java/org/elasticsearch/percolator/PercolatorService.java</file><file>src/main/java/org/elasticsearch/search/highlight/FastVectorHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightBuilder.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlightUtils.java</file><file>src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java</file><file>src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/PostingsHighlighter.java</file><file>src/main/java/org/elasticsearch/search/highlight/SearchContextHighlight.java</file><file>src/test/java/org/elasticsearch/percolator/PercolatorTests.java</file><file>src/test/java/org/elasticsearch/search/highlight/HighlighterSearchTests.java</file></files><comments><comment>Added the `force_source` option to highlighting that enforces to use of the _source even if there are stored fields.</comment><comment>The percolator uses this option to deal with the fact that the MemoryIndex doesn't support stored fields,</comment><comment>this is possible b/c the _source of the document being percolated is always present.</comment></comments></commit></commits></item></channel></rss>